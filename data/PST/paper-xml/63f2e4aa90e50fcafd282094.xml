<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Search to Capture Long-range Dependency with Stacking GNNs for Graph Classification</title>
				<funder ref="#_qydZCVd">
					<orgName type="full">NSFC</orgName>
				</funder>
				<funder>
					<orgName type="full">CCF-Tencent Open Research Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-02-17">17 Feb 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lanning</forename><surname>Wei</surname></persName>
							<email>weilanning18z@ict.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhiqiang</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
							<email>zhaohuan@4paradigm.com</email>
						</author>
						<author>
							<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computing Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Chinese Academy of Sciences</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences Lenovo</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">4Paradigm. Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">WWW &apos;23</orgName>
								<address>
									<addrLine>May 1-5</addrLine>
									<postCode>2023</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Search to Capture Long-range Dependency with Stacking GNNs for Graph Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-02-17">17 Feb 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3543507.3583486</idno>
					<idno type="arXiv">arXiv:2302.08671v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Neural Networks</term>
					<term>Graph Classification</term>
					<term>Neural Architecture Search</term>
					<term>Over-smoothing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, Graph Neural Networks (GNNs) have been popular in the graph classification task. Currently, shallow GNNs are more common due to the well-known over-smoothing problem facing deeper GNNs. However, they are sub-optimal without utilizing the information from distant nodes, i.e., the long-range dependencies. The mainstream methods in the graph classification task can extract the long-range dependencies either by designing the pooling operations or incorporating the higher-order neighbors, while they have evident drawbacks by modifying the original graph structure, which may result in information loss in graph structure learning. In this paper, by justifying the smaller influence of the over-smoothing problem in the graph classification task, we evoke the importance of stacking-based GNNs and then employ them to capture the long-range dependencies without modifying the original graph structure. To achieve this, two design needs are given for stacking-based GNNs, i.e., sufficient model depth and adaptive skipconnection schemes. By transforming the two design needs into designing data-specific inter-layer connections, we propose a novel approach with the help of neural architecture search (NAS), which is dubbed LRGNN (Long-Range Graph Neural Networks). Extensive experiments on five datasets show that the proposed LRGNN can achieve the best performance, and obtained data-specific GNNs with different depth and skip-connection schemes, which can better capture the long-range dependencies. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In recent years, Graph Neural Networks (GNNs) have been the stateof-the-art (SOTA) method on graph classification <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b53">54]</ref>, a popular task which can be applied into various domains, e.g., chemistry <ref type="bibr" target="#b10">[11]</ref>, bioinformatics <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b49">50]</ref>, text <ref type="bibr" target="#b54">[55]</ref> and social networks <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b50">51]</ref>. In general, they encode the graph structure to learn node embeddings with the help of the message-passing scheme <ref type="bibr" target="#b10">[11]</ref>, i.e., aggregating messages from the connected nodes. Then, the graph representation vector can be generated with one readout operation, e.g., take the mean or summation of all node embeddings. In the literature, two-or three-layer GNNs are widely used since the performance decreases as the network goes deeper <ref type="bibr" target="#b20">[21]</ref>. However, it limits the ability of GNNs to capture the long-range dependencies, i.e., incorporate information from long-distant neighbors <ref type="bibr" target="#b24">[25]</ref>, which may be useful in the graph classification task <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>For the graph classification task, the majority of methods focus on designing the pooling operations to extract the hierarchical information in the graph <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b49">50]</ref>. We review the literature from the perspective of long-range dependencies, and then observe that not all pooling operations are helpful in extracting the long-range dependencies. The grouping-based pooling operations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b52">53]</ref> group nodes into clusters and then re-design the edges between these clusters. These newly added edges may shorten the distance between node pairs, and then faster the feature propagation in the graph. On the contrary, the selection-based ones <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19]</ref> construct the coarse graph by removing the low-ranked nodes and edges connected with them, which means the distance between nodes will not be shortened and are not helpful in incorporating the longdistant neighbors. Apart from pooling operations, several methods tend to incorporate the higher-order neighbors directly. For example, GraphTrans <ref type="bibr" target="#b13">[14]</ref> adopts the transformer module which allows the interactions between all node pairs. Similarly, <ref type="bibr" target="#b28">[29]</ref> provides one virtual node which connected with all nodes, and <ref type="bibr" target="#b1">[2]</ref> designs one fully adjacent (FA) aggregation layer in which all nodes are connected directly.</p><p>Despite the popularity of these methods in graph classification, they are deficient when capturing long-range dependencies on graphs. The aforementioned methods update the graph structure and may result in information loss in graph structure learning, either by generating the coarse graph or connecting to higher-order neighbors. Considering this, we revisit the stacking-based GNNs, which is a direct approach to obtaining the long-range dependencies while keeping the graph structures unchanged. In general, scaling the model depth is the most common way to improve the model performance <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. and deeper stacking-based GNNs further bring the larger receptive field to incorporate the longer-distant neighbors. Nevertheless, the major concern to utilize the deeper GNNs is the over-smoothing problem <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b46">47]</ref>, i.e., the connected nodes tend to have similar features as the network goes deeper, which results in a performance drop in the node classification task. Yet, in this paper, we justify the smaller influence of this problem on the graph classification task compared with the node classification task both in theoretical analysis and experimental results. Therefore, designing stacking-based GNNs is indeed a feasible solution to capture the long-range dependencies.</p><p>Motivated by this, we propose a novel method LRGNN (Long-Range Graph Neural Networks), and employ the stacking-based GNNs to capture the long-range dependencies in graphs. There are two aspects that will affect the utilization of the long-distant neighbors: (a) sufficient model depth is required to incorporate the longer-distant neighbors; (b) adaptive skip-connection schemes are required considering the information mixing from different ranges of neighbors. The former is correlated with the inter-layer connections between the consecutive layers while the latter one related to the inconsecutive layers. Therefore, designing effective stacking-based GNNs to capture the long-range dependencies can be achieved by designing the inter-layer connections adaptively. We adopt the widely used neural architecture search (NAS) methods to achieve this.</p><p>To be specific, we provide one framework to design the interlayer connections, which contains a set of learnable connections in the directed acyclic graphs (DAG). Two candidate choices are provided to represent the "used" and "unused" states in learnable connections, and then designing the inter-layer connections is transformed into deciding which choice should be adopted in each learnable connection. Then, the search space of LRGNN can be constructed based on the combinations of those connection choices, while the search space size is growing exponentially as the network goes deeper. Considering the search efficiency, we further provide one cell-based framework that only enables the connection designs in each cell, on top of which two variants LRGNN-Diverse and LRGNN-Repeat can be constructed. The differentiable search algorithm is adopted in this paper to enable the adaptive architecture design. In the experiments, we first evaluate the rationality of LRGNN by showing the higher performance achieved with sufficient model depth and various skip-connection schemes. Then, the extensive experimental results demonstrate that the proposed LRGNN can achieve the SOTA performance by designing GNNs with the inter-layer connections adaptively, on top of which the effectiveness of the proposed method can be verified.</p><p>To summarize, our contributions are as follows:</p><p>? We evoke the importance of stacking-based GNNs in extracting the long-range dependencies, and verify the smaller influence of the over-smoothing problem on the graph classification, on top of which two design needs are provided for designing stacking-based GNNs , i.e., the sufficient GNN depth and adaptive skip-connection schemes. ? To meet these two design needs, we firstly unify them into designing the inter-layer connections in stacking-based GNNs, and then achieve this with the help of NAS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK: GRAPH NEURAL NETWORK (GNN)</head><p>GNNs have advantages in encoding the graph structure information with the help of the message-passing scheme <ref type="bibr" target="#b10">[11]</ref>, i.e., aggregating the messages from connected nodes. It can be represented as h ? = W(h ? , AGGR{h ? , ? ? N (?)}), where W is the learnable parameter, AGGR is the aggregation function used in this aggregation operation. Diverse aggregation operations are proposed and widely used in graph representation learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">33]</ref>. Based on these aggregation operations, one GNN can be constructed by stacking these aggregation operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Long-range Dependency Extraction in Graph Classification</head><p>In the graph classification task, existing literature can be grouped into three categories according to their methods in extracting the long-range dependencies, i.e., designing pooling operations, incorporating the higher-order neighbors, and stacking GNN layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Designing pooling operations.</head><p>Pooling operations are widely used in the graph classification task, and they aim to generate one coarse graph to extract the hierarchical information. They can be classified into two groups, i.e., the selection-based ones and the grouping-based ones. The selection-based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19]</ref> focus on evaluating the node importance. The top-ranked nodes are preserved, and then they construct the coarse graph by dropping the edges connected with the un-selected nodes. On the contrary, the grouping-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b52">53]</ref> aim to group nodes into several clusters based on their similarities. They first design one assignment matrix, on top of which the cluster features and new edges are constructed.</p><p>Despite the success of these methods in the graph classification task, not all pooling operations help obtain the long-range dependencies in the graph. The selection-based operations drop the edges connected with the un-selected nodes, and then the shortest path between node pairs will not decrease. Therefore, these pooling operations cannot faster the feature propagation in the graph. The grouping-based pooling operations reconstruct the edges in the coarse graph, and these edges may shorten the distance between the node pairs. 2.1.2 Incorporating higher-order neighbors. The general aggregation operation only propagates messages from the connected neighbors. By connecting nodes with higher-order neighbors, the longrange dependencies can be obtained with fewer aggregation operations. In the graph classification task, <ref type="bibr" target="#b28">[29]</ref> provides the virtual node which is connected with the other nodes, on top of which the distance for each node pair is less than two. <ref type="bibr" target="#b1">[2]</ref> provides a fully-adjacent layer at the end of GNNs, in which every pair of nodes are connected with an edge. Transformer modules <ref type="bibr" target="#b31">[32]</ref> are designed to communicate with other nodes. Existing methods designed diverse positional encoders to learn graph structures. For example, GraphTrans <ref type="bibr" target="#b13">[14]</ref> uses the stacked GNNs to encode the graph structure. Graphormer <ref type="bibr" target="#b48">[49]</ref> designs three encoders to embed the node centrality, node pairs, and edge features separately, and then applied the Transformer on these node sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Stacking GNN layers.</head><p>Apart from designing specific operations to obtain the long-range dependencies, stacking more GNN layers can incorporate the message from longer-distant neighbors. To be specific, each aggregation operation aggregates messages from the connected neighbors, on top of which the receptive field of each node can be expanded one hop away. Therefore, for one ?-layer stacking GNN, each node can incorporate the messages from its ?-hop neighbors. Therefore, to extract the long-range dependencies which may be important in the graph classification task, one GNN can stack more aggregation operations. In the graph classification task, GIN <ref type="bibr" target="#b45">[46]</ref> aims to design power aggregation operations, and then five layers are applied in GNN. DGCNN <ref type="bibr" target="#b53">[54]</ref> stacks three graph convolution layers, and then ranks nodes based on the node features, on top of which the graph representations can be generated with those top-ranked ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Architecture Search</head><p>Researchers tried to design GNN architectures by neural architecture search (NAS) automatically <ref type="bibr" target="#b35">[36]</ref>. The majority of these methods focus on designing the aggregation layers in GNNs, e.g., Graph-NAS <ref type="bibr" target="#b9">[10]</ref> and GraphGym <ref type="bibr" target="#b51">[52]</ref> provide diverse dimensions and candidate operations to design the GNN layers. Besides, SANE <ref type="bibr" target="#b57">[58]</ref>, SNAG <ref type="bibr" target="#b56">[57]</ref>, F2GNN <ref type="bibr" target="#b42">[43]</ref> and AutoGraph <ref type="bibr" target="#b21">[22]</ref> design the skip-connections based on the stacked aggregation operations. Policy-GNN <ref type="bibr" target="#b17">[18]</ref> and NWGNN <ref type="bibr" target="#b40">[41]</ref> aim to design the GNN depth. Apart from these methods designed for the node classification task, NAS-GCN <ref type="bibr" target="#b14">[15]</ref> learns adaptive global pooling functions additionally, and PAS <ref type="bibr" target="#b43">[44]</ref> is proposed to design global and hierarchical pooling methods adaptively for the graph classification task. Despite the success of these methods, they are usually shallow, e.g., use two-or three-layer GNNs in general. DeepGNAS <ref type="bibr" target="#b7">[8]</ref> designs deep architectures in the block and architecture stages for the node classification task. As to the search algorithm, differentiable search algorithms are preferred in recent years <ref type="bibr" target="#b23">[24]</ref> considering the search efficiency. The discrete search space is relaxed into continuous with one relaxation function <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b44">45]</ref>, on top of which the gradient descent can be applied.</p><p>More graph neural architecture search methods can be found in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b55">56]</ref>. Compared with existing methods which use shallow GNNs or only design deep GNNs in the node classification task, LRGNN evokes the importance of deep stacking-based GNNs in the graph classification task, and the rationality has been justified in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section, we first show the feasibility of employing the stackingbased GNNs to extract the long-range dependencies, on top of which two design needs for these GNNs are provided. Then, we will introduce the proposed method LRGNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Feasibility of Stacking-based GNNs</head><p>For the graph classification task, one GNN aims to learn one representation vector for each input graph to predict the graph label, and it is related to the interactions between long distance pairs <ref type="bibr" target="#b13">[14]</ref>, e.g., counting local substructures <ref type="bibr" target="#b5">[6]</ref>. Therefore, how effectively embracing long-range dependencies is the key factor in designing neural networks. However, existing methods mentioned in Section 2.1.1 and 2.1.2 are deficient in capturing long-range dependencies. These methods update the graph structures either by generating the coarse graph or connecting to higher-order neighbors, which may result in insufficient discriminability, i.e., the discriminative graph structures may become indistinguishable anymore. For example, the pooling operations may generate the same coarse graph based on two distinguishable graphs, and the instances are provided in Appendix A. <ref type="bibr" target="#b3">4</ref>.</p><p>Considering the deficiency of these methods, we turn to the stacking-based GNNs and search for better solutions to capture long-range dependencies. In general, scaling the model depth is the common way to improve the model performance in computation vision <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b30">31]</ref>, and deeper GNNs can enlarge the receptive field which enables the extraction of longer-range dependencies. Although, the over-smoothing problem is hard to evade in deep GNNs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b46">47]</ref>, i.e., the connected nodes will have similar representations as the network becomes deeper, which will result in a performance drop on the node classification task. This problem hinders the development of deeper GNNs, and two-or three-layer shallow GNNs are widely used. However, as shown in proposition 1, we theoretically justify that the over-smoothing problem has smaller influence on the graph classification task compared with the node classification task. The proof is provided in Appendix A.1. Therefore, it is one potential solution to obtaining the long-range dependencies by stacking sufficient GNN layers. In this paper, we extract the longrange dependencies by stacking sufficient GNN layers as mentioned in Section 2.1.3. Compared with the aforementioned methods, it can preserve the graph structure information in the computational graph without modifying the graph structure.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Design Needs for Stacking-based GNNs</head><p>When using the stacking-based GNNs to capture the long-range dependencies, two aspects affect the utilization of long-distant neighbors, i.e., the GNN depth and the skip-connection schemes. The former reflects the longest distance one node can access, and the latter is related to the information mixing from different ranges of neighbors. Therefore, to effectively capture the long-range dependencies, these two design needs should be considered when designing the stacking GNNs in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.2.1</head><p>Depth needs to be sufficient. By stacking more aggregation operations, deeper GNNs can incorporate the interactions from more distant neighbors which affect the prediction results. We theoretically justify the discriminative power, i.e., how well GNNs with different depths can distinguish the non-isomorphic graphs, in Proposition 2. The proof is proved in Appendix A.2. Use the cases shown in Figure <ref type="figure" target="#fig_0">1</ref> (a) as an example, the key difference between these two graphs is the graph structures within nodes 0 and 11, and these two graphs can be distinguished by the 1-WL test in the 4-th iteration. As shown in Figure <ref type="figure" target="#fig_0">1</ref> (b), two-layer GNN cannot distinguish two graphs since the same computation graphs are obtained in two molecular graphs, and the same graph representation vector will be obtained based on this. However, as shown in Figure <ref type="figure" target="#fig_0">1</ref> (c), the visualized computation graph of node 2 in G 2 contains ?1 and ?3 simultaneously, while it cannot be achieved in G 1 . On top of this, a four-layer GNN can distinguish these two graphs. Combined with the 1-WL test, deeper GNNs are more expressive than shallower ones, while two-or three-layer GNNs are widely used in the graph classification task. Therefore, it is necessary to enlarge the model depth when capturing the long-range dependencies in the graph classification.</p><p>Proposition 2. For any two graphs G 1 and G 2 which are nonisomorphic and can be distinguished by the first-order Weisfeiler-Lehman test in ?-th iteration, one aggregation-based ?-layer GNN A ? : G ? R ? can come up the following conclusions: </p><formula xml:id="formula_0">A ? (G 1 ) ? A ? (G 2 ), ? ? ?, A ? (G 1 ) = A ? (G 2 ), ? 0 ? ? &lt; ?.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Skip-connection needs to be adaptive. Based on proposition 2, different graph pairs may have different distinguishable iteration ?</head><p>in the 1-WL test. Therefore, it is unreasonable to incorporate the distant neighbors with only one variety, i.e., only from the ?-hop away, when facing those graphs in the datasets. In stacking-based GNNs, each node can increasingly expand the receptive field as the network goes deeper, and then designing the skip-connection schemes in GNN can make up for this deficiency since the information extracted from different ranges of neighbors can be mixed based on the skip-connections, on top of which the extracted information can be enriched.</p><p>As shown in Figure <ref type="figure" target="#fig_2">2</ref>, we design six GNNs with different model depths and skip-connection schemes, and then evaluate them on three datasets that have different graph diameter distributions. We observe that: (a) deeper GNNs achieve higher performance in general, which demonstrates the importance of sufficient large layer numbers in the graph classification task; (b) different skipconnection schemes result in diverse performances in each dataset, and they also have different ranks. Therefore, it is a necessity to design the skip-connections data-specifically <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Designing Inter-layer Connections in Stacking-based GNNs</head><p>When using the stacking-based GNNs to extract the long-range dependencies, the model depth and the skip-connection schemes should be considered. These two designing needs can be unified by designing inter-layer connections, which correspond to the interlayer connections between the consecutive and inconsecutive layers, respectively. To effectively extract the long-range dependencies, we adopt NAS to design the inter-layer connections in GNNs, on top of which the GNN depth and skip-connection schemes will be obtained adaptively. 3.3.1 Architecture framework. As shown in Figure <ref type="figure" target="#fig_3">3</ref> (a), we provide a unified framework to design the inter-layer connections in GNNs, which is constructed with an ordered sequence of ? aggregation operations (? = 4 and GCN <ref type="bibr" target="#b15">[16]</ref> operation for example). Motivated by GraphGym <ref type="bibr" target="#b51">[52]</ref>, we provide one pre-processing operation 0 and one post-processing operation ? + 1, each of which is one two-layer MLP (Multilayer Perceptron) to support the integration in the following procedures. At the end of this framework, one readout operation is provided to learn the graph representations. By deciding whether to use these inter-layer connections, i.e., the dashed lines shown in the framework, we can obtain GNNs with different depths and skip-connections to extract the long-range dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.2</head><p>Designing search space based on the framework. We provide a set of candidates to construct the search space. As shown in Figure <ref type="figure" target="#fig_3">3</ref> (a), the connections among different operations (with dashed lines) are learnable, and each of them is an "on-off switch" when utilizing features, i.e., only "used" and "unused" states existed. Therefore, we provide two candidate choices to represent these two states on each learnable connection in the framework, i.e., O ? = {ON, OFF}. They can be represented as ? (H) = H and ? (H) = 0, respectively. For aggregation operations and the post-processing operation, more than one connection may be used. A straightforward manner is to add up these selected features. To further improve the model expressiveness, we provide one merge operation to incorporate these selected features. Based on the literature, we provide six candidates to merge them with the concatenation, LSTM cell, attention mechanism, summation, average and maximum. Then the merge operations are denoted as O ? = {CONCAT, LSTM, ATT, SUM, MEAN, MAX }. For readout operation, we provide seven candidates to obtain the graph representation vector and it can be denoted as O ? = {GMEAN, GMAX, GSUM, GSORT, GATT, SET2SET , MEMA}. These candidates represent: three simple global mean, max and sum functions; four readout operations derived from <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b18">[19]</ref>, respectively. As shown in Figure <ref type="figure" target="#fig_3">3</ref> (a), the operations used in each learnable connection are designed independently, and then different depth and skip-connection schemes will be obtained in GNNs. Meanwhile, designing the merge and readout operations can further improve the expressiveness of GNNs <ref type="bibr" target="#b45">[46]</ref>. This variant is denoted as LRGNN-Full in this paper.</p><p>As mentioned in Section 3.2.1, the GNN depth should be sufficiently large, which demonstrates the large number of aggregation operations in the framework. Then, the search space size is growing exponentially along with the increasing number of aggregation operations. Considering the search efficiency, we adopt the cells which are widely used in NAS <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28]</ref> and provide one cell-based framework as shown in Figure <ref type="figure" target="#fig_3">3 (b)</ref>. Each cell can be treated as one basic unit to design the stacking-based GNNs. ? cells are provided and then these ? aggregation operations are assigned to the cells equally. The operations in each cell can be learned independently, and we will obtain different cells with diverse connection schemes and merge operations in the searched GNN, which is dubbed LRGNN-Diverse. On the contrary, we can share the operations among cells, on top of which the same cell is used in the searched GNN, which is dubbed LRGNN-Repeat in this paper. ? cross cells. The differentiable method is employed in this paper which is widely used in the NAS methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b44">45]</ref>. We adopted the Gumbel Softmax to relax the search space directly. As shown in the following:</p><formula xml:id="formula_1">? ? = ?(O, ? , ?) = exp((log ? ? + G ? )/?) | O | ?=1 exp((log ? ? + G ? )/?) ,<label>(1)</label></formula><p>G ? =log(-log(U ? )) is the Gumble random variable, and U ? is a uniform random variable, ? is the temperature of softmax. As shown in Figure <ref type="figure" target="#fig_3">3</ref> (c), we use the aggregation operation 3 as an example, the output can be represented as</p><formula xml:id="formula_2">H (?,3) = | O ? | ?? ?=1 ? ? ? ? (H ? ), ? ? = ?(O ? , ? (1,?,3) ? , ?),<label>(2)</label></formula><formula xml:id="formula_3">H 3 ?? = | O ? | ?? ?=1 ? ? ? ? ({H (?,3) |0 ? ? ? 2}), ? ? = ?(O ? , ? (1,3) ? , ?),<label>(3)</label></formula><formula xml:id="formula_4">H 3 = AGG(A, H 3 ?? ). (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>H ? is the output feature matrix of operation ?, and H (?,3) represent the results collected based on the connection from operation ? to operation 3. Then, these selected features are merged and the results are denoted as H 3 ?? , on top of which one aggregation operation AGG can be applied and the output of operation 3, i.e., H 3 , will be generated. The graph representation can be generated as shown in the Alg. 1 in Appendix A.5. We optimize the parameters ? and W with the gradient descent, and the details are provided in Alg. 2 in Appendix A.5. After the training finished, we obtained the searched GNNs by preserving the operations with the largest weights.</p><p>In summary, LRGNN aims to design the inter-layer connections in deep stacking-based GNNs to capture the long-range dependencies in the graph classification task. Compared with existing methods which use shallow GNNs or only design deep GNNs in the node classification task, LRGNN evokes the importance of deep stacking-based GNNs in the graph classification task, and the rationality has been justified in Section 3.1. Besides, the methods which aim to design adaptive aggregation operations are orthogonal with LRGNN, and they can be incorporated into this paper directly. In the following, we empirically evaluate the rationality and effectiveness of LRGNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate the proposed LRGNN against a number of SOTA methods and widely used GNNs, with the goal of answering the following research questions: Q1: How does the over-smoothing problem and two design needs empirically affect the performance of stacking-based GNNs in the graph classification task? (Section 4.2)? Q2: How does the proposed LRGNN compare to other methods when extracting the long-range dependencies (Section 4.3)? Q3: How does each component affect the method performance, i.e., the merge and readout operations used in the search space, and the cell-based search space designed considering the search efficiency (Section 4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>4.1.1 Datasets. As shown in Tab. 1, we use five datasets with different graph diameters from different domains. NCI1 and NCI109 are datasets of chemical compounds <ref type="bibr" target="#b34">[35]</ref>. DD and PROTEINS datasets are both protein graphs <ref type="bibr" target="#b4">[5]</ref>. IMDB-BINARY dataset is movie-collaboration datasets <ref type="bibr" target="#b47">[48]</ref>.</p><p>4.1.2 Baselines. We provide three kinds of baselines in this paper: ? For stacking-based GNNs, we adopt three baselines in this paper which use different skip-connection schemes: GCN <ref type="bibr" target="#b15">[16]</ref>, Res-GCN <ref type="bibr" target="#b19">[20]</ref> and GCNJK <ref type="bibr" target="#b46">[47]</ref>. We vary the GNN layers in {4, 8, 12, 16}, ? We provide three pooling methods used in this task: DGCNN <ref type="bibr" target="#b53">[54]</ref> baseline with 8-layer stacked GCN operations and the designed readout function; 2-layer SAGPool <ref type="bibr" target="#b18">[19]</ref> and DiffPool <ref type="bibr" target="#b49">[50]</ref> baselines in which each layer has one GCN operation and one pooling operation.</p><p>? Considering the methods which incorporate the higher-order neighbors, we provide four baselines. Firstly, we use the 4-layer stacked GCN, and in each layer nodes aggregate messages from the neighbors exactly one and two hops away. This baseline is denoted as TwoHop(L4) in this paper <ref type="bibr" target="#b0">[1]</ref>. For GCNFA(L8) <ref type="bibr" target="#b1">[2]</ref> baseline, we use the 8-layer stacked GCN in which the 8-th layer uses the fully-adjacency matrix instead. For GCNVIR(L8) <ref type="bibr" target="#b28">[29]</ref>, we use the 8-layer stacked GNN, and in each layer, we add one virtual node.</p><p>For GraphTrans <ref type="bibr" target="#b13">[14]</ref>, we adopt the small variant in this paper which use three GCN layers and four transformer layers.</p><p>Compared with these baselines, we provide four LRGNN variants: LRGNN-Full with 8 and 12 aggregation operations, which denoted as B8C1 and B12C1, respectively; LRGNN-Repeat and LRGNN-Diverse with 12 aggregation operations and 3 cells, which denoted as Repeat B12C3 and Diverse B12C3, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Implementation details.</head><p>For all datasets, we perform 10-fold cross-validation to evaluate the model performance and an inner holdout technique with a 80%/10%/10% training/validation/test for model training and selection. In this paper, we set the training and finetuning stages to get the performance for the proposed LRGNN. In the training stage, we derived the candidate GNNs from the corresponding search space. For the searched GNNs and the baselines, we fine-tune the hyper-parameters of these methods in the finetuning stage. The details are provided in Appendix B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Rationality of LRGNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.1</head><p>The importance of sufficient model depth. In this paper, we use the averaged node pair distance, which is calculated based on</p><formula xml:id="formula_6">? (? ? , ? ? ) = ? ? ? ?? ? ? 1 - ? ? ?? ? ? 1 ? 1 ,</formula><p>to evaluate the over-smoothing problem. As shown in Figure <ref type="figure" target="#fig_5">4</ref>, the lower distance values are observed on two datasets as the network goes deeper, which demonstrates the smoother features between node pairs are observed and the over-smoothing problem appeared in both the node classification and graph classification tasks. However, the upward trend of model performance is apparent in the NCI1 dataset, while the opposite trend is observed in the Cora dataset. It indicates the smaller influence of the over-smoothing problem on the graph classification task compared with the node classification task. Therefore, stacking sufficient layers to obtain the long-range dependencies is a feasible way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">The necessity of adaptive skip-connections. As mentioned in Section 3.2.2, designing adaptive skip-connections can enhance</head><p>Table <ref type="table">2</ref>: Performance comparisons. We first show the average diameters of the datasets. Then, we report the mean test accuracy and the standard deviations based on the 10-fold cross-validation data. For the proposed method, BICJ represent these architectures contains ? aggregation operations and ? cells.</p><p>[??] represents the model depth is ?. "OOM" means out of memory. The best result in each group is underlined, and the best result in this dataset is highlighted in gray. The average rank on all datasets is provided and the Top-3 methods are highlighted. For GCNFA and GraphTrans, the averaged rank is calculated based the other four datasets.   In summary, sufficient GNN depth and adaptive skip-connection schemes are empirically important for stacking-based GNNs in extracting the long-range dependencies. In the following, we compare the proposed LRGNN with the SOTA methods to show its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Effectiveness of LRGNN</head><p>As shown in Table <ref type="table">2</ref>, the variants B8C1, B12C1 and LRGNN-Repeat B12C3 can outperform all baselines, which demonstrates the effectiveness of the LRGNN method by designing the inter-layer connections in GNNs to utilize the long-range dependencies adaptively. Besides, for these methods which achieve the best results on these datasets (the results highlighted in gray in Tab. 2), we observe that the model depth in these methods are very close to the graph radius (diameter/2). This observation highlights the significance of sufficient model depth in GNNs. LRGNN-Repeat and LRGNN-Diverse variants constrain the search space and group the aggregation operations into different cells. Compared with LRGNN-Full, these two variants have fewer parameters based on the same aggregation operations (see Tab. 3), which enables the explorations on much deeper GNNs. However, they have limited search space, which may filter out the expressive architectures and lead to a performance decrease as shown in Tab. 2.</p><p>We visualize the searched inter-layer connections of LRGNN-Full B8C1 in Figure <ref type="figure" target="#fig_7">6</ref>, and more results can be found in Appendix B.2. It is obvious that different inter-layer connection schemes are obtained on different datasets, and each operation has its own preference for turning on the connections from previous operations. The connections between the consecutive layers are emphasized in the NCI109 dataset, while the connections between the inconsecutive layers are widely used in the NCI1 dataset. We highlight one of the longest paths in the searched GNNs, and the searched architecture on the NCI109 datasets is deeper than that on the NCI1 dataset. Combine with the SOTA performance in Tab. 2, the effectiveness of designing inter-layer connections is obvious.</p><p>Compared with the stacking-based methods, we observe that three baselines could achieve higher performance with more GNN layers. Besides, by utilizing different skip-connection schemes, Res-GCN and GCNJK achieve higher performance than GCN baseline which has no skip-connections. The performance benefits a lot from the inter-layer connections, and the proposed LRGNN achieves better performance by designing the inter-layer connections adaptively. As to the other two kinds of baselines, GraphTrans has the strongest ability in modeling the long-range information with the help of the Transformer module, which has been proved in the related methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b48">49]</ref>. Besides, the global pooling method DGCNN and two hierarchical pooling methods, i.e., SAGPool and DiffPool, have different performances on these datasets, and no absolute winner from these two kinds of pooling methods on all datasets, which is consistent with existing methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b43">44]</ref>. The grouping-based methods, e.g., DiffPool, achieves higher ranks than the selectionbased method SAGPool in general. Compared with these methods, LRGNN has better performance due to the strong ability of the stacking-based GNNs in preserving the graph structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To evaluate the contributions of different components, in this section, we conduct ablation studies on the cell-based search space, and two operations added to improve the model expressiveness. Considering the limited space, we show the ablation study experiments on aggregation operation and the search algorithm in Appendix B.3.</p><p>Table <ref type="table">3</ref>: Evaluations on the cell-based architecture. We show the comparisons of the performance, the parameter numbers(M) in the supernet and the search cost (GPU second) in the training stage.  As to the performance, these variants have a smaller search space, and they may filter out the expressive architectures. Therefore, they may be outperformed by LRGNN-Full in some cases. As a conclusion, LRGNN-Full can achieve higher-performance while the other two variants have advantages in the training efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.4.2</head><p>The evaluation of the merge and readout operations. In the search space, we provide the merge operation and readout operation to improve the model expressiveness as mentioned in Section 3.3. Then, we conduct ablation studies on these two operations to show their effectiveness in improving the model performance.</p><p>In this paper, we provide a set of merge operations that are expected to improve the model expressiveness. As shown in Figure <ref type="figure" target="#fig_9">7</ref> (a), compared with LRGNN which designs the merge operations adaptively, using the pre-defined merge operations may not achieve higher performance in general. The searched results are visualized in Appendix B.2, from which we can observe the different merge operations are utilized. Combine with better performance, the effectiveness to design the merge operations adaptively is obvious. The readout operation is responsible for generating the graph representation vector for the given graph. In this paper, we provide seven candidates and learn to design this operation adaptively. As shown in Figure <ref type="figure" target="#fig_9">7</ref> (b), LRGNN achieves the SOTA performance in three datasets, and all of them utilize the GSUM readout operation in the searched architectures. Besides, the baseline which uses the predefined readout operation GSUM achieves comparable performance as well. Nevertheless, using the other two readout operations lead to a performance drop, and then it is important to design the readout operation adaptively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we provide a novel method LRGNN to capture the long-range dependencies with stacking GNNs in the graph classification task. We justify that the over-smoothing problem has smaller influence on the graph classification task, and then employ the stacking-based GNNs to extract the long-range dependencies. Two design needs, i.e., sufficient model depth and adaptive skip-connections, are provided when designing the stacking-based GNNs. To meet these two design needs, we unify them into interlayer connections, and then design these connections with the help of NAS. Extensive experiments demonstrate the rationality and effectiveness of the proposed LRGNN. For future work, we will evaluate the importance of different hops' neighbors in different graph pairs, and then extract this information for each graph independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A METHOD A.1 The Proof of Proposition 1</head><p>Proof. It has been proved in Theorem 1 <ref type="bibr" target="#b20">[21]</ref> that (? -? ??? ) ? W will converge to the linear combination of eigenvectors of eigenvalue 1. The satisfied graphs will converged to different points, which leads to differnt node features in these two graphs. Then, based on the Lemma 5 in <ref type="bibr" target="#b45">[46]</ref>, unique graph representation vector will be generated for each graph, and then two graphs be distinguished. ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 The Proof of Proposition 2</head><p>Proof. It has been proved in Theorem 3 <ref type="bibr" target="#b45">[46]</ref> that GIN is as powerful as the 1-WL test. For two graphs which can be distinguished ?</p><p>We use the graphs in Figure <ref type="figure" target="#fig_0">1</ref>(a) as an example. Two graphs have the same number of nodes, and all nodes have the same feature, e.g., all-one vector.</p><p>? 0-layer GNN, which is constructed by MLP and a readout operation, cannot distinguish these two graphs. These two graphs have the same feature set, and MLPs with any layers will generate the same feature set for them as well. Based on these two graphs, any simple readout function designed on the node features will generate the same graph representation vectors. For example, take the mean or summation of all node embeddings as the graph-level representation vector. Set2Set operation <ref type="bibr" target="#b33">[34]</ref> and SortPool <ref type="bibr" target="#b53">[54]</ref> have the same results. Therefore, the 0-layer GNN cannot distinguish these two graphs.</p><p>? Shallow GNN cannot distinguish these two graphs. As shown in Figure <ref type="figure" target="#fig_0">1</ref> (b), we visualize the computation graph, which is denoted as ?1-?4 in the figure, and nodes in each row use the same computation graph. In 2-th iteration, 1-WL test will generate the same feature for the nodes in the same row. Therefore, 2-layer GIN, which is constructed by stacking two GIN aggregation operations and one readout function mentioned before, cannot distinguish two graphs as well. The 3-layer GNN has the same situation.</p><p>? 4-layer GNN can distinguish these two graphs. As shown in Figure <ref type="figure" target="#fig_0">1</ref> (c), we visualize the computation graph for node 2 in G 2 . It contains the ?1 and ?3 graphs, while the nodes in G 1 cannot achieve this. Therefore, two graphs have different feature sets in 4-iteration in the 1-WL test. Therefore, 4-layer GIN can distinguish two graphs.</p><p>In summary, capturing long-range dependencies is useful for the graph classification task, which is correlated with the 1-WL test.</p><p>A.3 Experimental Details for Figure <ref type="figure" target="#fig_2">2</ref> Datasets. Three datasets are constructed based on the NCI1 dataset which will be introduced in the following. For the first dataset, we sample 10 graphs with diameter 5 and 50 graphs with diameter 14. The other two datasets are constructed in the same way. The graph label is determined by the graph diameter. Therefore, these three datasets only have two labels. The diameter distribution for these datasets is provided in Figure <ref type="figure" target="#fig_2">2</ref>. Baselines. We provide six GNNs with different inter-layer connections as shown in Figure <ref type="figure" target="#fig_2">2</ref>. Based on GCN aggregation operation, ?1-?3 have different connection schemes between the consecutive layers and lead to different layer numbers, i.e., 2, 5, and 7, respectively. GNN ?4-?6 use the different connections among inconsecutive layers. Implementation details. We evaluate each GNN on three datasets. We fix the hidden dimension to 128, and use a batch size of 64, a fixed learning rate 0.01, and Adam optimizer. We visualize the averaged test accuracy in Figure <ref type="figure" target="#fig_2">2</ref> based on 10-fold cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Limitations of Existing Methods</head><p>Using the graph pairs shown in Figure <ref type="figure" target="#fig_0">1</ref> as an example, we visualize the pooling results in Figure <ref type="figure">8</ref>, in which two different graphs generate the same coarse graph after pooling operations. The selectionbased pooling operations preserve the same node sets in two graphs, and the formulated coarse graphs are the same as the other. As to the grouping-based operations, the subgraphs in each colored block have similar features and are densely connected, and they are grouped into the corresponding new nodes in the coarse graph. The same coarse graphs are obtained as well.   of a stratification technique with the same seed. We finetune the human-designed architectures and the searched architectures with the help of Hyperopt 3 . In the finetuning stage, each GNN has 20 hyper steps. For LRGNN, a set of hyperparameters will be sampled from Tab. 4 in each step, and the baselines will be sampled from Tab. 5. Then we select the final hyper-parameters on the validation set, from which we can obtain the final performance of this GNN. For stacking-based baselines GCN and ResGCN, we vary the GNN layers in {4, 8, 12, 16}, and then report the best methods in Tab. 2. The detailed results are shown in Tab. 6. Especially, for the Graph-Trans baseline, we adopt the provided code 4 and split the data as mentioned before.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Searched GNNs</head><p>The the searched GNNs are provided in Fig. <ref type="figure" target="#fig_17">11</ref>-15. In this paper, we use the GCN operation to construct LRGNN. In this section, we employ different aggregation operations and evaluate their influence. As shown in Figure <ref type="figure" target="#fig_15">9</ref>, compared with using fixed GCN operations in our paper, using the GIN operation will achieve higher performance, which is consistent with <ref type="bibr" target="#b45">[46]</ref>. Besides, we provide one variant AdaAGG which provide the aggregation design dimension additionally, i.e., {SAGE, GCN, GIN, GAT}. It achieves higher performance than the proposed method LRGNN which uses fixed GCN operation. It indicates that the performance can be further improved in the future by adding more effective aggregation operations in the search space. To make a fair comparison with LRGNN which adopts the Gumbel-Softmax function to relax the search space, we provide one baseline which uses the Softmax function as the relaxation function. We adopt the Random and Bayesian search algorithm in this section. These two methods have 100 search epochs in which one architecture is sampled from the search space and trained from scratch. We select the Top-1 architecture and tune this architecture, from which we obtained the final performance. As shown in Figure <ref type="figure" target="#fig_16">10</ref>, the Random and Bayesian baselines cannot achieve higher performance than the differentiable methods on the designed search space. Besides, compared with these two methods, the relaxation function has smaller influences and could achieve comparable performance. Therefore, the presence of these differentiable search algorithm is essential for good performance, and the relaxation function matter less. This function can be further updated and could achieve considerable performance in expectation.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Proposition 1 .</head><label>1</label><figDesc>Let G 1 and G 2 are two different graphs that satisfy: (a) have no bipartite components, and (b) at least one of the eigenvectors or eigenvalues are different. Then, for any learnable</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a): The illustrations of two molecule graphs (? 12 ? 26 ) where the green node is carbon and the orange node is hydrogen. (b): We visualize the computation graph used in the 2-layer GNNs for each node in the table. Two graphs have the same subtree distribution and lead to the same graph representation. (c): Especially, we visualize the computation graph for node 2 in G 2 used in the 4-layer GNN. It contains ?1 and ?3 simultaneously, which is unique in G 2 . parameter W, (? -? ??? 1 ) ? W ? (? -? ??? 2 ) ? W, where ???? 1 and</figDesc><graphic url="image-1.png" coords="4,58.68,83.68,228.22,203.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The performance comparisons with GNNs in different model depth and skip-connection schemes. Each dataset has a different number of graphs in diameter of 5 and 14. Graph label is determined by its diameter. The details are provided in Appendix A.3.</figDesc><graphic url="image-2.png" coords="4,334.85,236.14,204.20,86.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) GNN framework used to design the inter-layer connections. (b) Cell-based GNN architecture. Each cell contains ?/? (? = 8 and ? = 2 for example) aggregation operations and a post-processing operation. (c) One architecture design example. For aggregation operation 3, the connection from operation 0 is unused, and two inputs from operation 1 and 2 are merged with operation SUM, on top of which the aggregation operation 3 can be operated.</figDesc><graphic url="image-3.png" coords="5,58.68,120.65,228.24,144.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3. 3 . 3</head><label>33</label><figDesc>Architecture search from the search space. The architecture design can be formulated as the bi-level optimization problem as shown in the following: min ? ?A L val (W * (? ), ? ), s.t. W * (? ) = arg min W L train (W, ? ), A represents the search space, and ? represents the neural architecture in A. W represents the parameters of a model from the search space, and W * (? ) represents the corresponding operation parameter after training. L train and L val represent the training and validation loss, respectively. In this paper, ? = {? ? , ? ? , ? ? } has three components need to be designed. As shown in Figure 3 (c), ? (?,?,?) ? ? R | O ? | represent the learnable connections between the operation ? and ? in ?-th cell. ? (?,?) ? ? R | O ? | represent the merge operation used in the aggregation operation ? in ?-th cell. The LRGNN-Full variant adopts ? = 1 merely, while LRGNN-Repeat share the parameters ? (?,?) ? and ? (?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The test accuracy and the averaged distance comparisons on the node classification dataset Cora (Left) and graph classification dataset NCI1 (Right).</figDesc><graphic url="image-4.png" coords="7,63.57,328.57,108.11,54.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The influence of different skip-connection schemes on NCI1 (Left) and NCI109 (Right) datasets.</figDesc><graphic url="image-6.png" coords="7,75.58,548.87,96.10,76.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The searched inter-layer connections of LRGNN-Full B8C1 on NCI1 (Top) and NCI109 (Bottom) datasets. We highlight one of the longest path in the GNNs, on top of which the model depth can be obtained.</figDesc><graphic url="image-8.png" coords="8,106.73,83.69,132.11,83.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>4. 4 . 1</head><label>41</label><figDesc>The evaluation on cell-based search space. Considering the search efficiency, we provide one cell-based framework which can constrain the search space by designing the connections in cells. To evaluate the efficiency and effectiveness of the cell-based architectures, we varify the cell numbers and the results are shown in Tab.<ref type="bibr" target="#b2">3</ref> It is obvious that LRGNN-Full cannot work on DD dataset given 16 aggregation operations in one cell, while it can work when we assign these 16 aggregation operations into 4 cells. With the same GPU, the cell-based LRGNN-Repeat and LRGNN-Diverse raise the ceiling of the GNN depth compared with the LRGNN-Full. Besides, the Repeat and Diverse variants have fewer parameters and search costs, which demonstrates the search efficiency when using cells.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Evaluations on the merge and readout operation. These methods are constructed based on LRGNN-Full B12C1 variant and different merge or readout operations.</figDesc><graphic url="image-9.png" coords="8,328.85,289.54,216.18,54.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>by 1 -</head><label>1</label><figDesc>WL test at 4-th iteration, we can obtain that A ?? ? ? (G 1 ) ? A ?? ? ? (G 2 ) and A ?? ? ? (G 1 ) = A ?? ? ? (G 2 ), ? ? ?. Furthermore, the 1-WL test can distinguish two graphs after ?-th iterations, and then A ?? ? ? (G 1 ) ? A ?? ? ? (G 2 ) also holds when ? &gt; ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :Algorithm 1 1 :</head><label>811</label><figDesc>Figure 8: The pooling results of G1 (left) and G2 (right). The coarse graphs in the top row is the results of the selectionbased operations, and the bottom row is the results of grouping-based operations.</figDesc><graphic url="image-10.png" coords="10,352.87,306.97,168.18,94.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>4: end for 5 :</head><label>5</label><figDesc>Update the node feature in the post-processing stage H ? = ????? (H ? ).6: ? G = |?? | ?=1 ? ? ? ? (A, H ? ) where ? ? = ? ( O? , ?? , ?) . // Readout 7: return ? G B EXPERIMENTS B.1 Implementation DetailsAll models are implemented with Pytorch<ref type="bibr" target="#b26">[27]</ref> on a GPU RTX 3090 (Memory: 24GB, Cuda version: 11.2). We adopt the same data preprocessing manner provided by PyG 2 and split data by means Algorithm 2 Designing the inter-layer connections for the stacking-based GNNs. Require: Training dataset D ????? , validation dataset D ??? , the search epoch ? . Ensure: The searched GNN. Random initialize the architecture parameters ? and operation parameter W.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>2 : 12 :</head><label>212</label><figDesc>while ? = 1, . . . ,? do for each minibatch G ? ? D ????? do 4: Calculate the graph representations ? G ? as shown in Alg. 1each minibatch G ? ? D ??? do 8: Calculate the graph representations ? G ? as shown in Alg. 1. Update ? . Preserve the candidates with the largest weight in each learnable connection, merge and readout operations. return The searched GNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>3 https://github.com/hyperopt/hyperopt 4 https://github.com/ucbrise/graphtrans B.3 Ablation Study B.3.1 The influence of different aggregation operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Evaluations on the basic aggregation operations. These methods are contructed with LRGNN-Full B12C1 variant and different aggregations.</figDesc><graphic url="image-11.png" coords="11,364.88,232.01,144.14,59.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Evaluations on the search algorithm. We use LRGNN-Full B12C1 variant as an example. Softmax represent the differentiable algorithm with Softmax relaxation function, and our method employ the Gumbel-Softmax function instead.</figDesc><graphic url="image-12.png" coords="11,364.88,541.92,144.14,59.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: The searched architectures on NCI1 dataset. Four variants from the left to right are the Full-B8C1, Full-B12C1, Repeat-B12C3, and Diverse-B12C3, respectively. In each row, the operations on each connections are showed. Besides, the merge operation and the aggregation operation are shown as well. In the post-processing operation, which denoted as "C" in this figure, use the MLP operation instead.</figDesc><graphic url="image-15.png" coords="12,77.90,254.92,226.98,170.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: The searched architectures on NCI109 dataset.</figDesc><graphic url="image-19.png" coords="13,77.90,254.92,226.98,170.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: The searched architectures on DD dataset.</figDesc><graphic url="image-23.png" coords="14,77.90,254.92,226.98,170.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: The searched architectures on IMDB-BINARY dataset.</figDesc><graphic url="image-31.png" coords="16,77.90,254.92,226.98,170.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>? We conduct extensive experiments on five datasets from different domains, and the proposed LRGNN achieves the SOTA performance by designing GNNs with sufficient model depth and adaptive skip-connection schemes. Notations. We represent a graph as G = (V, E),where V and E represent the node and edge sets. A ? R |V |?|V | is the adjacency matrix of this graph where |V | is the node number. N (?) is the neighbors of node ?. H ? R |V |?? is the node feature matrix and ? is the feature dimension, and h ? is the feature representation of node ?.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets from three domains.</figDesc><table><row><cell></cell><cell># of</cell><cell># of</cell><cell># of</cell><cell>Avg.# of</cell><cell>Avg.# of</cell><cell></cell></row><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Domain</cell></row><row><cell></cell><cell>Graphs</cell><cell>Feature</cell><cell>Classes</cell><cell>Nodes</cell><cell>Edges</cell><cell></cell></row><row><cell>NCI1</cell><cell>4,110</cell><cell>89</cell><cell>2</cell><cell>29.87</cell><cell>32.3</cell><cell>Chemistry</cell></row><row><cell>NCI109</cell><cell>4,127</cell><cell>38</cell><cell>2</cell><cell>29.69</cell><cell cols="2">32.13 Chemistry</cell></row><row><cell>DD</cell><cell>1,178</cell><cell>89</cell><cell>2</cell><cell>384.3</cell><cell>715.7</cell><cell>Bioinfo</cell></row><row><cell>PROTEINS</cell><cell>1,113</cell><cell>3</cell><cell>2</cell><cell>39.1</cell><cell>72.8</cell><cell>Bioinfo</cell></row><row><cell cols="2">IMDB-BINARY 1,000</cell><cell>0</cell><cell>2</cell><cell>19.8</cell><cell>96.5</cell><cell>Social</cell></row><row><cell cols="7">and then report the best methods in Tab. 2 (More results can be</cell></row><row><cell cols="3">found in Appendix B.1).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameter space in the finetuning stage for the proposed method.</figDesc><table><row><cell>Dimension</cell><cell>Operation</cell></row><row><cell>Embedding size</cell><cell>8, 16, 32, 64, 128, 256</cell></row><row><cell>Dropout rate</cell><cell>0, 0.1, 0.2, ? ? ? , 0.9</cell></row><row><cell>Learning rate</cell><cell>[0.001, 0.025]</cell></row><row><cell>Optimizer</cell><cell>Adam, AdaGrad</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameter space for human-designed baselines.</figDesc><table><row><cell>Dimension</cell><cell>Operation</cell></row><row><cell>Global pooling function</cell><cell>GMEAN, GSUM</cell></row><row><cell>Embedding size</cell><cell>8, 16, 32, 64, 128, 256, 512</cell></row><row><cell>Dropout rate</cell><cell>0, 0.1, 0.2,...,0.9</cell></row><row><cell>Learning rate</cell><cell>[0.001, 0.025]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Performance comparisons on the stacking-based baselines GCN, ResGCN and GCNJK with different layers.</figDesc><table><row><cell>Method</cell><cell>NCI1</cell><cell>NCI109</cell><cell>DD</cell><cell>PROTEINS</cell><cell>IMDBB</cell></row><row><cell>GCN(L4)</cell><cell>76.96(3.69)</cell><cell>75.70(4.03)</cell><cell>73.59(4.17)</cell><cell>74.84(3.07)</cell><cell>73.80(5.13)</cell></row><row><cell>GCN(L8)</cell><cell>77.32(3.17)</cell><cell>76.37(2.62)</cell><cell>74.01(2.85)</cell><cell>75.11(4.51)</cell><cell>71.60(4.30)</cell></row><row><cell>GCN(L12)</cell><cell>78.88(2.76)</cell><cell>76.13(2.15)</cell><cell>74.53(4.49)</cell><cell>74.30(3.79)</cell><cell>73.50(3.17)</cell></row><row><cell>GCN(L16)</cell><cell>78.98(3.24)</cell><cell>76.64(1.04)</cell><cell>75.63(2.95)</cell><cell>72.42(4.83)</cell><cell>71.50(4.94)</cell></row><row><cell>ResGCN(L4)</cell><cell>75.13(2.87)</cell><cell>74.10(5.183)</cell><cell>73.85(3.70)</cell><cell>74.40(5.22)</cell><cell>73.50(4.83)</cell></row><row><cell>ResGCN(L8)</cell><cell>76.57(2.306)</cell><cell>75.48(3.533)</cell><cell>76.65(2.73)</cell><cell>75.11(3.22)</cell><cell>73.20(6.36)</cell></row><row><cell>ResGCN(L12)</cell><cell>77.88(2.214)</cell><cell>75.58(2.027)</cell><cell>75.38(3.03)</cell><cell>73.40(5.62)</cell><cell>73.70(5.70)</cell></row><row><cell>ResGCN(L16)</cell><cell>76.98(2.907)</cell><cell>76.71(1.836)</cell><cell>75.46(3.29)</cell><cell>73.23(3.89)</cell><cell>73.19(3.61)</cell></row><row><cell>GCNJK(L4)</cell><cell>77.54(3.47)</cell><cell>74.99(3.54)</cell><cell>72.40(4.51)</cell><cell>73.21(2.71)</cell><cell>73.20(6.14)</cell></row><row><cell>GCNJK(L8)</cell><cell>77.13(3.29)</cell><cell>75.34(3.14)</cell><cell>73.16(5.12)</cell><cell>75.24(4.15)</cell><cell>70.91(5.50)</cell></row><row><cell>GCNJK(L12)</cell><cell>79.24(2.11)</cell><cell>75.91(3.61)</cell><cell>71.90(4.87)</cell><cell>71.25(2.67)</cell><cell>72.89((5.79)</cell></row><row><cell>GCNJK(L16)</cell><cell>76.27(3.32)</cell><cell>75.65(2.84)</cell><cell>69.86(4.40)</cell><cell>71.16(4.92)</cell><cell>74.20(3.76)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>?: Both authors contributed equally to this paper. * : Corresponding author. The implementation of LRGNN is available at: https://github.com/LARS-research/LRGNN.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/pyg-team/pytorch_geometric</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>Q. Yao was in part sponsored by <rs type="funder">NSFC</rs> (No. <rs type="grantNumber">92270106</rs>) and <rs type="funder">CCF-Tencent Open Research Fund</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_qydZCVd">
					<idno type="grant-number">92270106</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral clustering with graph neural networks for graph pooling</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cesare</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="874" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology (JMB)</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ladislav</forename><surname>Ramp??ek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Parviz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.08164</idno>
		<title level="m">Long Range Graph Benchmark</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A fair comparison of graph neural networks for graph classification</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Guosheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunnan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10047</idno>
		<title level="m">Search For Deep Graph Neural Networks</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<title level="m">Graph u-nets. ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graphnas: Graph neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Representing Long-Range Context for Graph Neural Networks with Global Attention</title>
		<author>
			<persName><forename type="first">Paras</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanghao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph neural network architecture search for molecular property prediction</title>
		<author>
			<persName><forename type="first">Shengli</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasanna</forename><surname>Balaprakash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1346" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Policy-GNN: Aggregation Optimization for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Kwei-Herng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="461" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-Attention Graph Pooling</title>
		<author>
			<persName><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">AutoGraph: Automated Graph Neural Network</title>
		<author>
			<persName><forename type="first">Yaoman</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICONIP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="189" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Gated Graph Sequence Neural Networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">DARTS: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving Breadth-Wise Backpropagation in Graph Neural Networks Helps Learning Long-Range Dependencies</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Lukovnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7180" to="7191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Rethinking pooling in graph neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Diego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><forename type="middle">H</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><surname>Kaski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In NeurIPS. 4800-4810</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In NeurIPS. 8026-8037</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient Neural Architecture Search via Parameter Sharing</title>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4092" to="4101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Graph classification via deep learning with virtual nodes</title>
		<author>
			<persName><forename type="first">Trang</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoa</forename><surname>Dam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04357</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Graph attention networks. ICLR</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Order Matters: Sequence to sequence for sets</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Comparison of Descriptor Spaces for Chemical Compound Retrieval and Classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="678" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.01288</idno>
		<title level="m">Automated Graph Machine Learning: Approaches, Libraries and Directions</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Graph Property Prediction on Open Graph Benchmark: A Winning Solution by Graph Neural Architecture Search</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lanning</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.06027</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Curgraph: Curriculum learning for graph classification</title>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The WebConf</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1238" to="1248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mixup for node and graph classification</title>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The WebConf</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3663" to="3674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Autogel: An automated graph neural network with explicit link information</title>
		<author>
			<persName><forename type="first">Zhili</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimin</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="24509" to="24522" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph Neural Networks with Node-wise Architecture</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weirui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1949" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Profiling the Design Space for Graph Neural Networks based Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Zhenyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1109" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Designing the Topology of Graph Neural Networks: A Novel Feature Fusion Perspective</title>
		<author>
			<persName><forename type="first">Lanning</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The WebConf</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1381" to="1391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Quanming Yao, and Zhiqiang He. 2021. Pooling architecture search for graph classification</title>
		<author>
			<persName><forename type="first">Lanning</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<biblScope unit="page" from="2091" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SNAS: stochastic neural architecture search</title>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks? ICLR</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>ICML. 5453-5462</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Do transformers really perform badly for graph representation?</title>
		<author>
			<persName><forename type="first">Chengxuan</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="28877" to="28888" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Model-agnostic augmentation for accurate graph classification</title>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sooyeon</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The WebConf</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1281" to="1291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Design Space for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">StructPool: Structured graph pooling via conditional random fields</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Every document owns its structure: Inductive text classification via graph neural networks</title>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueli</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongzhen</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="334" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00742</idno>
		<title level="m">Automated Machine Learning on Graphs: A Survey</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Simplifying Architecture Search for Graph Neural Network</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lanning</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Search to aggregate neighborhood for graph neural network</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In ICDE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
