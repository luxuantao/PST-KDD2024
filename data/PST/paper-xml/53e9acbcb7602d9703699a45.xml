<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Match and Cluster Large High-Dimensional Data Sets For Data Integration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
							<email>william@wcohen.com</email>
							<affiliation key="aff0">
								<orgName type="institution">WhizBang</orgName>
								<address>
									<addrLine>Labs 4616 Henry St. Pittsburgh</addrLine>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jacob</forename><surname>Richman</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">WhizBang</orgName>
								<address>
									<addrLine>Labs 4616 Henry St. Pittsburgh</addrLine>
									<postCode>15213</postCode>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Match and Cluster Large High-Dimensional Data Sets For Data Integration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">BAE7E0CB0B214A565EC8096FAFDC2E8E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Learning</term>
					<term>clustering</term>
					<term>text mining</term>
					<term>large datasets</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Part of the process of data integration is determining which sets of identifiers refer to the same real-world entities. In integrating databases found on the Web or obtained by using information extraction methods, it is often possible to solve this problem by exploiting similarities in the textual names used for objects in different databases. In this paper we describe techniques for clustering and matching identifier names that are both scalable and adaptive, in the sense that they can be trained to obtain better performance in a particular domain. An experimental evaluation on a number of sample datasets shows that the adaptive method sometimes performs much better than either of two non-adaptive baseline systems, and is nearly always competitive with the best baseline system.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Data integration is the problem of combining information from multiple heterogeneous databases. One step of data integration is relating the primitive objects that appear in the different databases-specifically, determining which sets of identifiers refer to the same real-world entities. A number of recent research papers have addressed this problem by exploiting similarities in the textual names used for objects in different databases. (For example one might suspect that two objects from different databases named "USAMA FAYYAD" and "Usama M. Fayyad" " respectively might refer to the same person.) Integration techniques based on textual similarity are especially useful for databases found on the Web <ref type="bibr" target="#b1">[1]</ref> or obtained by extracting information from text <ref type="bibr">[6,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b11">11]</ref>, where descriptive names generally exist but global object identifiers are rare.</p><p>Previous publications in using textual similarity for data integration have considered a number of related tasks. Although the terminology is not completely standardized, in this paper we define entity-name matching as the task of taking two lists of entity names from two different sources and determining which pairs of names are co-referent (i.e., refer to the same real-world entity). We define entity-name clustering as the task of taking a single list of entity names and assigning entity names to clusters such that all names in a cluster are co-referent. Matching is important in attempting to join information across of pair of relations from different databases, and clustering is important in removing duplicates from a relation that has been drawn from the union of many different information sources. Previous work in this area includes work in distance functions for matching <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b8">8]</ref> and scalable matching <ref type="bibr" target="#b2">[2]</ref> and clustering <ref type="bibr" target="#b13">[13]</ref> algorithms. Work in record linkage <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b7">7]</ref> is similar but does not rely as heavily on textual similarities.</p><p>In this paper we synthesize many of these ideas. We present techniques for entity-name matching and clustering that are scalable and adaptive, in the sense that accuracy can be improved by training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">LEARNING TO MATCH AND CLUSTER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Adaptive systems</head><p>We will begin defining the problems of adaptive matching and clustering by describing a very general notion of an adaptive system. Assume a source of training examples.</p><p>Each training example is a pair (x, y * ), where x is a problem instance and y * is a desired solution to x. We will also assume a loss function, Loss(y, y * ), measuring the quality of a proposed solution y relative to a desired solution y * . The goal of an adaptive system L is to take a set of training examples (x1, y * 1 ), . . . , (xm, y * m ) and learn to propose "good" solutions to novel problems xj. In other words, the input to L is the set {(x i , y * i )} m i=1 and the output is a function f such that the loss Loss(f (x j ), y * j ) is small, where y * j is the desired solution for x j . One simple, well-explored example of an adaptive system is classification learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adaptive matching</head><p>Consider the task of learning to match names from some domain A with names from a second domain B. For exam-ple, we might wish to learn to match a researcher's name and address with a university name if and only if the researcher is affiliated with that university. To formalize this, we let each problem instance x be a pair, x = (A, B), where A and B are sets of strings. For instance, A might be names and addresses of researchers registered for KDD-02, and B might be names of universities in the United States. A solution y is a set of pairs y = {(a1, b1), . . . , (a k , b k )}, specifically a subset of A × B that indicates which pairs are to be matched. A natural loss function Loss(y, y * ) might be the size of the symmetric difference of y and y</p><formula xml:id="formula_0">* : i.e. if y = {(a i , b i )} k i=1 and y * = {(a * j , b * j )} k * j=1 then Loss(y, y * ) ≡ |{(a i , b i ) ∈ y : (a i , b i ) ∈ y * }| + |{(a * i , b * i ) ∈ y * : (a * i , b * i ) ∈ y}|</formula><p>Other related measures are recall, precision, and F-measureall of which are based on the symmetric difference of two sets.</p><p>Many matching problems are more constrained than this example. For instance, if the a'a and b's are entity names, and each b ∈ B refers to a distinct entity, then it makes little sense for a proposed solution y to contain both (a, b) and (a, b ). We define a constrained adaptive matching problem to be one in which the set of pairs in every desired pairing y * is a one-to-one function.</p><p>Constrained matching problems are common-in fact, both of the matching problems considered in Section 4 are constrained. However, we consider here the more general case, which is useful (for instance) in matching datasets that may duplicates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Adaptive clustering</head><p>The second problem we consider is adaptive clustering. In this case, each problem instance x is set of strings D = d1, . . . , dm. A solution y * is an assignment of the strings di to clusters, encoded as a function z from D to the integers between 1 and k (where k is the number of clusters).</p><p>For example, consider clustering descriptions consisting of a researcher's name, together with some additional piece of identifying information, such as his or her affiliation in July, 2002. A problem instance x would be a set of strings (like "William W. Cohen, Whizbang Labs", "W. Cohen, WhizBang Labs -Research", "Jude Shavlik, University of Wisconsin", etc) and a solution y * would be a function z such that z(d1) = z(d2) iff d1 and d2 refer to the same person. Adaptive clustering is learning to cluster better given a sequence of training data in the form of (x, z) pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SCALABLE ADAPTIVE METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Clustering</head><p>The definitions above are extensions of the model for adaptive ranking systems described by Cohen, Singer and Schapire <ref type="bibr" target="#b5">[5]</ref>. To oversimply slightly, Cohen, Singer and Schapire considered adaptive systems in which each problem instance x was an unordered set of objects x = {d 1 , . . . , d m }, and each desired solution y * was a total ordering over the objects in x. The problem of learning to order instances was addressed by learning a preference function, p(d, d )-conceptually, a function p : X × X → {0, 1} indicating if d should be ranked before d in the desired ordering y * .</p><p>Adaptive matching and clustering can be implemented in ii. Let label (d, d</p><formula xml:id="formula_1">) ≡ + if zi(d) = zi(d ) -otherwise iii. Add the labeled example (d, d ) to S.</formula><p>2. Train a classification learner on S. The result will be a hypothesis h that labels pairs (d, d ) as positive or negative.</p><p>To cluster a new set D = {d 1 , . . . , d n }:</p><p>1. Build a graph G with vertex set D, where an edge exists between di and dj iff h(d1, dj) = +.</p><p>2. Make each connected component of G be a cluster. and d should be placed in the same cluster. Figure <ref type="figure" target="#fig_1">1</ref> gives a simple algorithm for clustering using a pairing function.</p><p>The algorithm of Figure <ref type="figure" target="#fig_1">1</ref> has two problems: a small number of errors in the learned pairing function h may lead to large mistakes in the clusters created; and the algorithm is inefficient, since it requires generation of all pairs.</p><p>To address these problems, we modify Figure <ref type="figure" target="#fig_1">1</ref> in three ways. First, in training, we will enumerate only a limited number of "candidate" pairs in Step 1(b)i. Ideally the candidate set will be of manageable size, but will include all pairs (d, d ) that should be clustered together.</p><p>Second, we will exploit the fact that classification learners can provide a confidence for their classifications. We replace Steps 1 and 2 with better methods for building and using the "pairing graph" G. In clustering Step 1, we construct the edges of G by using the same candidate-pair generation procedure used in training, and then weight each edge (d, d ) by the confidence of the learned hypothesis h so that the label of (d, d ) should be "+". In Step 2, we cluster the resulting edge-weighted graph (in this paper, using greedy agglomerative clustering). The resulting algorithm is shown in Figure <ref type="figure">2</ref>.</p><p>We next consider the generation of candidate pairs (an operation often all called "blocking" in the record linkage literature). We use the canopy method , proposed by Mc-Callum, Nigam and Unger <ref type="bibr" target="#b13">[13]</ref>. This method relies on the ability to take an entity-name d and efficiently find all nearby points d according to some "approximate" distance metric. Following McCallum et al we used a TFIDF distance metric based on tokens. In this case, an inverted-index based ranked retrieval system can find nearby pairs quite quickly.</p><p>The canopy method, shown in Figure <ref type="figure" target="#fig_2">3</ref>, begins with an empty set of candidate pairs, and operates by repeatedly </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Perform greedy agglomerative clustering (GAC) on G</head><p>to produce K clusters.</p><p>(a) Create a singleton cluster to hold each vertex.</p><p>(b) While there are more than K clusters:</p><p>• Merge the two "closest" clusters, where cluster distance is the minimum distance between any members of the clusters.</p><p>3. Use the clustering produced by GAC on G as the clustering of D.</p><p>Figure <ref type="figure">2</ref>: A better and more efficient adaptive clustering algorithm picking a random "center point" d. After d is picked, all points d that are "close enough" to d (within distance T loose ) are found. These "canopy" points are paired with each other, and the resulting pairs are added to the set of candidate pairs. Next, the set of possible "center points" is decreased by removing all points d within distance T tight of d, where T tight &lt; T loose . This process repeats until all possible center points are chosen.</p><p>For the benchmark problems considered in Section 4, it was fairly easy to find thresholds T tight and T loose that allow generation of nearly all "true" pairs (pairs that belong in a desired cluster) without generating too many spurious pairs.</p><p>In learning, two issues must be addressed: how to represent a pair (d, d ), and which learning algorithm to use. We explored several different classification learning systems, and different feature sets for representing pairs (d, d ). Here we will report results for a maximum entropy learner <ref type="bibr" target="#b16">[16]</ref>. This learning system requires that examples be represented as a vector of binary features. Examples of the features used to encode a pair are shown in Table <ref type="table" target="#tab_0">1</ref>. Here the edit distance To compute CandidatePairs(D): In some of the test datasets we considered, the items to be clustered are not strings, but records consisting of several strings (for instance, a record containing a name and an address, or a bibliographic entry containing a title, author, date, and publication venue). For such datasets, a pair was encoded by extracting the features of Table <ref type="table" target="#tab_0">1</ref> for every pair of fields, and combining all the features: for instance, in pairing name/address records, we computed the features SubstringMatch name , SubstringMatch address , PrefixMatch name , PrefixMatch address , . . . , StrongNumberMatch name , StrongNumberMatch address .) A functionally equivalent but somewhat more efficient approach would be to use a soft join algorithm <ref type="bibr" target="#b3">[3]</ref> however, should be replaced with an operation that enforces the constraints required for constrained adaptive matching. This can be done by computing the minimal weight cutset of G, and returning the edges of this cutset as the pairing.</p><formula xml:id="formula_2">1. Let CandidatePairs = ∅. 2. Let PossibleCenters = D.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Matching and Constrained Matching</head><p>We have experimented with both a greedy approach and an exact minimization (which exploits the fact that the graph is bipartite <ref type="bibr" target="#b17">[17]</ref>). The experiments in this paper are for a simple greedy mincut-finding algorithm, which is more efficient for large graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relationships</head><p>We note that the problems of learning pairing functions, clustering, and matching are closely related, but distinct. In unconstrained matching, the pairs do not correspond immediately to clusters, since pairs may overlap, but clusters are disjoint. In constrained matching, matching can be reduced to clustering, but exploiting the additional constraint that a pairing is one-to-one can substantially change the difficulty of a clustering task. Finally, while learning a pairing function is a natural way of making a clustering system adaptive, obtaining an accurate hypothesis h does not mean that the ensuing clustering will be any good, as it is possible for small errors in h to cause large clustering errors <ref type="bibr" target="#b4">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>We used several datasets for evaluation purposes. Two of the datasets require clustering, and two require matching. The first clustering dataset, Cora, is a collection of paper citations from the Cora project <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13]</ref>. The second dataset, OrgName, is a collection of 116 organization names. We considered two target clusterings of this data, one into There are also two constrained matching datasets. The Restaurant dataset contains 533 restaurants from one restaurant guide to be matched with 331 from a second guide. <ref type="foot" target="#foot_1">2</ref>The Parks dataset contains 388 national park names from one listing and 258 from a second listing, with 241 names in common.</p><p>We assumed that the number of intended clusters K is known. For OrgName, Restaurant, and Parks, we constrained all systems (adaptive and non-adaptive) to produce the true number of clusters or pairings. For Cora, we wished to compare to the best previous clustering result, which was obtained varying cluster size widely. We tried two different target cluster sizes and report the one which gave the best result, obtained setting K to 1.5 times the true number of clusters.</p><p>To evaluate performance we split the data into two partitions, then trained on the first and tested on the second, and finally trained on the second and tested on the first. The datasets used are summarized in Table <ref type="table" target="#tab_3">2</ref>. For each dataset, we record the number of entities in each partition; the number of desired clusters or pairs; the thresholds used for the canopy algorithm; and the number of positive and negative examples generated.</p><p>As success measures for the algorithms, we used several different definitions of "loss". Recall that for matching, a solution y * is a set of pairs (a, b). Following the usual con-   <ref type="table" target="#tab_3">2</ref> shows the maximum recall obtainable using the CandidatePairs produced by the canopy algorithm. 4  In addition to the algorithm described in Section 3, we considered two additional clustering/matching algorithms as performance baselines. The first one replaces c(a, b) in the graphs above with Levenstein edit distance. Applied to clustering, this aseline algorithm is similar to the algorithm proposed by McCallum, Nigam and Unger; applied to matching, it is similar to the method proposed by Monge and Elkan <ref type="bibr" target="#b14">[14]</ref>. The second baseline replaces c(a, b) with TFIDF distance, using the formula given in <ref type="bibr" target="#b18">[18]</ref>, which is similar to the algorithm used in WHIRL <ref type="bibr" target="#b2">[2]</ref>.</p><p>The experimental results for these algorithms on the datasets of Table <ref type="table" target="#tab_3">2</ref> are shown in Tables <ref type="table" target="#tab_1">3</ref> and<ref type="table" target="#tab_2">4</ref>. The baseline results for edit distance are taken from <ref type="bibr" target="#b13">[13]</ref>, who used hand-tuned edit distance, and unlike the other entries in the table, they apply to the whole set, rather than a single partition. In Table <ref type="table" target="#tab_2">4</ref>, the best F-measure obtained on each problem is placed in bold.</p><p>A first observation on the results of Table <ref type="table" target="#tab_2">4</ref> is that neither baseline system appears to outperform the other. Discounting Cora (for which the edit-distance function was hand-3 That is, F = 2•P •R (P +R) . 4 Creating appropriate partitions for training and test is nontrivial, since one must ensure that the test cases are independent of the training cases, and a simple random partition of would likely lead to a situation in which some of the intended clusters were split between the training and test sets. To avoid this, we split the data so that no algorithm that considers only pairs produced by the canopy algorithm would ever consider a pair containing one instance from the test set and one instance from the training set. A disadvantage of this procedure is that it was sometimes impossible to create well-balanced splits, biasing the results away from adaptive methods. engineered), the TFIDF-based baseline obtains a better F1 score than the distance-function baseline on five runs, performs worse on two runs, and performs identically on one run. This confirms our belief that both TFIDF and editdistance distance metrics are useful in data integration settings.</p><p>The adaptive method does far better than either baseline technique on the Cora dataset. Notice that the Cora dataset is the largest of the datasets considered, as well as the one for which the baseline methods perform the worst; hence it offers the most opportunity for adaptive techniques to improve performance. In the remaining eight runs, the adaptive technique performs best on five, and nearly equals the best result on two more (the first split of OrgName1 and the second split of Restaurant). Thus on nine of the ten partitions, the adaptive method obtains results comparable to or better than the best of the baseline approaches.</p><p>The adaptive methods performs poorly on only one of the ten runs-the second partition of OrgName1. We conjecture that for this dataset (by far the smallest we considered) the constraints on partitioning used above resulted in substantial variation across the two partitions used for training and testing.<ref type="foot" target="#foot_2">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>We have presented a scalable adaptive scheme for clustering or matching entity names. Experimental results with the method are comparable to or better than results obtained by clustering or matching with two plausible fixed distance metrics.</p><p>As noted above, our formalization of adaptive clustering and matching is inspired by the model of "learning to order" of Cohen, Schapire, and Singer <ref type="bibr" target="#b5">[5]</ref>. They consider adaptive ordering systems and show that this problem can be solved by supervised learning of a binary ordering relation, followed by a greedy method for constructing a total order given a set of (possibly inconsistent) binary ordering decisions. They also give provable bounds on the loss of such a system. Finding such bounds for adaptive clustering or learning remains a problem for future work.</p><p>The architecture of the adaptive matching and clustering method is modeled after the system of McCallum, Nigam and Unger <ref type="bibr" target="#b13">[13]</ref>. However, in our system, we consider matching as well as clustering, we also replace a fixed, hand-coded, edit-distance metric with a learned pairing function. Our focus on general-purpose adaptive clustering and matching methods also distinguishes this work from previous work on general-purpose non-adaptive similarity metrics for entity names (e.g. <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b14">14]</ref>) or general frameworks for manually implementing similarity metrics (e.g., <ref type="bibr" target="#b8">[8]</ref>).</p><p>The "core" idea of learning distance functions for entity pairs is not new-there is a substantial literature on the "record-linkage" problem in statistics (e.g., <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b20">20]</ref> much of which based on a record-linkage theory proposed by Felligi and Sunter <ref type="bibr" target="#b7">[7]</ref>. The maximum entropy learning approach we use has an advantage over Felligi-Sunter in that it does not require features to be independent, allowing a broader range of potential similarity features to be used; at the same time the method is fairly efficient, in contrast to Felligi-Sunter extensions based on latent class models <ref type="bibr" target="#b19">[19]</ref>.</p><p>ChoiceMaker.com, a recent start-up company, has also implemented a matching procedure based on a maximum entropy learner. We extend this work with a systematic experimental evaluation, use of canopies to eliminate the potentially quadratic cost of learning and clustering, and application of the pairing function to both clustering and matching.</p><p>A number of enhancements to the current method are possible. In future work we hope to examine other features; for instance, one notable current omission is the lack of any feature that directly measures TFIDF similarity. We also hope to compare these methods directly to other matching techniques developed in the statistical literature <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>To</head><label></label><figDesc>train from {(D 1 , z 1 ), . . . , (D m , z m )}: 1. Build a training sample S for the pairing function h. (a) Let S = ∅. (b) For i = 1, . . . , m: i. Generate all pairs (d, d ) ∈ Di × Di.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A naive clustering algorithm based on a learned pairing function</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 .</head><label>3</label><figDesc>To train from {(D 1 , z 1 ), . . . , (D m , z m )}: 1. Build a training sample S for the pairing function h. (a) Let S = ∅. (b) For i = 1, . . . , m: i. Let CandidatePairs(D) be a set of "candidate" pairings (d, d ). ii. For each (d, d ) ∈ CandidatePairs(D), let label (d, d ) ≡ + if zi(d) = zi(d ) -otherwise iii. Add the labeled example (d, d ) to S. 2. Train a classification learner on S. The result will be a hypothesis h that labels pairs (d, d ) as positive or negative. Let c(d, d ) be the confidence given by h that the h(d, d ) = +. To cluster a new set D = {d 1 , . . . , d n } into K clusters: 1. Build a graph G with vertex set D, where an edge exists between d i and d j iff (d i , d j ) ∈ CandidatePairs(D), and the weight of the edge between d and dj is c(di, dj).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 .Figure 3 :</head><label>33</label><figDesc>Figure 3: Computing a set of candiate pairs using the canopy algorithm of McCallum, Nigam and Unger</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>It is fairly simple to adapt the algorithm above to the problem of constrained adaptive matching. Generation of candidate pairs is substantially easier, since one need only consider pairs (a, b) where a ∈ A and b ∈ B. One possible technique is to use the canopy algorithm of Figure 3 with these modifications: • in Step 2, let PossibleCenters = A; • in Step 3b, let Canopy(a) = {(a, b) : b ∈ B and approxDist(a, b) &lt; T loose }; and • in Step 3d, let T tight = 0 (i.e., only remove a from the set of PossibleCenters).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>For clustering algorithms, recall that a problem instance x is a set of objects D, and a solution y * is a mapping z from D into the integers {1, . . . , K}, and define pairs(D, z) to be the set of all pairs {(d, d ) ∈ D × D : z(d) = z(d )}. We will define recall and precision in terms of pairs(D, z): i.e., we define the recall of z relative to z * is |pairs(D, z) ∩ pairs(D, z * )|/|pairs(D, z * )|, and the precision of z relative to z * is |pairs(D, z) ∩ pairs(D, z * )|/|pairs(D, z)|. The final column of Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>9}, true iff the Jaccard distance between the sets of tokens in d and d is less than k. StrongNumberMatch true if both d and d contain the same number. Features used in learning the pairing function.</figDesc><table><row><cell>.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Experimental results: precision and recall</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Experimental results: F-measure 56 clusters, and one into 60 clusters.1  </figDesc><table><row><cell cols="4">Benchmark TFIDF Edit Distance Adaptive</cell></row><row><cell>Cora</cell><cell>0.751</cell><cell>0.839</cell><cell>0.945</cell></row><row><cell></cell><cell>0.721</cell><cell></cell><cell>0.964</cell></row><row><cell>OrgName1</cell><cell>0.925</cell><cell>0.633</cell><cell>0.923</cell></row><row><cell></cell><cell>0.366</cell><cell>0.950</cell><cell>0.776</cell></row><row><cell>OrgName2</cell><cell>0.958</cell><cell>0.571</cell><cell>0.958</cell></row><row><cell></cell><cell>0.778</cell><cell>0.912</cell><cell>0.984</cell></row><row><cell cols="2">Restaurant 0.981</cell><cell>0.827</cell><cell>1.000</cell></row><row><cell></cell><cell>0.967</cell><cell>0.867</cell><cell>0.950</cell></row><row><cell>Parks</cell><cell>0.976</cell><cell>0.967</cell><cell>0.984</cell></row><row><cell></cell><cell>0.967</cell><cell>0.967</cell><cell>0.967</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Datasets used in the experimental evaluation vention in information retrieval, we define the recall of y relative to y * to be |y ∩ y * |/|y * |, the precision of y relative to y * to be |y ∩ y * |/|y|, the F-measure of y relative to y * to be the harmonic mean of recall and precision.3  </figDesc><table><row><cell>Benchmark Name</cell><cell>Cluster</cell><cell>Partition Size</cell><cell></cell><cell cols="2">Thresholds</cell><cell cols="3">Pairing Examples Potential</cell></row><row><cell></cell><cell cols="5">or Match? #Entities #Clusters T tight T loose</cell><cell>#Pos</cell><cell>#Neg</cell><cell>Recall</cell></row><row><cell>Cora</cell><cell>(c)</cell><cell>991</cell><cell>65</cell><cell>0.36</cell><cell cols="2">0.53 19,111</cell><cell>7,379</cell><cell>0.972</cell></row><row><cell></cell><cell></cell><cell>925</cell><cell>64</cell><cell></cell><cell></cell><cell>15,431</cell><cell>8,711</cell><cell>0.998</cell></row><row><cell>OrgName1</cell><cell>(c)</cell><cell>60</cell><cell>42</cell><cell>0.24</cell><cell>0.40</cell><cell>33</cell><cell>56</cell><cell>1.000</cell></row><row><cell></cell><cell></cell><cell>56</cell><cell>17</cell><cell></cell><cell></cell><cell>196</cell><cell>250</cell><cell>1.000</cell></row><row><cell>OrgName2</cell><cell>(c)</cell><cell>53</cell><cell>34</cell><cell>0.24</cell><cell>0.40</cell><cell>36</cell><cell>48</cell><cell>1.000</cell></row><row><cell></cell><cell></cell><cell>63</cell><cell>22</cell><cell></cell><cell></cell><cell>270</cell><cell>181</cell><cell>1.000</cell></row><row><cell>Restaurant</cell><cell>(m)</cell><cell>430</cell><cell>52</cell><cell>0.28</cell><cell>0.93</cell><cell>52</cell><cell>426</cell><cell>1.000</cell></row><row><cell></cell><cell></cell><cell>434</cell><cell>60</cell><cell></cell><cell></cell><cell>59</cell><cell>153</cell><cell>0.983</cell></row><row><cell>Park names</cell><cell>(m)</cell><cell>325</cell><cell>124</cell><cell>0.30</cell><cell>0.90</cell><cell>124</cell><cell>304</cell><cell>0.992</cell></row><row><cell></cell><cell></cell><cell>321</cell><cell>117</cell><cell></cell><cell></cell><cell>117</cell><cell>357</cell><cell>0.975</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The difference is that in the second clustering, different branches of an organization (such as "Virginia Polytechic Institute, Blacksburg" and "Virginia Polytechnic Institute, Charlottesville") are considered distinct, and in the first, they are not. Thanks to Nick Kushmeric for providing this data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Thanks to Sheila Tejada for providing this data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>Notice that the TFIDF-based baseline system does much better than the edit-distance based baseline on the first partition, but that the opposite holds on the second partition. Thus even the trivial adaptive system that chooses the better of the two baseline systems based on training data would perform poorly. The size of the pairing-function training sets and the number of entities per cluster is also varies greatly in the two partitions.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors thank Andrew McCallum for numerous helpful suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reasoning about textual similarity in information access</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Agents and Multi-Agent Systems</title>
		<imprint>
			<biblScope unit="page" from="65" to="86" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Data integration using similarity joins and a word-based information representation language</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="288" to="321" />
			<date type="published" when="2000-07">July 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">WHIRL: A word-based information representation language</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="163" to="196" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to match and cluster entity names</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Richman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGIR-2001 Workshop on Mathematical/Formal Methods in Information Retrieval</title>
		<meeting>the ACM SIGIR-2001 Workshop on Mathematical/Formal Methods in Information Retrieval<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to order things</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="243" to="270" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to extract symbolic knowledge from the world wide web</title>
		<author>
			<persName><forename type="first">M</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dipasquo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Slattery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI-98)</title>
		<meeting>the Fifteenth National Conference on Artificial Intelligence (AAAI-98)<address><addrLine>Madison, WI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A theory for record linkage</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">P</forename><surname>Felligi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Sunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Society</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="1183" to="1210" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AJAX: an extensible data-cleaning tool</title>
		<author>
			<persName><forename type="first">H</forename><surname>Galhardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Florescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shasha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGMOD-2000</title>
		<meeting>ACM SIGMOD-2000</meeting>
		<imprint>
			<date type="published" when="2000-06">June 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The merge/purge problem for large databases</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stolfo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1995 ACM SIGMOD</title>
		<meeting>the 1995 ACM SIGMOD</meeting>
		<imprint>
			<date type="published" when="1995-05">May 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Record linkage techniques-1985. Statistics of Income Division, Internal Revenue Service Publication 1299-2-96</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kilss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Alvey</surname></persName>
		</author>
		<ptr target="http://www.bts.gov/fcsm/methodology/" />
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Digital libraries and autonomous citation indexing</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="67" to="71" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient clustering of high-dimensional data sets with application to reference matching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Sixth International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The field-matching problem: algorithm and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Monge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Second International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="1996-08">August 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic linkage of vital records</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Axford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="954" to="959" />
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using maximum entropy for text classification</title>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning for Information Filtering Workshop, IJCAI &apos;99</title>
		<meeting>Machine Learning for Information Filtering Workshop, IJCAI &apos;99<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Matching algorithms for bipartite graph</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Baier Saip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Lucchesi</surname></persName>
		</author>
		<idno>DCC-03/93</idno>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Departamento de Cincia da Computao</publisher>
		</imprint>
		<respStmt>
			<orgName>Universidade Estudal de Campinas</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m">Automatic Text Processing</title>
		<editor>
			<persName><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</editor>
		<meeting><address><addrLine>Reading, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Welsley</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved decision rules in the Felligi-Sunter model of record linkage. Statistics of Income Division</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Winkler</surname></persName>
		</author>
		<ptr target="http://www.census.gov/srd/www/byname.html" />
	</analytic>
	<monogr>
		<title level="j">Internal Revenue Service Publication RR</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The state of record linkage and current research problems. Statistics of Income Division</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Winkler</surname></persName>
		</author>
		<ptr target="http://www.census.gov/srd/www/byname.html" />
	</analytic>
	<monogr>
		<title level="j">Internal Revenue Service Publication R</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Matching and record linkage</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">E</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Business Survey methods</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
