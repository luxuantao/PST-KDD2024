<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Sang</forename><forename type="middle">Il</forename><surname>Park</surname></persName>
							<email>sipark@jupiter.kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Division of Computer Science</orgName>
								<orgName type="institution">Korea Advanced Institute of Science and Technology</orgName>
								<address>
									<settlement>Daejeon</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5D287F54B8A59183A71608C019432CE8</idno>
					<idno type="DOI">10.1002/cav.15)</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>computer animation</term>
					<term>example-based motion synthesis</term>
					<term>motion blending</term>
					<term>locomotion generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present an integrated framework of on-line motion blending for locomotion generation. We first provide a novel scheme for incremental timewarping, which always guarantees that the time goes forward. Combining the idea of motion blending with that of posture rearrangement, we introduce a motion transition graph to address on-line motion blending and transition simultaneously. Guided by a stream of motion specifications, our motion synthesis scheme moves from node to node in an on-line manner while blending a motion at a node and generating a transition motion at an edge. For smooth on-line motion transition, we also attach a set of example transition motions to an edge. To represent similar postures consistently, we exploit the inter-frame coherency embedded in the input motion specification. Finally, we provide a comprehensive solution to on-line motion retargeting by integrating existing techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>On-line motion blending for real-time locomotion generation</head><p>By Sang Il Park*, Hyun Joon Shin, Tae Hoon Kim and Sung Yong Shin</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Synthesizing convincing lifelike motions has been a recurring theme in computer graphics. Due to the recent popularity of motion capture and reuse, example-based approaches to human motion generation have been prevailing in applications such as computer games and character animation. Those approaches are based on either motion blending <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> or posture rearrangement. <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> In this paper, we propose a novel hybrid approach to on-line real-time locomotion generation by combining advantages of both approaches.</p><p>With their inherent efficiency, example-based approaches have potential for on-line motion synthesis. Historically, performance-based computer puppetry initiated on-line character animations. <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref> Recently, Park et al. <ref type="bibr" target="#b0">1</ref> proposed an on-line motion blending scheme for locomotion generation, based on scattered data interpolation. <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3</ref> Using labelled example motions, their work demonstrated the capability of on-line locomotion generation with a real-time performance. However, their work also brought up some key issues in on-line motion blending.</p><p>To facilitate on the fly motion blending, they proposed to combine motions called 'verbs' by incorporating motion types and speeds into parameters called 'adverbs' on top of other parameters such as turning angles, happiness, anger, and tiredness as given in. <ref type="bibr" target="#b1">2</ref> They were able to produce motions of various types, speeds, and turning angles on the fly. However, motions of different types have structural differences, which may not be blended in a meaningful manner. For example, consider two example motions of different types such as walking and running. Suppose that all other parameters of those motions are the same. Blending such motions would result in a quite unfamiliar motion, which is neither walking nor running.</p><p>They also proposed an incremental timewarping scheme to prevent the simulation time from going in reverse. Their scheme works as long as all example motions have non-negative weights. Motion blending schemes <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4</ref> in general do not guarantee this condition. In fact, the strength of scattered data interpolation <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3</ref> is to incorporate both interpolation and extrapolation in a single framework. Some negative weights facilitate this paradigm.</p><p>Finally, to make similar poses have similar representation, they represented the example joint orientations with respect to the reference orientation. To obtain the reference orientation, their scheme solved an eigenvector problem for each joint at every frame, which is timeconsuming. A main advantage of motion blending is its supreme time efficiency as demonstrated in. <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3</ref> This advantage may be undermined by the eigenvector problem if it needs to be solved excessively often. In particular, performances are degraded considerably for applications such as crowd simulation systems required to handle massive requests for motion synthesis.</p><p>In this paper, we fully address each of these issues and provide an integrated framework for on-line motion blending. We present a novel scheme for incremental timewarping, which always guarantees that the time goes forward. Inspired by a verb graph 2 together with motion graphs, <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> we introduce a motion transition graph to address on-line motion blending and transition simultaneously. Guided by a stream of on-line motion specifications, our scheme moves from node to node while blending a motion at a node and generating a transition motion at an edge. Unlike the verb graph, we also attach a set of example transition motions to an edge to facilitate smooth on-line motion transition. To represent similar motions consistently, we exploit the inter-frame coherency embedded in the input stream of motion specifications. Finally, we provide a comprehensive solution to on-line motion retargeting by integrating previous results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>There have been many research results for locomotion generation using live captured motion clips. For our purpose, we focus on two categories of schemes directly related to our work: those based on motion blending <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> and those based on posture rearrangement. <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> The main difference between the two categories of schemes is the structures of example motions to be used. Exploiting the example motions of an identical structure, the former synthesizes a novel motion by blending labelled example motions, and the latter generates a motion by seamlessly rearranging exiting postures in example motions.</p><p>Motion Blending. The former category includes various motion blending schemes. Guo and Roberge ´5 and Wiley and Hahn 4 provided interpolation techniques for example motions located regularly in parameter spaces. Rose et al. <ref type="bibr" target="#b1">2</ref> presented a framework of motion blending based on scattered data interpolation with radial basis functions. For smooth motion transition, they used a verb graph, which is conceptually similar to the motion graphs introduced in the latter category of schemes. Sloan et al. <ref type="bibr" target="#b2">3</ref> reformulated their scheme to provide a more efficient scheme. Park et al. <ref type="bibr" target="#b0">1</ref> proposed an on-line motion blending scheme to control virtual characters in real time. Kim et al. <ref type="bibr" target="#b5">6</ref> presented a method for synthesizing rhythmic motions by exploiting the motion blending scheme of Park et al. However, the original version of this scheme may cause artifacts in resulting transition motions since they achieved motion transition by incorporating motion types as a parameter. In addition, the scheme exhibited an anomaly in timewarping and required relatively heavy computation for posture blending compared to others. <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3</ref> We provide an elegant solution to each of those problems.</p><p>Posture Rearrangement. The latter category of schemes obtain realistic motion by picking up postures in example motions while traversing the motion graphs representing the examples. The motion graphs facilitate seamlessly stitching the postures picked up. Kovar et al. <ref type="bibr" target="#b7">8</ref> introduced a motion graph to represent the transitions among the poses of captured motion data. They pointed out that automatic generation of a convincing transition motion is as difficult as creating a new motion in the first place. Lee et al. 9 also represented captured motion data with a similar graph structure, and provided effective user interfaces for interactive character control. Arikan and Forsyth 7 applied a randomized algorithm to search for motions from a hierarchy of transition graphs. Pullen and Bregler 11 developed a method for enhancing roughly-keyframed animations with captured motion data. Arikan et al. <ref type="bibr" target="#b11">12</ref> generated a motion by rearranging motion segments while satisfying user-specified annotations for the resulting motion. This category of schemes can generate realistic motions while preserving details of the original motions. However, these works may not be suitable for on-line motion generation, since it is time-consuming to search for desired motions from the graph when the number of example motions is large. We incorporate the notion of motion graphs into our scheme for smooth motion transition.</p><p>Motion Retargeting. Motion retargeting is a crucial part of motion synthesis. Gleicher <ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17</ref> gave the spacetime formulation for motion retargeting. Lee and Shin <ref type="bibr" target="#b17">18</ref> provided an interactive motion retargeting scheme based on hierarchical curve fitting. Choi et al. <ref type="bibr" target="#b12">13</ref> presented an on-line motion retargeting scheme based on inverse rate control, which computes the amount of joint angle change needed to place an end-effector at a desired position. Shin et al. <ref type="bibr" target="#b14">15</ref> suggested an importancebased approach for computer puppetry. They provided the notion of importance of an end-effector and introduced a fast, robust inverse kinematics solver to realize the important aspect of the end-effector according to its importance value. Park et al. proposed a motion retargeting scheme for on-line motion blending. We provide a comprehensive motion retargeting scheme for on-line motion blending by combining the previous work. <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>To facilitate on-line motion blending, we construct a motion transition graph as shown in Figure <ref type="figure" target="#fig_0">1</ref>, by exploiting advantages of the two typical categories of example-based schemes based on motion blending and posture rearrangement, respectively.</p><p>Every node of the graph represents a set of parameterized example motions of the same type. Thus, they have the same structure as described in <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> . For example, they start with the same foot and have the same number of footsteps to move. Each directed edge represents the transition from a motion to a motion. In general, a pair of nodes connected by an edge represent different types of motions, unless the edge represents a self-transition. For smooth motion transition, we also attach a set of parameterized transition motions of the same type to each edge representing a non-self transition. We assume that the example transition motions are available: they could be precaptured or extracted from unlabelled motion sequences.</p><p>For motion synthesis, the motion transition graph is traversed from node to node, guided by a stream of motion specifications, which are supplied by the user in an on-line manner. A motion specification contains two pieces of data, that is, motion type and motion parameters. The motion type is used to choose a motion, and the motion parameters define the time-varying characteristics of the motion to be synthesized. We characterize a motion mainly by two parameters, that is, speed and turning angle since our main concern is on locomotion generation.</p><p>On visiting a node, the example motions at the node are blended to produce the motion corresponding to the motion parameters. While making the transition to another node via an edge, a transition motion is first synthesized in a similar manner by using the example transition motions attached to the edge. This motion is then blended with the motion synthesized independently at each incident node. The motion generation at nodes and edges is done on the fly.</p><p>Finally, the resulting motions are retargeted to the target character and the environment. Although motion retargeting is important for convincing motion generation, it has not been treated rigorously in motion blending <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> except in the work in <ref type="bibr" target="#b0">1</ref> . By combining recent results <ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18</ref> , we provide a comprehensive solution to the retargeting problem for on-line motion blending.</p><p>The remainder of this paper is organized as follows: First, motion synthesis at a node and at an edge are presented together with parameterizing locomotions. We then describe how to generate a smooth transition between motions. Next, we show how to automatically find kinematic constraints to retarget the motion. Finally, we demonstrate experimental results and conclude our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motion Blending</head><p>To synthesize motion at a node or an edge, we employ a motion blending scheme. To blend a motion, we first parameterize a set of example motions as a preprocessing step. Once motion clips are parameterized, we blend motion clips at run time based on multi-dimensional scattered data interpolation. Our motion blending scheme consists of three steps: weight computation, incremental timewarping, and posture blending. Given a vector of parameters at each frame in an on-line manner, our scheme first determines the weights of example motions, and then performs incremental timewarping to choose the proper postures for blending from the example motions. Finally, a novel motion is synthesized by blending them on the fly with respect to their weights. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ON-LINE MOTION BLENDING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Locomotion Parameterization</head><p>To parameterize locomotions automatically, we employ the scheme of Park et al. <ref type="bibr" target="#b0">1</ref> with minor modifications. For effective control over a variety of locomotion, they used three parameters: types, speeds, and turning angles. Other parameters such as happiness, anger, and tiredness may also be specified interactively as discussed in. <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3</ref> They used the type parameter to deal with motion transition. As pointed out in the introduction, motions of different types have different structures. Based on scattered data interpolation, 2,3 their scheme may not guarantee convincing motion transition. Therefore, excluding the type parameter, we mainly use two parameters: speeds and turning angles. We summarize their scheme for completeness. They assumed that each example motion is short enough for its speed and turning angle to be invariant. To satisfy this assumption, a lengthy motion with nonhomogeneous speed and turning angle were decomposed into short motions. Since a motion of constant speed and turning angle traces a circular trajectory, they approximated the projected root trajectory of a motion on the floor to a circular arc a. The arc a can possibly be a straight line segment, that is, a circular arc of infinite radius.</p><p>The circular arc for a motion clip is computed as follows. Let aðp c ; a 0 ; Þ be the circular arc centered at p c with subtending angle measured from a point a 0 on the circle of radius r. Note that r is implicitly given by the distance between p c and a 0 . They found the circular arc aðp c ; a 0 ; Þ that is closest to p p in a least-squares sense:</p><formula xml:id="formula_0">argmin p c ;a0; X N f i¼1 p p i À að i ; p c ; a 0 ; Þ ½ 2<label>ð1Þ</label></formula><p>where p p i is the point on the projected root trajectory at frame i, að i ; p c ; a 0 ; Þ is the point on the arc apart from a 0 by the angle i ¼ ði Á N f À1 Þ, and N f is the number of frames. From the duration T of the motion together with the length l of the arc, they obtained the speed and turning angle of the motion, which are l T and T , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weight Computation</head><p>We adopt the scheme of Sloan et al. <ref type="bibr" target="#b2">3</ref> to compute the weights of example motions. They applied these weights to both timewarping and posture blending. However, unlike posture blending, timewarping is needed to extract the postures to be blended at each frame (time) from the example motions. Therefore, the weights should be non-negative so that the simulation time always moves forward. To achieve this, we devise a different scheme to compute the weights for timewarping than those for posture blending. The weights computed in this section will be applied to only posture blending.</p><p>Given the vector p of parameters, the weight w i ðpÞ of example motion i is defined as follows:</p><formula xml:id="formula_1">w i ðpÞ ¼ X Np l¼0 a il A l ðpÞ þ X Ne j¼1 r ij R j ðpÞ ð<label>2Þ</label></formula><p>where A l ðpÞ and a il are respectively the linear basis functions and their coefficients, and R j ðpÞ and r ij are respectively the radial basis functions and their coefficients. N p and N e are the number of the parameters and that of the examples, respectively. Let p i , 1 i N e , be the parameter vector of example motion i. To interpolate the example motions exactly, the weight of example motion i is set to be one at p i and zero at p j , i 6 ¼ j, that is,</p><formula xml:id="formula_2">w i ðp j Þ ¼ 1 for i ¼ j and w i ðp j Þ ¼ 0 for i 6 ¼ j.</formula><p>Ignoring the second term of equation ( <ref type="formula" target="#formula_1">2</ref>), we first solve for the linear coefficients a il to fix the first term:</p><formula xml:id="formula_3">w i ðpÞ ¼ X Np l¼0 a il A l ðpÞ ð<label>3Þ</label></formula><p>The linear bases are simply A l ðpÞ ¼ p l , 1 l N p , where p l is the lth component of p, and A o ðpÞ ¼ 1. A least squares method is used to determine the unknown coefficients a il of the linear bases using the parameter vector p i of each example and its weight w i ðp i Þ.</p><p>We employ the radial basis functions to interpolate the residuals w i ðpÞ ¼ w i ðpÞ À P Np l¼0 a il A l ðpÞ for all i. The radial basis function R j ðpÞ is a function of the Euclidean distance between p and p j in the parameter space:</p><formula xml:id="formula_4">R j ðpÞ ¼ B kp À p j k ; 1 j N e<label>ð4Þ</label></formula><p>where BðÁÞ is the cubic B-spline function, and is the dilation factor, which is the separation to the nearest other example in the parameter space. The coefficients r ij are calculated by solving the linear system:</p><formula xml:id="formula_5">rR ¼ w<label>ð5Þ</label></formula><p>where r is an N e Â N e matrix of the unknown coefficients r ij , and R and w are the matrices of the same size defined by the radial basis functions and the residuals, respectively, such that R ij ¼ R i ðp j Þ and w ij ¼ w i ðp j Þ. Provided with the solutions for a il and r ij , we finally obtain the weight function of each example motion as given in equation ( <ref type="formula" target="#formula_1">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incremental Timewarping</head><p>To blend the example motions of various speeds, we align them according to their keytimes, that is, the moments of interaction with the environment such as heel-strikes and toe-offs. <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18</ref> All example motions consist of the same sequence of keytime phases, that is, the example motions start with the same foot and take the same number of steps. We denote a keytime by K i ; 1 i N k , where N k is the number of the keytimes.</p><p>Monotonicity. Using the keytimes, timewarping is defined as a piecewise linear mapping of actual time T 2 ½K 1 ; K N k onto generic time t 2 ½0; 1. Given an actual time T, the corresponding generic time tðTÞ is</p><formula xml:id="formula_6">tðTÞ ¼ ðm À 1Þ þ T À K m K mþ1 À K m 1 N k À 1<label>ð6Þ</label></formula><p>where m is the largest index such that T &gt; K m . As illustrated in Figure <ref type="figure" target="#fig_1">2</ref>, the mapping tðTÞ for a single example motion is monotone in actual time T so that its inverse is uniquely defined. Conversely, actual time T is also monotonically increasing in generic time t. Given a set of example motions, what timewarping does is to establish their correspondence at each time instance along the common generic time. After aligning the motions by timewarping as shown in Figure <ref type="figure" target="#fig_2">3</ref>, we first specify the generic time and then choose the proper frame (actual time) corresponding to the generic time for each example motion clip by untimewarping. The postures at these frames are blended to obtain a novel posture. Again, the monotonicity of the actual time for each example motion is required to obtain a convincing blended motion. Otherwise, as pointed out in, <ref type="bibr" target="#b0">1</ref> the actual time would go in reverse to result in motion artifacts.</p><p>Incremental Timewarping. To blend the example motions of different speeds with their time-varying weights, Park et al. <ref type="bibr" target="#b0">1</ref> proposed an incremental timewarping scheme. Their basic idea was to blend the change rates of actual times with respect to the generic time rather than the actual times themselves. However, their scheme does not guarantee the monotonicity unless the weights are non-negative. Based on our interpretation on timewarping as depicted in Figure <ref type="figure" target="#fig_2">3</ref>, we propose a new scheme that always gives the non-negative weights and thus shows the desired monotonicity. Let t nÀ1 and t n  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ON-LINE MOTION BLENDING</head><p>be the generic times at frames n À 1 and n for n ! 1, respectively. Then,</p><formula xml:id="formula_7">t n ¼ t nÀ1 þ Át nÀ1<label>ð7Þ</label></formula><p>where Át nÀ1 is the blended change rate of the generic time with respect to the actual time measured at t nÀ1 . Given the generic time t n , the corresponding actual time Tðt n Þ of each example motion is obtained by untimewarping:</p><formula xml:id="formula_8">Tðt n Þ ¼ ðN k À 1Þ Á t n À ðm À 1Þ ð Þ Á ð K mþ1 À K m Þ þ K m<label>ð8Þ</label></formula><p>The change rate of the generic time for an example motion is the same as the first derivative of its timewarping function. Thus, we compute the blended change Át nÀ1 as follows:</p><formula xml:id="formula_9">Át nÀ1 ¼ X Ne i¼1 i ðpÞ Á À i ðt nÀ1 Þ ! Á ÁT<label>ð9Þ</label></formula><p>where i ðpÞ is the weight value for timewarping of example motion i at a parameter vector p, À i ðt nÀ1 Þ is the derivative value of the timewarping function for motion i computed at t nÀ1 , and ÁT is the actual interframe time normally set to one. Inspired by scattered data interpolation, we define i ðpÞ as follows:</p><formula xml:id="formula_10">i ðpÞ ¼ 1 N e þ X Ne j¼1 r ij R j ðpÞ ð<label>10Þ</label></formula><p>where R j ðpÞ and r ij are the radial basis functions and their coefficients, respectively. To ensure the non-negativity of i ðpÞ for all p, 1 i N e , we simplify the linear term to 1 Ne and choose the support radius for the radial basis function equal to the Euclidean distance to the nearest other example motion. R j and r ij are computed as the posture blending weight.</p><p>As illustrated in Figure <ref type="figure" target="#fig_3">4</ref>, i ðpÞ ! 0 for all p. We have Át nÀ1 ! 0 since ð P Ne i¼1 i ðpÞ Á À i ðt nÀ1 ÞÞ ! 0 from equation ( <ref type="formula" target="#formula_4">4</ref>), which guarantees the monotonicity of the actual time with respect to the generic time for every example motion. When the parameter vector p is coincident with that of example i, 1 i N e , the weight i becomes one to interpolate À i exactly. As p moves away from the positions of examples in the parameter space, i tends to be <ref type="bibr" target="#b0">1</ref> Ne , which is simply the mean weight over N e samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incremental Posture Blending</head><p>We generate the target posture at a given generic time by blending the corresponding postures of example motions at the same generic time. The posture of an articulated body is defined by the position of the root segment, its orientation, and the joint angles. We take the weighted sum of example root positions to synthesize the root position. However, due to the non-linearity of the orientation space, this scheme cannot be applied directly to blending orientation data such as root orientations and joint angles. With Euler angles, it is nontrivial to ensure that similar poses use similar Euler angles for example motions with various speeds and turning angles. Therefore, based on quaternion algebra, <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref> we propose an incremental posture blending scheme to address this problem by exploiting interframe coherency.</p><p>To facilitate incremental posture blending, we represent the example postures chosen at every frame with respect to the reference orientation q Ã . Given the reference orientation q Ã , our basic idea for blending orientations is to transform the orientation data to their analogues in a vector space with respect to the reference orientation q Ã to compute their weighted sum, and then to transform the result back to the orientation space, as suggested in. <ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20</ref> The transformations between unit quaternions and vectors are done by logarithm and exponential maps.</p><p>Orientation Parametrization. By the logarithm map, a unit quaternion q is transformed to the corresponding displacement vector v with respect to the reference orientation q Ã , that is,</p><formula xml:id="formula_11">v ¼ logðq À1 Ã qÞ ð<label>11Þ</label></formula><p>Due to antipodal equivalence, it is well known that an orientation has two equivalent quaternion representations. If q lies outside the hemisphere centered at q Ã , that is, klogðq À1 Ã qÞk &gt; =2, then we use Àq rather than q to ensure the consistent representation. The original orientation is recovered by the exponential map:</p><formula xml:id="formula_12">q ¼ q Ã expðvÞ ð<label>12Þ</label></formula><p>Geometrically, the logarithm and exponentiation maps are transformations between the tangent space R 3 at the reference unit quaternion q Ã and the unit quaternion space S 3 . <ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20</ref> The logarithm map of a unit quaternion is not well defined at ÀI ¼ ðÀ1; 0; 0; 0Þ. Moreover, the exponential and logarithm maps facilitate a natural, non-singular parametrization of orientations for small angular displacements. <ref type="bibr" target="#b19">20</ref> To ensure that similar poses have similar representations, we need to choose the reference orientation q Ã carefully.</p><p>Inter-frame Coherency. Park et al. <ref type="bibr" target="#b0">1</ref> formulated the problem of finding the reference orientation q Ã as an eigenvector problem. The solution to the problem is the point on S 3 that minimizes the squared distances from all example orientations q i , 1 i N e . However, this formulation chooses the reference orientation q Ã for each joint at every frame by solving an eigenvector problem, which is time-consuming.</p><p>To address this problem, we adopt an incremental approach. Initially, we obtain the blended joint configuration by solving the eigenvector problem. In general, provided with the blended joint orientation at the previous frame, we use it as the reference orientation at the current frame to incrementally update the posture. Our new scheme not only provides inter-frame coherency in motion representation but also avoids solving the eigenvector problem at every frame except for the first frame to enhance efficiency.</p><p>Given the blended joint orientation qðt nÀ1 Þ at frame n À 1, we transform each example orientation q i ðt n Þ, 1 i N e at frame n into its corresponding displacement vector v i ðt n Þ through the logarithm map, that is,</p><formula xml:id="formula_13">v i ðt n Þ ¼ logðqðt nÀ1 Þ À1 q i ðt n ÞÞ<label>ð13Þ</label></formula><p>Then, we blend v i ðt n Þ, 1 i N e with respect to their weights to obtain the displacement vector</p><formula xml:id="formula_14">vðt n Þ ¼ X Ne i¼1 w i v i ðt n Þ ð<label>14Þ</label></formula><p>at frame n. Finally, we compute the blended orientation qðt n Þ by transforming vðt n Þ back to the orientation space and then apply it to qðt nÀ1 Þ incrementally, that is,</p><formula xml:id="formula_15">qðt n Þ ¼ qðt nÀ1 Þexpðvðt n ÞÞ<label>ð15Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motion Transition</head><p>In this section, we present a novel on-line motion transition scheme between a pair of motions of different types. Those motions usually have different structures; in particular, they have different keytime sequences.</p><p>Our scheme can handle this difference properly to generate a convincing transition motion.</p><p>Example Transition Motions. Suppose that we have a motion transition request from node i to node j.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> shows the local structure of the motion transition graph and the information attached to it. Let K i ¼ fK i 1 ; K i 2 ; . . . ; K i ni g be the keytime sequence of the synthesized motion at node i, where n i is the number of keytimes in K i . Similarly, K i;j ¼ fK i;j 1 ; K i;j 2 ; . . . ; K i;j n i;j g represents the keytime sequence of the synthesized transition motion at edge ði; jÞ from node i to node j. As illustrated in Figure <ref type="figure" target="#fig_5">6</ref>, K i;j partially shares keytimes at both extremes with K i and K j . Thus, we parameterize an example transition motion by a vector, which is obtained by simply concatenating the parameter vectors of example motions to be stitched.</p><p>Motion Transition. The posture of the transition motion at an edge ði; jÞ is obtained on the fly by blending the corresponding postures of a node and the edge, each of which is synthesized independently using the scheme as given in the previous section.</p><p>First, we explain how to obtain the transition motion at the beginning. Let B ¼ ½b 1 ; b 2 for b 1 &lt; b 2 be the period of transition at the beginning as shown in Figure <ref type="figure" target="#fig_5">6</ref> (the period marked 'B'), where b 1 and b 2 are relative frame numbers counting from the initial frame. Then, the root position p i;j 0 of the transition motion at each frame f 2 B is computed as follows:</p><formula xml:id="formula_16">p i;j 0 ¼ Á p p i;j 0 þ ð1 À Þ Á p p i 0<label>ð16Þ</label></formula><p>where p p i;j 0 and p p i 0 are the root positions synthesized independently. To compute the weight , we use a sinusoidal function, 2 that is, ¼ 0:5cosðbÞ þ 0:5, where b ¼ fÀb1 b2Àb1 (see the upper right part of Figure <ref type="figure" target="#fig_5">6</ref>). Unlike that of Rose et al., 2 b is varied adaptively to the transition periods. Since we have a pair of motions to blend, we can simply employ slerp (spherical linear interplation) <ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21</ref> to obtain the orientation q i;j of each joint: q i;j ¼ q q i;j ðq q i;j Þ À1 q q i ð1ÀÞ ð17Þ</p><p>where q q i;j and q q i are again the synthesized joint orientations. Now, we explain how to compute the transition motions at the end. Let E ¼ ½e 1 ; e 2 for e 1 &lt; e 2 be the period of transition at the end (see the period marked 'E' in Figure <ref type="figure" target="#fig_5">6</ref>), where e 1 and e 2 are relative frame numbers. Then, by a similar derivation, we obtain the transition posture at each frame f 2 E as follows:</p><formula xml:id="formula_17">p i;j 0 ¼ Á p p j 0 þ ð1 À Þ Á p p i;j</formula><p>0 ; and q i;j ¼ q q j ðq q j Þ À1 q q i;j ð1ÀÞ ð18Þ</p><p>where ¼ 0:5cosðeÞ þ 0:5, and e ¼ fÀe1 e2Àe1 . Finally, we simply use the synthesized transition motion for edge ði; jÞ at each frame f 2 ½b 2 ; e 1 .</p><p>Transition Period Construction. The periods, B ¼ ½b 1 ; b 2 and E ¼ ½e 1 ; e 2 play a key role in computing the transition motion at edge ði; jÞ. Now, we describe how to obtain B and E. b 2 and e 1 are fixed when the motion transition graph is constructed. e 2 is also given  implicitly since it is the last frame. To obtain b 1 , suppose that the motion transition request from node i to node j occurs at time t Ã while synthesizing a motion at node i. Let f i Ã be the relative frame number at time t Ã . If f i Ã is less than the first keytime K i b1 in the transition period, then the motion transition is scheduled at K i b1 , that is, f i;j Ã ¼ b 1 . Otherwise, we find the corresponding frame number f i;j Ã by aligning the shared keytimes for node i and edge ði; jÞ. Using f i;j Ã , b 1 is given as follows:</p><formula xml:id="formula_18">b 1 ¼ f i;j Ã if f i;j Ã &lt; b 2 b 2 otherwise<label>ð19Þ</label></formula><p>Thus, b ¼ fÀb1 b2Àb1 is allowed to vary adaptively in accordance with the relative frame number b 1 to support the on-line capability. To avoid jerkiness in motion transition, we ignore the motion transition request if b 2 À b 1 is less than a threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motion Retargeting</head><p>Our posture blending scheme takes into account neither the trajectory of a blended motion nor the segment proportions of a target character. However, those should be considered for convincing motion generation. We first show how to adapt a blended posture to the trajectory, and then retarget the resulting posture to the target character.</p><p>Motion Trajectory. Provided with a blended posture, we can adjust its root position and orientation to follow the given trajectory. Let p t ðtÞ and d t ðtÞ be the projected position and tangent vector of the trajectory at time t onto a plane perpendicular to the y-axis. p t ðtÞ and d t ðtÞ are specified by the user in an on-line manner. We compute the target root position p pðtÞ so that the projection onto the plane (floor) is coincident with p t ðtÞ while preserving its elevation:</p><formula xml:id="formula_19">p pðtÞ ¼ pðtÞ þ ðp t ðtÞ À p pðtÞÞ<label>ð20Þ</label></formula><p>where pðtÞ is the blended root position and p pðtÞ is its projection onto the plane. We then consider the blended orientation of the root. The direction dðtÞ for the character to move forward is obtained by blending the tangent vectors of the arcs approximating the projected root trajectories of example motions. We determine the target root orientation q qðtÞ such that the forward direction dðtÞ is coincident with d t ðtÞ. Let be the angular distance between dðtÞ and d t ðtÞ. The rotation by about the unit normal vector n of the plane is represented by a unit quaternion, e n=2 . Therefore, the target root orientation q qðtÞ is q qðtÞ ¼ e n=2 qðtÞ ð 21Þ</p><p>where qðtÞ is the blended root orientation.</p><p>Target Character. Now, we are ready to retarget the resulting motion to the target character. For this purpose, we employ an importance-based approach for online motion retargeting. <ref type="bibr" target="#b14">15</ref> To apply their approach, we have to provide the target stance foot position as input data at each frame. We obtain it by blending the foot positions of example motions. We first represent each of them in the local coordinate frame of the root segment. For example motion i, the foot position f 0 i ðtÞ at time t in the local coordinate frame is</p><formula xml:id="formula_20">f 0 i ðtÞ ¼ q À1 i ðtÞ f i ðtÞ À p i ðtÞ ð Þ q i ðtÞ ð<label>22Þ</label></formula><p>where f i ðtÞ is the foot position of example motion i at time t measured in the global coordinate frame, and p i ðtÞ and q i ðtÞ are the position of the root segment and its orientation, respectively. The target foot position f 0 ðtÞ is obtained by blending f 0 i ðtÞ, 1 i N e with respect to their weights w i :</p><formula xml:id="formula_21">f 0 ðtÞ ¼ X Ne i w i f 0 i ðtÞ ð<label>23Þ</label></formula><p>Given the target root position p pðtÞ and orientation q qðtÞ that define the local coordinate frame of the root segment, the target foot position fðtÞ in the global coordinate frame is</p><formula xml:id="formula_22">fðtÞ ¼ q qðtÞf 0 ðtÞq q À1 ðtÞ þ p pðtÞ ð<label>24Þ</label></formula><p>The target foot position may vary from time to time in accordance with its weight change. Therefore, we force the target foot position to be fixed while the foot contacts the floor. The duration of contact can be easily obtained from the keytimes. When the foot is approaching or contacting the floor, we change the joint angles to keep the foot position fðtÞ. Otherwise, we keep the joint angles to preserve the motion characteristics inherited from the example motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>In this section, we show experimental results to support our motion blending scheme. Experiments were performed on a pentium PC (Intel PentiumIV 2 GHz with 512 MB main memory). Our human model had 43 DOFs (degree of freedoms): 6 DOFs for the foot position and orientation, 3 DOFs for the spine, 7 DOFs for each limb, 3 DOFs for the neck, and 3 DOFs for the head. We have mainly compared our scheme with that of Park et al. <ref type="bibr" target="#b0">1</ref> due to their similarity in the motion blending scheme.</p><p>The first experiment was to show the effectiveness of our timewarping scheme. Example running motions and their parameter values are given in Table <ref type="table">1</ref>. For speed ¼ 7.4 and turning angle ¼ 0.0, Table <ref type="table" target="#tab_1">2</ref> shows the weight values of example motions and the blended change rate Át of the generic time [see equation (9)]. For our scheme, the blended change rate Át is always positive. To make Át negative for Park et al.'s scheme, we intentionally gave a large speed value to cause an extrapolation that forces some weight values to be negative. Figures <ref type="figure">8(a</ref>) and 8(b) show the motion synthesized by Park et al.'s and ours, respectively. The former exhibited motion artifacts, that is, repeating a similar posture for a while unlike the latter generated by ours. Figure <ref type="figure">8</ref>(c) compares the resulting timewarping functions.</p><p>The second experiment was to demonstrate the efficiency of our motion blending scheme, compared to Park et al.'s. With the same input parameter stream, we generated the target motion using each of the schemes. As summarized in Table <ref type="table">3</ref>, both schemes exhibited small differences between successive postures to show inter-frame coherency. However, the performance of our scheme was almost twice as fast as that of Park et al. This performance gain is mainly ascribed to avoiding the time-consuming eigenvector problem. As shown in Figure <ref type="figure">9</ref>, the computation time of our scheme was linearly proportional to the number of example motions. With twenty example motions, it took 0.33 milliseconds to produce a target motion. Thus, our scheme was able to generate more than 3000 frames per second.</p><p>Our third example was to demonstrate smooth motion transition.  As shown in those figures, the characteristics of resulting motions can also be controlled with parameters such as speeds and turning angles while making transitions.</p><p>In the final experiment, we demonstrate the on-line, real-time capability of our motion blending scheme.</p><p>Mouse pointer positions were sampled in an on-line manner so that the target character chased the pointer (Figure <ref type="figure" target="#fig_10">12</ref>). Based on the sampled position at each frame, we computed the speed and turning angle. The type was also specified in an on-line manner with the keyboard. Using our motion retargeting scheme, we were able to adapt a blended motion to the target character and the environment. Thus, the resulting motion did not suffer from artifacts such as foot sliding and penetration as demonstrated in Figure <ref type="figure" target="#fig_11">13</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we provide an integrated framework for on-line motion blending for locomotion generation, by combining the idea of motion blending with that of posture rearrangement. We address three issues: timewarping, posture blending, and motion transition. For each of these issues, we provide an elegant solution. We also provide a comprehensive solution to on-line motion retargeting for motion blending based on existing techniques. The experimental results have demonstrated that the proposed approach can produce convincing motions in real time. We believe that our approach can be extended for motion synthesis in general.</p><p>The major premise of motion blending is the availability of a good motion labeling scheme. At present, motion labeling depends on skilled labor in general. For future research, we are planning to develop an automatic scheme for motion labeling.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ON-LINE MOTION BLENDING</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Motion transition graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Piecewise linear mapping between the actual time T and the generic time t.</figDesc><graphic coords="5,303.48,63.58,216.00,158.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Timewarping and untimewarping.</figDesc><graphic coords="5,63.89,533.28,456.59,118.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Weight function i ðpÞ for timewarping.</figDesc><graphic coords="6,79.94,457.54,384.59,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Example motions in each node and edge.</figDesc><graphic coords="8,44.45,63.58,216.00,121.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Generating a transition motion based on the shared keytime sequences.</figDesc><graphic coords="8,79.94,473.53,384.59,178.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Foot positions in the global and the local coordinate frames: (a) example motion and (b) target motion.</figDesc><graphic coords="9,303.48,63.58,216.00,140.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>exhibit the resulting motions. To follow the trajectory, we exploited the turning angle obtained from the tangent vector of the trajectory at each frame. The color of a curve segment represents the specified type of motion, and motion transition occurs where the color changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Motion transition from a walking motion to a standing motion: (a) Park et al.'s (b) ours.</figDesc><graphic coords="12,79.94,63.58,384.59,247.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Motion following the trajectory: (a) generated motion; (b) running at the beginning; (c) walking in the middle (d) running at the end.</figDesc><graphic coords="12,43.94,374.37,456.59,109.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Locomotion chasing the mouse pointer.</figDesc><graphic coords="13,63.89,63.58,456.59,109.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Motion retargeting.</figDesc><graphic coords="13,63.89,218.98,456.59,107.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,69.90,63.58,443.95,215.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I .</head><label>I</label><figDesc>To construct a motion transition graph Example motions and their parameters for the experiment of timewarping: p 1 ¼speed [m/s] and p 2 ¼ turning angle [ /s]</figDesc><table><row><cell></cell><cell>Run1</cell><cell>Run2</cell><cell>Run3</cell><cell>Run4</cell><cell>Run5</cell><cell cols="2">Run6</cell><cell>Run7</cell><cell>Run8</cell><cell>Run9</cell></row><row><cell>p 1</cell><cell>0.3</cell><cell>0.1</cell><cell>0.3</cell><cell>2.6</cell><cell>2.6</cell><cell>2.6</cell><cell></cell><cell>5.2</cell><cell>5.3</cell><cell>5.3</cell></row><row><cell>p 2</cell><cell>3</cell><cell>À56</cell><cell>À55</cell><cell>6</cell><cell>À91</cell><cell>97</cell><cell></cell><cell>4</cell><cell>À124</cell><cell>129</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Weight values of example motions</cell><cell></cell><cell></cell><cell></cell><cell>Át</cell></row><row><cell></cell><cell>Run1</cell><cell>Run2</cell><cell>Run3</cell><cell>Run4</cell><cell>Run5</cell><cell>Run6</cell><cell>Run7</cell><cell>Run8</cell><cell>Run9</cell><cell></cell></row><row><cell>A</cell><cell>À0.19</cell><cell>À0.18</cell><cell>À0.18</cell><cell>0.11</cell><cell>0.10</cell><cell>0.09</cell><cell>0.45</cell><cell>0.40</cell><cell>0.40</cell><cell>À0.01</cell></row><row><cell>B</cell><cell>0.11</cell><cell>0.11</cell><cell>0.11</cell><cell>0.11</cell><cell>0.11</cell><cell>0.11</cell><cell>0.16</cell><cell>0.11</cell><cell>0.11</cell><cell>0.06</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(speed ¼ 7.4 and turning angle = 0.0)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The weight values of example motions and the blended change rate of the generic time: A ¼Park et al. 's and B ¼ ours</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was supported by the NRL (National Research Laboratory) project of KISTEP (Korea Institute of Science &amp; Technology Evaluation and Planning) and the Brain Korea 21 Program of Korea Ministry of Education &amp; Human Resource Development.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(see Figure <ref type="figure">1</ref>), we used three types of example motions comprising 13 walking motions, 13 running motions, and 1 standing motion. 24 example transition motions were used so that each edge had 4 transition motions attached to it. Figures <ref type="figure">10(a</ref> In the fourth experiment, we demonstrated the pathfollowing capability of our scheme with various motions. Provided with a curved trajectory together with the type and speed of the desired motion at each point along the curve, we generated the motion satisfying these constraints. Figures <ref type="figure">11(a</ref>    </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On-line locomotion generation based on motion blending</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIG-GRAPH Symposium on Computer Animation</title>
		<meeting>ACM SIG-GRAPH Symposium on Computer Animation</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="105" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Verbs and adverbs: mulidimensional motion interpolation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bodenheimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="32" to="40" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shape by example</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sloan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2001 ACM Symposium on Interactive 3D Graphics</title>
		<meeting>2001 ACM Symposium on Interactive 3D Graphics</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="135" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Interpolation synthesis for articulated fixture motion</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Wiley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="39" to="45" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A high-level control mechanism for human locomotion based on parametric frame space interpolation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">´j</forename><surname>Roberge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Eurographics Workshop on Computer Animation and Simulation</title>
		<meeting>Eurographics Workshop on Computer Animation and Simulation</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="95" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rhythmic-motion synthesis based on motion-beat analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH&apos;03</title>
		<meeting>SIGGRAPH&apos;03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="392" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Interactive motion generation from examples</title>
		<author>
			<persName><forename type="first">O</forename><surname>Arikan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH&apos;02</title>
		<meeting>SIGGRAPH&apos;02</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="483" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Motion graphs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kovar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH&apos;02</title>
		<meeting>SIGGRAPH&apos;02</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="473" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interactive control of avatars animated with human motion data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Psa</forename><surname>Reitsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Pollard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIG-GRAPH&apos;02</title>
		<meeting>SIG-GRAPH&apos;02</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="491" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Motion texture: a two-level statistical model for character motion synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH&apos;02</title>
		<meeting>SIGGRAPH&apos;02</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="465" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Motion capture assisted animation: texturing and synthesis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pullen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH&apos;02</title>
		<meeting>SIGGRAPH&apos;02</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="501" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Motion synthesis from annotations</title>
		<author>
			<persName><forename type="first">O</forename><surname>Arikan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH&apos;03</title>
		<meeting>SIGGRAPH&apos;03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="402" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Online motion retargetting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Visualization and Computer Animation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="223" to="235" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Comparing constraint-based motion editing methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Graphical Models</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="107" to="134" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Computer puppetry: an importance-based approach</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions On Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="67" to="94" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Computer puppetry</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sturman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Application</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="45" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Retargeting motion to new characters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH&apos;98</title>
		<meeting>SIGGRAPH&apos;98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A hierarchical approach to interactive motion editing for human-like figures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIG-GRAPH&apos;99</title>
		<meeting>SIG-GRAPH&apos;99</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A general construction scheme for unit quaternion curves with simple high order derivatives</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH&apos;95</title>
		<meeting>SIGGRAPH&apos;95</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">General construction of time-domain filters for orientation data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="128" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Animating rotations using quaternion curves</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shoemake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH&apos;85</title>
		<meeting>SIGGRAPH&apos;85</meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="245" to="254" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
