<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What Language Model to Train if You Have One Million GPU Hours? The BigScience Architecture &amp; Scaling Group</title>
				<funder>
					<orgName type="full">Grand ?quipement national de calcul intensif (GENCI)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-27">27 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lucile</forename><surname>Saulnier</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stas</forename><surname>Bekman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Saiful Bari</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ofir</forename><surname>Press</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Washington</orgName>
								<address>
									<addrLine>9 Berkeley University 10 Big Science</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jaesung</forename><surname>Tae</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><forename type="middle">Xin</forename><surname>Yong</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julien</forename><surname>Launay</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hugging</forename><surname>Face</surname></persName>
						</author>
						<author>
							<persName><surname>Lighton</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Singapore</forename><surname>Ntu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Booz</forename><surname>Allen</surname></persName>
						</author>
						<author>
							<persName><surname>Eleutherai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Naver</forename><surname>Labs</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">What Language Model to Train if You Have One Million GPU Hours? The BigScience Architecture &amp; Scaling Group</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-27">27 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.15424v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The crystallization of modeling methods around the Transformer architecture has been a boon for practitioners. Simple, well-motivated architectural variations can transfer across tasks and scale, increasing the impact of modeling research. However, with the emergence of state-of-the-art 100B+ parameters models, large language models are increasingly expensive to accurately design and train. Notably, it can be difficult to evaluate how modeling decisions may impact emergent capabilities, given that these capabilities arise mainly from sheer scale alone. In the process of building BLOOM-the Big Science Large Open-science Open-access Multilingual language model-our goal is to identify an architecture and training setup that makes the best use of our 1,000,000 A100-GPU-hours budget. Specifically, we perform an ablation study at the billion-parameter scale comparing different modeling practices and their impact on zero-shot generalization. In addition, we study the impact of various popular pretraining corpora on zero-shot generalization. We also study the performance of a multilingual model and how it compares to the Englishonly one. Finally, we consider the scaling behaviour of Transformers to choose the target model size, shape, and training setup. All our models and code are open-sourced at https: //huggingface.co/bigscience.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have seen the advent of large language models characterized by emergent capabilities (e.g., zero-shot generalization) arising from sheer scale alone <ref type="bibr" target="#b52">(Radford et al., 2019;</ref><ref type="bibr" target="#b9">Brown et al., 2020)</ref>. Scaling LLMs results in a predictable increase in performance: simple scaling laws connect the number of parameters, pretraining dataset size, and compute budget <ref type="bibr" target="#b31">(Kaplan et al., 2020;</ref><ref type="bibr" target="#b19">Ganguli et al.</ref>, * Equal contribution.</p><p>? Equal supervision. Figure <ref type="figure">1</ref>: Smooth scaling of language modeling loss as compute budget and model size increase. We observe a power-law coefficient ? C ? 0.046, in-line with <ref type="bibr" target="#b31">Kaplan et al. (2020)</ref>. We use this fit to estimate the optimal size and number of tokens to train on for the final model given the available budget.</p><p>2022 <ref type="bibr">;</ref><ref type="bibr" target="#b25">Hoffmann et al., 2022)</ref>, providing a clear path towards more capable models. This paradigm shift has been fueled by the wide adoption of the Transformer <ref type="bibr" target="#b66">(Vaswani et al., 2017)</ref>, providing a scalable basis for practitioners to build upon.</p><p>In this paper, we design an architecture and training setup for a multilingual 100B+ parameters model (BLOOM, BigScience <ref type="bibr" target="#b6">Workshop (2022)</ref>), seeking to best use a fixed 1,000,000 A100-hours budget. Because of the costs involved with training large language models, we cannot exhaustively explore the landscape of possible models. Instead, we position ourselves as practitioners exploring "offthe-shelf" solutions. We thus test promising additions to the Transformer to attempt to reproduce their findings in a controlled, large-scale setting.</p><p>Although our main goal was to prepare the architecture and training setup of BLOOM, our findings are also valuable for practitioners building models in the 1-10B range, as they equally improve the performance of such smaller models. At variance with major works on large language models, we also make a significant effort towards reproducibility and openness: all of our pretrained models, code, and notes from our weekly meetings are made available. See Appendix A for the relevant links.</p><p>Contributions. We first study the impact of pretraining corpora, positional embeddings, activation functions, and embedding norm on zero-shot generalization. We base our study on the popular GPT-2 architecture <ref type="bibr" target="#b52">(Radford et al., 2019)</ref>, with experiments at the 1.3B parameters scale. We then consider the impact of massive multilinguality, showing language-specific scaling laws in a multilingual setting for the first time. Finally, we describe our approach to drafting an architecture for the final 176B parameters BLOOM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>We first justify our choice to base our model on the popular recipe of combining a decoder-only model with an autoregressive language modeling objective, and introduce our experimental setup. We then discuss our evaluation benchmarks, and motivate our choice of zero-shot generalization as our key metric. Finally, we introduce the baselines we compare to throughout the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture and Pretraining Objective</head><p>In this paper, we base all models on a decoder-only Transformer pretrained with an autoregressive language modeling objective. This is a popular choice for large language models <ref type="bibr" target="#b9">(Brown et al., 2020;</ref><ref type="bibr">Rae et al., 2021;</ref><ref type="bibr" target="#b65">Thoppilan et al., 2022)</ref>, possibly because it lends itself to zero-shot application to many downstream tasks <ref type="bibr" target="#b52">(Radford et al., 2019)</ref>. Alternatives include encoder-decoder models trained with a span-corruption objective (e.g., <ref type="bibr">T5 Raffel et al. (2019)</ref>), as well as non-causal decoders models with visibility over a prefix (so-called Prefix LMs, <ref type="bibr" target="#b44">Liu et al. (2018)</ref>; <ref type="bibr" target="#b18">Dong et al. (2019)</ref>).</p><p>Our decision is motivated by the findings of <ref type="bibr">Wang et al. (2022)</ref>, which showed that decoderonly models combined with an autoregressive language modeling objective provide the best zeroshot generalization abilities immediately after pretraining. Although multitask finetuning <ref type="bibr" target="#b58">(Sanh et al., 2021;</ref><ref type="bibr" target="#b76">Wei et al., 2021)</ref> will instead favor an encoder-decoder with span corruption for best zero-shot generalization, <ref type="bibr">Wang et al. (2022)</ref> found a compromise between these two practices. Following autoregressive pretraining, decoder-only models can be efficiently adapted into non-causal decoders, simply by extending pretraining with span corruption. This adaptation produces a second model, which can provide excellent zero-shot generalization after multitask finetuning. Accordingly, we follow their recommendation, and train an autoregressive decoder-only model first which we will later consider adapting and finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Experimental Setup</head><p>We follow the architectures GPT-2 <ref type="bibr" target="#b52">(Radford et al., 2019)</ref> and the hyperparameters of GPT-3 <ref type="bibr" target="#b9">(Brown et al., 2020)</ref>. For learning rate, we use a maximum value of 2 ? 10 -4 , with a linear warm-up over 375M tokens, followed by cosine decay to a minimum value of 1 ? 10 -5 . We use a 1M tokens batch size, with linear ramp-up over the first 4B tokens, and a sequence length of 2,048. We use the Adam optimizer (Kingma and Ba, 2014), with ? 1 = 0.9, ? 2 = 0.999, = 1 ? 10 -8 , weight decay 0.1, and gradient clipping to 1.0. We also tie the word embedding and softmax matrix <ref type="bibr">(Press and Wolf, 2017)</ref>. Unless noted otherwise, we conduct our experiments with 1.3B parameters models, pretraining on 112B tokens.</p><p>We picked this size and dataset size as a compromise between compute cost and the likelihood that our conclusions would transfer to the target 100B+ model. Notably, we needed to be able to reliably measure zero-shot generalization above random chance. We note that training for 112B tokens 1.3B parameters models bring them significantly above the optimality threshold of <ref type="bibr" target="#b31">Kaplan et al. (2020), and</ref><ref type="bibr" target="#b25">of Hoffmann et al. (2022)</ref>.</p><p>The main architectural difference with GPT-3 is that all our layers use full attention, while GPT-3 uses alternating sparse attention layers <ref type="bibr" target="#b10">(Child et al., 2019)</ref>. The main value of sparse attention layers is to save compute with long sequence lengths. However, at the 100B+ scale, sparse attention layers provide negligible compute savings, as the vast majority of the compute is spent on the large feedforward layers. <ref type="bibr" target="#b31">Kaplan et al. (2020)</ref> estimated the amount of compute per token to be:</p><formula xml:id="formula_0">C forward = 2 ? (12n layer d 2 + n layer n ctx d),</formula><p>where C forward is the cost for the forward pass, n layer is the number of layers, d is the hidden dimension, and n ctx is the sequence length. This means if 12d &gt;&gt; n ctx , the second n layer n ctx d term is negligible, which is the case for our final model where d &gt; 10, 000 and n ctx = 2048. What is a FLOP exactly? We report throughput per GPU in FLOPS and total budgets in PF-days (i.e. one PFLOPS sustained for a day). It is important to highlight that FLOPS are never directly measured, but always estimated, with widely different practices across papers. We refer to model FLOP the estimates based on the C = 6N D formula from <ref type="bibr" target="#b31">Kaplan et al. (2020)</ref>, where C is the total compute, N the model size, and D the number of tokens processed. These are the FLOP actually used to train the model, and which are used for scaling laws. We refer to hardware FLOP the estimates reported by our codebase, using the formula from <ref type="bibr" target="#b46">Narayanan et al. (2021)</ref>. This notably includes gradient checkpointing, which trades additionnal computations for reduced memory needs, and a more thorough accounting of operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation Benchmarks</head><p>We measure upstream performance using the language modeling loss on an held out sample of the pretraining dataset. However, it is not always possible to compare losses across objectives and tokenizers. Moreover, as upstream performance is not always aligned with task performance <ref type="bibr" target="#b64">(Tay et al., 2021)</ref>, we must also measure downstream performance explicitly. We could use zero/few-shot generalization, with or without specific finetuning. Specifically, we choose to measure zero-shot generalization on a diverse set of tasks. Few-shot and zero-shot results are strongly correlated: we found a Pearson correlation coefficient of 0.93 between zero-shot and few-shot performance across model sizes in <ref type="bibr" target="#b9">Brown et al. (2020)</ref>. We do not rely on finetuning as it is not how the main final model is likely to be used, given its size and the challenges associated with finetuning at the 100B+ scale.</p><p>We use the popular EleutherAI Language Model Evaluation Harness (EAI harness, <ref type="bibr" target="#b22">Gao et al. (2021)</ref>), evaluating models across 27 diverse tasks that are similar to those used in <ref type="bibr" target="#b9">Brown et al. (2020)</ref> (see Appendix C for a list of tasks). Overall, the random baseline on our benchmark sits at 33.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Baselines</head><p>We use GPT-Neo <ref type="bibr" target="#b8">(Black et al., 2021)</ref>, a 1.3B decoder-only autoregressive language model trained on the Pile <ref type="bibr" target="#b21">(Gao et al., 2020)</ref>, and GPT-3 <ref type="bibr" target="#b9">(Brown et al., 2020)</ref>, accessed via the OpenAI API. We evaluate two models, Babbage and Curie 1 . Based on <ref type="bibr" target="#b20">Gao (2021)</ref> and our own analysis, we assume Babbage is 1.3B while Curie is 6.7B based on how close our computed results are to those reported in the original paper. However, as details of the OpenAI API are kept secret, there is no way to make sure that the models are actually the ones described in <ref type="bibr" target="#b9">Brown et al. (2020)</ref> -the number of pretraining tokens reported in Table <ref type="table" target="#tab_0">1</ref> is thus to be taken cautiously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Impact of Pretraining Data</head><p>We first study the impact of pretraining data on zero-shot generalization. More diverse pretraining data, ideally curated from a cross-domain collection of high-quality datasets, has been suggested to help with downstream task performance and zeroshot generalization <ref type="bibr" target="#b57">(Rosset, 2020;</ref><ref type="bibr" target="#b21">Gao et al., 2020)</ref>. 1 These models are now referred to as text-babbage-001 and text-curie-001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Corpora</head><p>We evaluate three possible corpora, all commonly used to train large language models:</p><p>? OSCAR v1 (Ortiz Su?rez et al., 2019)<ref type="foot" target="#foot_0">2</ref> , a multilingual, filtered version of Common Crawl;</p><p>? C4 <ref type="bibr" target="#b54">(Raffel et al., 2019)</ref>, specifically its replication by AllenAI, a processed and filtered version of Common Crawl;</p><p>? The Pile <ref type="bibr" target="#b21">(Gao et al., 2020)</ref>, a diverse pretraining corpus that contains webscrapes from Common Crawl in addition to high-quality data from cross-domain sources such as academic texts and source code.</p><p>For each pretraining corpus, we train a 1.3B parameter model for 112B tokens. For the Pile specifically, motivated by good early results at 112B tokens, we train up to 300B tokens, to compare with GPT-3 models and validate against GPT-Neo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Evaluation results are outlined in Table <ref type="table" target="#tab_0">1</ref>. We find that training on the Pile produces models that are better at zero-shot generalization, with C4 a close second, and OSCAR significantly behind.</p><p>Importantly, this finding transfers to larger scales: as part of engineering test runs, a 13B model was trained on OSCAR for 300B tokens. We found this 13B model to underperform the 6.7B model from OpenAI API which we attribute to the low quality of the English data in OSCAR.</p><p>We also note that our model trained on The Pile outperforms the 1.3B GPT-Neo trained on the same dataset. Finally, our 1.3B model still underperforms the 1.3B model from the OpenAI API by 1.6%. It seems most likely that the difference is that of data, but we cannot investigate this further as the GPT-3 training dataset is neither publicly available nor reproducible.</p><p>Finding 1. Diverse cross-domain pretraining data combining web crawls with curated highquality sources improves zero-shot generalization over pretraining datasets constructed from Common Crawl only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Architecture Ablations</head><p>We now consider ablation studies to better identify the best positional embedding, activation function, and embedding normalization placement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Positional Embeddings</head><p>Background Originally, both static sinusoidal position embeddings and learned position embeddings were proposed to capture positionnal information; the latter are popular in large language models <ref type="bibr" target="#b9">(Brown et al., 2020)</ref>. <ref type="bibr" target="#b62">Su et al. (2021)</ref> proposed rotary embeddings, where the query and key representations inside the self-attention mechanism are modified such that the attention captures relative distances between them. Recently, <ref type="bibr" target="#b50">Press et al. (2022)</ref> introduced a method which does not use embeddings, instead directly attenuating the attention scores based on how far away the keys/queries are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We compare learned, rotary, and ALiBi position embeddings, and include a baseline without position embeddings. Our results are presented in Table <ref type="table">2</ref>. Although learned positional embeddings outperform rotary embeddings, ALiBi yields significantly better results than all alternatives. We also confirm the findings of Biderman (2021): a baseline with no positional information exhibits competitive performance. While bidirectional models require positional embeddings to determine the location of tokens, we find autoregressive models can simply leverage the causal attention mask. We also confirm the ability of ALiBi to extrapolate to longer sequences than trained on in Figure <ref type="figure" target="#fig_1">2</ref>. Note that results in Table <ref type="table">2</ref>   zero-shot generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Activation Functions</head><p>Background. Large language models by and large still mostly use the GELU activation <ref type="bibr" target="#b24">(Hendrycks and Gimpel, 2016)</ref>. We evaluate a recently proposed alternative, SwiGLU <ref type="bibr" target="#b59">(Shazeer, 2020)</ref>, which combines both Gated Linear Units <ref type="bibr" target="#b15">(Dauphin et al., 2016)</ref> with the Swish activation function <ref type="bibr" target="#b56">(Ramachandran et al., 2017)</ref>. SwiGLU uses 50% extra parameters in the feedforward layers. As suggested in Shazeer (2020), we compensate for this by reducing the hidden size of the feed-forward layer.</p><p>Results. We present our results in Table <ref type="table" target="#tab_2">3</ref>. SwiGLU produces slightly better results than GELU. For our final model, we adopted GELU, as we initially observed a lower throughput for SwiGLU. However, further benchmarking identified that this overhead was primarily associated with the change in the hidden size of the feedforward network. Indeed, this new size, 5,456, is divisible by neither the warp size of the GPU <ref type="bibr" target="#b36">(Lashgar et al., 2013)</ref> nor the number of streaming multiprocessors, resulting in both tile and wave quantiza-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activation function Average EAI Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GELU</head><p>42.79 SwiGLU 42.95 tion. We accordingly recommend using SwiGLU for future models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Embedding Norm</head><p>Dettmers et al. ( <ref type="formula">2021</ref>) suggests that greater stability of training can be achieved by including an extra layer normalization <ref type="bibr" target="#b2">(Ba et al., 2016)</ref> after the embedding layer. We evaluate the performance impact of such a modification in Table <ref type="table" target="#tab_3">4</ref>. We note that this incurs a significant reduction in the performance of the model. However, models above 100 billion parameters are notoriously unstable and require considerable engineering efforts in order to be kept stable. If this addition provides increased stability when training, it may be valuable.</p><p>Finding 3. Adding layer normalization after the embedding layer incurs a significant penalty on zero-shot generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Multilinguality</head><p>The majority of 100B+ language models have been trained in English, with notable exceptions in Chinese <ref type="bibr" target="#b76">(Zeng et al., 2021;</ref><ref type="bibr" target="#b70">Wu et al., 2021)</ref> and Korean <ref type="bibr" target="#b33">(Kim et al., 2021)</ref> models. Smaller massively multilingual models have seen wider adoption <ref type="bibr">(Xue et al., 2020)</ref>, but these models are not suitable for zero-shot. Recent results on large GPT-like multilingual models show that English-only performance is usually disappointing <ref type="bibr" target="#b42">(Lin et al., 2021)</ref>.</p><p>Training data. We train a multilingual model to evaluate the effectiveness and potential impacts of this practice. We use the OSCAR  <ref type="bibr">(2021)</ref>. First row is the reported XGLM results, and the second is our reproduction of their results to validate our multilingual evaluation setup. Last two rows show that our multilingual model matches the XGLM results.</p><p>the least frequent ones, so that all languages are represented. We estimate the sampling probabilities similar to <ref type="bibr">Xue et al. (2021)</ref>.</p><p>English-only evaluation. We first evaluate our multilingual model on the same set of English benchmarks we have used previously, in Table <ref type="table">6</ref>. Multilinguality significantly lowers accuracy on the English benchmark, which is in line with the results from Lin et al. ( <ref type="formula">2021</ref>).</p><p>Multilingual evaluation. Zero-shot multilingual evaluation is more challenging to setup because it requires writing new prompts for each new language. Therefore, instead of manually writing prompts for each language, we follow the strategy proposed by Lin et al. ( <ref type="formula">2021</ref>), using English prompts for non-English examples-this can be viewed as cross-lingual zero-shot generalization. They validated this strategy by demonstrating its ability to achieve zero-shot performance on par with (and sometimes even better than) humanwritten language-specific prompts. This strategy also demonstrates cross-lingual abilities. We evaluate on XNLI <ref type="bibr" target="#b13">(Conneau et al., 2018)</ref>, a multilingual NLI dataset that covers 8 of the languages we use for training. Our evaluation is different from the zero-shot evaluation of the XTREME benchmark <ref type="bibr" target="#b26">(Hu et al., 2020)</ref>. XTREME first finetunes the model on the English training data of each downstream task, then evaluates it on the non-English dataset, attempting cross-lingual general- ization. Our evaluation avoids any finetuning, and instead relies entirely on zero-shot generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretraining Average EAI Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English</head><p>Results. Language-specific scaling laws. To explore how scale influences multilinguality, we train a wider range of models (i.e. 0.3-6B parameters) on a larger corpus of more than 300B tokens of text drawn from a variety of languages <ref type="bibr" target="#b37">(Lauren?on et al., 2022)</ref>. In Figure <ref type="figure" target="#fig_2">3</ref>, we show scaling laws for Arabic, Catalan, Code, English, Spanish, Basque, French, Indonesian, Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Nepali, Odia, Punjabi, Tamil, Telugu, Urdu, aggregated Niger-Congo languages, Portuguese, Vietnamese, Simplified and Traditional Chinese. Smaller models struggle more with underrepresented languages such as those in the Indic and Niger-Congo family. For example, the loss of the sub-1 billion models goes up at the end of training for Malayalam, Odia, and Telugu. As data is not repeated, it is unlikely that this effect is due to overfitting; we interpret this as insufficient capacity in the model to handle many language representations, with data in the dominant language sets causing catastrophic forgetting of less represented languages. In contrast, the largest model sees its loss decrease smoothly for every language: larger models handle multilinguality more easily. Overall, scaling laws coefficients are consistent across wellrepresented languages, only differing in offsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Scaling to 176B parameters</head><p>We now detail how our previous findings influence our architecture and scaling decisions for the final 176B BLOOM model.</p><p>Compute allocation. We have been allocated 18 weeks of dedicated use of partition with 52 nodes of 8x 80GB A100 GPUs on the Jean Zay supercomputer. We set four nodes aside as spare, so that our compute budget amounts to 1,161,216 A100-hours in total. Assuming a throughput of 100 model TFLOPS, approximately corresponding to state-ofthe-art hardware FLOPS of 150 <ref type="bibr" target="#b46">(Narayanan et al., 2021)</ref>, we have a compute budget of 4,838 PF-days for the model training. We round this down to 4,500 PF-days, this ? 10% safety margin accounting for potential downtime and inefficiencies (e.g., batch size ramp-up) during training. To put this number in perspective, this is ? 23% more than the training budget of GPT-3. Given this compute budget, our English-only scaling laws in 1 predict an optimal allocation for training a 392B parameter model for 165B tokens. We will use these as an upper bound in size: the largest model we can afford is 392B parameters, and the minimum number of tokens to train on is 165B tokens.</p><p>Model shape. <ref type="bibr" target="#b31">Kaplan et al. (2020)</ref> studied the dependence of the loss with model shape, and found only a limited impact within a wide range of feed-forward ratios d f f /d model , aspect ratios d model /n layer , and attention head dimensions. <ref type="bibr" target="#b40">Levine et al. (2020)</ref> proposed a theoretically motivated and empirically backed law describing the optimal compromise between width and depth. They predict that 100B+ parameters models such as GPT-3 are too deep, while models in the 10B or smaller range are usually too shallow. For a GPT-3-sized model with 175B parameters, they predict an ideal depth of 80 layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Final Model Architecture</head><p>We set three main guidelines for our final model:</p><p>? 300-400B tokens. We want to guarantee our model will train on around 300-400B tokens of data. This is in the upper range for models in the size range we are pursuing, ensuring that lowresource languages will not be allocated too few tokens. Using the C = 6N D approximation <ref type="bibr" target="#b31">(Kaplan et al., 2020)</ref>, with C = 4, 500 PF-days and D = 300-400B tokens, this constrains the model size to be around 160-200B parameters.</p><p>? 70-80 layers. From <ref type="bibr" target="#b40">Levine et al. (2020)</ref> and the size constraint above, we estimate that our model should have between 70 and 80 layers.</p><p>? Maximum throughput. Finally, we want the final architecture to have as high of a throughput per GPU as possible, as more compute will translate directly into longer pretraining and thus a better model. Engineering constraints also come into light here: wide shallow models are typically easier to parallelize across nodes, up to a point where excessive tensor paralellism becomes necessary due to memory constraints.</p><p>We detail in Table <ref type="table">7</ref> the architectures of current state-of-the-art 100B+ models. From these guidelines, we benchmark 20 model configurations, detailed in Appendix D. Among these configurations, we select three of particular interest, outlined in Table 8. They best fit our guidelines above, and offer high throughput, maximizing our training budget.</p><p>We discard configuration (1), as its attention heads are much larger than other models in the literature. Configuration (3) is shallower than recommended by <ref type="bibr" target="#b40">Levine et al. (2020)</ref>, but delivers 3% higher throughput compared to (2). Thus, we choose configuration (3) and its better throughput, and because a shallower model is easier to deal with at inference time by introducing less latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations</head><p>Optimal scaling. Concurrent to this work, Hoffmann et al. ( <ref type="formula">2022</ref>) identified more optimal scaling laws. For our compute budget, they would suggest a 50B parameters model trained for a trillion tokens. Interestingly, even in hindsight, it would have been difficult to follow this recommendation as we would have been limited by the limited availability of high-quality multilingual data and by the size of the BigScience training dataset, ROOTS <ref type="bibr" target="#b37">(Lauren?on et al., 2022)</ref>. Note that our Figure <ref type="figure">1</ref> reproduces <ref type="bibr" target="#b31">Kaplan et al. (2020)</ref> as we did not account for the learning rate schedule as suggested by <ref type="bibr" target="#b25">Hoffmann et al. (2022)</ref>.</p><p>Other hyperparameters. In this work we have focused on a subset of the available hyperparameter space of large language models. We have in- vestigated architecture decisions around positional embeddings, activation functions and the embedding norm. Alternative attention mechanisms <ref type="bibr" target="#b63">(Tay et al., 2020)</ref> or optimizers are examples of other dimensions that could be investigated, potentially leading to improved models.</p><p>Efficient fine-tuning. Our study is focused on zero-shot use and does not consider efficient finetuning <ref type="bibr" target="#b38">(Lester et al., 2021;</ref><ref type="bibr" target="#b73">Zaken et al., 2021)</ref>, which is quite relevant for large language models, and which may lead to different conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Seeking to establish the best possible model architecture that can be accommodated within a fixed 1,000,000 GPU-hours compute budget, we have presented an extensive study on principled modeling decisions for large language models. First, we have found that complimenting Common Crawl data with high-quality cross-domain curated data can boost zero-shot generalization, validating previous suggestions <ref type="bibr" target="#b57">(Rosset, 2020;</ref><ref type="bibr" target="#b21">Gao et al., 2020)</ref>. Through an ablation study, we have identified ALiBi as the position embedding of choice, confirmed the potential of SwiGLU, and highlighted that stabilizing techniques such as embedding normalization sometimes come at the expense of zero-shot generalization. Exploring multilinguality, we have found that multilingual models significantly underperform their monolingual counterparts on English zero-shot benchmarks, but that they can learn under-resourced languages along with larger ones if given enough scale. Finally, we identified a candidate architecture for BLOOM 176B, outlining the full reasoning behind every architectural parameter, including model shape.</p><p>At variance with previous 100B+ models, such as GPT-3 <ref type="bibr" target="#b9">(Brown et al., 2020)</ref> or Gopher <ref type="bibr">(Rae et al., 2021)</ref>, this project was conducted in the open, and resulted in a number of open-access artefacts. Notable similar projects conducted in parallel to this one include OPT <ref type="bibr" target="#b77">(Zhang et al., 2022)</ref> and GLM <ref type="bibr" target="#b75">(Zeng et al., 2022)</ref>, although they lacked the collaborative and massively multilingual components of this project.</p><p>We hope our work can help practitioners better understand modeling decisions, leading to better language models, and that this transparency will accelerate future similar work.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: ALiBi embeddings can effectively extrapolate past the sequence length on which the model was trained, while rotary embeddings can not. This is in line with the findings of Press et al. (2022).</figDesc><graphic url="image-1.png" coords="5,70.87,70.87,218.27,148.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Scaling laws across languages for the smaller BLOOM models. Black line is Pareto frontier of optimality (best loss at a given compute), dashed line is best fit. Fit coefficients are detailed in Appendix B. All sufficiently represented languages exhibit similar scaling behaviour, with mostly differences in loss offsets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Pretraining datasets with diverse cross-domain high-quality data improves zero-shot generalization. Average accuracy on EAI harness (higher is better) using different pretraining corpora and comparison with baseline models. Bold is best 1.3B model for amount of tokens seen, underline is best overall.</figDesc><table><row><cell></cell><cell>Parameters</cell><cell cols="2">Pretraining tokens</cell></row><row><cell></cell><cell></cell><cell>Dataset</cell><cell>112B 250B 300B</cell></row><row><cell>OpenAI -Curie</cell><cell>6.7B</cell><cell></cell><cell>49.28</cell></row><row><cell>OpenAI -Babbage</cell><cell>1.3B</cell><cell></cell><cell>45.30</cell></row><row><cell>EleutherAI -GPT-Neo</cell><cell>1.3B</cell><cell>The Pile</cell><cell>42.94</cell></row><row><cell>Ours</cell><cell>13B</cell><cell>OSCAR v1</cell><cell>47.09</cell></row><row><cell></cell><cell>1.3B</cell><cell>The Pile</cell><cell>42.79 43.12 43.46</cell></row><row><cell>Ours</cell><cell>1.3B</cell><cell>C4</cell><cell>42.77</cell></row><row><cell></cell><cell>1.3B</cell><cell cols="2">OSCAR v1 41.72</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>do not use any extrapolation: ALiBi embeddings are a better choice even without taking into account their ability to extrapolate.</figDesc><table><row><cell cols="2">Finding 2. ALiBi positional embeddings sig-</cell></row><row><cell cols="2">nificantly outperforms other embeddings for</cell></row><row><cell cols="2">Positional Embedding Average EAI Results</cell></row><row><cell>None</cell><cell>41.23</cell></row><row><cell>Learned</cell><cell>41.71</cell></row><row><cell>Rotary</cell><cell>41.46</cell></row><row><cell>ALiBi</cell><cell>43.70</cell></row></table><note><p>Table 2: ALiBi significantly outperforms other embeddings for zero-shot generalization. All models are trained on the OSCAR dataset for 112 billion tokens.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>SwiGLU</figDesc><table /><note><p>slightly outperforms GELU for zero-shot generalization. Models trained on The Pile for 112 billion tokens.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Layer normalization after the embedding layer diminishes performance significantly. Models trained on The Pile for 300 billion tokens.</figDesc><table><row><cell>dataset (Or-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Our multilingual 1.3B model achieves accuracy on zero-shot XNLI in line with XGLM Lin et al.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>shows the XNLI results of</cell></row><row><cell>our multilingual model and how it compares to</cell></row><row><cell>XGLM (Lin et al., 2021). We were able to repro-</cell></row><row><cell>duce the results of XGLM-7.5B which validates</cell></row><row><cell>our evaluation setup. Furthermore, the table shows</cell></row><row><cell>that the performance of our 1.3B is in line with the</cell></row><row><cell>XNLI 1.7B model, validating that our multilingual</cell></row><row><cell>setup achieves competitive results. It is worth not-</cell></row><row><cell>ing that our 1.3B model is trained on only 112B</cell></row><row><cell>tokens from 13 languages while XGLM is trained</cell></row><row><cell>on 500B tokens from 30 languages. As far as we</cell></row><row><cell>are aware, this is the first independent replication</cell></row><row><cell>of the main results of Lin et al. (2021).</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The recent release of OSCAR v2 is a better dataset, but it wasn't available when we started this project.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was granted access to the <rs type="institution">HPC resources of Institut du d?veloppement et des ressources en informatique scientifique (IDRIS) du Centre national de la recherche scientifique (CNRS)</rs> under the allocation 2021-A0101012475 made by <rs type="funder">Grand ?quipement national de calcul intensif (GENCI)</rs>.</p><p>In particular, all the trainings ran on the Jean-Zay cluster of IDRIS, and we want to thank the <rs type="institution">IDRIS team</rs> for responsive support throughout the project, in particular <rs type="person">R?mi Lacroix</rs>. Evaluations of GPT-3 models were provided in part by the <rs type="institution">Allen Institute for Artificial Intelligence</rs>. We thank <rs type="person">Leo Gao</rs> for his expertise and advice on language model evaluation.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Open artefacts: models, code, and logs</p><p>We make public all artefacts produced as part of this work:</p><p>? Models. All trained models are centralized at https://huggingface.co/bigscience;</p><p>? Code. All code is available at https://github.com/bigscience-workshop/ Megatron-DeepSpeed/tree/main/megatron;</p><p>? Discussions and logbook.</p><p>The notes from the weekly meetings of our working group are made available at https://docs.google.com/document/d/ 1qbIkhd6bvbOsJOWXL7SfKQ0jey3MWQYQb_SshqH1LII/.   <ref type="bibr" target="#b27">(Iyer et al., 2017)</ref> Paraphrase Identification 50.0 HellaSwag <ref type="bibr" target="#b74">(Zellers et al., 2019)</ref> Sentence Completion 25.0 LAMBADA <ref type="bibr" target="#b48">(Paperno et al., 2016)</ref> Sentence Completion 0.0 LogiQA <ref type="bibr" target="#b43">(Liu et al., 2020)</ref> Multiple-Choice Question Answering 25.0 MathQA <ref type="bibr" target="#b0">(Amini et al., 2019)</ref> Multiple-Choice Question Answering 20.1 MC-TACO <ref type="bibr" target="#b3">(Ben Zhou and Roth, 2019)</ref> Multiple-Choice Question Answering 36.2 OpenBookQA <ref type="bibr" target="#b45">(Mihaylov et al., 2018)</ref> Multiple-Choice Question Answering 25.0 PIQA <ref type="bibr" target="#b7">(Bisk et al., 2020)</ref> Multiple-Choice Question Answering 50.0 PROST <ref type="bibr" target="#b1">(Aroca-Ouellette et al., 2021)</ref> Multiple-Choice Question Answering 25.0 PudMedQA <ref type="bibr" target="#b28">(Jin et al., 2019)</ref> Multiple-Choice Question Answering 33.3 QNLI <ref type="bibr" target="#b55">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b67">Wang et al., 2019)</ref> Sentence Completion 50.0 Race <ref type="bibr" target="#b35">(Lai et al., 2017)</ref> Closed-Book Question Answering 25.0 SciQ <ref type="bibr" target="#b29">(Johannes Welbl, 2017)</ref> Multiple-Choice Question Answering 25.0 SST <ref type="bibr" target="#b61">(Socher et al., 2013)</ref> Sentiment 50.0 SuperGLUE Boolq <ref type="bibr" target="#b11">(Clark et al., 2019)</ref> Multiple-Choice Question Answering 50.0 COPA <ref type="bibr" target="#b23">(Gordon et al., 2012)</ref> Sentence Completion 50.0 MultiRC <ref type="bibr" target="#b32">(Khashabi et al., 2018)</ref> Multiple-Choice Question Answering 5.8 RTE <ref type="bibr" target="#b14">(Dagan et al., 2005)</ref> Natural Language Inference 50.0 WIC (Pilehvar and os'e <ref type="bibr" target="#b49">Camacho-Collados, 2018)</ref> Word Sense Disambiguation 50.0 WSC <ref type="bibr" target="#b39">(Levesque et al., 2012)</ref> Word Sense Disambiguation 50.0 TriviaQA <ref type="bibr" target="#b30">(Joshi et al., 2017)</ref> Closed-Book Question Answering 0.0 WebQuestions <ref type="bibr" target="#b4">(Berant et al., 2013)</ref> Closed-Book Question Answering 0.0 Winogrande <ref type="bibr" target="#b57">(Sakaguchi et al., 2019)</ref> Coreference resolution 50.0 WNLI <ref type="bibr" target="#b57">(Sakaguchi et al., 2019)</ref> Natural Language Inference 50.0 EAI harness 33.3  Table <ref type="table">12</ref>: Summary of all results obtained in this study. The final three columns indicate the average EAI Harness results at across different billion tokens trained. Some rows are duplicated for ease of reading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Multilingual scaling laws</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">MathQA: Towards interpretable math word problem solving with operation-based formalisms</title>
		<author>
			<persName><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanchuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1245</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2357" to="2367" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PROST: Physical reasoning about objects through space and time</title>
		<author>
			<persName><forename type="first">St?phane</forename><surname>Aroca-Ouellette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cory</forename><surname>Paik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Roncone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Kann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4597" to="4608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">going on a vacation&quot; takes longer than &quot;going for a walk&quot;: A study of temporal commonsense understanding</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">You: Gee stella, #eleutherai sure hypes rotary embeddings a lot. are you sure that they</title>
		<author>
			<persName><forename type="first">]</forename><surname>Blancheminerva</surname></persName>
		</author>
		<author>
			<persName><surname>Stella Biderman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>re that good? me:. Twitter</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Bigscience</forename><surname>Workshop</surname></persName>
		</author>
		<idno type="DOI">10.57967/hf/0003</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Bloom (revision 4ab0472</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Piqa: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow. If you use this software, please cite it using these metadata</title>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5297715</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://openai.com/blog/sparse-transformers" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Boolq: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Think you have solved question answering? try arc, the AI2 reasoning challenge</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno>CoRR, abs/1803.05457</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Xnli: Evaluating crosslingual sentence representations</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Grangier</surname></persName>
		</author>
		<idno>CoRR, abs/1612.08083</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">2021. 8-bit optimizers via block-wise quantization</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing</title>
		<meeting>the Third International Workshop on Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07785</idno>
		<title level="m">Predictability and surprise in large generative models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On the sizes of openai api models</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Presser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The Pile: an 800GB dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A framework for few-shot language model evaluation</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Dipofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laria</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.5371628</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">*SEM 2012: The First Joint Conference on Lexical and Computational Semantics</title>
		<meeting><address><addrLine>Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012. SemEval 2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="394" to="398" />
		</imprint>
	</monogr>
	<note>Proceedings of the Sixth International Workshop on Semantic Evaluation</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<title level="m">Training compute-optimal large language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization</title>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<idno>ArXiv, abs/2003.11080</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">First quora dataset release: Question pairs</title>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Dandekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kornel</forename><surname>Csernai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pubmedqa: A dataset for biomedical research question answering</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghua</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2567" to="2577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Crowdsourcing multiple choice science questions</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Vancouver, Canada. Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Looking beyond the surface:a challenge set for reading comprehension over multiple sentences</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of North American Chapter</title>
		<meeting>North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Boseop</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoungseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gichang</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Hyeon Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seonhoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Hyung Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heungsub</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minyoung</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsub</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sukhyun</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seokhun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taeyong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinuk</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soyoung</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na-Hyeon</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsuk</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soobin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sookyo</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinseong</forename><surname>In</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyungduk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jisu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><forename type="middle">Goo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Do-Hyoung</forename><surname>Dong Hyun Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><forename type="middle">Young</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inho</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Woo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><surname>Ha</surname></persName>
		</author>
		<idno>ArXiv, abs/2109.04650</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Woo Chul Park, and Nako Sung</publisher>
		</imprint>
	</monogr>
	<note>What changes can large-scale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04683</idno>
		<title level="m">Race: Large-scale reading comprehension dataset from examinations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Warp size impact in gpus: large or small?</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Lashgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirali</forename><surname>Baniasadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Khonsari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GPGPU@ASPLOS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The bigscience ROOTS corpus: A 1.6TB composite multilingual dataset</title>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Hugo Lauren?on</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Saulnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Akiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Villanova Del Moral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leandro</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Von Werra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><forename type="middle">Gonz?lez</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huu</forename><surname>Ponferrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rg</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Frohberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>?a?ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G?rard</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loubna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><forename type="middle">De</forename><surname>Ben Allal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giada</forename><surname>Toni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Pistilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somaieh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maraim</forename><surname>Nikpoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Masoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paulo</forename><surname>De La Rosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Thrush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Romero Mu?oz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Van Strien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><surname>Almubarak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itziar</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Dios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Soroa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Ortiz</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamik</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Ifeoluwa Adelani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhas</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Violette</forename><surname>Chim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzana</forename><surname>Lepercq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName><surname>Jernite</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtysixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Limits to depth efficiencies of self-attention</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Wies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hofit</forename><surname>Bata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22640" to="22651" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Jurassic-1: Technical details and evaluation</title>
		<author>
			<persName><forename type="first">Opher</forename><surname>Lieber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barak</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Shoham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>AI21 Labs</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Few-shot learning with multilingual language models</title>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakanth</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian O'</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Horo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno>ArXiv, abs/2112.10668</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Logiqa: A challenge dataset for machine reading comprehension with logical reasoning</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanmeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yile</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR, abs/2007.08124</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generating wikipedia by summarizing long sequences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient large-scale language model training on gpu clusters using megatron-lm</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Vainbrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prethvi</forename><surname>Kashinkunti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Bernauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Javier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ortiz</forename><surname>Su?rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beno?t</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Romary</surname></persName>
		</author>
		<idno type="DOI">10.14618/ids-pub-9021</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7)</title>
		<meeting>the Workshop on Challenges in the Management of Large Corpora (CMLC-7)<address><addrLine>Mannheim</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07-22">2019. 2019. Cardiff, 22nd July 2019</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
		<respStmt>
			<orgName>Institut f?r Deutsche Sprache</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The LAMBADA dataset: Word prediction requiring a broad discourse context</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germ?n</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc</forename><forename type="middle">Quan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Fern?ndez</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1144</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1525" to="1534" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Wic: 10, 000 example pairs for evaluating context-sensitive representations</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pilehvar</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<idno>CoRR, abs/1808.09121</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Train short, test long: Attention with linear biases enables input length extrapolation</title>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Using the output embedding to improve language models</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><surname>Ring</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11446</idno>
		<title level="m">Susannah Young, et al. 2021. Scaling language models: Methods, analysis &amp; insights from training gopher</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<title level="m">Squad: 100,000+ questions for machine comprehension of text</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Turing-nlg: A 17-billionparameter language model by microsoft</title>
		<author>
			<persName><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10641</idno>
	</analytic>
	<monogr>
		<title level="m">Winogrande: An adversarial winograd schema challenge at scale</title>
		<imprint>
			<date type="published" when="2019">2020. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><forename type="middle">A</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bari</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmish</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanya</forename><surname>Sharma Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nihal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debajyoti</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Tian-Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng Xin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trishala</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos</forename><surname>Neeraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abheesht</forename><surname>Rozen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Santilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>F?vry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Teehan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rush</surname></persName>
		</author>
		<idno>ArXiv, abs/2110.08207</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<pubPlace>Stella Rose Biderman, Leo Gao, T. G. Owe Bers, Thomas Wolf</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Glu variants improve transformer</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model</title>
		<author>
			<persName><forename type="first">Shaden</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Norick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Zerveas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11990</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengfeng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09864</idno>
		<title level="m">Roformer: Enhanced transformer with rotary position embedding</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04006</idno>
		<title level="m">Long range arena: A benchmark for efficient transformers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Scale efficiently: Insights from pre-training and fine-tuning transformers</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno>ArXiv, abs/2109.10686</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08239</idno>
		<title level="m">Lamda: Language models for dialog applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In the Proceedings of ICLR</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hesslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><surname>Scao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05832</idno>
		<title level="m">Hyung Won Chung, Iz Beltagy, Julien Launay, and Colin Raffel. 2022. What language model architecture and pretraining objective work best for zero-shot generalization? arXiv preprint</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01652</idno>
		<title level="m">Finetuned language models are zero-shot learners</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Yuan 1.0: Large-scale pre-trained language model in zero-shot and few-shot learning</title>
		<author>
			<persName><forename type="first">Shaohua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongli</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04725</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11934</idno>
		<title level="m">Aditya Barua, and Colin Raffel. 2020. mt5: A massively multilingual pre-trained text-to-text transformer</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Aditya Barua, and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer</title>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked languagemodels</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Ben Zaken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10199</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">HellaSwag: Can a machine really finish your sentence?</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1472</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4791" to="4800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">Aohan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02414</idno>
		<title level="m">Glm-130b: An open bilingual pre-trained model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhe</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoda</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12369</idno>
		<title level="m">Pangu?: Large-scale autoregressive pretrained chinese language models with auto-parallel computation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01068</idno>
		<title level="m">Opt: Open pre-trained transformer language models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
