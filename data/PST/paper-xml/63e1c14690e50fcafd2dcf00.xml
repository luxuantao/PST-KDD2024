<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Interpretability via Explicit Word Interaction Graph Layer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-02-03">3 Feb 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Arshdeep</forename><surname>Sekhon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanjie</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aman</forename><surname>Shrivastava</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yangfeng</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
							<email>yanjun@virginia.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<settlement>Charlottesville</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Interpretability via Explicit Word Interaction Graph Layer</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-02-03">3 Feb 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2302.02016v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent NLP literature has seen growing interest in improving model interpretability. Along this direction, we propose a trainable neural network layer that learns a global interaction graph between words and then selects more informative words using the learned word interactions. Our layer, we call WIGRAPH, can plug into any neural network-based NLP text classifiers right after its word embedding layer 1 . Across multiple SOTA NLP models and various NLP datasets, we demonstrate that adding the WIGRAPH layer substantially improves NLP models' interpretability and enhances models' prediction performance at the same time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks (DNNs) have achieved remarkable results in the field of natural language processing (NLP) <ref type="bibr">(Zhang, Zhao, and LeCun 2015a;</ref><ref type="bibr" target="#b23">Miwa and Bansal 2016;</ref><ref type="bibr" target="#b43">Wu et al. 2016;</ref><ref type="bibr" target="#b42">Wolf et al. 2020)</ref>. Trustworthy real-world deployment of NLP models requires models to be not only accurate but also interpretable <ref type="bibr" target="#b44">(Xie et al. 2020)</ref>. Literature has included a growing focus on providing posthoc explanations or rationales for NLP models' predictions (Ribeiro, <ref type="bibr" target="#b28">Singh, and Guestrin 2016;</ref><ref type="bibr" target="#b20">Lundberg and Lee 2017;</ref><ref type="bibr" target="#b25">Murdoch, Liu, and Yu 2018;</ref><ref type="bibr" target="#b36">Singh, Murdoch, and Yu 2018;</ref><ref type="bibr" target="#b8">Chen, Zheng, and Ji 2020)</ref>. However, explaining DNNs using a posthoc manner cannot improve a model's intrinsic interpretability.</p><p>As shown in <ref type="bibr">(Chen and Ji 2020)</ref>, two NLP models may have the same prediction behavior but different interpretation ability. This concept of "intrinsic interpretability" motivates a compelling research direction to improve the interpretability of NLP models. A few recent studies used user-specified priors as domain knowledge to guide model training <ref type="bibr" target="#b5">(Camburu et al. 2018;</ref><ref type="bibr" target="#b10">Du et al. 2019;</ref><ref type="bibr" target="#b6">Chen and Ji 2019;</ref><ref type="bibr" target="#b10">Erion et al. 2019;</ref><ref type="bibr" target="#b24">Molnar, Casalicchio, and Bischl 2019)</ref>, hence improving model interpretability. Such information priors, however, may not be available in many tasks. Several other studies proposed to develop inherently interpretable models (Alvarez-Melis and Jaakkola 2018a; Rudin 2019), but these require intensive engineering efforts. More recently, <ref type="bibr">Chen and Ji (2020)</ref> proposed to add a variational word mask, VMASK, to improve the interpretability of NLP neural classifiers.</p><p>The aforementioned literature on improving NLP models' intrinsic interpretability have mostly focused on highlighting important words. Strategies like VMASK just select important words unilaterally, without accounting for how one word influences other words regarding interpretability. Studies have shown that word interactions are critical in explaining how NLP models make decisions <ref type="bibr" target="#b11">(Halford, Wilson, and Phillips 2010)</ref>. For instance, for a sentiment classification task, when without a context, it is hard to conclude if the word 'different' by itself is vital for sentiment? However, if we find 'different' highly relates to the word 'refreshingly', it will likely contribute substantially to the model's sentiment prediction (see Table <ref type="table" target="#tab_0">1</ref> <ref type="foot" target="#foot_0">2</ref> ).</p><p>Along this direction, we propose a novel neural network layer, we call WIGRAPH, to improve NLP models' intrinsic interpretability. WIGRAPH is a plug-and-play layer and uses a graph-oriented neural network design: (1) It includes a stochastic edge discovery module that can discover significant interaction relations between words for a target prediction task; <ref type="bibr" target="#b47">(2)</ref> It then uses a neural message passing module to update word representations by using information from their interacting words; and (3) It designs a variational information bottleneck based loss objective to suppress irrelevant word interactions (regarding target predictions). We call such a loss: VIB-WI loss. To improve a target text classifier's intrinsic interpretability, we propose to add the proposed WIGRAPH layer right after the word embedding layer and fine-tune such an augmented model using a combination of the original objective and VIB-WI loss objective.</p><p>In summary, this paper makes the following contributions:</p><p>? We design WIGRAPH to augment neural text classifiers to improve these models' intrinsic interpretability. WI-GRAPH does not require external user priors or domain knowledge. WIGRAPH can plug-and-play into any neural NLP models' architectures, right after the word embedding layer. results in better explanations (locally, globally as well as regarding interactions) and better model predictions at the same time.</p><p>2 Method: A Novel WIGRAPH Layer</p><p>Our main hypothesis is: a novel layer that can extract crucial global word interactions will improve neural text classifiers' interpretability. This is because we envision plugging such a layer will enhance a target model's decision-making process by providing explicit guidance on what words are more important using the information on those words they interact with. We aim for three properties in such a layer design:</p><p>(1) plug-and-play; (2) model agnostic; and (3) no loss of prediction performance. We denote vectors using lowercase bold symbols. We assume text inputs include a maximum length of L tokens. We denote the whole word vocabulary as set V. We use V to denote its size (the total number of unique words in this vocabulary V). Besides, we use f to describe a neural text classification model. f classifies an input text into y ? {1, . . . , C}, where C is the number of classes. For an input sentence, we denote its i-th word as w i and its embedding representation as vector x i : ?i ? {1, . . . , L}. Therefore, the embeddings of an input text make a matrix form X = [x 1 , . . . , x L ] T (this means X ? R L?d where L is the length of input and d is the dimension of each x i .).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">To Discover Word Interaction Graph: A</head><p>Now we explain the first component of the proposed WI-GRAPH layer. This module aims to discover how words globally interact for a predictive task. Our primary strategy to describe how words relate is to treat words as nodes and their interaction as edges in an interaction graph. We follow such an idea and choose to learn an undirected word interaction graph using a stochastic neural network module. We represent this unknown graph as A = {A ij } V ?V . A includes the edges representing word interactions. We assume each A ij ? {0, 1} is one binary random variable. A is stochastic whose A ij specifies the presence or absence of an interaction between word i and word j in vocabulary V. A ij ? {0, 1} is sampled from Sigmoid (? ij ), following Bernoulli distribution with parameter Sigmoid (? ij ). In Section 2.2, we show how A can help us understand how certain words are more important than others owing to the learned word interactions.</p><p>Learning the word interaction graph A means to learn the parameter matrix ? = {? ij } V ?V . In Section 2.4, we show how ? (and therefore A) is learned through the variational information bottleneck framework <ref type="bibr" target="#b0">(Alemi et al. 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Message Passing on Word Interaction Graph using Graph Convolution: E</head><p>In our second module, we represent the i-th word x i of an input text x as a node on the A graph. We use a modified version of graph convolutional operation <ref type="bibr" target="#b15">(Kipf and Welling 2016)</ref> to update each x i with its neighboring words x j . Here j ? N (i), and N (i) denotes those neighbor nodes of x i on the graph A and in x. Specifically, we denote the resulting word representation vector as e i . Each x i is revised using a graph based summation from its neighbors' embedding x j , j ? N (i):</p><formula xml:id="formula_0">e i = x i + ? 1 |N (i)| j?N (i) x j ,<label>(1)</label></formula><p>Eq. ( <ref type="formula" target="#formula_0">1</ref>) is motivated by the design of Graph convolutional networks (GCNs) that were recently introduced to learn useful node representations that encode both node-level features and relationships between connected nodes <ref type="bibr" target="#b15">(Kipf and Welling 2016)</ref>. Different from the ReLU activation function used in vanilla GCNs, we use GeLU as the ?(?) , the non-linear activation function proposed in <ref type="bibr" target="#b12">(Hendrycks and Gimpel 2016)</ref>. We want to point out that Eq. ( <ref type="formula" target="#formula_0">1</ref>) is different from a typical GCN operation from <ref type="bibr" target="#b15">(Kipf and Welling 2016)</ref>. First, we only conduct one hop of neighbor aggregation in Eq. (1). A typical GCN module does multi-hops. Second, we drop W t ? R d?d used for t-th hop of GCN update in <ref type="bibr" target="#b15">(Kipf and Welling 2016)</ref>. This is because we assume that the BASE text classifier model f has taken into account this prior and our WIGRAPH layer will not bias to prefer short range interactions. The third difference is the most important distinction that differentiates ours apart from <ref type="bibr" target="#b15">(Kipf and Welling 2016)</ref>. The graph has been given apriori to typical GCNs. However, in our work, we need to learn the graph A (see Section 2.4 on how to learn A).</p><p>We can compute the simultaneous update of all words in input text x together by concatenating all e i . This gives us one matrix E ? R L?d , where L is the length of input and d is the embedding dimension of each x i . The simultaneous update can be written as:</p><formula xml:id="formula_1">E = ?(A X).</formula><p>( where</p><formula xml:id="formula_3">A = D-1 2 (A x + I) D-1 2</formula><p>, that is the normalized adjacency matrix and D is the diagonal degree matrix of (A x + I). Note: A x denotes those edges from A that are local for the current sample text x. In summary, our second module computes:</p><formula xml:id="formula_4">E = g GC (X, A)</formula><p>2.3 To Build and Use WIGRAPH Layer: X ? Z Our design of WIGRAPH layer is that it can take the embedding matrix of a text example as input (X ? R L?d ), and output a revised matrix representing each word with revised embedding (Z ? R L?d ). WIGRAPH layer aids the selection of more informative words based on their interactions for current predictive task. The proposed layer does not need significant efforts on engineering network architectures and does not require pre-collected importance attributions or explanations.</p><p>Our main goal is to improve the intrinsic interpretability of neural text classifiers with a simple model augmentation. Therefore, for a given neural text classifier, we propose to simply insert a WIGRAPH layer right after the word embedding input layer and before the subsequent network layers of that target model.</p><p>There exist many possible ways to build WIGRAPH layer from our first two modules (Section (2.1) and Section (2.2)). The simplest way is that we can just pass E as Z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Z = E</head><p>(3) Figure <ref type="figure" target="#fig_0">1</ref> visualizes how this vanilla version of WIGRAPH layer updates word representations with the X ? E ? Z data flow during inference. During training, it needs to learn the graph A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Model Training with VIB-WI loss</head><p>Now we propose to train WIGRAPH jointly with other layers using a new objective that we name as variational information bottleneck loss for word interaction (VIB-WI loss). VIB-WI loss aims to restrict the information of globally irrelevant word interactions flowing to subsequent network layers, hence forcing the model to focus on important interactions to make predictions. Following the Information Bottleneck framework <ref type="bibr" target="#b0">(Alemi et al. 2016)</ref>, we aim to learn A and all subsequent layers' weights {W}, to make Z maximally informative of the prediction label Y, while being maximally compressive of X (see Figure <ref type="figure" target="#fig_0">1</ref>). That is</p><formula xml:id="formula_5">max A,{W} {I(Z; Y) -?I(Z; X)}<label>(4)</label></formula><p>Here I(?; ?) denotes the mutual information, and ? ? R + is a coefficient balancing the two mutual information terms. Given a specific example (x m , y m ), we can further simplify the lower bound of first term I(Z; Y) in Eq. (4) as:</p><formula xml:id="formula_6">I(z; y m ) ? E q(z|x m ) log(p(y m |x m ; A, {W})) (5)</formula><p>Similarly for the second term I(Z; X) in Eq. ( <ref type="formula" target="#formula_5">4</ref>) and for a given example (x m , y m ), we can simplify its upper bound as:</p><formula xml:id="formula_7">I(z; x m ) ? KL(q(A|x m )||p a0 (A))<label>(6)</label></formula><p>Due to the difficulty in calculating two mutual information terms in Eq. ( <ref type="formula" target="#formula_5">4</ref>), we follow <ref type="bibr" target="#b35">(Schulz et al. 2020;</ref><ref type="bibr" target="#b0">Alemi et al. 2016)</ref> to use a variational approximation q(X, Y, Z) to approximate the true distribution p(X, Y, Z). Details on how to derive Eq. ( <ref type="formula">5</ref>) and Eq. ( <ref type="formula" target="#formula_7">6</ref>) are in Section (A.4). Now combining Eq. ( <ref type="formula">5</ref>) and Eq. ( <ref type="formula" target="#formula_7">6</ref>) into Eq. ( <ref type="formula" target="#formula_5">4</ref>), we get the revised objective as:</p><formula xml:id="formula_8">max A,R,{W} {E q(Z|x m ) log(p(y m |x m ; A, R, {W})) -? g KL(q(A|x m )||p a0 (A))}<label>(7)</label></formula><p>Eq. ( <ref type="formula" target="#formula_8">7</ref>) is the proposed VIB objective for a given observation (x m , y m ).</p><p>Detailed Model Specification: During training, for the stochastic interaction graph A, we learn its trainable parameter matrix ? ? R |V |?|V | , that is also optimized along with the model parameters during training. Further, we use the mean field approximation <ref type="bibr" target="#b4">(Blei, Kucukelbir, and McAuliffe 2017)</ref>, that is,</p><formula xml:id="formula_9">q(A x |x) = L i=1 L j=1 q(A xi,xj |x i , x j ).</formula><p>Equation 7 requires prespecified prior distributions p a0 . We use a Bernoulli distribution prior (a non-informative prior) for each word-pair interaction q</p><formula xml:id="formula_10">? [A xi,xj |x i , x j ]. p a0 (A x ) = L i=1 L j=1 p a0 (A xi,xj</formula><p>) and p a0 (A xi,xj ) = Bernoulli(0.5). This leads to:</p><formula xml:id="formula_11">KL(q(A x |x m )||p a0 (A)) = -H q (A x |x m )<label>(8)</label></formula><p>Here, H q denotes the entropy of the term A x |x m under the q distribution. Besides, we add a sparsity regularization on A x to encourage learning of sparse interactions. Now, we have the following loss function for (x m , y m ):</p><p>-</p><formula xml:id="formula_12">(E x p(y|x m ; A, {W}) + ? g H q (A x |x m )) +? sparse ||A x || 1 (9)</formula><p>During training, A is discrete and is drawn from Bernoulli distributions that are parametrized with matrix ? ? R |V |?|V | . We, therefore, use the Gumbel-Softmax(Jang, Gu, and Poole 2016) trick to differentiate through the sampling step and propagate the gradients to the respective parameters ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Variation: WIGRAPH-A-R</head><p>We also try another possible design of WIGRAPH that includes one more separate module that learns an attribution word mask R on top of E . Aiming for better word selection, R is designed as a stochastic layer we need to learn and R ? {0, 1} V . Each entry in R (e.g., R j ? {0, 1}) follows a Bernoulli distribution with parameter ? (to be learned).</p><p>During inference, for an input text x, we get a binary vector R x from R that is of size L. Its i-th entry R xi ? {0, 1} is a binary random variable associated with the word token at the i-th position. We use the following operation (a masking operation!) to generate the final representation of the i-th word from a WIGRAPH layer:</p><formula xml:id="formula_13">z i = R xi e i (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>We can compute the simultaneous update of all words in input text x together by concatenating all z i denoted as matrix Z ? R L?d . The simultaneous update can then be written as:</p><formula xml:id="formula_15">Z = diag (R x ) L?L E L?d (11)</formula><p>During training, we need to learn both A and R. Now the loss function VIB-WI loss turns to:</p><formula xml:id="formula_16">-(E x p(y|x m ; A, R, {W}) + ? i H q (R x |x m )+ ? g H q (A x |x m )) + ? sparse ||A x || 1</formula><p>Due to page limit, we put detailed derivations of above and specification of R in Section (A.1). We call the vanilla version of WIGRAPH as WIGRAPH-A and the version with the word mask R as WIGRAPH-A-R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Connecting to Related Work</head><p>Our design orients from one basic notion that we treat interpretability as an intrinsic property of neural network models. We expect a neural text classifier will be more interpretable, when focusing on important word interactions to make predictions. Our work connects to multiple related topics:</p><p>Self-Explaining Models Recent literature has seen growing interests in treating interpretability as an inherent property of NLP deep models. (Alvarez-Melis and Jaakkola 2018b; Rudin 2019) proposed to design self-interpretable models by requiring human annotations in model engineering. <ref type="bibr">(Chen and Ji 2020)</ref> proposed VMASK layer for improving NLP models' interpretability. This layer automatically learns taskspecific word importance and guides a model to make predictions based on important words. However, this method does not consider interactions between words.</p><p>Explanations as Feedback Anoter category of work uses explanations as feedback for improving model prediction performance as well as to encourage explanation faithfulness. <ref type="bibr" target="#b5">(Camburu et al. 2018</ref> Differently, our proposed method augments a model with a special layer that improves both prediction performance and interpretability (see Section ( <ref type="formula" target="#formula_5">4</ref>)).</p><p>Graph Neural Networks Graph Neural Networks (GNNs) generalize neural networks from regular grids, like images to irregular structures like graphs. There exists a wide variety of GNN architectures like <ref type="bibr" target="#b15">(Kipf and Welling 2016;</ref><ref type="bibr" target="#b34">Scarselli et al. 2008;</ref><ref type="bibr" target="#b40">Veli?kovi? et al. 2017;</ref><ref type="bibr" target="#b33">Santoro et al. 2017)</ref>. They share the same underlying concept of message passing between connected nodes in the graph. However, little attention has been paid to address cases when the underlying graph is unknown. In contrast, in WIGRAPH, we do not know the global interaction graph apriori. It is learnt along with the prediction model as part of the training.</p><p>Post-Hoc Explanation NLP literature includes a number of methods that focus on disentangling the rationales of a trained NLP model's decision by finding which words contributed most to a prediction, including the popularly used LIME(Ribeiro, Singh, and Guestrin 2016) and SampleShapley <ref type="bibr" target="#b17">(Kononenko et al. 2010)</ref> methods. Recent studies have proposed to generate post-hoc explanations beyond wordlevel features by detecting feature interactions, including for instance, contextual decomposition by <ref type="bibr" target="#b25">(Murdoch, Liu, and Yu 2018)</ref>. Other work adopted Shapley interaction index to compute feature interactions <ref type="bibr" target="#b19">(Lundberg, Erion, and Lee 2018)</ref>. In contrast to these post-hoc interpretation systems, our method focuses on designing a strategy to improve the inherent interpretability of NLP models.</p><p>Information Bottleneck Based Methods The information bottleneck method was first proposed by <ref type="bibr" target="#b38">(Tishby, Pereira, and Bialek 2000;</ref><ref type="bibr" target="#b39">Tishby and Zaslavsky 2015)</ref>. <ref type="bibr" target="#b0">(Alemi et al. 2016</ref>) introduced a variational approximation to the information bottleneck that enables usage for deep neural networks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We design experiments to answer the following:</p><p>1. Are NLP models augmented with WIGRAPH layer more interpretable models? 2. Do NLP models augmented with WIGRAPH layer predict well?</p><p>Besides, we extend WIGRAPH to one concept based vision task in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup: Datasets, Models and Metrics</head><p>Datasets Our empirical analysis covers six popular text classification datasets as detailed by Table <ref type="table" target="#tab_3">3</ref>. These six datasets are "sst1", "sst2" <ref type="bibr" target="#b37">(Socher et al. 2013)</ref>, "imdb" <ref type="bibr" target="#b21">(Maas et al. 2011)</ref>, "AG news"(Zhang, Zhao, and LeCun 2015b), "TREC" <ref type="bibr" target="#b18">(Li and Roth 2002)</ref> and "Subj" <ref type="bibr" target="#b27">(Pang and Lee 2005)</ref>. Three of the datasets are for binary classification, and the rest are for multi-class text classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BASE Models</head><p>We use four commonly used neural text classifiers to evaluate WIGRAPH: LSTM, and transformer based SOTA models including BERT, RoBERTa and distil-BERT. As Section 2.4 described, we plug our WIGRAPH layer right after the word embedding input layer. For the LSTM models(Hochreiter and Schmidhuber), we initialize word embeddings from <ref type="bibr" target="#b22">(Mikolov et al. 2013)</ref> with dimension d = 300. For BERT, RoBERTa and distilBERT models, we use fine-tuned base models from <ref type="bibr" target="#b42">(Wolf et al. 2020)</ref> on each dataset (Table <ref type="table" target="#tab_3">3</ref>).</p><p>Hyperparameter Tuning We perform fine-tuning on each model (batch size=64). We fix the word embedding layer and train WIGRAPH layer along with the rest of a BASE model. For the LSTM models, we vary the hidden size ? {100, 300, 500}, and dropout in {0.0, 0.2, 0.3}. We set ? sparse ? {1e -02, 1e -03, 1e -04}, ? g ? {1.0, 1e -02, 1e -03, 1e -04} and ? i ? {1.0, 1e -02, 1e -03, 1e -04}. The learning rate is tuned from the set {0.0001, 0.0005, 0.005, 0.001}. For transformer based models, we vary dropout in range {0.2, 0.3, 0.5}, hidden dimension to compute R ? {128, 256, 512}. We set ? sparse , ? g , ? i = 1.0 and anneal it by a factor of 0.1 every epoch. For the larger vocabulary cases (IMDB, AG-News datasets and transformer-base models), we filter words for learning our interaction matrix A, i.e., we learn interactions for the top frequent 10, 000 words.</p><p>Baselines: To our best knowledge, WIGRAPH is the only plug-and-play layer to improve a target neural text classifier's interpretability using explicit pairwise word interactions. (3) We further design interaction interpretability measures to compare different models in Section (4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Prediction Performance Comparison</head><p>In Table <ref type="table" target="#tab_2">2</ref>, we compare prediction performance using four different SOTA models across six different datasets. This makes 24 different (BASE, data) combinations , and on each case, we compare BASE model, VMASK augmented base model versus our WIGRAPH augmented model regarding the prediction accuracy. Here we refer to WIGRAPH as the best performing model between WIGRAPH-A and WIGRAPH-A-R. Table <ref type="table" target="#tab_2">2</ref> shows that adding WIGRAPH layer into SOTA neural text classifier models makes the models predict better! Empirically, the performance gains on LSTM models appear more than on Transformer models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Attribution Interpretability Comparison: Area Under Perturbation Curve (AOPC)</head><p>Here we empirically check our hypothesis that training a model augmented with WIGRAPH layer leads to improvements of model explanation faithfulness during downstream post-hoc interpretation analyses. We use Area Over Perturbation Curve (AOPC) <ref type="bibr" target="#b26">(Nguyen 2018;</ref><ref type="bibr" target="#b32">Samek et al. 2016)</ref> as the evaluation metric. AOPC is defined as the average change of prediction probability on the predicted class over a test dataset by deleting top K words in explanations. Higher AOPC scores reflect better interpretation faithfulness.</p><formula xml:id="formula_17">AOP C = 1 K + 1 K k=1 &lt; f (x) -f (x \1,...,k ) &gt; p(x) (12)</formula><p>We generate word-level attribution explanations using two popular post-hoc explanation methods: LIME(Ribeiro, Singh, and Guestrin 2016) and SampleShapley <ref type="bibr" target="#b17">(Kononenko et al. 2010)</ref>. Across all datasets, we use 500 test samples and k ? {1, . . . , 10}. Table <ref type="table" target="#tab_4">4</ref> shows that WIGRAPH-A outperforms the original BASE model and the BASE with VMASK.</p><p>When LIME is used to post-hoc explain models, across all 24 cases of (model, dataset) combinations, WIGRAPH outperforms the original BASE model and the BASE with VMASK layer regarding AOPC score in 21 cases. The only three exception include the IMDB/BERT, TREC/BERT and IMDB/RoBERTa setups. When SampleShapley is used, WIGRAPH outperforms the original BASE model and the BASE with VMASK layer in 22 cases out of 24 (model, dataset) combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Interaction Analysis and Ablation</head><p>In this experiment, we introduce a new metric: Interaction Occlusion Score (IoS). The IoS score measures the interaction interpretability faithfulness of a target model on its learnt interactions.</p><p>WIGRAPH discovers globally informative interactions with the importance score E q [A xi,j |x i,j ] (see q in Section (2.4)). We sort entries of A and filter out the top K global interaction scores, denoted by A k ij . We then calculate the accuracy of the model after only using these top k interactions via: Table <ref type="table">5</ref>: Ablations analysis regarding prediction accuracy from: WIGRAPH-A and WIGRAPH-A-R.</p><formula xml:id="formula_18">IOS(k) = 1 M M m=1</formula><p>Here, we represent the label of the model on the m th test sample as y m . Table <ref type="table" target="#tab_6">6</ref> in Section (A.2) shows using the top interactions outperforms the setting when no pairwise interactions are used during inference.</p><p>Ablation: We perform extensive ablation analysis to compare WIGRAPH-A and WIGRAPH-A-R. Table <ref type="table" target="#tab_7">7</ref> provides comparison analysis regarding prediction accuracy and we can tell no clear winner between the two variations across all 24 cases of (BASE, data) combinations. See Section (A.2) and Table <ref type="table" target="#tab_9">8</ref> for more ablation results via other metrics. We recommend to use WIGRAPH-A in most real-world applications, due to less parameters.</p><p>Qualitative Visualization Section (A.2) includes an extensive list of qualitative analyses we make to understand WI-GRAPH, including correlation analysis of interaction against co-occurrence, word clouds visualization, and interacting word-pair examples. We show in Figure <ref type="figure" target="#fig_1">2</ref> that our learned interaction matrix does not merely mirror the co-occurrence statistics, instead learns more general informative global interactions. Figure <ref type="figure" target="#fig_2">3</ref> uses word clouds to visualize top interacting word pairs obtained from TREC, SST1, and SST2 datasets.</p><p>The word pairs are consistent with corresponding tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Modeling Concept Interaction in Vision</head><p>Koh et al. ( <ref type="formula">2020</ref>) introduced the notion of high-level concepts as an intermediate interpretable interface in the inputmodel-predictions pipeline. These concepts describe high level attributes of an image. This enables users to directly interact with a model by intervening on human-interpretable concepts. The interactions between these concepts can affect prediction, however these interactions are unknown. In this section, we investigate the utility of learning these interactions for aiding prediction using our WIGRAPH layer. We train a concept bottleneck <ref type="bibr" target="#b16">(Koh et al. 2020</ref>) model on the CUB dataset <ref type="bibr" target="#b41">(Wah et al. 2011</ref>) which is jointly trained with a WIGRAPH layer. In Section B, we show that the WIGRAPH layer is able to learn concept embeddings, model interactions between concepts, and can also be used with test time concept intervention to improve prediction accuracy on the final task. In order to extend WIGRAPH for this setup, we introduce a dynamic interaction graph with concepts and the image as nodes. Intuitively, the image is considered to be a composition of concept embeddings. The interaction between the concepts are learned variables whereas the interactions between an image and its concepts are computed using the cosine similarity. The details of our model architecture are in Section B.2.</p><p>Results: As a baseline, we fine-tune an Inception V3-based joint concept bottleneck model <ref type="bibr" target="#b16">(Koh et al. 2020</ref>) that achieve a prediction accuracy of 80.04% on the CUB dataset <ref type="bibr" target="#b41">(Wah et al. 2011)</ref>. Together with this model, we jointly train our WIGRAPH and the concept embedding layer to learn the interactions between the concepts. As described in <ref type="bibr" target="#b16">(Koh et al. 2020)</ref>, test time intervention (TTI) helps improve prediction accuracy to 89.41%(+9.37%). Interestingly, we observe that TTI achieves more improvements of prediction accuracy when using WIGRAPH augmented concept vision model to 95.74%(+15.70%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we try to answer the question: Does adding a special layer in the form of discovering word-word interactions lead to improvements in a neural text classifier's interpretability? Our paper gives a firm "Yes" to the question and provides a neural-network based design for making such a layer. The second component of WIGRAPH layer uses the message passing framework and it can be expanded to allow for learning and accounting for higher-order interactions (not only pairwise) in a scalable way. We will explore this in our future works. Furthermore, WIGRAPH can easily extend to cross sentence tasks like Natural Language Inference, and we will leave it to future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Model Training for WIGRAPH-A-R</head><p>We propose to train WIGRAPH jointly with other layers using a new objective that we name as variational information bottleneck loss for word interaction (VIB-WI loss). VIB-WI loss aims to restrict the information of globally irrelevant word interactions flowing to subsequent network layers, hence forcing the model to focus on important interactions to make predictions. Following the Information Bottleneck framework <ref type="bibr" target="#b0">(Alemi et al. 2016)</ref>, we aim to learn A, R and all subsequent layers' weights {W}, to make Z maximally informative of Y, while being maximally compressive of X (see Figure <ref type="figure" target="#fig_0">1</ref>). That is</p><formula xml:id="formula_19">max A,R,{W} {I(Z; Y) -?I(Z; X)}<label>(14)</label></formula><p>Here I(?; ?) denotes the mutual information, and ? ? R + is a coefficient balancing the two mutual information terms. Given a specific example (x m , y m ), we can further simplify the lower bound of first term I(Z; Y) in Eq. ( <ref type="formula" target="#formula_19">14</ref>) as:</p><formula xml:id="formula_20">I(z; y m ) ? E q(z|x m ) log(p(y m |x m ; A, R, {W})) (15)</formula><p>Similarly for the second term I(Z; X) in Eq. ( <ref type="formula" target="#formula_19">14</ref>) and for a given example (x m , y m ), we can simplify its upper bound as:</p><formula xml:id="formula_21">I(z; x m ) ? KL(q(R|x m )||p r0 (R)) +KL(q(A|x m )||p a0 (A))<label>(16)</label></formula><p>Due to the difficulty in calculating two mutual information terms in Eq. ( <ref type="formula" target="#formula_19">14</ref>), we follow <ref type="bibr" target="#b35">(Schulz et al. 2020;</ref><ref type="bibr" target="#b0">Alemi et al. 2016</ref>) to use a variational approximation q(X, Y, Z) to approximate the true distribution p(X, Y, Z). Details on how to derive Eq. ( <ref type="formula">15</ref>) and Eq. ( <ref type="formula" target="#formula_21">16</ref>) are in Section (A.4). Now combining Eq. ( <ref type="formula">15</ref>) and Eq. ( <ref type="formula" target="#formula_21">16</ref>) in Eq. ( <ref type="formula" target="#formula_19">14</ref>), we get the revised objective as: <ref type="formula">17</ref>) is the proposed VIB objective for a given observation (x m , y m ). To increase the flexibility, we associate two different coefficients from R + with the two KL-terms. In practice we treat them as hyper-parameters.</p><formula xml:id="formula_22">max A,R,{W} {E q(Z|x m ) log(p(y m |x m ; A, R, {W})) -? i KL(q(R|x m )||p r0 (R)) -? g KL(q(A|x m )||p a0 (A))} (17) Eq. (</formula><p>Detailed Model Specification: In this section, we describe in detail how to learn discrete A and R along with the model parameters during training. To learn the word mask R, we use amortized variational inference <ref type="bibr" target="#b28">(Rezende and Mohamed 2015)</ref>. That is, we use a single-layer feedforward neural network as the inference network q ? (R xt |x t ), associated parameters ? are optimized with the model parameters during training. For the interaction mask (graph) A, we use a trainable parameter matrix ? ? R |V |?|V | , that is also optimized along with the model parameters during training. Further, we use the mean field approximation <ref type="bibr" target="#b4">(Blei, Kucukelbir, and McAuliffe 2017)</ref> for both the word mask and the variational interaction mask, that is, q(R|x)</p><formula xml:id="formula_23">= L i=1 q(R xt |x t ) and q(A x |x) = L i=1 L j=1 q(A xi,xj |x i , x j ).</formula><p>Equation 17 requires prespecified prior distributions p r0 and p a0 . We use the Bernoulli distribution prior (a non-informative prior) for each word-pair interaction</p><formula xml:id="formula_24">q ? [A xi,xj |x i , x j ]. p a0 (A x ) = L i=1 L j=1 p a0 (A xi,xj</formula><p>) and p a0 (A xi,xj ) = Bernoulli(0.5). This leads to:</p><formula xml:id="formula_25">KL(q(A x |x m )||p a0 (A)) = -H q (A x |x m )<label>(18)</label></formula><p>Here, H q denotes the entropy of the term A x |x m under the q distribution. Similarly, for the word mask, p r0 (R) = L i=1 p r0 (R xi ), and p r0 (R xi ) = Bernoulli(0.5). Therefore,</p><formula xml:id="formula_26">KL(q(R x |x m )||p a0 (R)) = -H q (R x |x m )<label>(19)</label></formula><p>Finally, we have the following loss function for (x m , y m ):</p><formula xml:id="formula_27">-(E x p(y|x m ; A, R, {W}) + ? i H q (R x |x m )+ ? g H q (A x |x m )) + ? sparse ||A x || 1<label>(20)</label></formula><p>We use stochastic gradient descent to optimize the above loss using all training samples. During training, both A and R are discrete samples drawn from Bernoulli distributions in Eq. ( <ref type="formula" target="#formula_2">2</ref>) and Eq. ( <ref type="formula">11</ref>). We, therefore, use the Gumbel-Softmax(Jang, Gu, and Poole 2016) trick to differentiate through the sampling step and propagate the gradients to their respective parameters ? and ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 More results on ablations and Global Interaction Analysis results</head><p>We present our results on the IoS score proposed in Section 4.4 in Table <ref type="table" target="#tab_6">6</ref>. We show using the top interactions outperforms the setting when no pairwise interactions are used during inference. We perform further ablations to calculate the different proposed interpretability scores in Table <ref type="table" target="#tab_9">8</ref>. We use LIME as the post-hoc explanation method for AOPC. WIGRAPH-A achieves higher interpretability scores, both global and local, than its variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 More Qualitative Results</head><p>We show in Figure <ref type="figure" target="#fig_1">2</ref> that our learnt interaction matrix does not merely mirror the co-occurrence statistics, instead learns more general informative global interactions. Figure <ref type="figure" target="#fig_2">3</ref> use word clouds to visualize interacted word pairs obtained from TREC, SST1 and SST2 datasets. The interactions are ranked based on E q [A xi,j |x i,j ]. The selected word pairs are consistent with the corresponding tasks. Figure <ref type="figure">4</ref> visualizes exemplar words' interactions that an improved model considers during inference.</p><p>In Table <ref type="table" target="#tab_10">9</ref>, we observe that the correlation between WI-GRAPH's word importance score E q [R xt |x t ] and word frequency is lower than VMASK. This indicates WIGRAPH can discover some important words that VMASK can not. Table <ref type="table" target="#tab_10">9</ref> also show a weak negative correlation between the interaction importance scoreE q [A xi,j |x i,j ] and word co-occurrence frequency.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Detailed Derivation</head><p>We follow the markov factorization : p(X, Y, Z) = p(Z|X, Y)P (X, Y) = P (Z|X, Y)P (Y|X)P (X) = P (Z|X)P (Y|X)P (X), motivated from the Markov assumption : Y ? ? X ? ? Z, i.e. Y and Z are independent of each other given X. We assume the data are generated using the above assumption where Z is not observed, i.e. a latent variable. Our derivation is based on <ref type="bibr" target="#b9">(Chen et al. 2018;</ref><ref type="bibr" target="#b35">Schulz et al. 2020;</ref><ref type="bibr" target="#b0">Alemi et al. 2016)</ref>, where we start from an approximation q(X, Y, Z) instead of the true distribution p(X, Y, Z).</p><p>The lower bound for I(Z; Y).</p><formula xml:id="formula_28">I(Z, Y) = y,z</formula><p>q(y, z) log q(y, z) q(y)q(z) = y,z q(y, z) log q(y|z) q(y) = y,z q(y, z) log q(y|z)</p><formula xml:id="formula_29">+H q (Y),<label>(21)</label></formula><p>where H q (?) represents entropy, and can be ignored for the purpose of training the model.  Figure <ref type="figure">4</ref>: Visualization of some learnt interactions viewed at the sentence level: For example, for the input sentence: "no, it's not nearly good as any of it's influences": "good" was picked out as an important word, "not" by itself was not an important word, using R, prediction using WIGRAPH aggregates "good" with "not" in the sentence as 'not-good' is an important interaction globally, which is used to make a prediction.  <ref type="bibr">VMASK)</ref>. We also show the correlation of the interaction importance scores E q [A xi,xj |x i , x j ] with the co-occurrence frequency, which shows a weak negative correlation for all datasets(Column-WIGRAPH-A).</p><p>y,z q(y, z) log q(y|z) = y,z q(y, z) log q(y|z)p(y|z) p(y|z) = y,z q(y, z) log p(y|z) + KL[q(y|z)||p(y|z)] ? y,z q(y, z) log p(y|z),</p><p>where KL[?||?] denotes Kullback-Leibler divergence. This gives us the following lower bound:</p><formula xml:id="formula_31">I(Z, Y) ? y,z q(y, z) log p(y|z) + H q (y) = y,z,x q(x, y, z) log p(y|z) + H q (y) = y,z,x q(x, y)q(z|x) log p(y|z)+H q (y),<label>(23)</label></formula><p>where the last step uses q(x, y, z) = q(x)q(y|x)q(z|x), which is a factorization based on the conditional dependency: y ? x ? z: y and z are independent given x. Given a sample, (x (m) , y (m) ), we can assume the empirical distribution q(x (m) , y (m) ) simply defined as a multiplication of two Delta functions</p><formula xml:id="formula_32">q(x = x (m) , y = y (m) ) = ? x (m) (x) ? ? y (m) (y).<label>(24)</label></formula><p>So we simplifying further of the first term:</p><formula xml:id="formula_33">I(z; y (m) ) ? z q(z|x (m) ) log p(y (m) |z) = E q(z|x (m) ) log(p(y (m) |z))<label>(25)</label></formula><p>Since Z = diag (R x ) E , and E is a deterministic function of A, we have:</p><formula xml:id="formula_34">I(z; y (m) ) ? E q(z|x (m) ) log(p(y (m) |R, A, x (m) )) (26)</formula><p>The upper bound for I(Z; X).</p><formula xml:id="formula_35">I(Z, X) = x,z q(x, z) log q(x, z) q(x)q(z) = x,z q(x, z) log q(z|x) q(z) = x,z q(x, z) log q(z|x) - x,z</formula><p>q(x, z) log q(z)</p><p>By replacing q(z) with a prior distribution of z, p 0 (z), we have</p><p>x,z q(x, z) log q(z) ?</p><p>x,z q(x, z) log p 0 (z).</p><p>(</p><formula xml:id="formula_36">)<label>27</label></formula><p>Then we can obtain an upper bound of the mutual information</p><formula xml:id="formula_37">I(Z; X) ? x,z q(x, z) log q(z|x) - x,z q(x, z) log p 0 (z) = x q(x)KL[q(z|x)||p 0 (z)] = E q(x) KL[q(z|x)||p 0 (z)].<label>(28)</label></formula><p>For a given sample (x m , y m ),</p><formula xml:id="formula_38">I(z; x (m) ) = KL[q(z|x (m) )||p 0 (z)]<label>(29)</label></formula><p>Given X, we assume R and A are independent. We also assume a factorable prior p 0 (z) = p r0 (R)p a0 (A). This gives us:</p><formula xml:id="formula_39">I(z, x (m) ) ? KL(q(R|x (m) )||p r0 (R)) +KL(q(A|x (m) )||p a0 (A))<label>(30)</label></formula><p>Figure <ref type="figure">5</ref>: Qualitative assessment of the WIGRAPH layer. We find the most similar concepts to a given image by sorting them using cosine distances between the image and concept embeddings. Ground truth concepts are highlighted in green.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Extending WIGRAPH for Capturing Concept Interactions in Vision Tasks</head><p>In this section, we demonstrate the application of the WI-GRAPH layer in the context of image modeling with convolutional neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Dataset and Concept Pre-processing</head><p>We train a joint concept bottleneck <ref type="bibr" target="#b16">(Koh et al. 2020</ref>) model on the CUB dataset <ref type="bibr" target="#b41">(Wah et al. 2011</ref>) which comprises of 11,788 photographs of birds from 200 species, where each image has additional annotations of 312 binary concepts corresponding to bird attributes like wing color, beak shape, etc. The concept annotations in the CUB dataset are noisy, hence we adopt the method described in concept bottleneck models <ref type="bibr" target="#b16">(Koh et al. 2020</ref>) to pre-process the concept level annotations and remove noisy labels. Each concept annotation was provided by a single crowdworker who is not an avian expert, and the concepts can be quite similar to each other, e.g., some annotators might say a bird has a red belly, while other annotators might say that the belly is rufous (reddish-brown) instead. In order to deal with such issues, we aggregate instance-level concept annotations into class-level concepts via majority voting, i.e if more than 50% of crows have black wings in the data, then the class crow is considered to always have black wings. As a result, images with the same class have the same concept annotations. While this approximation is mostly true for this dataset, there are some exceptions due to visual occlusion, as well as sexual and age dimorphism.</p><p>After majority voting, we further filter out concepts that are too sparse, keeping only concepts (binary attributes) that are present after majority voting in at least 10 classes. After this filtering, we are left with 112 concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Method</head><p>Given an image x ? R d , its class label y, and concept labels c, which is a vector of k concepts; we train a concept bottleneck model with a WIGRAPH layer in order to model the interactions between the k concepts. Additionally, we encode concepts as learned high-dimensional representations in a shared image-concept latent space, such that a dynamic interaction graph A can be defined with concepts and the image as neighboring nodes. Next we use the graph convolutional operation step to update the representation of each node with its neighbors. The resulting node representations are maxpooled and the final image-concept representation is used for task classification. Overall, we learn a global interaction matrix A = {A} k?k between concepts, we use the learnt graph and concept representations to train a task prediction head for the final task.</p><p>Consider an image encoder model, f : R d ? R k , that encodes the image into a d-dimensional representation, f (x). This representation is then transformed through a shallow MLP, g : R d ? R k which maps the image from the representation space to the concept space to give g(f (x)). This is then passed through a concept prediction head layer and treated as a mutli-label classification problem for predicting concepts. We also define a MLP h : R k ? R which finally maps the representation from the concept to the task prediction space which is modeled as a multi-class classification problem. Now, in order to adapt the WIGRAPH layer for learning concept interactions, we need to formulate these concepts as high-dimensional representations in the image embedding space, such that the WIGRAPH layer can used to treat the concept embeddings as nodes and their interactions as the edges in an interaction graph. As such we define a randomly initialized embedding layer that helps us query concept representations given concept labels c. We make the assumption that the latent space for the concept representation is aligned with that of image representations. Intuitively, we are assuming that the image itself can be represented as a composition of different concept representations and hence can be embedded in the same space. This allows us to use the image representation f (x) as another node in the A graph during the subsequent graph convolution update step. Therefore during training, we dynamically compute the interaction between the image and the concept representations as their cosine similarity scaled between 0 and 1. These interactions are then used to expand the graph to give A = {A } (k+1)?(k+1) where each A ij ? {0, 1} with image as the added node and the imageconcept scaled similarities as the corresponding interactions in the original graph A. The dynamic interaction graph A , image representation f (x), and concept embeddings X c are transformed using the graph convolution update described in section 2.2 as, E = g GC (X , A ), where X = [f (x), X c ] is a concatenation of the image and concept representations corresponding to the interaction graph A . Output representa-tions E are max-pooled and passed to the prediction head p : R d ? R. The classification loss at the prediction head is used to train the WIGRAPH layer.</p><p>Overall, for each data point {x, y, c}, we get the image representation f (x) from the CNN encoder, and corresponding concept representations for each c i from the embedding layer. The interactions are computed between f (x) and each concept representation and the image itself is added to the interaction graph as the neighbor of the observed concepts, c. Similar to previous formulations, we intend to learn the unknown graph A = {A} V ?V where each A ij ? {0, 1} specifies the strength of the interaction. Through this task we also learn concept embeddings in the same latent space as the image embeddings from the CNN encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Results</head><p>We use a concept bottleneck models <ref type="bibr" target="#b16">(Koh et al. 2020</ref>) as a baseline to demonstrate the applicability of WIGRAPH for image modeling scenarios. Specifically, we train an Inception-V3 model with a concept bottleneck for the bird identification task on the Caltech-UCSD Birds-200-2011 (CUB) dataset <ref type="bibr" target="#b41">(Wah et al. 2011</ref>). With the model, we jointly train a WIGRAPH layer to learn the interactions between the concepts. WIGRAPH also learns concept embeddings in a joint image-concept space. In order to demonstrate this, we use the image representation to find most similar concepts using the cosine metric. In Figure <ref type="figure">5</ref>, we show that the model is able to encode the image very close to the ground-truth concept embeddings. Additionally, as demonstrated in <ref type="bibr" target="#b16">Koh et al. (2020)</ref>, the model is able to predict concepts very accurately at the bottleneck while maintaining a final bird classification task accuracy of 80.04%. Note that this allows us to reasonably intervene on the concepts. With test-time intervention, the model is able to improve its performance by +9.37% to 89.41%. Similar to this, WIGRAPH when used with the CNN's image representation and intervened concepts is able to achieve a prediction accuracy of 95.74%(+15.70%) at its prediction head.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: WIGRAPH layer (components inside the gray box): during inference, embeddings of words (for example, Hawaii and state) are aggregated based on their interactions using a modified Graph Convolutional operation. Here graph A was learnt from training along with the prediction task. A WIGRAPH layer is inserted into a neural text classifier right after the word embedding input layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: SST2 dataset with LSTM model. Interaction Importance Score vs word co-occurrence.</figDesc><graphic url="image-2.png" coords="11,66.68,408.74,245.13,119.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pairwise Word Interactions Cloud on LSTM model for (left) TREC, (middle) SST-2, and (right) SST-1.</figDesc><graphic url="image-3.png" coords="12,66.49,87.82,156.24,78.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>? We provide extensive empirical results showing that adding WIGRAPH layer into SOTA neural text classifiers Top ranked important words are shown in pink for BASE and blue for WIGRAPH augmented BASE model. We can tell that word attributions from WIGRAPH augmented model are easier to understand, and highlight more relevant sentiment words. This indicates WIGRAPH augmented model has better intrinsic interpretibility than BASE.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>;<ref type="bibr" target="#b6">Chen and Ji 2019;</ref><ref type="bibr" target="#b10">Erion et al. 2019;</ref><ref type="bibr" target="#b24">Molnar, Casalicchio, and Bischl 2019)</ref> focuses on aligning human judgments with generated explanations and further incorporating it into the training of the model. These methods require human annotations that are expensive to obtain and also have the risk of not aligning well with the separately trained model's decision making process.</figDesc><table /><note><p><p><ref type="bibr" target="#b30">(Ross, Hughes, and Doshi-Velez 2017;</ref> Ross and Doshi-Velez 2017;<ref type="bibr" target="#b28">Rieger et al. 2020</ref></p>) use explanations as feedback into the model to improve prediction performance. However, these heavily rely on ground-truth explanations and domain knowledge.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>.12 +1.73 46.47 +2.63 86.21 +2.63 91.16 +0.13 92.20 +1.80 91.40 +1.20 Prediction Accuracy (%). Models augmented with WIGRAPH layer predict better than BASE.</figDesc><table><row><cell></cell><cell>BASE</cell><cell cols="2">Models</cell><cell cols="2">IMDB</cell><cell>SST-1</cell><cell>SST-2</cell><cell>AG News TREC</cell><cell>Subj</cell></row><row><cell></cell><cell></cell><cell cols="2">BASE</cell><cell>88.39</cell><cell></cell><cell>43.84</cell><cell>83.74</cell><cell>91.03</cell><cell>90.40</cell><cell>90.20</cell></row><row><cell></cell><cell>LSTM</cell><cell cols="2">VMASK</cell><cell>90.07</cell><cell></cell><cell>44.12</cell><cell>84.35</cell><cell>92.19</cell><cell>90.80</cell><cell>91.20</cell></row><row><cell></cell><cell cols="4">BASE WIGRAPH 90BERT 91.88 VMASK 93.04</cell><cell></cell><cell>51.63 51.36</cell><cell>92.15 92.26</cell><cell>92.05 94.24</cell><cell>97.40 97.00</cell><cell>96.40 96.40</cell></row><row><cell></cell><cell></cell><cell cols="7">WIGRAPH 92.48 +0.60 52.49 +0.86 92.59 +0.44 92.72 +0.67 97.40 +0.00 96.60 +0.20</cell></row><row><cell></cell><cell></cell><cell cols="2">BASE</cell><cell>89.87</cell><cell></cell><cell>55.20</cell><cell>94.73</cell><cell>93.46</cell><cell>96.2</cell><cell>96.00</cell></row><row><cell></cell><cell>RoBERTa</cell><cell cols="2">VMASK</cell><cell>90.02</cell><cell></cell><cell>54.21</cell><cell>93.47</cell><cell>93.47</cell><cell>96.0</cell><cell>96.50</cell></row><row><cell></cell><cell></cell><cell cols="7">WIGRAPH 90.10 +0.23 55.52 +0.32 94.75 +0.02 93.52 +0.06 96.60 +0.20</cell><cell>96.40 +0.40</cell></row><row><cell></cell><cell></cell><cell cols="2">BASE</cell><cell>86.96</cell><cell></cell><cell>51.31</cell><cell>90.50</cell><cell>93.34</cell><cell>97.20</cell><cell>96.20</cell></row><row><cell></cell><cell>distilBERT</cell><cell cols="2">VMASK</cell><cell>87.00</cell><cell></cell><cell>48.01</cell><cell>89.02</cell><cell>93.81</cell><cell>95.20</cell><cell>95.00</cell></row><row><cell></cell><cell></cell><cell cols="5">WIGRAPH 88.32 +1.36 50.81</cell><cell cols="2">90.77 +0.22 93.85 +0.51 97.40 +0.20 96.30 +0.10</cell></row><row><cell>Dataset</cell><cell cols="3">Train/Dev/Test C</cell><cell>V</cell><cell>L</cell><cell></cell><cell></cell></row><row><cell>sst1</cell><cell cols="5">8544/1101/2210 5 17838 50</cell><cell></cell><cell></cell></row><row><cell>sst2</cell><cell cols="2">6920/872/1821</cell><cell cols="3">2 16190 50</cell><cell></cell><cell></cell></row><row><cell>imdb</cell><cell cols="2">20K/5K/25K</cell><cell cols="3">2 29571 250</cell><cell></cell><cell></cell></row><row><cell cols="3">AG News 114K/6K/7.6K</cell><cell cols="3">4 21838 50</cell><cell></cell><cell></cell></row><row><cell>TREC</cell><cell cols="2">5000/452/500</cell><cell>6</cell><cell>8026</cell><cell>15</cell><cell></cell><cell></cell></row><row><cell>Subj</cell><cell cols="3">8000/1000/1000 2</cell><cell>9965</cell><cell>25</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note><p><p><p>Summary of datasets we use in experiments: number of classes (C), vocabulary size (V ) and sentence length (L).</p><ref type="bibr" target="#b35">(Schulz et al. 2020;</ref><ref type="bibr" target="#b3">Bang et al. 2019</ref></p>) utilized the information bottleneck principle to generate post-hoc explanations by highlighting important features while suppressing unimportant ones. Differently, we incorporate the information bottleneck in model training to make model prediction behavior more interpretable.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>In our experiments, we compare WIGRAPH to a BASE model without WIGRAPH layer and to a BASE model augmented by the VMASK layer. (Chen and Ji 2020) proposed VMASK layer for improving NLP models' intrinsic interpretability, though this layer does not consider word interactions. AOPCs (%) obtained from results using LIME and SampleShapley to interpret the base, WIGRAPH-based models and the baseline VMASK across four SOTA models, and over six different datasets.</figDesc><table><row><cell>Evaluation Metrics: We use three types of evaluations to</cell></row><row><cell>compare WIGRAPH with baselines. (a) Prediction accuracy:</cell></row><row><cell>this is to measure if NLP models augmented with WIGRAPH</cell></row><row><cell>layer predict well. (b) To compare different models' inter-</cell></row></table><note><p><p><p>pretability, we will apply two post-hoc attribution techniques: LIME(Ribeiro, Singh, and Guestrin 2016) and SampleShapley</p><ref type="bibr" target="#b17">(Kononenko et al. 2010</ref></p>) on model predictions. The resulting feature attribution outputs will be evaluated using explanation faithfulness scores like AOPCs (details in Section (4.3)).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Global Interaction Analysis: Average Post Hoc Interaction Occlusion Score when using top K scoring interactions for prediction. We compare to WIGRAPH-NOA that sets A to identity matrix.</figDesc><table><row><cell cols="2">Methods</cell><cell>Models</cell><cell cols="4">IMDB SST-1 SST-2 AG News TREC Subj</cell></row><row><cell>LSTM</cell><cell></cell><cell cols="2">WIGRAPH-NOA 88.53 WIGRAPH-topA 88.84</cell><cell cols="3">45.70 83.96 91.07 45.82 84.48 90.91</cell><cell>91.00 91.27</cell><cell>90.30 90.58</cell></row><row><cell>BERT</cell><cell></cell><cell cols="2">WIGRAPH-NOA 85.62 WIGRAPH-topA 85.67</cell><cell cols="3">51.31 89.18 90.79 51.52 89.02 90.47</cell><cell>96.40 97.04</cell><cell>95.90 95.97</cell></row><row><cell cols="2">RoBERTa</cell><cell cols="2">WIGRAPH-NOA 89.02 WIGRAPH-topA 90.02</cell><cell cols="3">52.10 91.52 90.13 53.84 92.51 91.50</cell><cell>95.2 96.00</cell><cell>95.50 96.20</cell></row><row><cell cols="2">distilBERT</cell><cell cols="2">WIGRAPH-NOA 85.08 WIGRAPH-topA 86.32</cell><cell cols="3">47.10 86.82 90.08 47.25 87.00 90.10</cell><cell>95.00 96.40</cell><cell>95.20 96.00</cell></row><row><cell>Models</cell><cell></cell><cell>Method</cell><cell cols="4">IMDB SST-1 SST-2 AG News TREC Subj</cell></row><row><cell>LSTM</cell><cell cols="2">VMASK WIGRAPH-A-R</cell><cell cols="3">89.42 45.96 85.11 88.2 43.00 83.21</cell><cell>92.00 91.78</cell><cell>93.48 92.50 91.15 91.60</cell></row><row><cell></cell><cell></cell><cell>WIGRAPH-A</cell><cell cols="3">88.12 44.26 84.78</cell><cell>92.89</cell><cell>93.03 91.50</cell></row><row><cell></cell><cell></cell><cell>VMASK</cell><cell cols="3">92.90 51.95 92.32</cell><cell>93.72</cell><cell>96.68 98.00</cell></row><row><cell>BERT</cell><cell cols="5">WIGRAPH-A-R 90.50 52.77 92.43</cell><cell>90.10</cell><cell>96.68 97.90</cell></row><row><cell></cell><cell></cell><cell>WIGRAPH-A</cell><cell cols="3">91.88 50.50 92.89</cell><cell>92.00</cell><cell>96.02 97.60</cell></row><row><cell></cell><cell></cell><cell>VMASK</cell><cell cols="3">87.00 53.68 93.35</cell><cell>94.05</cell><cell>96.68 97.50</cell></row><row><cell>RoBERTa</cell><cell cols="5">WIGRAPH-A-R 83.12 52.01 93.46</cell><cell>93.91</cell><cell>95.38 97.30</cell></row><row><cell></cell><cell></cell><cell>WIGRAPH-A</cell><cell cols="3">86.20 53.41 93.81</cell><cell>94.55</cell><cell>95.50 97.20</cell></row><row><cell></cell><cell></cell><cell>VMASK</cell><cell cols="3">88.24 48.86 90.14</cell><cell>94.20</cell><cell>93.58 95.80</cell></row><row><cell>distilBERT</cell><cell cols="3">WIGRAPH-A-R 88.14</cell><cell>49.5</cell><cell>90.14</cell><cell>93.01</cell><cell>93.14 95.50</cell></row><row><cell></cell><cell></cell><cell>WIGRAPH-A</cell><cell cols="3">89.12 49.05 91.17</cell><cell>94.00</cell><cell>95.58 97.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Validation Prediction Accuracy</figDesc><table><row><cell>Interaction Importance</cell><cell>Score ?(?)</cell></row><row><cell></cell><cell>Word-word co-occurrence</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Ablations analysis for TREC and SST-2 datasets on distilbert model regarding : IoS scores and AOPC from LIME post-hoc explanation model for ablations WIGRAPH-A and WIGRAPH-A-R.</figDesc><table><row><cell></cell><cell></cell><cell>Metric</cell><cell>Models</cell><cell>TREC SST-2</cell></row><row><cell></cell><cell></cell><cell>IoS</cell><cell cols="2">WIGRAPH-A-R-topA 96.20 WIGRAPH-A-topA 96.40</cell><cell>86.90 87.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell>BASE</cell><cell>67.63</cell><cell>42.25</cell></row><row><cell></cell><cell></cell><cell>AOPC</cell><cell>VMASK WIGRAPH-A-R</cell><cell>63.14 61.52</cell><cell>39.77 36.22</cell></row><row><cell></cell><cell></cell><cell></cell><cell>WIGRAPH-A</cell><cell>68.74</cell><cell>44.33</cell></row><row><cell cols="4">Dataset WIGRAPH-A-R WIGRAPH-A VMASK</cell></row><row><cell>SST2</cell><cell>-0.0095</cell><cell>-0.1182</cell><cell>-0.0064</cell></row><row><cell>SST1</cell><cell>-0.0101</cell><cell>-0.1059</cell><cell>-0.0054</cell></row><row><cell>SUBJ</cell><cell>-0.0137</cell><cell>-0.0912</cell><cell>-0.0077</cell></row><row><cell>TREC</cell><cell>-0.0139</cell><cell>-0.1016</cell><cell>-0.0118</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Correlation with co occurrence statistics: We summarize these observations using correlation statistics: WI-GRAPH has a lower correlation of the global word importance score E q [x t |x t ] with word frequency(Column WIGRAPH-R) than when it does not account for interactions (Column</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>These attribution interpretations were generated by LIME (Ribeiro, Singh, and Guestrin 2016) and use BERT-base model on SST-2 dataset to explain two models' predictions.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00410</idno>
		<title level="m">Deep variational information bottleneck</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Towards robust interpretability with self-explaining neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07538</idno>
		<title level="m">Towards Robust Interpretability with Self-Explaining Neural Networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06918</idno>
		<title level="m">Explaining a black-box using deep variational information bottleneck approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">e-snli: Natural language inference with natural language explanations</title>
		<author>
			<persName><forename type="first">O.-M</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9539" to="9549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Improving the Explainability of Neural Sentiment via Data Augmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.04225</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00667</idno>
		<title level="m">Learning Variational Word Masks to Improve the Interpretability of Neural Text Classifiers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generating hierarchical explanations on text classification via feature interaction detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02015</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07814</idno>
		<title level="m">Learning to explain: An information-theoretic perspective on model interpretation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On attribution of recurrent neural network predictions via additive decomposition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Erion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Janizek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturmfels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.10670</idno>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="383" to="393" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Learning explainable models using attribution priors</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Relational knowledge: the foundation of higher cognition</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Halford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="497" to="505" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<title level="m">Long short-term memory</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Concept bottleneck models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mussmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5338" to="5348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An efficient explanation of individual classifications using game theory</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kononenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2010-01">2010. Jan</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2002: The 19th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Erion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03888</idno>
		<title level="m">Consistent individualized feature attribution for tree ensembles</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4765" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.00770</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Quantifying model complexity via functional decomposition for better post-hoc interpretability</title>
		<author>
			<persName><forename type="first">C</forename><surname>Molnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Casalicchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="193" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Beyond word importance: Contextual decomposition to extract interactions from LSTMs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Murdoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05453</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Comparing automatic and human evaluation of local explanations for text classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1069" to="1078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<idno>arXiv preprint cs/0506075</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interpretations are useful: penalizing explanations to align neural networks with prior knowledge</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Murdoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05770</idno>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015. 2016. 2020</date>
			<biblScope unit="page" from="8116" to="8126" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09404</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03717</idno>
		<title level="m">Right for the right reasons: Training differentiable models by constraining their explanations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Evaluating the visualization of what a deep neural network has learned</title>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2660" to="2673" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01427</idno>
		<title level="m">A simple neural network module for relational reasoning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sixt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Landgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00396</idno>
		<title level="m">Restricting the flow: Information bottlenecks for attribution</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Murdoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05337</idno>
		<title level="m">Hierarchical interpretations for neural network predictions</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
		<idno>arXiv preprint physics/0004057</idno>
		<title level="m">The information bottleneck method</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Information Theory Workshop (ITW)</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2015">2015. 2015</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Graph attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds-200-2011 dataset</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Gerven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14545</idno>
		<title level="m">Explainable deep learning: A field guide for the uninitiated</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Character-level Convolutional Networks for Text Classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In NIPS. 1. has_wing_pattern::multi-colored</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m">perching-like</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m">cone 10. has_size::small_(5_-_9_in</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m">Top 10 Most Similar Concepts</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m">cone</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
