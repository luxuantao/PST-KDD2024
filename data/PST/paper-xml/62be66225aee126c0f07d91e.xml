<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On-Device Training Under 256KB Memory</title>
				<funder>
					<orgName type="full">Qualcomm</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel</orgName>
				</funder>
				<funder>
					<orgName type="full">Amazon</orgName>
				</funder>
				<funder>
					<orgName type="full">Ford</orgName>
				</funder>
				<funder>
					<orgName type="full">Google</orgName>
				</funder>
				<funder ref="#_fwdC4JB">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-30">30 Jun 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei-Ming</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wei-Chen</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Song</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mit-Ibm</forename><surname>Watson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Lab</surname></persName>
						</author>
						<title level="a" type="main">On-Device Training Under 256KB Memory</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-30">30 Jun 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2206.15472v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>On-device training enables the model to adapt to new data collected from the sensors by fine-tuning a pre-trained model. However, the training memory consumption is prohibitive for IoT devices that have tiny memory resources. We propose an algorithm-system co-design framework to make on-device training possible with only 256KB of memory. On-device training faces two unique challenges:</p><p>(1) the quantized graphs of neural networks are hard to optimize due to mixed bitprecision and the lack of normalization; (2) the limited hardware resource (memory and computation) does not allow full backward computation. To cope with the optimization difficulty, we propose Quantization-Aware Scaling to calibrate the gradient scales and stabilize quantized training. To reduce the memory footprint, we propose Sparse Update to skip the gradient computation of less important layers and sub-tensors. The algorithm innovation is implemented by a lightweight training system, Tiny Training Engine, which prunes the backward computation graph to support sparse updates and offload the runtime auto-differentiation to compile time. Our framework is the first practical solution for on-device transfer learning of visual recognition on tiny IoT devices (e.g., a microcontroller with only 256KB SRAM), using less than 1/100 of the memory of existing frameworks while matching the accuracy of cloud training+edge deployment for the tinyML application VWW <ref type="bibr" target="#b19">[21]</ref>. Our study enables IoT devices to not only perform inference but also continuously adapt to new data for on-device lifelong learning. * indicates equal contributions.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>On-device training allows us to adapt the pre-trained model to newly collected sensory data after deployment. By training and adapting locally on the edge, the model can learn to continuously improve its predictions and perform lifelong learning and user customization. By bringing training closer to the sensors, it also helps to protect user privacy when handling sensitive data (e.g., healthcare). However, on-device training on tiny edge devices is extremely challenging and fundamentally different from cloud training. Tiny IoT devices (e.g., microcontrollers) typically have a limited SRAM size like 256KB. Such a small memory budget is hardly enough for the inference of deep learning models <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b58">60]</ref>, let alone the training, which requires extra computation for the backward and extra memory for intermediate activation <ref type="bibr" target="#b17">[19]</ref>. On the other hand, modern deep training frameworks (e.g., PyTorch <ref type="bibr" target="#b55">[57]</ref>, TensorFlow <ref type="bibr" target="#b2">[4]</ref>) are usually designed for cloud servers and require a large memory footprint (&gt;300MB) even when training a small model (e.g., MobileNetV2-w0.35 <ref type="bibr" target="#b59">[61]</ref>) with batch size 1. The huge gap (&gt;1000?) makes it impossible to run on tiny IoT devices. Furthermore, devices like microcontrollers are bare-metal and do not have an operational system and the runtime support needed by existing training frameworks. Therefore, we need to jointly design the algorithm and the system to enable tiny on-device training.</p><p>Deep learning training systems like PyTorch <ref type="bibr" target="#b55">[57]</ref>, TensorFlow <ref type="bibr" target="#b2">[4]</ref>, JAX <ref type="bibr" target="#b9">[11]</ref>, MXNet <ref type="bibr" target="#b15">[17]</ref>, etc. do not consider the tight resources on edge devices. Edge deep learning inference frameworks like TVM <ref type="bibr" target="#b16">[18]</ref>, </p><formula xml:id="formula_0">+</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.8x 2077x</head><p>Figure 1. Algorithm and system co-design reduces the training memory from 303MB (PyTorch) to 149KB with the same transfer learning accuracy, leading to 2077? reduction. The numbers are measured with MobilenetV2-w0.35 <ref type="bibr" target="#b59">[61]</ref>, batch size 1 and resolution 128?128. It can be deployed to a microcontroller with 256KB SRAM.</p><p>TF-Lite [3], NCNN <ref type="bibr" target="#b1">[2]</ref>, etc. provide a slim runtime, but lack the support for back-propagation. There are low-cost efficient transfer learning algorithms like training only the final classifier layer, biasonly update <ref type="bibr" target="#b11">[13]</ref>, etc. However, existing training system can not realize the theoretical saving into measured saving. The downstream accuracy of such update schemes is also low (Figure <ref type="figure" target="#fig_5">8</ref>).</p><p>In this paper, we aim to bridge the gap and enable tiny on-device training with algorithm-system co-design. We investigate tiny on-device training and find two unique challenges: (1) the model is quantized on edge devices. A real quantized graph is difficult to optimize due to mixed-precision tensors and the lack of Batch Normalization layers <ref type="bibr" target="#b32">[34]</ref>; <ref type="bibr" target="#b1">(2)</ref> the limited hardware resource (memory and computation) of tiny hardware does not allow full back-propagation, whose memory usage can easily exceed the SRAM of microcontrollers by more than an order of magnitude. Only updating the last layer leads to poor accuracy (Figure <ref type="figure" target="#fig_5">8</ref>). To cope with the optimization difficulty, we propose Quantization-Aware Scaling (QAS) to automatically scale the gradient of tensors with different bit-precisions, which effectively stabilizes the training and matches the accuracy of the floatingpoint counterpart (Section 2.1). QAS is hyper-parameter free and no tuning is required. To reduce the memory footprint of the full backward computation, we propose Sparse Update to skip the gradient computation of less important layers and sub-tensors. We developed an automated method based on contribution analysis to find the best update scheme under different memory budgets (Section 2.2). Finally, we propose a lightweight training system, Tiny Training Engine (TTE) , to assist with the algorithm innovation (Section 2.3). TTE is based on code generation; it offloads the auto-differentiation to the compile-time to greatly cut down the runtime overhead. It also supports advanced graph optimization like graph pruning and reordering to support sparse updates, achieving measured memory saving and speedup.</p><p>Our framework is the first solution to actually enable tiny on-device training of convolutional neural networks under 256KB memory budget. (1) Our solution enables the parameter update not only for the classifier but also for the backbone, which provides a high transfer learning accuracy (Figure <ref type="figure" target="#fig_5">8</ref>). For tinyML application VWW <ref type="bibr" target="#b19">[21]</ref>, our on-device finetuned model matches the accuracy of cloud training+edge deployment, and surpasses the common requirement of tinyML (MLPerf Tiny <ref type="bibr" target="#b6">[8]</ref>) by 9%.</p><p>(2) Our system-algorithm co-design scheme effectively reduces the memory footprint. As shown in Figure <ref type="figure">1</ref>, the proposed techniques greatly reduce the memory usage by more than 1000? compared to cloud training frameworks, and more than 100? compared to the best edge training framework we can find (MNN <ref type="bibr" target="#b34">[36]</ref>).</p><p>(3) Our framework also greatly accelerates training, reducing the per-iteration time by more than 20? compared to dense update and vanilla system design (Figure <ref type="figure">9</ref>), leading to significant energy saving and promoting practical usage. (4) We deployed our training system to a microcontroller STM32F746 to demonstrate the feasibility. Our study suggests that tiny IoT devices can not only perform inference but also adapt to new data and enjoy lifelong learning capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>Neural networks usually need to be quantized to fit the limited memory of edge devices <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b33">35]</ref>. For a fp32 linear layer y fp32 = W fp32 x fp32 + b fp32 , the int8 quantized counterpart is:</p><formula xml:id="formula_1">?int8 = cast2int8[s fp32 ? ( Wint8 xint8 + bint32 )],<label>(1)</label></formula><p>where ? denotes the tensor being quantized to fixed-point numbers, and s is a floating-point scaling factor to project the results back into int8 range. We compute the gradient update for the weights as:</p><formula xml:id="formula_2">W int8 = cast2int8( Wint8 -? ? G W),</formula><p>where ? is the learning rate, and G W is the gradient of the weights. After applying the gradient update, we round the weights back to 8-bit integers. QAS stabilizes the W / G ratio and helps optimization. For example, in the highlighted area, the ratios of the quantized model fluctuate dramatically in a zigzag pattern (weight, bias, weight, bias, ...); after applying QAS, the pattern stabilizes and matches the fp32 counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Optimizing Real Quantized Graphs</head><p>Unlike fine-tuning floating-point model on the cloud, training with a real quantized graph * is difficult: the quantized graph has tensors of different bit-precisions (int8, int32, fp32, shown in Equation <ref type="formula" target="#formula_1">1</ref>) and lacks Batch Normalization <ref type="bibr" target="#b32">[34]</ref> layers (fused), leading to unstable gradient update. Gradient scale mismatch. When optimizing a quantized graph, the accuracy is lower compared to the floating-point counterpart. We hypothesize that the quantization process distorts the gradient update. To verify the idea, we plot the ratio between weight norm and gradient norm (i.e., W / G ) for each tensor at the beginning of the training on the CIFAR dataset <ref type="bibr" target="#b39">[41]</ref> in Figure <ref type="figure" target="#fig_0">2</ref>. The ratio curve is very different after quantization: (1) the ratio is much larger (could be addressed by adjusting the learning rate); ( <ref type="formula" target="#formula_3">2</ref>) the ratio has a different pattern after quantization. Take the highlighted area (red box) as an example, the quantized ratios have a zigzag pattern, differing from the floating-point curve.</p><p>If we use a fixed learning rate for all the tensors, then the update speed of each tensor would be very different compared to the floating-point case, leading to inferior accuracy. We empirically find that adaptive-learning rate optimizers like Adam <ref type="bibr" target="#b35">[37]</ref> cannot fully address the issue (Section 3.2). Quantization-aware scaling (QAS). To address the problem, we propose a hyper-parameter-free learning rate scaling rule, QAS. Consider a 2D weight matrix of a linear layer W ? R c1?c2 , where c 1 , c 2 are the input and output channel. To perform per-tensor quantization ? , we compute a scaling rate s W ? R, such that W's largest magnitude is 2 7 -1 = 127:</p><formula xml:id="formula_3">W = s W ? (W/s W ) quantize ? s W ? W, G W ? s W ? G W ,<label>(2)</label></formula><p>The process (roughly) preserves the mathematical functionality during the forward (Equation <ref type="formula" target="#formula_1">1</ref>), but it distorts the magnitude ratio between the weight and its corresponding gradient:</p><formula xml:id="formula_4">W / G W ? W/s W / s W ? G W = s -2 W ? W / G .<label>(3)</label></formula><p>We find that the weight and gradient ratios are off by s -2 W , leading to the distorted pattern in Figure <ref type="figure" target="#fig_0">2</ref>: (1) the scaling factor is far smaller than 1, making the weight-gradient ratio much larger; (2) weights and biases have different data type (int8 vs. int32) and thus have scaling factors of very different magnitude, leading to the zigzag pattern. To solve the issue, we propose Quantization-Aware Scaling (QAS) by compensating the gradient of the quantized graph according to Equation 3:</p><formula xml:id="formula_5">G W = G W ? s 2 W , Gb = Gb ? s 2 W ? s 2 x = Gb ? s 2 (4)</formula><p>where s 2 X is the scaling factor for quantizing input x (a scalar following <ref type="bibr" target="#b33">[35]</ref>, note that s = s W ? s x in Equation <ref type="formula" target="#formula_1">1</ref>). We plot the W / G curve with QAS in Figure <ref type="figure" target="#fig_0">2</ref> (int8+scale). After scaling, the gradient ratios match the floating-point counterpart. It also improves transfer learning accuracy (Table <ref type="table">1</ref>), matching the accuracy of the floating-point counterpart without incurring memory overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Memory-Efficient Sparse Update</head><p>Though QAS makes optimizing a quantized model possible, updating the whole model (or even the last several blocks) requires a large amount of memory, which is not affordable for the tinyML setting. We propose to sparsely update the layers and the tensors.  For bias update, the accuracy generally goes higher as more layers are updated, but plateaus soon. (b) For updating the weight of a specific layer, the later layers appear to be more important; the first point-wise conv (pw1) in an inverted bottleneck block <ref type="bibr" target="#b59">[61]</ref> appears to be more important; and the gains are bigger with more channels updated. (c) The automated selection based on contribution analysis is effective: the actual downstream accuracy shows a positive correlation with ?acc.</p><p>Sparse layer/tensor update. Pruning techniques prove to be quite successful for achieving sparsity and reducing model size <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b48">50]</ref>. Instead of pruning weights for inference, we "prune" the gradient during backpropagation, and update the model sparsely. Given a tight memory budget, we skip the update of the less important parameters to reduce memory usage and computation cost. We consider updating a linear layer y = Wx+b (similar analysis applies to convolutions). Given the output gradient G y from the later layer, we can compute the gradient update by</p><formula xml:id="formula_6">G W = f 1 (G y , x) and G b = f 2 (G y ).</formula><p>Notice that updating the biases does not require saving the intermediate activation x, leading to a lighter memory footprint [13]  ? ; while updating the weights is more memory-intensive but also more expressive. For hardware like microcontrollers, we also need an extra copy for the updated parameters since the original ones are stored in read-only FLASH <ref type="bibr" target="#b46">[48]</ref>. Given the different natures of updating rules, we consider the sparse update rule in three aspects (Figure <ref type="figure">3</ref>): (1) Bias update: how many layers should we backpropagate to and update the biases (bias update is cheap, we always update the biases if we have backpropagated to a layer). (2) Sparse layer update: select a subset of layers to update the corresponding weights. (3) Sparse tensor update: we further allow updating a subset of weight channels to reduce the cost.</p><p>However, finding the right sparse update scheme under a memory budget is challenging due to the large combinational space. For MCUNet <ref type="bibr" target="#b46">[48]</ref> model with 43 convolutional layers and weight update ratios from {0, 1/8, 1/4, 1/2, 1}, the combination is about 10 30 , making exhaustive search impossible. Automated selection with contribution analysis. We propose to automatically derive the sparse update scheme by contribution analysis. We find the contribution of each parameter (weight/bias) to the downstream accuracy. Given a convolutional neural network with l layers, we measure the accuracy improvement from (1) biases: the improvement of updating last k biases b l , b l-1 , ..., b l-k+1 (bias-only update) compared to only updating the classifier, defined as ?acc b[:k] ; (2) weights: the improvement of updating the weight of one extra layer W i (with a channel update ratio r) compared to bias-only update, defined as ?acc Wi,r . An example of the contribution analysis can be found in Figure <ref type="figure" target="#fig_2">4</ref> (MCUNet on Cars <ref type="bibr" target="#b38">[40]</ref> dataset; please find more results in appendix Section G). After we find ?acc b[:k] and ?acc Wi (1 ? k, i ? l), we solve an optimization problem to find:</p><formula xml:id="formula_7">k * , i * , r * = max k,i,r (?acc b[:k] + i?i,r?r ?acc Wi,r ) s.t. Memory(k, i, r) ? constraint, (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>where i is a collection of layer indices whose weights are updated, and r is the corresponding update ratios (1/8, 1/4, 1/2, 1). Intuitively, by solving this optimization problem, we find the combination of (#layers for bias update, the subset of weights to update), such that the total contribution are Need remake the fusio maximized while the memory overhead does not exceed the constraint. The problem can be efficiently solved with evolutionary search (see Section E). Here we assume that the accuracy contribution of each tensor (?acc) can be summed up. Such approximation is quite effective (Figure <ref type="figure" target="#fig_2">4</ref>(c)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tiny Training Engine (TTE)</head><p>The theoretical saving from real quantized training and sparse update does not translate to measured memory saving in existing deep learning frameworks, due to the redundant runtime and the lack of graph pruning. We co-designed an efficient training system, Tiny Training Engine (TTE), to transform the above algorithms into slim binary codes (Figure <ref type="figure">5</ref>). Compile-time differentiation and code generation. TTE offloads the auto-differentiation from the runtime to the compile-time, generating a static backward graph which can be pruned and optimized (see below) to reduce the memory and computation. TTE is based on code generation: it compiles the optimized graphs to executable binaries on the target hardware, which minimizes the runtime library size and removes the need for host languages like Python (typically uses Megabytes of memory). Backward graph pruning for sparse update. We prune the redundant nodes in the backward graph before compiling it to binary codes. For sparse layer update, we prune away the gradient nodes of the frozen weights, only keeping the nodes for bias update. Afterwards, we traverse the graph to find unused intermediate nodes due to pruning (e.g., saved input activation) and apply dead-code elimination (DCE) to remove the redundancy. For sparse tensor update, we introduce a sub-operator slicing mechanism to split a layer's weights into trainable and frozen parts; the backward graph of the frozen subset is removed. Our compiler translates the sparse update algorithm into measured memory saving, reducing the training memory by 7-18? without losing accuracy (Figure <ref type="figure">9(a)</ref>). Operator reordering. The execution order of different operations affects the life cycle of tensors and the overall memory footprint. This has been well-studied for inference <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b43">45]</ref> but not for training due to the extra complexity. Traditional training frameworks usually derive the gradients of all the trainable parameters before applying the update. Such a practice leads to a significant memory waste for storing the gradients. By reordering operators, we can immediately apply the gradient update to a specific tensor (in-place update) before back-propagating to earlier layers, so that the gradient can be released. As such, we trace the dependency of all tensors (weights, gradients, activation) and reorder the operators, so that some operators can be fused to reduce memory footprint (by 1.6-3.4?, Figure <ref type="figure">9</ref>(a)). We provide an example in Figure <ref type="figure">6</ref> to reflect the memory saving from reordering.</p><p>Table <ref type="table">1</ref>. Updating real quantized graphs (int8) with SGD is difficult: the transfer learning accuracy falls behind the floating-point counterpart (fp32), even with adaptive learning rate optimizers like Adam <ref type="bibr" target="#b35">[37]</ref> and LARS <ref type="bibr" target="#b67">[69]</ref>. QAS helps to bridge the accuracy gap without memory overhead (slightly higher due to randomness). The numbers are for updating the last two blocks of MCUNet-5FPS <ref type="bibr" target="#b46">[48]</ref>   <ref type="bibr" target="#b46">[48]</ref> (the 5FPS ImageNet model, backbone 23M MACs, 0.48M Param). We pre-trained the models on ImageNet <ref type="bibr" target="#b21">[23]</ref> and perform post-training quantization <ref type="bibr" target="#b33">[35]</ref>. The quantized models are fine-tuned on downstream datasets to evaluate the transfer learning capacity. We perform the training and memory/latency measurement on a microcontroller STM32F746 (320KB SRAM, 1MB Flash) using a single batch size. To faster obtain the accuracy statistics on multiple downstream datasets, we simulate the training results on GPUs, and we verified that the simulation obtains the same level of accuracy compared to training on microcontrollers. Please refer to the the appendix (Section D) for detailed training hyper-parameters. We also provide a video demo of deploying our training system on microcontroller in the appendix (Section A).</p><p>Datasets. We measure the transfer learning accuracy on multiple downstream datasets and report the average accuracy <ref type="bibr" target="#b36">[38]</ref>. We follow <ref type="bibr" target="#b11">[13]</ref> to use a set of vision datasets including Cars <ref type="bibr" target="#b38">[40]</ref>, CIFAR-10 [41], CIFAR-100 <ref type="bibr" target="#b39">[41]</ref>, CUB <ref type="bibr" target="#b66">[68]</ref>, Flowers <ref type="bibr" target="#b53">[55]</ref>, Food <ref type="bibr" target="#b8">[10]</ref>, and Pets [56]  ? . We fine-tuned the models on all these datasets for 50 epochs following <ref type="bibr" target="#b11">[13]</ref>. We also include VWW dataset <ref type="bibr" target="#b19">[21]</ref>, a widely used benchmark for tinyML applications. We train on VWW for 10 epochs following <ref type="bibr" target="#b46">[48]</ref>. We used resolution 128 for all datasets and models for a fair comparison.</p><p>Memory estimation. The memory usage of a computation graph is related to its implementation <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b45">47]</ref>. We provide two settings for memory measurement: (1) analytic profiling: we count the size of extra tensors required for backward computation, including the saved intermediate activation, binary truncation task, and the updated weights. The size is implementation-agnostic. It is used for a fast profiling; (2) on-device profiling: we measure the actual memory usage when running model training on an STM32F746 MCU (320KB SRAM, 1MB Flash). We used TinyEngineV2 <ref type="bibr" target="#b45">[47]</ref> as the backend and 2?2 patch-based inference <ref type="bibr" target="#b45">[47]</ref> for the initial stage to reduce the forward peak memory.</p><p>The measured memory determines whether a solution can be deployed on the hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Results</head><p>Quantization-aware scaling (QAS) addresses the optimization difficulty. We fine-tuned the last two blocks (simulate low-cost fine-tuning) of MCUNet to various downstream datasets (Table <ref type="table">1</ref>). With momentum SGD, the training accuracy of the quantized model (int8) falls behind the floatingpoint counterpart due to the optimization difficulty. Adaptive learning rate optimizers like Adam <ref type="bibr" target="#b35">[37]</ref> can improve the accuracy but are still lower than the fp32 fine-tuning results; it also costs 3? memory consumption due to second-order momentum, which is not desired for tinyML settings. LARS <ref type="bibr" target="#b67">[69]</ref> cannot converge well on most datasets despite extensive hyper-parameter tuning (over both learning rate and the "trust coefficient"). We hypothesize that the aggressive gradient scaling  Sparse update obtains better accuracy at lower memory. We compare the performance of our searched sparse update schemes with two baseline methods: fine-tuning only biases of the last k layers; fine-tuning weights and biases of the last k layers. For each configuration, we measure the average accuracy on the 8 downstream datasets and the analytic extra memory usage. We also compare with a simple baseline by only fine-tuning the classifier. As shown in Figure <ref type="figure" target="#fig_5">8</ref>, the accuracy of classifier-only update is low due to the limited learning capacity. Updating the classifier alone is not enough; we also need to update the backbone. Bias-only update outperforms classifier-only update but the accuracy quickly plateaus and does not improve even more biases are tuned. For updating last k layers, the accuracy generally goes higher as more layers are tuned; however, it has a very large memory footprint. Take MCUNet as an example, updating the last two blocks leads to an extra memory surpassing 256KB, making it infeasible for IoT devices/microcontrollers. Our sparse update scheme can achieve higher downstream accuracy at a much lower memory cost: compared to updating last k layers, sparse update can achieve higher downstream accuracy at 4.5-7.5? smaller memory overhead. We also measure the highest accuracy achievable by updating the last k layers ? as the baseline upper bound (denoted as "upper bound"). Interestingly, our sparse update achieves a better downstream accuracy compared to the baseline best statistics. We hypothesize that the sparse update scheme alleviates over-fitting or makes momentum-free optimization easier.</p><p>Matching cloud training accuracy for tinyML. Remarkably, the downstream accuracy of our on-device training has matched or even surpassed the accuracy of cloud-trained results on tinyML application VWW <ref type="bibr" target="#b19">[21]</ref>. Our framework uses 206KB measured SRAM while achieving 89.1% top-1 accuracy for on-device training (we used gradient accumulation for the VWW dataset; see the appendix Section D for details). The result is higher than the accuracy of the same model reported by the state-of-the-art solution MCUNet (88.7%, trained on cloud and deployed to MCU). Both settings transfer the ImageNet pre-trained model to VWW. The on-device accuracy is far above the common requirement for tinyML (&gt;80% by MLPerf Tiny <ref type="bibr" target="#b6">[8]</ref>) and surpassed the results of industry solution TF-Lite Micro+MobileNetV2 (86.2% <ref type="bibr" target="#b46">[48]</ref> under 256KB, inference-only, no training support).</p><p>Tiny Training Engine: memory saving. We measure the training memory of three models on STM32F746 MCU to compare the memory saving from TTE. We measure the peak SRAM usage under three settings: general full update, sparse update, and sparse update with TTE graph reordering (Figure <ref type="figure">9</ref>(a)). The sparse update effectively reduces peak memory by 7-10? compared to the  The weight and activation memory cost of updating each layer of MCUNet (analytic). We find that the activation cost is high for the starting layers; the weight cost is high for the later layers; the overall memory cost is low for the middle layers. (b) Dissecting the sparse update scheme: we update the biases of the last 22 layers due to its low activation cost. For weight update, we update some middle layers due to its low memory cost, and update partial channels of the two later layers since they are important for accuracy (Figure <ref type="figure" target="#fig_2">4</ref>). full update thanks to the graph pruning mechanism, while achieving the same or higher transfer learning accuracy (compare the data points connected by arrows in Figure <ref type="figure" target="#fig_5">8</ref>). The memory is further reduced with operator reordering, leading to 22-28? total memory saving. With both techniques, the training of all 3 models fits 256KB SRAM. We also compare the memory saving of reordering under different update schemes on MCUNet (Figure <ref type="figure" target="#fig_5">8</ref>(b), indicated by different accuracy levels). Reordering consistently reduces the peak memory for different sparse update schemes of varying learning capacities. Tiny Training Engine: faster training. We further measure the training latency per image on the STM32F746 MCU with three settings: full update with TF-Lite Micro kernels, sparse update with TF-Lite Micro kernels, and sparse update with TTE kernels (Figure <ref type="figure">9(c)</ref>). Notice that TF-Lite does not support training; we just used the kernel implementation to measure latency. By graph optimization and exploiting multiple compiler optimization approaches (such as loop unrolling and tiling), our sparse update + TTE kernels can significantly enhance the training speed by 21-23? compared to the full update + TF-Lite Micro kernels, leading to energy saving and making training practical. Note that TF-Lite with full update leads to OOM, so we report the projected latency according to the average speed of each op type (marked in dashed columns).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Studies and Analysis</head><p>Dissecting update schedules. We visualize the update schedule of the MCUNet <ref type="bibr" target="#b46">[48]</ref> model searched under 100KB extra memory (analytic) in Figure <ref type="figure" target="#fig_6">10</ref> (lower subfigure (b), with 10 classes). It updates the biases of the last 22 layers, and sparsely updates the weights of 6 layers (some are sub-tensor update). The initial 20 layers are frozen and run forward only. To understand why this scheme makes sense, we also plot the memory cost from activation and weight when updating each layer in the upper subfigure (a). We see a clear pattern: the activation cost is high for the initial layers; the weight cost is high for the ending layers; while the total memory cost is low when we update the middle layers (layer index <ref type="bibr" target="#b16">[18]</ref><ref type="bibr" target="#b17">[19]</ref><ref type="bibr" target="#b18">[20]</ref><ref type="bibr" target="#b19">[21]</ref><ref type="bibr" target="#b20">[22]</ref><ref type="bibr" target="#b21">[23]</ref><ref type="bibr" target="#b22">[24]</ref><ref type="bibr" target="#b23">[25]</ref><ref type="bibr" target="#b24">[26]</ref><ref type="bibr" target="#b25">[27]</ref><ref type="bibr" target="#b26">[28]</ref><ref type="bibr" target="#b27">[29]</ref><ref type="bibr" target="#b28">[30]</ref>. The update scheme matches the memory pattern: to skip the initial stage of high activation memory, we only update biases of the later stage of the network; we update the weights of 4 intermediate layers due to low overall memory cost; we also update the partial weights of two later layers (1/8 and 1/4 weights) due to their high contribution to the downstream accuracy (Figure <ref type="figure" target="#fig_2">4</ref>). Interestingly, all the updated weights are from the first point-wise convolution in each inverted residual block <ref type="bibr" target="#b59">[61]</ref> as they generally have a higher contribution to accuracy (the peak points on the zigzag curve in Figure <ref type="figure" target="#fig_2">4(b)</ref>).</p><p>Effectiveness of contribution analysis. We verify if the update scheme search based on contribution analysis is effective. We collect several data points during the search process (the update scheme and the search criteria, i.e., the sum of ?acc). We train the model with each update scheme to get the average accuracy on the downstream datasets (the real optimization target) and plot the comparison in Figure <ref type="figure" target="#fig_2">4(c)</ref>. We observe a positive correlation, indicating the effectiveness of the search.</p><p>Sub-channel selection. Similar to weight pruning, we need to select the subset of channels for subtensor update. We update the last two blocks of the MCUNet <ref type="bibr" target="#b46">[48]</ref> model and only 1/4 of the weights for each layer to compare the accuracy of different channel selection methods (larger magnitude, smaller magnitude, and random). The results are quite similar (within 0.2% accuracy difference). Channel selection is not very important for transfer learning (unlike pruning). We choose to update the channels with a larger weight magnitude since it has slightly higher accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Efficient transfer learning. There are several ways to reduce the transfer learning cost compared to fine-tuning the full model <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b36">38]</ref>. The most straightforward way is to only update the classifier layer <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b60">62]</ref>, but the accuracy is low when the domain shift is large <ref type="bibr" target="#b11">[13]</ref>. Later studies investigate other tuning methods including updating biases <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b69">71]</ref>, updating normalization layer parameters <ref type="bibr" target="#b52">[54,</ref><ref type="bibr" target="#b24">26]</ref>, updating small parallel branches <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b31">33]</ref>, etc. These methods only reduce the trainable parameter number but lack the study on system co-design to achieve real memory savings. Most of them do not fit tinyML settings (cannot handle quantized graph and lack of BatchNorm <ref type="bibr" target="#b32">[34]</ref>).</p><p>Systems for deep learning. The success of deep learning is built on top of popular training frameworks such as PyTorch <ref type="bibr" target="#b55">[57]</ref>, TensorFlow <ref type="bibr" target="#b3">[5]</ref>, MXNet <ref type="bibr" target="#b15">[17]</ref>, JAX <ref type="bibr" target="#b9">[11]</ref>, etc. These systems usually depend on a host language (e.g. Python) and various runtimes, which brings significant overhead (&gt;300MB) and does not fit tiny edge devices. Inference libraries like TVM <ref type="bibr" target="#b16">[18]</ref>, TF-Lite [3], MNN <ref type="bibr" target="#b34">[36]</ref>, NCNN <ref type="bibr" target="#b0">[1]</ref>, TensorRT <ref type="bibr" target="#b1">[2]</ref>, and OpenVino <ref type="bibr" target="#b64">[66]</ref> provide lightweight runtime environments but do not support training (only MNN has preliminary support for full model training). None of the existing frameworks can fit tiny IoT devices with tight memory constraints.</p><p>Tiny deep learning on microcontrollers. Tiny deep learning on microcontrollers is challenging. Existing work explores model compression (pruning <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b47">49,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b49">51,</ref><ref type="bibr" target="#b68">70,</ref><ref type="bibr" target="#b44">46]</ref>, quantization <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b65">67,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b58">60,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b33">35]</ref>) and neural architecture search <ref type="bibr" target="#b70">[72,</ref><ref type="bibr" target="#b71">73,</ref><ref type="bibr" target="#b63">65,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b45">47]</ref> to reduce the required resource of deep learning models. There are several deep learning systems for tinyML (TF-Micro <ref type="bibr" target="#b3">[5]</ref>, CMSIS-NN <ref type="bibr" target="#b40">[42]</ref>, TinyEngine <ref type="bibr" target="#b46">[48]</ref>, MicroTVM <ref type="bibr" target="#b16">[18]</ref>, CMix-NN <ref type="bibr" target="#b13">[15]</ref>, etc.). However, the above algorithms and systems are only for inference but not training. There are several preliminary attempts to explore training on microcontrollers <ref type="bibr" target="#b57">[59,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b62">64,</ref><ref type="bibr" target="#b61">63]</ref>. However, due to the lack of efficient algorithm and system support, they are only able to tune one layer or a very small model, while our work supports the tuning of modern CNNs for real-life applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose the first solution to enable tiny on-device training on microcontrollers under a tight memory budget of 256KB. Our algorithm system co-design solution significantly reduces the training memory (more than 100? compared to existing work) and per-iteration latency, allowing us to obtain higher downstream accuracy. Our study suggests that tiny IoT devices can not only perform inference but also continuously adapt to new data for lifelong learning.</p><p>Limitations and societal impacts. Our work achieves the first practical solution for transfer learning on tiny microcontrollers. However, our current study is limited to vision recognition with CNNs. In the future, we would like to extend to more modalities (e.g., audio) and more models (e.g., RNNs, Transformers). Our study improves tiny on-device learning, which helps to protect the privacy on sensitive data (e.g., healthcare). However, to design and benchmark our method, we experimented on many downstream datasets, leading to a fair amount of electricity consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Video Demo</head><p>We prepared a video demo showing that we can deploy our framework to a microcontroller (STM32F746, 320KB SRAM, 1MB Flash) to enable on-device learning. We adapt the MCUNet model to classify whether there is a person in front of the camera or not. The training leads to decent accuracy within the tight memory budget. Please find the demo here. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Variance of Different Runs</head><p>We notice that the variance of different runs is quite small in our experiments. Here we provide detailed information about the variance.</p><p>Firstly, if we use the same random seed for the data loader, we will get exactly the same results for multiple runs. The weight quantization process after each iteration (almost) eliminates the non-determinism from GPU training || . Therefore, we study the randomness from different random seeds in data shuffling. Here we provide the results of 3 runs in Table <ref type="table">2</ref> to show the variance. We train the MobileNetV2-w0.35 model with the sparse update scheme (searched under 100KB analytic memory usage) 3 times independently. We find the variance is very small, especially when we report the average accuracy (for most of our results): the standard derivation is only ?0.07%.</p><p>Table <ref type="table">2</ref>. The variance between different runs is small, especially when we report the average accuracy (only ?0.07%). Results obtained by training MobileNetV2-w0.35 for three times using the sparse update scheme searched under 100KB analytic memory constraint. C Real Quantized Graphs vs. Fake Quantized Graphs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head><p>One key difference between our training setting and quantization-aware training (QAT) is that we are optimizing a real quantized graph designed for efficiency, while QAT is optimizing a fake quantized graph designed for simulation. Conventionally, people can perform QAT with a fake quantized graph on the cloud, and convert it to a real quantized graph for deployment. However, to perform on-device training, we have to optimize the real quantized graph itself, which brings optimization difficulty.</p><p>|| https://developer.download.nvidia.com/video/gputechconf/gtc/2019/ presentation/s9911-determinism-in-deep-learning.pdf To help better understand the difference, we provide a figure below (Figure <ref type="figure" target="#fig_8">12</ref>) to differentiate the two types of graphs (adapted from <ref type="bibr" target="#b33">[35]</ref>). We can see several differences: 1. Firstly, the weights and biases in the real quantized graphs are fixed point integers (int8 and int32), while they are floating-point in the fake quantize graphs. For fake quantized graphs, the weights can be gradually nudged during training, while the weights in real quantized graphs need to be quantized after each optimization step, making the training more difficult.</p><p>2. The activations in real quantized graphs are of int8 range, while they are of floating-point ranges (e.g., <ref type="bibr">(-6, 6)</ref>, the same range as the floating-point model) in the fake quantized graphs (just discretized to 2 8 levels with the straight-through estimator (STE) <ref type="bibr" target="#b7">[9]</ref>).</p><p>3. Batch Normalization is available in the fake quantized graph, which makes optimization easier, but not available in the real quantized graph. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Training Setups &amp; Discussions</head><p>In this section, we introduce detailed training setups and discuss the reasons that lead to several design choices.</p><p>We used SGD optimizer+QAS for training. We set weight decay as 0 since we observed no over-fitting during experiments. This is also a common choice in transfer learning <ref type="bibr" target="#b36">[38]</ref>. We find the initial learning rate significantly affects the accuracy, so we extensively tuned the learning rate for each run to report the best accuracy. We used cosine learning rate decay and performed warm-up <ref type="bibr" target="#b26">[28]</ref> for 1 epoch on VWW and 5 epochs on other datasets. We used Ray <ref type="bibr" target="#b51">[53]</ref> for experiment launching and hyper-parameter tuning.</p><p>Data type of the classifier. During transfer learning, we usually need to randomly initialize the classifiers (or add some classes) for novel categories. Although the backbone is fully quantized for efficiency, we find that using a floating-point classifier is essential for transfer learning performance. Using a floating-point classifier is also cost-economical since the classifier consists of a very small part of the model size (0.3% for 10 classes).</p><p>We compare the results of the quantized classifier and floating-point classifier in Table <ref type="table" target="#tab_4">3</ref>. We update the last two blocks of the MCUNet model with SGD-M optimizer and QAS to measure the downstream accuracy. We find that keeping the classifier as floating-point significantly improves the downstream accuracy by 2.3% (on average) at a marginal overhead. Therefore, we use floating-point for the classifier by default. We compare the results of different batch sizes in Table <ref type="table" target="#tab_5">4</ref>, with and without momentum. Due to the extremely low efficiency of single-batch training, we only report results on datasets of a smaller size. We used SGD+QAS as the optimizer and updated the last two blocks of the MCUNet <ref type="bibr" target="#b46">[48]</ref> model. We extensively tuned the initial learning rate to report the best results. We can make two observations:</p><p>1. Firstly, momentum helps optimization for normal-batch training as expected (average accuracy 74.4% vs. 72.4%). However, it actually makes the accuracy slightly worse for the single-batch setting (71.5% vs. 72.3%). Since using momentum will double the memory requirement for updating parameters (assume we can safely quantize momentum buffer; otherwise the memory usage will be 5? larger), we will not use momentum for tinyML on-device learning. Gradient accumulation. With the above training setting, we can get a similar average accuracy compared to actual on-device training on microcontrollers. The reported accuracy on each dataset is quite close to the real on-device accuracy, with only one exception: the VWW dataset, where the accuracy is 2.5% lower. This is because VWW only has two categories (binary classification), so the information from each label is small, leading to unstable gradients. For the cases where the number of categories is small, we can add gradient accumulation to make the update more stable. We show the comparison of adapting the pre-trained MCUNet model in Table <ref type="table" target="#tab_6">5</ref>. The practice closes the accuracy gap at a small extra memory cost (11%), allowing us to get 89.1% top-1 accuracy within 256KB memory usage.</p><p>To provide a clear comparison, we do not apply gradient accumulation in our experiments by default. We find that evolutionary search can efficiently explore the search space to find a good sparse update scheme given a memory constraint. Here we provide the comparison between evolutionary search and random search in Figure <ref type="figure" target="#fig_10">13</ref>. We collect the curves when searching for an update scheme of the MCUNet-5FPS <ref type="bibr" target="#b46">[48]</ref> model under 100KB memory constraint (analytic). We find that evolutionary search has a much better sample efficiency and can find a better final solution (higher sum of ?acc) compared to random search. The search process is quite efficient: we can search for a sparse update scheme within 10 minutes based on the contribution information. Note that we use the same update scheme for all downstream datasets.</p><p>Best ?acc </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Amount of Compute</head><p>To evaluate the performance of different training schemes, we simulate the training on GPUs to measure the average accuracy on 8 downstream datasets. Thanks to the small model size (for the tinyML setting) and the small dataset size, the training cost for each scheme is quite modest: it only takes 3.2 GPU hours for training on all 8 downstream datasets (cost for one run; do not consider hyper-parameter tuning).</p><p>For the pre-training on ImageNet <ref type="bibr" target="#b21">[23]</ref>, it takes about 31.5 GPU hours (300 epochs). Note that we only need to pre-train each model once.</p><p>We performed training with NVIDIA GeForce RTX 3090 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G More Contribution Analysis Results</head><p>Here we provide the contribution analysis results of the MobileNetV2-w0.35 <ref type="bibr" target="#b59">[61]</ref> and ProxylessNAS-w0.3 <ref type="bibr" target="#b12">[14]</ref> on the Cars dataset <ref type="bibr" target="#b38">[40]</ref> (Figure <ref type="figure" target="#fig_2">14</ref> and 15). The pattern is similar to the one from the MCUNet model: the later layers contribute to the accuracy improvement more; within each block, the first point-wise convolutional layer contributes to the accuracy improvement the most. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Other Partial Update Methods That Did Not Work</head><p>During our experiments, we also considered other efficient partial update methods (apart from sparse layer/tensor update) but they did not work well. Here are a few methods we tried but failed:</p><p>1. Low-rank update. LoRA <ref type="bibr" target="#b31">[33]</ref> aims to adapt a model by adding a low-rank decomposed weight to each of the original weight matrix. It is designed for adapting large language models, but could potentially be applied here. Specifically, LoRA freezes the original weight W ? R c?c but trains a small ?W = MN, where M ? R c?c , N ? R c ?c , c &lt;&lt; c. The low-rank decomposed ?W has much fewer parameters compared to W. After training, we can merge ?W so that no extra computation is incurred: y = (W + ?W)x. However, such method does not work in our case:</p><p>1. The weights are quantized in our models. If we merge ?W and W, we will produce a new weight W = ?W + W that has the same size as W, taking up a large space on the SRAM (that is why we need the sparse tensor update).</p><p>2. Even if we can tolerate the extra memory overhead by running y = Wx + ?Wx, the ?W is randomly initialized and we empirically find that it is difficult to update a quantized weight from scratch, leading to worse performance.</p><p>2. Replacing convolutions with lighter alternatives. As shown in the contribution curves (Figure <ref type="figure" target="#fig_2">4</ref> in the main paper, Figure <ref type="figure" target="#fig_2">14</ref>, and Figure <ref type="figure">15</ref>), the first point-wise convolutional layer in each block has the highest contribution to accuracy. We tried replacing the first point-wise convolutional layer with a lighter alternative, like grouped convolutions. However, although such replacement greatly reduces the cost to update the layers, it also hinders transfer learning accuracy significantly. Therefore, we did not choose to use such modification. It also involves extra complexity by changing model architectures, which is not desired.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The quantized model has a very different weight/gradient norm ratio (i.e., W / G ) compared to the floating-point model at training time.QAS stabilizes the W / G ratio and helps optimization. For example, in the highlighted area, the ratios of the quantized model fluctuate dramatically in a zigzag pattern (weight, bias, weight, bias, ...); after applying QAS, the pattern stabilizes and matches the fp32 counterpart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 PathwaysFigure 3 .</head><label>23</label><figDesc>Figure 3. Different update paradigms of two linear layers in a deep neural network.</figDesc><graphic url="image-7.png" coords="4,134.97,164.50,84.04,60.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Contribution analysis of updating biases and weights. (a) For bias update, the accuracy generally goes higher as more layers are updated, but plateaus soon. (b) For updating the weight of a specific layer, the later layers appear to be more important; the first point-wise conv (pw1) in an inverted bottleneck block<ref type="bibr" target="#b59">[61]</ref> appears to be more important; and the gains are bigger with more channels updated. (c) The automated selection based on contribution analysis is effective: the actual downstream accuracy shows a positive correlation with ?acc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Figure 5. The workflow of our Tiny Training Engine (TTE). (a,b) Our engine traces the forward graph for a given model and derives the corresponding backward graph at compile time. The red cycles denote the gradient descent operators. (c) To reduce memory requirements, nodes related with frozen weights (colored in light blue) are pruned from backward computation. (d) To minimize memory footprint, the gradient descent operators are re-ordered to be interlaced with backward computations (colored in yellow). (e) TTE compiles forward and backward graphs using code generation and deploys training on tiny IoT devices (best viewed in colors).</figDesc><graphic url="image-86.png" coords="5,126.46,217.03,180.32,67.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Training and validation loss curves w/ and w/o QAS. QAS effectively helps convergence, leading to better accuracy. The results are from updating the last two blocks of the MCUNet model on the Cars dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure8. Sparse update can achieve higher transfer learning accuracy using 4.5-7.5? smaller extra memory (analytic) compared to updating the last k layers. For classifier-only update, the accuracy is low due to limited capacity. Bias-only update can achieve a higher accuracy but plateaus soon.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. (a) The weight and activation memory cost of updating each layer of MCUNet (analytic). We find that the activation cost is high for the starting layers; the weight cost is high for the later layers; the overall memory cost is low for the middle layers. (b) Dissecting the sparse update scheme: we update the biases of the last 22 layers due to its low activation cost. For weight update, we update some middle layers due to its low memory cost, and update partial channels of the two later layers since they are important for accuracy (Figure4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. A screenshot of our video demo.</figDesc><graphic url="image-110.png" coords="14,167.40,145.44,277.19,154.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Real quantized graphs (our optimized graph) vs. fake quantized graphs (for QAT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>2 .</head><label>2</label><figDesc>Without momentum, normal-batch training, and single-batch training lead to a similar average accuracy (72.4% vs. 72.3%), allowing us to use batched training results for evaluation. Given the above observation, we report the results of batched training without momentum by default, unless otherwise stated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Evolutionary search has a better sample efficiency and leads to a better final result compared with random search when optimizing sparse update schemes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>ProxylessFigure 14 .ProxylessFigure 15 .</head><label>1415</label><figDesc>Figure 14. Contribution analysis of updating biases and weights for MobileNetV2-w0.35<ref type="bibr" target="#b59">[61]</ref>.</figDesc><graphic url="image-121.png" coords="17,333.67,534.11,167.05,64.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>model.</figDesc><table><row><cell cols="2">Precision Optimizer</cell><cell cols="7">Accuracy (%) (MCUNet backbone: 23M MACs, 0.48M Param ) Cars CF10 CF100 CUB Flowers Food Pets VWW</cell><cell>Avg Acc.</cell></row><row><cell>fp32</cell><cell>SGD-M</cell><cell>56.7</cell><cell>86.0</cell><cell>63.4</cell><cell>56.2</cell><cell>88.8</cell><cell>67.1 79.5</cell><cell>88.7</cell><cell>73.3</cell></row><row><cell></cell><cell>SGD-M</cell><cell>31.2</cell><cell>75.4</cell><cell>54.5</cell><cell>55.1</cell><cell>84.5</cell><cell>52.5 81.0</cell><cell>85.4</cell><cell>64.9</cell></row><row><cell>int8</cell><cell>Adam [37] LARS [69]</cell><cell>54.0 5.1</cell><cell>84.5 64.8</cell><cell>61.0 39.5</cell><cell>58.5 9.6</cell><cell>87.2 28.8</cell><cell>62.6 80.1 46.5 39.1</cell><cell>86.5 85.0</cell><cell>71.8 39.8</cell></row><row><cell></cell><cell cols="2">SGD-M+QAS 55.2</cell><cell>86.9</cell><cell>64.6</cell><cell>57.8</cell><cell>89.1</cell><cell>64.4 80.9</cell><cell>89.3</cell><cell>73.5</cell></row><row><cell cols="2">3 Experiments</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>3.1 Setups</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">Training. We used three popular tinyML models in our experiments: MobileNetV2 [61] (width</cell></row><row><cell cols="10">multiplier 0.35, backbone 17M MACs, 0.25M Param), ProxylessNAS [14] (width multiplier 0.3,</cell></row><row><cell cols="5">backbone 19M MACs, 0.33M Param), MCUNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Measured peak memory and latency: (a) Sparse update with TTE graph optimization can reduce the measured peak memory by 22-28? for different models, making training feasible on tiny edge devices. (b) Graph optimization consistently reduces the peak memory for different sparse update schemes (denoted by different average transfer learning accuracies). (c) Sparse update with TTE operators achieves 21-23? faster training speed compared to the full update with TF-Lite Micro operators, leading to less energy usage. Note: for sparse update, we choose the config that achieves the same accuracy as full update.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">full update</cell><cell></cell><cell cols="3">sparse update</cell><cell></cell><cell></cell><cell cols="3">w/o reorder</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">TF-Lite, full (projected, OOM)</cell></row><row><cell>Peak Mem (KB)?</cell><cell>0 1250 2500 3750 5000</cell><cell cols="7">28x smaller sparse update + reorder 4226 420 149 140 425 3362 4510 537 24x smaller smaller 201 22x</cell><cell>0 175 350 525 700</cell><cell cols="3">w/ reorder 156 486 511 178 2.9x MCUNet 3.1x</cell><cell>537</cell><cell>201 2.8x</cell><cell></cell><cell>0 3000 6000 9000 12000 15000 Latency (ms)?</cell><cell cols="6">403 TF-Lite, sparse 4111 457 3448 faster faster TTE, sparse 583 13398 5607 21x 23x faster 8501 23x 10523</cell></row><row><cell></cell><cell></cell><cell cols="2">MbV2</cell><cell cols="2">Proxyless</cell><cell cols="2">MCUNet</cell><cell></cell><cell>Acc.:</cell><cell cols="5">72.0% 73.4% 75.1%</cell><cell></cell><cell></cell><cell></cell><cell>MbV2</cell><cell></cell><cell cols="2">Proxyless</cell><cell>MCUNet</cell></row><row><cell></cell><cell></cell><cell cols="6">(a) Peak memory vs. models</cell><cell></cell><cell cols="7">(b) Peak memory vs. schemes</cell><cell></cell><cell></cell><cell cols="5">(c) Training latency vs. models</cell></row><row><cell cols="23">0 15 30 45 60 75 Figure 9. 0 Memory (KB) (a) per-layer memory usage high activation memory 60 (b) sparse update scheme (high activation cost) not update bias/forward only sparse layer update (low memory cost) activation weight low memory cost high weight memory 1/8 update bias (low activation cost) sparse tensor update (high acc) 1/4 1 Ratio 1/2 W 0 b always update the classifier</cell></row><row><cell></cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell>10</cell><cell>12</cell><cell>14</cell><cell>16</cell><cell>18</cell><cell>20</cell><cell>22</cell><cell cols="2">24</cell><cell>26</cell><cell>28</cell><cell>30</cell><cell>32</cell><cell>34</cell><cell>36</cell><cell>38</cell><cell>40</cell><cell>42</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Layer Index</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Keeping the classifier as floating-point greatly improves the downstream accuracy. For on-device training on microcontrollers, we can only fit batch size 1 due to the tight memory constraint. However, single-batch training has very low efficiency when simulated on GPUs since it cannot leverage the hardware parallelism, making experiments slow. We study the performance gap between single-batch training and normal-batch training (batch size 128) to see if we can use the latter as an approximation.</figDesc><table><row><cell>fp32</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Accuracy (%)</cell><cell></cell><cell></cell><cell>Avg</cell></row><row><cell>classifier</cell><cell cols="7">Cars CF10 CF100 CUB Flowers Food Pets VWW</cell><cell>Acc.</cell></row><row><cell></cell><cell>50.8</cell><cell>86.1</cell><cell>62.7</cell><cell>56.8</cell><cell>82.5</cell><cell>61.7 80.8</cell><cell>87.8</cell><cell>71.2</cell></row><row><cell></cell><cell>55.2</cell><cell>86.9</cell><cell>64.6</cell><cell>57.8</cell><cell>89.1</cell><cell>64.4 80.9</cell><cell>89.3</cell><cell>73.5</cell></row><row><cell cols="4">Single-batch training &amp; momentum.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Momentum helps transfer learning with batch size 128, but not with batch size 1; without momentum, we can use the normal-batch training results as an approximation for single-batch training. Results obtained by updating the last two blocks of MCUNet<ref type="bibr" target="#b46">[48]</ref> with SGD+QARS.</figDesc><table><row><cell>Batch size</cell><cell cols="2">Momentum Mem Cost</cell><cell cols="5">Accuracy (%) Cars CUB Flowers Pets VWW</cell><cell>Avg Acc.</cell></row><row><cell>128 (GPU simulate)</cell><cell>0.9 0</cell><cell>2? 1?</cell><cell>55.2 47.8</cell><cell>57.8 57.2</cell><cell>89.1 87.3</cell><cell>80.9 80.8</cell><cell>89.3 88.8</cell><cell>74.4 72.4</cell></row><row><cell>1 (tinyML)</cell><cell>0.9 0</cell><cell>2? 1?</cell><cell>47.8 51.1</cell><cell>54.8 56.2</cell><cell>88.5 88.7</cell><cell>80.5 79.3</cell><cell>86.2 86.0</cell><cell>71.5 72.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Gradient accumulation helps the optimization on datasets with a small category number. Numbers obtained by training with batch size 1, the same setting as on microcontrollers.</figDesc><table><row><cell>model</cell><cell cols="2">accumulate grad SRAM VWW accuracy</cell></row><row><cell>MCUNet-5FPS</cell><cell>178KB 206KB</cell><cell>86.6% 89.1%</cell></row><row><cell cols="2">E Evolutionary Search vs. Random Search</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>* Note that this is contrary to the fake quantization graph widely used in quantization-aware training<ref type="bibr" target="#b33">[35]</ref>. Please refer to Section C for details.? For simplicity. We actually used per-channel quantization<ref type="bibr" target="#b33">[35]</ref> and the scaling factor is a vector of size c2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>?  If we update many layers, the intermediate activation could consume a large memory<ref type="bibr" target="#b17">[19]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>? Note that fine-tuning the entire model does not always lead to the best accuracy. We grid search for the best k on Cars dataset: k =36 for MobileNetV2, 39 for ProxylessNAS, 12 for MCUNet, and apply it to all datasets.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="funder">National Science Foundation (NSF)</rs>, <rs type="institution">MIT-IBM Watson AI Lab</rs>, <rs type="programName">MIT AI Hardware Program</rs>, <rs type="funder">Amazon</rs>, <rs type="funder">Intel</rs>, <rs type="funder">Qualcomm</rs>, <rs type="funder">Ford</rs>, <rs type="funder">Google</rs> for supporting this research.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_fwdC4JB">
					<orgName type="program" subtype="full">MIT AI Hardware Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Ncnn : A high-performance neural network inference computing framework optimized for mobile platforms</title>
		<ptr target="https://github.com/Tencent/ncnn" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Nvidia tensorrt, an sdk for high-performance deep learning inference</title>
		<ptr target="https://developer.nvidia.com/tensorrt" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandelion</forename><surname>Man?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Ordering chaos: Memory-aware scheduling of irregularly wired neural networks for edge devices</title>
		<author>
			<persName><forename type="first">Jinwon</forename><surname>Byung Hoon Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><forename type="middle">Menjay</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Pai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><surname>Esmaeilzadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02369</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Micronets: Neural network architectures for deploying tinyml applications on commodity microcontrollers</title>
		<author>
			<persName><forename type="first">Colby</forename><surname>Banbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuteng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramon</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmish</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dibakar</forename><surname>Gope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">Janapa</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Whatmough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">Janapa</forename><surname>Colby R Banbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Holleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Kanter</surname></persName>
		</author>
		<author>
			<persName><surname>Lokhmotov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04821</idno>
		<title level="m">Benchmarking tinyml systems: Challenges and direction</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>L?onard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Food-101-mining discriminative components with random forests</title>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="446" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">JAX: composable transformations of Python+NumPy programs</title>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">James</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Necula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skye</forename><surname>Wanderman-Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic end-to-end deployment of real-world dnns on low-cost iot mcus</title>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Burrello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelo</forename><surname>Garofalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazareno</forename><surname>Bruschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Tagliavini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Conti</surname></persName>
		</author>
		<author>
			<persName><surname>Dory</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1253" to="1268" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Tinytl: Reduce activations, not trainable parameters for efficient on-device learning</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.11622</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cmix-nn: Mixed low-precision cnn library for memory-constrained edge devices</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Capotondi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuele</forename><surname>Rusci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Fariselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems II: Express Briefs</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="871" to="875" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName><forename type="first">Ken</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">{TVM}: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Jungwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swagath</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Jen</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayalakshmi</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailash</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><surname>Gopalakrishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06085</idno>
		<title level="m">Pact: Parameterized clipping activation for quantized neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rocky</forename><surname>Rhodes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05721</idno>
		<title level="m">Visual wake words dataset</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large scale fine-grained categorization and domain-specific transfer learning</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sparse: Sparse architecture search for cnns on resource-constrained microcontrollers</title>
		<author>
			<persName><forename type="first">Igor</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Whatmough</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00152</idno>
		<title level="m">Training batchnorm and only batchnorm: On the expressive power of random features in cnns</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Devnet: A deep event network for multimedia event detection and evidence recounting</title>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2568" to="2577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On-device training of machine learning models on microcontrollers with a look at federated learning</title>
		<author>
			<persName><forename type="first">Marc Monfort</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Pueyo Centelles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Information Technology for Social Good</title>
		<meeting>the Conference on Information Technology for Social Good</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="198" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><surname>Amc</surname></persName>
		</author>
		<title level="m">AutoML for Model Compression and Acceleration on Mobile Devices. In ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName><forename type="first">Yihui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Lora: Low-rank adaptation of large language models</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integerarithmetic-only inference</title>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skirmantas</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Xiaotang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiliu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianhang</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12418</idno>
		<title level="m">A universal and efficient inference engine</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="491" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Do better imagenet models transfer better?</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Liangzhen</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06601</idno>
		<title level="m">Cmsis-nn: Efficient neural network kernels for arm cortex-m cpus</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Tent: Efficient quantization of neural networks on the tiny edge with tapered fixed point</title>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Hamed F Langroudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tej</forename><surname>Karia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhireesha</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName><surname>Kudithipudi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02233</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Liberis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Dudziak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14246</idno>
		<title level="m">?nas: Constrained neural architecture search for microcontrollers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Neural networks on microcontrollers: saving memory at inference via operator reordering</title>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Liberis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05110</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Liberis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08350</idno>
		<title level="m">Differentiable network pruning for microcontrollers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Mcunetv2: Memory-efficient patch-based inference for tiny deep learning</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.15352</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Mcunet: Tiny deep learning on iot devices</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Runtime neural pruning</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<pubPlace>In NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">MetaPruning: Meta Learning for Automatic Neural Network Channel Pruning</title>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning efficient convolutional networks through network slimming</title>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoumeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Resource-constrained neural architecture search on edge devices</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longfei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunye</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Network Science and Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Ray: A distributed framework for emerging {AI} applications</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Tumanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melih</forename><surname>Elibol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><surname>Michael I Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="561" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">K for the price of 1: Parameter-efficient multi-task and transfer learning</title>
		<author>
			<persName><forename type="first">Pramod</forename><surname>Kaushik Mudrakarta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Omkar M Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3498" to="3505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Tinyol: Tinyml with online-learning on microcontrollers</title>
		<author>
			<persName><forename type="first">Darko</forename><surname>Haoyu Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Anicic</surname></persName>
		</author>
		<author>
			<persName><surname>Runkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Memory-driven mixed low precision quantization for enabling deep network inference on microcontrollers</title>
		<author>
			<persName><forename type="first">Manuele</forename><surname>Rusci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Capotondi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Benini</surname></persName>
		</author>
		<editor>MLSys</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">MobileNetV2: Inverted Residuals and Linear Bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Globe2train: A framework for distributed ml model training using iot devices across the globe</title>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Sudharsan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">G</forename><surname>Breslin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Intizar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename></persName>
		</author>
		<ptr target="SmartWorld/SCALCOM/UIC/ATC/IOP/SCI" />
	</analytic>
	<monogr>
		<title level="m">2021 IEEE SmartWorld, Ubiquitous Intelligence &amp; Computing, Advanced &amp; Trusted Computing, Scalable Computing &amp; Communications, Internet of People and Smart City Innovation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Train++: An incremental ml model training algorithm to create self-learning iot devices</title>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Sudharsan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">G</forename><surname>Breslin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Intizar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename></persName>
		</author>
		<ptr target="SmartWorld/SCALCOM/UIC/ATC/IOP/SCI" />
	</analytic>
	<monogr>
		<title level="m">2021 IEEE SmartWorld, Ubiquitous Intelligence &amp; Computing, Advanced &amp; Trusted Computing, Scalable Computing &amp; Communications, Internet of People and Smart City Innovation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Platform-Aware Neural Architecture Search for Mobile</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Mnasnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">HAQ: Hardware-Aware Automated Quantization with Mixed Precision</title>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Caltech-ucsd birds 200</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-201</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Caltech</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Scalpel: Customizing dnn pruning to the underlying hardware parallelism</title>
		<author>
			<persName><forename type="first">Jiecao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lukefahr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Palframan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Dasika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reetuparna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Mahlke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="548" to="560" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Ben Zaken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno>CoRR, abs/2106.10199</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Neural Architecture Search with Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning Transferable Architectures for Scalable Image Recognition</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
