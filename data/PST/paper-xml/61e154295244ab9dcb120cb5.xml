<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ProteinBERT: A universal deep-learning model of protein sequence and function</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nadav</forename><surname>Brandes</surname></persName>
							<email>nadav.brandes@mail.huji.ac.il</email>
						</author>
						<author>
							<persName><forename type="first">Dan</forename><surname>Ofer</surname></persName>
							<email>dan.ofer@mail.huji.ac.il</email>
							<affiliation key="aff1">
								<orgName type="institution">Medtronic Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yam</forename><surname>Peleg</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Deep Trading ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nadav</forename><surname>Rappoport</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Software and Information Systems Engineering</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Faculty of Engineering Sciences</orgName>
								<orgName type="institution">Ben-Gurion University of the Negev</orgName>
								<address>
									<settlement>Beer Sheva</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michal</forename><surname>Linial</surname></persName>
							<email>michall@mail.huji.ac.il</email>
							<affiliation key="aff5">
								<orgName type="department">Department of Biological Chemistry</orgName>
								<orgName type="institution" key="instit1">The Alexander Silberman Institute of Life Sciences</orgName>
								<orgName type="institution" key="instit2">The Hebrew University of Jerusalem</orgName>
								<address>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">The Hebrew University of Jerusalem</orgName>
								<address>
									<settlement>Jerusalem</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ProteinBERT: A universal deep-learning model of protein sequence and function</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1101/2021.05.24.445464</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>transformer</term>
					<term>attention</term>
					<term>neural language model</term>
					<term>protein language model</term>
					<term>TAPE</term>
					<term>NLP</term>
					<term>self-supervised learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised deep language modeling has shown unprecedented success across natural language tasks, and has recently been repurposed to biological sequences. However, existing models and pretraining methods are designed and optimized for text analysis. We introduce ProteinBERT, a deep language model specifically designed for proteins. Our pretraining scheme consists of masked language modeling combined with a novel task of Gene Ontology (GO) annotation prediction. We introduce novel architectural elements that make the model highly efficient and flexible to very large sequence lengths. The architecture of ProteinBERT consists of both local and global representations, allowing end-to-end processing of these types of inputs and outputs. ProteinBERT obtains state-of-the-art performance on multiple benchmarks covering diverse protein properties (including protein structure, post translational modifications and biophysical attributes), despite using a far smaller model than competing deep-learning methods. Overall, ProteinBERT provides an efficient framework for rapidly training protein predictors, even with limited labeled data. Code and pretrained model weights are available at https://github.com/nadavbra/protein_bert.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>Proteins are nature's ultimate machines, found across the entire tree of life. While knowledge of protein sequences is accumulating exponentially, understanding their functions remains one of the greatest scientific challenges of our time, with numerous implications to human health. Protein sequences can be viewed as strings of amino-acid letters. As such, machinelearning methods developed for natural language and other sequences are a natural fit to predictive protein tasks <ref type="bibr" target="#b0">[1]</ref>.</p><p>Modern deep neural network architectures specifically designed for sequences (such as BERT <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>), combined with pretraining on massive datasets, have led to a revolution in automated text analysis <ref type="bibr" target="#b3">[4]</ref>. The attention-based Transformer architecture in particular has shown astounding performance over a wide range of benchmarks across many domains <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>At the heart of these successes are self-supervised and transfer learning. According to the transfer-learning paradigm, a model is first pre-trained on one task, and then fine-tuned on other downstream tasks of interest <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. Assuming that the pretraining and downstream tasks are somehow related (e.g. both require understanding texts in the same language), pretraining can help the model learn useful representations for the downstream tasks. In self-supervised pretraining, labels are automatically generated, allowing models to learn from enormous, unlabeled datasets <ref type="bibr" target="#b9">[10]</ref>. A common example of self-supervised learning is language modeling, where a model (typically a deep neural network) learns language structure by filling missing parts in a text (which have been hidden with a special mask token) or reconstructing corrupted text <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Fine-tuning, on the other hand, is typically supervised and requires labeled data. The transfer-learning paradigm has allowed predictive models to achieve substantial performance gains across numerous benchmarks, especially in tasks where labeled data is scarce <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Most sequence-based language models (e.g. BERT <ref type="bibr" target="#b1">[2]</ref>, ULMFiT <ref type="bibr" target="#b10">[11]</ref>, XLNet <ref type="bibr" target="#b14">[15]</ref>, ELECTRA <ref type="bibr" target="#b15">[16]</ref>) have been designed for processing natural languages (with a bias towards English). Thus, their architectures and pretraining tasks may not be optimal for proteins, which, despite many structural similarities, have different properties from human language <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref>. Most notably, proteins do not have clear-cut multi-letter building blocks (such as words and sentences). Moreover, proteins are more variable in length than sentences, and show many interactions between distant positions (due to their 3D structure). To this day, protein research is still dominated by classical sequence-similarity methods (such as BLAST <ref type="bibr" target="#b17">[18]</ref> and hidden Markov models <ref type="bibr" target="#b18">[19]</ref>), in contrast to domains such as computer vision which have become dominated by deep learning. A few recent studies have pretrained deep neural language models on protein sequences (e.g. ESM <ref type="bibr" target="#b19">[20]</ref>, TAPE-Transformer [21], UDSMProt <ref type="bibr" target="#b20">[22]</ref>, UniRep <ref type="bibr" target="#b21">[23]</ref>) <ref type="bibr" target="#b22">[24]</ref><ref type="bibr" target="#b23">[25]</ref><ref type="bibr" target="#b24">[26]</ref>. Such works usually import existing architectures and tasks from the natural language domain, without taking advantage of the unique characteristics of proteins.</p><p>Here, we present ProteinBert, a new deep-learning model designed for protein sequences. We improve upon the classic Transformer/BERT architecture, and introduce a novel pretraining task of predicting protein functions. We pretrained ProteinBert on ~106M proteins (representing the entire known protein space) on two simultaneous tasks. The first task is bidirectional language modeling of protein sequences. The second task is Gene Ontology (GO) annotation prediction, which captures diverse protein functions <ref type="bibr" target="#b25">[27]</ref>. GO annotations are a manually curated set of ~45K terms defined at the whole-protein level, covering the entire protein space across all organisms. They cover molecular functions, biological processes and subcellular locations. Unlike classic Transformers, ProteinBERT separates local (character level) and global (whole protein level) representations, thereby supporting multitasking of both local and global tasks in a principled way. While ProteinBERT is considerably smaller and faster than existing models, it approaches or exceeds state-of-the-art performance on a diverse set of benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Protein dataset for pretraining</head><p>ProteinBERT was pretrained on ~106M proteins derived from UniProtKB/UniRef90, covering the entire tree of life <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b27">29]</ref>. UniRef90 provides a non-redundant set of protein clusters sharing at least 90% sequence identity. Each cluster is represented by a single representative protein, ensuring a relatively homogenous coverage of the protein space. For each protein, we extracted its amino-acid sequence and associated GO annotations (according to UniProtKB). We considered only the 8,943 most frequent GO annotations that occurred at least 100 times in UniRef90. Of the ~106M UniRef90 proteins, ~46M had at least one of the 8,943 considered annotations (with ~2.3 annotations per protein, on average across the ~46M proteins).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Protein benchmarks</head><p>To evaluate ProteinBERT, we tested it on nine benchmarks concerning all major facets of protein research, covering protein structure, post-translational modifications and biophysical properties (Table <ref type="table" target="#tab_0">1</ref>). Labels in these benchmarks are either local (e.g. post-translational modifications) or global (e.g. remote homology), and they are either continuous (e.g. protein stability), binary (e.g. signal peptide) or categorical (e.g. secondary structure). Notably, in local benchmarks the number of training samples is much greater than the number of protein sequences, as target labels are per residue.</p><p>Four out of nine benchmarks (secondary structure, remote homology, fluorescence and stability) were taken from TAPE (Tasks Assessing Protein Embeddings), a standardized set of benchmarks for evaluating protein sequence models <ref type="bibr">[21]</ref>. In addition, we introduce five new benchmarks (see Supplementary Methods). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence and annotation encoding</head><p>Protein sequences were encoded as sequences of integer tokens. We used 26 unique tokens representing the 20 standard amino acids, selenocysteine (U), an undefined amino-acid (X), another amino acid (OTHER), and 3 additional tokens (START, END and PAD). For each sequence, START and END tokens were added before the first amino acid and after the last amino acid, respectively. The PAD token was added to pad sequences shorter than the sequence length chosen for the minibatch.</p><p>The architecture of ProteinBERT (like all deep learning models) dictates that each minibatch has a fixed sequence length. We included the START and END tokens to help the model interpret proteins that are longer than the chosen sequence length. When encoding a protein longer than the chosen sequence length, we selected a random subsequence of the protein, leaving out at least one of its two ends. The absence of the START or END token allowed the model to recognize that it only received part of a sequence.</p><p>GO annotations of every sequence were encoded as a binary vector of fixed size <ref type="bibr" target="#b7">(8,</ref><ref type="bibr">943)</ref>, where all entries are zeros except those corresponding to GO annotation associated with the protein.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-supervised pretraining on protein sequences and annotations</head><p>To learn efficient protein representations, ProteinBERT was pretrained on protein sequences and GO annotations extracted from UniRef90. The model received corrupted inputs (protein sequences and GO annotations) and had to recover the uncorrupted data. The corruption of protein sequences was performed by randomly replacing tokens with 5% probability (i.e. keeping the original token with 95% probability, or replacing it with a uniformly-selected random token with 5% probability). Input GO annotations were corrupted by randomly removing existing annotations with 25% probability, and adding random false annotations with probability of 0.01% for each annotation not associated with the protein. For 50% of the processed proteins, we removed all input annotations altogether, to force the model to predict GO annotations from sequence alone (as was the case in all tested benchmarks). In summary, the described pretraining is a dual task, where the model has to recover both the protein sequence and its known GO annotations. The latter task is relevant to numerous domains of protein research, given the wide range of functions covered by GO terms.</p><p>To avoid learning the GO annotations of proteins in the tested benchmarks (Table <ref type="table" target="#tab_0">1</ref>), we ignored the GO annotations of proteins with at least 40% sequence similarity to any record in the test-sets of the benchmarks. To this end, we used BLASTP <ref type="bibr" target="#b38">[40]</ref> (with default settings), identifying ~600K such sequences (out of the ~106M pretraining proteins).</p><p>The loss function minimized by ProteinBERT during pretraining was a sum of the categorical cross-entropy over the protein sequences and the binary cross-entropy over the GO annotations, namely</p><formula xml:id="formula_0">â„’ = âˆ’ âˆ‘ ğ‘™ğ‘œğ‘”(ğ‘† Ì‚ğ‘–,ğ‘† ğ‘– ) ğ‘™ ğ‘–=1 âˆ’ âˆ‘ (ğ´ ğ‘— â‹… ğ‘™ğ‘œğ‘”(ğ´ Ì‚ğ‘—) + (1 âˆ’ ğ´ ğ‘— ) â‹… ğ‘™ğ‘œğ‘”(1 âˆ’ ğ´ Ì‚ğ‘—)) 8943 ğ‘—=1</formula><p>, where ğ‘™ is the sequence length, ğ‘† ğ‘– âˆˆ {1, â€¦ ,26} is the sequence's true token at position ğ‘–, ğ‘† Ì‚ğ‘–,ğ‘˜ âˆˆ [0,1] is the predicted probability that position ğ‘– has the token ğ‘˜ (for any ğ‘˜ âˆˆ {1, â€¦ ,26}), ğ´ ğ‘— âˆˆ {0,1} is the true indicator for annotation ğ‘— (for any ğ‘— âˆˆ {1, â€¦ 8943}), and ğ´ Ì‚ğ‘— âˆˆ [0,1] is the predicted probability for the protein to have annotation ğ‘—.</p><p>An important feature of ProteinBERT is sequence length flexibility. To avoid the risk of overfitting the model to a specific constant length, we periodically (every 15 minutes of training) switched the encoding length of protein sequences, using lengths of 128, 512 or 1,024 tokens.</p><p>Pretraining speed on a single GPU (Nvidia Quadro RTX 5000) was 280 protein records per second. We trained the model for 28 days over ~670M records (i.e. ~6.4 iterations over the entire training dataset of ~106M records). The trained model weights are publicly available along with our code (see below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised fine-tuning on protein benchmarks</head><p>Following pretraining, we fine-tuned and evaluated the model on a diverse set of benchmarks (Table <ref type="table" target="#tab_0">1</ref>). For all benchmarks, ProteinBERT was initialized from the same pretrained state and fine-tuned through the same protocol. Initially, all layers of the pretrained model were frozen, and only a newly added fully-connected layer was allowed to train for up to 40 epochs. Next, we unfroze all the layers and trained the model for up to 40 additional epochs. Finally, we trained the model for 1 final epoch of a larger sequence length (see Supplementary Methods). Throughout all epochs, we reduced the learning rate on plateau and applied early stopping based on an independent validation set. Model evaluation was then performed over a held-out test set. The entire fine-tuning procedure took ~14 minutes on a single GPU (on average across the nine benchmarks).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep-learning architecture</head><p>While inspired by BERT <ref type="bibr" target="#b1">[2]</ref>, the architecture of ProteinBERT is different and includes several innovations. ProteinBERT is a type of a denoising autoencoder, with corresponding inputs and outputs (Fig. <ref type="figure" target="#fig_0">1</ref>). The two inputs (and outputs) of ProteinBERT are i) protein sequences (encoded as token sequences) and ii) protein GO annotations (encoded as fixed-size binary vectors).</p><p>The architecture of the model consists of two almost parallel paths: one for local representations and the other for global representations (Fig. <ref type="figure" target="#fig_0">1</ref>). where ğ‘£ ğ‘– âˆˆ â„ ğ‘‘ ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ is the value associated with each position ğ‘– âˆˆ {1, â€¦ ğ¿} and ğ‘§ ğ‘– âˆˆ [0,1] is the amount of attention allocated to that position (satisfying ğ‘§ 1 + â‹¯ + ğ‘§ ğ¿ = 1). Like in self-attention, the value associated with each position is calculated by ğ‘£ ğ‘– = ğœ(ğ‘Š ğ‘£ ğ‘  ğ‘– ), using a parameter matrix ğ‘Š ğ‘£ âˆˆ â„ ğ‘‘ ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ Ã—ğ‘‘ ğ‘™ğ‘œğ‘ğ‘ğ‘™ and an activation function ğœ (we chose GELU). Attention values are calculated by</p><formula xml:id="formula_1">ğ‘§ 1 , â€¦ , ğ‘§ ğ¿ = ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ { âŒ©ğ‘,ğ‘˜ ğ‘– âŒª âˆš ğ‘‘ ğ‘˜ğ‘’ğ‘¦ } ğ‘–=1 ğ¿</formula><p>, based on query and key vectors ğ‘, ğ‘˜ ğ‘– âˆˆ â„ ğ‘‘ ğ‘˜ğ‘’ğ‘¦ . Notice that while the key vectors ğ‘˜ 1 , â€¦ , ğ‘˜ ğ¿ are specific to each position, the query vector ğ‘ is global. Like in self-attention, the key vectors are calculated by ğ‘˜ ğ‘– = ğ‘¡ğ‘ğ‘›â„(ğ‘Š ğ‘˜ ğ‘  ğ‘– ), using a second parameter matrix ğ‘Š ğ‘˜ âˆˆ â„ ğ‘‘ ğ‘˜ğ‘’ğ‘¦ Ã—ğ‘‘ ğ‘™ğ‘œğ‘ğ‘ğ‘™ . The global query vector is calculated by ğ‘ = ğ‘¡ğ‘ğ‘›â„(ğ‘Š ğ‘ ğ‘¥), using a third parameter matrix ğ‘Š ğ‘ âˆˆ â„ ğ‘‘ ğ‘˜ğ‘’ğ‘¦ Ã—ğ‘‘ ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ . Overall, a single-head global attention layer uses three parameter matrices fit during training, ğ‘Š ğ‘ , ğ‘Š ğ‘˜ and ğ‘Š ğ‘£ . It is also parameterized by the key dimension ğ‘‘ ğ‘˜ğ‘’ğ‘¦ (we used ğ‘‘ ğ‘˜ğ‘’ğ‘¦ = 64). A multi-head global attention layer is obtained by applying ğ‘› â„ğ‘’ğ‘ğ‘‘ğ‘  independent single-head global attention layers (each with its own parameters) and concatenating their outputs, obtaining an output of dimension ğ‘› â„ğ‘’ğ‘ğ‘‘ğ‘  â‹… ğ‘‘ ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ (we used ğ‘› â„ğ‘’ğ‘ğ‘‘ğ‘  = 4 across all 6 transformer blocks of ProteinBERT). To satisfy dimensionality constraints, ProteinBERT uses ğ‘‘ ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ = ğ‘‘ ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ ğ‘› â„ğ‘’ğ‘ğ‘‘ğ‘  = 128.</p><p>Overall, the ProteinBERT model includes 6 transformer blocks with 4 global attention heads in each block. Altogether, it includes ~16M trainable parameters, making it substantially smaller than other protein language models. For comparison, there are ~38M parameters in the TAPE Transformer [21], ~110M in BERT-base <ref type="bibr" target="#b1">[2]</ref>, and ~650M in the ESM-1b model <ref type="bibr" target="#b19">[20]</ref>.</p><p>The ProteinBERT architecture has several appealing properties. Most importantly, the entire architecture is agnostic to the length of the processed sequences, and it can be applied over sequences of any given length without changing its learned parameters (our experiments prove that the model indeed generalizes very well across different lengths). This good generality across sequence lengths is also achieved by avoiding positional embeddings used in the standard version of BERT which, in accordance with previous reports <ref type="bibr" target="#b40">[42]</ref> and our experimentation, do not always generalize well to sequence lengths longer than those present in the training data. Instead, the convolutional layers and special tokens used at the beginning and end of each sequence provide the model with information on the relative locations of positions. Due to the use of global attention rather than self-attention, the amount of computation performed by the model grows only linearly with sequence length (as opposed to quadratic growth in models with standard self-attention). This linear growth also applies to the model's memory consumption, allowing ProteinBERT to process extremely long protein sequences (of tens of thousands of amino-acids) intact. Despite this simplification, each position in the local representations and sequence outputs can still depend on the content of each other position, thanks to the alternating information flow between the local and global representations. On top of that, the wide and narrow convolutional layers allow the representation of each position to depend on a large context. By relying on convolutional and attention layers, but avoiding recurrent layers, the computation performed by the network is more efficient and stable with respect to sequence length (as there are no long-term dependencies). Notably, we did not use dropout or any other form of regularization (except for the final fully-connected layer added when fine-tuning the model, which included dropout).</p><p>When fine-tuning ProteinBERT on a labeled dataset, another layer is added to its output. The final layer is fed with a concatenation of either the local or global hidden states of the model, depending on whether the output labels are local or global. The activation used for the final layer depends on the output type (i.e. softmax activation for categorical labels, sigmoid activation for binary labels, or no activation for continuous labels). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretraining improves protein modeling</head><p>ProteinBERT was pretrained on ~106M UniRef90 records for ~6.4 epochs. We see that the language modeling loss continues to improve on the training set (i.e does not saturate), even after multiple epochs (Fig. <ref type="figure">2</ref>), in accordance with other studies <ref type="bibr" target="#b19">[20]</ref>. The GO annotations task, on the other hand, does show saturation. During pretraining, we periodically changed the sequence length used to encode the input and output protein sequences (128, 512 or 1024 tokens). We observe somewhat lower performance for the 128-token encoding, but similar for 512 and 1024.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2: Pretraining loss</head><p>Training-set loss over the two pretraining tasks: i) protein sequence language modeling, and ii) annotation recovery. Losses were evaluated with input sequence length of 128, 512 or 1,024 tokens on the first 100 batches of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ProteinBERT achieves state-of-the-art results on diverse protein benchmarks</head><p>To evaluate ProteinBERT, we used nine benchmarks covering a variety of tasks in protein research (see definitions of the benchmarks in Table <ref type="table" target="#tab_0">1</ref>; full results for all benchmarks are available in Supplementary Table <ref type="table" target="#tab_0">S1</ref>). For the four benchmarks taken from TAPE (secondary structure, remote homology, fluorescence and stability prediction), we compared our performance to other state-of-the-art sequence models which had been evaluated on the same benchmarks. Specifically, we compared against a BERT Transformer and LSTM models evaluated in TAPE [21, <ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b43">45]</ref> (Table <ref type="table" target="#tab_2">2</ref>). Notably, the compared deep learning models have ~38M parameters, in contrast to ~16M parameters in ProteinBERT. We evaluated ProteinBERT with and without pretraining, observing that pretraining has a major, positive effect on performance in all tasks. Across these benchmarks, ProteinBERT shows performance comparable, or that exceeds similar, larger models, such as the Transformer used in TAPE.</p><p>To further discern the impact of pretraining on downstream benchmark performance, we evaluated ProteinBERT following different pretraining durations. Specifically, we initiated the model from different snapshots along its pretraining and evaluated its down-stream performance after fine-tuning from these states (Fig. <ref type="figure" target="#fig_1">3</ref>). While some tasks do not benefit from pretraining, other tasks (e.g. secondary structure and remote homology) show clear gains from ever more pretraining, and do not show saturation in that improvement. This is notable given that these are among the more challenging tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ProteinBERT generalizes across protein lengths</head><p>The architecture of ProteinBERT is efficient and flexible towards different sequence lengths (i.e. the number of tokens encoding the input and output sequences). To test the model's capacity to generalize across sequence lengths, we measured the test-set performance of ProteinBERT on the 4 of 9 benchmarks that had a non-negligible number of test-set records in proteins longer than 512 tokens (Fig. <ref type="figure" target="#fig_2">4</ref>). Specifically, we required at least 25 such records, where a record comprises either an entire protein (in the case of global tasks) or a residue (in the case local tasks). We observe that in most cases ProteinBERT performs slightly worse for longer sequences, but only modestly, showing that it indeed generalizes across a very wide range of protein lengths. Moreover, the fact that in some cases longer sequences achieve better performance (e.g. 16,384-token sequences in the "Major PTMs" benchmark, or 1,024token sequences in the "Neuropeptide cleavage" benchmark) suggests that the changes in performance might be due to other factors (e.g. predicting the secondary structure of longer sequences might be an inherently more difficult task). Sequence lengths (e.g. 512, 1,024, etc.) always encode proteins of shorter lengths (e.g. a protein of 700 residues will be encoded as a 1,024-long sequence). Boxplot distributions are over the 371 pretraining snapshots used in Fig. <ref type="figure" target="#fig_1">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Understanding global attention</head><p>To demonstrate the inner workings of the global attention mechanism, we extracted the values of the 24 attention heads in ProteinBERT for two proteins selected from the test-set of the signal peptide benchmark, before and after fine-tuning the model on that task (Fig. <ref type="figure" target="#fig_3">5</ref>). The patterns of global attention are clearly distinct across different proteins, but some shared patterns exist. For example, attention head #3 in the 3rd transformer block tends to concentrate on the beginning of protein sequences, while attention head #2 in the same layer tends to concentrate on the other parts (i.e. the middle and end of sequences). Fine-tuning the model on signal peptide prediction appears to have mostly altered the last (6th) global attention layer. For example, attention head #1 in that layer changed to emphasize more the beginning of sequences. It is worth stressing that the exact attention values are dependent on the model weights obtained from training, which can change between runs. From our experience, fine-tuning tends to produce rather consistent results, but small differences are sometimes observed. Global attention values obtained for two selected proteins: Heme-binding protein 1 (Hebp1) in mouse (top), and Gamma carbonic anhydrase-like 2, mitochondrial protein (GAMMACAL2) in arabidopsis (bottom). The left panels (red colors) show the attention values obtained by the generic ProteinBERT model, after pretraining it as a language model on UniRef90 (but before fine-tuning it on any specific task). The heatmap shows the global attention values at each residue of the protein by each of the 24 attention heads of the model. The bar plot shows the total attention at each residue by summing the attention values across all attention heads. The right panels show the difference in attention values after fine-tuning ProteinBERT on the signal peptide prediction benchmark. The heatmap shows the increase (green) or decrease (purple) of attention across all positions and attention heads. The bar plot shows the total difference in attention at each residue by summing the differences across all attention heads. Notice that each attention head necessarily sums up to 100%. Accordingly, differences sum up to 0%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We have presented ProteinBERT, a new deep language model for protein sequences designed to capture local and global representations of proteins in a natural way (Fig. <ref type="figure" target="#fig_0">1</ref>). We have demonstrated the universality of the model, showing that it can be fine-tuned on a wide range of protein tasks in a matter of minutes and achieve state-of-the-art results (Table <ref type="table" target="#tab_2">2</ref>).</p><p>To pretrain ProteinBERT, we introduce a novel pretraining task of protein annotation prediction which is highly suited to proteins (unlike sentence order prediction and other natural language centric tasks <ref type="bibr" target="#b44">[46]</ref>). We argue that GO annotations <ref type="bibr" target="#b25">[27]</ref> are a sensible extension to language modeling in proteins. They are ubiquitous and available for a large portion of curated proteins (~46M of the ~106M proteins in the UniRef90 dataset). Additionally, they can teach the model about a wide range of protein functions (from subcellular localizations to pathways to biochemical roles).</p><p>Unlike previous works which included ~250M putative, redundant sequences <ref type="bibr" target="#b19">[20]</ref>, we constrained the pretraining of ProteinBERT to ~106M representative proteins taken from UniRef90 <ref type="bibr" target="#b27">[29]</ref>, out of the entire known protein space of ~215M proteins in UniProt <ref type="bibr" target="#b26">[28]</ref>. We argue that using a non-redundant set of proteins is more sensible and eliminates a lot of unnecessary bias caused by uneven sampling of the protein space, which is prevalent in the non-filtered version of UniProt. For example, there are &gt;1M proteins in UniProt from the proteome of human immunodeficiency virus 1 (HIV-1), even though the real virus contains only 9 proteins. Such redundancy reflects the abundance of sequence variations along HIV-1 evolution, and the great interest that researchers have had in this variation (compared to most other, far less studied organisms). Using a non-redundant set of proteins is also more efficient, especially when pretraining the model for less than an entire epoch (such as when searching for optimal hyper-parameter combinations).</p><p>ProteinBERT's architecture is efficient and highly scalable, allowing it to process protein sequences of any length. The same model weights conform to any sequence length, allowing it to be trained on a specific range of lengths and then generalize to other, unseen sequence lengths (Fig. <ref type="figure" target="#fig_2">4</ref>). By supporting extremely long sequences (more than tens of thousands of residues), ProteinBERT spares the complication of splitting long sequences into smaller chunks, a common practice with self-attention-based models which grow quadratically (rather than linearly) with sequence length <ref type="bibr" target="#b45">[47,</ref><ref type="bibr" target="#b46">48]</ref>. At the core of the model's flexibility is its use of global attention layers, a new architectural innovation. The compactness of global attention (relative to self-attention) also allows easier inspection of the model's attention, as all attention values (across all positions and attention heads) can be displayed as a simple 2D map (Fig. <ref type="figure" target="#fig_3">5</ref>), as opposed to the 3D map that would be required to cover all-by-all self-attention.</p><p>Compatible with the general trends in the field of language modeling <ref type="bibr" target="#b4">[5]</ref>, we observe that longer pretraining of ProteinBERT shows clear performance gains, both as a language model (Fig. <ref type="figure">2</ref>) and across many specific tasks (Fig. <ref type="figure" target="#fig_1">3</ref>, Supplementary Fig. <ref type="figure" target="#fig_0">S1</ref>). Existing works show that, other things being equal, larger models and additional pretraining computation time correlates with improved model performance <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20]</ref>. Thus, we expect larger versions of ProteinBERT (e.g. with more, wider layers) to yield additional improvements. Yet, even with the modest computing resources used in this work (a single GPU), ProteinBERT competes with state-of-the-art models (Table <ref type="table" target="#tab_2">2</ref>), providing a simple and efficient out-of-the-box solution for a wide range of protein tasks. The representations learned by the model through its are universally applicable across a wide array of tasks, making it useful for fewshot-learning tasks involving limited labelled data.</p><p>To facilitate easy usage of ProteinBERT, we provide the pretrained model as a Python package (based on TensorFlow and Keras <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b42">44]</ref>), which allows automatic downloading of a pretrained model state, fine-tuning and evaluation on labeled datasets.</p><p>By providing an effective and accessible model of protein sequence and function, we hope to expedite the adoption of deep language modeling by the protein research community and allow this new powerful tool to further push the boundaries of protein research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The ProteinBERT architecture ProteinBERT's architecture is inspired by BERT. Unlike standard Transformers, ProteinBERT supports both local (sequential) and global data. The model consists of 6 transformer blocks manipulating local (left side) and global (right side) representations. Each such block manipulates these representations by fully-connected and convolutional layers (in the case of local representations), with skip connections and normalization layers between them. The local representations affect the global representations through a global attention layer, and the global representations affect the local representations through a broadcast fully-connected layer.</figDesc><graphic url="image-1.png" coords="8,154.73,72.00,285.85,357.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The impact of pretraining on downstream tasksPerformance of fine-tuned ProteinBERT models over the 4 TAPE benchmarks as a function of pretraining amount (measured by the number of processed proteins). Similar plots for all nine benchmarks are shown in Supplementary Fig.S1.</figDesc><graphic url="image-3.png" coords="10,89.15,400.74,416.40,297.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Performance across sequence lengthsTest-set performance of fine-tuned ProteinBERT models with different input sequence lengths. Sequence lengths (e.g. 512, 1,024, etc.) always encode proteins of shorter lengths (e.g. a protein of 700 residues will be encoded as a 1,024-long sequence). Boxplot distributions are over the 371 pretraining snapshots used in Fig.3.</figDesc><graphic url="image-4.png" coords="11,87.35,299.50,420.60,148.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Global attention before and after fine-tuning on signal peptide prediction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="9,72.00,132.60,451.25,139.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Protein benchmarks Topic Benchmark Target type a Resolution # Training sequences Source</head><label>1</label><figDesc></figDesc><table><row><cell>Protein</cell><cell>Secondary</cell><cell>Categorical</cell><cell>Local</cell><cell>8,678</cell><cell>[21, 30]</cell></row><row><cell>structure</cell><cell>structure</cell><cell>(3)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Disorder</cell><cell>Binary</cell><cell>Local</cell><cell>8,678</cell><cell>[30]</cell></row><row><cell></cell><cell>Remote</cell><cell>Categorical</cell><cell>Global</cell><cell>12,312</cell><cell>[21, 31, 32]</cell></row><row><cell></cell><cell>homology</cell><cell>(1,195)</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Fold classes</cell><cell>Categorical</cell><cell>Global</cell><cell>15,680</cell><cell>[31, 32]</cell></row><row><cell></cell><cell></cell><cell>(7)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Post-</cell><cell cols="2">Signal peptide Binary</cell><cell>Global</cell><cell>16,606</cell><cell>[33]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The global representations are 2D tensors of shape ğµ Ã— ğ‘‘ ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ (using ğ‘‘ ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ = 512). In the first layers of the model, the input sequences are transformed into the local-representation 3D tensor by an embedding layer with ğ‘‘ ğ‘™ğ‘œğ‘ğ‘ğ‘™ output features (which is applied independently and identically position-wise), and the input binary annotations are transformed into the globalrepresentation 2D tensor by a fully-connected layer with ğ‘‘ ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ output features.The local and global representations are processed by a series of 6 transformer blocks with skip connections and layer normalizations between their hidden layers. Within each block, the local representation is transformed first by 1D convolutional layers, and then by a (locationwise) fully-connected layer. To allow the local representations at each position to be based on other positions at both short and remote proximity, we used both a narrow (without dilation) and a wide (with dilation rate of 5) convolutional layer. Both types of convolution layers have a kernel size of 9 and stride size of 1. Accordingly, each narrow layer has a receptive field of 9 and each wide layer has a receptive field of 41 over the previous layer, meaning that the 6th transformer block has a receptive field of 241 over the input sequence. The global representations, on the other hand, are transformed by two simple fully-connected layers per block (with normalizations between them). All the hidden fully-connected and convolutional layers of the model use GELU (Gaussian Error Linear Unit) activations<ref type="bibr" target="#b39">[41]</ref>.The only information flow between the local and global representations occurs through broadcast fully-connected layers (from the global to the local representations) and global attention layers (from the local to the global representations). The broadcast layers are fullyconnected layers that transform the ğ‘‘ ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ features of the global representation into ğ‘‘ ğ‘™ğ‘œğ‘ğ‘ğ‘™ features of the local representations, and then replicate that representation across each of the ğ¿ sequence positions.The global attention layer is a novel architectural innovation inspired by self-attention<ref type="bibr" target="#b2">[3]</ref>. While self-attention takes an input sequence and outputs another sequence by allowing each position to attend to each other position, global attention takes as input both a sequence and a global fixed-size vector and outputs a global fixed-size vector created by attending to each of the local input positions according to the global input vector. Formally, a single-head global attention layer takes as inputs a global representation vector ğ‘¥ âˆˆ â„ ğ‘‘ ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ and local representation vectors across ğ¿ positions, ğ‘  1 , â€¦ , ğ‘  ğ¿ âˆˆ â„ ğ‘‘ ğ‘™ğ‘œğ‘ğ‘ğ‘™ , and outputs a global output ğ‘¦ âˆˆ â„ ğ‘‘ ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ . Similar to self-attention, the output is calculated by ğ‘¦ = âˆ‘ ğ‘§ ğ‘– ğ‘£ ğ‘–</figDesc><table><row><cell>ğ¿ ğ‘–=1</cell></row></table><note>The local representations are 3D tensors of shape ğµ Ã— ğ¿ Ã— ğ‘‘ ğ‘™ğ‘œğ‘ğ‘ğ‘™ where ğµ is the batch size, ğ¿ is the minibatch sequence length, and ğ‘‘ ğ‘™ğ‘œğ‘ğ‘ğ‘™ is the number of channels for the local representations (we used ğ‘‘ ğ‘™ğ‘œğ‘ğ‘ğ‘™ = 128).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : TAPE benchmark results</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Structure</cell><cell>Evolutionary</cell><cell>Engineering</cell><cell></cell></row><row><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Secondary</cell><cell>Remote</cell><cell>Fluorescence</cell><cell>Stability</cell></row><row><cell></cell><cell></cell><cell>structure</cell><cell>homology</cell><cell></cell><cell></cell></row><row><cell></cell><cell>TAPE</cell><cell>0.70</cell><cell>0.09</cell><cell>0.22</cell><cell>-0.06</cell></row><row><cell></cell><cell>Transformer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Without</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pretraining</cell><cell>LSTM</cell><cell>0.71</cell><cell>0.12</cell><cell>0.21</cell><cell>0.28</cell></row><row><cell></cell><cell>ProteinBERT</cell><cell>0.70</cell><cell>0.06</cell><cell>0.65</cell><cell>0.63</cell></row><row><cell></cell><cell>TAPE</cell><cell>0.73</cell><cell>0.21</cell><cell>0.68</cell><cell>0.73</cell></row><row><cell>With</cell><cell>Transformer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>LSTM</cell><cell>0.75</cell><cell>0.26</cell><cell>0.67</cell><cell>0.69</cell></row><row><cell></cell><cell>UniRep</cell><cell>0.73</cell><cell>0.23</cell><cell>0.67</cell><cell>0.73</cell></row><row><cell></cell><cell>mLSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ProteinBERT</cell><cell>0.74</cell><cell>0.22</cell><cell>0.66</cell><cell>0.76</cell></row></table></figure>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability</head><p>Python code for ProteinBERT's architecture, pretraining and fine-tuning is open source and available at https://github.com/nadavbra/protein_bert. The repository also includes pretrained model weights and code for downloading and generating the datasets and benchmarks. ProteinBERT is implemented in TensorFlow's Keras <ref type="bibr" target="#b41">[43,</ref><ref type="bibr" target="#b42">44]</ref>.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This research was partially funded by ISF grant 2753/20 (M.L).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author contribution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interests</head><p>The authors declare no conflict of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary materials</head><p>Supplementary Table <ref type="table">S1</ref> -Full benchmark results: Test-set performance of ProteinBERT (following fine-tuning) over all nine benchmarks across 371 snapshots along the pretraining process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Figure S1</head><p>Supplementary Methods</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The language of proteins: NLP, machine learning \&amp; protein sequences</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Brandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Linial</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Struct Biotechnol J</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>arXiv Prepr arXiv181004805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<idno>arXiv Prepr arXiv170603762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<idno>arXiv Prepr arXiv200514165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Ctrl: A conditional transformer language model for controllable generation</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<idno>arXiv Prepr arXiv190905858</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<idno>arXiv Prepr arXiv191010683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Transfer learning for text classification</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv Neural Inf Process Syst</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="299" to="306" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Knowl Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<idno>arXiv Prepr arXiv200610029</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno>arXiv Prepr arXiv180106146</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Is learning the n-th thing any easier than learning the first?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="640" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<idno>arXiv Prepr arXiv190500537</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno>arXiv Prepr arXiv190608237</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>arXiv Prepr arXiv200310555</idno>
		<title level="m">Pre-training text encoders as discriminators rather than generators</title>
				<meeting><address><addrLine>Electra</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Shannon information entropy of protein sequences</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Strait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dewey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biophys J</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="148" to="155" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Basic local alignment search tool</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Altschul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mol Biol</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="page" from="403" to="410" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pfam: the protein families database</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bateman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clements</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkt1223</idno>
		<ptr target="https://doi.org/10.1093/nar/gkt1223" />
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="D222" to="230" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv Neural Inf Process Syst</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page">9689</biblScope>
			<date type="published" when="2019">2021. 2019</date>
		</imprint>
	</monogr>
	<note>Proc Natl Acad Sci</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">UDSMProt: universal deep sequence models for protein classification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Strodthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wenzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2401" to="2409" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unified rational protein engineering with sequence-based deep representation learning</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Khimulya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1315" to="1322" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modeling aspects of the language of life through transfer-learning protein sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Progen: Language modeling for protein generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<idno>arXiv Prepr arXiv200403497</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transforming the language of life: Transformer neural networks for protein prediction tasks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nambiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heflin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM International Conference on Bioinformatics</title>
				<meeting>the 11th ACM International Conference on Bioinformatics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gene ontology: tool for the unification of biology</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ashburner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Genet</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="25" to="29" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">UniProtKB/Swiss-Prot, the manually annotated section of the UniProt KnowledgeBase: how to use the entry view</title>
		<author>
			<persName><forename type="first">E</forename><surname>Boutet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lieberherr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tognolli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plant Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="23" to="54" />
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">UniRef: comprehensive and nonredundant UniProt reference clusters</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Suzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcgarvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1282" to="1288" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Critical assessment of methods of protein structure prediction (CASP)-Round XII</title>
		<author>
			<persName><forename type="first">J</forename><surname>Moult</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fidelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kryshtafovych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins Struct Funct Bioinforma</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="7" to="15" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SCOP2 prototype: a new approach to protein structure mining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andreeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Howorth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chothia</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkt1242</idno>
		<ptr target="https://doi.org/10.1093/nar/gkt1242" />
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="D310" to="314" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The SCOP database in 2020: expanded classification of representative family and superfamily domains of known protein structures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Andreeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kulesha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Murzin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="D376" to="D382" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SignalP 5.0 improves signal peptide predictions using deep neural networks</title>
		<author>
			<persName><forename type="first">Jja</forename><surname>Armenteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Tsirigos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>SÃ¸nderby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Biotechnol</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="420" to="423" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">PhosphoSitePlus, 2014: mutations, PTMs and recalibrations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hornbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="D512" to="D520" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ProFET: Feature engineering captures high-level protein functions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Linial</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="3429" to="3436" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">ASAP: A machine learning framework for local protein properties</title>
		<author>
			<persName><forename type="first">N</forename><surname>Brandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Linial</surname></persName>
		</author>
		<idno type="DOI">10.1093/database/baw133</idno>
		<ptr target="https://doi.org/10.1093/database/baw133" />
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">NeuroPID: a predictor for identifying neuropeptide precursors from metazoan proteomes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Linial</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="931" to="940" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Local fitness landscape of the green fluorescent protein</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Sarkisyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Bolotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">533</biblScope>
			<biblScope unit="page" from="397" to="401" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Global analysis of protein folding using massively parallel design, synthesis, and testing</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Rocklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Chidyausiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goreshnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="168" to="175" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Gapped BLAST and PSI-BLAST : a new generation of protein database search programs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Altschul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>SchÃ¤ffer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="3389" to="3402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno>arXiv Prepr arXiv160608415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On the relation between position information and sentence length in neural machine translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Neishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yoshinaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
				<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="328" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th $\{$USENIX$\}$ symposium on operating systems design and implementation ($\{$OSDI$\}$ 16)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">keras</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning protein sequence embeddings using information from structure</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bepler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Berger</surname></persName>
		</author>
		<idno>arXiv Prepr arXiv190208661</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<idno>arXiv Prepr arXiv190911942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<idno>arXiv Prepr arXiv200914794</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<idno>arXiv Prepr arXiv200714062</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
