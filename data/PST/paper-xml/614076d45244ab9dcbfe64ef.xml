<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CTC-Based End-To-End ASR for the Low Resource Sanskrit Language with Spectrogram Augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Ramakrishnan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical Engineering</orgName>
								<orgName type="institution">Indian Institute of Science Bengaluru</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CTC-Based End-To-End ASR for the Low Resource Sanskrit Language with Spectrogram Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/NCC52529.2021.9530162</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>connectionist temporal classification</term>
					<term>CTC</term>
					<term>ASR</term>
					<term>Sanskrit</term>
					<term>spectrogram augmentation</term>
					<term>WFST decoding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sanskrit is one of the Indian languages which fares poorly, with regard to the development of language-based tools. In this work, we build a connectionist temporal classification (CTC) based endto-end large vocabulary continuous speech recognition system for Sanskrit. To our knowledge, this is the first time an end-to-end framework is being used for automatic speech recognition in Sanskrit. A Sanskrit speech corpus with around 5.5 hours of speech data is used for training a neural network with a CTC objective. 80-dimensional mel-spectrogram together with their delta and delta-delta is used as the input features. Spectrogram augmentation techniques are used to effectively increase the amount of training data. The trained CTC acoustic model is assessed in terms of character error rate (CER) on greedy decoding. Weighted finite-state transducer (WFST) decoding is used to obtain the word level transcriptions from the character level probability distributions obtained at the output of the CTC network. The decoder WFST, which maps the CTC output characters to the words in the lexicon, is constructed by composing 3 individual finite-state transducers (FST), namely token, lexicon and grammar. Trigram models trained from a text corpus of 262338 sentences are used for language modeling in grammar FST. The system achieves a word error rate (WER) of 7.64% and a sentence error rate (SER) of 32.44% on the Sanskrit test set of 558 utterances with spectrogram augmentation and WFST decoding. Spectrogram augmentation provides an absolute improvement of 13.86% in WER.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Introduction</head><p>Automatic speech recognition (ASR) technologies require a large amount of annotated training data to work reasonably well. Recent approaches to speech recognition employ deep learning techniques and this has made the data scarcity problem more severe, as the deep learning methods are quite data hungry. It is estimated that only 1% of the languages of the world have the minimum amount of data needed to train an ASR <ref type="bibr" target="#b0">[1]</ref>. Due to this, speech recognition researchers have been focusing mainly on high resource languages like English and Mandarin till a few years back.</p><p>In recent years there has been a stronger focus on developing ASR for low resource languages <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b4">[5]</ref>. Different approaches like multilingual pre-training <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b10">[11]</ref> and data augmentation <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b14">[15]</ref> have been applied to improve the performance of ASR for low resource languages. However such advancements in technology have not been propagated to many of the low resource Indian languages. The eighth schedule of the constitution of India, lists 22 official languages, most of which are low resource in nature. Sanskrit is one among them and is considered as the second oldest language next to Tamil. It is believed to be the mother of many languages in the Indo-European family in the sense that their genesis is tracked to Sanskrit. A huge body of literature in various areas spanning from mathematics, astronomy, science, linguistics, mythology, history and mysticisms are available in this language. Sanskrit assumes enormous importance given the aforementioned considerations although it is not used widely for active communication. There are only very few attempts on building technical tools for Sanskrit <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. This motivates us to investigate on the application of some of the state-of-the-art technologies and techniques towards building a Sanskrit ASR. We believe that these efforts will aid in enhancing accessibility of the language and thus its contents to a larger population.</p><p>Conventional ASR systems consists of three submodules namely acoustic models, pronunciation models and language models. The existence of these modules give rise to the following limitations <ref type="bibr" target="#b17">[18]</ref>.</p><p>1) Each of these modules are trained independently with different objectives, which may result in the sub-optimal performance of the resulting system. 2) Preparation of acoustic models requires phonetic alignments, which needs to be obtained from some other system. For example, to train a hybrid ASR architecture with deep neural network (DNN) and hidden Markov models (HMM) <ref type="bibr" target="#b18">[19]</ref>, we need tiedtriphone state alignments which are prepared using a separate ASR architecture consisting of Gaussian mixture models (GMM) and HMMs.</p><p>3) Preparation of pronunciation models requires linguistic knowledge and are generally curated by expert linguists. This being a manual process is subjected to human errors. The above limitations have prompted the ASR community to move away from the conventional ASR systems to endto-end trained systems which map the input acoustic features directly to graphemes or word sequences. Two most popular approaches to end-to-end speech recognition are: i) attention-based encoder-decoder <ref type="bibr" target="#b19">[20]</ref> and ii) connectionist temporal classification (CTC) <ref type="bibr" target="#b20">[21]</ref>. Attention-based methods have the advantage that they do not require any conditional independence assumptions. However, the disadvantage is that they do not guarantee the monotonic alignments required in speech recognition problems. On the other hand, CTC allows only monotonic alignments but suffers from conditional independence assumptions, i.e., every output is conditionally independent of other outputs.</p><p>To utilise the monotonic alignments offered with CTC, we choose the CTC-based scheme for building a large vocabulary continuous speech recognition (LVCSR) system for Sanskrit. We propose an architecture based on residual convolutional neural networks (CNN) <ref type="bibr" target="#b21">[22]</ref> and bidirectional gated recurrent units (GRU). As the multilingual pre-training experiments require paired audio and transcriptions from other languages, we restrict ourselves to the experiments with data augmentation alone. Specifically, we use a feature augmentation technique called SpecAugment <ref type="bibr" target="#b14">[15]</ref> to effectively increase the amount of data available for training. We use greedy decoding to assess the performance of the learnt CTC acoustic model. Weighted finite-state transducers (WFST) are used for word level decoding.</p><p>The remaining part of the paper is organized as follows: Section II gives an overview of the theoretical background of the CTC-based ASR. Section III describes the Sanskrit data corpus used in this work. Section IV describes the training of the CTC acoustic model followed by the decoding approaches in section V. Performance of the system in terms of the CER and WER are stated in section VI. Conclusions of our experiments are summarised in section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. An overview of the approach</head><p>In speech recognition problem, we have to generate a sequence of output symbols (letters) given a sequence of input features. However, when to output the symbol is unknown, as the alignment of letters in the transcription to the input audio is not available. CTC overcomes this issue by computing the probability of an output sequence given the input sequence as the sum of probabilities of all possible alignments between the two.</p><p>Consider a sequence of input acoustic features X = {x 1 , x 2 , . . . , x T } and the corresponding output labels Y = {y 1 , y 2 , . . . , y U }, U ≤ T . Each of the output labels y i ∈ U, i = 1, 2, . . . , U , where U is an alphabet of size L. The alignment between X and Y is unknown. However, the alignments are monotonic in the speech recognition problem, i.e., the alignment between input and output happens to be in the same order. CTC introduces a special blank character "−" to the alphabet U. Let the modified alphabet be U = U ∪ {−}. For a given x i , i = 1, . . . , T , the CTC network gives the distribution over over all the possible output labels in U . This distribution is used to evaluate the probability of any given output sequence Y.</p><p>Consider a path Z = {z 1 , z 2 , . . . , z T } ∈ U T in the CTC output distribution over time. Define a many-to-one function F, which maps any given path Z to a label sequence Y, by collapsing repeats and then removing all the blank symbols in the path. For example, the function F maps the sequence { म,म,−,म,म,−,&lt;SPACE&gt;,−,न,न,−,ा,−,म,− } to { म,म,&lt;SPACE&gt;,न,ा,म }. Symbols separated by the blank symbol are not merged during the process of collapsing repeats. Thus the blank symbol helps to handle the repetition of letters in the orthography.</p><p>The probability of a path Z can be computed as</p><formula xml:id="formula_0">P (Z|X) = T t=1 p(z t |z 1 , z 2 , . . . , z t−1 , X)<label>(1)</label></formula><formula xml:id="formula_1">≈ T t=1 p(z t |X)<label>(2)</label></formula><p>Each term inside the product in ( <ref type="formula" target="#formula_1">2</ref>) can be obtained from the output probability distributions computed by the CTC. Now the probability of the output sequence Y can be computed by summing over all the possible paths Z that gets mapped to Y by the function F.</p><formula xml:id="formula_2">P (Y|X) = {Z|F (Z)=Y} P (Z|X)<label>(3)</label></formula><p>The summation over all the paths is achieved through dynamic programming. During the training, blank symbols are inserted between each of the labels in the output and also at the beginning and end of the output sequence. Training proceeds with the objective to maximise the probability of the correct label sequence. During the decoding, we infer the most likely label sequence Y given X.</p><p>Y * = arg max</p><formula xml:id="formula_3">Y P (Y|X)<label>(4)</label></formula><p>The approximation in (2) means that the output at any time instant is conditionally independent of the other outputs given the input. Though this assumption is rather strong, considering the benefits of the monotonic alignment property offered by CTC, we chose to build our architecture based on CTC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. Dataset preparation</head><p>We have collected around 6.5 hours of Sanskrit speech data consisting of 3395 utterances from 23 speakers. All the data were collected online, mainly from 4 sources: (a) news recordings from All India Radio (AIR) website <ref type="bibr" target="#b22">[23]</ref>, (b) video lectures from Indian Heritage Group (IHG), C-DAC, (c) video lectures from Central Sanskrit University <ref type="bibr" target="#b23">[24]</ref>, and (d) short stories read by Samskrita Bharati volunteers <ref type="bibr" target="#b24">[25]</ref>. The data is mainly read speech and lectures and are encoded in mp3 format. The collected data was converted to single channel raw wav file format with 16 kHz sampling frequency and 16 bits per sample. There are 23 speakers: 17 male and 6 female. Each audio file contains recordings from a single speaker. The corpus contains around 12250 words.</p><p>The data was randomly divided into 2 sets -train and test, with approximately 5.5 hours in the train set and 1 hour in the test set, the details of which are shown in Table <ref type="table" target="#tab_0">I</ref>. The word-level transcriptions of these utterances are saved in Unicode text format. There were 1712 new words in the test set not belonging to the training set. Due to the limited size of the dataset, we have speaker overlap between the train and test sets. We have randomly selected 6% of the training data for the validation set. Thus, 2667 utterances are used for training and 170 utterances are used for validation.</p><p>The text corpus for building the language models makes use of the wiki Sanskrit data dump <ref type="bibr" target="#b25">[26]</ref>. We have extracted text data from several Sanskrit websites as well. The extracted text is cleaned to remove unwanted characters and pre-processed to restrict the graphemes to the Devanagari Unicode symbols. There are a total of 262338 sentences in the text corpus with around 436800 unique words. We use 3-gram language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. Acoustic model training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature extraction</head><p>We use mel-spectrogram as the input features. Input wav files have a sampling rate of 16000 Hz. Window length of 25 ms and hop length of 10 ms are used for framing. Hanning window is applied to each of the frames and 1024 point short-time Fourier transform (STFT) is computed. Power spectrum is computed using the magnitude STFT and fed to a mel-filterbank consisting of 80 filters. Delta features are computed from the filterbank output using a window of 5 frames. Delta-delta features were computed from delta features in a similar manner. Filterbank features, delta and delta-delta features are stacked as 3 channels of an image and used as the input to the neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature augmentation</head><p>Data augmentation schemes help us to increase the effective amount of data available for training the neural networks. We use data augmentation at the feature-level using SpecAugment <ref type="bibr" target="#b14">[15]</ref>. SpecAugment has been shown to achieve state-of-the-art performance in LibriSpeech 960h and Swichboard datasets. We apply frequency masking and time masking in the mel-spectrogram domain with parameters 10 and 70 respectively, before the computation of delta and delta-delta features. In frequency masking, f consecutive mel frequency channels [f 0 , f 0 +f ) are masked. First f is chosen from a uniform distribution from 0 to the frequency mask parameter F , and then f 0 is chosen from [0, ν−f ), where ν is the number of mel frequency channels. In time masking, t consecutive time steps [t 0 , t 0 + t) are masked, where t is chosen from a uniform distribution from 0 to the time mask parameter T , and then t 0 is chosen from [0, τ −t), where τ is the number of time steps. Figure <ref type="figure" target="#fig_0">1</ref> shows an example of the frequency and time masking scheme in SpecAugment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network architecture</head><p>For acoustic model training we use an architecture employing CNNs and bidirectional GRUs (BiGRU) as in <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. The details of the architecture are given in Figure <ref type="figure" target="#fig_1">2</ref> is used in each stage of the network. Logarithm of the softmax function is applied to the CTC output to convert it into log-probabilities. The alphabet U consists of 13 independent vowel symbols, 12 dependent vowel symbols, 34 consonants, 4 consonants with nukta (ज़, ड़, ढ़, फ़), chandrabindu( ँ ), anusvara( ं ), visarga( ः), avagraha(ऽ), virama( ् ), ohm(ॐ), &lt;SPACE&gt; and &lt;UNK&gt;. &lt;UNK&gt; stands for any unknown grapheme symbol occuring in the input labels apart from the above listed ones. The alphabet size, L = 71. Including the blank label (−), we have 72 output classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training of the CTC acoustic model</head><p>AdamW optimiser <ref type="bibr" target="#b28">[29]</ref> is used for training. The network is trained with CTC objective <ref type="bibr" target="#b20">[21]</ref> for a maximum of 200 epochs. Early stopping, with a patience value of 30 is applied. A batch-size of 10 is employed during training. We adopt the one cycle learning rate policy <ref type="bibr" target="#b29">[30]</ref> with a maximum learning rate of 0.001. The network is implemented in PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. Decoding</head><p>We use greedy decoding <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b30">[31]</ref> to assess the classification performance of the trained CTC network. Greedy decoding does not use any linguistic information. To assess the word level performance of the ASR, WFST decoding <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b33">[34]</ref> is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Greedy decoding</head><p>In greedy decoding, the best path Z * ∈ U</p><p>T is computed in a greedy fashion by picking up the most probable letter from the CTC output distribution at each time step.</p><formula xml:id="formula_4">z * t = arg max l∈U P (z t = l|X), t = 1, 2, . . . , T.<label>(5)</label></formula><p>To obtain the decoded output, the repeats are collapsed (i.e., letters appearing successively, but not separated by the blank symbols are merged together) and thereafter blank symbols are removed.</p><formula xml:id="formula_5">Y * = F(Z * )<label>(6)</label></formula><p>Output is post-processed to remove some of the invalid combinations of graphemes. This post-processing step includes: 1) removal of multiple occurrences of viramas ( ् ), 2) removal of viramas before and after the dependent/independent vowel symbols, and 3) replacing multiple occurances of vowels with the last vowel appearing in the sequence. The output of the post-processing step is used to compute the character and word error rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. WFST Decoding</head><p>A weighted finite-state transducer (WFST) <ref type="bibr" target="#b31">[32]</ref> is a finite automata in which each state transition is associated with an input/output label pair and a weight. A path in the WFST accepts a sequence of input symbols and emits the corresponding sequence of output symbols and the weights associated with that path. WFST constructed for CTC decoding should accept a sequence of letters z t ∈ U as input and should output a sequence of words from the lexicon. Such a WFST is constructed by fusing 3 individual WFSTs as in <ref type="bibr" target="#b33">[34]</ref>.  . An example of a token FST. Input labels appear before ':' and output labels appear after ':'. Node 0 is the start node and double circled node is the end node. '-' denotes the blank label and ' ' indicates that no inputs are accepted or no outputs are emitted. This FST maps different CTC paths like "--ककक-", "कककककक", "--कककक", etc. to a single unit "क" (collapse-repeats and blank removal operations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Token FST (T ): Maps the sequence of frame level</head><p>CTC labels l ∈ U to a single character in U. This FST effectively performs the collapse-repeat and blank label removal on the input sequence. An example of token FST is shown in Figure <ref type="figure" target="#fig_3">3</ref>.</p><p>2) Lexicon FST (L): Maps the sequence of characters in U to words. Each entry in the lexicon is the representation of the word in terms of the constituent characters. An example of the lexicon FST is shown in Figure <ref type="figure">4</ref>. 3) Grammar FST (G): Encodes the permissible word sequences in Sanskrit. They are generated using the word level 3-gram language models learnt from the text corpus.  These three WFSTs are composed into a search graph which is used to find the most probable word sequence.</p><formula xml:id="formula_6">S = T o min(det(L o G))<label>(7)</label></formula><p>where o, min and det are the FST operations; composition, minimisation and determinization and S is the search graph.</p><p>During the WFST decoding, we normalise the posterior probability of the classes by their priors. The priors for the classes are computed using the label sequences with the blank symbol inserted between each successive character labels and also at the beginning and end of the sentence. For example the label sequence "एषः&lt;SPACE&gt;कः " will be converted to the sequence "-ए-ष-ः-&lt;SPACE&gt;-क-ः-" before the computation of prior. This technique has been shown to improve the performance of WFST decoding in <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. Results</head><p>Performance of the CTC acoustic model is measured using character error rate (CER) on greedy decoding. Recognition performance of the whole ASR sytem is measured using word error rate (WER) and sentence error rate (SER) on WFST decoding. Both CER and WER were computed using Levenshtein distance between the reference sequence and the predicted sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W ER</head><formula xml:id="formula_7">= S + D + I N<label>(8)</label></formula><p>where S, D and I are the number of substitutions, deletions and insertions at the word level and N is the total number of words in the reference. CER is computed in a similar manner, but with errors computed at the character level. SER is calculated as the ratio of the number of correctly decoded sentences to the total number of sentences in the test set. The results of greedy decoding on the test set are listed in Table <ref type="table" target="#tab_1">II</ref>. There are absolute improvements of ≈ 3.1% in CER and ≈ 6.7% in WER when SpecAugment is applied. The results of WFST decoding on the test set are given in Table <ref type="table" target="#tab_1">III</ref>. SpecAugment achieves an absolute improvement of 13.86% in WER and 24.73% in SER over the system without spectral augmentation, when WFST decoding is employed. To assess the performance of the system, we also compare the results with a baseline hybrid HMM/DNN system trained with feature space maximum likelihood linear regression (fMLLR) features in Kaldi <ref type="bibr" target="#b32">[33]</ref>, using the same training-validation split. This is a multilayer perceptron system with 7 hidden layers and 2048 nodes in each layer. The baseline results are listed as the third row in Table <ref type="table" target="#tab_1">III</ref>. The kaldi system still fares better, as our end to end system is limited by the amount of training data available. However, SpecAugment reduces the large gap of 15.67% between end to end CTC architecture and hybrid DNN/HMM system to just 1.81%. The limited size of the dataset may also be a factor in attaining a WER closer to the hybrid DNN-HMM system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. Conclusions</head><p>In this work, we have built a LVCSR system for Sanskrit using CTC-based end-to-end framework and spectrogram augmentation. The system achieves a WER of 7.64% and a SER of 32.44% on the Sanskrit test set with 558 utterances. The trained CTC acoustic model achieves a CER of 20.05% using greedy decoding. Since a large amount of paired data (audio and transcriptions) were not available for Sanskrit, we experimented with SpecAugment to increase the effective amount of training data. This gives an absolute improvement of 3.11% in CER on greedy decoding and an absolute improvement of 13.86% in WER on WFST decoding. We hope the results can be improved further if more training data is available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Time and frequency masking using SpecAugment. Top image is the original spectrogram and the bottom image is the augmented spectrogram.</figDesc><graphic url="image-2.png" coords="3,313.26,429.79,254.47,81.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Block diagram of our architecture for learning the CTC acoustic model illustrating an input utterance with timesteps T = 788. 80-dimensional mel-spectrogram, their delta and delta-delta are fed as 3 channels of an image. x 3 in the figure indicates that the blocks are repeated 3 times. Conv2d represents the 2D convolutional layers and FC represents the fully connected layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3</head><label>3</label><figDesc>Figure 3. An example of a token FST. Input labels appear before ':' and output labels appear after ':'. Node 0 is the start node and double circled node is the end node. '-' denotes the blank label and ' ' indicates that no inputs are accepted or no outputs are emitted. This FST maps different CTC paths like "--ककक-", "कककककक", "--कककक", etc. to a single unit "क" (collapse-repeats and blank removal operations).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. An example of a lexicon FST. Words are allowed to have an optional &lt;SPACE&gt; character at the beginning and at the end.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I</head><label>I</label><figDesc>Details of the Sanskrit dataset</figDesc><table><row><cell></cell><cell>Train</cell><cell>Test</cell></row><row><cell>Files</cell><cell>2837</cell><cell>558</cell></row><row><cell>Words</cell><cell>10541</cell><cell>2911</cell></row><row><cell>Speakers</cell><cell>23</cell><cell>22</cell></row><row><cell>Male</cell><cell>17</cell><cell>16</cell></row><row><cell>Female</cell><cell>6</cell><cell>6</cell></row><row><cell>Duration</cell><cell>5:22:12</cell><cell>1:05:56</cell></row><row><cell>Male</cell><cell>4:09:19</cell><cell>0:50:32</cell></row><row><cell>Female</cell><cell>1:12:52</cell><cell>0:15:23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table II</head><label>II</label><figDesc>Results of greedy decoding on Sanskrit test set</figDesc><table><row><cell>Architecture</cell><cell>CER</cell><cell>WER</cell></row><row><cell>Proposed network</cell><cell>23.16</cell><cell>78.74</cell></row><row><cell>Proposed network + SpecAugment</cell><cell>20.05</cell><cell>72.04</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 03:34:56 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Breaking the unwritten language barrier: The BULB project</title>
		<author>
			<persName><forename type="first">G</forename><surname>Adda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Adda-Decker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ambouroue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blachon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bonneau-Maynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hamlaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Idiatov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-N</forename><surname>Kouarata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-M</forename><surname>Makasso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rialland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van De Velde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yvon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zerbian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="8" to="14" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ISI ASR system for the low resource speech recognition challenge for Indian languages</title>
		<author>
			<persName><forename type="first">J</forename><surname>Billa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the International Speech Communication Association</title>
				<meeting>the Annual Conference of the International Speech Communication Association</meeting>
		<imprint>
			<publisher>INTERSPEECH</publisher>
			<date type="published" when="2018-09">2018. September</date>
			<biblScope unit="page" from="3207" to="3211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BUT system for low resource Indian language ASR</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pulugundla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Baskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kesiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Egorova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cernocký</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the International Speech Communication Association</title>
				<meeting>the Annual Conference of the International Speech Communication Association</meeting>
		<imprint>
			<publisher>INTERSPEECH</publisher>
			<date type="published" when="2018-09">2018. September</date>
			<biblScope unit="page" from="3182" to="3186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised and activelearning scenarios: Efficient acoustic model refinement for a low resource Indian language</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chellapriyadharshini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toffy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Srinivasa Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</title>
				<meeting>the Annual Conference of the International Speech Communication Association, INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2018-09">2018. September</date>
			<biblScope unit="page" from="1041" to="1045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Design and development of a large vocabulary, continuous speech recognition system for Tamil</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madhavaraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE India Council International Conference (INDICON)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multilingual speech recognition with a single end-to-end model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Toshniwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2018-04">2018. April</date>
			<biblScope unit="page" from="4904" to="4908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multilingual representations for low resource speech recognition and keyword search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="259" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multilingual techniques for low resource automatic speech recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chuangsuwanich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Massachusetts Institute of Technology</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Investigation of transfer learning for ASR using LF-MMI trained neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Automatic Speech Recognition and Understanding Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="279" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multilingual sequenceto-sequence speech recognition: Architecture, transfer learning, and language modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Baskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Mallidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yalta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop (SLT)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="521" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with transfer learning</title>
		<author>
			<persName><forename type="first">C.-X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eurasip Journal on Audio, Speech, and Music Processing</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Elastic spectral distortion for low resource speech recognition with deep neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Obuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="309" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data augmentation for low resource languages</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ragni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Knill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Rath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J F</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the International Speech Communication Association</title>
				<meeting>the Annual Conference of the International Speech Communication Association</meeting>
		<imprint>
			<publisher>INTERSPEECH</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="810" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the International Speech Communication Association</title>
				<meeting>the Annual Conference of the International Speech Communication Association</meeting>
		<imprint>
			<publisher>INTERSPEECH</publisher>
			<date type="published" when="2015-01">2015. January</date>
			<biblScope unit="page" from="3586" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the International Speech Communication Association</title>
				<meeting>the Annual Conference of the International Speech Communication Association</meeting>
		<imprint>
			<date type="published" when="2019-09">2019. September</date>
			<biblScope unit="page" from="2613" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic speech recognition for Sanskrit</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Anoop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Intelligent Computing, Instrumentation and Control Technologies (ICICICT)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1146" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic speech recognition in sanskrit: A new speech corpus and modelling insights</title>
		<author>
			<persName><forename type="first">D</forename><surname>Adiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">59th Annual Meeting of the Association for Computational Linguistics (ACL Findings)</title>
				<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hybrid ctc/attention architecture for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1240" to="1253" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4945" to="4949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
				<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">RNU/NSD audio archive search</title>
		<author>
			<persName><forename type="first">India</forename><surname>All</surname></persName>
		</author>
		<author>
			<persName><surname>Radio</surname></persName>
		</author>
		<ptr target="http://new-sonair.com/RNU-NSD-Audio-Archive-Search.aspx" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sanskrit language teaching</title>
		<ptr target="http://www.sanskrit.nic.in/sanskrit_language_teaching.php" />
		<imprint/>
		<respStmt>
			<orgName>Central Sanskrit University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Balamodini children&apos;s stories in sanskrit</title>
		<author>
			<persName><forename type="first">Samskrita</forename><surname>Bharati</surname></persName>
		</author>
		<ptr target="https://archive.org/details/bAlamodinI-01" />
		<imprint>
			<date type="published" when="1994">1994 to 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Wikimedia database dumps</title>
		<ptr target="https://dumps.wikimedia.org/sawiki/20200901/sawiki-20200901-pages-articles.xml.bz2" />
		<imprint/>
	</monogr>
	<note>Wiki Sanskrit data dump</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in English and Mandarin</title>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Building an end-to-end speech recognition model in pytorch</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nguyen</surname></persName>
		</author>
		<ptr target="https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A disciplined approach to neural network hyperparameters: Part 1 -learning rate, batch size, momentum, and weight decay</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-03">Mar. 2018</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Comparison of decoding strategies for CTC acoustic models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zenkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sanabria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH</title>
				<meeting>the Annual Conference of the International Speech Communication Association, INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2017-08">2017. August</date>
			<biblScope unit="page" from="513" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weighted finite-state transducers in speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer, Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="88" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Silovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vesely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
				<imprint>
			<publisher>IEEE Signal Processing Society</publisher>
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
