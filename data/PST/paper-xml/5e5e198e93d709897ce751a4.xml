<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quantum based Whale Optimization Algorithm for wrapper feature selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Agrawal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Systems Sciences</orgName>
								<orgName type="institution">Jawaharlal Nehru University</orgName>
								<address>
									<postCode>110067</postCode>
									<settlement>Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Baljeet</forename><surname>Kaur</surname></persName>
							<email>baljeetkaur26@hotmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Systems Sciences</orgName>
								<orgName type="institution">Jawaharlal Nehru University</orgName>
								<address>
									<postCode>110067</postCode>
									<settlement>Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Hansraj College</orgName>
								<orgName type="institution" key="instit2">University of Delhi</orgName>
								<address>
									<postCode>110007</postCode>
									<settlement>Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Surbhi</forename><surname>Sharma</surname></persName>
							<email>surbhisharma9099@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Systems Sciences</orgName>
								<orgName type="institution">Jawaharlal Nehru University</orgName>
								<address>
									<postCode>110067</postCode>
									<settlement>Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Quantum based Whale Optimization Algorithm for wrapper feature selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">98573B863B5444F5235B6605D4C3BFF8</idno>
					<idno type="DOI">10.1016/j.asoc.2020.106092</idno>
					<note type="submission">Received date : 3 July 2019 Revised date : 5 December 2019 Accepted date : 7 January 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Quantum</term>
					<term>Whale Optimization Algorithm</term>
					<term>Bio-Inspired Technique</term>
					<term>Evolutionary Techniques</term>
					<term>Swarm Based techniques</term>
					<term>Feature Selection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is a PDF file of an article that has undergone enhancements after acceptance, such as the addition of a cover page and metadata, and formatting for readability, but it is not yet the definitive version of record. This version will undergo additional copyediting, typesetting and review before it is published in its final form, but we are providing this version to give early visibility of the article. Please note that, during the production process, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>All natural processes are inherently optimal in behaviour, since they drive an organism and/ or a complete social setup towards survival. The principle of natural selection and evolution; and the strategies that various living organisms pursue to search for food have become the basis of many evolutionary algorithms and swarm computing techniques.</p><p>The evolutionary algorithms are characterized by a) proper representation of the individuals within a population, b) the fitness function that evaluates the goodness of the individual, c) variation operators like mutation and crossover that update the individuals and drive them towards optimal solution and d) other parameters such as the size of the population, the selection strategy etc.. The judicious search space exploration and exploitation of the probable candidate solutions are instrumental in the timely convergence of the evolutionary algorithms. Genetic Algorithm (GA) <ref type="bibr">[1]</ref> and Genetic Programming <ref type="bibr">[2]</ref> are popular evolutionary algorithms, which have successfully been applied to the areas of signal processing <ref type="bibr">[3]</ref>, wireless communication <ref type="bibr" target="#b3">[4]</ref>, and financial modelling <ref type="bibr" target="#b4">[5]</ref> to name a few.</p><p>Nature also inspires swarm based computing techniques that are motivated by the social behaviour of animals that work in a team to forage for food. Swarm-based techniques like Ant Colony Optimization (ACO) <ref type="bibr" target="#b5">[6]</ref>, Particle Swarm Optimization (PSO) <ref type="bibr" target="#b6">[7]</ref> and Artificial Bee Colony Optimization (ABC) <ref type="bibr" target="#b7">[8]</ref> are based upon the social behaviour of living organisms. The Bat algorithm (BA) <ref type="bibr" target="#b8">[9]</ref>, the Grey Wolf Optimization Algorithm (GWO) <ref type="bibr" target="#b9">[10]</ref> and the Swarm Salp Algorithm (SSA) <ref type="bibr" target="#b10">[11]</ref> are some recently proposed swarm algorithms. They too propel the system towards an optimal solution, through repeated exploration and exploitation techniques. Various applications of swarm computation include aircraft scheduling <ref type="bibr" target="#b11">[12]</ref>, crowd simulation <ref type="bibr" target="#b12">[13]</ref>, medical imaging <ref type="bibr" target="#b13">[14]</ref> among others. Biological evolution and natural adaptation have laid down the foundation of evolutionary algorithms which have seen phenomenal progress in their role in optimization of functions <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b18">[19]</ref> and feature subset search techniques using GA <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, PSO <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, ACO <ref type="bibr" target="#b23">[24]</ref>, Binary Bat Algorithm (BBA) <ref type="bibr" target="#b8">[9]</ref>, GWO <ref type="bibr" target="#b24">[25]</ref> and SSA <ref type="bibr" target="#b10">[11]</ref>.</p><p>The application of feature selection plays a pivotal role in building efficient and improved decision systems. Feature selection techniques help to obtain the relevant and non-redundant features, which reduce the complexity of the system in terms of time and space. The reduced relevant features also provide better performance. Methods for feature selection are broadly categorized into filter and wrapper <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. The filter feature selection methods are based on the statistical properties of the individual features. As these are independent of the classifier, they are scalable but do not guarantee to perform well with a given learning algorithm. Wrapper methods select a subset of features based on the evaluation criteria of a learning algorithm. Further, wrapper methods may be sequential or random in nature <ref type="bibr" target="#b26">[27]</ref>. In sequential wrapper methods, like the forward feature selection and backward elimination method, features are included and removed respectively in the selected subset, incrementally. However, the features so selected/removed are not removed/selected in further iterations to enhance the performance of the decision system by these methods. Random search based wrapper methods overcome the limitations of the sequential wrapper methods as they provide an improved strategy for exploring the feature space. Some well known random search based wrapper methods are the Genetic Algorithm <ref type="bibr">[1]</ref>, Ant Colony Optimization <ref type="bibr" target="#b5">[6]</ref> and Particle Swarm Optimization <ref type="bibr" target="#b6">[7]</ref> and. These algorithms have been studied due to their metaheuristic capabilities. In these methods, a population of probable solutions is generated, which converges towards the optimal solution, over generations. encircling and closing in on the prey. Feature selection based WOA <ref type="bibr" target="#b27">[28]</ref> involves the mutation and crossover operators, for enhancing exploitation, along with the Tournament selection, for a thorough exploration. Results on benchmark datasets show improvements over other algorithms like the GA, PSO and ALO.</p><p>Recently, there is an exponential surge in the dimension of the feature space, which encourages the application of feature selection algorithms that are time and space efficient. However, the classical evolutionary and swarm-based feature selection methods involve huge computation overheads due to the large population size and the representation of the individuals of the population.</p><p>To handle the time and space complexity, the principle of quantum computing is amalgamated with some nature-inspired techniques in the recent past <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b33">[34]</ref>. Quantum computing is based on the principles of the quantum bit <ref type="bibr" target="#b29">[30]</ref> and promotes population diversity due to the superposition of states while facilitating the convergence within a short span of time. Quantum computing is successfully combined with the Genetic Algorithm <ref type="bibr" target="#b30">[31]</ref> to experience parallel computing. The Quantum Evolutionary Algorithm (QEA) for combinatorial optimization <ref type="bibr" target="#b29">[30]</ref> outperformed the conventional GA, without premature convergence even when small population size was used. Sun et. al have proposed a quantum version of PSO <ref type="bibr" target="#b33">[34]</ref> and established its efficacy using some benchmark functions. Quantum-inspired GA (QGA) <ref type="bibr" target="#b31">[32]</ref>, as well as the Quantum inspired PSO (QPSO) <ref type="bibr" target="#b33">[34]</ref> have succeeded as the preferred feature selection techniques over their conventional counterparts.</p><p>Motivated by the research works <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b33">[34]</ref>, we propose to enhance the performance of the classical Whale Optimization Algorithm with the inclusion of the quantum computing principle. In the proposed Quantum Whale Optimization Algorithm (QWOA) method, the Q-bit (quantum bit) representation, the quantum rotation gate, and the modified evolutionary operators like selection, mutation, and crossover are utilized. The performance of the QWOA is compared with the classical WOA and seven well-known meta-heuristic methods such as GA, PSO, BBA, GWO, SSA, QGA and QPSO in terms of a fitness function (based on the classification error and number of selected features), classification accuracy, area under the ROC curve and the selected feature ratio on fourteen challenging datasets from diversified domains. These datasets have been chosen from different domains of microarray gene expression, facial images, text and a variety of UCI datasets. The feature dimension of these datasets ranges from tens upto many thousands. For the high dimensional datasets, the QWOA and other meta-heuristic methods incur high time and space cost. To handle the complexity involved with high dimension datasets, prior to the implementation of the meta-heuristic method, a clustering method is employed that chooses a relevant and non-redundant feature set to be input to the method. This two-stage implementation improves the performance of the methods with high dimensional data. We have used four wellknown classifiers such as the k-Nearest Neighbour (kNN), the Linear discriminant classifier (LDC), the Support Vector Machine (SVM) and the C4.5. A statistical significance test is also J o u r n a l P r e -p r o o f performed to evaluate the statistical significance of the proposed method in comparison to the eight well-known evolutionary, swarm and quantum algorithms.</p><p>Following are the main contributions of our work:</p><p>1. A quantum-based approach of the classical WOA, QWOA, is proposed that further enhances the diversification and convergence properties of this swarm based wrapper feature selection method, through the quantum-bit representation of the population. 2. Modified mutation and crossover operators are introduced for quantum-based exploration, shrinking and spiral movement of the whales. 3. The inclusion of the quantum rotation gate operator boosts the convergence to a single state, hence balancing the exploration (diversification) and exploitation (convergence) property of the proposed method. 4. Reduction of the feature input set using the clustering step prior to QWOA for high dimensional datasets is instrumental in building a high performing decision system. 5. Performance comparison of the proposed method with eight popular evolutionary, swarm and quantum algorithms on fourteen datasets is carried out.</p><p>A brief introduction of the Whale Optimization Algorithm and the characteristics of the proposed Quantum Whale Optimization Algorithm (QWOA) for improved feature selection are presented in Section 2. Experimental setup, results, and discussions are presented in Section 3. The conclusion section and future work conclude the paper.</p><p>2. Proposed QWOA for Feature Selection</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Whale Optimization Algorithm for Feature Selection</head><p>Whale Optimization Algorithm <ref type="bibr" target="#b28">[29]</ref> is a recently proposed nature inspired optimization method that emulates the hunting behaviour of the humpback whales. Humpback whales are characterized by the exploration, shrinking and spiralling behaviour while hunting, which has been mathematically modelled in the WOA.</p><p>Consider 𝑾 to be a randomly generated set of solutions (whale positions) having n whales in a d-dimensional search space. While implementing WOA for feature selection <ref type="bibr" target="#b27">[28]</ref>, each whale position corresponds to a feature vector of length d, where value 1 or 0 of a feature indicates its inclusion or exclusion respectively in the feature subset. In each generation, the performance of each of the whale position, 𝑾 (i=1, 2, ... , n) is assessed based on some fitness criteria. After calculating the fitness of each solution, 𝑾 , the fittest whale position, 𝑾 * is ascertained. The positions of the n whales are updated in every generation till a terminating condition is reached. The updation may be in the form of exploration, shrinking or spiralling.</p><p>In exploration, a random candidate, 𝑾 is chosen from the population to guide the search. The current whale, 𝑾 , is mutated and crossed over with the randomly chosen whale position to give J o u r n a l P r e -p r o o f the updated position of the current whale. While in shrinking, the best whale position, 𝑾 * is mutated and crossed-over with the current whale, 𝑾 , to give the updated position of the current whale. For the spiral movement, the distance vector between the best whale position and the current whale is calculated as:</p><formula xml:id="formula_0">𝐃 = 𝑾 * -𝑾<label>(1)</label></formula><p>The updated position on spiralling is computed as:</p><formula xml:id="formula_1">𝑾 = 𝐃 • 𝑒 • 𝑐𝑜𝑠(2𝜋𝑙) + 𝑾 *<label>(2)</label></formula><p>where p defines the shape of the logarithmic spiral. The value of 𝑙 is chosen in the interval [-1, 1]. The dot operator represents the element by element multiplication.</p><p>In each generation, the decision as to which of the three options of exploration, spiralling or shrinking, is to be performed, is based on a random choice, p ϵ [0,1] and the value of the coefficient 𝑨. To calculate 𝑨, a parameter q is considered, which decreases linearly from 2 to 0 over the generations, using the following:</p><formula xml:id="formula_2">𝑞 = 2 -t * 2/𝑚𝑎𝑥𝐺𝑒𝑛<label>(3)</label></formula><p>where 𝑡 is the current generation and maxGen is the maximum generations for which the method is run. Coefficient vector 𝑨 is defined to control the search space around the best solution. It is given as: 𝑨 = 2𝑞 • 𝒓 -𝑞 (4) where 𝒓 is a random vector.</p><p>The classical WOA for feature selection briefly introduced above, has shown improvement over other evolutionary algorithms like PSO, ALO and GA <ref type="bibr" target="#b27">[28]</ref>, for some problems. The WOA leads the search either by using the best solution obtained or, by choosing a random whale position. This facilitates in the exploratory behaviour of WOA and prevents the system from getting stuck in the local minima.</p><p>Feature selection based on WOA uses the binary representation of the feature vectors. Hence the population size required for an effective search is high, which leads to more exploratory and exploitation time to converge to an optimal solution. To handle the complexity of the classical WOA in terms of time and space, to select an optimal number of features, we propose the quantum-based WOA (QWOA) which is based on the quantum computing principles and takes the advantage of a probabilistic representation of the Q-bits and enhances the population diversity. The proposed quantum-based WOA (QWOA) is motivated by the promising results for feature selection achieved by the quantum-based evolutionary algorithms, like QGA <ref type="bibr" target="#b31">[32]</ref>and QPSO <ref type="bibr" target="#b32">[33]</ref>. The amalgamation of quantum computing and the Whale Optimization Algorithm for feature selection is being attempted for the first time. The proposed principle of the QWOA is presented next, which involves the Q-bit representation, quantum rotation operator and modified  </p><formula xml:id="formula_3">|𝑎| + |𝑏| = 1<label>(5)</label></formula><p>If a representation is composed of d states, then this representation is said to be at all of the 2 d states at the same time, each with a given probability whose sum is equal to 1. The d-length Q-bit individual can be collapsed to a d-length binary vector using the following <ref type="bibr" target="#b29">[30]</ref>:</p><formula xml:id="formula_4">𝑖𝑓|𝑎 | &lt; 𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑 𝑡ℎ𝑒𝑛 𝑦 ← 1 𝑒𝑙𝑠𝑒 𝑦 ← 0<label>(6)</label></formula><p>where 𝑦 represents the corresponding binary bit of the observed Q-bit. The threshold value maybe generated randomly. The set of binary vectors obtained are evaluated for fitness.</p><p>Variation operators such as selection, mutation, crossover and quantum gates are used to update the position of the Q-bits in subsequent generations. Selection operators like Best, Random, Roulette, and Tournament <ref type="bibr" target="#b34">[35]</ref> are used to select those individuals that participate in the generation of the next set of population. The mutation operator involves altering the probabilities of one or more number of Q-bits of an individual, as explained below. For example, for a quantum individual 𝑷, the mutated individual, 𝑷 at the mutation point (in bold) is represented as follows:</p><formula xml:id="formula_5">𝑷 = 𝒂 𝟑 𝒃 𝟑 … … 𝑷 = 𝒃 𝟑 𝒂 𝟑 … …<label>(7)</label></formula><p>Crossover operates on two quantum individuals and outputs two individuals based on either 1point, n-point crossover or random point crossover. Given below is an illustration for a one-point crossover at the third position between parents 𝑷 and 𝑷 to give children 𝑪 and 𝑪 .</p><formula xml:id="formula_6">𝑷 = 𝒂 𝟑 𝟏 𝒃 𝟑 𝟏 … … 𝑷 = 𝒂 𝟑 𝟐 𝒃 𝟑 𝟐 … …<label>(8)</label></formula><p>J o u r n a l P r e -p r o o f</p><p>Journal Pre-proof</p><formula xml:id="formula_7">𝑪 = … … 𝑪 = … …<label>(9)</label></formula><p>The application of a quantum gate operator <ref type="bibr" target="#b29">[30]</ref> on the probability amplitude of each Q-bit individual balances exploitation and exploration of the QWOA to speed up the search and giving the result within a shorter span of time. Commonly used quantum gates are the Hadamard gate, the NOT gate, the controlled-NOT gate, and the rotation gate <ref type="bibr" target="#b29">[30]</ref>. We have used the rotation quantum gate:</p><formula xml:id="formula_8">𝑼(φ) = cos(φ) -sin(φ) sin(φ) cos(φ)<label>(10)</label></formula><p>The rotation angle (φ) and the direction of rotation are decided from a lookup table <ref type="bibr" target="#b35">[36]</ref>.</p><p>The proposed algorithm, QWOA for feature selection, and its flowchart are depicted in Figure <ref type="figure" target="#fig_3">1</ref>(a) and Figure <ref type="figure" target="#fig_3">1</ref>(b) respectively. Each whale position in a population of size n is represented in terms of d-dimensional Q-bits, corresponding to an individual. During exploration, the Tournament selection method is used to select a random individual that will guide the search. Crossover is performed between the mutated random individual and the mutated current individual. The shrinking method is carried out with the crossover between the best individual and its mutated form. The spiral movement is carried out using (1) and <ref type="bibr">(2)</ref>. As in WOA, the decision about the update method is based on the values of p and A. Equation ( <ref type="formula" target="#formula_4">6</ref>) is used to obtain the binary equivalent of the Q-bit individual. At each generation, for each quantum individual of the population, the quantum update is carried out for each Q-bit with the rotation gate using <ref type="bibr" target="#b9">(10)</ref>.</p><p>The fitness function, F used to evaluate the goodness of each binary feature vector computed as <ref type="bibr" target="#b27">[28]</ref> :</p><formula xml:id="formula_9">𝐹 = 𝛼(𝑒𝑟𝑟) + 𝛽(𝑘)<label>(11)</label></formula><p>Where α and β give the weight to err and k respectively. It is designed to select that feature subset that minimizes the classification error (err) as well as the feature subset size (k). The fitness values at each generation are calculated and the whole population is updated using the shrinking, spiralling and exploration methods until the termination condition is reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J o u r n a l P r e -p r o o f</head><p>Algorithm : Quantum Whale Optimization Algorithm for Feature Selection t ← 1 T ← maxGen Initialize a feature population of size n,   </p><formula xml:id="formula_10">W(t)={W 1 (t), W 2 (t), …, W i (t), …, W n (t)} T where W i (t)={ W i1 (t), W i2 (t), …, W ij (t)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Section</head><p>For comparing the performance seven well-known meta-heuristic methods such QPSO, experiments have been performed on a set of fourteen challenging domains of microarray gene expression, face image detection, high dimensional text and a collection of diverse datasets from the UCI repository. The datasets can be downloaded from <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b38">[39]</ref>. The aim is to test the across datasets and to be able to check if the performance improves across all the diverse datasets in comparison to the competing methods upon the number of their features. Further, some datasets are two multiclass in nature. As the datasets belong the datasets range from tens upestablish the suitability of the proposed method. The performance of the QWOA and methods is evaluated in terms of the curve (AUC) and size of the selected features well-known classifiers such as k datasets used are presented in Table <ref type="table" target="#tab_1">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flowchart of the proposed QWOA method for feature selection</head><p>For comparing the performance of the proposed QWOA method with the classical heuristic methods such as GA, PSO, BBA, GWO, SSA, QGA and experiments have been performed on a set of fourteen challenging datasets domains of microarray gene expression, face image detection, high dimensional text and a collection of diverse datasets from the UCI repository. The datasets can be downloaded from . The aim is to test the suitability of the proposed method by observing be able to check if the performance improves across all the diverse datasets competing methods. The datasets are of low and high dimension depending upon the number of their features. Further, some datasets are two-class while others are multiclass in nature. As the datasets belong to diverse domains and the dimension of -to thousands, the experiments performed form a sound basis to establish the suitability of the proposed method. The performance of the QWOA and in terms of the fitness function, classification accuracy, area under size of the selected features. We have performed the experiments with four k-NN, LDC, SVM (linear kernel) and C4. <ref type="bibr" target="#b4">5</ref> The details of the datasets used are presented in Table <ref type="table" target="#tab_1">1</ref>.</p><p>for feature selection the classical WOA and as GA, PSO, BBA, GWO, SSA, QGA and datasets from the domains of microarray gene expression, face image detection, high dimensional text and a collection of diverse datasets from the UCI repository. The datasets can be downloaded from of the proposed method by observing its impact be able to check if the performance improves across all the diverse datasets are of low and high dimension depending class while others are dimension of features of to thousands, the experiments performed form a sound basis to establish the suitability of the proposed method. The performance of the QWOA and other area under the ROC We have performed the experiments with four</p><p>The details of the J o u r n a l P r e -p r o o f The datasets are preprocessed, wherein m features with the highest variance are selected, and are normalized using z-score. For the global cancer map dataset (GCM), the preprocessing strategy used in the research work <ref type="bibr" target="#b39">[40]</ref> is followed. For GCM, the train and test datasets were available separately.</p><p>In the case of high dimension datasets, if the whole feature set is given as input to a given metaheuristic method, the required time and space will be very large. Moreover, as the number of samples available is very low, it leads to the problem of overfitting. Hence, it is desirable to build a system with a fewer number of features, which are relevant to classification. To achieve this, the experiments for high dimension datasets are performed in two phases. In the first phase, the input feature space is reduced through the Hierarchical Agglomerative Clustering method with Euclidean distance. This reduced feature set is then given as input to the method in the second phase.</p><p>The clustering in the first phase facilitates the grouping of features into a predetermined number of clusters. Further, from each of these clusters, a representative feature is selected using tstatistic, resulting in a smaller number of relevant and non-redundant features. The first phase of clustering greatly reduces the requirement of both time and space of the method in the second phase. The reduced feature set, as shown in the last column of Table <ref type="table" target="#tab_1">1</ref>, is given as input to the second phase. The second phase involves the meta-heuristic method which further optimizes the feature set while improving the fitness value.</p><p>The population size for the experiments is fixed as 20, and the length of the individual is set as the feature set dimension as mentioned in Table <ref type="table" target="#tab_1">1</ref>. The experiments have been carried out for 100 generations. The 10-fold cross-validation scheme is used for evaluation, which is run 10</p><p>J o u r n a l P r e -p r o o f times. For each run, the fitness value of the best solution of the population is considered. The average fitness value from 10 runs is considered for comparison of the performance of the methods. Similarly, the average classification accuracy, the average number of selected features and the average AUC is calculated over 10 runs for comparison. It can be observed from table 2 (a) that the performance of the QWOA method in terms of fitness value is best among all the 9 meta-heuristic methods, for each dataset. Hence the best average performance is achieved by the QWOA method. It can also be noted that WOA has the second best performance in terms of the overall average fitness value. From table 2 (b) we observe that QWOA outperforms other methods for 10 datasets, and achieves the same performance with some methods for 3 datasets. The QWOA achieved the second best classification accuracy for PIX10P dataset. However, the overall average of the QWOA method in terms of classification accuracy is best among the 9 competing methods. In table 2 (c), we observe that the QWOA method achieves the best AUC value for 12 datasets, same with another method for 1 dataset and second best with 1 dataset. The best overall average AUC is achieved by the QWOA method. Table <ref type="table" target="#tab_2">2</ref> (d) shows that no method is the clear winner among the 9 methods in terms of the average number of selected features. But it can be observed that the QWOA achieves the best performance followed by the QPSO and the QGA.</p><p>The deviation of each performance measure from the mean over different runs is obtained by the standard deviation. It can be observed from tables 2 (a) -2 (d) that the standard deviation obtained for most of the methods is low and also comparable to each other. It can also be observed that the performance of the QWOA in terms of standard deviation of each performance measure is better than the WOA for most of the datasets. This suggests that the proposed QWOA provides consistent performance over different runs.</p><p>It can be observed from tables 2 (a) -2 (d) that the performance of quantum algorithms QGA, QPSO and QWOA is better than their classical counterparts GA, PSO and WOA respectively in terms of average fitness value, average classification accuracy, average AUC and average number of selected features over all the considered datasets. The better performance of the quantum methods in comparison to their classical versions may be attributed to better representation of the population in terms of Q-bits and improved exploration and exploitation due to the involved operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J o u r n a l P r e -p r o o f</head><p>Journal Pre-proof     Boxplot is a method to graphically represent the variation in the values through their central tendency measures. The higher the median/mean and the smaller the spread, the better is the method among methods in comparison. To understand the robustness of each method, for illustration, a boxplot of the classification accuracy using k-NN for the QWOA versus the boxplots of other methods are shown in figure <ref type="figure" target="#fig_1">2</ref> for (a) StatlogHeart (b) Ionosphere (c) GCM (d) Tumor_9 (e) AR10P and (f) TR11WC. It can be observed from the boxplots that both mean and median for the QWOA are higher than other competing methods for the StatlogHeart, Ionosphere, Tumor_9, AR10P and TR11WC datasets. The median of the WOA is higher than that of QWOA for the GCM dataset but the mean is comparable. Similar performance is also observed for other datasets. To study the role of the population size on the performance of the classical and the quantum version of WOA, we conducted experiments with four different population sizes such as 1, 10, 20 and 50. Ten <ref type="bibr" target="#b9">(10)</ref> independent runs of each algorithm were carried out for the 10 fold crossvalidation with the k-Nearest Neighbour classifier. The maximum number of generations for each run is fixed to 100. Average results obtained with 10 runs for the three chosen datasets are presented in Table <ref type="table" target="#tab_7">3</ref>. GLA_BRA80 is a microarray dataset, TR11WC is a text-based dataset, while Ionosphere is a UCI dataset. These datasets are chosen from each of the domains to be able to explore diverse domains to show the comparison between the QWOA and the WOA. Based on the comparative results, the following can be observed:</p><p>J o u r n a l P r e -p r o o f classifier, viz. fitness value, classification accuracy, and AUC. For α=0.001, the QWOA is statistically significantly better than all other methods for all the three metrics except with the WOA for the fitness value and AUC.</p><p>To understand the variation in the performance of the proposed method with the choice of the classifier, we have investigated four classifiers, such as k-NN, LDA, SVM, and C4.5. The performance is compared with the WOA for each classifier. The comparison of the QWOA with the WOA in the terms classification accuracy and AUC with variation in the classifier is shown in Table <ref type="table" target="#tab_8">5</ref> (a). Similarly, the comparison of the QWOA with the WOA in terms of fitness value and the number of selected features is shown in Table <ref type="table" target="#tab_8">5</ref> (b). The best performance for each classifier for each dataset is shown in bold. A summary of observations regarding the performance of the different classifiers for the metrics: classification accuracy, AUC, fitness value and the minimum number of features is shown in Table <ref type="table">6</ref>. The summary is shown in terms of the number of wins, draws, and losses. It can be observed from table 5 (a) and (b) that the average of the classification accuracy, AUC, fitness values, and the minimum number of selected features obtained using QWOA is superior to the WOA for all the four classifiers.</p><p>The proposed method QWOA is compared with the state-of-the-art in Table <ref type="table">7</ref>, both on the basis of classification accuracy (k-Nearest Neighbour classifier) and also on the basis of the percentage ratio of the selected feature size. In the case of Tumor-9, linear SVM is used for evaluating the performance as described in <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>. Similarly, for TR23WC, the C4.5 classifier is used as suggested in the work of Wang et. al <ref type="bibr" target="#b43">[44]</ref>. It can be observed from table <ref type="table">7</ref> that the QWOA has consistently outperformed all the state-of-the-art methods for all the datasets. Experiments on datasets from diverse domains for both low and high dimensional features have shown the superior performance of the QWOA over the existing methods. The QWOA has also performed better for both the binary-class and the multi-class datasets in comparison to the existing methods. Moreover, the selected feature % is also very low, implying the low cost of building the decision model. The QWOA extends the promise of high performing yet low complexity wrapper based nature inspired feature selection method.</p><p>J o u r n a l P r e -p r o o f  J o u r n a l P r e -p r o o f instrumental in building a high performing decision system. Experiments on fourteen diversified datasets establish the merit of the proposed method as a meta-heuristic method that performs well on challenging datasets of both low and high feature dimensions. Comparison of the QWOA with eight meta-heuristic methods shows the improved performance in terms of the average fitness, average classification accuracy and average AUC. The statistical test also demonstrates the significantly better performance of the QWOA in comparison to the other competing methods. Experimental results demonstrate that the performance of quantum algorithms QGA, QPSO and QWOA is better than their classical counterparts GA, PSO and WOA respectively in terms of average fitness value, average classification accuracy, average AUC and average number of selected features over all the considered datasets. The better performance of the quantum methods in comparison to their classical versions may be attributed to better representation of the population in terms of Q-bits and improved exploration and exploitation due to the involved operators.</p><p>In future work, population selection methods other than the Tournament method can be explored to improve the performance further. Quantum variation operators like the Hadamard, the NOT gate and the controlled-NOT gate, and other lookup tables can also be tested for faster convergence. The proposed method is effective only for feature selection or binary unconstrained optimization problem. Further modification in the QWOA is required to solve continuous optimization problems and multi-objective optimization problems, which we will explore in future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>J o u r</head><label></label><figDesc>n a l P r e -p r o o f Journal Pre-proof mutation and crossover operators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2. 2 .</head><label>2</label><figDesc>QWOA for Feature SelectionQuantum computing is based on the representation of data as quantum bits (Q-bit) and capitalizes on the advantage offered by the superposition of states. The Q-bit is the basic information unit of a 2-state quantum computer<ref type="bibr" target="#b30">[31]</ref>. Given individual states, | 0〉 and | 1〉, the quantum state | Ѱ〉 = a| 0〉 + b| 1〉 is the linear superposition of the individual states. The probability amplitudes, a and b are the complex number weights of the quantum particle in location | 0〉 and | 1〉 respectively. Here, |a| 2 and |b| 2 give the probability of the Q-bit to be in the state | 0〉 and | 1〉 respectively such that:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>J o u r n a l P r e -p r o o f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 (</head><label>1</label><figDesc>Figure 1(b): Flowchart of the proposed</figDesc><graphic coords="10,248.14,336.70,151.45,58.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>J o u rFigure 2 :</head><label>2</label><figDesc>Figure 2: Boxplots of classification accuracy for QWOA versus other methods for (a) StatlogHeart (b) Ionosphere (c) GCM (d) Tumor_9 (e) AR10P and (f) TR11WC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Datasets used and their details</figDesc><table><row><cell cols="2">Dimension Domain</cell><cell>Dataset</cell><cell cols="3">Class Samples Original</cell><cell>Pre-</cell><cell>Features</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Features</cell><cell>processed</cell><cell>after</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Features (m)</cell><cell>clustering</cell></row><row><cell>High</cell><cell cols="2">Microarray GLI-85</cell><cell>2</cell><cell>85</cell><cell>22283</cell><cell>5000</cell><cell>100</cell></row><row><cell>High</cell><cell cols="3">Microarray GLA_BRA180 4</cell><cell>180</cell><cell>49151</cell><cell>5000</cell><cell>100</cell></row><row><cell>High</cell><cell cols="2">Microarray GCM</cell><cell>14</cell><cell>198</cell><cell>16063</cell><cell>5000</cell><cell>100</cell></row><row><cell>High</cell><cell cols="2">Microarray Tumor-9</cell><cell>9</cell><cell>60</cell><cell>5726</cell><cell>5000</cell><cell>100</cell></row><row><cell>High</cell><cell cols="2">Face Image AR10P</cell><cell>10</cell><cell>130</cell><cell>2400</cell><cell>2400</cell><cell>100</cell></row><row><cell>High</cell><cell cols="2">Face Image PIE10P</cell><cell>10</cell><cell>210</cell><cell>2400</cell><cell>2400</cell><cell>100</cell></row><row><cell>High</cell><cell cols="2">Face Image PIX10P</cell><cell>10</cell><cell>100</cell><cell>10000</cell><cell>2400</cell><cell>100</cell></row><row><cell>High</cell><cell>Text</cell><cell>TR11WC</cell><cell>9</cell><cell>414</cell><cell>6430</cell><cell>6430</cell><cell>200</cell></row><row><cell>High</cell><cell>Text</cell><cell>TR23WC</cell><cell>6</cell><cell>204</cell><cell>5833</cell><cell>5833</cell><cell>200</cell></row><row><cell>Low</cell><cell>UCI</cell><cell cols="2">Lymphography 4</cell><cell>148</cell><cell>18</cell><cell>18  ǂ</cell><cell>-</cell></row><row><cell>Low</cell><cell>UCI</cell><cell>Waveform</cell><cell>3</cell><cell>5000</cell><cell>40</cell><cell>40  ǂ</cell><cell>-</cell></row><row><cell>Low</cell><cell>UCI</cell><cell>Tictactoe</cell><cell>2</cell><cell>958</cell><cell>27 *</cell><cell>27  ǂ</cell><cell>-</cell></row><row><cell>Low</cell><cell>UCI</cell><cell>StatlogHeart</cell><cell>2</cell><cell>270</cell><cell>13</cell><cell>13  ǂ</cell><cell>-</cell></row><row><cell>Low</cell><cell>UCI</cell><cell>Ionosphere</cell><cell>2</cell><cell>351</cell><cell>34</cell><cell>34  ǂ</cell><cell>-</cell></row></table><note><p><p>*</p>Encoded ǂ All features selected</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 (</head><label>2</label><figDesc>a), table 2 (b), table 2 (c) and table2 (d)  show the average fitness value, the average classification accuracy, average AUC and the average number of selected features respectively with the k-NN classifier (k=5). To investigate the stability and robustness of the proposed QWOA method in comparison to the other methods, the standard deviation is also reported for each dataset. The best performance among all nine meta-heuristic methods for each of the fourteen datasets is shown in bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 (</head><label>2</label><figDesc>a): Comparison between the proposed QWOA method and other competitive methods in terms of fitness values.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">Journal Pre-proof</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Measure</cell><cell>GA</cell><cell>PSO</cell><cell>BBA</cell><cell>GWO</cell><cell>SSA</cell><cell>QGA</cell><cell>QPSO</cell><cell cols="2">WOA QWOA</cell></row><row><cell>Lymphography</cell><cell>avg</cell><cell cols="8">0.1752 0.1802 0.1847 0.1538 0.1672 0.1469 0.1630 0.1538</cell><cell>0.1445</cell></row><row><cell></cell><cell>std</cell><cell cols="8">0.0373 0.0106 0.0226 0.0163 0.0120 0.0104 0.0166 0.0110</cell><cell>0.0109</cell></row><row><cell cols="10">Waveform Tictactoe StatlogHeart Ionosphere GLI-85 GLA-BRA180 GCM Tumour-9 AR10P PIE10 PIX10P TR11WC TR23WC Average J o u r n a l P r e -p r o o f avg 0.1723 0.1763 0.2001 0.1673 0.1793 0.1708 0.1707 0.1692 std 0.0036 0.0047 0.0063 0.0042 0.0022 0.0042 0.0052 0.0048 avg 0.1247 0.1135 0.1291 0.1124 0.1120 0.1102 0.1103 0.1089 std 0.0033 0.0035 0.0061 0.0033 0.0021 0.0009 0.0017 0.0004 avg 0.1547 0.1493 0.1734 0.1441 0.1504 0.1476 0.1429 0.1422 std 0.0166 0.0085 0.0084 0.0071 0.0059 0.0048 0.0074 0.0065 avg 0.1045 0.1156 0.1141 0.1145 0.1186 0.0921 0.1039 0.0838 std 0.0232 0.0083 0.0101 0.0071 0.0041 0.0084 0.0107 0.0135 avg 0.0073 0.0077 0.0197 0.0055 0.0049 0.0047 0.0064 0.0033 std 0.0078 0.0054 0.0077 0.0007 0.0005 0.0008 0.0034 0.0006 avg 0.2179 0.2147 0.2392 0.1977 0.2174 0.2080 0.2013 0.2080 std 0.0243 0.0121 0.0275 0.0106 0.0094 0.0128 0.0141 0.0119 avg 0.3532 0.4108 0.4973 0.3383 0.3891 0.3488 0.3836 0.2869 std 0.0478 0.0243 0.0322 0.0303 0.0239 0.0297 0.0290 0.0447 avg 0.2703 0.2924 0.3585 0.2526 0.2975 0.2660 0.2726 0.2460 std 0.0298 0.0234 0.0334 0.0307 0.0157 0.0281 0.0186 0.0188 avg 0.0945 0.2138 0.2753 0.1844 0.2279 0.0883 0.2121 0.0112 std 0.0221 0.0188 0.0129 0.0216 0.0118 0.0374 0.0306 0.0064 avg 0.0051 0.0059 0.0152 0.0063 0.0053 0.0048 0.0058 0.0041 std 0.0015 0.0023 0.0039 0.0004 0.0004 0.0010 0.0007 0.0007 avg 0.0110 0.0116 0.0120 0.0126 0.0113 0.0091 0.0106 0.0048 std 0.0013 0.0039 0.0034 0.0040 0.0046 0.0097 0.0053 0.0030 avg 0.2042 0.2236 0.2559 0.2050 0.2255 0.1870 0.2151 0.1820 std 0.0055 0.0105 0.0126 0.0086 0.0052 0.0116 0.0080 0.0096 avg 0.2097 0.2223 0.2457 0.2062 0.2179 0.1764 0.2005 0.1422 std 0.0086 0.0148 0.0142 0.0070 0.0063 0.0318 0.0092 0.0180 0.1503 0.1670 0.1943 0.1501 0.1660 0.1401 0.1571 0.1247</cell><cell>0.1662 0.0032 0.1085 0.0000 0.1366 0.0039 0.0754 0.0092 0.0028 0.0007 0.1960 0.0118 0.2830 0.0256 0.2124 0.0183 0.0074 0.0040 0.0034 0.0007 0.0043 0.0049 0.1769 0.0099 0.1404 0.0181 0.1184</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 (</head><label>2</label><figDesc>b): Comparison between the proposed QWOA method and other competitive methods in terms of classification accuracy.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Journal Pre-proof</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Meas</cell><cell>GA</cell><cell>PSO</cell><cell>BBA</cell><cell>GWO</cell><cell>SSA</cell><cell>QGA</cell><cell>QPSO</cell><cell>WOA</cell><cell>QWOA</cell></row><row><cell>Lymphography</cell><cell>avg</cell><cell>82.7682</cell><cell>82.5000</cell><cell>81.8243</cell><cell>85.1352</cell><cell>83.7162</cell><cell>85.8085</cell><cell>84.1216</cell><cell>85.0000</cell><cell>86.0135</cell></row><row><cell></cell><cell>std</cell><cell>3.8907</cell><cell>1.0778</cell><cell>2.1473</cell><cell>1.6551</cell><cell>1.1682</cell><cell>1.0560</cell><cell>1.6929</cell><cell>1.1832</cell><cell>1.1057</cell></row><row><cell cols="9">Waveform Tictactoe StatlogHeart Ionosphere GLI-85 GLA-BRA180 GCM Tumour-9 AR10P PIE10 PIX10P TR11WC TR23WC Average J o u r n a l P r e -p r o o f avg 83.1586 83.0640 80.3600 83.8520 82.5060 83.2485 83.3340 std 0.3274 0.4439 0.6034 0.3911 0.2565 0.3811 0.5173 avg 87.9799 89.3215 87.5574 89.3842 89.3737 89.5111 89.5199 std 0.3930 0.3191 0.6761 0.2871 0.2350 0.1619 0.1320 avg 84.9991 85.6667 83.1111 86.2222 85.3333 85.8455 86.2222 std 1.7554 0.8562 0.9434 0.7366 0.5576 0.3349 0.7156 avg 89.8872 88.8034 88.7465 88.9744 88.4331 90.8326 89.8576 std 2.1371 0.7484 0.9708 0.6448 0.4074 0.8375 1.0068 avg 99.5290 99.6471 98.3530 100.0000 100.0000 99.9993 99.8824 std 0.8231 0.5683 0.8226 0.0000 0.0000 0.0014 0.3720 avg 78.3141 78.8333 76.3333 80.8206 78.6111 79.6556 80.3333 std 2.5811 1.1843 2.7492 1.6276 0.9533 1.2773 1.4152 avg 64.7832 59.1305 50.2174 66.5217 61.3044 65.4337 61.7392 std 4.8934 2.4680 3.3128 3.1083 2.4680 2.9783 2.9346 avg 73.1465 71.0000 64.3333 75.1667 70.5000 73.8330 73.1667 std 2.9860 2.3831 3.3518 3.0882 1.5812 2.8385 1.8342 avg 90.8101 79.0769 72.6154 82.0769 77.5385 91.5990 79.0769 std 2.1696 1.8419 1.3175 2.1772 1.1917 3.6940 3.0512 avg 99.8990 99.5238 98.9524 100.0000 100.0000 100.0000 100.0000 std 0.0000 0.0000 0.4376 0.0000 0.0000 0.0000 0.0000 avg 99.1040 99.2000 99.1000 99.2000 99.3000 90.1697 99.4000 std 0.1384 0.4216 0.3162 0.4216 0.4830 28.1853 0.5164 avg 79.8838 77.9227 74.6377 80.0725 77.8261 82.0000 78.9855 std 0.5483 1.0694 1.2933 0.8765 0.5437 1.1297 0.7467 avg 79.2409 78.2843 75.5392 79.9020 78.5294 82.8177 80.2451 std 0.9157 1.4440 1.4514 0.6932 0.6454 3.2002 0.9258 85.2503 83.7124 80.8344 85.5234 83.7837 85.7682 84.7060</cell><cell>83.6640 0.4239 89.5616 0.0012 86.3704 0.5466 91.8234 1.2313 100.0000 0.0000 79.6919 1.1548 71.5217 4.5195 75.3000 1.8002 99.4615 0.6333 100.0000 0.0000 99.9000 0.3162 82.4155 0.9095 86.2745 1.7750 87.9275</cell><cell>83.8660 0.2742 89.5616 0.0010 86.8519 0.2619 92.9900 0.9153 100.0000 0.0000 80.8805 1.1057 71.9565 2.6027 79.1600 1.8159 99.8462 0.3243 100.0000 0.0000 99.7000 0.4830 82.8744 0.9625 86.3725 1.8019 88.5766</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 (</head><label>2</label><figDesc>c): Comparison between the proposed QWOA method and other competitive methods in terms of AUC.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Journal Pre-proof</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Measure</cell><cell>GA</cell><cell>PSO</cell><cell>BBA</cell><cell>GWO</cell><cell>SSA</cell><cell>QGA</cell><cell>QPSO</cell><cell>WOA</cell><cell>QWOA</cell></row><row><cell>Lymphography</cell><cell>avg</cell><cell>0.8031</cell><cell>0.8204</cell><cell>0.6924</cell><cell>0.8349</cell><cell>0.8314</cell><cell>0.8438</cell><cell>0.8312</cell><cell>0.8282</cell><cell>0.8520</cell></row><row><cell></cell><cell>std</cell><cell>0.0710</cell><cell>0.0277</cell><cell>0.0817</cell><cell>0.0302</cell><cell>0.0335</cell><cell>0.0127</cell><cell>0.0109</cell><cell>0.0210</cell><cell>0.0209</cell></row><row><cell cols="9">Waveform Tictactoe StatlogHeart Ionosphere GLI-85 GLA-BRA180 GCM Tumour-9 AR10P PIE10 PIX10P TR11WC TR23WC Average J o u r n a l P r e -p r o o f avg 0.9116 0.9462 0.7904 0.9494 0.9460 0.9205 0.9483 std 0.0020 0.0021 0.1414 0.0029 0.0028 0.0026 0.0030 avg 0.9697 0.9692 0.7695 0.9712 0.9728 0.9712 0.9729 std 0.0133 0.0040 0.0898 0.0037 0.0026 0.0018 0.0027 avg 0.8670 0.8713 0.8006 0.8823 0.8751 0.8694 0.8813 std 0.0264 0.0163 0.0477 0.0107 0.0160 0.0084 0.0141 avg 0.9214 0.9182 0.9103 0.9244 0.9241 0.9249 0.9264 std 0.0156 0.0077 0.0104 0.0144 0.0118 0.0091 0.0153 avg 0.9931 0.9958 0.9553 0.9993 0.8878 0.9950 0.9993 std 0.0156 0.0052 0.0609 0.0015 0.0162 0.0143 0.0015 avg 0.8819 0.8861 0.7970 0.8935 0.8092 0.8908 0.8953 std 0.0249 0.0146 0.0742 0.0128 0.0272 0.0099 0.0093 avg 0.8160 0.7996 0.5527 0.8084 0.8092 0.8282 0.7959 std 0.0271 0.0261 0.2256 0.0229 0.0272 0.0295 0.0244 avg 0.7323 0.7408 0.6757 0.7473 0.7412 0.7573 0.7412 std 0.0154 0.0095 0.0644 0.0113 0.0159 0.0124 0.0148 avg 0.9696 0.9331 0.8506 0.9444 0.9411 0.9704 0.9502 std 0.0104 0.0083 0.1046 0.0116 0.0103 0.0119 0.0151 avg 0.9998 0.9997 0.9654 0.9999 0.9998 0.9999 0.9998 std 0.0002 0.0003 0.0615 0.0001 0.0002 0.0001 0.0001 avg 0.9970 0.9959 0.9867 0.9958 0.9969 0.9993 0.9969 std 0.0035 0.0034 0.0135 0.0033 0.0033 0.0012 0.0035 avg 0.9176 0.9075 0.7731 0.9182 0.9128 0.9212 0.9209 std 0.0158 0.0141 0.0950 0.0100 0.0143 0.0097 0.0061 avg 0.8884 0.8629 0.7743 0.8667 0.8659 0.8909 0.8750 std 0.0199 0.0093 0.0678 0.0198 0.0192 0.0206 0.0259 0.9049 0.9033 0.8067 0.9097 0.8938 0.9131 0.9096</cell><cell>0.9496 0.0018 0.9732 0.0025 0.8763 0.0156 0.9343 0.0141 0.9996 0.0013 0.8998 0.0126 0.8259 0.0300 0.7357 0.0151 0.9978 0.0025 0.9998 0.0002 0.9998 0.0006 0.9207 0.0083 0.8866 0.0183 0.9162</cell><cell>0.9511 0.0024 0.9737 0.0022 0.8838 0.0179 0.9349 0.0149 0.9998 0.0012 0.9010 0.0006 0.8322 0.0203 0.7698 0.0123 0.9982 0.0025 0.9998 0.0002 0.9990 0.0026 0.9322 0.0053 0.8917 0.0204 0.9228</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 (</head><label>2</label><figDesc>d): Comparison between the proposed QWOA method and other competitive methods in terms of the minimum number of selected features.</figDesc><table><row><cell>Dataset</cell><cell>Measure</cell><cell>GA</cell><cell>PSO</cell><cell>BBA</cell><cell>GWO</cell><cell>SSA</cell><cell>QGA</cell><cell>QPSO</cell><cell cols="2">WOA QWOA</cell></row><row><cell>Lymphography</cell><cell>avg</cell><cell>15.2</cell><cell>12.6</cell><cell>8.6</cell><cell>11.9</cell><cell>10.8</cell><cell>8.6</cell><cell>10.5</cell><cell>9.6</cell><cell>10.9</cell></row><row><cell></cell><cell>std</cell><cell>10.71</cell><cell>1.84</cell><cell>3.31</cell><cell>1.1</cell><cell>2.25</cell><cell>0.84</cell><cell>1.72</cell><cell>2.72</cell><cell>1.45</cell></row><row><cell>Waveform</cell><cell>avg</cell><cell>29.2</cell><cell>34.7</cell><cell>32.5</cell><cell>29.8</cell><cell>24.4</cell><cell>17</cell><cell>22.9</cell><cell>29.9</cell><cell>25.9</cell></row><row><cell></cell><cell>std</cell><cell>12.63</cell><cell>1.95</cell><cell>2.84</cell><cell>1.75</cell><cell>3.13</cell><cell>1.62</cell><cell>2.13</cell><cell>3.51</cell><cell>2.69</cell></row><row><cell>Tictactoe</cell><cell>avg</cell><cell>22.5</cell><cell>21</cell><cell>15.9</cell><cell>19.7</cell><cell>18.4</cell><cell>14.8</cell><cell>17.7</cell><cell>15</cell><cell>14</cell></row><row><cell></cell><cell>std</cell><cell>13.11</cell><cell>1.56</cell><cell>2.38</cell><cell>1.83</cell><cell>1.35</cell><cell>1.55</cell><cell>1.83</cell><cell>1.15</cell><cell>0</cell></row><row><cell>StatlogHeart</cell><cell>avg</cell><cell>15.1</cell><cell>9.6</cell><cell>8</cell><cell>10</cell><cell>6.7</cell><cell>7.2</cell><cell>8.5</cell><cell>9.4</cell><cell>8.4</cell></row><row><cell></cell><cell>std</cell><cell>10.11</cell><cell>1.65</cell><cell>2.87</cell><cell>1.05</cell><cell>1.16</cell><cell>1.55</cell><cell>2.51</cell><cell>1.78</cell><cell>2.32</cell></row><row><cell>Ionosphere</cell><cell>avg</cell><cell>21.8</cell><cell>16.1</cell><cell>9</cell><cell>18.2</cell><cell>14</cell><cell>1.6</cell><cell>12</cell><cell>9.6</cell><cell>6.1</cell></row><row><cell></cell><cell>std</cell><cell>14.3</cell><cell>3.75</cell><cell>2.94</cell><cell>2.97</cell><cell>3.16</cell><cell>0.7</cell><cell>3.62</cell><cell>4.7</cell><cell>1.2</cell></row><row><cell>GLI-85</cell><cell>avg</cell><cell>23</cell><cell>31.4</cell><cell>32.1</cell><cell>33.2</cell><cell>29.6</cell><cell>25.4</cell><cell>25.1</cell><cell>19.8</cell><cell>17</cell></row><row><cell></cell><cell>std</cell><cell>14.9</cell><cell>7.07</cell><cell>4.89</cell><cell>4.16</cell><cell>2.95</cell><cell>1.99</cell><cell>6.79</cell><cell>3.52</cell><cell>4.08</cell></row><row><cell>GLA-BRA180</cell><cell>avg</cell><cell>26.5</cell><cell>39.6</cell><cell>29.3</cell><cell>43.6</cell><cell>33.8</cell><cell>36.3</cell><cell>34.8</cell><cell>41.7</cell><cell>40.3</cell></row><row><cell></cell><cell>std</cell><cell>17.5</cell><cell>4.27</cell><cell>3.92</cell><cell>2.07</cell><cell>3.91</cell><cell>4.12</cell><cell>3.26</cell><cell>7.96</cell><cell>6.55</cell></row><row><cell>GCM</cell><cell>avg</cell><cell>34.5</cell><cell>37.4</cell><cell>47</cell><cell>41.4</cell><cell>36</cell><cell>36.8</cell><cell>32.1</cell><cell>32.3</cell><cell>30</cell></row><row><cell></cell><cell>std</cell><cell>16.94</cell><cell>7.37</cell><cell>6.45</cell><cell>3.17</cell><cell>6.65</cell><cell>0.91</cell><cell>4.58</cell><cell>3.02</cell><cell>4.14</cell></row><row><cell>Tumour-9</cell><cell>avg</cell><cell>53.7</cell><cell>41.7</cell><cell>32.5</cell><cell>40.6</cell><cell>32.7</cell><cell>38.5</cell><cell>35.5</cell><cell>43.3</cell><cell>37.1</cell></row><row><cell></cell><cell>std</cell><cell>19.28</cell><cell>5.48</cell><cell>3.95</cell><cell>3.78</cell><cell>2.5</cell><cell>3.34</cell><cell>3.37</cell><cell>7.2</cell><cell>2.94</cell></row><row><cell>AR10P</cell><cell>avg</cell><cell>52.5</cell><cell>66.2</cell><cell>51.4</cell><cell>69.5</cell><cell>55.1</cell><cell>44.5</cell><cell>49.4</cell><cell>59</cell><cell>58.3</cell></row><row><cell></cell><cell>std</cell><cell>24.98</cell><cell>7.36</cell><cell>9.87</cell><cell>4.38</cell><cell>2.51</cell><cell>14.38</cell><cell>4.84</cell><cell>5.87</cell><cell>10.04</cell></row><row><cell>PIE10</cell><cell>avg</cell><cell>48.1</cell><cell>57.9</cell><cell>58.3</cell><cell>63.4</cell><cell>53.1</cell><cell>54.9</cell><cell>44.6</cell><cell>40.6</cell><cell>36.2</cell></row><row><cell></cell><cell>std</cell><cell>22.47</cell><cell>6.71</cell><cell>7.45</cell><cell>4.25</cell><cell>3.96</cell><cell>6.99</cell><cell>3.66</cell><cell>6.88</cell><cell>7.27</cell></row><row><cell>PIX10P</cell><cell>avg</cell><cell>28.3</cell><cell>46.8</cell><cell>41</cell><cell>46.5</cell><cell>43.8</cell><cell>8</cell><cell>36.7</cell><cell>18.3</cell><cell>12.8</cell></row><row><cell></cell><cell>std</cell><cell>14.96</cell><cell>6.39</cell><cell>12.43</cell><cell>5.28</cell><cell>4.1</cell><cell>9.72</cell><cell>5.92</cell><cell>3.06</cell><cell>3.39</cell></row><row><cell>TR11WC</cell><cell>avg</cell><cell>107.6</cell><cell>141.5</cell><cell>96.3</cell><cell>154.4</cell><cell>118.8</cell><cell>144.4</cell><cell>100.1</cell><cell>158.3</cell><cell>122.2</cell></row><row><cell></cell><cell>std</cell><cell>60.62</cell><cell>23.13</cell><cell>16.81</cell><cell>8.45</cell><cell>6.99</cell><cell>5.77</cell><cell>5.88</cell><cell>19.2</cell><cell>29.1</cell></row><row><cell>TR23WC</cell><cell>avg</cell><cell>90.1</cell><cell>146.7</cell><cell>70.3</cell><cell>143.9</cell><cell>107.1</cell><cell>100.7</cell><cell>105.1</cell><cell>126.8</cell><cell>110.7</cell></row><row><cell></cell><cell>std</cell><cell>51.74</cell><cell>15.62</cell><cell>18.37</cell><cell>7.78</cell><cell>7.92</cell><cell>23.6</cell><cell>6.3</cell><cell>17.29</cell><cell>22.4</cell></row><row><cell>Average</cell><cell></cell><cell>40.58</cell><cell cols="2">50.229 38.014</cell><cell>51.86</cell><cell cols="4">41.736 38.479 38.214 43.829</cell><cell>37.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Variation in performance based on population size</figDesc><table><row><cell>Dataset</cell><cell cols="3">Pop.Size Average Fitness</cell><cell cols="4">Average Accuracy Average Features</cell></row><row><cell></cell><cell></cell><cell>WOA</cell><cell cols="2">QWOA WOA</cell><cell cols="2">QWOA WOA</cell><cell>QWOA</cell></row><row><cell cols="2">GLA_BRA180 1</cell><cell>0.262</cell><cell>0.251</cell><cell>74.17</cell><cell>75.33</cell><cell>39.3</cell><cell>37.8</cell></row><row><cell></cell><cell>10</cell><cell>0.218</cell><cell>0.216</cell><cell>78.67</cell><cell>78.89</cell><cell>42.1</cell><cell>40.3</cell></row><row><cell></cell><cell>20</cell><cell>0.208</cell><cell>0.196</cell><cell>79.67</cell><cell>80.89</cell><cell>41.7</cell><cell>40.3</cell></row><row><cell></cell><cell>50</cell><cell>0.193</cell><cell>0.189</cell><cell>81.17</cell><cell>81.56</cell><cell>38.8</cell><cell>38.2</cell></row><row><cell>TR11</cell><cell>1</cell><cell>0.212</cell><cell>0.201</cell><cell>79.37</cell><cell>80.39</cell><cell>156.3</cell><cell>140.7</cell></row><row><cell></cell><cell>10</cell><cell>0.191</cell><cell>0.18</cell><cell>81.45</cell><cell>82.51</cell><cell>154.3</cell><cell>143.6</cell></row><row><cell></cell><cell>20</cell><cell>0.182</cell><cell>0.177</cell><cell>82.42</cell><cell>82.87</cell><cell>158.3</cell><cell>122.2</cell></row><row><cell></cell><cell>50</cell><cell>0.174</cell><cell>0.163</cell><cell>83.19</cell><cell>84.18</cell><cell>152.5</cell><cell>134.6</cell></row><row><cell>Ionosphere</cell><cell>1</cell><cell>0.119</cell><cell>0.097</cell><cell>88.46</cell><cell>90.57</cell><cell>15.1</cell><cell>7.33</cell></row><row><cell></cell><cell>10</cell><cell>0.102</cell><cell>0.072</cell><cell>90.03</cell><cell>92.93</cell><cell>12</cell><cell>4.6</cell></row><row><cell></cell><cell>20</cell><cell>0.087</cell><cell>0.072</cell><cell>91.54</cell><cell>92.99</cell><cell>10.4</cell><cell>6</cell></row><row><cell></cell><cell>50</cell><cell>0.077</cell><cell>0.066</cell><cell>92.48</cell><cell>93.56</cell><cell>8.2</cell><cell>4.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 (</head><label>5</label><figDesc>a): Comparison of Classification accuracy and AUC with variation in classifier</figDesc><table><row><cell>Accuracy</cell></row><row><cell>Dataset</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 (</head><label>5</label><figDesc>b): Comparison of Fitness and Number of Selected Features with variation in classifier</figDesc><table><row><cell>Fitness</cell></row><row><cell>Dataset</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>J o u r n a l P r e -p r o o fJournal Pre-proof</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p> For each of the population sizes, the QWOA outperforms the WOA for all the datasets in terms of the average value of the three chosen metrics.  Both the average fitness value and the average classification accuracy improve with the increase in the population size for all the datasets. It is clearly evident that the QWOA outperforms the WOA consistently for every population size, across generations. In general, initially, the average performance of the QWOA is better than that of WOA, as the generations advance, the results become similar. Even with the population of a single individual, the QWOA performs better than WOA. The QWOA explores and exploits the search space better due to its quantum representation, thereby giving better performance of the QWOA than that of the WOA. The consistent improvement of performance, for all the chosen population sizes, point towards the potential of the quantum version of the WOA as a possible metaheuristic algorithm of swarm computing.  QWOA vs. BBA 0.000221</p><p>To determine whether the QWOA and other eight meta-heuristic methods are statistically significantly different, the non-parametric two-sided Wilcoxon Signed Rank test has been carried out between the QWOA and each of the eight methods. The test does not assume a normal distribution, and outliers do not affect its performance <ref type="bibr" target="#b40">[41]</ref>. It tests the presence of a significant statistical difference between the two methods. The statistical significant difference between each pair of methods is evaluated on the basis of the fitness value, classification accuracy and AUC on the 14 datasets. Their p-values are tabulated in Table <ref type="table">4</ref>. Based on the decision test, the logical value, h=1, indicates a statistically significant difference between the two methods at the given significance level. For the levels of significance, α=0.05, α=0.01 and α=0.005, the QWOA outperformed all the eight methods in terms of all the three metrics obtained using the k-NN J o u r n a l P r e -p r o o f </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, a quantum-based Whale Optimization Algorithm (QWOA) is proposed that enhances the diversification and convergence properties of the classical Whale Optimization Algorithm for feature selection. The quantum representation of whale positions, in the QWOA facilitates a better chance of population diversity, and hence provide better exploration even with smaller population size. Modified mutation and crossover operators are introduced for quantumbased exploration, shrinking and spiral movement of the whales in the proposed QWOA. The application of the quantum rotation gate operator balances the exploration (diversification) and exploitation (convergence) property of the proposed method. Further, reduction of the feature input set using the clustering step prior to QWOA for high dimensional datasets is also J o u r n a l P r e -p r o o f COMPUTING INFORMATION AND CONTROL, vol. 13, no. 5, pp. 1759-1767, 2017.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J o u r n a l P r e -p r o o f</head><p>Journal Pre-proof 1. A quantum-based approach of the classical WOA (QWOA) is proposed that further enhances the diversification and convergence properties of this swarm based wrapper feature selection method, through the quantum-bit representation of the population. 2. Modified mutation and crossover operators are introduced for quantum-based exploration, shrinking and spiral movement of the whales. 3. The inclusion of the quantum rotation gate variation operator boosts the convergence to a single state, hence balancing the exploration (diversification) and exploitation (convergence) property of the proposed method. 4. Reduction of the feature input set using the clustering step prior to QWOA for high dimensional datasets is instrumental in building a high performing decision system. 5. The performance of the QWOA is compared with the classical WOA and seven well-known meta-heuristic methods such as GA, PSO, BBA, GWO, SSA, QGA and QPSO. 6. Experiments on fourteen diversified datasets establish the merit of the proposed approach as a meta-heuristic method that performs well on challenging datasets of low and high feature dimension in comparison to of the methods in terms of a fitness, classification accuracy, area under the ROC curve and the selected number of features. 7. We have also used four well-known classifiers such as the k-Nearest Neighbour (kNN), the Linear discriminant classifier (LDC), the Support Vector Machine (SVM) and the C4.5 for the comparison. 8. Statistical test also demonstrates the significantly better performance of the QWOA in comparison to the other eight meta-heuristic methods.</p><p>*Highlights (for review)</p><p>J o u r n a l P r e -p r o o f</p><p>Journal Pre-proof</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of interests</head><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Genetic algorithms</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific american</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="73" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Genetic programming</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Koza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Genetic algorithms and their applications</title>
		<author>
			<persName><forename type="first">K.-S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-F</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="22" to="37" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Genetic algorithms in wireless networking: techniques, applications, and issues</title>
		<author>
			<persName><forename type="first">U</forename><surname>Mehboob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vasilakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2467" to="2501" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Genetic algorithms and Darwinian approaches in financial applications: A survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Aguilar-Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valenzuela-Rendón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rodríguez-Ortiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="7684" to="7697" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ant colony optimization: a new meta-heuristic</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dorigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Di</forename><surname>Caro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 congress on evolutionary computation-CEC99 (Cat. No. 99TH8406)</title>
		<meeting>the 1999 congress on evolutionary computation-CEC99 (Cat. No. 99TH8406)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1995 IEEE International Conference on Neural Networks</title>
		<meeting>the 1995 IEEE International Conference on Neural Networks<address><addrLine>Perth, Australia, IEEE Service Center, Piscataway, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1942">1995. 1942</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An artificial bee colony (ABC) algorithm for numeric function optimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Basturk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Swarm Intelligence Symposium</title>
		<meeting><address><addrLine>Indianapolis, IN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Binary bat algorithm for feature selection</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y M</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A M</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A P</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Papa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Swarm Intelligence and Bio-Inspired Computation</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="225" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Grey wolf optimizer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in engineering software</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="46" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An efficient binary salp swarm algorithm with crossover scheme for feature selection problems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Faris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="page" from="43" to="67" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Study on an improved adaptive PSO algorithm for solving multi-objective gate assignment</title>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="288" to="302" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Controlling the movement of crowds in computer graphics by using the mechanism of particle swarm optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1170" to="1176" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deploying swarm intelligence in medical imaging</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Al-Rifaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="14" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evolutionary programming made faster</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="82" to="102" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Differential evolution-a simple and efficient heuristic for global optimization over continuous spaces</title>
		<author>
			<persName><forename type="first">R</forename><surname>Storn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of global optimization</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="341" to="359" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reducing the time complexity of the derandomized evolution strategy with covariance matrix adaptation (CMA-ES)</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koumoutsakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bat algorithm for multi-objective optimisation</title>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1203.6571</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-objective grey wolf optimizer: a novel algorithm for multi-criterion optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saremi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Coelho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="106" to="119" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hybrid genetic algorithms for feature selection</title>
		<author>
			<persName><forename type="first">I.-S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-R</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1424" to="1437" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature subset selection using a genetic algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Feature extraction, construction and selection</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="117" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A distributed PSO-SVM hybrid system with feature selection and parameter optimization</title>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Dun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied soft computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1381" to="1391" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Particle swarm optimization for feature selection in classification: A multi-objective approach</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1656" to="1671" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An advanced ACO algorithm for feature subset selection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kashef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nezamabadi-Pour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="271" to="279" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Binary grey wolf optimization approaches for feature selection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Emary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Zawbaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassanien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="371" to="381" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gene selection for cancer classification using support vector machines</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barnhill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="389" to="422" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wrappers for feature subset selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="273" to="324" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Whale optimization approaches for wrapper feature selection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mafarja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="441" to="453" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The whale optimization algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Engineering Software</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="51" to="67" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Quantum-inspired evolutionary algorithm for a class of combinatorial optimization</title>
		<author>
			<persName><forename type="first">K.-H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="580" to="593" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Quantum-inspired genetic algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference</title>
		<meeting>IEEE International Conference</meeting>
		<imprint>
			<date type="published" when="1996">1996. 1996</date>
			<biblScope unit="page" from="61" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Clustering in conjunction with quantum genetic algorithm for relevant genes selection for cancer microarray data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sardana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kaur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="428" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cancer feature selection and classification using a binary quantum-behaved particle swarm optimization and support vector machine</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational and mathematical Methods in Medicine</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Particle swarm optimization with particles having quantum behavior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 congress on evolutionary computation</title>
		<meeting>the 2004 congress on evolutionary computation</meeting>
		<imprint>
			<publisher>IEEE Cat</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">8753</biblScope>
			<biblScope unit="page" from="325" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">An introduction to genetic algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hybridization of genetic and quantum algorithm for gene selection and classification of microarray data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abderrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-G</forename><surname>Talbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khaled</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Foundations of Computer Science</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="431" to="444" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">ASU datasets</title>
		<ptr target="http://featureselection.asu.edu/old/datasets.php" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Text Datasets</title>
		<ptr target="http://tunedit.org/repo/Data/Text-wc" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">UCI datasets</title>
		<ptr target="https://archive.ics.uci.edu/ml/index.php" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multiclass cancer diagnosis using tumor gene expression signatures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="15149" to="15154" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A practical tutorial on the use of nonparametric statistical tests as a methodology for comparing evolutionary and swarm intelligence algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Derrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Swarm and Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An unsupervised approach to feature discretization and selection</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3048" to="3060" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Chemosensitivity prediction by transcriptional profiling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Staunton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="10787" to="10792" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Selecting feature subset for high dimensional data via the propositional FOIL rules</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="199" to="214" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A review of microarray datasets and applied feature selection methods</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bolón-Canedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sánchez-Marono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alonso-Betanzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Benítez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">282</biblScope>
			<biblScope unit="page" from="111" to="135" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A fast clustering-based feature subset selection algorithm for high-dimensional data</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">An incremental feature selection approach based on scatter matrices for classification of cancer microarray data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sardana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kaur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Mathematics</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="277" to="295" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">High-dimensional supervised feature selection via optimized kernel mutual information</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="81" to="95" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Salp Swarm algorithm: theory, literature review, and application in extreme learning machines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Faris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Aljarah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mafarja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Heidari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature-Inspired Optimizers</title>
		<imprint>
			<biblScope unit="page" from="185" to="199" />
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">CLASSIFICATION OF CLASS OVERLAPPING DATASETS BY KERNEL-MTS METHOD</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INTERNATIONAL JOURNAL OF INNOVATIVE Authors Contribution Section</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Agrawal: Proposed the quantum version of the Whale Optimization Algorithm</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Contributed in the development of the proposed the quantum version of the Whale Optimization Algorithm and conducted experiments</title>
		<author>
			<persName><forename type="first">Baljeet</forename><surname>Kaur</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Partial conduction of experiments. *Author Contributions Section J</title>
		<author>
			<persName><forename type="first">Surbhi</forename><surname>Sharma</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
