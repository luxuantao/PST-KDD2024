<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Contextualized Self-Attention Network for Session-based Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chengfeng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of AI</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Zhejiang Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Pengpeng</forename><surname>Zhao</surname></persName>
							<email>ppzhao@suda.edu.cn.</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of AI</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Zhejiang Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of IIP of CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanchi</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<region>New Jersey</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Sheng</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">The University of Central Arkansas</orgName>
								<address>
									<settlement>Conway</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiajie</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of AI</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="laboratory">Key Lab of IIP of CAS</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junhua</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of AI</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Technology</orgName>
								<orgName type="institution">Soochow University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaofang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Zhejiang Lab</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">The University of Queensland</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Contextualized Self-Attention Network for Session-based Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Session-based recommendation, which aims to predict the user's immediate next action based on anonymous sessions, is a key task in many online services (e.g., e-commerce, media streaming). Recently, Self-Attention Network (SAN) has achieved significant success in various sequence modeling tasks without using either recurrent or convolutional network. However, SAN lacks local dependencies that exist over adjacent items and limits its capacity for learning contextualized representations of items in sequences. In this paper, we propose a graph contextualized self-attention model (GC-SAN), which utilizes both graph neural network and self-attention mechanism, for sessionbased recommendation. In GC-SAN, we dynamically construct a graph structure for session sequences and capture rich local dependencies via graph neural network (GNN). Then each session learns long-range dependencies by applying the self-attention mechanism. Finally, each session is represented as a linear combination of the global preference and the current interest of that session. Extensive experiments on two real-world datasets show that GC-SAN outperforms state-of-the-art methods consistently.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recommender systems play an important role in helping users alleviate the problem of information overload and select interesting contents in many applications domains, e.g., ecommerce, music, and social media. Most of existing recommender systems are based on user historical interactions. However, in many application scenarios, user identification may be unknown and there are only user historical actions during an ongoing session. To solve this problem, sessionbased recommendation is proposed to predict the next action (e.g., click on an item) that a user may take based on the sequence of the user's previous behaviors in the current session.</p><p>Due to its highly practical value, many kinds of approaches for session-based recommendation have been proposed. Markov Chain (MC) is a classic example, which assumes that the next action is based on the previous ones <ref type="bibr" target="#b7">[Rendle et al., 2010]</ref>. With such a strong assumption, an independent combination of the past interactions may limit the accuracy of recommendation. Recent studies have highlighted the importance of using recurrent neural network (RNN) in session-based recommender systems and obtained promising results <ref type="bibr" target="#b10">[Zhao et al., 2019]</ref>. For instance, Hidasi et al. <ref type="bibr" target="#b1">[Hidasi et al., 2016]</ref> proposed to model short-term preferences with GRU (a variant of RNN), and then an improved version <ref type="bibr" target="#b7">[Tan et al., 2016]</ref> is proposed to further boost its recommendation performance. Recently, NARM <ref type="bibr">[Li et al., 2017]</ref> is designed to capture the user's sequential pattern and main purpose simultaneously by employing a global and local RNN. However, the existing methods usually model single-way transitions between consecutive items and neglect complex transitions among the entire session sequence.</p><p>More recently, a new sequential model, T ransf ormer <ref type="bibr">[Vaswani et al., 2017]</ref>, has achieved state-of-the-art performance and efficiency in various translation tasks. Instead of using recurrence or convolution, Transformer utilizes an encoder-decoder structure composed of stacked self-attention network to draw global dependencies between input and output. Self-attention, as a special attention mechanism, has been widely used to model the sequential data and achieved remarkable results in many applications, e.g., machine translation <ref type="bibr">[Vaswani et al., 2017]</ref>, sentiment analysis <ref type="bibr" target="#b4">[Lin et al., 2017]</ref>, and sequential recommendation <ref type="bibr" target="#b2">[Kang and McAuley, 2018;</ref><ref type="bibr" target="#b10">Zhou et al., 2018]</ref>. The success of the Transformer model can be attributed to its self-attention network, which takes full account of all signals with a weighted averaging operation. Despite its success, such an operation disperses the distribution of attention, which results in lacking local dependencies over adjacent items and limiting its capacity for learning contextualized representations of items <ref type="bibr" target="#b6">[Liu et al., 2019]</ref>. While the local contextual information of adjacent items has been shown that it can enhance the ability of modeling dependencies among neural representations, especially for the attention models <ref type="bibr" target="#b8">[Yang et al., 2018;</ref><ref type="bibr" target="#b6">Liu et al., 2019]</ref>.</p><p>In this work, we propose to strengthen self-attention network through graph neural network (GNN). On the one hand, the strength of self-attention is to capture long-range dependencies by explicitly attending to all the positions. On the other hand, GNN is capable of providing rich local contextual information by encoding edge or node attribute features <ref type="bibr" target="#b0">[Battaglia et al., 2018]</ref>. Specifically, we introduce a graph contextual self-attention network, named GC-SAN, for session-based recommendation, which benefits from the complementary strengths of GNN and self-attention. We first construct a directed graph from all historical session sequences. Based on the session graph, GC-SAN is able to capture transitions of neighbor items and generate the latent vectors for all nodes involved in the graph correspondingly. Then we apply the self-attention mechanism to model longrange dependencies regardless of the distance, where session embedding vectors are composed by the latent vectors of all nodes in the graph. Finally, we use the linear weighted sum of the user's global interests and his/her local interests in that session as the embedding vector to predict the probability of clicking on the next item.</p><p>The main contributions of this work are summarized as follows.</p><p>‚Ä¢ To improve the representation of session sequences, we present a novel graph contextual self-attention model based on graph neural network (GC-SAN). GC-SAN utilizes the complementarity between self-attention network and graph neural network to enhance the recommendation performance. al. <ref type="bibr" target="#b7">[Rendle et al., 2010]</ref> proposed to combine the power of M-F and Markov Chain (MC) for next-basket recommendation. Though these methods are proved to be effective and widely employed, they only take into account the most recent click of the session, ignoring the global information of the whole click sequence.</p><p>Recently, researchers turn to neural networks and attention-based models for session-based recommender system. For instance, <ref type="bibr" target="#b1">Hidasi et al. [Hidasi et al., 2016]</ref> were among the first to explore Gated Recurrent Unit (GRU) as a special form of RNN for the prediction of the next action in a session, and later an improved version <ref type="bibr" target="#b7">[Tan et al., 2016]</ref> is proposed to boost its recommendation performance further. Nowadays, Graph Neural Network (GNN) has been proposed to learn the representation for graph structured data <ref type="bibr" target="#b7">[Scarselli et al., 2009;</ref><ref type="bibr" target="#b8">Wang et al., 2019]</ref> in the form of RNN, which is broadly applied for the different tasks, e.g., image classification <ref type="bibr" target="#b6">[Marino et al., 2017]</ref>, script event prediction <ref type="bibr" target="#b4">[Li et al., 2018]</ref> and recommender systems <ref type="bibr" target="#b8">[Wu et al., 2018]</ref>.</p><p>On the other hand, several attention-based mechanisms have been introduced across various applications, e.g. natural language processing and computer vision. Standard vanilla attention mechanism has been incorporated into recommender systems <ref type="bibr">[Li et al., 2017;</ref><ref type="bibr" target="#b5">Liu et al., 2018]</ref>. <ref type="bibr">More recently, Vaswani et al. [Vaswani et al., 2017]</ref> proposed to model the dependencies between words based entirely on self-attention without any recurrence or convolution, which has achieved state-of-the-art performance on machine translation task. Based on the simple and parallelized self-attention mechanism, <ref type="bibr" target="#b2">Kang et al. [Kang and McAuley, 2018]</ref> proposed a self-attention based sequential model, which outperforms MC/RNN/RNN-based sequential recommendation methods. Huang et al. <ref type="bibr" target="#b2">[Huang et al., 2018]</ref> proposed a unified contextual self-attention network at feature level to capture the polysemy of heterogeneous user behaviors for sequential recommendation.</p><p>Most existing sequential recommendation models utilize the self-attention mechanism to capture distant item-item transitions in a sequence and have achieved state-of-the-art performance. However, it is still challenging for establishing complex contextual information between adjacent items. In this paper, we strengthen the self-attention network through graph neural network and meanwhile maintain the model's simplicity and flexibility. To the best of our knowledge, this is the first attempt to complement SAN and GNN for sessionbased recommendation, in which the first one can model the global item-item information of a session and the latter is capable of learning local contextual information by encoding attribute features of constructing graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Graph Contextualized Self-Attention Network</head><p>In this section, we introduce the proposed contextualized selfattention recommendation model based on graph neural network (GC-SAN). We first formulate the problem of sessionbased recommendation, and then describe the architecture of our model in detail (As shown in Figure <ref type="figure">1</ref>). The general architecture of the proposed model. We first construct a directed graph of all session sequences. Based on the graph, we apply graph neural network to obtain all node vectors involved in the session graph. After that, we use a multi-layer self-attention network to capture long-range dependencies between items in the session. In prediction layer, we represent each session as a linear of the global preference and the current interest of that session. Finally, we compute the ranking scores of each candidate item for recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>Session-based recommendation aims to predict which item a user would like to click next, only based upon his/her current interaction sequence. Here we give a formulation of the session-based recommendation problem as below.</p><p>Let V = {v 1 , v 2 , ..., v |V | } denote a set of all unique items involved in all sessions. For each anonymous session, a sequence of clicked actions by the user are denoted as S = {s 1 , s 2 , ..., s n } in time order, where s t ‚àà V represents a clicked item of the user at time step t. Formally, our model aims to predict the next possible click (i.e., s t+1 ) for a given prefix of the action sequence truncated at time t, S t = {s 1 , s 2 , ..., s t‚àí1 , s t } (1 ‚â§ t &lt; n). To be exact, our model generates a ranking list over all candidate items that may occur in that session. ≈∑ = {≈∑ 1 , ≈∑2 , ..., y |V | } denotes the output probability for all items, where ≈∑i corresponds to the recommendation score of item v i . Since a recommender typically makes more than one recommendation for the user, thus we choose the top-N items from ≈∑ for recommendation. </p><formula xml:id="formula_0">ùêå I Incoming Matrix 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 2 1 1 2 2 3 3 4 4 4 4 3 3 2 2 1 1 1 2 v4 v1 v3 v2 ùêå O 4 4 3 3 2 2 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 2 2 3 3 4 4 Outgoing Matrix</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dynamic Graph Structure</head><p>Graph Construction. The first part of GNN is to a construct meaningful graph from all sessions. Given a session S = {s 1 , s 2 , ..., s n }, we treat each item s i as a node and (s i‚àí1 , s i ) as an edge which represents a user clicks item s i after s i‚àí1 in the session S. Therefore, each session sequence can be modeled as a directed graph. The graph structure is updated by promoting communication between different nodes. Specifically, let M I , M O ‚àà R n√ón denote weighted connections of outgoing and incoming edges in the session graph, respectively. For example, considering a session S = {s 1 , s 3 , s 2 , s 4 , s 3 }, the corresponding graph and the matrix (i.e., M I and M O ) are shown in Figure2. Since several items may appear in the session sequence repeatedly, we assign each edge with normalized weight, which is calculated as the occurrence of the edge divided by the outdegree of that edge's start node. Note that our model can support various strategies of constructing session graph and generate the corresponding connection matrices. Then we can apply the two weighted connection matrices with graph neural network to capture the local information of the session sequence.</p><p>Node Vectors Updating. Next, we present how to obtain latent feature vectors of nodes via graph neural network. We first convert every item v ‚àà V into an unified lowdimension latent space and the node vector s ‚àà R d denotes a d-dimensional real-valued latent vector of item v. For each node s t at time t in the graph session, given by the connection matrices M I and M O , the information propagation between different nodes can be formalized as:</p><formula xml:id="formula_1">a t = Concat(M I t ([s 1 , ..., s n ]W I a + b I ), M O t ([s 1 , ..., s n ]W O a + b O )),<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">W I a , W O a ‚àà R d√ód are the parameter matrices. b I , b O ‚àà R d are the bias vectors. M I t , M O t ‚àà R 1√ón</formula><p>are tth row of each matrix corresponding to node s t , respectively. a t extracts the contextual information of neighborhoods for node s t . Then we take them and the previous state s t‚àí1 as input and feed into the graph neural network. Thus, the final output h t of GNN layer is computed as follows.</p><formula xml:id="formula_3">z t = œÉ(W z a t + P z s t‚àí1 ), r t = œÉ(W r a t + P r s t‚àí1 ), ht = tanh(W h a t + P h (r t s t‚àí1 )), h t = (1 ‚àí z t ) s t‚àí1 + z t ht .</formula><p>(2) where W z , W r , W h ‚àà R 2d√ód , P z , P r , P h ‚àà R d√ód , are learnable parameters. œÉ(‚Ä¢) represents the logistic sigmoid function and denotes element-wise multiplication. z t , r t are update gate and reset gate, which decide what information to be preserved and discarded, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Self-Attention Layers</head><p>Self-attention is a special case of the attention mechanism and has been successfully applied in lots of research topics including NLP <ref type="bibr">[Vaswani et al., 2017]</ref> and QA <ref type="bibr" target="#b4">[Li et al., 2019]</ref>. The self-attention mechanism can draw global dependencies between input and output, and capture item-item transitions across the entire input and output sequence itself without regard to their distances.</p><p>Self-Attention Layer. After feeding a session sequence into the graph neural network, we can obtain the latent vectors of all nodes involved in the session graph, i.e., H = [h 1 , h 2 , ..., h n ]. Next, we feed them into the self-attention layer to better capture the global session preference.</p><formula xml:id="formula_4">F = sof tmax( (HW Q )(HW K ) T ‚àö d )(HW V )<label>(3)</label></formula><p>where the projection matrices</p><formula xml:id="formula_5">W Q , W K , W V ‚àà R 2d√ód .</formula><p>Point-Wise Feed-Forward Network. After that, we apply two linear transformations with a ReLU activation function to endow the model with nonlinearity and consider interactions between different latent dimensions. However, transmission loss may occur in self-attention operations. Thus we add a residual connection after the feed-forward network, which makes the model much easier to leverage low-layer information inspired by <ref type="bibr">[Vaswani et al., 2017]</ref>.</p><formula xml:id="formula_6">E = ReLU (FW 1 + b 1 )W 2 + b 2 + F (4)</formula><p>where W 1 and W 2 are d √ó d matrices, b 1 and b 2 are ddimensional bias vectors. Moreover, to alleviate overfitting problems in deep neural networks, we apply "Dropout" regularization techniques during training. For simplicity, we define the above whole self-attention mechanism as:</p><formula xml:id="formula_7">E = SAN (H)<label>(5)</label></formula><p>Multi-layer Self-Attention. Recent work shows that different layers capture different types of features. In this work, we investigate which levels of layers benefit most from the features modeling to learn more complex item transitions. The 1-st layer is defined as E (<ref type="foot" target="#foot_1">1</ref>) = E. The k-th (k &gt; 1) self-attention layer is defined as:</p><formula xml:id="formula_8">E (k) = SAN (E (k‚àí1) )<label>(6)</label></formula><p>where E (k) ‚àà R n√ód is the final output of the multi-layer selfattention network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prediction Layer</head><p>After several self-attention blocks that adaptively extract sequential information of sessions, we achieve the long-term self-attentive representation E (k) . To better predict the user's next clicks, we combine the long-term preference and the current interest of the session, and then use this combined embedding as the session representation. For a session S = {s 1 , s 2 , ..., s n }, we take the last dimensions of E (k) as the global embedding following <ref type="bibr" target="#b2">[Kang and McAuley, 2018]</ref>. The local embedding can be simply defined as the last clicked-item vector, i.e., h n . Then we weight them together as the final session embedding.</p><formula xml:id="formula_9">S f = œâE (k) n + (1 ‚àí œâ)h n (7)</formula><p>where</p><formula xml:id="formula_10">E (k)</formula><p>n ‚àà R d represent n-th row of the matrix. Finally, we predict the next click for each candidate item v i ‚àà V given session embedding S f as follows:</p><formula xml:id="formula_11">≈∑i = sof tmax(S T f v i ). (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>where ≈∑i denotes the recommendation probability of item v i to be the next click in session S. Finally, we train our model by minimizing the following objective function:</p><formula xml:id="formula_13">J = ‚àí n i=1 y i log(≈∑ i ) + (1 ‚àí y i )log(1 ‚àí ≈∑i ) + Œª||Œ∏|| 2 . (<label>9</label></formula><formula xml:id="formula_14">)</formula><p>where y denotes the one-hot encoding vector of the ground truth item, Œ∏ is the set of all learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Analysis</head><p>In this section, we first set up the experiment. And then we conduct experiments to answer the following questions: RQ1: Does the proposed graph contextualized self-attention session-based recommendation model (GC-SAN) achieve state-of-the-art performance? RQ2: How do the key hyper-parameters affect model performance, such as the weight factor and embedding size ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets.</p><p>We study the effectiveness of our proposed approach GC-SAN on two real-world datasets, i.e., Diginetica 1 and Retailrocket<ref type="foot" target="#foot_2">2</ref> . Diginetica dataset comes from CIKM Cup 2016, where only the transactional data is used in this study. Retailrocket dataset is published by a personalized e-commerce company, which contains six months of user browsing activities. To filter noisy data, we filter out items appearing less than 5 times and then remove all sessions with fewer than 2 items on both datasets. Furthermore, for session-based recommendation, we set the sessions data of last week as the test data, and the remaining for training. Similar to <ref type="bibr" target="#b7">[Tan et al., 2016;</ref><ref type="bibr" target="#b9">Yuan et al., 2019]</ref>, for a session sequence S = {s 1 , s 2 , ..., s n }, we generate the input and corresponding labels ({s 1 }, s 2 ), ({s 1 , s 2 }, s 3 )...({s 1 , ..., s (n‚àí1) }, s n ) for training and testing on both datasets. After preprocessing, the statistics of the datasets are shown in Table <ref type="table" target="#tab_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets Diginetica Retailrocket</head><p>Measures HR@5 HR@10 MRR@5 MRR@10 NDCG@5 NDCG@10 HR@5 HR@10 MRR@5 MRR@10 NDCG@5 NDCG@10 Evaluation Metrics. To evaluate the recommendation performance of all models, we adopt three common metrics, i.e., Hit Rate (HR@N), Mean Reciprocal Rank (MRR@N) and Normalized Discounted Cumulative Gain (NDCG@N). The former one is an evaluation of unranked retrieval results, while the latter two are evaluations of ranked lists. Here, we consider Top-N (N = {5, 10}) for recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We consider the following compared methods for performance comparisons:</p><p>‚Ä¢ Pop is a simple baseline that recommends top rank items based on popularity in training data. ‚Ä¢ BPR-MF <ref type="bibr" target="#b7">[Rendle et al., 2009]</ref> is the state-of-the-art method for non-sequential recommendation, which optimizes matrix factorization using a pairwise ranking loss.</p><p>‚Ä¢ <ref type="bibr">IKNN [Sarwar et al., 2001]</ref> is a traditional item-to-item model, which recommends items similar to the candidate item within the session based on cosine similarity. ‚Ä¢ FPMC<ref type="foot" target="#foot_3">3</ref>  <ref type="bibr" target="#b7">[Rendle et al., 2010]</ref> is a classic hybrid model combing matrix factorization and first-order Markov chain for next-basket recommendation. Note that in our recommendation problem, each basket is a session. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons of Performance</head><p>To demonstrate the recommendation performance of our model GC-SAN, we compare it with other state-of-the-art methods (RQ1). The experimental results of all methods on Diginetica and Retailrocket datasets are illustrated in Table1, and we have the following observations. The non-personalized Popularity-based methods (i.e., Pop) has the most unfavorable performance on both datasets. By profiling users individually and optimizing the pairwise ranking loss function, BPR-MF performs better than Pop. This suggests the importance of personalization in recommendation tasks. IKNN and FPMC achieve better performance than BPR-MF on Diginetica dataset, while BPR-MF outperforms IKNN and FPMC on Retailrocket dataset. In fact, IKNN utilizes the similarity between items in the session and FPMC is based on first-order Markov Chain.</p><p>All of the neural network methods, such as GRU4Rec and STAMP, outperform the traditional baselines (e.g., F-PMC and IKNN) in nearly all the cases, which verifies the power of deep learning technology in this field. GRU4Rec leverages the recurrent structure with GRU as a special form of RNN to capture the user's general preference, while STAMP improves the short-term memory through the last clicked item. Unsurprisingly, STAMP performs better than GRU4Rec, which indicates the effectiveness of short-term behavior for predicting the next item problem. On the other hand, by modeling every session as a graph and applying graph neural network and the attention mechanism, SR-GNN outperforms all other baselines on both datasets. This further proves the power of neural network in recommender systems.</p><p>Compared to SR-GNN, our approach GC-SAN adopts the self-attention mechanism to adaptively assign weights to previous items regardless of their distances in the current session and captures long-range dependencies between items of a session. We combine the long-range self-attention representation and the short-term interest of the last-click in a linear way to generate the final session representation. As we can see, our method achieves the best performance among all the methods on both datasets in terms of HR, MRR, and NDCG. These results demonstrate the efficacy and validity of GC-SAN for session-based recommendation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Analysis and Discussion</head><p>In this subsection, we take an in-depth model analysis study, aiming to further understand the framework of GC-SAN (RQ2). Due to the space limit, we only show the analysis results in terms of HR@10 and NDCG@10. We have obtained similar experimental results in terms of other metrics. Table <ref type="table">3</ref>: The performance of GC-SAN with and without graph neural network in terms of HR@10 and NDCG@10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of graph neural network.</head><p>Although we can infer the effectiveness of graph neural network implicitly from Table <ref type="table" target="#tab_1">1</ref>, we would like to verify the contribution of graph neural network in GC-SAN. We remove the graph neural network module from GC-SAN, replace it with a randomly initialized item embedding, and feed into the self-attention layer. Table <ref type="table">3</ref> displays the comparisons between with and without GNN. From Table <ref type="table" target="#tab_1">1 and Table 3</ref>, we find that even without GNN, GC-SAN can still outperform STAMP on Retailrocket dataset, while it was beaten by GRU4Rec on Diginetica dataset. In fact, the maximum session length of Retailrocket dataset is almost four times that of Diginetica dataset. A possible reason is that short sequence lengths can construct more dense session graphs that provide richer contextual information, while the self-attention mechanism performs better with long sequence lengths. This further demonstrates that the self-attention mechanism and graph neural network play important roles in improving recommendation performance.</p><p>Impact of weight factor œâ. The weight parameter œâ controls the contribution of self-attention representation and the last-clicked action. Observing from Figure <ref type="figure" target="#fig_2">3</ref>(a), taking only global self-attention dependencies (œâ = 1.0) as final session embedding usually achieves a better performance than considering only current interests (œâ = 0). Setting œâ to a value from 0.4 to 0.8 is more desirable. This indicates that while the self-attention mechanism with graph neural network can adaptively assign weights to focus on long-range dependencies or more recent actions, the short-term interest is also indispensable for improving the recommendation performance.  Impact of the number of self-attention blocks k. As aforementioned, we investigate which levels of self-attention layers benefit most from GC-SAN. Figure <ref type="figure" target="#fig_2">3</ref>(b) displays the experimental results of applying different self-attention blocks with k varying from 1 to 6. On both datasets, we can observe that increasing k can boost the performance of GC-SAN. However, it achieves the best performance when k is chosen properly and gets worse for a larger k. This may be because using more blocks (k ‚â• 4) would make GC-SAN easier to lose low-layer information. Impact of the embedding size d. In Figure <ref type="figure" target="#fig_4">4</ref>, we investigate the effectiveness of the embedding size d ranging from 10 to 120 on both datasets. Among all the baselines, STAM-P and SR-GNN perform well and stable. Hence, we use S-TAMP and SR-GNN as two baselines for ease of comparisons. From figure <ref type="figure" target="#fig_4">4</ref>, we can observe that our model GC-SAN consistently outperforms STAMP on all latent dimensions. When d is less than a certain value, SR-GNN performs better than GC-SAN. Once this value is exceeded, the performance of GC-SAN still grows and eventually stabilizes with d ‚â• 100, while the performance of SR-GNN slightly reduces. This may be because that a relatively small d limits GC-SAN to capture complex transitions between item latent factors, while SR-GNN may suffer from overfitting with a larger d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed a graph contextualized selfattention network (GC-SAN) based on graph neural network for session-based recommendation. Specifically, we first constructed directed graphs from anonymous session records and then applied graph neural network to generate new latent vectors for all items, which contained local contextual information of sequences. Next, we used the self-attention network to capture global dependencies between distant position. Finally, we combined the local short-term dynamics (i.e., the last-clicked item) and global self-attended dependencies to represent session sequences in a linear way. Extensive experimental analysis verified that our proposed model GC-SAN consistently outperformed the state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure1: The general architecture of the proposed model. We first construct a directed graph of all session sequences. Based on the graph, we apply graph neural network to obtain all node vectors involved in the session graph. After that, we use a multi-layer self-attention network to capture long-range dependencies between items in the session. In prediction layer, we represent each session as a linear of the global preference and the current interest of that session. Finally, we compute the ranking scores of each candidate item for recommendation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of a session graph structure and the connection matrices M I and M O .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Effects of the weight factor œâ and effects of the number of stacked self-attention blocks k on both datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The performance under different embedding sizes d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The performance of different methods on the two datasets. We generate Top-5 and 10 items for recommendation. The best performance in each column is boldfaced (the higher, the better). Improvements over the best baseline are shown in the last row.</figDesc><table><row><cell>Pop</cell><cell>0.0036</cell><cell cols="2">0.0077</cell><cell cols="2">0.0019</cell><cell>0.0025</cell><cell>0.0023</cell><cell>0.0037</cell><cell>0.0133</cell><cell>0.0208</cell><cell>0.0066</cell><cell>0.0076</cell><cell>0.0082</cell><cell>0.0107</cell></row><row><cell>BPR-MF</cell><cell>0.1060</cell><cell cols="2">0.1292</cell><cell cols="2">0.0789</cell><cell>0.0842</cell><cell>0.0586</cell><cell>0.0672</cell><cell>0.2106</cell><cell>0.2719</cell><cell>0.1356</cell><cell>0.1407</cell><cell>0.1138</cell><cell>0.1322</cell></row><row><cell>IKNN</cell><cell>0.1407</cell><cell cols="2">0.2083</cell><cell cols="2">0.0776</cell><cell>0.0867</cell><cell>0.0693</cell><cell>0.0902</cell><cell>0.1709</cell><cell>0.2248</cell><cell>0.0972</cell><cell>0.1043</cell><cell>0.0855</cell><cell>0.1020</cell></row><row><cell>FPMC</cell><cell>0.1855</cell><cell cols="2">0.2309</cell><cell cols="2">0.0875</cell><cell>0.0986</cell><cell>0.0811</cell><cell>0.1037</cell><cell>0.1732</cell><cell>0.2319</cell><cell>0.1013</cell><cell>0.1152</cell><cell>0.0901</cell><cell>0.1095</cell></row><row><cell>GRU4Rec</cell><cell>0.2577</cell><cell cols="2">0.3657</cell><cell cols="2">0.1434</cell><cell>0.1577</cell><cell>0.1276</cell><cell>0.1607</cell><cell>0.2196</cell><cell>0.2869</cell><cell>0.1286</cell><cell>0.1489</cell><cell>0.1076</cell><cell>0.1323</cell></row><row><cell>STAMP</cell><cell>0.3998</cell><cell cols="2">0.5014</cell><cell cols="2">0.2357</cell><cell>0.2469</cell><cell>0.2039</cell><cell>0.2394</cell><cell>0.3287</cell><cell>0.3972</cell><cell>0.2241</cell><cell>0.2334</cell><cell>0.1758</cell><cell>0.1970</cell></row><row><cell>SR-GNN</cell><cell>0.4082</cell><cell cols="2">0.5269</cell><cell cols="2">0.2439</cell><cell>0.2599</cell><cell>0.2078</cell><cell>0.2443</cell><cell>0.3502</cell><cell>0.4268</cell><cell>0.2422</cell><cell>0.2525</cell><cell>0.1885</cell><cell>0.2121</cell></row><row><cell>GC-SAN</cell><cell>0.4280</cell><cell cols="2">0.5351</cell><cell cols="2">0.2694</cell><cell>0.2838</cell><cell>0.2223</cell><cell>0.2552</cell><cell>0.3644</cell><cell>0.4380</cell><cell>0.2506</cell><cell>0.2604</cell><cell>0.1956</cell><cell>0.2181</cell></row><row><cell>Improv.</cell><cell>4.84%</cell><cell cols="2">1.56%</cell><cell cols="2">10.46%</cell><cell>9.20%</cell><cell>6.98%</cell><cell>4.44%</cell><cell>4.07%</cell><cell>2.62%</cell><cell>3.49%</cell><cell>3.15%</cell><cell>3.79%</cell><cell>2.85%</cell></row><row><cell>Dataset</cell><cell cols="2"># clicks</cell><cell cols="2"># train</cell><cell># test</cell><cell># items</cell><cell>avg.len</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Diginetica</cell><cell cols="2">858,107</cell><cell cols="2">526,134</cell><cell>44,279</cell><cell>40,840</cell><cell>5.97</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Retailrocket</cell><cell cols="2">710,856</cell><cell cols="2">433,648</cell><cell>15,132</cell><cell>36,968</cell><cell>5.43</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of datasets.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">http://cikm2016.cs.iupui.edu/cikm-cup/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">https://www.kaggle.com/retailrocket/ecommerce-dataset Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">http://github.com/khesui/FPMC</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">http://github.com/hidasib/GRU4Rec</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">http://github.com/CRIPAC-DIG/SR-GNN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6">Proceedings the International Joint Conference on Artificial Intelligence </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was partially supported by NSFC (No.  61876117, 61876217, 61872258, 61728205), Major Project of Zhejiang Lab (No. 2019DH0ZX01), Open Program of Key Lab of IIP of CAS (No. IIP2019-1) and PAPD.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast matrix factorization for online recommendation with implicit feedback</title>
		<author>
			<persName><forename type="first">Jannach</forename><surname>Bonnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffray</forename><surname>Bonnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Jannach ; Al. ; Xiangnan He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bal√°zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk</title>
				<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2015">2015. 2015. 2016. 2016. 2018. 2018. 2016. 2016</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="2354" to="2366" />
		</imprint>
	</monogr>
	<note>Sessionbased recommendations with recurrent neural networks</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Csan: Contextual self-attention network for user sequential recommendation</title>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018. 2018. 2018</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
	<note>ICDM</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural attentive session-based recommendation</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="1419" to="1428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Beyond rnns: Positional self-attention with co-attention for video question answering</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.05081</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<meeting><address><addrLine>Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang; Bowen</addrLine></address></meeting>
		<imprint>
			<publisher>Zhouhan Lin</publisher>
			<date type="published" when="2017">2018. 2018. 2019. 2019. 2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stamp: short-term attention/memory priority model for session-based recommendation</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1831" to="1839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ruslan Salakhutdinov, and Abhinav Gupta. The more you know: Using knowledge graphs for image classification</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="2017">2019. 2019. 2017. 2017</date>
			<publisher>Kenneth Marino</publisher>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Christoph Freudenthaler, and Lars Schmidt-Thieme. Factorizing personalized markov chains for next-basket recommendation</title>
		<author>
			<persName><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
				<editor>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">≈Åukasz</forename><surname>Kaiser</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2009. 2009. 2010. 2001. 2001. 2009. 2009. 2016. 2016. 2017. 2017</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Session-based recommendation with graph neural networks</title>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2018">2019. 2019. 2018. 2018. 2018. 2018</date>
		</imprint>
	</monogr>
	<note>Modeling localness for self-attention networks</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A simple convolutional generative network for next item recommendation</title>
		<author>
			<persName><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="582" to="590" />
		</imprint>
	</monogr>
	<note>Alexandros Karatzoglou an-dIoannis Arapakis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Atrank: An attention-based user behavior modeling framework for recommendation</title>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2019. 2019. 2018. 2018</date>
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
