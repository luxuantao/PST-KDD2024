<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Variance Reduction in Stochastic Gradient Descent and its Asynchronous Variants</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sashank</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ahmed</forename><surname>Hefny</surname></persName>
							<email>ahefny@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
							<email>suvrit@mit.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
							<email>bapoczos@cs.cmu.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">On Variance Reduction in Stochastic Gradient Descent and its Asynchronous Variants</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DC06E58FCEAEB9327AC117596FB4CF94</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study optimization algorithms based on variance reduction for stochastic gradient descent (SGD). Remarkable recent progress has been made in this direction through development of algorithms like SAG, SVRG, SAGA. These algorithms have been shown to outperform SGD, both theoretically and empirically. However, asynchronous versions of these algorithms-a crucial requirement for modern large-scale applications-have not been studied. We bridge this gap by presenting a unifying framework for many variance reduction techniques. Subsequently, we propose an asynchronous algorithm grounded in our framework, and prove its fast convergence. An important consequence of our general approach is that it yields asynchronous versions of variance reduction algorithms such as SVRG and SAGA as a byproduct. Our method achieves near linear speedup in sparse settings common to machine learning. We demonstrate the empirical performance of our method through a concrete realization of asynchronous SVRG.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There has been a steep rise in recent work <ref type="bibr">[6, 7, 9-12, 25, 27, 29]</ref> on "variance reduced" stochastic gradient algorithms for convex problems of the finite-sum form:</p><formula xml:id="formula_0">min x2R d f (x) := 1 n X n i=1 f i (x). (1.1)</formula><p>Under strong convexity assumptions, such variance reduced (VR) stochastic algorithms attain better convergence rates (in expectation) than stochastic gradient descent (SGD) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>, both in theory and practice. <ref type="foot" target="#foot_0">1</ref> The key property of these VR algorithms is that by exploiting problem structure and by making suitable space-time tradeoffs, they reduce the variance incurred due to stochastic gradients. This variance reduction has powerful consequences: it helps VR stochastic methods attain linear convergence rates, and thereby circumvents slowdowns that usually hit SGD.</p><p>Although these advances have great value in general, for large-scale problems we still require parallel or distributed processing. And in this setting, asynchronous variants of SGD remain indispensable <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>. Therefore, a key question is how to extend the synchronous finite-sum VR algorithms to asynchronous parallel and distributed settings.</p><p>We answer one part of this question by developing new asynchronous parallel stochastic gradient methods that provably converge at a linear rate for smooth strongly convex finite-sum problems.</p><p>Our methods are inspired by the influential SVRG <ref type="bibr" target="#b9">[10]</ref>, S2GD <ref type="bibr" target="#b11">[12]</ref>, SAG <ref type="bibr" target="#b24">[25]</ref> and SAGA <ref type="bibr" target="#b5">[6]</ref> family of algorithms. We list our contributions more precisely below.</p><p>Contributions. Our paper makes two core contributions: (i) a formal general framework for variance reduced stochastic methods based on discussions in <ref type="bibr" target="#b5">[6]</ref>; and (ii) asynchronous parallel VR algorithms within this framework. Our general framework presents a formal unifying view of several VR methods (e.g., it includes SAGA and SVRG as special cases) while expressing key algorithmic and practical tradeoffs concisely. Thus, it yields a broader understanding of VR methods, which helps us obtain asynchronous parallel variants of VR methods. Under sparse-data settings common to machine learning problems, our parallel algorithms attain speedups that scale near linearly with the number of processors.</p><p>As a concrete illustration, we present a specialization to an asynchronous SVRG-like method. We compare this specialization with non-variance reduced asynchronous SGD methods, and observe strong empirical speedups that agree with the theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work.</head><p>As already mentioned, our work is closest to (and generalizes) SAG <ref type="bibr" target="#b24">[25]</ref>, SAGA <ref type="bibr" target="#b5">[6]</ref>, SVRG <ref type="bibr" target="#b9">[10]</ref> and S2GD <ref type="bibr" target="#b11">[12]</ref>, which are primal methods. Also closely related are dual methods such as SDCA <ref type="bibr" target="#b26">[27]</ref> and Finito <ref type="bibr" target="#b6">[7]</ref>, and in its convex incarnation MISO <ref type="bibr" target="#b15">[16]</ref>; a more precise relation between these dual methods and VR stochastic methods is described in Defazio's thesis <ref type="bibr" target="#b4">[5]</ref>. By their algorithmic structure, these VR methods trace back to classical non-stochastic incremental gradient algorithms <ref type="bibr" target="#b3">[4]</ref>, but by now it is well-recognized that randomization helps obtain much sharper convergence results (in expectation). Proximal <ref type="bibr" target="#b28">[29]</ref> and accelerated VR methods have also been proposed <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26]</ref>; we leave a study of such variants of our framework as future work. Finally, there is recent work on lower-bounds for finite-sum problems <ref type="bibr" target="#b0">[1]</ref>.</p><p>Within asynchronous SGD algorithms, both parallel <ref type="bibr" target="#b20">[21]</ref> and distributed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref> variants are known.</p><p>In this paper, we focus our attention on the parallel setting. A different line of methods is that of (primal) coordinate descent methods, and their parallel and distributed variants <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Our asynchronous methods share some structural assumptions with these methods. Finally, the recent work <ref type="bibr" target="#b10">[11]</ref> generalizes S2GD to the mini-batch setting, thereby also permitting parallel processing, albeit with more synchronization and allowing only small mini-batches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A General Framework for VR Stochastic Methods</head><p>We focus on instances of (1.1) where the cost function f (x) has an L-Lipschitz gradient, so that krf (x) rf (y)k  Lkx yk, and it is -strongly convex, i.e., for all x, y 2 R d ,</p><formula xml:id="formula_1">f (x) f (y) + hrf (y), x yi + 2 kx yk 2 . (2.1)</formula><p>While our analysis focuses on strongly convex functions, we can extend it to just smooth convex functions along the lines of <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Inspired by the discussion on a general view of variance reduced techniques in <ref type="bibr" target="#b5">[6]</ref>, we now describe a formal general framework for variance reduction in stochastic gradient descent. We denote the collection {f i } n i=1 of functions that make up f in (1.1) by F. For our algorithm, we maintain an additional parameter ↵ t i 2 R d for each f i 2 F. We use A t to denote {↵ t i } n i=1 . The general iterative framework for updating the parameters is presented as Algorithm 1. Observe that the algorithm is still abstract, since it does not specify the subroutine SCHEDULEUPDATE. This subroutine determines the crucial update mechanism of {↵ t i } (and thereby of A t ). As we will see different schedules give rise to different fast first-order methods proposed in the literature. The part of the update based on A t is the key for these approaches and is responsible for variance reduction.</p><p>Next, we provide different instantiations of the framework and construct a new algorithm derived from it. In particular, we consider incremental methods SAG <ref type="bibr" target="#b24">[25]</ref>, SVRG <ref type="bibr" target="#b9">[10]</ref> and SAGA <ref type="bibr" target="#b5">[6]</ref>, and classic gradient descent GRADIENTDESCENT for demonstrating our framework.  In case of SVRG, SCHEDULEUPDATE is triggered every m iterations (here m denotes precisely the number of inner iterations used in <ref type="bibr" target="#b9">[10]</ref>); so A t remains unchanged for the m iterations and all ↵ t i are updated to the current iterate at the m th iteration. For SAGA, unlike SVRG, A t changes at the t th iteration for all t 2 [T ]. This change is only to a single element of A t , and is determined by the index i t (the function chosen at iteration t). The update of SAG is similar to SAGA insofar that only one of the ↵</p><formula xml:id="formula_2">ALGORITHM 1: GENERIC STOCHASTIC VARIANCE REDUCTION ALGORITHM Data: x 0 2 R d , ↵ 0 i = x 0 8i 2 [n] ,</formula><p>i is updated at each iteration. However, the update for A t+1 is based on i t+1 rather than i t . This results in a biased estimate of the gradient, unlike SVRG and SAGA. Finally, the schedule for gradient descent is similar to SAG, except that all the ↵ i 's are updated at each iteration. Due to the full update we end up with the exact gradient at each iteration. This discussion highlights how the scheduler determines the resulting gradient method.</p><p>To motivate the design of another schedule, let us consider the computational and storage costs of each of these algorithms. For SVRG, since we update A t after every m iterations, it is enough to store a full gradient, and hence, the storage cost is O(d). However, the running time is O(d) at each iteration and O(nd) at the end of each epoch (for calculating the full gradient at the end of each epoch). In contrast, both SAG and SAGA have high storage costs of O(nd) and running time of O(d) per iteration. Finally, GRADIENTDESCENT has low storage cost since it needs to store the gradient at O(d) cost, but very high computational costs of O(nd) at each iteration.</p><p>SVRG has an additional computation overhead at the end of each epoch due to calculation of the whole gradient. This is avoided in SAG and SAGA at the cost of additional storage. When m is very large, the additional computational overhead of SVRG amortized over all the iterations is small. However, as we will later see, this comes at the expense of slower convergence to the optimal solution. The tradeoffs between the epoch size m, additional storage, frequency of updates, and the convergence to the optimal solution are still not completely resolved. A straightforward approach to design a new scheduler is to combine the schedules of the above algorithms. This allows us to tradeoff between the various aforementioned parameters of our interest. We call this schedule hybrid stochastic average gradient (HSAG). Here, we use the schedules of SVRG and SAGA to develop HSAG. However, in general, schedules of any of these algorithms can HSAG:SCHEDULEUPDATE(x t , A t , t, I T ) for i = 1 to n do [n], it corresponds to SVRG. HSAG exhibits interesting storage, computational and convergence trade-offs that depend on S. In general, while large cardinality of S likely incurs high storage costs, the computational cost per iteration is relatively low. On the other hand, when cardinality of S is small and s i 's are large, storage costs are low but the convergence typically slows down. Before concluding our discussion on the general framework, we would like to draw the reader's attention to the advantages of studying Algorithm 1. First, note that Algorithm 1 provides a unifying framework for many incremental/stochastic gradient methods proposed in the literature. Second, and more importantly, it provides a generic platform for analyzing this class of algorithms. As we will see in Section 3, this helps us develop and analyze asynchronous versions for different finite-sum algorithms under a common umbrella. Finally, it provides a mechanism to derive new algorithms by designing more sophisticated schedules; as noted above, one such construction gives rise to HSAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVRG:SCHEDULEUPDATE({x</head><formula xml:id="formula_3">i } t+1 i=0 , A t , t, I T ) for i = 1 to n do ↵ t+1 i = (m | t)x t + (m6 | t)↵ t i ; end return A t+1 SAGA:SCHEDULEUPDATE({x i } t+1 i=0 , A t , t, I T ) for i = 1 to n do ↵ t+1 i = (i t = i)x t + (i t 6 = i)↵ t i ; end return A t+1 SAG:SCHEDULEUPDATE({x i } t+1 i=0 , A t , t, I T ) for i = 1 to n do ↵ t+1 i = (i t+1 = i)x t+1 + (i t+1 6 = i)↵ t i ; end return A t+1 GD:SCHEDULEUPDATE({x i } t+1 i=0 , A t , t, I T ) for i = 1 to n do ↵ t+1 i = x t+1 ; end return A t+1</formula><formula xml:id="formula_4">↵ t+1 i = ⇢ (i t = i)x t + (i t 6 = i)↵ t i if i 2 S (s i | t)x t + (s i 6 | t)↵ t i if i / 2 S end return A t+1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Convergence Analysis</head><p>In this section, we provide convergence analysis for Algorithm 1 with HSAG schedules. As observed earlier, SVRG and SAGA are special cases of this setup. Our analysis assumes unbiasedness of the gradient estimates at each iteration, so it does not encompass SAG. For ease of exposition, we assume that all s i = m for all i 2 [n]. Since HSAG is epoch-based, our analysis focuses on the iterates obtained after each epoch. Similar to <ref type="bibr" target="#b9">[10]</ref> (see Option II of SVRG in <ref type="bibr" target="#b9">[10]</ref>), our analysis will be for the case where the iterate at the end of (k + 1) st epoch, x km+m , is replaced with an element chosen randomly from {x km , . . . , x km+m 1 } with probability {p 1 , • • • , p m }. For brevity, we use xk to denote the iterate chosen at the k th epoch. We also need the following quantity for our analysis:</p><formula xml:id="formula_5">Gk , 1 n X i2S f i (↵ km i ) f i (x ⇤ ) hrf i (x ⇤ ), ↵ km i x ⇤ i .</formula><p>Theorem 1. For any positive parameters c, ,  &gt; 1, step size ⌘ and epoch size m, we define the following quantities:</p><formula xml:id="formula_6">=   1 ✓ 1 1  ◆ m ✓ 2c⌘(1 L⌘(1 + )) 1 n 2c  ◆ ✓ = max ⇢ 2c ✓ 1 1  ◆ m + 2Lc⌘ 2 ✓ 1 + 1 ◆   1 ✓ 1 1  ◆ m , ✓ 1 1  ◆ m .</formula><p>Suppose the probabilities p i / (1 1  ) m i , and that c, , , step size ⌘ and epoch size m are chosen such that the following conditions are satisfied:</p><formula xml:id="formula_7">1  + 2Lc⌘ 2 ✓ 1 + 1 ◆  1 n , &gt; 0, ✓ &lt; 1.</formula><p>Then, for iterates of Algorithm 1 under the HSAG schedule, we have</p><formula xml:id="formula_8">E h f (x k+1 ) f (x ⇤ ) + 1 Gk+1 i  ✓ E h f (x k ) f (x ⇤ ) + 1 Gk i .</formula><p>As a corollary, we immediately obtain an expected linear rate of convergence for HSAG. Corollary 1. Note that Gk 0 and therefore, under the conditions specified in Theorem 1 and with</p><formula xml:id="formula_9">✓ = ✓ (1 + 1/ ) &lt; 1 we have E ⇥ f (x k ) f (x ⇤ ) ⇤  ✓k ⇥ f (x 0 ) f (x ⇤ ) ⇤ .</formula><p>We emphasize that there exist values of the parameters for which the conditions in Theorem 1 and Corollary 1 are easily satisfied. For instance, setting ⌘ = 1/16( n + L),  = 4/ ⌘, = (2 n + L)/L and c = 2/⌘n, the conditions in Theorem 1 are satisfied for sufficiently large m. Additionally, in the high condition number regime of L/ = n, we can obtain constant ✓ &lt; 1 (say 0.5) with m = O(n) epoch size (similar to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>). This leads to a computational complexity of O(n log(1/✏)) for HSAG to achieve ✏ accuracy in the objective function as opposed to O(n 2 log(1/✏)) for batch gradient descent method. Please refer to the appendix for more details on the parameters in Theorem 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Asynchronous Stochastic Variance Reduction</head><p>We are now ready to present asynchronous versions of the algorithms captured by our general framework. We first describe our setup before delving into the details of these algorithms. Our model of computation is similar to the ones used in Hogwild! <ref type="bibr" target="#b20">[21]</ref> and AsySCD <ref type="bibr" target="#b13">[14]</ref>. We assume a multicore architecture where each core makes stochastic gradient updates to a centrally stored vector x in an asynchronous manner. There are four key components in our asynchronous algorithm; these are briefly described below.</p><p>1. Read: Read the iterate x and compute the gradient rf it (x) for a randomly chosen i t . 2. Read schedule iterate: Read the schedule iterate A and compute the gradients required for update in Algorithm 1. 3. Update: Update the iterate x with the computed incremental update in Algorithm 1. 4. Schedule Update: Run a scheduler update for updating A.</p><p>Each processor repeatedly runs these procedures concurrently, without any synchronization. Hence, x may change in between Step 1 and Step 3. Similarly, A may change in between Steps 2 and 4. In fact, the states of iterates x and A can correspond to different time-stamps. We maintain a global counter t to track the number of updates successfully executed. We use D(t) 2 [t] and D 0 (t) 2 [t] to denote the particular x-iterate and A-iterate used for evaluating the update at the t th iteration. We assume that the delay in between the time of evaluation and updating is bounded by a non-negative integer ⌧ , i.e., t D(t)  ⌧ and t D 0 (t)  ⌧ . The bound on the staleness captures the degree of parallelism in the method: such parameters are typical in asynchronous systems (see e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14]</ref>). Furthermore, we also assume that the system is synchronized after every epoch i.e., D(t) km for t km. We would like to emphasize that the assumption is not strong since such a synchronization needs to be done only once per epoch.</p><p>For the purpose of our analysis, we assume a consistent read model. In particular, our analysis assumes that the vector x used for evaluation of gradients is a valid iterate that existed at some point in time. Such an assumption typically amounts to using locks in practice. This problem can be avoided by using random coordinate updates as in <ref type="bibr" target="#b20">[21]</ref> (see Section 4 of <ref type="bibr" target="#b20">[21]</ref>) but such a procedure is computationally wasteful in practice. We leave the analysis of inconsistent read model as future work. Nonetheless, we report results for both locked and lock-free implementations (see Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Convergence Analysis</head><p>The key ingredients to the success of asynchronous algorithms for multicore stochastic gradient descent are sparsity and "disjointness" of the data matrix <ref type="bibr" target="#b20">[21]</ref>. More formally, suppose f i only depends on x ei where e i ✓ [d] i.e., f i acts only on the components of x indexed by the set e i . Let kxk 2 i denote P j2ei kx j k 2 ; then, the convergence depends on , the smallest constant such that</p><formula xml:id="formula_10">E i [kxk 2 i ]  kxk 2 .</formula><p>Intuitively, denotes the average frequency with which a feature appears in the data matrix. We are interested in situations where ⌧ 1. As a warm up, let us first discuss convergence analysis for asynchronous SVRG. The general case is similar, but much more involved. Hence, it is instructive to first go through the analysis of asynchronous SVRG. Theorem 2. Suppose step size ⌘, epoch size m are chosen such that the following condition holds:</p><formula xml:id="formula_11">0 &lt; ✓ s := ⇣ 1 ⌘m + 4L ⇣ ⌘+L ⌧ 2 ⌘ 2 1 2L 2 ⌘ 2 ⌧ 2 ⌘⌘ ⇣ 1 4L ⇣ ⌘+L ⌧ 2 ⌘ 2 1 2L 2 ⌘ 2 ⌧ 2 ⌘⌘ &lt; 1.</formula><p>Then, for the iterates of an asynchronous variant of Algorithm 1 with SVRG schedule and probabilities</p><formula xml:id="formula_12">p i = 1/m for all i 2 [m], we have E[f (x k+1 ) f (x ⇤ )]  ✓ s E[f (x k ) f (x ⇤ )].</formula><p>The bound obtained in Theorem 2 is useful when is small. To see this, as earlier, consider the indicative case where L/ = n. The synchronous version of SVRG obtains a convergence rate of ✓ = 0.5 for step size ⌘ = 0.1/L and epoch size m = O(n). For the asynchronous variant of SVRG, by setting ⌘ = 0.1/2(max{1, 1/2 ⌧ }L), we obtain a similar rate with m = O(n</p><formula xml:id="formula_13">+ 1/2 ⌧ n).</formula><p>To obtain this, set ⌘ = ⇢/L where ⇢ = 0.1/2(max{1, 1/2 ⌧ }) and ✓ s = 0.5. Then, a simple calculation gives the following:</p><formula xml:id="formula_14">m n = 2 ⇢ ✓ 1 2 ⌧ 2 ⇢ 2 1 12⇢ 14 ⌧ 2 ⇢ 2 ◆  c 0 max{1, 1/2 ⌧ },</formula><p>where c 0 is some constant. This follows from the fact that ⇢ = 0.1/2(max{1, 1/2 ⌧ }). Suppose ⌧ &lt; 1/ 1/2 . Then we can achieve nearly the same guarantees as the synchronous version, but ⌧ times faster since we are running the algorithm asynchronously. For example, consider the sparse setting where = o(1/n); then it is possible to get near linear speedup when</p><formula xml:id="formula_15">⌧ = o(n 1/2</formula><p>). On the other hand, when 1/2 ⌧ &gt; 1, we can obtain a theoretical speedup of 1/ 1/2 .</p><p>We finally provide the convergence result for the asynchronous algorithm in the general case. The proof is complicated by the fact that set A, unlike in SVRG, changes during the epoch. The key idea is that only a single element of A changes at each iteration. Furthermore, it can only change to one of the iterates in the epoch. This control provides a handle on the error obtained due to the staleness. Due to space constraints, the proof is relegated to the appendix. Theorem 3. For any positive parameters c, ,  &gt; 1, step size ⌘ and epoch size m, we define the following quantities:</p><formula xml:id="formula_16">⇣ = c⌘ 2 + ✓ 1 1  ◆ ⌧ cL ⌧ 2 ⌘ 3 ! , a =   1 ✓ 1 1  ◆ m " 2c⌘ 8⇣L(1 + ) 2c  96⇣L⌧ n ✓ 1 1  ◆ ⌧ 1 n # , ✓ a = max 8 &lt; : 2 4 2c a ✓ 1 1  ◆ m + 8⇣L ⇣ 1 + 1 ⌘ a   1 ✓ 1 1  ◆ m 3 5 , ✓<label>1 1  ◆ m 9 =</label></formula><p>; .</p><p>Suppose probabilities p i / (1 1  ) m i , parameters , , step-size ⌘, and epoch size m are chosen such that the following conditions are satisfied:</p><formula xml:id="formula_17">1  + 8⇣L ✓ 1 + 1 ◆ + 96⇣L⌧ n ✓ 1 1  ◆ ⌧  1 n , ⌘ 2  ✓ 1 1  ◆ m 1 1 12L 2 ⌧ 2 , a &gt; 0, ✓ a &lt; 1.</formula><p>Then, for the iterates of asynchronous variant of Algorithm 1 with HSAG schedule we have</p><formula xml:id="formula_18">E  f (x k+1 ) f (x ⇤ ) + 1 a Gk+1  ✓ a E  f (x k ) f (x ⇤ ) + 1 a Gk .</formula><p>Corollary 2. Note that Gk 0 and therefore, under the conditions specified in Theorem 3 and with By using step size normalized by 1/2 ⌧ (similar to Theorem 2) and parameters similar to the ones specified after Theorem 1 we can show speedups similar to the ones obtained in Theorem 2. Please refer to the appendix for more details on the parameters in Theorem 3.</p><formula xml:id="formula_19">✓a = ✓ a (1 + 1/ a ) &lt; 1, we have E ⇥ f (x k ) f (x ⇤ ) ⇤  ✓k a ⇥ f (x 0 ) f (x ⇤ ) ⇤ .</formula><p>Before ending our discussion on the theoretical analysis, we would like to highlight an important point. Our emphasis throughout the paper was on generality. While the results are presented here in full generality, one can obtain stronger results in specific cases. For example, in the case of SAGA, one can obtain per iteration convergence guarantees (see <ref type="bibr" target="#b5">[6]</ref>) rather than those corresponding to per epoch presented in the paper. Also, SAGA can be analyzed without any additional synchronization per epoch. However, there is no qualitative difference in these guarantees accumulated over the epoch. Furthermore, in this case, our analysis for both synchronous and asynchronous cases can be easily modified to obtain convergence properties similar to those in <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We present our empirical results in this section. For our experiments, we study the problem of binary classification via l 2 -regularized logistic regression. More formally, we are interested in the following optimization problem:</p><formula xml:id="formula_20">min x 1 n n X i=1 log(1 + exp( y i z &gt; i x)) + kxk 2 ,<label>(4.1)</label></formula><p>where z i 2 R d and y i is the corresponding label for each i 2 [n]. In all our experiments, we set = 1/n. Note that such a choice leads to high condition number. A careful implementation of SVRG is required for sparse gradients since the implementation as stated in Algorithm 1 will lead to dense updates at each iteration. For an efficient implementation, a scheme like the 'just-in-time' update scheme, as suggested in <ref type="bibr" target="#b24">[25]</ref>, is required. Due to lack of space, we provide the implementation details in the appendix.</p><p>We evaluate the following algorithms for our experiments:</p><p>• Lock-Free SVRG: This is the lock-free asynchronous variant of Algorithm 1 using SVRG schedule; all threads can read and update the parameters with any synchronization. Parameter updates are performed through atomic compare-and-swap instruction <ref type="bibr" target="#b20">[21]</ref>. A constant step size that gives the best convergence is chosen for the dataset.</p><p>• Locked SVRG: This is the locked version of the asynchronous variant of Algorithm 1 using SVRG schedule. In particular, we use a concurrent read exclusive write locking model, where all threads can read the parameters but only one threads can update the parameters at a given time. The step size is chosen similar to Lock-Free SVRG.</p><p>• Lock-Free SGD: This is the lock-free asynchronous variant of the SGD algorithm (see <ref type="bibr" target="#b20">[21]</ref>). We compare two different versions of this algorithm: (i) SGD with constant step size (referred to as CSGD). (ii) SGD with decaying step size ⌘ 0 p 0 /(t + 0 ) (referred to as DSGD), where constants ⌘ 0 and 0 specify the scale and speed of decay. For each of these versions, step size is tuned for each dataset to give the best convergence progress. ) versus time plot of Lock-Free SVRG, DSGD and CSGD on rcv1 (left), real-sim (left center), news20 (right center) and url (right) datasets. The experiments are parallelized over 10 cores.</p><p>All the algorithms were implemented in C++ 2 . We run our experiments on datasets from LIBSVM website 3 . Similar to <ref type="bibr" target="#b28">[29]</ref>, we normalize each example in the dataset so that kz i k 2 = 1 for all i 2 [n]. Such a normalization leads to an upper bound of 0.25 on the Lipschitz constant of the gradient of f i . The epoch size m is chosen as 2n (as recommended in <ref type="bibr" target="#b9">[10]</ref>) in all our experiments. In the first experiment, we compare the speedup achieved by our asynchronous algorithm. To this end, for each dataset we first measure the time required for the algorithm to each an accuracy of 10 10 (i.e., f (x) f (x ⇤ ) &lt; 10 10 ). The speedup with P threads is defined as the ratio of the runtime with a single thread to the runtime with P threads. Results in Figure <ref type="figure" target="#fig_3">3</ref> show the speedup on various datasets. As seen in the figure, we achieve significant speedups for all the datasets. Not surprisingly, the speedup achieved by Lock-free SVRG is much higher than ones obtained by locking. Furthermore, the lowest speedup is achieved for rcv1 dataset. Similar speedup behavior was reported for this dataset in <ref type="bibr" target="#b20">[21]</ref>. It should be noted that this dataset is not sparse and hence, is a bad case for the algorithm (similar to <ref type="bibr" target="#b20">[21]</ref>).</p><p>For the second set of experiments we compare the performance of Lock-Free SVRG with stochastic gradient descent. In particular, we compare with the variants of stochastic gradient descent, DSGD and CSGD, described earlier in this section. It is well established that the performance of variance reduced stochastic methods is better than that of SGD. We would like to empirically verify that such benefits carry over to the asynchronous variants of these algorithms. Figure <ref type="figure" target="#fig_4">4</ref> shows the performance of Lock-Free SVRG, DSGD and CSGD. Since the computation complexity of each epoch of these algorithms is different, we directly plot the objective value versus the runtime for each of these algorithms. We use 10 cores for comparing the algorithms in this experiment. As seen in the figure, Lock-Free SVRG outperforms both DSGD and CSGD. The performance gains are qualitatively similar to those reported in <ref type="bibr" target="#b9">[10]</ref> for the synchronous versions of these algorithms. It can also be seen that the DSGD, not surprisingly, outperforms CSGD in all the cases. In our experiments, we observed that Lock-Free SVRG, in comparison to SGD, is relatively much less sensitive to the step size and more robust to increasing threads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion &amp; Future Work</head><p>In this paper, we presented a unifying framework based on <ref type="bibr" target="#b5">[6]</ref>, that captures many popular variance reduction techniques for stochastic gradient descent. We use this framework to develop a simple hybrid variance reduction method. The primary purpose of the framework, however, was to provide a common platform to analyze various variance reduction techniques. To this end, we provided convergence analysis for the framework under certain conditions. More importantly, we propose an asynchronous algorithm for the framework with provable convergence guarantees. The key consequence of our approach is that we obtain asynchronous variants of several algorithms like SVRG, SAGA and S2GD. Our asynchronous algorithms exploits sparsity in the data to obtain near linear speedup in settings that are typically encountered in machine learning.</p><p>For future work, it would be interesting to perform an empirical comparison of various schedules. In particular, it would be worth exploring the space-time-accuracy tradeoffs of these schedules. We would also like to analyze the effect of these tradeoffs on the asynchronous variants.</p><p>Acknowledgments. SS was partially supported by NSF IIS-1409802. 2 All experiments were conducted on a Google Compute Engine n1-highcpu-32 machine with 32 processors and 28.8 GB RAM. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure1shows the schedules for the aforementioned algorithms.In case of SVRG, SCHEDULEUPDATE is triggered every m iterations (here m denotes precisely the number of inner iterations used in<ref type="bibr" target="#b9">[10]</ref>); so A t remains unchanged for the m iterations and all ↵ t i are updated to the current iterate at the m th iteration. For SAGA, unlike SVRG, A t changes at the t th iteration for all t 2 [T ]. This change is only to a single element of A t , and is determined by the index i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: SCHEDULEUPDATE function for SVRG (top left), SAGA (top right), SAG (bottom left) and GRADIENTDESCENT (bottom right). While SVRG is epoch-based, rest of algorithms perform updates at each iteration. Here a|b denotes that a divides b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>SCHEDULEUPDATE for HSAG. This algorithm assumes access to some index set S and the schedule frequency vector s. Recall that a|b denotes a divides b be combined to obtain a hybrid algorithm. Consider some S ✓ [n], the indices that follow SAGA schedule. We assume that the rest of the indices follow an SVRG-like schedule with schedule frequency s i for all i 2 S , [n] \ S. Figure2shows the corresponding update schedule of HSAG. If S = [n] then HSAG is equivalent to SAGA, while at the other extreme, for S = ; and s i = m for all i 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: l 2 -regularized logistic regression. Speedup curves for Lock-Free SVRG and Locked SVRG on rcv1 (left), real-sim (left center), news20 (right center) and url (right) datasets. We report the speedup achieved by increasing the number of threads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: l 2 -regularized logistic regression. Training loss residual f (x) f (x ⇤) versus time plot of Lock-Free SVRG, DSGD and CSGD on rcv1 (left), real-sim (left center), news20 (right center) and url (right) datasets. The experiments are parallelized over 10 cores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>3</head><label></label><figDesc>http://www.csie.ntu.edu.tw/ ˜cjlin/libsvmtools/datasets/binary.html</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>{1, . . . , n}, step size ⌘ &gt; 0</figDesc><table><row><cell>Randomly pick a I for t = 0 to T do Update iterate as x t+1 T = {i 0 , . . . , i x t ⌘ rf T } where i it (x t t 2 {1, . . . , n} 8 t 2 {0, . . . , T } ; ) rf it (↵ t it ) + 1 n P i (↵ t i ) ; i rf A t+1 i=0 , A t , t, I T ) ; = SCHEDULEUPDATE({x i } t+1 end</cell></row><row><cell>return x T</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Though we should note that SGD also applies to the harder stochastic optimization problem min F (x) = E[f (x; ⇠)], which need not be a finite-sum.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A lower bound for the optimization of finite sums</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0723</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributed delayed stochastic optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="873" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Parallel and Distributed Computation: Numerical Methods</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tsitsiklis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Incremental gradient, subgradient, and proximal methods for convex optimization: A survey</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization for Machine Learning</title>
		<imprint>
			<date type="published" when="2010">2010. 2011</date>
			<biblScope unit="page" from="1" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">New Optimization Methods for Machine Learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Defazio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>Australian National University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives</title>
		<author>
			<persName><forename type="first">A</forename><surname>Defazio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 27</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Defazio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Domke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.2710</idno>
		<title level="m">Finito: A faster, permutable incremental gradient method for big data problems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimal distributed online prediction using minibatches</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gilad-Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="165" to="202" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A globally convergent incremental Newton method</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gürbüzbalaban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ozdaglar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Parrilo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="283" to="313" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Accelerating stochastic gradient descent using predictive variance reduction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 26</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mini-Batch Semi-Stochastic Gradient Descent in the Proximal Setting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Konečný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takáč</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.04407</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Konečný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.1666</idno>
		<title level="m">Semi-Stochastic Gradient Descent Methods</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Communication Efficient Distributed Machine Learning with the Parameter Server</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 27</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An asynchronous parallel stochastic coordinate descent algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bittorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2014</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Asynchronous stochastic coordinate descent: Parallelism and convergence properties</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="351" to="376" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Optimization with first-order surrogate functions</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1305.3120</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed asynchronous incremental subgradient methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nedić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Borkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in Computational Mathematics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="381" to="407" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust stochastic approximation approach to stochastic programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nemirovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Juditsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1574" to="1609" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficiency of coordinate descent methods on huge-scale optimization problems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="341" to="362" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stochastic Proximal Gradient Descent with Acceleration Techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nitanda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 27</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1574" to="1582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent</title>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 24</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="693" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Large-scale randomized-coordinate descent methods with non-separable linear constraints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hefny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<idno>UAI 31</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function</title>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takáč</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A stochastic approximation method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="400" to="407" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Minimizing Finite Sums with the Stochastic Average Gradient</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.2388</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Accelerated mini-batch stochastic dual coordinate ascent</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 26</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="378" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stochastic dual coordinate ascent methods for regularized loss</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="567" to="599" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On distributed stochastic optimization and learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Allerton Conference on Communication, Control, and Computing</title>
		<meeting>the 52nd Annual Allerton Conference on Communication, Control, and Computing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A proximal stochastic gradient method with progressive variance reduction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2057" to="2075" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Parallelized stochastic gradient descent</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2595" to="2603" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
