<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Is Robustness the Cost of Accuracy? -A Comprehensive Study on the Robustness of 18 Deep Image Classification Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dong</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
							<email>ecezhang@ucdavis.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Davis</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongge</forename><surname>Chen</surname></persName>
							<email>chenhg@mit.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Massachusetts Institute of Technology 4 JD AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
							<email>yijinfeng@jd.com</email>
						</author>
						<author>
							<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
							<email>pin-yu.chen@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yupeng</forename><surname>Gao</surname></persName>
							<email>yupeng.gao@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Is Robustness the Cost of Accuracy? -A Comprehensive Study on the Robustness of 18 Deep Image Classification Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Neural Networks</term>
					<term>Adversarial Attacks</term>
					<term>Robustness</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The prediction accuracy has been the long-lasting and sole standard for comparing the performance of different image classification models, including the ImageNet competition. However, recent studies have highlighted the lack of robustness in well-trained deep neural networks to adversarial examples. Visually imperceptible perturbations to natural images can easily be crafted and mislead the image classifiers towards misclassification. To demystify the trade-offs between robustness and accuracy, in this paper we thoroughly benchmark 18 ImageNet models using multiple robustness metrics, including the distortion, success rate and transferability of adversarial examples between 306 pairs of models. Our extensive experimental results reveal several new insights:</p><p>(1) linear scaling law -the empirical ℓ2 and ℓ∞ distortion metrics scale linearly with the logarithm of classification error; (2) model architecture is a more critical factor to robustness than model size, and the disclosed accuracy-robustness Pareto frontier can be used as an evaluation criterion for ImageNet model designers; (3) for a similar network architecture, increasing network depth slightly improves robustness in ℓ∞ distortion; (4) there exist models (in VGG family) that exhibit high adversarial transferability, while most adversarial examples crafted from one model can only be transferred within the same family. Experiment code is publicly available at https://github.com/huanzhang12/Adversarial Survey.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image classification is a fundamental problem in computer vision and serves as the foundation of multiple tasks such as object detection, image segmentation, object tracking, action recognition, and autonomous driving. Since the breakthrough achieved by AlexNet <ref type="bibr" target="#b0">[1]</ref> in ImageNet Challenge (ILSVRC) 2012 <ref type="bibr" target="#b1">[2]</ref>, deep neural networks (DNNs) have become the dominant force in this domain. From then on, DNN models with increasing depth and more complex building blocks have been proposed. While these models continue to achieve steadily increasing accuracies, their robustness has not been thoroughly studied, thus little is known if the high accuracies come at the price of reduced robustness.</p><p>A common approach to evaluate the robustness of DNNs is via adversarial attacks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, where imperceptible adversarial examples are crafted to mislead DNNs. Generally speaking, the easier an adversarial example can be generated, the less robust the DNN is. Adversarial examples may lead to significant property damage or loss of life. For example, <ref type="bibr" target="#b11">[12]</ref> has shown that a subtly-modified physical Stop sign can be misidentified by a real-time object recognition system as a Speed Limit sign. In addition to adversarial attacks, neural network robustness can also be estimated in an attack-agnostic manner. For example, <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b13">[14]</ref> theoretically analyzed the robustness of some simple neural networks by estimating their global and local Lipschitz constants, respectively. <ref type="bibr" target="#b14">[15]</ref> proposes to use extreme value theory to estimate a lower bound of the minimum adversarial distortion, and can be efficiently applied to any neural network classifier. <ref type="bibr" target="#b15">[16]</ref> proposes a robustness lower bound based on linear approximations of ReLU activations. In this work, we evaluate DNN robustness by using specific attacks as well as attack-agnostic approaches. We also note that the adversarial robustness studied in this paper is different from <ref type="bibr" target="#b16">[17]</ref>, where "robustness" is studied in the context of label semantics and accuracy.</p><p>Since the last ImageNet challenge has ended in 2017, we are now at the beginning of post-ImageNet era. In this work, we revisit 18 DNN models submitted to the ImageNet Challenge or achieved state-of-the-art performance. These models have different sizes, classification performance, and belong to multiple architecture families such as AlexNet <ref type="bibr" target="#b0">[1]</ref>, VGG Nets <ref type="bibr" target="#b17">[18]</ref>, Inception Nets <ref type="bibr">[19]</ref>, ResNets <ref type="bibr" target="#b19">[20]</ref>, DenseNets <ref type="bibr" target="#b20">[21]</ref>, MobileNets <ref type="bibr" target="#b21">[22]</ref>, and NASNets <ref type="bibr" target="#b22">[23]</ref>. Therefore, they are suitable to analyze how different factors influence the model robustness. Specifically, we aim to examine the following questions in this study:</p><p>1. Has robustness been sacrificed for the increased classification performance? 2. Which factors influence the robustness of DNNs?</p><p>In the course of evaluation, we have gained a number of insights and we summarize our contributions as follows:</p><p>-Tested on a large number of well-trained deep image classifiers, we find that robustness is scarified when solely pursuing a higher classification performance. Indeed, Figure <ref type="figure">2</ref>(a) and Figure <ref type="figure">2</ref>(b) clearly show that the ℓ 2 and ℓ ∞ adversarial distortions scale almost linearly with the logarithm of model classification errors. Therefore, the classifiers with very low test errors are highly vulnerable to adversarial attacks. We advocate that ImageNet network designers should evaluate model robustness via our disclosed accuracy-robustness Pareto frontier.</p><p>-The networks of a same family, e.g., VGG, Inception Nets, ResNets, and DenseNets, share similar robustness properties. This suggests that network architecture has a larger impact on robustness than model size. Besides, we also observe that the ℓ ∞ robustness slightly improves when ResNets, Inception Nets, and DenseNets become deeper.</p><p>-The adversarial examples generated by the VGG family can transfer very well to all the other 17 models, while most adversarial examples of other models can only transfer within the same model family. Interestingly, this finding provides us an opportunity to reverse-engineer the architecture of black-box models.</p><p>-We present the first comprehensive study that compares the robustness of 18 popular and state-of-the-art ImageNet models, offering a complete picture of the accuracy v.s. robustness trade-off. In terms of transferability of adversarial examples, we conduct thorough experiments on each pair of the 18 ImageNet networks (306 pairs in total), which is the largest scale to date.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Experimental Setup</head><p>In this section, we introduce the background knowledge and how we set up experiments. We study both untargeted attack and targeted attack in this paper. Let x 0 denote the original image and x denote the adversarial image of x 0 . The DNN model F (•) outputs a class label (or a probability distribution of class labels) as the prediction. Without loss of generality, we assume that F (x 0 ) = y 0 , which is the ground truth label of x 0 , to avoid trivial solution. For untargeted attack, the adversarial image x is crafted in a way that x is close to x 0 but F (x) = y 0 . For targeted attack, a target class t (t = y 0 ) is provided and the adversarial image x should satisfy that (i) x is close to x 0 , and (ii) F (x) = t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Deep Neural Network Architectures</head><p>In this work, we study the robustness of 18 deep image classification models belonging to 7 architecture families, as summarized below. Their basic properties of these models are given in Table <ref type="table" target="#tab_0">1</ref>.</p><p>-AlexNet AlexNet <ref type="bibr" target="#b0">[1]</ref> is one of the pioneering and most well-known deep convolutional neural networks. Compared to many recent architectures, AlexNet has a relatively simple layout that is composed of 5 convolutional layers followed by two fully connected layers and a softmax output layer.</p><p>-VGG Nets The overall architecture of VGG nets <ref type="bibr" target="#b17">[18]</ref> are similar to AlexNet, but they are much deeper with more convolutional layers. Another main difference between VGG nets and AlexNet is that all the convolutional layers of VGG nets use a small (3×3) kernel while the first two layers of AlexNet use 11×11 and 5×5 kernels, respectively. In our paper, we study VGG networks with 16 and 19 layers, with 138 million and 144 million parameters, respectively.</p><p>-Inception Nets The family of Inception nets utilizes the inception modules <ref type="bibr" target="#b23">[24]</ref> that act as multi-level feature extractors. Specifically, each inception module consists of multiple branches of 1 × 1, 3 × 3, and 5 × 5 filters, whose outputs will stack along the channel dimension and be fed into the next layer in the network. In this paper, we study the performance of all popular networks in this family, including Inception-v1 (GoogLeNet) [19], Inception-v2 <ref type="bibr" target="#b24">[25]</ref>, Inception-v3 <ref type="bibr" target="#b25">[26]</ref>, Inception-v4, and Inception-ResNet <ref type="bibr" target="#b26">[27]</ref>. All these models are much deeper than AlexNet/VGG but have significantly fewer parameters.</p><p>-ResNets To solve the vanishing gradient problem for training very deep neural networks, the authors of <ref type="bibr" target="#b19">[20]</ref> proposes ResNets, where each layer learns the residual functions with reference to the input by adding skip-layer paths, or "identity shortcut connections". This architecture enables practitioners to train very deep neural networks to outperform shallow models. In our study, we evaluate 3 ResNets with different depths.</p><p>-DenseNets To further exploit the "identity shortcut connections" techniques from ResNets, <ref type="bibr" target="#b20">[21]</ref> proposes DenseNets that connect all layers with each other within a dense block. Besides tackling gradient vanishing problem, the authors also claimed other advantages such as encouraging feature reuse and reducing the number of parameters in the model. We study 3 DenseNets with different depths and widths.</p><p>-MobileNets MobileNets <ref type="bibr" target="#b21">[22]</ref> are a family of light weight and efficient neural networks designed for mobile and embedded systems with restricted computational resources. The core components of MobileNets are depthwise separable filters with factorized convolutions. Separable filters can factorize a standard convolution into two parts, a depthwise convolution and a 1 × 1 pointwise convolution, which can reduce computation and model size dramatically. In this study, we include 3 MobileNets with different depths and width multipliers.</p><p>-NASNets NASNets <ref type="bibr" target="#b22">[23]</ref> are a family of networks automatically generated by reinforcement learning using a policy gradient algorithm to optimize architectures <ref type="bibr" target="#b27">[28]</ref>. Building blocks of the model are first searched on a smaller dataset and then transfered to a larger dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Robustness Evaluation Approaches</head><p>We use both adversarial attacks and attack-agnostic approaches to evaluate network robustness. We first generate adversarial examples of each network using multiple state-of-the-art attack algorithms, and then analyze the attack success rates and the distortions of adversarial images. In this experiment, we assume to have full access to the targeted DNNs, known as the white-box attack. To further study the transferability of the adversarial images generated by each network, we consider all the 306 network pairs and for each pair, we conduct transfer attack that uses one model's adversarial examples to attack the other model. Since transfer attack is widely used in the black-box setting <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>, where an adversary has no access to the explicit knowledge of the target models, this experiment can provide some evidence on networks' black-box robustness. Finally, we compute CLEVER <ref type="bibr" target="#b14">[15]</ref> score, a state-of-the-art attack-agnostic network robustness metric, to estimate each network's intrinsic robustness. Below, we briefly introduce all the evaluation approaches used in our study.</p><p>We evaluate the robustness of DNNs using the following adversarial attacks: -Fast Gradient Sign Method (FGSM) FGSM <ref type="bibr" target="#b2">[3]</ref> is one of the pioneering and most efficient attacking algorithms. It only needs to compute the gradient once to generate an adversarial example x: -Iterative FGSM (I-FGSM) Albeit efficient, FGSM suffers from a relatively low attack success rate. To this end, <ref type="bibr" target="#b36">[37]</ref> proposes iterative FGSM to enhance its performance. It applies FGSM multiple times with a finer distortion, and is able to fool the network in more than 99% cases. When we run I-FGSM for T iterations, we set the per-iteration perturbation to ǫ T sgn(∇J(x 0 , t)). I-FGSM can be viewed as a projected gradient descent (PGD) method inside an ℓ ∞ ball <ref type="bibr" target="#b37">[38]</ref>, and it usually finds adversarial examples with small ℓ ∞ distortions.</p><formula xml:id="formula_0">x ← clip[x 0 − ǫ sgn(∇J(x 0 , t))],</formula><p>-C&amp;W attack <ref type="bibr" target="#b38">[39]</ref> formulates the problem of generating adversarial examples x as the following optimization problem min</p><formula xml:id="formula_1">x λf (x, t) + x − x 0 2 2 s.t. x ∈ [0, 1] p ,</formula><p>where f (x, t) is a loss function to measure the distance between the prediction of x and the target label t. In this work, we choose</p><formula xml:id="formula_2">f (x, t) = max{max i =t [(Logit(x)) i − (Logit(x)) t ], −κ}</formula><p>as it was shown to be effective by <ref type="bibr" target="#b38">[39]</ref>. Logit(x) denotes the vector representation of x at the logit layer, κ is a confidence level and a larger κ generally improves transferability of adversarial examples.</p><p>C&amp;W attack is by far one of the strongest attacks that finds adversarial examples with small ℓ 2 perturbations. It can achieve almost 100% attack success rate and has bypassed 10 different adversary detection methods <ref type="bibr" target="#b39">[40]</ref>.</p><p>-EAD-L1 attack EAD-L1 attack <ref type="bibr" target="#b40">[41]</ref> refers to the Elastic-Net Attacks to DNNs, which is a more general formulation than C&amp;W attack. It proposes to use elastic-net regularization, a linear combination of ℓ 1 and ℓ 2 norms, to penalize large distortion between the original and adversarial examples. Specifically, it learns the adversarial example x via min</p><formula xml:id="formula_3">x λf (x, t) + x − x 0 2 2 + β x − x 0 1 s.t. x ∈ [0, 1] p ,</formula><p>where f (x, t) is the same as used in the C&amp;W attack. <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref> show that EAD-L1 attack is highly transferable and can bypass many defenses and analysis.</p><p>We also evaluate network robustness using an attack-agnostic approach: -CLEVER CLEVER <ref type="bibr" target="#b14">[15]</ref> (Cross-Lipschitz Extreme Value for nEtwork Robustness) uses extreme value theory to estimate a lower bound of the minimum adversarial distortion. Given an image x 0 , CLEVER provides an estimated lower bound on the ℓ p norm of the minimum distortion δ required to misclassify the distorted image x 0 + δ. A higher CLEVER score suggests that the network is likely to be more robust to adversarial examples. CLEVER is attack-agnostic and reflects the intrinsic robustness of a network, rather than the robustness under a certain attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dataset</head><p>In this work, we use the ImageNet <ref type="bibr" target="#b44">[45]</ref> as the benchmark dataset, due to the following reasons: (i) ImageNet dataset can take full advantage of the studied DNN models since all of them were designed for ImageNet challenges; (ii) comparing to the widely-used small-scale datasets such as MNIST, CIFAR-10 <ref type="bibr" target="#b45">[46]</ref>, and GTSRB <ref type="bibr" target="#b46">[47]</ref>, ImageNet has significantly more images and classes and is more challenging; and (iii) it has been shown by <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b47">48]</ref> that ImageNet images are easier to attack but harder to defend than the images from MNIST and CI-FAR datasets. Given all these observations, ImageNet is an ideal candidate to study the robustness of state-of-the-art deep image classification models.</p><p>A set of randomly selected 1,000 images from the ImageNet validation set is used to generate adversarial examples from each model. For each image, we conduct targeted attacks with a random target and a least likely target as well as an untargeted attack. Misclassified images are excluded. We follow the setting in <ref type="bibr" target="#b14">[15]</ref> to compute CLEVER scores for 100 out of the all 1,000 images, as CLEVER is relatively more computational expensive. Additionally, we conducted another experiment by taking the subset of images (327 images in total) that are correctly classified by all of 18 examined ImageNet models. The results are consistent with our main results and are given in supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Evaluation Metrics</head><p>In our study, the robustness of the DNN models is evaluated using the following four metrics: -Attack success rate For non-targeted attack, success rate indicates the percentage of the adversarial examples whose predicted labels are different from their ground truth labels. For targeted attack, success rate indicates the percentage of the adversarial examples that are classified as the target class. For both attacks, a higher success rate suggests that the model is easier to attack and hence less robust. When generating adversarial examples, we only consider original images that are correctly classified to avoid trial attacks.</p><p>-Distortion We measure the distortion between adversarial images and the original ones using ℓ 2 and ℓ ∞ norms. ℓ 2 norm measures the Euclidean distance between two images, and ℓ ∞ norm is a measure of the maximum absolute change to any pixel (worst case). Both of them are widely used to measure adversarial perturbations <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref>. A higher distortion usually suggests a more robust model. To find adversarial examples with minimum distortion for each model, we use a binary search strategy to select the optimal attack parameters ǫ in I-FGSM and λ in C&amp;W attack. Because each model may have different input sizes, we divide ℓ 2 distortions by the number of total pixels for a fair comparison.</p><p>-CLEVER score For each image, we compute its ℓ 2 CLEVER score for target attacks with a random target class and a least-likely class, respectively. The reported number is the averaged score of all the tested images. The higher the CLEVER score, the more robust the model is.</p><p>-Transferability We follow <ref type="bibr" target="#b30">[31]</ref> to define targeted and non-targeted transferability. For non-targeted attack, transferability is defined as the percentage of the adversarial examples generated for one model (source model ) that are also misclassified by another model (target model ). We refer to this percentage as error rate, and a higher error rate means better non-targeted transferability. For targeted attack, transferability is defined as matching rate, i.e., the percentage of the adversarial examples generated for source model that are misclassified as the target label (or within top-k labels) by the target model. A higher matching rate indicates better targeted transferability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>After examining all the 18 DNN models, we have learned insights about the relationships between model architectures and robustness, as discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Evaluation of Adversarial Attacks</head><p>We have carefully conducted a controlled experiment by pulling images from a common set of 1000 test images when evaluating the robustness of different models. For assessing the robustness of each model, the originally misclassified images are excluded. We compare the success rates of targeted attack with a random target of FGSM, I-FGSM, C&amp;W and EAD-L1 with different parameters for all 18 models. The success rate of FGSM targeted attack is low so we also show its untargeted attack success rate in Figure <ref type="figure">1(b)</ref>.</p><p>For targeted attack, the success rate of FGSM is very low (below 3% for all settings), and unlike in the untargeted setting, increasing ǫ in fact decreases attack success rate. This observation further confirms that FGSM is a weak attack, and targeted attack is more difficult and needs iterative attacking methods. Figure <ref type="figure">1</ref>(c) shows that, with only 10 iterations, I-FGSM can achieve a very good targeted attack success rate on all models. C&amp;W and EAD-L1 can also achieve almost 100% success rate on almost all of the models when κ = 0.</p><p>For C&amp;W and EAD-L1 attacks, increasing the confidence κ can significantly make the attack harder to find a feasible adversarial example. A larger κ usually makes the adversarial distortion more universal and improves transferability (as we will show shortly), but at the expense of decreasing the success rate and increasing the distortion. However, we find that the attack success rate with large κ cannot be used as a robustness measure, as it is not aligned with the ℓ p norm of adversarial distortions. For example, for MobileNet-0.50-160, when κ = 40, the success rate is close to 0, but in Figure <ref type="figure">2</ref> we show that it is one of the most vulnerable networks. The reason is that the range of the logits output can be different for each network, so the difficulty of finding a fixed logit gap κ is different on each network, and is not related to its intrinsic robustness.</p><p>We defer the results for targeted attack with the least likely target label to the Supplementary section because the conclusions made are similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Linear Scaling Law in Robustness v.s. Accuracy</head><p>Here we study the empirical relation between robustness and accuracy of different ImageNet models, where the robustness is evaluated in terms of the ℓ ∞ and ℓ 2 distortion metrics from successful I-FGSM and C&amp;W attacks respectively, or ℓ 2 CLEVER scores. In our experiments the attack success rates of these attacks are nearly 100% for each model. The scatter plots of distortions/scores v.s. top-1 prediction accuracy are displayed in Figure <ref type="figure">2</ref>. We define the classification error as 1 minus top-1 accuracy (denoted as 1 − acc). By regressing the distortion metric with respect to the classification error of networks on the Pareto frontier of robustness-accuracy distribution (i.e., AlexNet, VGG 16, VGG 19, ResNet v2 152, Inception ResNet v2 and NASNet), we find that the distortion scales linearly with the logarithm of classification error. That is, the distortion and classification error has the following relation: distortion = a + b • log (classification-error). The fitted parameters of a and b are given in the captions of Figure <ref type="figure">2</ref>. Take I-FGSM attack as an example, the linear scaling law suggests that to reduce the classification error by a half, the ℓ ∞ distortion of the resulting network will be expected to reduce by approximately 0.02, which is roughly 60% of the AlexNet distortion. Following this trend, if we naively pursue a model with low test error, the model robustness may suffer. Thus, when designing new networks for ImageNet, we suggest to evaluate the model's accuracy-robustness tradeoff by comparing it to the disclosed Pareto frontier. Fig. <ref type="figure">2</ref>. Robustness vs. classification accuracy plots of I-FGSM attack <ref type="bibr" target="#b36">[37]</ref>, C&amp;W attack <ref type="bibr" target="#b38">[39]</ref> and CLEVER <ref type="bibr" target="#b14">[15]</ref> score on random targets over 18 models. Source Model 1.00 1.00 0.99 0.99 0.95 1.00 0.99 0.97 0.96 1.00 1.00 1.00 0.97 0.97 0.95 0.99 1.00 1.00 0.99 1.00 0.94 0.99 0.84 1.00 0.96 0.91 0.87 1.00 1.00 1.00 0.80 0.90 0.92 0.98 1.00 1.00</p><p>1.00 0.99 0.96 0.98 0.86 1.00 0.97 0.93 0.93 1.00 1.00 0.99 0.85 0.93 0.93 0.99 1.00 1.00 0.99 1.00 0.95 0.99 0.85 0.99 0.96 0.92 0.85 1.00 1.00 1.00 0.82 0.90 0.91 0.99 1.00 1.00 0.99 0.99 0.92 0.98 0.72 0.99 0.96 0.86 0.69 1.00 1.00 1.00 0.60 0.78 0.80 0.89 1.00 1.00 0.99 0.99 0.92 0.96 0.83 1.00 0.95 0.91 0.89 1.00 1.00 0.99 0.81 0.88 0.89 0.98 0.99 1.00 0.99 0.98 0.84 0.93 0.72 0.97 0.96 0.85 0.78 1.00 1.00 0.99 0.69 0.80 0.77 0.95 0.99 0.99 1.00 1.00 0.89 0.96 0.58 1.00 0.94 0.86 0.67 1.00 1.00 1.00 0.54 0.77 0.73 0.88 1.00 0.99 1.00 0.99 0.91 0.98 0.63 1.00 0.95 0.86 0.74 1.00 1.00 1.00 0.60 0.78 0.75 0.89 1.00 0.99 1.00 1.00 0.94 1.00 0.88 1.00 0.98 0.94 0.91 1.00 1.00 1.00 0.82 0.90 0.91 0.98 1.00 1.00 0.99 0.98 0.86 0.96 0.79 1.00 0.96 0.92 0.83 1.00 1.00 0.99 0.72 0.85 0.87 0.94 0.99 1.00 0.99 0.98 0.91 0.96 0.81 1.00 0.93 0.89 0.85 1.00 1.00 1.00 0.78 0.90 0.90 0.96 0.99 1.00</p><p>1.00 0.99 0.92 0.97 0.71 0.99 0.98 0.85 0.77 1.00 1.00 1.00 0.57 0.82 0.85 0.91 1.00 1.00</p><p>1.00 1.00 0.95 0.99 0.79 0.99 0.96 0.89 0.75 1.00 1.00 1.00 0.75 0.96 0.91 0.97 1.00 1.00</p><p>1.00 1.00 0.96 0.99 0.75 0.99 0.96 0.87 0.76 1.00 1.00 1.00 0.72 0.92 0.95 0.96 1.00 1.00</p><p>1.00 0.99 0.95 0.99 0.79 0.99 0.96 0.90 0.76 1.00 1.00 1.00 0.72 0.92 0.91 0.99 1.00 1.00</p><p>1.00 1.00 0.98 0.99 0.97 1.00 0.99 0.96 0.97 1.00 1.00 1.00 0.98 0.99 0.96 0.99 1.00 1.00</p><p>1.00 1.00 1.00 1.00 0.97 1.00 1.00 0.96 0.97 1.00 1.00 1.00 0.99 0.98 0.96 1.00 1.00 1.00 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Robustness of Different Model Sizes and Architectures</head><p>We find that model architecture is a more important factor to model robustness than the model size. Each family of networks exhibits a similar level of robustness, despite different depths and model sizes. For example, AlexNet has about 60 million parameters but its robustness is the best; on the other hand, Mobilenet-0.50-160 has only 1.5 million parameters but is more vulnerable to adversarial attacks in all metrics.</p><p>We also observe that, within the same family, for DenseNet, ResNet and Inception, models with deeper architecture yields a slight improvement of the robustness in terms of the ℓ ∞ distortion metric. This might provide new insights for designing robust networks and further improve the Pareto frontier. This result also echoes with <ref type="bibr" target="#b48">[49]</ref>, where the authors use a larger model to increase the ℓ ∞ robustness of a CNN based MNIST model. Source Model   0.9 1.0 0.9 1.0 0.9 1.0 1.0 0.9 1.0 0.9 1.0 1.0 0.9 0.9 0.8 0.9 1.0 1.0 0.9 1.0 1.0 1.0 0.9 1.0 1.0 1.0 1.0 0.9 1.0 1.0 1.0 0.9 0.8 0.9 0.8 1.0 1.0 1.0 0.9 1.0 0.9 0.9 1.0 0.9 1.0 1.0 0.9 0.8 0.8 0.9 1.0 1.0 0.8 1.0 1.0 1.0 0.9 1.0 1.0 0.9 1.0 0.9 1.0 1.0 0.9 0.9 0.8 0.9  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Transferability of Adversarial Examples</head><p>Figures 3, 4 and 5 show the transferability heatmaps of FGSM, I-FGSM and EAD-L1 over all 18 models (306 pairs in total). The value in the i-th row and j-th column of each heatmap matrix is the proportion of the adversarial examples successfully transferred to target model j out of all adversarial examples generated by source model i (including both successful and failed attacks on the source model). Specifically, the values on the diagonal of the heatmap are the attack success rate of the corresponding model. For each model, we generate adversarial images using the aforementioned attacks and pass them to the target model to perform black-box untargeted and targeted transfer attacks. To evaluate each model, we use the success rate for evaluating the untargeted transfer attacks and the top-5 matching rate for evaluating targeted transfer attacks. Note that not all models have the same input image dimension. We also find that simply resizing the adversarial examples can significantly decrease the transfer attack success rate <ref type="bibr" target="#b49">[50]</ref>. To alleviate the disruptive effect of image resiz- Source Model 1.00 0.01 0.00 0.01 0.01 0.00 0.01 0.00 0.00 0.04 0.01 0.01 0.01 0.00 0.01 0.01 0.01 0.01 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.01 0.00 0.00 0.01 0.00 0.00 0.00 1.00 0.01 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.01 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.01 0.02 0.01 0.02 1.00 0.01 0.01 0.01 0.01 0.05 0.02 0.02 0.00 0.01 0.01 0.01 0.01 0.02 0.01 0.01 0.00 0.02 0.00 1.00 0.01 0.00 0.01 0.01 0.01 0.01 0.01 0.02 0.00 0.01 0.00 0.00 0.01 0.00 0.01 0.01 0.00 0.01 1.00 0.00 0.00 0.01 0.01 0.01 0.01 0.01 0.00 0.01 0.00 0.00 0.00 0.01 0.01 0.01 0.00 0.01 0.01 0.99 0.00 0.00 0.01 0.01 0.01 0.00 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 1.00 0.01 0.01 0.02 0.00 0.01 0.00 0.01 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 1.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.01 0.01 0.01 0.00 0.00 0.01 0.00 0.00 0.00 0.01 0.01 1.00 0.00 0.01 0.00 0.01 0.00 0.00 0.18 0.16 0.16 0.16 0.13 0.18 0.16 0.13 0.12 1.00 0.21 0.17 0.12 0.15 0.13 0.15 0.17 0.17  0.18 0.20 0.17 0.18 0.14 0.18 0.18 0.17 1.00 0.29 0.24 0.23 0.12 0.14 0.14 0.15 0.18 0.18 0.21 0.16 0.12 0.15 0.12 0.15 0.14 0.14 0.13 1.00 0.28 0.17 0.10 0.13 0.11 0.16 0.17 0.17 0.17 0.18 0.16 0.16 0.12 0.17 0.16 0.17 0.13 0.27 0.25 0.19 1.00 0.13 0.12 0.14 0.14 0.15 0.05 0.08 0.08 0.08 0.05 0.08 0.08 0.06 0.05 0.08 0.10 0.10 0.04 1.00 0.16 0.17 0.05 0.06 0.04 0.08 0.08 0.08 0.05 0.08 0.09 0.06 0.05 0.08 0.09 0.12 0.05 0.16 1.00 0.17 0.08 0.10 0.12 0.11 0.13 0.10 0.12 0.60 0.13 0.06 0.08 0.11 0.12 0.13 0.13 0.13 0.11 0.12 0.03 0.04 0.04 0.03 0.04 0.03 0.04 0.04 0.25 0.02 0.03 0.05 0.04 0.04 0.04 0.03 0.05 0.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02 0.00 0.00 0.00 0.00 0.01 0.01 0.00 0.00 0.00 0.01 0.00 0.00 0.01 0.00 0.00 0.00 0.05 0.03 0.00 0.01 0.01 0.00 0.02 0.02 0.19 0.24 0.24 0.24 0.21 0.25 0.24 0.21 0.21 0.17 ing on adversarial perturbations, when transferring an adversarial image from a network with larger input dimension to a smaller dimension, we crop the image from the center; conversely, we add a white boarder to the image when the source network's input dimension is smaller. Generally, the transferability of untargeted attacks is significantly higher than that of targeted attacks, as indicated in Figure <ref type="figure" target="#fig_1">3</ref>, 4 and 5. We highlighted some interesting findings in our experimental results:</p><p>1. In the untargeted transfer attack setting, FGSM and I-FGSM have much higher transfer success rates than those in EAD-L1 (despiting using a large κ). Similar to the results in <ref type="bibr" target="#b40">[41]</ref>, we find that the transferability of C&amp;W is even worse than that of EAD-L1 and we defer the results to the supplement. The ranking of attacks on transferability in untargeted setting is given by FGSM I-FGSM EAD-L1 C&amp;W.</p><p>2. Again in the untargeted transfer attack setting, for FGSM, a larger ǫ yields better transferability, while for I-FGSM, less iterations yield better transfer-ability. For untargeted EAD-L1 transfer attacks, a higher κ value (confidence parameter) leads to better transferability, but it is still far behind I-FGSM. 3. Transferability of adversarial examples is sometimes asymmetric; for example, in Figure <ref type="figure" target="#fig_4">4</ref>, adversarial examples of VGG 16 are highly transferable to Inception-v2, but adversarial examples of Inception-v2 do not transfer very well to VGG. 4. We find that VGG 16 and VGG 19 models achieve significantly better transferability than other models, in both targeted and untargeted setting, for all attacking methods, leading to the "stripe patterns". This means that adversarial examples generated from VGG models are empirically more transferable to other models. This observation might be explained by the simple convolutional nature of VGG networks, which is the stem of all other networks. VGG models are thus a good starting point for mounting black-box transfer attacks. We also observe that the most transferable model family may vary with different attacks. 5. Most recent networks have some unique features that might restrict adversarial examples' transferability to only within the same family. For example, as shown in Figure <ref type="figure" target="#fig_4">4</ref>, when using I-FGSM in the untargeted transfer attack setting, for DenseNets, ResNets and VGG, transferability between different depths of the same architecture is close to 100%, but their transfer rates to other architectures can be much worse. This provides us an opportunity to reserve-engineer the internal architecture of a black-box model, by feeding it with adversarial examples crafted for a certain architecture and measure the attack success rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this paper, we present the largest scale to date study on adversarial examples in ImageNet models. We show comprehensive experimental results on 18 stateof-the-art ImageNet models using adversarial attack methods focusing on ℓ 1 , ℓ 2 and ℓ ∞ norms and also an attack-agnostic robustness score, CLEVER. Our results show that there is a clear trade-off between accuracy and robustness, and a better performance in testing accuracy in general reduces robustness. Tested on the ImageNet dataset, we discover an empirical linear scaling law between distortion metrics and the logarithm of classification errors in representative models. We conjecture that following this trend, naively pursuing high-accuracy models may come with the great risks of lacking robustness. We also provide a thorough adversarial attack transferability analysis between 306 pairs of these networks and discuss the robustness implications on network architecture. In this work, we focus on image classification. To the best of our knowledge, the scale and profound analysis on 18 ImageNet models have not been studied thoroughly in the previous literature. We believe our findings could also provide insights to robustness and adversarial examples in other computer vision tasks such as object detection <ref type="bibr" target="#b50">[51]</ref> and image captioning <ref type="bibr" target="#b4">[5]</ref>, since these tasks often use the same pre-trained image classifiers studied in this paper for feature extraction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>26 =</head><label>26</label><figDesc>0.55 0.43 0.49 0.41 0.62 0.49 0.45 0.94 0.94 0.77 0.35 0.48 0.48 0.57 0.80 0.78 0.84 0.85 0.50 0.59 0.38 0.51 0.47 0.46 0.42 0.90 0.82 0.59 0.35 0.46 0.44 0.53 0.72 0.71 0.84 0.52 0.66 0.49 0.25 0.42 0.36 0.33 0.29 0.86 0.80 0.54 0.21 0.35 0.32 0.38 0.62 0.61 0.79 0.50 0.42 0.70 0.22 0.40 0.33 0.35 0.26 0.90 0.79 0.47 0.22 0.31 0.32 0.39 0.63 0.62 0.80 0.30 0.18 0.21 0.17 0.29 0.22 0.16 0.08 0.93 0.85 0.54 0.07 0.13 0.11 0.15 0.53 0.52 0.72 0.44 0.23 0.32 0.24 0.36 0.34 0.25 0.91 0.79 0.48 0.19 0.32 0.33 0.40 0.59 0.58 0.65 0.28 0.21 0.24 0.16 0.28 0.51 0.24 0.17 0.86 0.68 0.33 0.14 0.23 0.22 0.30 0.41 0.42 0.78 0.28 0.17 0.21 0.09 0.25 0.20 0.35 0.09 0.91 0.82 0.47 0.07 0.14 0.12 0.14 0.48 0.47 0.81 0.29 0.19 0.22 0.10 0.30 0.23 0.16 0.22 0.90 0.82 0.48 0.08 0.12 0.12 0.13 0.58 0.51 0.80 0.40 0.23 0.31 0.26 0.39 0.36 0.36 0.25 1.00 0.95 0.53 0.17 0.32 0.28 0.41 0.57 0.56 0.80 0.33 0.18 0.32 0.17 0.32 0.32 0.31 0.21 0.89 0.99 0.53 0.14 0.24 0.24 0.35 0.52 0.52 0.77 0.42 0.25 0.34 0.28 0.41 0.36 0.35 0.29 0.90 0.88 0.89 0.23 0.34 0.34 0.42 0.58 0.59 0.84 0.31 0.22 0.29 0.09 0.35 0.26 0.14 0.11 0.90 0.91 0.62 0.12 0.13 0.13 0.15 0.60 0.59 0.81 0.42 0.26 0.33 0.17 0.41 0.34 0.26 0.16 0.89 0.85 0.59 0.15 0.62 0.35 0.41 0.61 0.59 0.81 0.41 0.25 0.33 0.18 0.40 0.31 0.25 0.15 0.89 0.82 0.62 0.15 0.36 0.57 0.37 0.63 0.59 0.83 0.40 0.27 0.35 0.18 0.42 0.35 0.27 0.19 0.92 0.85 0.60 0.15 0.34 0.33 0.63 0.64 0.61 0.90 0.85 0.68 0.76 0.59 0.79 0.70 0.63 0.63 0.94 0.92 0.82 0.56 0.63 0.58 0.71 0.99 0.98 0.88 0.80 0.69 0.77 0.61 0.81 0.68 0.65 0.64 0.96 0.93 0.84 0.58 0.62 0.60 0.72 0.97 1.00 = 0.1, untargeted, success rate 0.23 0.19 0.22 0.20 0.19 0.17 0.20 0.20 0.20 0.08 0.09 0.15 0.19 0.19 0.17 0.16 0.15 0.16 0.05 0.27 0.26 0.23 0.23 0.22 0.22 0.20 0.22 0.02 0.06 0.14 0.24 0.21 0.22 0.18 0.17 0.17 0.05 0.22 0.30 0.23 0.24 0.22 0.24 0.20 0.22 0.03 0.06 0.14 0.24 0.21 0.22 0.18 0.16 0.15 0.05 0.23 0.26 0.29 0.24 0.22 0.24 0.20 0.23 0.02 0.06 0.16 0.26 0.22 0.22 0.19 0.17 0.17 0.04 0.13 0.17 0.16 0.29 0.14 0.16 0.20 0.22 0.01 0.03 0.07 0.24 0.21 0.19 0.18 0.10 0.10 0.06 0.22 0.24 0.24 0.22 0.28 0.24 0.21 0.24 0.03 0.06 0.16 0.24 0.24 0.23 0.20 0.18 0.17 0.07 0.21 0.24 0.23 0.24 0.23 0.26 0.23 0.25 0.04 0.07 0.17 0.25 0.22 0.23 0.20 0.19 0.19 0.04 0.14 0.16 0.16 0.21 0.14 0.16 0.26 0.20 0.02 0.02 0.07 0.20 0.18 0.19 0.16 0.11 0.11 0.03 0.15 0.18 0.18 0.23 0.16 0.17 0.20 0.28 0.01 0.03 0.09 0.24 0.20 0.20 0.18 0.11 0.11 0.08 0.12 0.14 0.13 0.12 0.14 0.13 0.13 0.14 0.04 0.04 0.11 0.12 0.13 0.13 0.13 0.11 0.11 0.10 0.22 0.23 0.22 0.20 0.24 0.21 0.18 0.20 0.08 0.15 0.21 0.21 0.20 0.19 0.20 0.18 0.20 0.08 0.26 0.25 0.26 0.24 0.27 0.25 0.23 0.24 0.04 0.08 0.29 0.24 0.23 0.23 0.21 0.21 0.20 0.03 0.13 0.15 0.15 0.25 0.14 0.15 0.21 0.23 0.02 0.03 0.08 0.30 0.19 0.20 0.17 0.10 0.10 0.05 0.16 0.21 0.19 0.24 0.16 0.20 0.22 0.23 0.02 0.03 0.09 0.25 0.28 0.27 0.23 0.12 0.12 0.05 0.15 0.19 0.18 0.24 0.16 0.17 0.22 0.23 0.01 0.03 0.08 0.22 0.24 0.28 0.21 0.11 0.11 0.03 0.17 0.20 0.20 0.24 0.18 0.20 0.24 0.25 0.02 0.04 0.10 0.24 0.25 0.26 0.28 0.12 0.13 0.06 0.22 0.25 0.25 0.24 0.20 0.23 0.21 0.23 0.03 0.07 0.15 0.25 0.21 0.21 0.17 0.26 0.20 0.07 0.20 0.23 0.24 0.24 0.20 0.23 0.21 0.23 0.03 0.07 0.15 0.24 0.21 0.22 0.16 0.19 0.0.96 0.83 0.92 0.86 0.97 0.92 0.86 0.83 1.00 1.00 0.98 0.83 0.87 0.86 0.94 0.99 0.99 0.99 0.97 0.72 0.86 0.54 0.87 0.72 0.71 0.59 1.00 0.99 0.94 0.53 0.63 0.65 0.79 0.97 0.98 0.99 0.94 0.82 0.88 0.59 0.90 0.80 0.69 0.69 1.00 0.99 0.95 0.56 0.67 0.67 0.81 0.97 0.96 0.99 0.91 0.74 0.92 0.59 0.86 0.75 0.70 0.65 1.00 0.99 0.94 0.54 0.64 0.65 0.79 0.98 0.97 0.99 0.87 0.61 0.76 0.38 0.89 0.73 0.55 0.33 1.00 1.00 0.95 0.25 0.46 0.40 0.50 0.98 0.96 0.98 0.84 0.66 0.76 0.60 0.92 0.76 0.67 0.58 1.00 0.99 0.93 0.49 0.62 0.61 0.78 0.97 0.96 0.98 0.76 0.54 0.59 0.42 0.75 0.78 0.56 0.47 0.99 0.99 0.82 0.37 0.47 0.49 0.65 0.90 0.91 0.99 0.81 0.55 0.63 0.28 0.83 0.66 0.61 0.32 1.00 1.00 0.96 0.23 0.40 0.36 0.49 0.95 0.92 0.99 0.85 0.56 0.70 0.33 0.87 0.70 0.58 0.45 1.00 0.99 0.94 0.27 0.42 0.40 0.48 0.96 0.96 0.99 0.94 0.67 0.82 0.65 0.93 0.87 0.75 0.61 1.00 1.00 0.95 0.52 0.66 0.66 0.82 0.98 0.98 0.99 0.80 0.63 0.76 0.51 0.83 0.76 0.67 0.59 1.00 1.00 0.92 0.45 0.62 0.57 0.73 0.96 0.96 0.98 0.82 0.66 0.74 0.55 0.85 0.73 0.65 0.60 1.00 1.00 0.98 0.49 0.66 0.65 0.78 0.97 0.96 0.99 0.88 0.67 0.77 0.31 0.89 0.80 0.50 0.39 1.00 1.00 0.97 0.31 0.49 0.48 0.56 0.98 0.980.99 0.90 0.71 0.83 0.49 0.90 0.75 0.60 0.46 1.00 0.99 0.97 0.43 0.77 0.67 0.75 0.96 0.97 0.99 0.91 0.69 0.80 0.51 0.91 0.75 0.55 0.44 1.00 0.99 0.97 0.41 0.67 0.77 0.72 0.97 0.97 0.99 0.91 0.72 0.81 0.48 0.93 0.77 0.57 0.47 1.00 1.00 0.96 0.41 0.63 0.64 0.83 0.98 0.96 0.99 1.00 0.92 0.97 0.87 0.99 0.96 0.89 0.90 1.00 1.00 0.98 0.87 0.90 0.86 0.96 1.00 1.00 0.99 1.00 0.92 0.96 0.89 0.98 0.96 0.88 0.89 1.00 0.99 0.99 0.86 0.92 0.87 0.97 0.99 1.00 = 0.2, untargeted, success rate 0.03 0.07 0.13 0.08 0.11 0.06 0.10 0.13 0.13 0.01 0.02 0.03 0.12 0.09 0.09 0.06 0.02 0.03 0.00 0.06 0.11 0.08 0.15 0.05 0.08 0.10 0.13 0.01 0.01 0.03 0.14 0.10 0.11 0.07 0.02 0.02 0.01 0.05 0.11 0.07 0.14 0.05 0.08 0.08 0.12 0.01 0.01 0.02 0.14 0.10 0.10 0.07 0.02 0.02 0.01 0.06 0.10 0.09 0.15 0.05 0.08 0.09 0.12 0.01 0.01 0.02 0.16 0.10 0.11 0.08 0.03 0.03 0.00 0.03 0.07 0.05 0.18 0.03 0.06 0.10 0.14 0.01 0.01 0.01 0.18 0.12 0.12 0.09 0.02 0.02 0.01 0.06 0.11 0.08 0.14 0.07 0.10 0.10 0.14 0.01 0.00 0.03 0.14 0.13 0.12 0.07 0.03 0.03 0.01 0.08 0.14 0.13 0.16 0.08 0.09 0.12 0.14 0.01 0.01 0.05 0.17 0.12 0.14 0.10 0.04 0.04 0.01 0.04 0.07 0.06 0.14 0.03 0.06 0.12 0.13 0.01 0.01 0.02 0.15 0.11 0.11 0.09 0.02 0.02 0.01 0.04 0.07 0.05 0.17 0.04 0.07 0.10 0.18 0.01 0.01 0.01 0.17 0.13 0.13 0.10 0.02 0.02 0.01 0.03 0.09 0.04 0.08 0.04 0.04 0.07 0.09 0.01 0.00 0.01 0.10 0.07 0.08 0.06 0.01 0.02 0.01 0.09 0.14 0.11 0.14 0.10 0.09 0.12 0.13 0.01 0.01 0.06 0.16 0.13 0.13 0.12 0.04 0.05 0.01 0.09 0.15 0.12 0.17 0.08 0.11 0.14 0.16 0.01 0.01 0.06 0.19 0.13 0.12 0.10 0.04 0.05 0.00 0.05 0.07 0.06 0.16 0.03 0.05 0.11 0.14 0.01 0.01 0.01 0.22 0.12 0.12 0.09 0.01 0.01 0.00 0.04 0.08 0.06 0.16 0.03 0.06 0.12 0.15 0.01 0.01 0.02 0.17 0.13 0.14 0.10 0.02 0.01 0.00 0.03 0.09 0.05 0.15 0.03 0.07 0.11 0.14 0.01 0.01 0.02 0.16 0.12 0.13 0.09 0.01 0.01 0.00 0.03 0.09 0.06 0.16 0.04 0.08 0.13 0.16 0.01 0.01 0.01 0.16 0.12 0.14 0.11 0.01 0.01 0.01 0.05 0.11 0.07 0.10 0.04 0.07 0.10 0.12 0.00 0.01 0.02 0.12 0.09 0.09 0.06 0.04 0.04 0.01 0.04 0.10 0.07 0.11 0.04 0.07 0.10 0.11 0.01 0.01 0.02 0.11 0.09 0.09 0.05 0.03 0.05 = 0.2, targeted, top-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Transferability of FGSM attack over 18 ImageNet models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>#Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Transferability of I-FGSM attack over 18 ImageNet models, ǫ = 0.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>0.00 0.03 0.02 0.02 0.02 0.01 0.02 0.03 0.01 0.02 0.02 0.02 1.00 0.01 0.01 0.02 0.01 0.01 0.00 0.01 0.01 0.01 0.00 0.01 0.01 0.01 0.01 0.02 0.02 0.01 0.01 1.00 0.01 0.01 0.01 0.01 0.01 0.02 0.01 0.01 0.00 0.01 0.01 0.01 0.00 0.02 0.01 0.02 0.00 0.02 1.00 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.00 0.01 0.01 0.02 0.01 0.02 0.01 1.00 0.01 0.01 0.02 0.04 0.01 0.03 0.01 0.03 0.02 0.02 0.02 0.03 0.02 0.03 0.02 0.03 0.02 0.03 1.00 0.04 0.02 0.03 0.01 0.03 0.01 0.03 0.01 0.02 0.02 0.03 0.03 0.04 0.01 0.03 0.02 0.03 0.06 1.00 = 0, untargeted, success rate 1.00 0.21 0.20 0.20 0.16 0.21 0.19 0.16 0.16 0.20 0.22 0.21 0.16 0.19 0.18 0.18 0.21 0.22 0.20 1.00 0.30 0.31 0.22 0.28 0.28 0.22 0.23 0.15 0.23 0.27 0.23 0.26 0.24 0.25 0.28 0.28 0.21 0.30 1.00 0.31 0.22 0.27 0.28 0.22 0.25 0.15 0.22 0.26 0.24 0.26 0.25 0.23 0.27 0.27 0.21 0.29 0.30 1.00 0.22 0.28 0.27 0.22 0.24 0.15 0.21 0.26 0.22 0.26 0.25 0.24 0.28 0.28 0.17 0.22 0.24 0.23 0.99 0.22 0.25 0.25 0.27 0.12 0.17 0.21 0.27 0.24 0.25 0.23 0.23 0.23 0.21 0.27 0.26 0.27 0.22 1.00 0.27 0.23 0.23 0.18 0.24 0.26 0.22 0.25 0.25 0.24 0.27 0.27 0.19 0.26 0.27 0.27 0.24 0.27 1.00 0.24 0.24 0.16 0.22 0.24 0.24 0.25 0.25 0.23 0.25 0.26 0.16 0.21 0.22 0.22 0.23 0.21 0.23 1.00 0.24 0.12 0.17 0.20 0.22 0.24 0.24 0.23 0.22 0.22 0.17 0.22 0.23 0.23 0.26 0.22 0.24 0.25 1.00 0.12 0.16 0.20 0.25 0.24 0.25 0.23 0.23 0.22</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>0. 22 0</head><label>22</label><figDesc>.23 0.21 0.22 0.18 0.24 0.21 0.18 0.18 0.21 1.00 0.24 0.17 0.21 0.19 0.20 0.23 0.23 0.21 0.25 0.24 0.25 0.21 0.26 0.24 0.21 0.21 0.18 0.24 1.00 0.21 0.23 0.23 0.23 0.25 0.25 0.15 0.21 0.23 0.22 0.28 0.21 0.23 0.24 0.27 0.11 0.15 0.20 1.00 0.24 0.24 0.23 0.22 0.23 0.19 0.25 0.26 0.25 0.24 0.26 0.26 0.25 0.24 0.14 0.20 0.24 0.22 1.00 0.29 0.29 0.25 0.25 0.18 0.23 0.25 0.24 0.24 0.24 0.25 0.24 0.24 0.13 0.18 0.22 0.23 0.28 1.00 0.28 0.24 0.24 0.19 0.24 0.25 0.24 0.22 0.26 0.25 0.24 0.23 0.15 0.21 0.23 0.22 0.29 0.28 1.00 0.25 0.25 0.21 0.27 0.26 0.27 0.23 0.27 0.26 0.23 0.24 0.16 0.22 0.25 0.23 0.25 0.23 0.23 1.00 0.30 0.21 0.27 0.26 0.26 0.23 0.27 0.26 0.23 0.24 0.17 0.22 0.25 0.23 0.24 0.24 0.24 0.30 10.09 0.05 0.07 0.03 0.08 0.07 0.06 0.03 0.17 0.12 0.08 0.04 0.06 0.05 0.07 0.08 0.07 0.01 1.00 0.04 0.08 0.00 0.01 0.01 0.01 0.00 0.02 0.01 0.01 0.00 0.02 0.01 0.02 0.01 0.01 0.01 0.07 1.00 0.08 0.01 0.01 0.01 0.00 0.00 0.03 0.02 0.02 0.01 0.02 0.00 0.01 0.01 0.01 0.00 0.07 0.05 1.00 0.01 0.03 0.01 0.01 0.00 0.03 0.00 0.02 0.00 0.02 0.01 0.01 0.01 0.01 0.29 0.27 0.24 0.26 1.00 0.30 0.28 0.32 0.25 0.45 0.37 0.34 0.18 0.23 0.21 0.23 0.25 0.26 0.03 0.08 0.06 0.08 0.05 1.00 0.09 0.05 0.04 0.05 0.07 0.08 0.03 0.06 0.05 0.06 0.06 0.05 0.02 0.04 0.03 0.03 0.03 0.06 1.00 0.03 0.03 0.03 0.04 0.03 0.03 0.04 0.03 0.04 0.02 0.01 0.02 0.04 0.03 0.04 0.04 0.04 0.06 1.00 0.04 0.04 0.05 0.06 0.03 0.03 0.02 0.03 0.03 0.03 0.03 0.06 0.06 0.05 0.05 0.05 0.06 0.05 1.00 0.07 0.07 0.08 0.03 0.05 0.02 0.04 0.04 0.06 0.03 0.02 0.02 0.03 0.03 0.03 0.03 0.03 0.03 1.00 0.07 0.06 0.02 0.03 0.02 0.04 0.04 0.04 0.02 0.03 0.02 0.02 0.02 0.03 0.03 0.02 0.02 0.08 1.00 0.06 0.02 0.03 0.02 0.03 0.03 0.03 0.02 0.04 0.03 0.04 0.03 0.05 0.03 0.04 0.03 0.04 0.07 1.00 0.03 0.05 0.03 0.05 0.04 0.03 0.06 0.11 0.07 0.07 0.05 0.08 0.09 0.07 0.05 0.12 0.13 0.10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>0.06 0.05 0.05 0.05 0.04 0.06 0.05 0.05 0.04 0.16 1.00 0.11 0.04 0.04 0.04 0.05 0.06 0.06 0.03 0.07 0.05 0.06 0.05 0.07 0.05 0.05 0.06 0.07 0.14 1.00 0.04 0.07 0.05 0.07 0.05 0.06</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>0.06 0.05 0.03 0.08 0.06 0.06 0.04 0.07 0.07 0.05 0.04 0.08 0.07 0.11 0.03 0.16 0.14 1.00 0.05 0.06 0.14 0.46 0.35 0.40 0.24 0.44 0.35 0.30 0.34 0.19 0.30 0.33 0.27 0.29 0.27 0.30 1.00 0.93 0.14 0.45 0.34 0.37 0.25 0.43 0.34 0.32 0.32 0.19 0.29 0.33 0.27 0.28 0.26 0.29 0.93 10.23 0.22 0.22 0.18 0.22 0.21 0.19 0.19 0.22 0.24 0.23 0.18 0.20 0.20 0.20 0.24 0.23 0.21 1.00 0.33 0.33 0.23 0.29 0.29 0.24 0.26 0.16 0.25 0.28 0.25 0.27 0.26 0.26 0.30 0.29 0.21 0.32 1.00 0.33 0.23 0.29 0.29 0.24 0.26 0.16 0.24 0.27 0.25 0.27 0.25 0.25 0.29 0.28 0.21 0.33 0.33 1.00 0.23 0.29 0.29 0.24 0.25 0.16 0.23 0.28 0.24 0.26 0.26 0.25 0.29 0.29 0.00 0.01 0.01 0.01 0.03 0.01 0.01 0.01 0.01 0.01 0.01 0.03 0.01 0.01 0.01 0.01 0.03 0.03 0.03 0.05 0.05 0.06 0.04 0.24 0.05 0.04 0.04 0.04 0.04 0.06 0.04 0.04 0.05 0.04 0.06 0.07 0.01 0.02 0.03 0.03 0.02 0.03 0.12 0.02 0.02 0.02 0.02 0.04 0.02 0.03 0.02 0.02 0.05 0.05</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The transferability of EAD-L1 attack over 18 ImageNet models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc><ref type="bibr" target="#b17">18</ref> ImageNet models under robustness examination</figDesc><table><row><cell>Models</cell><cell cols="4">Year # layers # parameters Top-1/5 ImageNet accuracies</cell></row><row><cell>AlexNet [1]</cell><cell>2012</cell><cell>8</cell><cell>60 million</cell><cell>56.9% / 80.1% a</cell></row><row><cell>VGG 16 [18]</cell><cell>2014</cell><cell>16</cell><cell>138 million</cell><cell>71.5% / 89.8%[29]</cell></row><row><cell>VGG 19 [18]</cell><cell>2014</cell><cell>19</cell><cell>144 million</cell><cell>71.1% / 89.8%[29]</cell></row><row><cell>Inception-v1 [19]</cell><cell>2014</cell><cell>22</cell><cell>6.7 million</cell><cell>69.8% / 89.6%[29]</cell></row><row><cell>Inception-v2 [25]</cell><cell>2015</cell><cell>48</cell><cell>11.3 million</cell><cell>73.9% / 91.8%[29]</cell></row><row><cell>Inception-v3 [26]</cell><cell>2015</cell><cell>48</cell><cell>23.9 million</cell><cell>78.0% / 93.9%[29]</cell></row><row><cell>Inception-v4 [27]</cell><cell>2016</cell><cell>76</cell><cell>42.9 million</cell><cell>80.2% / 95.2%[29]</cell></row><row><cell cols="2">Inception-ResNet-v2 [27] 2016</cell><cell>96</cell><cell>56.1 million</cell><cell>80.4% / 95.3%[29]</cell></row><row><cell>ResNet-v2-50 [30]</cell><cell>2016</cell><cell>50</cell><cell>25.7 million</cell><cell>75.6% / 92.8%[29]</cell></row><row><cell>ResNet-v2-101 [30]</cell><cell>2016</cell><cell>101</cell><cell>44.8 million</cell><cell>77.0% / 93.7%[29]</cell></row><row><cell>ResNet-v2-152 [30]</cell><cell>2016</cell><cell>152</cell><cell>60.6 million</cell><cell>77.8% / 94.1%[29]</cell></row><row><cell>DenseNet-121-k32 [21]</cell><cell>2017</cell><cell>121</cell><cell>8.2 million</cell><cell>74.9% / 92.2 % b</cell></row><row><cell>DenseNet-169-k32 [21]</cell><cell>2017</cell><cell>169</cell><cell>14.4 million</cell><cell>76.1% / 93.1 % b</cell></row><row><cell>DenseNet-161-k48 [21]</cell><cell>2017</cell><cell>161</cell><cell>29.0 million</cell><cell>77.6% / 93.8 % b</cell></row><row><cell cols="2">MobileNet-0.25-128 [22] 2017</cell><cell>128</cell><cell>0.5 million</cell><cell>41.5% / 66.3%[29]</cell></row><row><cell cols="2">MobileNet-0.50-160 [22] 2017</cell><cell>160</cell><cell>1.4 million</cell><cell>59.1% / 81.9%[29]</cell></row><row><cell>MobileNet-1.0-224 [22]</cell><cell>2017</cell><cell>224</cell><cell>4.3 million</cell><cell>70.9% / 89.9% [29]</cell></row><row><cell>NASNet [23]</cell><cell>2017</cell><cell>-</cell><cell>88.9 million</cell><cell>82.7% / 96.2%[29]</cell></row><row><cell cols="4">com/pudae/tensorflow-densenet</cell><cell></cell></row><row><cell cols="5">where sgn(∇J(x 0 , t)) is the sign of the gradient of the training loss with respect</cell></row><row><cell cols="5">to x 0 , and clip(x) ensures that x stays within the range of pixel values. It is</cell></row><row><cell cols="5">efficient for generating adversarial examples as it is just an one-step attack.</cell></row></table><note>a https://github.com/BVLC/caffe/wiki/Models-accuracy-on-ImageNet-2012-val b https://github.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>1.00 0.05 0.04 0.06 0.07 0.06 .00 0.23 0.22 0.22 0.18 0.22 0.21 0.18 0.18 0.21 0.24 0.23 0.18 0.20 0.20 0.20 0.23 0.23 0.20 1.00 0.32 0.32 0.22 0.29 0.29 0.24 0.25 0.15 0.24 0.28 0.24 0.27 0.25 0.26 0.29 0.29 0.21 0.31 1.00 0.32 0.23 0.28 0.29 0.23 0.25 0.15 0.23 0.27 0.24 0.27 0.25 0.24 0.28 0.28 0.21 0.32 0.32 1.00 0.23 0.28 0.28 0.23 0.24 0.15 0.23 0.27 0.24 0.26 0.25 0.24 0.29 0.28 0.02 0.03 0.03 0.03 0.16 0.03 0.03 0.04 0.04 0.02 0.02 0.05 0.04 0.04 0.04 0.03 0.05 0.05 0.16 0.24 0.24 0.24 0.20 0.87 0.25 0.20 0.21 0.15 0.20 0.23 0.20 0.22 0.21 0.21 0.24 0.23 0.15 0.21 0.22 0.22 0.20 0.22 0.85 0.20 0.19 0.13 0.17 0.19 0.20 0.20 0.20 0.19 0.20 0.21 0.15 0.20 0.22 0.21 0.23 0.21 0.22 0.98 0.23 0.11 0.16 0.19 0.22 0.23 0.24 0.22 0.21 0.21 0.11 0.15 0.16 0.16 0.19 0.15 0.17 0.18 0.82 0.08 0.10 0.13 0.18 0.17 0.18 0.17 0.15 0.15 0.02 0.02 0.02 0.02 0.01 0.02 0.02 0.02 0.02 0.14 0.03 0.03 0.01 0.02 0.02 0.01 0.03 0.03 0.18 0.20 0.19 0.19 0.15 0.21 0.19 0.16 0.15 0.19 0.89 0.22 0.15 0.17 0.17 0.17 0.20 0.20 0.21 0.26 0.26 0.26 0.22 0.27 0.25 0.23 0.23 0.18 0.26 1.00 0.22 0.25 0.24 0.24 0.26 0.26 0.09 0.13 0.14 0.13 0.18 0.12 0.14 0.15 0.17 0.06 0.09 0.12 0.78 0.15 0.16 0.14 0.14 0.13 0.18 0.25 0.26 0.25 0.24 0.26 0.26 0.26 0.25 0.14 0.20 0.24 0.24 1.00 0.31 0.31 0.25 0.25 0.17 0.23 0.25 0.24 0.24 0.23 0.25 0.25 0.25 0.13 0.18 0.22 0.24 0.29 0.99 0.29 0.23 0.24 0.18 0.24 0.26 0.25 0.24 0.26 0.26 0.25 0.24 0.14 0.20 0.23 0.23 0.31 0.30 1.00 0.25 0.25 0.21 0.31 0.30 0.31 0.26 0.31 0.29 0.27 0.28 0.18 0.25 0.29 0.28 0.27 0.26 0.26 1.00 0.44 0.23 0.30 0.30 0.30 0.26 0.30 0.30 0.27 0.28 0.19 0.25 0.29 0.27 0.27 0.27 0.27 0.44 1.00 = 20, targeted, top-5 success rate</figDesc><table><row><cell>AlexNet DenseNet-121-k32 DenseNet-161-k48 DenseNet-169-k32 Inception-ResNet-v2 Inception-v1 Inception-v2 Inception-v3 Inception-v4 MobileNet-0.25-128 MobileNet-0.50-160 MobileNet-1.0-224 NASNet VGG 19 VGG 16 ResNet-v2-50 ResNet-v2-152 ResNet-v2-101 Source Model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AlexNet</cell><cell>DenseNet-121-k32</cell><cell>DenseNet-161-k48</cell><cell>DenseNet-169-k32</cell><cell>Inception-ResNet-v2</cell><cell>Inception-v1</cell><cell>Inception-v2 Target Model Inception-v3 Inception-v4 MobileNet-0.25-128 MobileNet-0.50-160 MobileNet-1.0-224</cell><cell>NASNet</cell><cell>ResNet-v2-101</cell><cell>ResNet-v2-152</cell><cell>ResNet-v2-50</cell><cell>VGG 16</cell><cell>VGG 19</cell></row></table><note>0.03 0.06 0.06 0.06 0.05 0.07 0.07 0.06 0.05 0.07 0.07 0.09 0.03 1.00 0.13 0.13 0.04 0.03 0.02 0.06 0.05 0.06 0.03 0.06 0.06 0.05 0.05 0.06 0.05 0.08 0.03 0.13 1.00 0.12 0.04 0.04 0.02 0.05 0.05 0.06 0.03 0.06 0.05 0.04 0.03 0.05 0.05 0.07 0.03 0.13 0.12 1.00 0.04 0.05 0.10 0.31 0.22 0.26 0.16 0.30 0.22 0.20 0.20 0.11 0.19 0.21 0.17 0.19 0.17 0.20 1.00 0.77 0.09 0.29 0.21 0.24 0.16 0.27 0.21 0.19 0.20 0.13 0.15 0.21 0.16 0.18 0.15 0.20 0.78 1.00 = 20, untargeted, success rate 11.00 0.14 0.09 0.11 0.05 0.13 0.10 0.09 0.06 0.26 0.19 0.13 0.06 0.09 0.07 0.11 0.13 0.12 0.02 1.00 0.07 0.11 0.01 0.02 0.02 0.02 0.01 0.05 0.02 0.03 0.01 0.02 0.02 0.03 0.01 0.01 0.02 0.12 1.00 0.15 0.01 0.03 0.02 0.01 0.01 0.04 0.01 0.02 0.01 0.02 0.01 0.02 0.01 0.02 0.01 0.13 0.12 1.00 0.01 0.02 0.02 0.01 0.01 0.05 0.02 0.01 0.01 0.03 0.02 0.03 0.01 0.01 0.86 0.84 0.82 0.84 1.00 0.84 0.84 0.85 0.81 0.94 0.90 0.86 0.78 0.80 0.79 0.80 0.85 0.85 0.07 0.12 0.09 0.11 0.06 1.00 0.13 0.09 0.07 0.11 0.13 0.12 0.05 0.10 0.08 0.11 0.09 0.10 0.06 0.08 0.06 0.07 0.04 0.09 1.00 0.06 0.05 0.10 0.10 0.08 0.03 0.06 0.05 0.07 0.05 0.05 0.04 0.08 0.06 0.07 0.06 0.08 0.10 1.00 0.05 0.10 0.10 0.12 0.05 0.05 0.04 0.05 0.06 0.06</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fooling vision and language models despite localization and attention mechanism</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the Thirtieth IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attacking visual language grounding with adversarial examples: A case study on neural image captioning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2587" to="2597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations against semantic image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01128</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Audio adversarial examples: Targeted attacks on speechto-text</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Security Workshop</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Identify susceptible locations in medical records via adversarial attacks on deep predictive models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="793" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generating adversarial examples with adversarial networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, International Joint Conferences on Artificial Intelligence Organization</title>
				<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18, International Joint Conferences on Artificial Intelligence Organization</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3905" to="3911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatially transformed adversarial examples</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust physical-world attacks on deep learning visual classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1625" to="1634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Formal guarantees on the robustness of a classifier against adversarial manipulation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriushchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2263" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluating the robustness of neural networks: An extreme value theory approach</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Daniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards fast computation of certified robustness for relu networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Daniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML)</title>
				<meeting>the 35th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11443</idno>
		<title level="m">Convnets and imagenet beyond accuracy: Explanations, bias detection, adversarial examples and model criticism</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015</title>
				<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">June 7-12, 2015. 2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
				<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">June 27-30, 2016. 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>CoRR abs/1704.04861</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR (ICLR)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML 2015</title>
				<meeting>the 32nd International Conference on Machine Learning, ICML 2015<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-11">6-11 July 2015. 2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
				<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">June 27-30, 2016. 2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">February 4-9, 2017. 2017</date>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Andersen</surname></persName>
		</author>
		<ptr target="https://github.com/tensorflow/models/tree/master/research/slim" />
		<title level="m">Tensorflow-Slim Image Classification Model Library</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Transferability in machine learning: from phenomena to black-box attacks using adversarial samples</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07277</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
		<imprint>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="2017">2017</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Autozoom: Autoencoder-based zeroth order optimization method for attacking blackbox neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<idno>CoRR abs/1805.11770</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Query-efficient hard-label black-box attack: An optimization-based approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04457</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11770</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Parseval networks: Improving robustness to adversarial examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="854" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy</title>
				<meeting><address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-05-22">2017. May 22-26, 2017. 2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. AISec &apos;17</title>
				<meeting>the 10th ACM Workshop on Artificial Intelligence and Security. AISec &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Ead: Elastic-net attacks to deep neural networks via adversarial examples</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Attacking the Madry defense model with L1-based adversarial examples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10733</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">On the limitation of magnet defense against L1-based adversarial examples</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>IEEE/IFIP DSN Workshop</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On the limitation of local intrinsic dimensionality for characterizing the subspaces of adversarial examples</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2009">2009. 2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="323" to="332" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deepfool: A simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
				<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">June 27-30, 2016. 2016</date>
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Synthesizing robust adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adversarial examples for semantic segmentation and object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
