<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hookworm Detection in Wireless Capsule Endoscopy Images with Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun-Yan</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Qiang</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName><roleName>Life Fellow, IEEE</roleName><forename type="first">Ramesh</forename><surname>Jain</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Conv</forename><surname>Conv</surname></persName>
						</author>
						<title level="a" type="main">Hookworm Detection in Wireless Capsule Endoscopy Images with Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B0E81D4CEBFF1B315D8CEB61F06D507D</idno>
					<idno type="DOI">10.1109/TIP.2018.2801119</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2018.2801119, IEEE Transactions on Image Processing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2018.2801119, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hookworm detection</term>
					<term>deep learning</term>
					<term>convolutional neural network</term>
					<term>computer-aided detection</term>
					<term>wireless capsule endoscopy Side-output-2 Side-output-3 FC Inception Hookworm Classification Network Tubular region Tubular region Conv</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As one of the most common human helminths, hookworm is a leading cause of maternal and child morbidity, which seriously threatens human health. Recently, wireless capsule endoscopy (WCE) has been applied to automatic hookworm detection. Unfortunately, it remains a challenging task. In recent years, deep convolutional neural network (CNN) has demonstrated impressive performance in various image and video analysis tasks. In this paper, a novel deep hookworm detection framework (DHDF) is proposed for WCE images, which simultaneously models visual appearances and tubular patterns of hookworms. This is the first deep learning framework specifically designed for hookworm detection in WCE images. Two CNN networks, namely edge extraction network and hookworm classification network, are seamlessly integrated in the proposed framework, which avoid the edge feature caching and speed up the classification. Two edge pooling layers are introduced to integrate the tubular regions induced from edge extraction network and the feature maps from hookworm classification network, leading to enhanced feature maps emphasizing the tubular regions. Experiments have been conducted on one of the largest WCE datasets with 440K WCE images, which demonstrate the effectiveness of the proposed hookworm detection framework. It significantly outperforms the state-of-the-art approaches. The high sensitivity and accuracy of the proposed method in detecting hookworms shows its potential for clinical application.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>H OOKWORM is an infection by a parasitic bloodsucking roundworm. It is a leading cause of maternal and child morbidity in developing countries of the tropics and subtropics due to poor sanitation. Hookworm infection seriously threatens human health, which will impair the physical and intellectual development of children. It is reported that hookworm has affected more than 600 million people worldwide <ref type="bibr" target="#b0">[1]</ref>.</p><p>As a miniature medical device for gastrointestinal (GI) diagnosis, Wireless Capsule Endoscopy (WCE) <ref type="bibr" target="#b1">[2]</ref> travels through the digestive system to collect images or physiological data This work was supported in part by the National Natural Science Foundation of China (61772436, 61373121, and 61272290), and Sichuan Science and Technology Innovation Seedling Fund (2017RZ0015, 2017018). Asterisk indicates corresponding author.</p><p>Jun-Yan He, Xiao Wu and Qiang Peng are with the School of Information Science and Technology, Xipu Campus, Southwest Jiaotong University, Chengdu, 611756 China. (e-mail: junyanhe1989@gmail.com; wuxiaohk@swjtu.edu.cn; qpeng@swjtu.edu.cn).</p><p>Yu-Gang Jiang is with the School of Computer Science, Fudan University, Shanghai. (email: ygj@fudan.edu.cn).</p><p>Ramesh Jain is with School of Information and Computer Science, University of California, Irvine, USA, e-mail: jain@ics.uci.edu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Sequence of Wireless Capsule Endoscopy</head><p>Wireless Capsule Endoscopy (WCE) Automatic Hookworm Detection Fig. <ref type="figure">1</ref>. Wireless Capsule Endoscopy (WCE) will take two or more color images per second to capture the whole gastrointestinal (GI) tract after swallowed by the patient. These WCE images are then analyzed by automatic abnormal detection software.</p><p>after swallowed by the patient. It will take two or more color images of GI tract per second, which will last for a few hours to capture the whole GI tract, totally around 50, 000 images. It is a laborious and tedious process for trained endoscopists to identify suspicious areas and analyze the potential diseases, which usually take a couple of hours to manually examine these images. To assist the endoscopists, a series of automatic lesion detection solutions have been proposed recently. The process is illustrated in Fig. <ref type="figure">1</ref>. It is reported that over one million patients globally have been examined with WCE, which has been widely used for several inflammatory bowel diseases and disorders in recent years, such as bleeding <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, polyp <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>, ulcer <ref type="bibr" target="#b4">[5]</ref>, tumor <ref type="bibr" target="#b7">[8]</ref>, Crohns disease <ref type="bibr" target="#b8">[9]</ref>, and so on. Unfortunately, automatic hookworm detection <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> in WCE images has not been fully explored.</p><p>Automatic hookworm detection in WCE images remains a challenging task. The quality of WCE images is usually poor due to the hardware limitation and the light condition. Its resolution is only 256 × 256 pixels. The free motion of the capsule and the contractions that the gut undertakes produce various orientations and perspectives of the scene. There exists complex structure for different parts of the intestinal tract (stomach, duodenum, jejunum-ileum, and cecum), presenting various appearances with multiple colors and textures. The existence of diverse extraneous matters mixed in GI tract, such as food, stool, bile and bubbles, seriously influences the detection. Moreover, the hookworms demonstrate different shapes, widths and bend orientations. These challenges pose a great difficulty for automatic hookworm detection.</p><p>Due to the superior ability of learning mid-level and highlevel abstractions obtained from raw data, deep learning, in particular, deep convolutional neural network (CNN), has become the leading machine learning tool for a broad range of computer vision tasks, such as image classification <ref type="bibr" target="#b11">[12]</ref>, object detection <ref type="bibr" target="#b12">[13]</ref>, and so on. Recently, deep CNN has been applied to a wide variety of medical image analysis tasks. Remarkable performance has been reported to discern complex patterns for diseases, which are usually difficult to be encoded by humans and traditional learning methods.</p><p>Hookworm is a kind of small tubular structure with different appearances from mucosa and bubble edges, usually having grayish white or pinkish semi-transparent body <ref type="bibr" target="#b10">[11]</ref>. The edges of hookworm bodies are usually in the form of nearly parallel curves. The tubular regions are the potential areas of hookworms. The hookworm bodies demonstrate either lighter or darker patterns compared to surrounding mucosa, meanwhile, hookworms have various orientations and inconsistent widths. These characteristics will be exploited for hookworm detection. The improved performance demonstrates that the tubular parallel regional information of hookworms is very beneficial for the hookworm detection <ref type="bibr" target="#b10">[11]</ref>. Motivated by this success, the correlation of visual appearances and tubular patterns should be captured, which could offer more discriminative features to boost the performance of detection.</p><p>In this paper, a novel deep hookworm detection framework (DHDF) based on convolutional neural network is proposed for WCE images, which is illustrated in Fig. <ref type="figure">2</ref>. The proposed deep hookworm detection framework models visual appearances and tubular patterns of hookworms. It seamlessly integrates two CNN networks, that is, edge extraction network and hookworm classification network, to extract the edge features and classify the hookworms, respectively. Since tubular regions are important clues for hookworm detection, edge extraction network is first used to produce the edge maps for tubular region detection. It is achieved with a state-of-theart method, holistically-nested edge detection (HED) network <ref type="bibr" target="#b13">[14]</ref>. To trade-off the cost and the performance, hookworm classification network is based on CNN with Inception <ref type="bibr" target="#b11">[12]</ref> module, which demonstrates high performance and with reasonable number of parameters. The key component of the proposed network is the edge pooling layer with two-level fusion architecture, which is used to integrate the tubular regions induced from the edge extraction network. To fully utilize multiple side-outputs from HED, two edge pooling layers are embedded into the shallow and deep parts of hookworm classification network, respectively, to enhance the feature maps corresponding to tubular regions. With this structure, tubular regions detection and feature maps are mutually enhanced.</p><p>Moreover, the tubular region maps are pretty sparse, in which only a small portion of the tubular region maps contains the hookworms and most of the areas are noises. Regularized fusion <ref type="bibr" target="#b14">[15]</ref> is proposed to impose sparsity on the weight matrix, which can help avoid information sharing among irrelevant classes. By treating the class correlation matrix as a prior, it minimizes an empirical loss regularized by a sparsity constraint to adaptively adjust the fusion weights. Enlightened by this idea, regularized edge pooling is novelly proposed in this work. A weight matrix is introduced to select the correct tubular regions for edge pooling, which is constrained by a regular norm to control the edge pooling. Although the proposed method and previous work <ref type="bibr" target="#b14">[15]</ref> both apply the regularized method to improve the performance of classification task, their focus is different. The regular terms in <ref type="bibr" target="#b14">[15]</ref> are used to explore the relationship between these two kinds of features. In this work, the regular term is employed in the edge pooling to denoise and generate the sparsity. Therefore, they are totally different.</p><p>The main contributions of this work are summarized as follows:</p><p>• To the best of our knowledge, this is the first deep learning framework specifically designed for hookworm detection in WCE images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we will first review the works on pathological abnormality detection for WCE images, and then introduce the deep learning techniques used for medical image processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pathological Abnormality Detection</head><p>Recently, computer aided detection systems for WCE have been extensively conducted, which bring endoscopists great convenience and efficiency. Comprehensive surveys <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b16">[17]</ref> systematically summarize the latest development on WCE research from different aspects. Existing WCE research focuses on pathological abnormality detection, including bleeding <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, polyp <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>, ulcer <ref type="bibr" target="#b4">[5]</ref>, tumor <ref type="bibr" target="#b7">[8]</ref>, Crohns disease <ref type="bibr" target="#b8">[9]</ref> and other intestinal contents.</p><p>The majority of the WCE research is related to bleeding detection since it is the main clinical pathology in GI tract. Bleeding region detection and frame localization is solved with support vector machine (SVM) and KNN classifier <ref type="bibr" target="#b3">[4]</ref>. A rapid bleeding detection method for WCE video is proposed in <ref type="bibr" target="#b2">[3]</ref>, in which red ratio of RGB color space is extracted from each superpixel and then classified with SVM. For the polyp and the perforated ulcer detection, a synergistic framework based on the range ratio color is proposed in <ref type="bibr" target="#b4">[5]</ref>. Based on this feature, the images are segmented to candidate regions, and geometric characteristics like curvature and eccentricity are applied to produce the final polyp candidates. Polyps are detected based on the geometrical analysis and the texture content of WCE frames <ref type="bibr" target="#b5">[6]</ref>. An improved bag of features (BoF) method is proposed in <ref type="bibr" target="#b6">[7]</ref> to assist classification of polyps in WCE images. Color texture feature is integrated with uniform local binary pattern (LBP) and wavelet for polyp and tumor detection <ref type="bibr" target="#b7">[8]</ref>. Feature selection is further employed to improve the detection accuracy. Discrete lesions created by mucosal inflammation in Crohn's disease are first classified, and the lesion severity is also assessed <ref type="bibr" target="#b8">[9]</ref>. A coding method called saliency and adaptive LLC (SALLC) is proposed in <ref type="bibr" target="#b17">[18]</ref> to detect multiple abnormal images, including bleeding, polyp, and ulcer from WCE images.</p><p>Although extensive works have been conducted on pathological abnormality detection for WCE images/videos, there are limited works on automatic hookworm detection. In <ref type="bibr" target="#b9">[10]</ref>, hybrid color gradient and contourlet transform are utilized to detect hookworms. However, this work is evaluated in a relative small and balanced dataset with 1, 500 images. Recently, we explore the automatic hookworm detection on a very large and imbalanced dataset <ref type="bibr" target="#b10">[11]</ref>, in which the piecewise parallel regions are first detected and then represented in the consistent format, i.e., uncurled tubular regions (UTR). To discriminate the unique features for different components of GI, the histogram of average intensity (HAI) is proposed to represent their properties. The experiments demonstrate promising performance. In this paper, instead of using the hand-drafted features, we further boost the performance by tailoring deep learning methods for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Learning for Medical Image Processing</head><p>Inspired by biological processes and designed to recognize patterns directly from image pixels, deep convolutional neural network (CNN) <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref> has achieved great success in various tasks, such as image classification, object detection, image segmentation, and so on. Recently, deep learning has been imported into medical image processing and presented impressive performance on different applications in image segmentation and lesion detection <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b28">[29]</ref>. In this subsection, we will first introduce the lesion detection works based on deep learning, followed by other aspects affecting deep learning performance, such as network architecture, training strategy, and so on.</p><p>The majority research based on CNN focuses on magnetic response (MR) and CT images. Different architectures based on convolutional neural networks are investigated to automatically segment tissue classes <ref type="bibr" target="#b21">[22]</ref>, gliomas <ref type="bibr" target="#b22">[23]</ref> and sclerosis lesions <ref type="bibr" target="#b23">[24]</ref> in MR images. In <ref type="bibr" target="#b24">[25]</ref>, a 3D CNN is exploited to detect cerebral microbleeds from MR images, which takes full advantages of spatial contextual information in MR volumes to extract more representative high-level features, achieving better detection accuracy. A spatially constrained convolutional neural network (SC-CNN) is proposed in <ref type="bibr" target="#b25">[26]</ref> to perform nucleus detection in histopathology images. The convolutional classification restricted Boltzmann machine is proposed in <ref type="bibr" target="#b26">[27]</ref> for lung texture classification and airway detection in CT images, which combines a generative and a discriminative learning objective. A deep neural network based segmentation method is proposed in <ref type="bibr" target="#b27">[28]</ref> to detect retinal blood vessels, in which several network architectures and image preprocessing methods are explored. A CNN is proposed for interstitial lung disease classification <ref type="bibr" target="#b28">[29]</ref>, which consists of 5 convolutional layers and LeakyReLU activations, followed by average pooling and three dense layers.</p><p>In addition to lesion detection, deep learning techniques have been investigated in other aspects of medical image processing. Three important factors, i.e., network architectures, dataset characteristics and transfer learning, are discussed for thoraco-abdominal lymph node detection and interstitial lung disease classification <ref type="bibr" target="#b29">[30]</ref>. A fast method is proposed in <ref type="bibr" target="#b30">[31]</ref> to speed up the CNN training for medical image analysis tasks, by dynamically selecting misclassified negative samples during training. One relevant CNN-based work for WCE images is digestive organ classification <ref type="bibr" target="#b31">[32]</ref>, in which a deep CNN framework is employed to learn layer-wise hierarchy models to classify the digestive organs in WCE images. Four distinct medical imaging applications based on deep learning are explored in three specialties (radiology, cardiology, and gastroenterology) <ref type="bibr" target="#b32">[33]</ref>, including polyp detection, pulmonary embolism detection, colonoscopy frame classification and intima-media boundary segmentation from different imaging modalities.</p><p>Although various works based on deep CNN have been applied to medical image processing, existing deep learning based approaches for other medical applications are not directly applicable for hookworm detection. So far, there are limited works investigating deep CNNs for hookworm detection in WCE images, which motivates this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EDGE EXTRACTION NETWORK</head><p>In this section, we will introduce the edge extraction network, corresponding to the top part of Fig. <ref type="figure">2</ref>. To capture the tubular patterns of hookworms, edge extraction network is first adopted to produce the edge maps and then they are fed into multi-scale dual matched filter for tubular region detection, which will be presented in Sections III-A and III-B, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Edge Map Generation</head><p>Due to the excellent performance and computational efficiency, the state-of-the-art approach, holistically-nested edge detection (HED) <ref type="bibr" target="#b13">[14]</ref> is adopted in this paper to extract the edges in WCE images, which performs image-to-image prediction by means of a deep learning model that leverages fully convolutional neural networks and deeply-supervised nets. It achieves the best performance for edge detection so far. The edge extraction network is illustrated in Fig. <ref type="figure">3</ref>. HED is based on VGG networks <ref type="bibr" target="#b19">[20]</ref>, which consists of 16 neural network layers. The architecture of HED comprises a single stream deep network with multiple side-outputs, in which the side responses are generated for individual layers. Five layers are selected as the side-outputs and fused by an average pooling layer. With deep supervision, the successive edge maps are inherited and progressively produced as side-outputs of the network layers. To perform the average fusion operation, the deconvolutional layers are adopted to resize all side-outputs to the same size. The fusion output is fed into the softmax layer to produce the final label map. Since the hierarchical side-outputs produce the multi-level edge maps from outlines to details, it is possible to extract the edges in the complex background by changing the hyper parameters. Guided by deep supervision on side responses, HED automatically learns rich hierarchical representations, in which the hidden layer supervision can improve both the optimization and the generalization for image classification tasks <ref type="bibr" target="#b33">[34]</ref>. The WCE images are fed into the pretrained HED model, and feed-forward operation is performed to produce the side-outputs for hookworm classification network.</p><p>The training data are denoted as S = {(X n , Y n ), n = 1, ..., N }, where X n is the raw input image and Y n ∈ {0, 1} is the corresponding ground truth binary edge map for image X n . Suppose there are K side-outputs in the network, each side-output layer is associated with a classifier, in which the corresponding weights are denoted as w = {w (1) , ...w (K) }, where all parameters of the standard network layer are donated as W . The objective function of side-outputs is defined as follows:</p><formula xml:id="formula_0">L side (W, w) = K k=1 α k l (k) side (W, w (k) )<label>(1)</label></formula><p>where the kth loss function for side-output l</p><formula xml:id="formula_1">(k)</formula><p>side is a classbalanced cross-entropy loss function: </p><formula xml:id="formula_2">l (k) side (W, w (k) ) = -β Y+ log P r(y i = 1|X; W, w (k) ) -(1 -β) Y- log P r(y j = 0|X; W, w (k) )<label>(2</label></formula><formula xml:id="formula_3">(k) ) = σ(a (k) j ) ∈ [0, 1] is computed using sigmoid function σ(.). Edge map from each side-output layer is obtained by Y (k) side = σ( A (k) side ), where A (k) side ≡ {a (k) j , j = 1, ...|Y |}</formula><p>are activations of the sideoutput of layer k. To directly utilize side-output predictions, a "weighted-fusion" layer is added to the network and simultaneously learns the fusion weight during training, which is minimized by standard (back-propagation) stochastic gradient descent.</p><p>In the testing phase, the edge map predictions are produced by the side-output layers and the weighted-fusion layer:</p><formula xml:id="formula_4">( Y f usion , Y (1) side , . . . , Y (K) side ) = HED(X, (W, w, h))<label>(3)</label></formula><p>where X donates the testing image and HED refers to the HED model.</p><p>The side-outputs at different scales produced by HED are illustrated in Fig. <ref type="figure" target="#fig_2">4</ref>. From this figure, we can see that there exists a curled tubular-like hookworm in the top-right region of this WCE image. The detailed edges in side-outputs become vague and are not easy to be discerned as the level of sideoutputs increases. For the side-outputs in higher levels (e.g., the 4th and 5th layers), the edge predictions are coarse. The edges of hookworms are merged with the borders of folders. Only the outline of the whole WCE image is remained and many critical edges are absent. On the contrary, the sideoutputs in lower levels (e.g., the second and third layers) include more details, in which the edges of hookworms can be well detected, especially the third layer side-output. It is critical to ensure that the edges of hookworms are preserved in the side-outputs, so that tubular regions of hookworms can be captured. Therefore, in this paper, the second and the third side-outputs are selected as the inputs of tubular region detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Tubular Region Detection</head><p>Multi-scale dual matched filter (MDMF) proposed in <ref type="bibr" target="#b10">[11]</ref> is used to detect the tubular regions in WCE images. It is a Gaussian-shaped template based on prior information that the cross-section of a hookworm is Gaussian-shaped. The tubular region will produce a higher response when the matched filter convolves with the image. The side-outputs produced by HED contain a large number of edges, among which only a small portion of edges belongs to the hookworms. The sideoutputs are then passed to MDMF to emphasize the edges of hookworms and suppress the noisy ones.</p><p>To capture two different patterns of hookworms, dual matched filter kernel is defined as follows,</p><formula xml:id="formula_5">K ± σ (x, y) = ±e -x 2 2σ 2 , ∀ |y| ≤ L/2<label>(4)</label></formula><p>where K ± σ (x, y) is the kernel at a scale of σ. The symbol ± denotes two different hookworm patterns, dark or light hookworms. The term exp(-x 2 /2σ 2 ) denotes that the crosssection (x-axis direction) of a hookworm is Gaussian-shaped. From the perspective of two-dimensional space, the Gaussian term is repeated along y-axis with length L.</p><p>In order to adapt to different orientations of hookworm bodies, the kernel is rotated to all possible orientations, and a set of filter banks is obtained. The filter bank is normalized to zero mean after rotation. The final response is the one with the maximum convolution in the filter bank.</p><p>To detect hookworms with different widths, multi-scale production with the filter bank is deployed. Let f (x, y) be the image and m i (x, y) be the matched filter at scale σ i . The response at scale σ i and orientation θ j can be expressed as</p><formula xml:id="formula_6">P c (x, y) = m i,j (x, y) * f (x, y), c ∈ [0, N × M )<label>(5)</label></formula><p>where * denotes the convolution operation. P c is the tubular region map with the filter m i,j , where i ∈ [0, N ), j ∈ [0, M ) respectively. The index of tubular region map c = i * M + j.</p><p>Since the match filter is rotated to eight different orientations and resized into two scales, there are totally sixteen tubular region maps containing the part of the hookworm bodies. Accordingly, feature maps are also divided into sixteen groups, in which each group corresponds to a single tubular region map. These tubular region maps will be integrated with hookworm classification network for detection through edge pooling and regularized edge pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. HOOKWORM CLASSIFICATION NETWORK</head><p>In this section, we will present the hookworm classification network, which refers to the bottom part of Fig. <ref type="figure">2</ref>. The intuition is that the performance of hookworm detection could be effectively boosted, if the tubular regions can be appropriately integrated into the deep learning framework. With the tubular regions induced from edge extraction network, we construct a deep hookworm classification network in this section, in which two edge pooling layers are novelly proposed to embed tubular regions into the convolutional neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture of Hookworm Classification Network</head><p>An appropriate deep learning architecture is critical for classification, which directly determines the classification accuracy and generalization capability. The deep neural networks, such as AlexNet <ref type="bibr" target="#b18">[19]</ref>, VGGNet <ref type="bibr" target="#b19">[20]</ref> and GoogLeNet <ref type="bibr" target="#b11">[12]</ref>, have achieved state-of-the-art performance in various image classification tasks. The most straightforward way for improving the performance of deep neural networks is to increase their sizes. Unfortunately, deeper and wider networks will cause significant increase of computational resources and involve a larger number of parameters, which may incur overfitting if they are trained with insufficient data. To trade-off the cost and effectiveness, Inception model <ref type="bibr" target="#b11">[12]</ref> is proposed to approximate a sparse network structure to solve these two drawbacks, which has been successfully applied to ILSVRC2015 image classification competition and superior performance has been achieved, outperforming other CNN approaches. The Inception architecture aims at approximating and covering the optimal local sparse structure of a convolutional neural network by readily available locally dense components.</p><p>Since the spatial correlation of input images (or feature maps) is multi-scale, the fixed-size filter kernels will fail to capture this property. For hookworm detection, the sizes of hookworms in WCE images are inconsistent. Therefore, the appearance patterns of hookworms could be omitted by single fixed-size filter kernel. The major capability of Inception model is to increase the width of CNNs. It adopts filter kernels with multiple sizes (1 × 1, 3 × 3 and 5 × 5) to produce feature maps, corresponding to the multi-scale spatial correlation of input images. These feature maps are combined as the output. It is obvious that feature maps with multiple sizes will greatly increase the computing cost, especially the memory space. To reduce the number of parameters, 1 × 1 convolutions are first used to compute reductions before performing the expensive 3 × 3 and 5 × 5 convolutions. The capability of Inception model is significantly improved and the number of network parameters is at a reasonable level.</p><p>A critical issue is how to integrate the tubular regions into deep network. Since the network is deep, most information of tubular regions integrated into the shallow part of the network will be lost after passing several layers. Meanwhile, the high-level fusion information is noisy, which deteriorates the classification performance. The low-level side-outputs contain rich detailed edges, in which the tubular regions of hookworms can be passed as deep as possible to the top of the networks. On the other hand, the edges of high-level side-outputs are less, which is suitable for the high-level fusion. To combine the tubular region information, two-level fusion from low-level and high-level is adopted in this work, which corresponds to the shallow and deep parts of the classification network, respectively.</p><p>The architecture details of the hookworm classification network is illustrated in the bottom part of Fig. <ref type="figure">2</ref> and listed in Table <ref type="table">I</ref>. It consists of nine Inception models with multi-scale filters to capture the spatial correlations, topped by an average pooling to reduce the parameters, and a fully-connected layer to boost the discriminative capability. To integrate the tubular regions, two edge pooling layers (L2 and H2 in Table <ref type="table">I</ref>) are embedded into the shallow and deep parts of classification network, respectively. Max pooling layers are applied to down sample the tubular region maps to perform the edge pooling operation, so that the feature maps of classification network and tubular region maps have the same spatial dimensions for further processing. On top of the network, softmax layer combined with a fully-connected (FC) layer is treated as the classifier to detect the hookworms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Two-level Edge Pooling</head><p>A two-level edge pooling is novelly proposed to integrate tubular region information into the shallow and deep parts of the classification network, respectively. The tubular region maps produced by filtering the side-outputs induced from edge extraction network, and feature maps produced from classification network are two inputs of the edge pooling layer, which is illustrated in Fig. <ref type="figure">2</ref>.</p><p>The edge pooling layer takes small rectangular blocks from the convolutional layer and sub-samples them to produce a single output. A weighting scheme is adopted to emphasize the feature maps corresponding to the tubular regions. The max pooling takes the maximum values of blocks to obtain the strongest response and passes them to next layer, leading to improved generalization performance and faster convergence rate by selecting superior invariant features. Feature maps corresponding to tubular regions will be enhanced, which are potential areas of hookworm bodies. It enhances the robustness of classification and boosts the performance. The edge pooling integrates the feature maps induced from classification network and tubular regions detected from edge maps. The output of edge pooling o i,j,k for position (i, j) of the kth channel is defined as follows, from which max pooling takes the maximum value,</p><formula xml:id="formula_7">o i,j,k =    max S {I k • ((1 + σ) • P c )} I k ∈ low-level max S {I k • ((1 + W E ) • P c )} I k ∈ high-level<label>(6)</label></formula><p>where I k is the feature map, P c is the c-th channel of tubular region map, S is the sliding window, and • represents the product of corresponding positions of two matrices. For lowlevel fusion, a parameter σ is used to control the weight of the tubular region map, which is set to 0.3. For high-level fusion, instead of using a scalar weight, a matrix W E is adopted to adjust the fusion weights, avoiding the noise information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Regularized Edge Pooling</head><p>In this subsection, we will introduce the details of regularized edge pooling used for high-level fusion, which is illustrated in Fig. <ref type="figure" target="#fig_3">5</ref>. A weight matrix is introduced to select the correct tubular regions for edge pooling, which is constrained by a regular norm to control the edge pooling. This kind of edge pooling with weight matrix is defined as regularized edge pooling. To obtain the sparsity, the objective function of network training is formulated as follows:</p><formula xml:id="formula_8">L = arg min Ω(X) i -y i + λ 1 W 2 + λ 2 W E 1 (7)</formula><p>1057-7149 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. Traditionally, gradient descent is used to solve the objective function. Unfortunately, simply using the gradient descent is not optimal due to non-smooth function, because it is not differentiable in the zero point. To solve the problem of non-smooth objective function, the popular proximal gradient descent (PGD) <ref type="bibr" target="#b34">[35]</ref> is adopted in this paper, which is a generalized form of projection used to solve non-differentiable convex optimization problems. It finds a data point near the non-differentiable point and computes its gradient to avoid the non-differentiable point.</p><p>According to the definition of PGD, the objective function can be split into two parts, a smooth function p and a nonsmooth function q, which are represented as follows:</p><formula xml:id="formula_9">p = Ω(X) i -y i + λ 1 W 2 (8) q = λ 2 W E 1<label>(9)</label></formula><p>The smooth function p can be solved by stochastic gradient descent (SGD) <ref type="bibr" target="#b35">[36]</ref> to obtain the weights W :</p><formula xml:id="formula_10">W l = W l -ηG l (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>where W l ∈ W donates the weight of l-th layers, and G l is the gradient of W l . The non-smooth function q is solved with proximal gradient descent to obtain the parameter W E .</p><p>The update of the i-th iteration is formulated as:</p><formula xml:id="formula_12">W E = P rox q (W (i) E -p(W E ) (i) )<label>(11)</label></formula><p>where p(W E ) i donates the gradient of W E in the i-th iteration. P rox q is the proximal mapping. According to the PGD method, W E can be solved by the shrinkage operator:</p><formula xml:id="formula_13">P rox q (W E ) =      W E -λ 2 W E ≥ λ 2 0 |W E | &lt; λ 2 W E + λ 2 W E ≤ -λ 2 (12)</formula><p>The edge extraction network performs the forward operation only. The parameters of classification network are optimized by the forward and backward iterations. The weights W E in regularized edge pooling are solved by Eqn. <ref type="bibr" target="#b11">(12)</ref>, while the parameters for other layers in the network are solved by Eqn. <ref type="bibr" target="#b9">(10)</ref>. Once the whole network is trained, we can use Ω(X) to do prediction. The output y is the prediction result:</p><formula xml:id="formula_14">ŷ = Ω(X)<label>(13)</label></formula><p>V. EXPERIMENTS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>To evaluate the performance of the proposed framework, we use the same dataset collected in <ref type="bibr" target="#b10">[11]</ref>, which is one of the largest datasets for automatic disease detection for WCE images. It consists of 440K WCE images of 11 patients aged 14 ∼ 74 years old, which are collected from the West China Hospital. On average, each patient has around 40K images. Each WCE image is a 24 bit RGB color image with the resolution of 256 × 256 pixels. Among the eleven patients, the eighth and ninth patients are heavily infected. The fifth patient has only 12, 850 non-hookworm images, because the WCE stayed in his stomach about four hours and cannot enter into the duodenum. The detailed information of the dataset is listed in Table <ref type="table" target="#tab_2">III</ref>. Unlike other works that only select a few hundreds to thousands of WCE images for evaluation, in this work, the whole dataset is used.</p><p>In order to be consistent with the realistic condition and to evaluate the performance of different approaches applied to unseen patients, the experiments are carried out in 11-fold leave-one-patient-out cross-validation. Each time, the images of an "unseen" patient is treated as the testing set and the data of remaining ten patients are used as the training set. After eleven rounds of experiments, the results are averaged as the final performance score. To evaluate the performance, two experienced endoscopists examine and label all images as the ground truth. The images containing hookworms are labeled as positive samples, and negative otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Criteria</head><p>To evaluate the performance of classification, Sensitivity (SE), Specificity (SP) and Accuracy (AC) are adopted as the performance metric, which are widely used in medical image classification tasks (e.g., <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>). They are defined as follows, Accuracy = (T P + T N )/N Sensitivity = T P/(T P + F N ) Specif icity = T N/(T N + F P ) <ref type="bibr" target="#b13">(14)</ref> where T P is the number of correctly classified images containing hookworms. F N is the number of hookworm images that are falsely classified as non-hookworm images. F P is the number of non-hookworm images that are falsely classified as hookworms. T N is the number of non-hookworm images that are correctly labeled as non-hookworms, and N is the total number of images.</p><p>Ideally, we expect both sensitivity and specificity to be as high as possible. High sensitivity means strong capability of detecting hookworm images. Due to the fact that an overwhelming majority of WCE images are non-hookworm, the detected "hookworm" images have to be manually inspected by a doctor. Thus, the specificity should be high to minimize the workload of the doctor. Since accuracy reflects sensitivity and specificity in relation to each other, it is also used to assess the overall performance of classification. In addition, we also evaluate the number of patients not been correctly detected, denoted as "Missed", which means none of the hookworm images of a patient are correctly detected. It is the worst condition for hookworm detection. Overall, sensitivity is more important than specificity and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Training 1) Model Training:</head><p>Our approach is implemented using caffe <ref type="bibr" target="#b38">[39]</ref>. The approach is trained in two stages: edge extraction network is first trained and then the whole deep hookworm classification network is then jointly trained, in which the weights of edge extraction network are loaded by trained model. Edge extraction network is trained using BSDS500 dataset <ref type="bibr" target="#b39">[40]</ref>, in which the parameters are set the same as <ref type="bibr" target="#b13">[14]</ref>. It performs the forward operation only to extract the edge maps. For hookworm classification network, the step policy is used in training. Since the distribution of the WCE images is totally different from general images, and there are no other WCE image datasets available for pre-training, we train the proposed model from scratch instead of using transfer learning. For fair comparison, all methods are also trained from scratch. The computation of DHDF model is about 17 × 10 9 FLOPs. The experiments are conducted on an Intel Xeon machine with four Tesla K20m graphics cards. It takes around 10 hours to train each model. In the testing time, it is about 0.05 second to test a WCE image.</p><p>2) Parameter Setting: The learning rate is set to 1 × 10 -3 , gamma to 0.8, step size to 4000, momentum to 0.9, and the batch size is fixed to 20 samples. The total number of training epochs is set to 20. According to general CNN setting in caffe toolkit, the weight decay parameter λ 1 is set to 5 × 10 -4 . The hyper parameter of regularized edge pooling λ 2 is set to 0.001 based on the experiments. Another critical parameter is σ, which controls the degree of regularized edge pooling. We have conducted several experiments to evaluate the parameter σ, which is set to 0.3. When σ is larger than 0.5, the whole network is hard to train, due to the noise introduced by the tubular region map. When σ is less than 0.1, we find that the performance is very close to the original network (GoogLeNet).</p><p>3) Data Augmentation: At the beginning of this study, we find that the performance without data augmentation is pretty poor, since the total number of the WCE images having hookworms is small, only around 4K. The patterns cannot be well captured by the model. Meanwhile, since the sharpness of WCE images changes greatly, smoothing with different sizes of kernels is the most useful operation for the model to learn different sharpness of the WCE images. To achieve better generalization ability, the training data are augmented by online augmentation method. Four families of transformations, that is, crop, flip, rotation and smooth, are used in the training procedure. Each input image of 256 × 256 is cropped into 227 × 227. These cropped images are flipped with 50% possibility, rotated between 0 ∼ 15 degrees, and randomly smoothed by Gaussian filter with 5 × 5 or 7 × 7 kernels. With data augmentation, the performance is significantly improved.</p><p>The distribution of training data has a significant impact on the performance of CNN. As suggested by earlier research <ref type="bibr" target="#b40">[41]</ref>, the balanced distribution yields the best performance. Unfortunately, the data distribution of WCE images is extremely imbalanced, in which the number of pathology samples is usually small, while there are a large number of normal images. To handle highly imbalanced WCE image data, the positive samples of training data are 5 times over-sampled. Meanwhile, simple random sampling is performed for negative data. The numbers of positive and negative training samples are both around 20k ∼ 30k. For the testing dataset, no over-sampling or random sampling are performed. All testing images are detected one by one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance Comparison on UTR Data</head><p>The discrimination of features determines the performance of the classification. In this subsection, we will compare the performance of our previous work based on hand-crafted features <ref type="bibr" target="#b10">[11]</ref> and the deep features extracted from two popular CNNs (AlexNet <ref type="bibr" target="#b18">[19]</ref> and GoogLeNet <ref type="bibr" target="#b11">[12]</ref>) on the UTR data. Since the hookworms attached on mucosa demonstrate different shapes, inconsistent lengths and diverse bending orientations, the detected tubular regions are usually curves with irregular shapes. In order to facilitate the feature extraction and pattern learning, Uncurled Tubular Region (UTR) is proposed in <ref type="bibr" target="#b10">[11]</ref>, by expanding and stretching the curved regions into flat and regular regions, so that all tubular regions with irregular shapes can be represented in the consistent features. The central part in UTR corresponds to the detected parallel regions, while both sides around it are the surrounding environment, such as mucosa. First, we will verify whether deep features are suitable for the hookworm detection task.</p><p>The UTR classification results associated with the total number of UTR for hookworm and non-hookworm images are listed in Table <ref type="table" target="#tab_2">II</ref>. The SVM and Rusboost methods are based on the hand-crafted features. Once the parallel regions in WCE images are detected and normalized to UTR, and the histograms of average intensity (HAI) are extracted. SVM classifier is adopted to distinguish hookworms. To deal with the classification problem on skewed data, Rusboost combines random under-sampling (RUS) with Adaboost, in which RUS removes examples stochastically from the majority class until the desired balance is achieved. CNN features have shown exciting performance in general computer vision tasks, compared to traditional hand-crafted features. AlexNet and GoogLeNet are the state-of-the-art classification CNN architectures, which are deployed to act as the baselines.</p><p>In our previous work, since the number of negative images is large, we sample the number of negative images 10 times The experiments show that CNN demonstrates superior performance on UTR classification, even though the data distribution is highly imbalanced. The discrimination of CNN features outperforms the hand-crafted features, which is the reason why CNN is adopted for hookworm classification. By simply over-sampling the hookworm images, CNN achieves good performance on this imbalanced dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Performance of Two-level Edge Pooling</head><p>First, we investigate the significance of two-level edge pooling for fusing tubular regions in automatic hookworm detection. The detailed comparison according to patients is listed in Table <ref type="table" target="#tab_2">III</ref>. These experiments are conducted on the WCE images instead of UTR regions. The overall sensitivity and specificity on image level of the proposed DHDF (AV G I ) are 84.6% and 88.6%, respectively. Most of the hookworm images are correctly detected. Only 15% hookworm images and 11% non-hookworm image are falsely detected. The similar sensitivity and specificity show that the approach can well handle the imbalanced data distribution.</p><p>Regularized edge pooling is proposed as the high-level edge pooling, to select the tubular regions and avoid noises from the tubular maps. We verify its effect (DHDF) compared to traditional edge pooling without a weight matrix (DHDF-LH) in Table <ref type="table" target="#tab_2">III</ref>. The overall performance is improved compared to two-level edge pooling. The overall sensitivity and specificity are both improved. Seven patients achieve higher performance, and the rest remains the similar performance. Patients 8 and 9 are heavily infected by the hookworm, which take up a large portion of the hookworm images. There are always more than 3 hookworms in a WCE image, from which it is easier to detect the hookworms. The patterns infected by hookworms are easier to be learned by the model than other patients. However, other patients have only small hookworms and most of the images only contain one small hookworm. The sensitivity of patients 2, 3, 4 and 11 is relatively low. This is because the hookworms of these patients are small and the background of the WCE images is pretty complex. Lots of folders and bubbles demonstrate tubular-like regions after tubular region detection, so that the weight matrix will select most of tubular regions from folders or bubbles instead of hookworms, causing poor performance. Moreover, part of the hookworms demonstrate high degree of transparency, so that the boundaries of hookworms are very blurred, even we adopt the edge pooling to enhance them. Therefore, in the training stage, the DHDF model cannot capture all the patterns of hookworms. From Table <ref type="table" target="#tab_2">III</ref>, we can notice that the results of sensitivity per patient vary a lot. The performance of patients having small positive images (e.g., patients 1, 6 and 10) is not satisfactory. On the contrary, the patients with a large number of positive samples (e.g., patients 8 and 9) have pretty high performance. Since the dataset is pretty unbalanced, the average and standard deviation of the performance at the patient level is also listed in Table <ref type="table" target="#tab_2">III</ref>, denoted as AV G P and ST D P , respectively. Generally, the overall performance of per patient is not good. Due to the limited patients in the dataset, the training dataset cannot comprehensively cover all hookworm patterns. Both DHDF and DHDF LH cannot correctly detect the hookworms, when they are very different from the training samples, leading to inconsistent detection results. This issue is worth further exploration. Fortunately, our method does not miss any patient infected by hookworms, outperforming most of other methods, which is the critical point for clinical application.</p><p>To better understand the contribution of low-level and highlevel fusion, the performance comparison of the proposed method (DHDF) with different fusion components is listed in Table <ref type="table" target="#tab_5">IV</ref>. Due to the space limitation, only the average results are listed. In addition, the missed patient ID is also added, denoted as "Missed", which means that the method cannot correctly detect any hookworm images of this patient. In such case, this patient belongs to "missed" detection. D-HDF L, DHDF H, and DHDF LH represent deep hookworm classification framework adopting only low-level edge pooling, only high-level edge pooling, and both low and high two-level fusion in classification network, respectively. DHDF consists of both low-level edge pooling and high-level regularized edge pooling.</p><p>The adoption of edge pooling (DHDF L and DHDF H) improves the performance, indicating that the tubular information fused by edge pooling is an effective way for hookworm detection. The feature maps corresponding to tubular regions of hookworms are enhanced by the edge pooling, which boost the feature representation for classification. However, the high-level edge pooling induces lots of noisy information, reducing the specificity. When both low-level and high-level edge pooling layers are integrated, DHDF LH improves the performance. As regularized edge pooling is involved, a sparse weight matrix is adopted in the high-level edge pooling of DHDF, which reduces the influence of noises. All performance metrics of DHDF are boosted. We notice that accuracy and specificity of DHDF L are higher than DHDF, but the key factor, i.e., the sensitivity is lower than DHDF. For medical detection, sensitivity is more important than specificity and accuracy, since high sensitivity means strong capability of detecting hookworm images. Therefore, DHDF outperforms DHDF L.</p><p>In addition, the second and third side-outputs are adopted as the inputs of tubular region detection in this work. We also compare the performance when side-outputs of the fourth and fifth levels are included, which are listed in Table <ref type="table" target="#tab_5">IV</ref>. DHDF 4F and DHDF 5F are DHDF architecture with sideouput 4 and side-ouput 5 fused with fusion-output, respectively. They have more or less the similar performance as DHDF LH. Since the high-level side-outputs lost a large amount of detailed edges of hookworms, most of the noisy edges are enhanced by the noise, leading to the decreased sensitivity and specificity compared with DHDF.</p><p>To verify the effectiveness of the proposed DHDF framework, ROC experiments are conducted, which are illustrated in Fig. <ref type="figure" target="#fig_4">6</ref>. The ROC curve is created by plotting the true rate against the false positive rate at various threshold settings. A larger area under the curve (AUC) represents better performance. The ROC-AUC is an effective evaluate metric for the binary classification, in which a larger area means a better performance. The ROC-AUC results are also listed in Table <ref type="table" target="#tab_5">V</ref>. From Fig. <ref type="figure" target="#fig_4">6</ref>, we can easily find that the AUC of DHDF framework is the largest compared with other methods. Since DHDF framework captures both the high and low-level tubular information. Although GoogLeNet achieves better accuracy than our DHDF model, the ROC-AUC of our approach outperforms it, which is listed in Table <ref type="table" target="#tab_5">V</ref>. It proves that the capability of DHDF is better than GoogLeNet. DHDF L and DHDF H have the similar performance. It shows that the fusion of the tubular regions and CNN is helpful for the final performance. The AUC of AlexNet is the smallest, because it is with the smallest architecture. It does not have enough capability to model the complex hookworm detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Performance Comparison with State-of-the-art Approaches</head><p>In this section, we will compare the performance of the proposed method with the state-of-the-art approaches in WCE abnormality detection and image classification. The comparison results are listed in Table <ref type="table" target="#tab_5">V</ref>. The first four methods are traditional methods based on hand-crafted features, while the last three ones are deep learning based approaches.</p><p>1) Hand-crafted feature based methods: Due to limited works on hookworm detection, we compare the proposed work  <ref type="bibr" target="#b10">11]</ref> is the state-of-the-art approach for hookworm detection. The multi-scale dual matched filter is first applied to detect the location of tubular structure. Piecewise parallel region detection method is then proposed to identify the potential regions having hookworm bodies. Histogram of average intensity (HAI) is proposed to represent their properties. Rusboost is deployed to classify WCE images to deal with the problem of imbalanced data.</p><p>2) CNN based methods: To evaluate the performance of the proposed method, we compare it with two well known CNN implementations, AlexNet <ref type="bibr" target="#b18">[19]</ref> and GoogLeNet <ref type="bibr" target="#b11">[12]</ref>. AlexNet <ref type="bibr" target="#b18">[19]</ref> achieves the state-of-the-art performance on various applications, which consists of five convolutional layers and three fully-connected layers, topped with a softmax layer for multi-class classification. GoogLeNet <ref type="bibr" target="#b11">[12]</ref> is much deeper than AlexNet, which consists of nine Inception models with multi-scale filters to capture different-scale spatial correlation, topped by an average pooling and a fully-connected layer to boost the discriminative capability.</p><p>From Table <ref type="table" target="#tab_5">V</ref>, we can see that SVM-HSV-CT and MLP-HCG methods have similar performance. Both the sensitivity and specificity are below 60%, and KNN-HSV-CT achieves better specificity but slightly worse sensitivity. For these traditional methods, hand-crafted features and classic classifiers cannot accurately capture the properties of hookworms, under the complex situation and diverse appearance of WCE images. Among them, HAI+Rusboost achieves the best performance for hookworm detection, which outperforms other traditional approaches based on hand-crafted features, since HAI features are more discriminative and Rusboost classifier can deal with the imbalanced data distribution better. CNN-based methods generally outperform the state-of-the-art approach HAI+Rusboost with an average 10% ∼ 20% improvement in terms of accuracy and specificity. AlexNet achieves extremely high specificity and accuracy. Unfortunately, the sensitivity is pretty poor. Moreover, five patients are missed. It is obviously observed that basic AlexNet does not work well for hookworm detection. Due to complex backgrounds, scale variances and perspective changes, lots of hookworm patterns are omitted by the single size filter kernels of AlexNet. AlexNet with 8 layers is not deep enough to model the complex appearance patterns of hookworms, leading to the poor sensitivity, in which more than half of the hookworms cannot be correctly detected. Meanwhile, the sensitivity of GoogLeNet is almost the same as the state-of-the-art HAI+Rusboost. However, the specificity and accuracy are pretty high compared to previous works. Because of the adoption of Inception model in GoogLeNet, the appearance patterns of hookworms can be well captured by multi-scale filters of Inception model. In addition, GoogLeNet with more than 20 layers offers powerful discrimination for hookworm detection. However, it is a pity that GoogLeNet fails to detect the first patient. Different from general image classification tasks, the hookworms in WCE images are usually not conspicuous as the objects in general images. The background has a substantial impact on the final performance. The vagueness of hookworms and the low contrast of WCE images make it difficult to discern some hookworms. The ROC-AUC results are also listed in Table <ref type="table" target="#tab_5">V</ref>. Because the compared methods only report the results of AC, SE and SP, and they did not release the source codes, their ROC-AUC results cannot be provided.</p><p>With the combination of two-level edge pooling and regularized edge pooling, the proposed framework outperforms baseline methods in terms of sensitivity, the most critical metric to evaluate the capability of hookworm detection. Our approach achieves 84.6% sensitivity, which is nearly 10% higher than GoogLeNet. More importantly, all patients are correctly detected. Due to the noisy information induced in edge pooling and regularized edge pooling, the accuracy and specificity of the proposed method are slightly worse than GoogLeNet. Compared with HAI+Rusboost, our method achieves around 10% ∼ 15% improvement in terms of sensitivity, accuracy and specificity. The edge maps are filtered by MDMF, so that the tubular-like edges are enhanced. The visual patterns of hookworms can be enhanced by the edge pooling, which fuses the feature maps with the edge maps. Moreover, the powerful abstract ability of deep CNNs from Inception model also boosts the feature discrimination, improving the final performance. Overall, the proposed deep hookworm classification framework achieves a balanced performance for all metrics, demonstrating that our approach is more robust than GoogLeNet for hookworm detection. The 88.5% accuracy with comparable sensitivity and specificity makes the proposed method clinically practical, which can be used in a real condition to assist endoscopists. Some success and failure examples are illustrated in the first two rows and the last row of Fig. <ref type="figure" target="#fig_5">7</ref>, respectively. The hookworm areas in the last row are labeled by yellow bounding boxes. The color of the first image is different from other WCE images, leading to inconsistent visual patterns compared to other hookworms. In addition, the hookworm is very close to the boundary of the image, which will be partially cut with the center crop operation. The remaining hookworm only contains very limited parts, causing false detection. The hookworm in the second image is transparent, so that the hookworm is very similar to the surrounding mucosa. The appearance pattern is hard to capture for the classification network. The edge response of the hookworm is relatively weak, in which edges of the hookworm cannot be effectively extracted by the edge extraction network. Due to the existence of a big bubble in the third image, it reflects the light, forming some shining regions along the bubble edges. Meanwhile, a slim hookworm with bright body is located near the bubble edge, which looks pretty similar to the bubble edge. It is ignored by the classification network. For the fourth image, the hookworm is covered exactly on top of the intestine fold with the similar color and direction, so that it is falsely treated as a fold by our method. From the aforementioned discussion, we can easily observe that hookworm detection is challenging due to poor quality of images, presence of extraneous matters, complex structure of GI, and diverse appearances in color and texture. It is even hard for a person without any training or domain knowledge to find out some hookworm images correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, a novel deep hookworm detection framework is proposed for WCE images to model the visual appearance and tubular regions of hookworms, which contains an edge extraction network and a hookworm classification network. To integrate the two networks, two-level edge pooling is embedded into the shallow and deep parts of hookworm classification network, respectively. Experiments on a large dataset demonstrate that the proposed framework outperforms state-of-the-art methods with hand-drafted features as well as the off-the-shelf deep learning approaches.</p><p>Since the proposed method detects the hookworms based on the whole images in this work, the influence of background cannot be completely avoided, even though the noisy edge maps have been filtered by MDMF and regularized edge pooling. To further improve the robustness and effectiveness of the method, we will explore the region based detection framework in our future work, which integrates the target location and pixel-level region as the supervised information to construct an end-to-end model. In addition, we plan to integrate temporal and spatial relationship between consecutive images to further boost the performance, and exploit advanced deep learning methods to detect other lesions for WCE images.</p><p>Ramesh Jain is a researcher, educator, and entrepreneur. He is the first Bren Professor in Information and Computer Sciences at University of California, Irvine. Before this he was Farmer Distinguished Chair and Georgia Research Alliance Eminent Scholar in School of Electrical and Computer Engineering and College of Computing at Georgia Institute of Technology. Ramesh is a worldrenowned pioneer in multimedia information systems, image databases, machine vision, and intelligent systems. While professor of computer science and engineering at the University of Michigan, Ann Arbor and the University of California, San Diego, he founded and directed artificial intelligence and multimedia information systems labs. He was also the founding Editor-in-Chief of IEEE Multimedia magazine and serves on the editorial boards of several magazines in multimedia, information retrieval, business and and vision processing. He has co-authored more than 300 research papers in well-respected journals and conference proceedings. He has co-authored and co-edited several books, including Machine Vision, a textbook used at several universities. Ramesh is a Fellow of ACM, IEEE, AAAI, SPIE, and ICPR. He is the recipient of several awards including the ACM SIGMM Technical Achievement Award 2010.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. An overview of the proposed deep hookworm detection framework (DHDF), which consists of two CNN networks (i.e., edge extraction network and hookworm classification network) to extract the edge features and classify the hookworms, respectively. Two edge pooling layers are embedded into the shallow and deep parts of hookworm classification network, respectively, to enhance the feature maps corresponding to tubular regions. The thin arrows represent the concatenation of feature maps and tubular region maps. The horizontal arrows denote the convolutional operation, which produce the next feature maps.</figDesc><graphic coords="4,58.54,277.20,235.39,173.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) where β = |Y -|/|Y | and 1 -β = |Y + |/|Y |. |Y -| and |Y + | donate the edge and non-edge ground truth label sets, respectively. P r(y j = 1|X; W, w</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The 2th∼5th side-outputs produced by HED network and the fusion output.</figDesc><graphic coords="5,321.69,145.85,70.32,70.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Details of the regularized edge pooling. The weight matrix constrained by L 1 norm is induced to select the tubular regions to enhance.It consists of three components, softmax loss of the objective function, L 2 regularizer λ 1 W 2 and non-smooth regularizer λ 2 W E 1 , where λ 1 and λ 2 are hyper parameters that control the generalization ability and sparsity, respectively. Softmax loss of the objective function measures the discrepancy between the output Ω(X) i and the ground-truth label y i . Ω(X) is the mapping from inputs to outputs in CNN network. L 2 normbased regularizer is used to prevent overfitting and achieves stronger generalization ability. W E donates the weight of regularized edge pooling, and W refers to the weights in other layers. Different from traditional loss functions, addition L 1 norm is used to make W E sparse, which is the key for tubular region selection.Traditionally, gradient descent is used to solve the objective function. Unfortunately, simply using the gradient descent is not optimal due to non-smooth function, because it is not differentiable in the zero point. To solve the problem of non-smooth objective function, the popular proximal gradient descent (PGD)<ref type="bibr" target="#b34">[35]</ref> is adopted in this paper, which is a generalized form of projection used to solve non-differentiable convex optimization problems. It finds a data point near the non-differentiable point and computes its gradient to avoid the non-differentiable point.According to the definition of PGD, the objective function can be split into two parts, a smooth function p and a nonsmooth function q, which are represented as follows:</figDesc><graphic coords="7,50.52,56.73,251.42,87.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. ROC curves of DHDF, DHDF H, DHDF L, AlexNet and GoogLeNet.</figDesc><graphic coords="10,315.57,-27.08,259.20,194.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Some correctly and failed detected examples are illustrated in the first two rows and the third row. For convenience, hookworm areas are labeled by gray and yellow bounding boxes, respectively.</figDesc><graphic coords="12,115.00,174.49,60.78,56.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>The proposed hookworm detection framework seamlessly integrates two CNN networks, edge extraction network and hookworm classification network, to simultaneously model visual appearances and tubular patterns of hookworms. These two networks are combined to avoid the edge feature caching and speed up the classification.</figDesc><table><row><cell>sensitivity and specificity makes the proposed method</cell></row><row><cell>clinically practical.</cell></row><row><cell>This paper is organized as follows. Section II gives a brief</cell></row><row><cell>overview of related works. Sections III and IV elaborate</cell></row><row><cell>edge extraction network and hookworm classification network,</cell></row><row><cell>respectively. Section V describes the experimental setup and</cell></row><row><cell>discusses the results. Finally, this paper is concluded with a</cell></row><row><cell>summary.</cell></row></table><note><p><p>• More specifically, two novel edge pooling layers are proposed to integrate the tubular regions obtained from edge extraction network and the feature maps from hookworm classification network, to enhance the feature maps corresponding to tubular regions.</p>• Experiments have been conducted on one of the largest WCE datasets with 440K images, which demonstrate that the proposed approach outperforms the state-of-theart approaches. The 88.5% accuracy with comparable</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>COMPARISON OF HAND-CRAFTED FEATURES AND DEEP FEATURES FOR UTR DATA of positives for SVM and CNN training. Based on our observation, the performance of SVM classification becomes worse as a larger number of negative images are included. From TableII, we can see that SVM achieves the highest specificity and accuracy, but lowest performance on sensitivity. It is easy to find that the performance of SVM is poor when facing imbalanced data. The overall performance of our previous method based on Rusboost method<ref type="bibr" target="#b10">[11]</ref> is much higher than SVM. For Rusboost, all samples are fed into the classifier and Rusboost will sample training data automatically. After the sampling procedure, the numbers of training data for positives and negatives are almost similar. AlexNet<ref type="bibr" target="#b18">[19]</ref> has better performance than Rusboost. It has around 5% improvement in terms of specificity and accuracy meanwhile slightly worse in sensitivity compared to Rusboost. Beneficial from the deeper and wider network architecture, GoogLeNet outperforms AlexNet in all aspects. It is a significant improvement compared to Rusboost.</figDesc><table><row><cell>Patient</cell><cell></cell><cell># UTR</cell><cell></cell><cell>SVM</cell><cell></cell><cell></cell><cell>Rusboost</cell><cell></cell><cell></cell><cell>AlexNet</cell><cell></cell><cell></cell><cell>GoogLeNet</cell><cell></cell></row><row><cell>ID</cell><cell>HW</cell><cell>Non-HW</cell><cell>AC</cell><cell>SE</cell><cell>SP</cell><cell>AC</cell><cell>SE</cell><cell>SP</cell><cell>AC</cell><cell>SE</cell><cell>SP</cell><cell>AC</cell><cell>SE</cell><cell>SP</cell></row><row><cell>1</cell><cell>69</cell><cell>153,266</cell><cell>99.1</cell><cell>4.4</cell><cell cols="2">99.1 86.4</cell><cell cols="3">37.7 86.4 99.9</cell><cell>8.7</cell><cell cols="3">99.9 99.9 18.8</cell><cell>99.9</cell></row><row><cell>2</cell><cell>53</cell><cell>170,423</cell><cell cols="3">99.4 17.0 99.5</cell><cell cols="2">90.7 79.3</cell><cell>90.7</cell><cell>90.7</cell><cell>34.0</cell><cell>90.7</cell><cell>94.0</cell><cell>83.0</cell><cell>94.0</cell></row><row><cell>3</cell><cell>16</cell><cell>212,292</cell><cell>98.6</cell><cell>6.3</cell><cell cols="2">98.7 83.1</cell><cell cols="3">56.3 83.1 99.7</cell><cell cols="2">62.5 99.7</cell><cell cols="3">98.9 87.5 98.9</cell></row><row><cell>4</cell><cell>139</cell><cell>143,734</cell><cell>99.4</cell><cell>15.1</cell><cell>99.5</cell><cell>92.2</cell><cell>48.9</cell><cell>92.3</cell><cell>98.7</cell><cell cols="2">38.1 98.7</cell><cell cols="2">98.3 69.8</cell><cell>98.3</cell></row><row><cell>5</cell><cell>185</cell><cell>42,265</cell><cell>98.3</cell><cell>16.8</cell><cell>98.6</cell><cell cols="2">89.4 69.2</cell><cell cols="2">89.5 92.5</cell><cell cols="3">36.8 92.6 91.0</cell><cell cols="2">60.0 91.0</cell></row><row><cell>6</cell><cell>58</cell><cell>236,449</cell><cell cols="3">99.4 17.2 99.4</cell><cell cols="2">91.6 55.2</cell><cell>91.7</cell><cell>97.4</cell><cell>25.9</cell><cell>97.4</cell><cell>94.7</cell><cell>79.3</cell><cell>94.7</cell></row><row><cell>7</cell><cell>51</cell><cell>121,144</cell><cell>98.7</cell><cell>3.1</cell><cell cols="2">98.7 85.2</cell><cell cols="3">51.0 85.2 75.7</cell><cell cols="2">84.4 76.3</cell><cell>91.9</cell><cell>6.3</cell><cell>92.0</cell></row><row><cell>8</cell><cell>2,838</cell><cell>231,321</cell><cell>97.6</cell><cell>8.1</cell><cell cols="2">98.7 83.5</cell><cell cols="2">67.0 84.0</cell><cell cols="2">91.6 77.0</cell><cell>92.0</cell><cell>93.0</cell><cell>72.6</cell><cell>93.0</cell></row><row><cell>9</cell><cell>7,309</cell><cell>165,589</cell><cell>95.3</cell><cell>7.8</cell><cell cols="2">99.1 84.4</cell><cell cols="2">60.0 85.5</cell><cell cols="2">93.2 93.0</cell><cell>93.5</cell><cell>97.1</cell><cell>82.0</cell><cell>97.1</cell></row><row><cell>10</cell><cell>41</cell><cell>250,716</cell><cell>98.1</cell><cell>19.5</cell><cell>98.1</cell><cell>85.3</cell><cell>65.9</cell><cell>85.3</cell><cell>89.0</cell><cell>92.7</cell><cell>89.2</cell><cell>95.5</cell><cell>90.2</cell><cell>95.5</cell></row><row><cell>11</cell><cell>167</cell><cell>202,331</cell><cell>99.0</cell><cell>6.6</cell><cell>99.1</cell><cell>88.9</cell><cell>50.9</cell><cell>89.0</cell><cell>80.2</cell><cell>78.4</cell><cell>80.2</cell><cell>92.2</cell><cell>77.8</cell><cell>92.2</cell></row><row><cell>Average</cell><cell></cell><cell></cell><cell>98.4</cell><cell>11.1</cell><cell>99.0</cell><cell>87.3</cell><cell>58.3</cell><cell>87.5</cell><cell>91.5</cell><cell>57.4</cell><cell>91.8</cell><cell>95.0</cell><cell cols="2">66.4 95.1</cell></row><row><cell>of the number</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V PERFORMANCE</head><label>V</label><figDesc>COMPARISON WITH STATE-OF-THE-ART APPROACHES The suitable parameters c and g are obtained by grid search. 2) KNN-HSV-CT<ref type="bibr" target="#b7">[8]</ref> is similar to SVM-HSV-CT, but SVM classifier is replaced by KNN. 3) MLP-HCG<ref type="bibr" target="#b9">[10]</ref> is the first work for hookworm detection. The hybrid color gradient (HCG) is proposed, in which four useful channels and corresponding gradient maps are calculated by oriented energy filters and then decomposed by Contourlet. A group of statistical values is fed into multi-layer perception neural network (MLP) to classify WCE images. 4) HAI+Rusboost [</figDesc><table><row><cell>Methods</cell><cell>AC (%)</cell><cell>SE (%)</cell><cell>SP (%)</cell><cell>ROC-AUC</cell><cell>Missed</cell></row><row><cell>1. SVM-HSV-CT [8]</cell><cell>50.9</cell><cell>53.4</cell><cell>51.2</cell><cell>-</cell><cell>1,3,5</cell></row><row><cell>2. KNN-HSV-CT [8]</cell><cell>69.4</cell><cell>52.3</cell><cell>69.4</cell><cell>-</cell><cell>7</cell></row><row><cell>3. MLP-HCG [10]</cell><cell>53.2</cell><cell>55.0</cell><cell>53.5</cell><cell>-</cell><cell>1,7,11</cell></row><row><cell>4. HAI+Rusboost [11]</cell><cell>78.2</cell><cell>77.2</cell><cell>77.9</cell><cell>-</cell><cell>None</cell></row><row><cell>5. AlexNet [19]</cell><cell>96.0</cell><cell>48.1</cell><cell>96.6</cell><cell>0.769</cell><cell>2,3,4,5,10</cell></row><row><cell>6. GoogLeNet [12]</cell><cell>93.7</cell><cell>77.1</cell><cell>94.1</cell><cell>0.883</cell><cell>1</cell></row><row><cell>7. DHDF</cell><cell>88.5</cell><cell>84.6</cell><cell>88.6</cell><cell>0.895</cell><cell>None</cell></row><row><cell cols="5">1 CT stands for the color texture feature (DWT+LBP).</cell><cell></cell></row><row><cell cols="6">with the state-of-the-art methods used for other lesion detec-</cell></row><row><cell cols="6">tion. 1) SVM-HSV-CT [8]: Color-texture features used for</cell></row><row><cell cols="6">polyp and tumor detection are applied to hookworm detection.</cell></row><row><cell cols="6">Each color channel of a WCE image is decomposed by discrete</cell></row><row><cell cols="6">wavelet transform (DWT). The texture histograms are generat-</cell></row><row><cell cols="6">ed by local binary patterns (LBP) and then classified by SVM.</cell></row><row><cell cols="6">LIBSVM toolbox [42] with RBF and Linear kernel functions</cell></row><row><cell>are deployed.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IEEE TRANSACTIONS ON IMAGE PROCESSING. VOL. X, NO. X, JANUARY 2018</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The global burden of neglected tropical diseases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fenwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Public health</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="236" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wireless capsule endoscopy</title>
		<author>
			<persName><forename type="first">G</forename><surname>Iddan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Glukhovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Swain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">405</biblScope>
			<biblScope unit="page">417</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computer-aided bleeding detection in wce video</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="636" to="642" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bleeding frame and region detection in the wireless capsule endoscopy video</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="624" to="630" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detection of small bowel polyps and ulcers in wireless capsule endoscopy videos</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karargyris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bourbakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Engineering</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2777" to="2786" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automated polyp detection in colon capsule endoscopy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mamonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1488" to="1502" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved bag of feature for automatic polyp detection in wireless capsule endoscopy images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Q</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automation Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="529" to="535" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tumor recognition in wireless capsule endoscopy images using textural features and svm-based feature selection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Technology in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="323" to="329" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Assessment of crohn&apos;s disease lesions in wireless capsule endoscopy images</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seshamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mullin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dassopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Engineering</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="355" to="362" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic hookworm image detection for wireless capsule endoscopy using hybrid color gradient and contourlet transform</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Biomedical Engineering and Informatics</title>
		<meeting>Intl. Conf. Biomedical Engineering and Informatics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="116" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic hookworm detection in wireless capsule endoscopy images</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1741" to="1752" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE conf. Comp. Vis. and Pattern Recog</title>
		<meeting>IEEE conf. Comp. Vis. and Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intl. Conf. Comp. Vis</title>
		<meeting>IEEE Intl. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling spatialtemporal clues in a hybrid deep learning framework for video classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Annual ACM Conf. Multimedia</title>
		<meeting>of Annual ACM Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="461" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Software for enhanced video capsule endoscopy: challenges for essential progress</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Iakovidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koulaouzidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Gastroenterology &amp; Hepatology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="172" to="186" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimizing lesion detection in small-bowel capsule endoscopy: from present problems to future solutions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Koulaouzidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Iakovidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karargyris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Plevris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Review of Gastroenterology &amp; Hepatology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="217" to="235" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">WCE abnormality detection based on saliency and adaptive locality-constrained linear coding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Q</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automation Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="149" to="159" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic segmentation of mr brain images with a convolutional neural network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Moeskops</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mendrik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>De Vries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1252" to="1261" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation using convolutional neural networks in mri images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1240" to="1251" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep 3d convolutional encoder networks with shortcuts for multiscale feature integration applied to multiple sclerosis lesion segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Traboulsee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1229" to="1239" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic detection of cerebral microbleeds from MR images via 3d convolutional neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1182" to="1195" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E A</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R J</forename><surname>Snead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1196" to="1206" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Combining generative and discriminative representation learning for lung ct analysis with convolutional restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">G</forename><surname>Van Tulder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Bruijne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1262" to="1272" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Segmenting retinal blood vessels with deep neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liskowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Krawiec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2369" to="2380" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lung pattern classification for interstitial lung diseases using a deep convolutional neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Anthimopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Christodoulidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Christe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mougiakakou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1207" to="1216" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1285" to="1298" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast convolutional neural network training using selective data sampling: Application to hemorrhage detection in color fundus images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J J P</forename><surname>Van Grinsven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Hoyng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Theelen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1273" to="1284" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Classifying digestive organs in wireless capsule endoscopy images based on deep convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intl. Conf. Digital Signal Processing</title>
		<meeting>IEEE Intl. Conf. Digital Signal essing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1274" to="1278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for medical image analysis: Full training or fine tuning?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1299" to="1312" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. Artificial Intelligence and Statistics</title>
		<meeting>Intl. Conf. Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Large-Scale Machine Learning with Stochastic Gradient Descent</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Physica-Verlag HD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Intestinal motility assessment with video capsule endoscopy: automatic annotation of phasic intestinal contractions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Vilarino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spyridonos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deiorio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vitria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Azpiroz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="246" to="259" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Categorization and segmentation of intestinal content frames for wireless capsule endoscopy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Segui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Vilarino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Malagelada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Azpiroz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Radeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vitria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Technology in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1341" to="1352" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The impact of imbalanced training data for convolutional neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Masko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hensman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
