<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning long-range spatial dependencies with horizontal gated recurrent units</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Drew</forename><surname>Linsley</surname></persName>
							<email>drew_linsley@brown.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Carney Institute for Brain Science Department of Cognitive Linguistic &amp; Psychological Sciences Brown University Providence</orgName>
								<address>
									<postCode>02912</postCode>
									<region>RI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junkyung</forename><surname>Kim</surname></persName>
							<email>junkyung_kim@brown.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Carney Institute for Brain Science Department of Cognitive Linguistic &amp; Psychological Sciences Brown University Providence</orgName>
								<address>
									<postCode>02912</postCode>
									<region>RI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vijay</forename><surname>Veerabadran</surname></persName>
							<email>vijay_veerabadran@brown.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Carney Institute for Brain Science Department of Cognitive Linguistic &amp; Psychological Sciences Brown University Providence</orgName>
								<address>
									<postCode>02912</postCode>
									<region>RI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charlie</forename><surname>Windolf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Carney Institute for Brain Science Department of Cognitive Linguistic &amp; Psychological Sciences Brown University Providence</orgName>
								<address>
									<postCode>02912</postCode>
									<region>RI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Serre</surname></persName>
							<email>thomas_serre@brown.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Carney Institute for Brain Science Department of Cognitive Linguistic &amp; Psychological Sciences Brown University Providence</orgName>
								<address>
									<postCode>02912</postCode>
									<region>RI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning long-range spatial dependencies with horizontal gated recurrent units</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5DD260CC3F4DAB8050CD1BFBF8DFAB8C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Progress in deep learning has spawned great successes in many engineering applications. As a prime example, convolutional neural networks, a type of feedforward neural networks, are now approaching -and sometimes even surpassing -human accuracy on a variety of visual recognition tasks. Here, however, we show that these neural networks and their recent extensions struggle in recognition tasks where co-dependent visual features must be detected over long spatial ranges. We introduce a visual challenge, Pathfinder, and describe a novel recurrent neural network architecture called the horizontal gated recurrent unit (hGRU) to learn intrinsic horizontal connections -both within and across feature columns. We demonstrate that a single hGRU layer matches or outperforms all tested feedforward hierarchical baselines including state-of-the-art architectures with orders of magnitude more parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Consider Fig. <ref type="figure" target="#fig_0">1a</ref> which shows a sample image from a representative segmentation dataset <ref type="bibr" target="#b0">[1]</ref> (left) and the corresponding contour map produced by a state-of-the-art deep convolutional neural network (CNN) <ref type="bibr" target="#b1">[2]</ref> (right). Although this task has long been considered challenging because of the need to integrate global contextual information with inherently ambiguous local edge information, modern CNNs are capable to detect contours in natural scenes at a level that rivals that of human observers <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Now, consider Fig. <ref type="figure" target="#fig_0">1b</ref> which depicts a variant of a visual psychology task referred to as "Pathfinder" <ref type="bibr" target="#b6">[7]</ref>. Reminiscent of the everyday task of reading a subway map to plan a commute (Fig. <ref type="figure" target="#fig_0">1c</ref>), the goal in Pathfinder is to determine if two white circles in an image are connected by a path. These images are visually simple compared to natural images like the one shown in Fig. <ref type="figure" target="#fig_0">1a</ref>, and the task is indeed easy for human observers <ref type="bibr" target="#b6">[7]</ref>. Nonetheless, we will demonstrate that modern CNNs struggle to solve this task.</p><p>Why is it that a CNN can accurately detect contours in a natural scene like Fig. <ref type="figure" target="#fig_0">1a</ref> but also struggle to integrate paths in the stimuli shown in Fig. <ref type="figure" target="#fig_0">1b</ref>? In principle, the ability of CNNs to learn such long-range spatial dependencies is limited by their localized receptive fields (RFs) -hence the need to consider deeper networks because they allow the buildup of larger and more complex RFs. Here, we use a large-scale analysis of CNN performance on the Pathfinder challenge to demonstrate that simply increasing depth in feedforward networks constitutes an inefficient solution to learning the long-range spatial dependencies needed to solve the Pathfinder challenge.</p><p>An alternative solution to problems that stress long-range spatial dependencies is provided by biology. The visual cortex contains abundant horizontal connections which mediate non-linear interactions between neurons across distal regions of the visual field <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. These intrinsic connections, popularly 32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada. called "association fields", are thought to form the main substrate for mechanisms of contour grouping according to Gestalt principles, by mutually exciting colinear elements while also suppressing clutter elements that do not form extended contours <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. Such "extra-classical receptive field" mechanisms, mediated by horizontal connections, allow receptive fields to adaptively "grow" without additional processing depth. Building on previous computational neuroscience work <ref type="bibr">[e.g., 10, 16-19]</ref>, our group has recently developed a recurrent network model of classical and extra-classical receptive fields that is constrained by the anatomy and physiology of the visual cortex <ref type="bibr" target="#b19">[20]</ref>. The model was shown to account for diverse visual illusions providing computational evidence for a novel canonical circuit that is shared across visual modalities.</p><p>Here, we show how this computational neuroscience model can be turned into a modern end-toend trainable neural network module. We describe an extension of the popular gated recurrent unit (GRU) <ref type="bibr" target="#b20">[21]</ref>, which we call the horizontal GRU (hGRU). Unlike CNNs, which exhibit a sharp decrease in accuracy for increasingly long paths, we show that the hGRU is highly effective at solving the Pathfinder challenge with just one layer and a fraction of the number of parameters and training samples needed by CNNs. We further find that, when trained on natural scenes, the hGRU learns connection patterns that coarsely resemble anatomical patterns of horizontal connectivity found in the visual cortex, and exhibits a detection profile that strongly correlates with human behavior on a classic contour detection task <ref type="bibr" target="#b21">[22]</ref>.</p><p>Related work Much previous work on recurrent neural networks (RNNs) has focused on modeling sequences with learnable gates in the form of long-short term memory (LSTM) units <ref type="bibr" target="#b23">[24]</ref> or gated recurrent units (GRUs) <ref type="bibr" target="#b20">[21]</ref>. RNNs have also been extended to learning spatial dependencies in static images with broad applications <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. In this approach, images are transformed into one-dimensional sequences that are used to train an RNN. In recent years, several approaches have introduced convolutions into RNNs, using the recursive application of convolutional filters as a method for increasing the depth of processing through time on tasks like object recognition and super-resolution without additional parameters <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>. Other groups have constrained these convolutional-RNNs with insights from neuroscience and cognitive science, engineering specific patterns of connectivity between processing layers <ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>. The proposed hGRU builds on this line of biologically-inspired implementations of RNNs, adding connectivity patterns and circuit mechanisms that are typically found in computational neuroscience models of neural circuits [e.g., <ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>.</p><p>Another class of models related to our proposed approach is Conditional Random Fields (CRFs), probabilistic models aimed at explicitly capturing associations between nearby features. The connectivity implemented in CRFs is similar to the horizontal connections used in the hGRU, and has been successfully applied as a post-processing stage in visual tasks such as segmentation <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> to smooth out and increase the spatial resolution of prediction maps. Recently, such probabilistic methods have been successfully incorporated in a generative vision model shown to break text-based CAPTCHAs <ref type="bibr" target="#b38">[39]</ref>. Originally formulated as probabilistic models, CRFs can also be cast as RNNs <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Horizontal gated recurrent units (hGRUs)</head><p>Original contextual neural circuit model We begin by referencing the recurrent neural model of contextual interactions developed by Mély et al. <ref type="bibr" target="#b19">[20]</ref>. Below we adapted the model notations to a computer vision audience. Model units are indexed by their 2D positions (x, y) and feature channel k. Neural activity is governed by the following differential equations (see Supp. Material for the full treatment):</p><formula xml:id="formula_0">η Ḣ(1) xyk + 2 H (1) xyk = ξX xyk -(αH<label>(1)</label></formula><formula xml:id="formula_1">xyk + µ) C (1) xyk + τ Ḣ(2) xyk + σ 2 H (2) xyk = γC (2) xyk + .<label>(1)</label></formula><p>where C</p><p>(1)</p><formula xml:id="formula_2">xyk = (W I * H (2) ) xyk C (2) xyk = (W E * H (1) ) xyk ,</formula><p>Here, X ∈ R W×H×K is the feedforward drive (i.e., neural responses to a stimulus), H (1) ∈ R W×H×K is the recurrent circuit input, and H (2) ∈ R W×H×K the recurrent circuit output. Modeling input and output states separately allows for the implementation of a particular form of inhibition known as "shunting" (or divisive) inhibition. Unlike the excitation in the model which acts linearly on a unit's input, inhibition acts on a unit's output and hence, regulates the unit response non-linearly (i.e., given a fixed amount of inhibition and excitation, inhibition will increase with the unit's activity unlike excitation which will remained constant).</p><p>The convolutional kernels W I , W E ∈ R S×S×K×K describe inhibitory vs. excitatory hypercolumn connectivity (constrained by anatomical data<ref type="foot" target="#foot_0">1</ref> ). The scalar parameters µ and α control linear and quadratic (i.e., shunting) inhibition by C (1) ∈ R W×H×K , γ scales excitation by C (2) ∈ R W×H×K , and ξ scales the feedforward drive. Activity at each stage is linearly rectified (ReLU) [•] + = max(•, 0). Finally, η, , τ and σ are time constants. To make this model amenable to modern computer vision applications, we set out to develop a version where all parameters could be trained from data. If we let η = τ and σ = for symmetry and apply Euler's method to Eq. 1 with a time step of ∆t = η/<ref type="foot" target="#foot_1">2</ref> , then we obtain the discrete-time equations:</p><formula xml:id="formula_3">H (1) xyk [t] = -2 ξX xyk -(αH (1) xyk [t -1] + µ)C (1) xyk [t] + H (2) xyk [t] = -2 γC (2) xyk [t] + .<label>(2)</label></formula><p>Here, •[t] denotes the approximation at the t-th discrete timestep. This results in a trainable convolutional recurrent neural network (RNN) which performs Euler integration of a dynamical system similar to the neural model of <ref type="bibr" target="#b19">[20]</ref>.</p><p>hGRU formulation We build on Eq. 2 to introduce the hGRU -a model with the ability to learn complex interactions between units via horizontal connections within a single processing layer (Fig. <ref type="figure">2</ref>). The hGRU extends the derivation from Eq. 2 with three modifications that improve the training of the model with gradient descent and its expressiveness 2 . (i) We introduce learnable gates, borrowed from the gated recurrent unit (GRU) framework (see Supp. Material for the full derivation from Eq. 2). (ii) The hGRU makes the operations for computing H (2) (excitation) symmetric with those of H (1) (inhibition), providing the circuit the ability to learn how to implement linear and quadratic interactions at each of these processing stages. (iii) To control unstable gradients, the hGRU uses a squashing pointwise non-linearity and a learned parameter to globally scale activity at every processing timestep (akin to a constrained version of the recurrent batchnorm <ref type="bibr" target="#b40">[41]</ref>).</p><formula xml:id="formula_4">W σ + + Inhibition Excitation + + σ Mix Gain ζ ζ U (1) U (2) W (x, y, k) (x, y, k) H (2) [t-1] H (2) [t]</formula><p>H (1) [t] X Figure <ref type="figure">2</ref>: The hGRU circuit. The hGRU can learn highly non-linear interactions between spatially neighboring units in the feedforward drive X, which are encoded in its hidden state H (2) . This computation involves two stages, which are inspired by a recurrent neural circuit of horizontal connections <ref type="bibr" target="#b19">[20]</ref>. First, the horizontal inhibition (blue) is calculated by applying a gain to H (2) [t -1], and convolving the resulting activity with the kernel W which characterizes these interactions. Linear (+ symbol) and quadratic (× symbol) operations control the convergence of this inhibition onto X. Second, the horizontal excitation (red) is computed by convolving H (1) [t] with W . Another set of linear and quadratic operations modulate this activity, before it is mixed with the persistent hidden state H (2) [t -1]. Note that the excitation computation involves an additional "peephole" connection, not depicted here. Small solid-line squares within the hypothetical activities that the circuit operates on denote the unit indexed by 2D position (x, y) and feature channel k, whereas dotted-line squares depict the unit's receptive field (a union of both classical and extra-classical definitions) in the previous activity.</p><p>In our hGRU implementation, the feedforward drive X corresponds to activity from a preceding convolutional layer. The hGRU encodes spatial dependencies between feedforward units via its (time-varying) hidden states H (1) and H (2) . Updates to the hidden states are managed using two activities, referred to as the reset and update "gates": G (1) and G (2) . These activities are derived from convolutions, denoted by * , between the kernels U (1) , U (2) ∈ R 1×1×K×K and hidden states H (1) and H (2) , shifted by biases b (1) , b (2) ∈ R 1×1×K , respectively. The pointwise non-linearity σ is applied to each activity, normalizing them in the range [0, 1]. Because these activities are real-valued, we hereafter refer to the reset gate as the "gain", and the update gate as the "mix".</p><p>Horizontal interactions between units are calculated by the kernel W ∈ R S×S×K×K , where S describes the spatial extent of these connections in a single timestep (Fig. <ref type="figure">2</ref>; but see Supp. Material for a version with separate kernels for excitation vs. inhibition, as in Eq. 2). Consistent with computational models of neural circuits (e.g., <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>), W is constrained to have symmetric weights between channels, such that the weight W x0+∆x,y0+∆y,k1,k2 is equal to the weight W x0+∆x,y0+∆y,k2,k1 where x 0 and y 0 denote the center of the kernel. This constraint reduces the number of learnable parameters by nearly half vs. a normal convolutional kernel. Hidden states H (1) and H (2) are recomputed via horizontal interactions at every timestep t ∈ [0, T ]. We begin by describing computation of H (1) [t]:</p><formula xml:id="formula_5">G (1) [t] = σ(U (1) * H (2) [t -1] + b (1) )<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C</head><p>(1)</p><formula xml:id="formula_6">xyk [t] = (W * (G (1) [t] H (2) [t -1])) xyk<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H</head><p>(1)</p><formula xml:id="formula_7">xyk [t] = ζ(X xyk -C (1) xyk [t](α k H (2) xyk [t -1] + µ k ))<label>(5)</label></formula><p>Channels in H (2) [t -1] are first modulated by the gain<ref type="foot" target="#foot_2">3</ref> G (1) [t]. The resulting activity is convolved with W to compute C (1) [t], which is the horizontal inhibition of the hGRU at this timestep. This inhibition is applied to X via the parameters µ and α, which are k-dimensional vectors that respectively scale linear and quadratic (akin to shunting inhibition described in Eq. 1) terms of the horizontal interaction with X. The pointwise ζ is a hyperbolic tangent that squashes activity into the range [-1, 1] (but see Supp. Material for a hGRU with a rectified linearity). Importantly, in contrast to the original circuit, in this formulation the update to H (1) [t] (Eq. 5) is calculated by combining horizontal connection contributions of C (1) [t] with H (2) [t -1] rather than H (1) [t -1], which we found improved learning on the visual tasks explored here.</p><p>The updated H (1) [t] is next used to calculate H (2) [t].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G</head><p>(2)</p><formula xml:id="formula_8">xyk [t] = σ((U (2) * H (1) [t]) xyk + b (2) k )<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C</head><p>(2)</p><formula xml:id="formula_9">xyk [t] = (W * H (1) [t]) xyk<label>(7)</label></formula><p>H( <ref type="formula" target="#formula_3">2</ref>)</p><formula xml:id="formula_10">xyk [t] = ζ(κ k H (1) xyk [t] + β k C (2) xyk [t] + ω k H (1) xyk [t]C (2) xyk [t])<label>(8)</label></formula><formula xml:id="formula_11">H (2) xyk [t] = η t (H<label>(2)</label></formula><formula xml:id="formula_12">xyk [t -1](1 -G (2) xyk [t]) + H(2) xyk [t]G (2) xyk [t])<label>(9)</label></formula><p>The mix G (2) [t] is calculated by convolving U (2) [t] with H (1) [t], followed by the addition of b (2) . The activity C (2) [t] represents the excitation of horizontal connections onto the newly-computed H (1) [t]. Linear and quadratic contributions of horizontal interactions at this stage are controlled by the k-dimensional parameters κ, ω, and β. The parameters κ and ω control the linear and quadratic contributions of horizontal connections to</p><formula xml:id="formula_13">H(2) [t].</formula><p>The parameter β is a gain applied to C (2) [t], giving W an additional degree of freedom in expressing this excitation. With this full suite of interactions, the hGRU can in principle implement both a linear and a quadratic form of excitation (i.e., to assess self-similarity), each of which play specific computational roles in perception <ref type="bibr" target="#b41">[42]</ref>. Note that the inclusion of H (1) [t] in Eq. 8 functions as a "peephole" connection between it and H(2) [t]. Finally, the mix G (2) integrates the candidate H(2) t with H</p><p>(2)</p><p>t . The learnable T -dimensional parameter η, which we refer to as a time-gain, helps control unstable gradients during training. This time-gain modulates H</p><p>(2) t with the scalar, η t , which as we show in our experiments below improves model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Pathfinder challenge</head><p>We evaluated the limits of feedforward and recurrent architectures on the "Pathfinder challenge", a synthetic visual task inspired by cognitive psychology <ref type="bibr" target="#b6">[7]</ref>. The task, depicted in Fig. <ref type="figure" target="#fig_0">1b</ref>, involves detecting whether two circles are connected by a path. This is made more difficult by allowing target paths to curve and introducing multiple shorter unconnected "distractor" paths. The Pathfinder challenge involves three separate datasets, for which the length of paths and distractors are parametrically increased. This challenge therefore screens models for their effectiveness in detecting complex long-range spatial relationships in cluttered scenes.</p><p>Stimulus design Pathfinder images were generated by placing oriented "paddles" on a canvas to form dashed paths. Each image contained two paths made of a fixed number of paddles and multiple distractors made of one third as many paddles. Positive examples were generated by placing two circles at the ends of a single path (Fig. <ref type="figure" target="#fig_0">1b</ref>, left) and negative examples by placing one circle at the end of each of the paths (Fig. <ref type="figure" target="#fig_0">1b</ref>, right). The paths were curved and variably shaped, with the possible number of shapes exponential to the path length. The Pathfinder challenge consisted of three datasets, in which path and distractor length was successively increased, and with them, the overall task difficulty. These datasets had path lengths of 6, 9 and 14 paddles, and each contained 1,000,000 unique images of 150×150 pixels. See Supp. Material for a detailed description of the stimulus generation procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model implementation</head><p>We performed a large-scale analysis of the effectiveness of feedforward and recurrent computations on the Pathfinder challenge. We controlled for the effects of idiosyncratic model specifications by using a standard architecture, consisting of "input", "feature extraction", and "readout" processing stages. Swapping different feedforward or recurrent layers into the feature extraction stage let us measure the relative effectiveness of each on the challenge. All models except for state-of-the-art "residual networks" (ResNets) <ref type="bibr" target="#b42">[43]</ref> and per-pixel prediction architectures were embedded in this architecture, and these exceptions are detailed below. See Supp. Material for a detailed description of the input and readout stage. Models were trained on each Pathfinder challenge dataset (Fig. <ref type="figure" target="#fig_1">3d</ref>), with 90% of the images used for training (900,000) and the remainder for testing (100,000). We measured model performance in two ways. First, as the accuracy on test images. Second, as the "area under the learning curve" (ALC), or mean accuracy on the test set evaluated after every 1000 batches of training, which summarized the rate at which a model learned the task. Accuracy and ALC were taken from the model that achieved the highest accuracy across 5 separate runs of model training. All models were trained for two epochs except for the ResNets, which were trained for four. Model training procedures are detailed in Supp. Material.</p><p>Recurrent models We tested 6 different recurrent layers in the feature extraction stage of the standard architecture: hGRUs with 8, 6, and 4-timesteps of processing; a GRU; and hGRUs with lesions applied to parameters controlling linear or quadratic horizontal interactions. Both the GRU and lesioned versions of the hGRU ran for 8 timesteps. These layers had 15×15 horizontal connection kernels (W ) with an equal number of channels as their input layer (25 channels).</p><p>We observed 3 overarching trends: First, each model's performance monotonically decreased, or "strained", as path length increased. Increasing path length reduced model accuracy (Fig. <ref type="figure" target="#fig_1">3a</ref>), and increased the number of batches it took to learn a task (Fig. <ref type="figure" target="#fig_1">3b</ref>). Second, the 8-timestep hGRU was more effective than any other recurrent model, and it outperformed each of its lesioned variants as well as a standard GRU. Notably, this hGRU was strained the least by the Pathfinder challenge out of all tested models, with a negligible drop in accuracy as path length increased. This finding highlights the effectiveness of the hGRU for processing long-range spatial dependencies, and how the dynamics implemented by its linear and quadratic horizontal interactions are important. Third, hGRU performance monotonically decreased with processing time. This revealed a minimum number of timesteps that the hGRU needed to solve each Pathfinder dataset: 4 for the length-6 condition, 6 for the length-9 condition, and 8 for the length-14 condition (first vs. second columns in Fig. <ref type="figure" target="#fig_1">3a</ref>). Such time-dependency in the Pathfinder task is consistent with the accuracy-reaction-time tradeoff found in humans as the distance between endpoints of a curve increases <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feedforward models</head><p>We screened an array of feedforward models on the Pathfinder challenge. Model performance revealed the importance of kernel size vs. kernel width, model depth, and feedforward operations for incorporating additional scene context for solving Pathfinder. Model construction began by embedding the feature extraction stage of the standard model with kernels of one of three different sizes: 10×10, 15×15, or 20×20. These are referred to as small, medium, and large kernel models (Fig. <ref type="figure" target="#fig_1">3</ref>). To control for the effect of network capacity on performance, the number of kernels given to each model was varied so that the number of parameters in each model configuration was equal to each other and the hGRU (36, 16, and 9 kernels). We also tested two other feedforward models that featured candidate operations for incorporating contextual information into local convolutional activities. One version used (2-pixel) dilated convolutions, which involves applying a stride to the kernel before convolving the input <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>, and has been found useful for many computer vision problems <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>. The other version applied a non-local operation to convolutional activities <ref type="bibr" target="#b47">[48]</ref>, which can introduce (non-recurrent) interactions between units in a layer. These operations were incorporated into the first feature extraction layer of the medium kernel (15×15 filter) model described above. We also considered deeper versions of each of the above "1-layer" models (referring to the depth of the feature extraction stage), stacking them to build 3-and 5-layer versions. This yielded a total of 15 different feedforward models.</p><p>Without exception, the performance of each feedforward model was significantly strained by the Pathfinder challenge. The magnitude of this straining was well predicted by model depth and size, and operations for incorporating additional contextual information made no discernible difference to the overall pattern of results. The 1-layer models were most effective on the 6-length Pathfinder dataset, but were unable to do better than chance on the remaining conditions. Increasing model capacity to 3 layers rescued the performance of all but the small kernel model on the 9-length Pathfinder dataset, but even then did little to improve performance on the 14-length dataset. Of the 5-layer models, only the large kernel configuration came close to solving the 14-length dataset. The ALC of this model, however, demonstrates that its rate of learning was slow, especially compared to the hGRU (Fig. <ref type="figure" target="#fig_1">3b</ref>). The failures of these feedforward models is all the more striking when considering that each had between 1× and 10× the number of parameters as the hGRU (Fig. <ref type="figure" target="#fig_1">3c</ref>, compare the red and green markers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Path Length</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual networks</head><p>We reasoned that if the performance of feedforward models on the Pathfinder challenge is a function of model depth, then state-of-the-art networks for object recognition with many times the number of layers should easily solve the challenge. We tested this possibility by training ResNets with 18, 50, and 152 layers on the Pathfinder challenge. Each model was trained "from scratch" with standard weight initialization <ref type="bibr" target="#b48">[49]</ref>, and given additional epochs of training (4) to learn the task because of their large number of parameters. However, even with this additional training, only the deepest 152-layer ResNet was able to solve the challenge (Fig. <ref type="figure" target="#fig_1">3a</ref>). Even so, the 152-layer ResNet was less efficient at learning the 14-length dataset than the hGRU (Fig. <ref type="figure" target="#fig_1">3b</ref>), and achieved its performance with nearly 1000× as many parameters (Fig. <ref type="figure" target="#fig_1">3c</ref>; see Supp. Material for additional ResNet experiments).</p><p>Per-pixel prediction models We considered the possibility that CNN architectures for per-pixel prediction tasks, such as contour detection and segmentation, might be better suited to the Pathfinder challenge than those designed for classification. We therefore tested three representative per-pixel prediction models: the fully-convolutional network (FCN), the skip-connection U-Net, and the unpooling SegNet. These models used an encoder/decoder style architecture, which was followed by the readout processing stage of the standard architecture described above to make them suitable for Pathfinder classification. Encoders were the VGG16 <ref type="bibr" target="#b49">[50]</ref>, and each model was trained from scratch with Xavier initialized weights. Like the ResNets, these models were given 4 epochs of training to accommodate their large number of parameters.</p><p>The fully-convolutional network (FCN) architecture is one of the first successful uses of CNNs for per-pixel prediction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>. Decoders in these models use "1×1" convolutions to combine upsampled activity maps from several layers of the encoder. We created an FCN model which applied this procedure to the last layer of each of the 5 VGG16-convolution blocks. These activity maps were upsampled by learnable kernels, which were initialized with weights for bilinear interpolation. In contrast to the feedforward models discussed above, the FCN successfully learned all conditions in the Pathfinder challenge (Fig. <ref type="figure" target="#fig_1">3a</ref>, purple circle). It did so less efficiently than the hGRU, however, with a lower ALC score on the 14-length dataset and 200× as many free parameters (Fig. <ref type="figure" target="#fig_1">3b</ref>).</p><p>Another approach to per-pixel prediction uses "skip connections" to connect specific layers of a model's encoder to its decoder. This approach was first described in <ref type="bibr" target="#b43">[44]</ref> as a method for more effectively merging coarse-layer information into a model's decoder, and later extended to the U-Net <ref type="bibr" target="#b52">[53]</ref>. We implemented a version of the U-Net architecture that had a VGG16 encoder and a decoder. The decoder consisted of 5 randomly initialized and learned upsampling layers, which had additive connections to the final convolutional layer in each of the encoder's VGG16 blocks. Using standard VGG16 nomenclature to define one of these connections, this meant that "conv 4_3" activity from the encoder was added to the second upsampled activity map in the decoder. The U-Net was on par with the hGRU and the FCN at solving the Pathfinder challenge. It was also nearly as efficient as the hGRU in doing so (Fig. <ref type="figure" target="#fig_1">3b</ref>), but used over 350× as many parameters as the hGRU (Fig. <ref type="figure" target="#fig_1">3c</ref>; see Supp. Materials for additional U-Net experiments).</p><p>Unpooling models eliminate the need for feature map upsampling by routing decoded activities to the locations of the winning max-pooling units derived from the encoder. Unpooling is also a leading approach for a variety of dense per-pixel prediction tasks, including segmentation, which is exemplified by SegNet <ref type="bibr" target="#b53">[54]</ref>. We tested a SegNet on the Pathfinder challenge. This model has a decoder that mirrors its encoder, with unpooling operations replacing its pooling. The SegNet achieved high accuracy on each of the Pathfinder datasets, but was less efficient at learning them than the hGRU, with worse ALC scores across the challenge (Fig. <ref type="figure" target="#fig_1">3b</ref>). The SegNet also featured the second-most parameters of any model tested, which was 400× more than the hGRU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Explaining biological horizontal connections with the hGRU</head><p>Statistical image analysis studies have suggested that cortical patterns of horizontal connections, commonly referred to as "association fields", may reflect the geometric regularities of oriented elements present in natural scenes <ref type="bibr" target="#b54">[55]</ref>. Because the hGRU is designed to capture such spatial regularities, we investigated whether it learned patterns of horizontal connections that resemble these association fields. We visualized the horizontal kernels learned by the hGRU to solve tasks (Fig. <ref type="figure">S5</ref>). When trained on the the Pathfinder challenge, hGRU kernels resembled the dominant patterns of horizontal connectivity in visual cortex. Prominent among these patterns are (1) the antagonistic near-excitatory vs. far-inhibitory surround organization also found in the visual cortex <ref type="bibr" target="#b55">[56]</ref>; (2) the association field, with collinear excitation and orthogonal inhibition <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>; and (3) other higher-order surround computations <ref type="bibr" target="#b56">[57]</ref>. We also visualized these patterns after training the hGRU to detect contours in the naturalistic BSDS500 image dataset <ref type="bibr" target="#b0">[1]</ref>. These horizontal kernels took on similar patterns of connectivity, but with far more definition and regularity, suggesting that the hGRU learns best from natural scene statistics.</p><p>How well does the hGRU explain human psychophysics data? We tested this by recreating the synthetic contour detection dataset used in <ref type="bibr" target="#b21">[22]</ref>. This task had human participants detect a contour formed by co-linearly aligned paddles in an array of randomly oriented distractors. Multiple versions of the task were created by varying the distance between paddles in the contour (5 conditions). Contour detection accuracy of the hGRU was recorded on each of dataset for comparison with participants in <ref type="bibr" target="#b21">[22]</ref>, whose responses were digitally extracted with WebPlotDigitizer from <ref type="bibr" target="#b21">[22]</ref> and averaged (N=2). Plotting hGRU accuracy against the reported "detection score" revealed that increasing inter-paddle distance caused similar performance straining for both (Fig. <ref type="figure">S6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>The present study demonstrates that long-range spatial dependencies generally strain CNNs, with only very deep and state-of-the-art networks overcoming the visual variability introduced by long paths in the Pathfinder challenge. Although feedforward networks are generally effective at learning and detecting relatively rigid objects shown in well-defined poses, these models tend towards a brute-force solution when tasked with the recognition of less constrained structures, such as a path connecting two distant locations. This study adds to a body of work highlighting examples of routine visual tasks where CNNs fall short of human performance <ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref>.</p><p>We demonstrate a solution to the Pathfinder challenge inspired by neuroscience. The hGRU leverages computational principles of visual cortical circuits to learn complex spatial interactions between units. For the Pathfinder challenge, this translates into an ability to represent the elements forming an extended path while ignoring surrounding clutter. We find that the hGRU can reliably detect paths of any tested length or form using just a single layer. This contrasts sharply with the successful state-ofthe-art feedforward alternatives, which used much deeper architectures and orders of magnitude more parameters to achieve similar success. The key mechanisms underlying the hGRU's performance are well known in computational neuroscience <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. However, these mechanisms have been typically overlooked in computer vision (but see <ref type="bibr" target="#b38">[39]</ref> for a successful vision model using horizontal connections and shown to break text-based CAPTCHAs).</p><p>We also found that hGRU performance on the Pathfinder challenge is a function of the amount of time it was given for processing. This finding suggests that it concurrently expands the facilitative influence of one end of a target curve to the other while suppressing the influence of distractors. The performance of the hGRU on the Pathfinder challenge captures the iterative nature of computations used by our visual system during similar tasks <ref type="bibr" target="#b62">[63]</ref> -exhibiting a comparable tradeoff between performance and processing-time <ref type="bibr" target="#b6">[7]</ref>. Visual cortex is replete with association fields that are thought to underlie perceptual grouping <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b63">64]</ref>. Theoretical models suggest that patterns of horizontal connections reflect the statistics of natural scenes, and here too we find that horizontal kernels in the hGRU learned from natural scenes resemble cortical patterns of horizontal connectivity, including association fields and the paired near-excitatory / far-inhibitory surrounds that may be responsible for many contextual illusions <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b55">56]</ref>. The horizontal connections learned by the hGRU reproduce another aspect of human behavior, in which the saliency of a straight contour decreases as the distance between its paddles increases. This sheds light on a possible relationship between horizontal connections and saliency computation.</p><p>In summary, this work diagnoses a computational deficiency of feedforward networks, and introduces a biologically-inspired solution that can be easily incorporated into existing deep learning architectures. The weights and patterns of behavior learned by the hGRU appear consistent with those associated with the visual cortex, demonstrating its potential for establishing novel connections between machine learning, cognitive science, and neuroscience.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: State-of-the-art CNNs excel at detecting contours in natural scenes, but they are strained by a task that requires the detection of long-range spatial dependencies. (a) Representative contour detection performance of a leading neural network architecture [23]. (b) Exemplars from the Pathfinder challenge: a task consisting of synthetic images which are parametrically controlled for long-range dependencies. (c) Long-range dependencies similar to those in the Pathfinder challenge are critical for everyday behaviors, such as reading a subway map to navigate a city.</figDesc><graphic coords="2,393.12,-35.30,211.54,210.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The hGRU efficiently learns long-range spatial dependencies that otherwise strain feedforward architectures. (a) Model accuracy is plotted for the three Pathfinder challenge datasets, which featured paths of 6-9-and 14-paddles. Each panel depicts the accuracy of a different model class after training on for each pathfinder dataset (see Supp. Material for additional models). Only the hGRU and state-of-the-art models for classification (the two right-most panels) approached perfect accuracy on each dataset. (b) Measuring the area under the learning curve (ALC) of each model (mean accuracy) demonstrates that the rate of learning achieved by the hGRU across the Pathfinder challenge is only rivaled by the U-Net architecture (far right). (c) The hGRU is significantly more parameter-efficient than feedforward models at the Pathfinder challenge, with its closest competitors needing at least 200× the number of parameters to match its performance. The x-axis shows the number of parameters in each model versus the hGRU, as a multiple of the latter. The y-axis depicts model accuracy on the 14-length Pathfinder dataset. (d) Pathfinder challenge exemplars of different path lengths (all are positive examples).</figDesc><graphic coords="7,430.04,317.48,104.90,104.90" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>There are four separate connectivity patterns in<ref type="bibr" target="#b19">[20]</ref> to describe inhibition vs. excitation and near vs. far interactions between units. We combine these into a separate inhibitory vs. excitatory kernels to simplify notation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>These modifications involved relaxing several constraints from the original neuroscience model that are less useful for solving the tasks investigated here (see Supp. Material for performance of an hGRU with constrained inhibition and excitation.)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>GRU gate activities are often a function of a hidden state and X[t]. Because the feedforward drive here is constant w.r.t. time, we omit it from these calculations. In practice, its inclusion did not affect performance.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by NSF early career award [grant number IIS-1252951] and DARPA young faculty award [grant number YFA N66001-14-1-4037]. Additional support was provided by the Carney Institute for Brain Science and the Center for Computation and Visualization (CCV) at Brown University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeply-Supervised nets</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2015-02">February 2015</date>
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Holistically-Nested edge detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2017-12">December 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Richer convolutional features for edge detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="5872" to="5881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional oriented boundaries: From image segmentation to High-Level tasks</title>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="819" to="833" />
			<date type="published" when="2018-04">April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep crisp boundaries</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3892" to="3900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parallel and serial grouping of image elements in visual perception</title>
		<author>
			<persName><forename type="first">R</forename><surname>Houtkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Roelfsema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1443" to="1459" />
			<date type="published" when="2010-12">December 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lateral connectivity and contextual interactions in macaque primary visual cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Stettler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="739" to="750" />
			<date type="published" when="2002-11">November 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Intrinsic laminar lattice connections in primate visual cortex</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Rockland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Lund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Comparative Neurology</title>
		<imprint>
			<biblScope unit="volume">216</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="318" />
			<date type="published" when="1983-05">May 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><surname>Grossberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mingolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural dynamics of perceptual grouping: textures, boundaries, and emergent segmentations</title>
		<imprint>
			<date type="published" when="1985-08">August 1985</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="141" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Contour integration by the human visual system: Evidence for a local &quot;association field</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Hess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="193" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The role of edges and line-ends in illusory contour formation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Lesher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mingolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="2253" to="2270" />
			<date type="published" when="1993-11">November 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Contour saliency in primary visual cortex</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Piëch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="951" to="962" />
			<date type="published" when="2006-06">June 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to link visual contours</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Piëch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="442" to="451" />
			<date type="published" when="2008-02">February 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stereo, shading, and surfaces: Curvature constraints couple neural computations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="812" to="829" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The &quot;silent&quot; surround of V1 receptive fields: theory and experiments</title>
		<author>
			<persName><forename type="first">P</forename><surname>Series</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lorenceau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Frégnac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physiology-Paris</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="453" to="474" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural circuit models for computations in early visual cortex</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhaoping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Neurobiology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="808" to="815" />
			<date type="published" when="2011-10">October 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Strong recurrent networks compute the orientation tuning of surround modulation in the primate primary visual cortex</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shushruth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mangapathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Ichida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Bressloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schwabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Angelucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="308" to="321" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The stabilized supralinear network: a unifying circuit motif underlying multi-input integration in sensory cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Van Hooser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="402" to="417" />
			<date type="published" when="2015-01">January 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Complementary surrounds explain diverse contextual phenomena across visual modalities</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mély</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Linsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="769" to="784" />
			<date type="published" when="2018-10">October 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN Encoder-Decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Global contour saliency and local colinear interactions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2846" to="2856" />
			<date type="published" when="2002-11">November 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Holistically-Nested edge detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2017-12">December 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11">November 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-dimensional recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Neural Networks</title>
		<meeting>the International Conference on Artificial Neural Networks<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2874" to="2883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generative image modeling using spatial LSTMs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1927" to="1935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1747" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural network for object recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3367" to="3375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Bridging the gaps between residual learning, recurrent neural networks and visual cortex</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03640</idno>
		<imprint>
			<date type="published" when="2016-04">April 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deeply-Recursive convolutional network for image Super-Resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>eeedings of the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks: A better model of biological object recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Spoerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcclure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1551</biblScope>
			<date type="published" when="2017-09">September 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno>arxiv: 1612.09508</idno>
		<imprint>
			<date type="published" when="2016-12">December 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Feedback Networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Task-Driven convolutional recurrent models of the visual system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nayebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bear</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kubilius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sussillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L K</forename><surname>Yamins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00053</idno>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of boundary detection using deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018-04">April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs</title>
		<author>
			<persName><forename type="first">D</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lehrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lázaro-Gredilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Laan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Marthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Phoenix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">358</biblScope>
			<biblScope unit="issue">6368</biblScope>
			<date type="published" when="2017-12">December 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recurrent batch normalization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to detect natural image boundaries using local brightness, color, and texture cues</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="530" to="549" />
			<date type="published" when="2004-05">May 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision<address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-Scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dilated deep residual network for image denoising</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th IEEE International Conference on Tools with Artificial Intelligence</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1272" to="1279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Effective use of dilated convolutions for segmenting small object instances in remote sensing imagery</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nemoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Imaizumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hikosaka</surname></persName>
		</author>
		<idno>CoRR, abs/1709.00179</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Non-Local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR, abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Modeling local and global deformations in deep learning: Epitomic convolution, multiple instance learning, and sliding window detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Savalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="390" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Convolutional oriented boundaries: From image segmentation to High-Level tasks</title>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="page" from="234" to="241" />
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">SegNet: A deep convolutional Encoder-Decoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
			<date type="published" when="2017-12">December 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Geometrical computations explain projection patterns of longrange horizontal connections in visual cortex</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ben-Shahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="445" to="476" />
			<date type="published" when="2004-03">March 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Different orientation tuning of near-and far-surround suppression in macaque primary visual cortex mirrors their tuning in human perception</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shushruth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nurminen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bijanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Ichida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Angelucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="119" />
			<date type="published" when="2013-01">January 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Surround suppression of V1 neurons mediates orientation-based representation of high-order visual features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ohzawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1444" to="1462" />
			<date type="published" when="2009-03">March 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Not-So-CLEVR: learning same-different relations strains feedforward neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interface Focus</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">20180011</biblScope>
			<date type="published" when="2018-08">August 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Do deep neural networks suffer from crowding?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Volokitin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5628" to="5638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unsupervised learning by program synthesis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="973" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Incremental grouping of image elements in vision. Attention</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Roelfsema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houtkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2542" to="2572" />
			<date type="published" when="2011-11">November 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Top-down influences on visual processing</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="350" to="363" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Can recurrent neural networks warp time?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ollivier</surname></persName>
		</author>
		<idno>arxiv: 1804.11188</idno>
		<imprint>
			<date type="published" when="2018-03">March 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">PsychoPy-Psychophysics software in python</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Peirce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience Methods</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8" to="13" />
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>arxiv: 1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014-12">December 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">TensorFlow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Conference on Operating Systems Design and Implementation</title>
		<meeting>the USENIX Conference on Operating Systems Design and Implementation<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>arxiv: 1502.03167</idno>
		<imprint>
			<date type="published" when="2015-02">February 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Highway networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>arxiv: 1505.00387</idno>
		<imprint>
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
