<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Preserving Structure in Model-Free Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
							<email>lu.zhang@tudelft.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Intelligent Systems</orgName>
								<orgName type="institution">Univer-sity of Technology</orgName>
								<address>
									<addrLine>Mekelweg 4</addrLine>
									<postCode>2600 GA</postCode>
									<settlement>Delft, Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
							<email>l.j.p.vandermaaten@tudelft.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Intelligent Systems</orgName>
								<orgName type="institution">Univer-sity of Technology</orgName>
								<address>
									<addrLine>Mekelweg 4</addrLine>
									<postCode>2600 GA</postCode>
									<settlement>Delft, Delft</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Preserving Structure in Model-Free Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2A1AF1A546A2B17547D43BAD0B8A78BE</idno>
					<idno type="DOI">10.1109/TPAMI.2013.221</idno>
					<note type="submission">received 17 June 2013; revised 11 Sept. 2013; accepted 23 Oct. 2013; date of publication 3 Nov. 2014; date of current version 19 Mar. 2014.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Model-free tracking</term>
					<term>multiple-object tracking</term>
					<term>online learning</term>
					<term>structured SVM</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Model-free trackers can track arbitrary objects based on a single (bounding-box) annotation of the object. Whilst the performance of model-free trackers has recently improved significantly, simultaneously tracking multiple objects with similar appearance remains very hard. In this paper, we propose a new multi-object model-free tracker (using a tracking-by-detection framework) that resolves this problem by incorporating spatial constraints between the objects. The spatial constraints are learned along with the object detectors using an online structured SVM algorithm. The experimental evaluation of our structure-preserving object tracker (SPOT) reveals substantial performance improvements in multi-object tracking. We also show that SPOT can improve the performance of single-object trackers by simultaneously tracking different parts of the object. Moreover, we show that SPOT can be used to adapt generic, model-based object detectors during tracking to tailor them towards a specific instance of that object.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ç 1 INTRODUCTION</head><p>T RACKING is a fundamental problem in computer vision with applications in a wide range of domains. Whilst significant progress has been made in tracking specific objects (e.g., in tracking faces <ref type="bibr" target="#b0">[1]</ref>, humans <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, cars <ref type="bibr" target="#b5">[6]</ref>, and rigid objects <ref type="bibr">[7]</ref>), the development of trackers that work well on arbitrary objects remains hard. Because manually annotating sufficient examples of all objects in the world is prohibitively expensive and time-consuming, recently, approaches for model-free tracking have received increased interest <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. In model-free tracking, an object of interest is manually annotated in the first frame of a video sequence, e.g., using a rectangular bounding box drawn around the object. The annotated object needs to be tracked throughout the remainder of the video. Modelfree tracking is a challenging task because (1) little prior information is available about the object to be tracked, <ref type="bibr" target="#b1">(2)</ref> this information is ambiguous in the sense that the initial bounding box only approximately distinguishes the object of interest from the background, and (3) the object appearance may change drastically over time, in particular, when the object is deformable.</p><p>Usually, a tracking system comprises three main components <ref type="bibr" target="#b9">[10]</ref>: <ref type="bibr" target="#b0">(1)</ref> an appearance model that predicts the likelihood that the object is present at a particular location based on the local image appearance, (2) a location model that predicts the prior probability that the object is present at a particular location; and (3) a search strategy for finding the maximum a posteriori location of the object. In our modelfree tracker, the appearance model is implemented by a classifier trained on histogram-of-gradient (HOG) features <ref type="bibr" target="#b10">[11]</ref>, the location model is based on the relative locations of the objects that are being tracked, and the search strategy is a sliding-window exhaustive search.</p><p>In many applications, it is necessary to track more than one object. For instance, in surveillance applications, one frequently needs to track particular people <ref type="bibr" target="#b2">[3]</ref>, faces <ref type="bibr" target="#b11">[12]</ref>, or cars <ref type="bibr" target="#b12">[13]</ref> in a complex environment. A simple approach to tracking multiple objects is to run multiple instances of a single-object tracker. In this paper, we argue that this is suboptimal because such an approach fails to exploit spatial constraints between the objects. For instance, nearby people tend to walk in the same direction on the side walk, cars drive in the same direction on the freeway, and when the camera shakes, all objects will move roughly in the same direction. We show that it is practical to exploit these types of spatial relations between objects in model-free tracking. In particular, we develop a structure-preserving object tracker (SPOT) that incorporates spatial constraints between objects using a pictorial-structures framework <ref type="bibr" target="#b13">[14]</ref>. We train the appearance models of all the objects and the structural constraints between these objects jointly in an online structured SVM framework <ref type="bibr" target="#b14">[15]</ref>. Our experimental evaluations show that the incorporation of structural constraints leads to substantial performance improvements in multi-object tracking: SPOT performs very well on Youtube videos with significant camera motion, rapidly moving objects, object appearance changes, and occlusions. We also show that SPOT may be used to improve single-object trackers by using part detectors in addition to the object detector, with spatial constraints between the parts. Moreover, we show that SPOT can be used to tailor detectors for generic objects to specific instances of that object, which leads to performance improvements in model-based tracking.</p><p>In summary, our main contributions are: <ref type="bibr" target="#b0">(1)</ref> we present a new approach that performs online learning of pictorialstructures models that incorporate spatial constraints between objects, which helps in simultaneous model-free tracking of multiple objects, <ref type="bibr" target="#b1">(2)</ref> we show that our approach may improve the performance of single-object model-free trackers by simultaneously tracking a target object and informative parts of that object, and (3) we show that our approach can be used to tailor state-of-the-art generic object detectors to particular objects.</p><p>An earlier version of this paper appeared in <ref type="bibr" target="#b15">[16]</ref>. Compared to the prior paper, this study contains (1) a substantial number of additional explanations and analysis, (2) various additional experiments to investigate the impact of spatialstructure preservations in tracking, and (3) a new approach for adapting existing generic-object detectors to improve their performance when tracking a specific object.</p><p>The outline of the remainder of this paper is as follows. We discuss related work in Section 2. Section 3 introduces our new SPOT tracker that incorporates spatial relations between different objects or difference object parts. We present the results of our experiments in Section 4. Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our structure-preserving object tracker incorporates ideas from prior work on model-free tracking, deformable template models, and online learning for structured prediction models. Below, we briefly review relevant prior work on these three topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model-Free Tracking</head><p>There exists a plethora of prior work on model-free tracking. Below, we give a concise overview of this work. For extensive reviews on model-free tracking, we refer the reader to <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p><p>Model-free trackers can be subdivided into (1) generative trackers that model only the appearance of the object itself <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b21">[20]</ref>, <ref type="bibr" target="#b22">[21]</ref> and ( <ref type="formula" target="#formula_1">2</ref>) discriminative trackers that model the appearance of both the object and the background <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b23">[22]</ref>, <ref type="bibr" target="#b24">[23]</ref>, <ref type="bibr" target="#b25">[24]</ref>, <ref type="bibr" target="#b26">[25]</ref>. Because generative trackers model the appearance of the target object without considering the appearance of the background, they often fail when the background is cluttered or when multiple moving objects are present. By contrast, discriminative trackers construct a classifier that distinguishes the target object from the background. Recent results suggest that discriminative trackers outperform generative trackers <ref type="bibr" target="#b8">[9]</ref> (similar results have been obtained for model-based trackers, e.g., <ref type="bibr" target="#b27">[26]</ref>, <ref type="bibr" target="#b28">[27]</ref>). This result is supported by theoretical results showing that discriminative models always outperform their generative counterparts on a discriminative task such as object tracking <ref type="bibr" target="#b29">[28]</ref>. Hence, in this paper, we will focus on learning discriminative object appearance models.</p><p>Many prior studies on model-free tracking focus on exploring different feature representations for the target object, including feature representations based on points <ref type="bibr" target="#b30">[29]</ref>, <ref type="bibr" target="#b31">[30]</ref>, <ref type="bibr" target="#b32">[31]</ref>, contours <ref type="bibr" target="#b33">[32]</ref>, <ref type="bibr" target="#b34">[33]</ref>, <ref type="bibr" target="#b35">[34]</ref>, integral histograms <ref type="bibr" target="#b36">[35]</ref>, subspace learning <ref type="bibr" target="#b22">[21]</ref>, sparse representations <ref type="bibr" target="#b37">[36]</ref>, and local binary patterns <ref type="bibr" target="#b8">[9]</ref>. In this work, we capitalize on the success of the Dalal-Triggs <ref type="bibr" target="#b10">[11]</ref> and Felzenszwalb <ref type="bibr" target="#b5">[6]</ref> detectors, and use HOG features instead.</p><p>Recent work on model-free tracking also focuses on developing new learning approaches to better distinguish the target object from the background. In particular, previous studies have investigated approaches based on boosting <ref type="bibr" target="#b24">[23]</ref>, <ref type="bibr" target="#b38">[37]</ref>, random forests <ref type="bibr" target="#b8">[9]</ref>, multiple instance learning <ref type="bibr" target="#b7">[8]</ref>, and structured output learning to predict object transformations <ref type="bibr" target="#b39">[38]</ref>. Our tracker is similar to these approaches in that it updates the appearance model of the target object online. Our tracker differs from previous approaches in that it uses a learner that aims to identify configurations of objects or object parts; specifically, we use a structured SVM model that is updated online.</p><p>Whilst simultaneous tracking of multiple objects using model-based trackers has received significant attention in prior work, e.g., in the context of tracking people <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b40">[39]</ref> and other objects <ref type="bibr" target="#b41">[40]</ref>, <ref type="bibr" target="#b42">[41]</ref>, up to the best of our knowledge, model-free tracking of multiple objects has hitherto remained unstudied. The most closely related to our work is a recent study <ref type="bibr" target="#b43">[42]</ref> that tries to improve the identification of a single target object by also tracking stable features in the background, thereby improving the location prior for the target object. Our work differs from <ref type="bibr" target="#b43">[42]</ref> in that it only tracks target objects and exploits the spatial relations between those; this makes it much easier to track, e.g., multiple objects that have a very similar appearance, such as individual flowers in a large flower field or individual cars on a freeway.</p><p>The present paper is related to several prior studies that illustrate the potential of using contextual information to improve the performance of model-free trackers. In particular, <ref type="bibr" target="#b44">[43]</ref>, <ref type="bibr" target="#b45">[44]</ref>, <ref type="bibr" target="#b46">[45]</ref>, <ref type="bibr" target="#b47">[46]</ref> automatically identify one or more auxiliary points or regions that are tracked in order to improve the location prior of the tracker. Auxiliary regions are generally defined as regions that are salient, that co-occur with the target object, and whose movement is correlated with that of the target object. They can be tracked, e.g., using color-based mean-shift tracking <ref type="bibr" target="#b45">[44]</ref>, optical flow <ref type="bibr" target="#b47">[46]</ref>, or KLT tracking <ref type="bibr" target="#b46">[45]</ref>. (In addition to auxiliary regions, <ref type="bibr" target="#b47">[46]</ref> also employs "distractor" regions, which are similar in appearance to the target and consistently co-occur with the target, as context.) Our work differs from <ref type="bibr" target="#b45">[44]</ref>, <ref type="bibr" target="#b47">[46]</ref> in that we adapt our model of the spatial relations between objects (or object parts) using an online-learning approach, which allows us to simultaneously track "target" and "auxiliary" objects for much longer periods of time. Our work also differs from prior work in that we do not make a distinction between "target" and "auxiliary" objects. Specifically, we use the same features and tracking-by-detection approach for both types of objects and the interaction between target and auxiliary objects is "undirected": the detection of auxiliary objects may help to identify the location of the target object, but vice versa, the detection of the target objects may also help to identify the location of the auxiliary object. This undirected interaction between target and auxiliary objects may lead to a more robust tracker.</p><p>In model-based tracking, various studies have also proposed the use of contextual information to improve the performance of the tracker. For instance, <ref type="bibr" target="#b48">[47]</ref> makes assumptions on the environment (viz. it assumes the environment is a meeting room) in order to learn and exploit relations between different object categories (e.g., humans and chairs). Stalder et al. <ref type="bibr" target="#b49">[48]</ref> propose an approach to increase the robustness of model-based trackers by incorporating constraints on the size of the target object(s), on the preponderance of the background, and on the smoothness of track trajectories. Roth et al. <ref type="bibr" target="#b50">[49]</ref> exploit prior information on the location of target objects by learning classifier grids, i.e., by learning a separate detection classifier for each image location. Our study is different from <ref type="bibr" target="#b48">[47]</ref>, <ref type="bibr" target="#b49">[48]</ref>, <ref type="bibr" target="#b50">[49]</ref> in that we take a model-free instead of a model-based approach: our tracker relies on just a single annotation of the target objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deformable Template Models</head><p>Deformable template models represent an object by a collection of part models that are spatially related. Each part models the appearance of a portion of the (deformable) object, and the spatial relations between the parts are modeled by some joint distribution over part locations. Popular choices for this joint distribution over part locations include (1) PCA-based models that use a low-rank Gaussian model and (2) pictorial-structures models <ref type="bibr" target="#b13">[14]</ref>, which use a collection of springs between the parts (typically, using a tree structure). PCA-based models are used in, among others, active appearance models <ref type="bibr" target="#b51">[50]</ref>, active shape models <ref type="bibr" target="#b52">[51]</ref>, and constrained local models <ref type="bibr" target="#b53">[52]</ref> that are commonly used in face or medical image analysis. Models based on pictorial structures are commonly used in tasks such as object detection <ref type="bibr" target="#b5">[6]</ref>, pose estimation <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b54">[53]</ref>, and gesture recognition <ref type="bibr" target="#b55">[54]</ref>, <ref type="bibr" target="#b56">[55]</ref>. In model-free tracking, there generally is insufficient training data to train PCA-based models, which is why we focus on pictorial-structures models in this work.</p><p>Pictorial-structures models represent spatial relations between object parts by springs that can be exerted or compressed (at a penalty) when the object under consideration is deformed. In many prior studies, pictorial-structures models are designed manually to incorporate the most important spatial relations between the object parts. For instance, <ref type="bibr" target="#b57">[56]</ref>, <ref type="bibr" target="#b58">[57]</ref> represent the hierarchical structure of the human upper body using eight parts (viz. upper/lower arms, head, torso, and hands), and <ref type="bibr" target="#b54">[53]</ref> further divide these upper body parts into smaller subparts in order to make the model more flexible. When designing models for the detection or tracking of arbitrary objects, however, such a manual approach to the design of pictorial-structures models is not viable. Therefore, detectors for arbitrary objects typically use generic structure models, such as (1) a star-shaped model that represents the object by a root node that is directly connect with all its parts <ref type="bibr" target="#b5">[6]</ref> or (2) a Chow-Liu tree <ref type="bibr" target="#b59">[58]</ref> that models only spatial relations between nearby parts <ref type="bibr" target="#b60">[59]</ref>. In such detectors, the parts are automatically identified by searching for the most discriminative regions of the object based on a large collection of annotated images.</p><p>In this work, we also adopt generic star-shaped and minimum spanning tree models to represent the relations between the multiple objects that are being tracked. (In a second experiment, we also study model-free trackers that incorporate models for object parts that are heuristically determined.). The key difference with prior studies, however, is that we not learn the parameters of the springs in the pictorial-structures model (i.e., the length and direction of the springs) based on a large collection of annotated data, but that we learn the spring parameters in an online fashion during tracking <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Online Learning for Structured Prediction</head><p>Structured prediction techniques such as conditional random fields <ref type="bibr" target="#b61">[60]</ref> and structured support vector machines <ref type="bibr" target="#b62">[61]</ref> make predictions over (exponentially) large output spaces that have some inherent structure, such as sequences, trees, or objects configurations, and have been used with much success in a variety of computer vision problems <ref type="bibr" target="#b54">[53]</ref>, <ref type="bibr" target="#b63">[62]</ref>, <ref type="bibr" target="#b64">[63]</ref>. The detection of multiple objects can naturally be framed as a structured-prediction problem of searching over all possible object configurations.</p><p>In our work, a structured predictor is trained in an online manner based on detections in earlier frames. Algorithms for online learning of structured predictors are generally based on perceptron-like learning algorithms <ref type="bibr" target="#b65">[64]</ref>, <ref type="bibr" target="#b66">[65]</ref> or on stochastic (sub)gradient descent algorithms <ref type="bibr" target="#b67">[66]</ref>, <ref type="bibr" target="#b68">[67]</ref>. Both learning algorithms perform cheap parameter updates based on a single training example or a small batch of examples, and work well in practice (in non-convex learning problems, they often construct better solutions than batch learners). We update our multi-object tracker after each frame using a simple (stochastic) subgradient descent algorithm cf. <ref type="bibr" target="#b62">[61]</ref>; the step size for each parameter update is determined using a passive-aggressive algorithm <ref type="bibr" target="#b69">[68]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SPOT TRACKER</head><p>The basis of our structure-preserving object tracker is formed by the popular Dalal-Triggs detector <ref type="bibr" target="#b10">[11]</ref>, which uses HOG features to describe image patches and an SVM to predict object presence. HOG features measure the magnitude and the (unsigned) direction of the image gradient in small cells (we used 8 Â 8 pixel cells). Subsequently, contrast normalization is applied on rectangular, spatially connected blocks of four cells. The contrast normalization is implemented by normalizing the L2-norm of all histograms in a block. The resulting values are clipped at 0:2 and then renormalized according the L2-norm. The advantages of HOG features are that (1) they consider more edge orientations than just horizontal or vertical ones, (2) they pool over relatively small image regions, and (3) they are robust to changes in the illumination of the tracked object. Together, this makes HOG features more sensitive to the spatial location of the object than, e.g., Haar features <ref type="bibr" target="#b10">[11]</ref>, which is particularly important in model-free tracking because the identified location of the object is used to update the classifiers: small localization errors may thus propagate over time, causing the tracker to drift. Moreover, efficient implementations can extract HOG features at high frame rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>We represent the bounding box that indicates object i 2 V (with V representing the set of objects we are tracking) by B i ¼ fx i ; w i ; h i g with center location x i ¼ ðx i ; y i Þ, width w i , and height h i . The HOG features extracted from image I that correspond to locations inside the object bounding box B i are concatenated to obtain a feature vector fðI; B i Þ. Subsequently, we define a graph G ¼ðV ; EÞ over all objects i 2 V that we want to track with edges ði; jÞ2E between the objects. The edges in the graph model can be viewed as springs that represent spatial constraints between the tracked objects. Next, we define the score of a configuration C ¼ fB 1 ; . . . ; B jV j g of multiple tracked objects as the sum of two terms: (1) an appearance score that sums the similarities between the observed image features and the classifier weights for all objects and (2) a deformation score that measures how much a configuration compresses or stretches the springs between the tracked objects (the relative angles of the springs are not taken into account in our model). Mathematically, the score of a configuration C is defined as:</p><formula xml:id="formula_0">sðC; I; QÞ ¼ X i2V w T i f ðI; B i Þ À ! X ði;jÞ2E kðx i À x j Þ À e ij k 2 : (1)</formula><p>Herein, the parameters w i represent linear weights on the HOG features for object i, e ij is the vector that represents the length and direction of the spring between objects i and j, and the set of all parameters is denoted by Q ¼ fw 1 ; . . . ; w jV j ; e 1 ; . . . ; e jEj g. We treat the parameter ! as a hyperparameter that determines the trade-off between the appearance and deformation scores. We use Platt scaling <ref type="bibr" target="#b70">[69]</ref> to convert the configuration score to a configuration likelihood pðCjI; QÞ. Our model is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. In preliminary experiments, we also tried to incorporate (temporal) Gaussian priors on the object locations in the score sðC; I; QÞ, but this did not appear to lead to performance improvements: the location prior specified by the spatial constraints in sðC; I; QÞ appears to be sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inference</head><p>Given the parameters of the model, finding the most likely object configuration amounts to maximizing Eq. ( <ref type="formula">1</ref>) over all possible configurations C. This maximization is intractable in general because it requires searching over exponentially many configurations, but for tree-structured graphs G, a combination of dynamic programming and min-convolutions can be used to perform the maximization in linear time. Here we only give briefly description of the inference algorithm; for more details, we refer to <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b71">[70]</ref>.</p><p>The dynamic programming equations for maximizing Eq. ( <ref type="formula">1</ref>) take the form of a message-passing procedure on the tree G. Herein, a message from object i to j is computed by (1) gathering all incoming messages into object i, (2) combining the resulting score with the object detection scores for object i, and (3) transforming the result to incorporate for the stress in the edge between object i and j. The message-passing equations take the form:</p><formula xml:id="formula_1">R ij ðxÞ ¼ w T i f ðI; B i Þ þ X 8k6 ¼j:ðk;iÞ2E m k!i ðxÞ;<label>(2)</label></formula><formula xml:id="formula_2">m i!j ðxÞ ¼ max x 0 ½R ij ðx 0 Þ À !kðx À x 0 Þ À e ij k 2 ; (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where B i ¼ fx; w i ; h i g. The message-passing procedure emanates from an arbitrary root object; after a full forwardbackward pass on the pictorial-structures tree, the sum of all incoming messages into the root corresponds to the configuration C Ã that maximizes sðC; I; QÞ. The configuration C Ã can be recovered by backtracking the computations already performed. See <ref type="bibr" target="#b5">[6]</ref> for more details.</p><p>The inference algorithm can be implemented very efficiently because the negate of Eq. ( <ref type="formula" target="#formula_2">3</ref>) is a two-dimensional min-convolution problem. The one-dimensional counterpart of Eq. ( <ref type="formula" target="#formula_2">3</ref>) can be rewritten in the form:</p><formula xml:id="formula_4">DðpÞ ¼ min q ½fðqÞ þ ðp À qÞ 2 ; (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where p; q 2 G with G a one-dimensional grid. The value of DðpÞ can be computed 8p 2 G in linear time by looping over the parabolas at all locations p, maintaining a list of coordinates for which the parabolas considered until now have the lowest value <ref type="bibr" target="#b71">[70]</ref>. At each step in this loop, the intersection of the current parabola with the last parabola in the list is computed analytically.</p><p>Based on the location of this intersection, the range of coordinates for which the current parabola has the lowest value can be determined, from which we can decide whether or not the last parabola in the list should be removed or not. The min-convolution algorithm is extended to two dimensions by first running the onedimensional algorithm over all rows of the image, and then running the one-dimensional algorithm on the output of this first stage across all columns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning</head><p>Like other model-free trackers <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b24">[23]</ref>, we use the tracked object configurations in previous frames as positive examples to update our model. After observing an image I, the most likely object configuration C is found by maximizing Eq. ( <ref type="formula">1</ref>). We assume that this object configuration C is a true positive example, and therefore, we would like the score at this ground truth location sðC; I; QÞ to be larger than any other configuration Ĉ by at least a margin DðC; ĈÞ. Herein, D is a task-loss function that equals zero iff Ĉ ¼ C. The task loss is assumed to be non-negative, 8 Ĉ 6 ¼ C : DðC; ĈÞ &gt; 0, and upper bounded by a constant M, 9M8C : max Ĉ DðC; ĈÞ &lt; M. To adapt our model in such a way that it assigns a higher score to the positive locations and lower scores to other locations, we update the model parameters Q by taking a gradient step in the direction of the structured SVM loss function <ref type="bibr" target="#b62">[61]</ref>. Here, the structured SVM loss ' is defined as the maximum violation of the task loss by configuration Ĉ:</p><p>'ðQ; I; CÞ ¼ max Ĉ ½sð Ĉ; I; QÞ À sðC; I; QÞ þ DðC; ĈÞ: (5)</p><p>The structured SVM loss function does not contain quadratic terms, but it is the maximum of a set of affine functions. As a result, the structured SVM loss in Eq. ( <ref type="formula">5</ref>) is a convex function in the parameters Q.</p><p>In our SPOT tracker, we define the task-loss DðC; ĈÞ based on the amount of overlap between the correct configuration C and the incorrect configuration Ĉ:</p><formula xml:id="formula_6">DðC; ĈÞ ¼ X i2V 1 À B i \ Bi B i [ Bi ! :<label>(6)</label></formula><p>Herein, the union and intersection of two bounding boxes B i and Bi are both measured in pixels. If Ĉ has no overlap with C, the task loss equals jV j, and the task loss equals zero iff Ĉ ¼ C. In preliminary experiments, we also explored task losses that only considered the distance between object centers (and not the size of objects), but we did not find these to improve the performance of our tracker.</p><p>The gradient of the structured SVM loss in Eq. ( <ref type="formula">5</ref>) with respect to the parameters Q is given by: r Q 'ðQ; I; CÞ ¼ r Q sðC Ã ; I; QÞ À r Q sðC; I; QÞ; <ref type="bibr">(7)</ref> in which the "negative" configuration C Ã is given by:</p><formula xml:id="formula_7">C Ã ¼ argmax Ĉ ðsð Ĉ; I; QÞ þ DðC; ĈÞÞ:<label>(8)</label></formula><p>In practice, this gradient is not very appropriate for learning the parameters of our multi-object tracker because due to the influence of the deformation score, the negative configuration C Ã tends to comprise bounding boxes B i that have very low appearance scores</p><formula xml:id="formula_8">w T i f I; B i ð Þ.</formula><p>As a result, these bounding boxes are inappropriate examples to use as a basis for updating w i . For instance, if we would incorporate the deformation score when identifying a negative example in the Air Show video (see Fig. <ref type="figure" target="#fig_1">2</ref>), we would often select empty sky regions as negative examples. These regions are very uninformative, which hampers the learning of good appearance models for the airplanes under consideration. To address this problem, we ignore the structural constraints when selecting the negative configuration. Specifically, we use a search direction p for learning that is defined as:</p><formula xml:id="formula_9">p ¼ r Q sð CÃ ; I; QÞ À r Q sðC; I; QÞ; (<label>9</label></formula><formula xml:id="formula_10">)</formula><p>where the negative configuration is given by: CÃ ¼ argmax Ĉ ðsð Ĉ; I; QÞ þ DðC; ĈÞÞ; <ref type="bibr" target="#b9">(10)</ref> with the negative appearance score s being defined as:</p><formula xml:id="formula_11">sð CÃ ; I; QÞ ¼ X i2V w T i f I; B i ð Þ:<label>(11)</label></formula><p>It is straightforward to show that the inner product between the search direction and the true gradient direction remains positive, p Á r Q 'ðQ; I; CÞ &gt; 0, as a result of which the learner will still converge (under suitable conditions on the step size <ref type="bibr" target="#b72">[71]</ref>). The configuration CÃ can be computed efficiently by <ref type="bibr" target="#b0">(1)</ref> adding a term to each object filter response that contains the ratio of overlapping pixels for a bounding box at that location with the detected bounding box to account for the task loss D and (2) re-running exactly the same efficient inference procedure as the one that was used to maximize Eq. ( <ref type="formula">1</ref>) over configurations.</p><p>We use a passive-aggressive algorithm to perform the parameter update <ref type="bibr" target="#b69">[68]</ref>. The passive-aggressive algorithm sets the step size in such a way as to substantially decrease the loss, while ensuring that the parameter update is not too large. In particular, the passive-aggressive algorithm uses the following parameter update:</p><formula xml:id="formula_12">Q Q À 'ðQ; I; CÞ p k k 2 þ 1 2K p;<label>(12)</label></formula><p>where K 2 0; þ1 ð Þ is a hyperparameter that controls the "aggressiveness" of the parameter update. The selection of K will be discussed in Section 4.1. In preliminary experiments, we also experimented with a confidence-weighted learning algorithm <ref type="bibr" target="#b73">[72]</ref> that finds an individual step size for each parameter. However, the performance of this confidence-weighted learning algorithm was very similar to that of the passive-aggressive algorithm, which is why we use the passive-aggressive algorithm in the remainder of the paper.</p><p>When an object is occluded, we do not want to update the appearance model for that object (even if its location was correctly predicted thanks to the spatial constraints in our model). To avoid erroneous updates of our appearance model, we only update a weight vector w i corresponding to detection B i when the exponentiated score for that object exceeds some threshold. In particular, we only update the w i and e ij whenever 1 Z expðw T i f I; B i ð ÞÞ&gt; 0:4. The weights w i are initialized by training an SVM that discriminates between the manually annotated object from 50 randomly selected negative examples (extracted from the first frame) that have little to no overlap with the ground-truth annotation. The parameters e ij are initialized based on the ground-truth annotations of the objects: e ij</p><p>x i À x j , with x i and x j the locations of objects i and j in the first frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Graph Structure</head><p>A remaining issue is how we determine the structure of the graph G, i.e., how we decide on which objects are connected by an edge. Ideally, we would employ a fully connected graph G, but this would make inference intractable (see Section 3.2). Hence, we explore two approaches to construct a tree on the objects i 2 V : (1) a star model <ref type="bibr" target="#b5">[6]</ref> and (2) a minimum spanning tree model. In the star model, each object is connected by an edge with a dummy object r 2 V that is always located at the center of all the objects, i.e., there are no direct relations between the objects. This requires a minor adaptation of the score function:</p><formula xml:id="formula_13">sðC; I; QÞ ¼ X i2V =r w T i fðI; B i Þ À ! X ði;rÞ2E kðx i À x r Þ À e i k 2 :<label>(13)</label></formula><p>The minimum spanning tree model is constructed based on the object annotations in the first frame; it is obtained by searching the set of all possible completely-connected tree models for the tree that minimizes P ði;jÞ2E kx i À x j k 2 , where x i and x j are the locations of objects i and j in the first frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Computational Complexity</head><p>The main computational costs of running our tracker are in the extraction of the HOG features (which are shared between object detectors) and in the computation of the appearance score per object; after these quantities are computed, the maximization of Eq. ( <ref type="formula">1</ref>) takes only a few milliseconds. The computational complexity grows linearly in the number of objects being tracked (i.e., in jV j). We found that it is possible to track approximately four objects simultaneously in real-time (in small videos of size 320 Â 240 pixels) on a consumer laptop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We performed three sets of experiments to evaluate the performance of our tracker. In the first set of experiments, we evaluate the performance of the SPOT tracker on a range of multi-object tracking problems, comparing it to the performance of various state-of-the-art trackers that do not employ structural constraints between the objects. In the second set of experiments, we study the use of SPOT to improve single-object tracking by tracking parts of an object and constraining the spatial configuration of those parts. The third and last experiment studies the use of SPOT for adapting generic object detectors to detectors for specific objects during tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment 1: Multiple-Object Tracking</head><p>We first evaluate the performance of the SPOT tracker on videos in which multiple objects need to be tracked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Setup</head><p>We used nine videos with multiple objects in this set of experiments. Three of these videos (Shaking, Basketball, and Skating) were already used in earlier studies <ref type="bibr" target="#b74">[73]</ref>; the other six were downloaded from YouTube. The videos were selected based on characteristics that are challenging for current model-free trackers, such as the presence of multiple, nearby objects with a very similar appearance. The average length of the videos is 842 frames. The left column of Fig. <ref type="figure" target="#fig_1">2</ref> shows the first frame of each of the nine videos along with the corresponding ground-truth annotations of the objects, i.e., the left column of Fig. <ref type="figure" target="#fig_1">2</ref> shows all labeled training data that is available to train our tracker. The videos are available from http://visionlab.tudelft.nl/spot.</p><p>We experiment with three variants of the SPOT tracker: (1) a baseline tracker that does not use structural constraints (i.e., a SPOT tracker with ! ¼ 0; no-SPOT), ( <ref type="formula" target="#formula_1">2</ref>) a SPOT tracker that uses a star tree to model spatial relations between objects (star-SPOT), and (3) a SPOT tracker that uses a minimum spanning tree to model structural constraints (mst-SPOT). We compare the performance of our SPOT trackers with that of two state-of-the-art (single-object) trackers, viz. the OAB tracker <ref type="bibr" target="#b24">[23]</ref> and the TLD tracker <ref type="bibr" target="#b8">[9]</ref>, of which we run multiple instances to separately track each object. The OAB and TLD trackers were run using the implementations provided by their developers.</p><p>Following <ref type="bibr" target="#b7">[8]</ref>, we evaluate the performance of the trackers by measuring 1 : (1) average location error (ALE): the average distance of the center of the identified bounding box to the center of the ground-truth bounding box and (2) correct detection rate (CDR): the percentage of frames for which the overlap between the identified bounding box and the ground truth bounding box is at least 50 percent. For each video, these two measurements are averaged over all target objects, and over five separate runs. In all experiments with star-SPOT and mst-SPOT, we fixed ! ¼ 0:001 and K ¼ 1. In preliminary experiments, we found that the results are very robust under changes of ! and K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Results</head><p>The performance of the five trackers (OAB, TLD, no-SPOT, star-SPOT, and mst-SPOT) on all nine videos is presented in Table <ref type="table" target="#tab_0">1</ref> and Fig. <ref type="figure" target="#fig_2">3</ref>. Fig. <ref type="figure" target="#fig_1">2</ref> shows five frames of all videos with the tracks obtained by mst-SPOT. Videos showing the complete tracks are provided in the supplemental material, available online. Videos showing all tracking results are available on http://visionlab. tudelft.nl/spot. We first qualitatively describe the results on four of the videos.</p><p>Air Show. The video contains a formation of four visually similar planes that fly very close to each other; the video contains a lot of camera shakes. Whereas our baseline trackers (OAB, TLD, and no-SPOT) confuse the  1. Unlike <ref type="bibr" target="#b8">[9]</ref>, we do not compute the recall (or F-measure) of the trackers: the recall of a tracker is only informative when in the target object is occluded or has disappeared in a substantial portion of the video. In the videos we considered in our experiments, there are no such occlusions or disappearances of the target object.</p><p>planes several times during the course of the video, star-SPOT and mst-SPOT track the correct plane throughout the entire video.</p><p>Car chase. This video is challenging because (1) the two cars are very small and (2) both cars are occluded for around 40 frames, while various other cars are still visible. Whereas this occlusion confuses the baseline trackers, the two SPOT trackers do not lose track because they can use the location of one car to estimate the location of the other.</p><p>Red flowers. The video shows several similar flowers that are moving and changing appearance due to the wind, and that sometimes (partially) occlude each other; we track four of these flowers. The baseline trackers lose track very quickly due these partial occlusions. By contrast, the two SPOT trackers flawlessly track all flowers during the entire length of the video (2,249 frames), because they can use the structural constraints to distinguish the different flowers.</p><p>Hunting. The cheetah and gazelle in this video clip are very hard to track, because their appearance changes significantly over time and because their relative location is changing (the cheetah passes the gazelle). Nevertheless, the SPOT trackers can exploit the fact that both animals move in a similar direction, which prevents them from losing track.</p><p>Taken together, the results presented in Table <ref type="table" target="#tab_0">1</ref> and Fig. <ref type="figure" target="#fig_1">2</ref> show (1) that our baseline no-SPOT tracker performs on par with state-of-the-art trackers such as TLD; and (2) that the use of spatial constraints between objects leads to substantial performance improvements when tracking multiple objects, in particular, when minimum spanning trees are used (mst-SPOT). The performance improvements are particularly impressive for videos in which objects with a similar appearance are tracked, such as the Car Chase and Red Flowers videos, because the structural constraints prevent the tracker from switching between objects. Structural constraints are also very helpful in videos with a lot of camera shake (such as the Air Show video), because camera shake causes all objects to move in the same direction in the image. The SPOT tracker even outperforms single-object trackers when perceptually different objects are tracked that have a relatively weak relation in terms of their location, such as in the Hunting video, because it can share information between objects to deal with, e.g., motion blur and camera shake. The mst-SPOT tracker outperforms the star-SPOT tracker in almost all videos, presumably, because a minimum spanning tree introduces direct (rather than indirect) constraints between the locations of the objects.</p><p>We also performed experiments with multi-scale versions of our SPOT tracker. The multiple-scale trackers are run at three scales for each frame (viz. relative scales 0.9, 1.0, and 1.1), and select the highest posterior probability among the three scales to determine the location and scale of the object. In all experiments, we assume that the aspect ratio of the rectangular bounding box is fixed. Table <ref type="table" target="#tab_1">2</ref> presents the performance obtained by three multi-scale, multi-object trackers on the same nine movies. In particular, we use the (multi-scale) CXT tracker <ref type="bibr" target="#b47">[46]</ref>, a multi-scale version of the TLD tracker <ref type="bibr" target="#b8">[9]</ref>, and a multi-scale version of the no-SPOT tracker as a baseline. 2 We compare the performance of these baseline trackers with that of our multi-scale mst-SPOT tracker.</p><p>(Following <ref type="bibr" target="#b7">[8]</ref>, we only measure tracking errors in terms of the average location error in these experiments. Measuring the correct detection rate is problematic because the ground truth bounding boxes are all in single scale.) The results presented in Table <ref type="table" target="#tab_1">2</ref> are in line with those presented in Table <ref type="table" target="#tab_0">1</ref>: the mst-SPOT tracker substantially outperforms trackers without structural constraints on most (but not all) of the videos. On videos such as the Parade and Sky Diving, the CXT and TLD trackers appear to outperform mst-SPOT despite their lack of structural constraints. We surmise this has to do with the size of the objects: the fixed-size of HOG cells (of 8 Â 8 pixels) that SPOT uses in its appearance model may be too large for the small target objects in these two videos. Indeed, the Haar-representations that the CXT and TLD tracker employ may be more suited for tracking such small objects. Comparing the results in Table <ref type="table" target="#tab_0">1</ref> and 2, it can be seen that the difference in performance between single-scale and multi-scale SPOT trackers is rather small, presumably, because most videos exhibit relatively little scale changes over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment 2: Single Object Tracking</head><p>With some minor modifications, our SPOT tracker may also be used to improve the tracking of single objects. As the appearance of an object may frequently change due to partially occlusions, a holistic appearance model may easily loose track. The appearance of (some of the) smaller parts of the object is not altered by partial occlusions; therefore, separately tracking parts may help to locate the object of interest. SPOT can be used to track the parts of a single object (treating them as individual objects in V ) with structural constraints between the parts. Inspired by <ref type="bibr" target="#b5">[6]</ref>, we experiment with a SPOT tracker that has a single "global" object detector and a number of "local" part detectors. We experiment with a star-shaped model in The results are averaged over five runs and over all target objects in each video The best performance on each video is boldfaced.</p><p>2. In the experiments with multi-scale trackers, we did not consider the OAB tracker because it cannot straightforwardly be adapted from single-scale to multi-scale mode.</p><p>which the global detector forms the root of the star (star-SPOT), and with a model that constructs a minimum spanning tree over the global object and the local part detectors (mst-SPOT). In single-object tracking, the alteration we made earlier to the search direction of the learner has become unneeded: because all part detectors involve the same object, it is actually important to incorporate the structural constraints when identifying a "negative" configuration. Therefore, we used SPOT trackers that learn using the true gradient of the structured SVM loss in our single-object tracking experiments, i.e., we set p ¼ r Q 'ðQ; I; CÞ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Setup</head><p>Because a single bounding box is used to annotate the object in the first frame of the video, we need to determine what parts the tracker will use. As a latent SVM <ref type="bibr" target="#b5">[6]</ref> is unlikely to work well on a single training example, we use a heuristic approach that assumes that relevant parts correspond to discriminative regions inside the object bounding box. We initialize part i at the location in which the weights of the initial global SVM w are large and positive, because these correspond to features that are highly indicative of object presence. In particular, we initialize B i as:</p><formula xml:id="formula_14">B i ¼ argmax B 0 i &amp;B X ðx 0 i ;y 0 i Þ2B 0 i À maxð0; w x 0 i y 0 i Þ Á 2 ; (<label>14</label></formula><formula xml:id="formula_15">)</formula><p>where B denotes the bounding box of the (single global) object. We fix the number of parts jV jÀ1 in advance, setting it to 2; we fix the width and height of the part bounding boxes to 40 percent of the width and height of the bounding box indicating the deformable object; and we ensure that the selected part cannot have more than 50 percent overlap with the other parts in the first frame. Unlike <ref type="bibr" target="#b5">[6]</ref>, we extract the features for the part detectors on the same scale as the features for the global detector. In preliminary experiments, we also tried using finer-scale HOG features to describe the parts, but we did not find this to lead to performance improvements. Using the same features for all detectors has computational advantages because the features only need to be computed once, which is why we opt for it here.</p><p>The experiments are performed on a publicly available collection of 13 videos. The first 12 videos of them are from <ref type="bibr" target="#b7">[8]</ref>, and the last one is from <ref type="bibr" target="#b8">[9]</ref>. The videos contain a wide range of objects that are subject to sudden movements and (out-of-plane) rotations, and have cluttered, dynamic backgrounds. The videos have an average length of 718 frames. Each video contains a single object to be tracked, which is indicated by a bounding box in the first frame of the video. (First-frame annotations for all movies are shown in <ref type="bibr" target="#b7">[8]</ref>.)</p><p>Again, we evaluate the performance of the trackers by measuring the average location error and the correct detection rate of the tracker, and averaging over five runs. We compare the performance of our tracker with that of three state-of-the-art trackers, viz., the OAB tracker <ref type="bibr" target="#b24">[23]</ref>, the MILBoost tracker <ref type="bibr" target="#b7">[8]</ref>, and the TLD tracker <ref type="bibr" target="#b8">[9]</ref>. (All trackers were run on a single scale.) We could not run the implementation of the MILBoost tracker ourselves as it is outdated (the MILBoost tracker was not considered in Experiment 1 for this reason), but because we use exactly the same experimental setup as <ref type="bibr" target="#b7">[8]</ref>, we adopt the results presented in <ref type="bibr" target="#b7">[8]</ref> here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Results</head><p>Table <ref type="table" target="#tab_2">3</ref> and Fig. <ref type="figure" target="#fig_5">5</ref> present the performance of all six trackers on all 13 videos. Fig. <ref type="figure" target="#fig_3">4</ref> illustrates the tracks obtained with the MIL, TLD, and mst-SPOT trackers on seven of the 13 videos. The results reveal the potential benefit of using additional part detectors when tracking a single object: mst-SPOT is the best-performing tracker on eight of the 13 videos in terms of average location error, and on nine of the 13 videos in terms of correct detection rate. The performance improvements are particularly impressive in challenging movies such as the Tiger videos, in which parts of the object are frequently occluded by leaves. In such situations, the SPOT trackers benefit from the presence of part detectors that can accurately detect the non-occluded part(s) of the object. The results also show that mst-SPOT generally outperforms star-SPOT, which is interesting because it suggests that for object detection in still images, pictorial-structure models with a minimum spanning tree <ref type="bibr" target="#b60">[59]</ref> may be better than those with a star tree <ref type="bibr" target="#b5">[6]</ref>. Whilst mst-SPOT outperforms the other tracker on most videos, its performance on the Motocross and Tea Box videos is poor. For the Motocross video, we surmise that this is because our HOG descriptors are not appropriate for the small target region being tracked in that video. The results on the Tea Box video suggest that single-object SPOT-like most other modelfree trackers-may be hampered by out-of-plane rotations of the target object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiment 3: Tracking and Identification</head><p>In the third and last experiment, we explore the merits of the SPOT tracker in the context of model-based trackers that follow the tracking-by-detection paradigm. In particular, we explore the ability of SPOT to improve the performance of object detectors that are trained to recognize generic objects classes such as faces, pedestrians, or cars. After an initial detection is made using the Felzenszwalb object detector <ref type="bibr" target="#b5">[6]</ref>, we may use single-object SPOT to track the detected object over time. In our experiments, the SPOT tracker is initialized using the parameters of the off-theshelf detector; after the initial detection, the parameters of the tracker are then adapted to recognize the particular object under consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Setup</head><p>We perform experiments on three videos for pedestrian detection from ETH data set <ref type="bibr" target="#b75">[74]</ref>, called Sunny day, Jelmoli, and Bahnhof. The Sunny day movie has a length of 354 frames and contains a total of 29 persons, the Jelmoli movie has 450 frames and about 40 persons, and the Bahnhof movie has 1,000 frames and about 70 persons. To avoid tracking false positive detections, we only select those detections which have high response at the same location longer than three frames. We compare our tracker with that of three state-ofthe-art pedestrian detectors that are used in a trackingby-detection scenario: (1) a Dalal-Triggs detector (HOG) that was obtained by training a linear SVM on HOG features <ref type="bibr" target="#b10">[11]</ref>, (2) a Felzenszwalb detector (LatSVM) that was obtained by training a latent SVM on HOG features at two scales <ref type="bibr" target="#b5">[6]</ref>, and (3) the "fastest pedestrian detector in the wild" (FPDW) that was obtained by Doll ar <ref type="bibr" target="#b76">[75]</ref>. All three detectors were trained on the pedestrian class of the Pascal VOC 2007 challenge. We ran the three detectors using their default parameter values. Following <ref type="bibr" target="#b77">[76]</ref>, we evaluate the performance of all pedestrian detectors by evaluating the miss rate as a function of the false positive rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Results</head><p>Fig. <ref type="figure" target="#fig_4">6</ref> shows the miss rate as a function of the false positive rate of all four detectors on all three videos (lower curves indicate better performance). The results show that SPOT consistently outperforms the baseline LatSVM detector, which is identical to SPOT without the onlinelearning component. This shows that tailoring the appearance model of an object detector to a particular object of interest may indeed improve performance. Both LatSVM and SPOT substantially outperform the Dalal-Triggs detector (HOG), which highlights the merits of using part detectors in addition to a global object detector. SPOT is also very competitive compared to FPDW, in particular, in scenarios in which low false positive rates are required. Fig. <ref type="figure">7</ref> shows some of the tracking results of mst-SPOT for on the Sunny day, Jelmoli, and Bahnhof videos. The results highlight the strong performance of mst-SPOT for pedestrian detection. In particular, mst-SPOT appears to perform well when multiple pedestrians partially occlude each other thanks to the use of part detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>In this paper, we have developed a new model-free tracker that tracks multiple objects by combining multiple single-object trackers via constraints on the spatial structure of the objects. Our experimental results show that the resulting SPOT tracker substantially outperforms traditional trackers in settings in which multiple objects need to be tracked. We have also showed that the SPOT tracker can improve the tracking of single objects by including additional detectors for object parts. Moreover, the SPOT tracker can be used to adapt a generic detector to a specific object while tracking. The computational costs of our tracker only grow linearly in the number of objects (or object parts) that is being tracked, which facilitates real-time tracking. Of course, the ideas presented in this paper may readily be implemented in other model-free trackers that are based on tracking-by-detection, such as the TLD tracker <ref type="bibr" target="#b8">[9]</ref>. It is likely that including structural constraints in those trackers will lead to improved tracking performance, too.</p><p>In future work, we aim to explore the use of different structural constraints between the tracked objects; for instance, for tracking certain deformable objects it may be better to use a structural model based on PCA (as is done in, e.g., constrained local models <ref type="bibr" target="#b28">[27]</ref>). We also plan to develop approaches that identify the relevant parts of a deformable object in a more principled way during tracking via online learning algorithms for latent SVMs.  Laurens van der Maaten was a postdoctoral researcher at University of California, San Diego, as a PhD student at Tilburg University, and as a visiting PhD student at the University of Toronto. He is an assistant professor in computer vision and machine learning at Delft University of Technology, The Netherlands. His research interests include deformable template models, dimensionality reduction, classifier regularization, and tracking.</p><p>" For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The left image shows a configuration formed by some patches, where vertices and edges indicate objects and spatial relationships between objects. The middle image shows the features extracted from the left configuration, where vertices and edges indicate HOG features of objects and relative location vectors between objects. The right images shows the structure model we trained, where vertices and edges indicate trained HOG templates of objects and relative location vectors between objects. Our goal is to find the configuration match the trained structure model best in each frame, and update the structure model as well.</figDesc><graphic coords="4,68.33,60.49,430.26,72.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Tracking results obtained by mst-SPOT on all nine videos used in Experiment 1 (from top to bottom: Air Show, Car Chase, Red Flowers, Hunting, Sky Diving, Shaking, and Basketball). The colors of the rectangles indicate the different objects that are tracked. Figure best viewed in color. Videos showing the full tracks are presented in the supplementary material, which can be found on the Computer Society Digital Library at http:// doi.ieeecomputersociety.org/10.1109/TPAMI.2013.221.</figDesc><graphic coords="6,73.30,51.19,420.00,387.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Average location error of five multiple-object trackers on all nine videos (lower is better). Figure best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Tracking results on seven of the 13 videos (David Indoor, Dollar, Girl, Tiger 2, Coke Can, Occl. Face 2, and Occl. Face) obtained by the MIL, OAB, TLD, and mst-SPOT trackers. Figure best viewed in color.</figDesc><graphic coords="10,34.30,51.19,498.24,460.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Miss rate and false positive rate of HOG, LatSVM, FPDW, and mst-SPOT tracker in pedestrian detection on three movies from the ETH data set (lower curves indicate better performance). Figure best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Average location error of six single-object trackers on all 13 videos (lower is better). Figure best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="12,34.30,51.19,498.24,282.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Performance of Five Model-Free Trackers on Multiple-Object Videos Measured in Terms of (1) Average Location Error (ALE; Lower Is Better) in Pixels between the Centers of the Predicted and the Ground-Truth Bounding Box and (2) Correct Detection Rate (CDR; Higher Is Better)To measure the correct detection rate, a detection is considered correct if the overlap between the identified bounding box and the ground truth bounding box is at least 50 percent The results are averaged over five runs and over all target objects in each video. The best performance on each video is boldfaced.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>Performance of Four Multi-Scale Model-Free Trackers on Multiple-Object Videos Measured in Terms of the Average Location Error (ALE; Lower Is Better) in Pixels between Centers of the Predicted and the Ground-Truth Bounding Box</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>Performance of Six Model-Free Trackers on Single-Object Videos Measured in Terms of (1) Average Location Error (ALE; lower Is better) in Pixels between the Centers of the Predicted and the Ground-Truth Bounding Box and (2) Correct Detection Rate (CDR; higher Is Better)To measure the correct detection rate, a detection is counted as correct if the overlap between the identified bounding box and the ground truth bounding box is at least 50 percent The results are averaged over five runs. The best performance on each video is boldfaced.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 36, NO. 4, APRIL 2014</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by EU-FP7 Social Signal Processing (SSPNet) and by the China Scholarship Council. The authors thank two anonymous reviewers for helpful comments on earlier versions of this paper. In addition, the authors thank David Tax, Marco Loog, and Martijn van de Giessen for many helpful discussions, and Zdenek Kalal and Helmut Grabner for their implementations of the TLD and OAB trackers.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rapid Object Detection Using a Boosted Cascade of Simple Features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bramble: A Bayesian Multi-Blob Tracker</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maccormick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="34" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Online Multiperson Tracking-by-Detection from a Single, Uncalibrated Camera</title>
		<author>
			<persName><forename type="first">M</forename><surname>Breitenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Reichlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Koller-Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1820" to="1833" />
			<date type="published" when="2011-09">Sept. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">You&apos;ll Never Walk Alone: Modeling Social Behavior for Multi-Target Tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coupling Detection and Data Association for Multiple Object Tracking</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thangali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Betke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1948" to="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object Detection with Discriminatively Trained Part Based Models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010-09">Sept. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Keypoint Recognition Using Randomized Trees</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1456" to="1479" />
			<date type="published" when="2006-09">Sept. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust Object Tracking with Online Multiple Instance Learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1619" to="1632" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">P-n Learning: Bootstrapping Binary Classifiers by Structural Constraints</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object Tracking: A Survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Histograms of Oriented Gradients for Human Detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Face-TLD: Tracking-Learning-Detection Applied to Faces</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Int&apos;l Conf. Image Processing</title>
		<meeting>IEEE Conf. Int&apos;l Conf. Image essing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A System for Real-time Detection and Tracking of Vehicles from a Single Car-Mounted Camera</title>
		<author>
			<persName><forename type="first">C</forename><surname>Caraffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Trefny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sochman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 15th Int&apos;l Conf. Intelligent Transportation Systems (ITS)</title>
		<meeting>IEEE 15th Int&apos;l Conf. Intelligent Transportation Systems (ITS)</meeting>
		<imprint>
			<date type="published" when="2012-09">Sept. 2012</date>
			<biblScope unit="page" from="975" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Representation and Matching of Pictorial Structures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Elschlager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computers</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="92" />
			<date type="published" when="1973-01">Jan. 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Strong Supervison from Weak Annotation: Interactive Training of Deformable Part Models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1832" to="1839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Structure Preserving Object Tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recent Advances and Trends in Visual Tracking: A Review</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="3823" to="3831" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Online Object Tracking: A Benchmark</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An Adaptive Appearance Model Approach for Model-Based Articulated Object Tracking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="758" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><surname>Fig</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pedestrian detections obtained using mst-SPOT on the Sunny day (top), Jelmoli (middle), and Bahnhof (bottom) videos from the ETH data set. Figure best viewed in color</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Eigentracking: Robust Matching and Tracking of Articulated Objects Using a View-Based Representation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Jepson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="84" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Incremental Learning for Robust Visual Tracking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ensemble Tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="271" />
			<date type="published" when="2007-02">Feb. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real-Time Tracking via On-Line Boosting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conf</title>
		<meeting>British Machine Vision Conf</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adaptive Discriminative Generative Model and Its Applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Information Processing Systems</title>
		<meeting>Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="801" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gradient Feature Selection for Online Boosting</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">.</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Support Vector Tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1064" to="1072" />
			<date type="published" when="2004-08">Aug. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automatic Feature Localisation with Constrained Local Models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3054" to="3067" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Discriminative Models, Not Discriminative Training</title>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<idno>MSR-TR-2005-144</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Microsoft Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An Iterative Image Registration Technique with an Application to Stereo Vision</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Joint Conf</title>
		<meeting>Int&apos;l Joint Conf</meeting>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="674" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Particle Video: Long-Range Motion Estimation Using Point Trajectories</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Teller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="91" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Good Features to Track</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Real-Time Tracking of Multiple Occluding Objects Using Level Sets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bibby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Condensation-Conditional Density Propagation for Visual Tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="28" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Elliptical Head Tracking Using Intensity Gradients and Color Histograms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Robust Fragments-Based Tracking Using The Integral Histogram</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="798" to="805" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust Visual Tracking and Vehicle Classification via Sparse Representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2259" to="2272" />
			<date type="published" when="2011-11">Nov. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semi-Supervised On-Line Boosting for Robust Tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Struck: Structured Output Tracking with Kernels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Decentralized Multiple Target Tracking Using Netted Collaborative Autonomous Trackers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A Boosted Particle Filter: Multitarget Detection and Tracking</title>
		<author>
			<persName><forename type="first">K</forename><surname>Okuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Taleghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Collaborative Tracking of Multiple Targets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="834" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Group Tracking: Exploring Mutual Relations for Multiple Object Tracking</title>
		<author>
			<persName><forename type="first">G</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sputnik Tracker: Looking for a Companion Improves Robustness of the Tracker</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hlavac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Scandinavian Conf. Image Analysis</title>
		<meeting>Scandinavian Conf. Image Analysis</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Context-Aware Visual Tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1195" to="1209" />
			<date type="published" when="2009-07">July 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Tracking the Invisible: Learning Where the Object Might Be</title>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cattin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1285" to="1292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Context Tracker: Exploring Supporters and Distracters in Unconstrained Environments</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Key Object Driven Multi-Category Object Recognition, Localization and Tracking Using Spatio-Temporal Context</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="409" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cascaded Confidence Filtering for Improved Tracking-by-Detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Stalder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="369" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Classifier Grids for Robust Adaptive Object Detection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sternig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2727" to="2734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Interpreting Face Images Using Active Appearance Models</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Automatic Face and Gesture Recognition</title>
		<meeting>IEEE Conf. Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="300" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<title level="m">Active Shape Models</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Facial Feature Detection and Tracking with Automatic Template Selection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Seventh Int&apos;l Conf. Automatic Face and Gesture Recognition</title>
		<meeting>Seventh Int&apos;l Conf. Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="429" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Articulated Pose Estimation Using Flexible Mixtures of Parts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Recognizing Proxemics in Personal Photos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3522" to="3529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Detecting Actions, Poses, and Objects with Relational Phraselets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="158" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pictorial Structures Revisited: People Detection and Articulated Pose Estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1014" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pose Search: Retrieving People Using Their Pose</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marin-Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Approximating Discrete Probability Distributions with Dependence Trees</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="462" to="467" />
			<date type="published" when="1968-05">May 1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Face Detection, Pose Estimation, and Landmark Localization in the Wild</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Int&apos;l Conf. Machine Learning</title>
		<meeting>18th Int&apos;l Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Large Margin Methods for Structured and Interdependent Output Variables</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1453" to="1484" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning to Localize Objects with Structured Output Regression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="2" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Shared Parts for Deformable Part-Based Models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1513" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL-02 Conf. Empirical Methods in Natural Language Processing</title>
		<meeting>ACL-02 Conf. Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Hidden-Unit Conditional Random Fields</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR W&amp;CP</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="479" to="488" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Online Algorithms and Stochastic Approximations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Cambridge Univ. Press</publisher>
		</imprint>
	</monogr>
	<note>Online Learning and Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Pegasos: Primal Estimated Sub-Gradient Solver for SVM</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. 24th Int&apos;l conf. Machine learning</title>
		<imprint>
			<biblScope unit="page" from="807" to="814" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Online Passive-Aggressive Algorithms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="551" to="585" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Probabilistic Outputs for Support Vector Machines and Comparison to Regularized Likelihood Methods</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Large Margin Classifiers</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Distance Transforms of Sampled Functions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Zoutendijk</surname></persName>
		</author>
		<title level="m">Methods of Feasible Directions</title>
		<imprint>
			<publisher>Elsevier Publishing Company</publisher>
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Confidence-Weighted Linear Classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Machine Learning</title>
		<meeting>Int&apos;l Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Visual Tracking Decomposition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1269" to="1276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A Mobile Vision System for Robust Multi-Person Tracking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">The Fastest Pedestrian Detector in the West</title>
		<author>
			<persName><forename type="first">P</forename><surname>Doll Ar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conf</title>
		<meeting>British Machine Vision Conf</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Pedestrian Detection: An Evaluation of the State of the Art</title>
		<author>
			<persName><forename type="first">P</forename><surname>Doll Ar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
