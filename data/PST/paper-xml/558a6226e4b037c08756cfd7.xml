<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">01E10E26B3B978DE20B352CD8D9513B3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>M OST MODERN automatic speech recognition (ASR)   systems model speech as a nonstationary stochastic process by statistically characterizing a sequence of spectral estimations <ref type="bibr" target="#b0">[1]</ref>. The common technique for spectral estimation includes an approximation of auditory filtering, a compressive nonlinearity (usually the logarithm), and decorrelation of the spectral estimation through an approximate Karhunen-Lo√®ve (KL) transform (the discrete cosine transform). These steps represent rough approximations of the most fundamental aspects of auditory processing: frequency selectivity and magnitude compression. In the last five to ten years, the frequency selectivity for ASR front-ends has migrated from a linear to a perceptually based frequency scale <ref type="bibr" target="#b1">[2]</ref>. This progress, toward a better auditory model for ASR, has improved robustness <ref type="bibr" target="#b2">[3]</ref>.</p><p>A large discrepancy remains between current auditory models and the approximations used in ASR front ends. Recent efforts to incorporate more sophisticated auditory models with ASR systems, however, have shown little to no improvements, typically at a severe increase in computational costs <ref type="bibr" target="#b2">[3]</ref>. The challenges are to determine what auditory functionality missing from the current front end would be useful for improving recognition robustness and to design efficient mechanisms which reproduce that functionality.</p><p>This paper focuses on two aspects of audition not included in current representations: short-term adaptation and sensitivity to the frequency position of local spectral peaks. For each, a mechanism with low computational complexity is described, which adds to the common front end and provides a representation that is more robust to background noise. The dynamic mechanism is parameterized by psychophysical data described here and in the literature <ref type="bibr" target="#b3">[4]</ref>. The peak isolation mechanism is a simple modification of a previous cepstral liftering technique <ref type="bibr" target="#b4">[5]</ref>. Emphasizing dynamic local peaks is shown to be more robust than emphasizing either dynamics or local peaks.</p><p>To incorporate a dynamic mechanism within a front end, a method of quantifying auditory adaptation must first be identified. There is considerable physiological and psychophysical evidence of dynamic audition. Short-term adaptation, usually defined as a decreasing response after the onset of a constant stimulus, has been measured in individual auditory nerve firings <ref type="bibr" target="#b5">[6]</ref>. The neural response to a stimulus is also reduced during the recovery period following adaptation to a prior stimulus <ref type="bibr" target="#b6">[7]</ref>. Here the general term adaptation is used for both dynamic processes (short-term adaptation and postadaptation recovery), and its direction is explicitly specified when significant. Attack refers to the decreasing response following stimulus onset, while release and recovery both refer to the increasing response following stimulus offset. Motility of outer hair cells, the likely source of an active cochlear response, also adapts with time constants which may be significant when quantifying short-term adaptation <ref type="bibr" target="#b7">[8]</ref>. Finally, neural responses to onsets and abrupt spectral changes are substantial <ref type="bibr" target="#b8">[9]</ref>, providing a physiological substrate for the sensitivity of human speech perception to onsets and dynamic spectral cues <ref type="bibr" target="#b9">[10]</ref>. Although recognition systems typically statistically characterize the evolution of relatively static spectral segments, the auditory system responds most strongly to dynamic segments. This response strength is a consequence of adaptation. What remains is to quantify the adaptation, and to design a mechanism that reproduces it.</p><p>The task is similar to observing evidence of frequency selectivity and requiring a specification (critical bandwidths) and a mechanism for its realization (a filterbank). Following the example of using static masking data to quantify frequency selectivity <ref type="bibr" target="#b10">[11]</ref>, adaptation was quantified from a series of dynamic, forward-masking experiments. The adaptation mechanism designed is a modified form of automatic gain control (AGC), which adds an exponentially adapting linear offset to logarithmic intensity. Just as the current triangular filters used in the common ASR front end are first-order approximations of auditory frequency selectivity, the simple dynamic mechanism provides a first-order approximation of auditory adaptation. The strategy is to parameterize simple dynamic mechanisms from forward masking thresholds to provide a better approximation of the auditory response to dynamic stimuli. Dynamic auditory models <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b16">[16]</ref> are often physiologically based computational models that characterize only a relatively low level of the complete auditory system, or resort to some speculation either about higher level processing and/or about appropriate dynamic parameters. Because these systems usually require processing time-domain signals for each auditory filter (often 100 filters) at the full sampling rate, they imply a large computational burden, making them difficult to use in engineering applications <ref type="bibr" target="#b2">[3]</ref>. Also, successfully separating and quantifying measurable functionality (e.g., frequency selectivity, or short-term adaptation), which may be distributed across several related physiological processes, is not a simple task. Some researchers <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b18">[18]</ref> propose novel computationally efficient techniques, targeted at automatic speech recognition, which emphasize spectral dynamics with varying perceptual accuracy and recognition improvements. The approach here differs from most detailed physiological models in that it "closes the loop" with observations of toplevel functionality. Because the relatively simple model of frequency selectivity followed by additive adaptation is consistent with underlying physiological processes, the resulting quantified nonlinear model provides useful approximations of the perception of (nonstationary) speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. FORWARD MASKING</head><p>Forward masking reveals that over short durations the usable dynamic range of the auditory system is relatively small, and largely dependent on the intensity and spectral characteristics of previous stimuli. A probe following a masker is less audible than a probe following silence. As the duration between the masker and probe decreases, the probe threshold is increasingly a function of the intensity of the preceding masker, and decreasingly a function of the absolute probe threshold in silence. Forward masking can be viewed as a consequence of auditory adaptation. After adaptation to the masker, recovery time is necessary before the relatively less intense probe becomes audible. The amount of forward masking is also a function of the duration of the masker, reflecting the time required for the auditory system to adapt completely to the masker. Forward masking, therefore, provides an opportunity to measure the rate and magnitude of effective auditory adaptation and recovery.</p><p>To build the dynamic model, data describing sinusoidal forward masking were desirable. The most complete data of pure-tone forward masking experiments is from <ref type="bibr" target="#b19">[19]</ref>. Although <ref type="bibr" target="#b19">[19]</ref> includes a wide range of frequencies and masker levels, the longest probe delay measured is 40 ms, short of the duration necessary for complete adaptation. To obtain recovery parameters, a set of pure-tone forward-masking experiments that included probe delays from 15 to 120 ms across wide ranging frequencies and masker levels was performed. Shortdelay pure-tone forward-masking data, from <ref type="bibr" target="#b3">[4]</ref>, as a function of masker duration, were used to quantify attack parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiments</head><p>The forward-masking experiments used long-tone maskers followed by short tonelike probes of the same frequency and phase. The masker was long enough to ensure complete auditory adaptation before masker offset, while the probe was short enough to measure the response of the auditory system at a relatively specific time. A two alternative forced choice (2AFC) experimental paradigm was used.</p><p>1) Stimuli: Fig. <ref type="figure" target="#fig_1">1</ref> shows an example of the stimuli. A decaying 60 ms probe tone followed one of two 300 ms maskers, which were separated by 500 ms (in Fig. <ref type="figure" target="#fig_1">1</ref>(a) the probe follows the second masker). The subjects chose which masker the probe followed. Masker and probe frequencies ranged from 250-4000 Hz in octave intervals, probe delays were 15, 30, 60, and 120 ms, and masker levels spanned roughly 50 dB with three points. All signals were ramped on and off in 5 ms with the appropriate half period of a raised cosine. Probe-delay times are specified between the peaks of the envelopes of the masker offset and probe onset.</p><p>In forward masking, it is often difficult to determine what cue subjects are using, or when the subject detects the probe. The solution here is similar to that in <ref type="bibr" target="#b20">[20]</ref>. Both the probe and the masker in the nonprobe interval decay with the same 20 ms time constant, and both end at the same time relative to the masker onset. With this arrangement, detecting the probe onset was a sufficient cue to determine the probe interval, but detecting a decaying sinusoid (the tail of the probe) was not. Subjects were not given feedback.</p><p>To reduce the spectral splatter of transitions, the entire stimulus was filtered through a linear-phase, finite impulse response (FIR) filter, with a bandwidth of one critical band <ref type="bibr" target="#b21">[21]</ref>. In the Fig. <ref type="figure" target="#fig_1">1</ref> example, the frequency is 1 kHz [Fig. <ref type="figure" target="#fig_1">1(b)</ref>], the delay from masker to probe is 15 ms [Fig. <ref type="figure" target="#fig_1">1(c)],</ref> and<ref type="figure"></ref> (measured at the envelope peak) the probe is 8 dB less intense than the masker. The stimulus is shown after the critical band filter.</p><p>2) Subjects: Five subjects, including the first author, participated in the experiments. All were native speakers of American English. One subject was female, and the others were male. Their ages ranged from 23 to 28 years. Hearing thresholds for each were at or below 20 dB HL at frequencies used in this study.</p><p>3) Methods: For each condition, the level of the probe was adaptively varied to find its threshold. An adaptive "transformed up-down" procedure <ref type="bibr" target="#b22">[22]</ref> determined the 79% correct point, defined as the threshold for the 2AFC task. The initial adaptation step size of 4 dB was reduced to 2 dB and 1 dB after the first and third reversals. The initial probe was clearly audible. The experiment continued for nine reversals. The probe levels at the last six reversals were averaged to determine the threshold. Thresholds were averaged across the five subjects to obtain the values used for parameterizing the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Equipment and Calibration:</head><p>Computer software generated the appropriate digital stimuli before each trial. The sampling rate was 16 kHz, and the quantization was 16-b linear. An Ariel Pro Port 656 converted the digital samples into an analog waveform, and the preamp of a Sony 59ES DAT recorder drove TDH-49P earphones. Tests were performed in a double-walled sound-isolated chamber. Stimuli were presented binaurally with identical waveforms to each ear. The system was calibrated by measuring the response to digitally synthesized sine waves using a 6cc coupler and a Larson-Davis 800B Sound Level Meter. Preamp levels and digital internal level offsets were set to place an 80 dB SPL (sound pressure level) 1 kHz tone within 0.2 dB. A linear-phase FIR equalization filter was adjusted until pure tones from 125-7500 Hz measured within 0.5 dB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results</head><p>Fig. <ref type="figure" target="#fig_2">2</ref> summarizes the average threshold increase (circles) across the five subjects as a function of masker level with probe delay as a parameter. The solid lines in Fig. <ref type="figure" target="#fig_2">2</ref> indicate the model's fit to the forward masking data. The derivation of the model is described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Modeling Implications</head><p>The amount of forward masking (in dB) decays as a straight line as a function of the logarithm of the probe delay (first described in <ref type="bibr" target="#b20">[20]</ref>). A straight line with respect to logarithmic probe delay can be approximated by an exponential with respect to linear probe delay. This suggests additive exponential adaptation in decibels.</p><p>Fig. <ref type="figure" target="#fig_3">3</ref>(a) plots the threshold increase as a function of probe delay, and Fig. <ref type="figure" target="#fig_3">3(b)</ref> shows the effective dynamic range below masker, defined as the difference between the masker and probe threshold levels, as a function of masker level. Fig. <ref type="figure" target="#fig_3">3(a)</ref> shows that the rate of decay of the forward masking (shown on a log time scale) increases with an increasing amount of masking. These data may suggest different adaptation rates for different masker intensities, or complexity beyond a simple exponential adaptation of dB level. Such complexity is not necessary. The adapting mechanism derived below has a greater initial distance to target after a more intense masker offset. Exponential processes decay more quickly over the same amount of time when the output is further from the final static target. Therefore, a simple exponential dynamic mechanism can predict a faster rate of decay of forward masking with more intense maskers.</p><p>Fig. <ref type="bibr">3(b)</ref> shows that even at short delays the dynamic range below masker depends on the level of the masker. At short delays there is little to no time for adaptation. Without time for adaptation, the static characteristics of the dynamic mechanism determine the forward masking threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FROM EXPERIMENTAL RESULTS TO MODEL PARAMETERS</head><p>In the perceptual model, a dynamic adaptation stage follows each output of a linear filterbank. At every time sample, each adaptation stage slowly adjusts an internal offset to move its output incrementally closer to an input/output (I/O) target, specified on a log/log scale.</p><p>The dynamic adaptation stages are referred to as automatic gain control (AGC). However, it is significant that the AGC is implemented as an adapting additive offset to the log energy of the signal, and not as an adapting multiplicative gain. There are at least two points that appear to require additive, and not multiplicative, adaptation. First, the measured incremental neural response to a second onset after partial adaptation to a first is not proportional to an adapted amount of multiplicative gain <ref type="bibr" target="#b5">[6]</ref>. Second, AGC that adjusts a multiplicative gain proportional to the linear distance to the I/O target does not predict a higher rate of decay of forward masking for greater amounts of masking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. AGC: I/O Curves, Attack, and Release Times</head><p>Time constants describing the rate of adaptation for the dynamic mechanisms are defined here as the time required for the logarithmic distance to target to reduce by a factor of Different time constants are used for attack (decreasing offset), and release (increasing offset). Over short durations, the AGC stage has little time to adapt, and is therefore nearly linear. I/O graphs do not include a time axis, so to discuss the temporal evolution of the system, we describe trajectories </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Derivation of Model Parameters</head><p>The model's forward-masking prediction is derived from the response of the dynamic mechanism to forward-masking stimuli. When the output of the adapting (dynamic) mechanism is just at threshold during the onset of the probe, the model predicts a forward-masking threshold.</p><p>To simplify the model and this derivation, a constant I/O slope is imposed across the compressive region. Fig. <ref type="figure" target="#fig_5">5</ref> describes the geometries necessary to measure the model's prediction of the forward-masking threshold with long maskers as a function of masker level and probe delay. Before the masker offset, the output trajectory reaches the target on the I/O curve (point A in Fig. <ref type="figure" target="#fig_5">5</ref>). As the masker shuts off abruptly, the output trajectory instantly falls along the diagonal (from A to B). Once the trajectory is below the compressive region, the distance to target is constant, and the model adapts by slowly increasing toward maximum additive offset (from B toward C). At some point during this adaptation (point C), the onset of the probe causes an abrupt transition from below threshold back up along a new diagonal (from C to D). If the probe level is intense enough to place the trajectory above threshold (at the instant of the probe onset) the probe is audible. If the internal level just reaches threshold, the model predicts a forward masking threshold (at point D).</p><p>Incremental adaptation of the model is implemented using a (nonconstant coefficient) first-order difference equation leading to an exponential decay of the logarithmic distance to target. From the geometry in Fig. <ref type="figure" target="#fig_5">5</ref>, probe level at threshold as a function of masker level discrete-time probe delay I/O slope and incremental adaptation is where and are both referenced to the static threshold. Instantaneously, or with no delay the model predicts a short-term dynamic range below masker equal to the vertical distance between the static I/O curve and threshold Therefore, the data points at the shortest delay [Fig. <ref type="figure" target="#fig_3">3(b)</ref>] provide an approximation for the I/O slope parameter An iterative procedure was used to minimize the total mean squared error (MSE) between the model predictions of the probe thresholds and the average forward masking data for all data points at each center frequency, as a function of the two model parameters and The total MSE is relatively insensitive to the I/O slope, compared to the adaptation parameter Therefore, the initial estimate of from the short-delay conditions was averaged with the value that minimizes total MSE, to determine a final estimate. A second MSE minimization as a function of only determined the final estimate.</p><p>Just as forward-masking data as a function of probe delay are used to characterize recovery, the change of forward masking with the duration of the masker is used to characterize attack. Short-duration maskers reduce the time for downward adaptation, which decreases the amount of adaptation, and in turn, reduces the time to recovery. Geometries necessary to derive attack (downward adaptation) parameters are described in Fig. <ref type="figure" target="#fig_6">6</ref>. Before the onset of the masker, the model reaches the static threshold (at point A in Fig. <ref type="figure" target="#fig_6">6</ref>). At the abrupt masker onset, the output trajectory translates diagonally upward (from A to B) and then slowly drops toward the I/O target as the model adapts (from B to C to D). If the duration of the masker is short relative to the downward time constant, the trajectory will not reach the I/O target by the time of the abrupt masker offset (point C). In response to the masker offset, the output trajectory corresponding to the short masker moves diagonally (from point C), crossing the internal threshold at a lower point than the trajectory corresponding to the longer masker (from point D). After brief recovery during a short probe delay, the model predicts less forward masking from the short-duration masker.</p><p>Following incomplete downward adaptation (or attack), and as a function of the attack parameter discrete-time masker duration and probe delay the model predicts a probe threshold of The probe threshold difference, between short and long masker durations is This probe threshold difference equation was solved for the model parameter and then its value was estimated from the differences reported in <ref type="bibr" target="#b3">[4]</ref>, using the and parameters derived above. Table <ref type="table">I</ref> summarizes the model parameters and adaptation time constants across frequencies. The and terms are with respect to a 100 Hz spectral sampling rate. Adaptation stages with center frequencies between measured points use a weighted average of neighboring parameters. Attack time constants are approximately three to four times shorter than release time constants. These times, and more accurately their ratio, approximate those derived from physiological data <ref type="bibr" target="#b12">[13]</ref>.</p><p>Fig. <ref type="figure">7</ref> shows the model's prediction of the decay of masking at 1 kHz. Note that the decay rate of forward masking is greater with more intense maskers, and that the decay is nearly Fig. <ref type="figure" target="#fig_2">2</ref>, includes the model's fit to the average forwardmasking data. The computational model approximates forward-masking data for a wide range of masker levels and probe delays across several frequencies. The standard deviation of the error is: 2.7, 2.9, 3.2, 3.1, and 2.4 dB, at 250, 500, 1k, 2k, and 4k Hz, respectively. Most notably, however, the model consistently underestimates forward masking at the shortest probe delays. At least two factors contribute to this error.</p><p>First, the exponential derivation assumes the 15 ms delay between the masker and probe is silence. This assumption provides the maximum possible distance to target during the 15 ms, the maximum amount of recovery, and the lowest prediction of forward masking. In fact, the stimuli had 5 ms of offset, 5 ms of silence, and 5 ms of onset during this interval. Any nonsilence during the 15 ms delay decreases the distance to target, reduces the amount of recovery, and increases the estimation of forward masking. Ignoring the finite onsets and offsets reduces the model's predictions of the amount of forward masking at short delays. Second, in this derivation, forward masking is assumed to occur when insufficient auditory recovery keeps the response to the probe below threshold. However, at shorter (near zero) delays, with extremely similar maskers and probes, the probe may only be audible as a change in level at the end of the masker <ref type="bibr" target="#b24">[24]</ref>, and not as a separate event. Even though the response to the probe is above threshold, the subject may not distinguish the probe from the masker, and therefore not detect the probe. Because the derivation requires the model's response to the probe to be below threshold to be masked, it underestimates the amount of forward masking, especially at short delays with intense maskers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Predicting Other Data</head><p>Fig. <ref type="figure" target="#fig_8">9</ref> (a)-(c) shows the model's predictions of previous forward masking data. Fig. <ref type="figure" target="#fig_8">9</ref>(a) shows the model's prediction of average data with wideband stimuli <ref type="bibr" target="#b20">[20]</ref>. These data provide relatively complete measurements of forward masking across level and delay. In the results shown in Fig. <ref type="figure" target="#fig_2">2</ref>, there is only slight variation of forward masking with frequency. Because the adapting response of the model to wideband stimuli approximates the response at middle frequencies, the wideband data were predicted using the model parameters derived from the 1 kHz data. Although the model underestimates these data, the trends are consistent.</p><p>Figs. 9(b) and 9(c) show the predictions for wideband and pure-tone maskers of 1 kHz pure tones, respectively <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b24">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE I MODEL PARAMETERS RELATIVE TO A 100 Hz SPECTRAL RATE</head><p>These measurements were made at relatively short delays. Authors have historically disagreed on how to specify delay in a forward-masking experiment <ref type="bibr" target="#b20">[20]</ref>. In this paper, delay is measured between the envelope peaks, while <ref type="bibr" target="#b19">[19]</ref> used zerovoltage points, and <ref type="bibr" target="#b24">[24]</ref> chose half-voltage points between the masker and probe offset. The present study used 5 ms ramps, <ref type="bibr" target="#b19">[19]</ref> used 10 ms, and <ref type="bibr" target="#b24">[24]</ref> used 5 ms for the masker and 10 ms for the probe. To compensate for these differences, 2.5 ms is subtracted from the delay reported in <ref type="bibr" target="#b24">[24]</ref>, and 10 ms is added to the numbers in <ref type="bibr" target="#b19">[19]</ref>. The masker level in the 1 kHz band for the wideband masker is determined by the energy in the critical band <ref type="bibr" target="#b21">[21]</ref> centered at 1 kHz. Although comparisons are only possible at relatively short delays, the model overestimates the amount of masking by wideband noises, and underestimates masking by pure tones. Once parameterized, however, the simple dynamic mechanism approximates dynamic psychophysical responses.</p><p>Fig. <ref type="figure" target="#fig_8">9</ref>(d) shows the prediction of data from this study by an equation proposed in <ref type="bibr" target="#b19">[19]</ref> and are the levels of the probe and masker above threshold, and the constants and are chosen to fit the average forward-masking data at 1 kHz in <ref type="bibr" target="#b19">[19]</ref>. Even though the parameters in this equation were chosen from a data set that did not include measurements at the longer delays used in this study, it provides an excellent prediction of the present data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Other Models Predicting Forward Masking</head><p>Other auditory models have been derived which, in general, provide a better fit to forward-masking data. Most, however, do not readily extend to a general processing scheme suitable for an ASR front end. For the dynamic mechanism derived in this paper, a signal is masked when the response is below threshold. To fit forward-masking data, other models typically parameterize a decision device, and thereby impose explicit interpretations of the front end's response. If the parameterized decision device is removed to use the auditory model for an ASR front end, it is less clear how the recognition system would correctly interpret a masked signal.</p><p>backward, and forward/backward masking combinations have been predicted with great precision assuming a relatively standard model of filtering, rectification, power-law compression, temporal integration and a decision device <ref type="bibr" target="#b25">[25]</ref>. In its original derivation, however, there was no mechanism to account for the level-dependence of forward masking. Either the temporal window shape <ref type="bibr" target="#b25">[25]</ref> or the power-law compression <ref type="bibr" target="#b26">[26]</ref> may vary with level. The decision device required an unusually high minimum detectable temporal amplitude variation of 6 dB, which may not extend well to a general processing scheme. Finally, if forward masking is entirely a consequence of temporal integration, physiological measurements of adaptation are ignored, and there is no mechanism that explains physiological and perceptual sensitivity to onsets and transitions.</p><p>Other researchers have proposed models using adaptation mechanisms to explain forward masking <ref type="bibr" target="#b27">[27]</ref>- <ref type="bibr" target="#b29">[29]</ref>. The first of these <ref type="bibr" target="#b27">[27]</ref> uses a modified version of a previous model <ref type="bibr" target="#b30">[30]</ref> that includes filtering, envelope detection, power-law compression, rapid and short-term adaptation, and long-term integration. The long-term integrator is bypassed in forwardmasking tasks. Immediately following a stimulus, the model assumes that there is no rapid onset component in response to a probe, that this component recovers exponentially with time, and that the relative level of this component is used to determine forward masking. The model is somewhere between a complete processing mechanism and an equation summarizing psychophysical responses, and therefore, is also difficult to incorporate into ASR systems. The exponential recovery of the rapid onset component has similarities to the exponential adaptation used in the dynamic mechanism described in this paper.</p><p>More recently, other researchers have developed a general auditory model that, together with an optimal decision device, predicts well a wide variety of psychophysical data <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref>. In each auditory channel, the model uses linear filtering, halfwave rectification, and lowpass filtering, followed by five adaptation stages. The output is correlated with templates that store the model's response to other (masker-only) conditions to predict masking thresholds, thereby imposing a relatively complex postprocessing mechanism to predict the data. The model provides a dynamic spectral representation of speech that is likely to improve recognition robustness; potential application improvements may warrant the significant computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PEAK ISOLATION</head><p>Both speech perception and the response of individual auditory nerves are extremely sensitive to the frequency position of local spectral peaks. There are several mechanisms and corresponding modeling approaches that may explain this sensitivity. Physiologically motivated by the local fan-out of the neural connection to outer hair cells, <ref type="bibr" target="#b13">[14]</ref> suggests cross-coupling AGC stages to improve static spectral contrast, providing functionality similar to the higher level lateral inhibitory network in <ref type="bibr" target="#b31">[31]</ref>. Significant effort <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b32">[32]</ref> also focuses on modeling how the auditory system derives, and makes use of, redundant temporal microstructure. Auditory nerves with center frequencies as far as an octave away from a local spectral peak can synchronize their response to the frequency of the peak, providing a composite neural representation dominated by that frequency <ref type="bibr" target="#b33">[33]</ref>. Similarly, perceptual discrimination of vowels is more sensitive to the frequency location of spectral peaks than to other aspects of the spectral shape <ref type="bibr" target="#b34">[34]</ref>. These data suggest that the auditory system may derive a noise-robust representation by attending to the frequency locations of local spectral peaks.</p><p>The dynamic model was therefore also evaluated with a novel processing technique, based on raised-sine cepstral liftering <ref type="bibr" target="#b4">[5]</ref> together with explicit peak normalization, which isolates local spectral peaks. Raised-sine cepstral liftering is weighting the cepstral vector by the first half-period of a raised-sine function.</p><p>The cepstral vector is an expansion of the even log spectrum in terms of cosine basis functions. The term specifies the log-spectrum average, the term approximates the logspectrum tilt, etc., and high cepstral terms represent quickly varying ripples across the log spectrum. Weighting the cepstral vector specifies the relative emphasis of different types of logspectrum variations. A raised-sine lifter deemphasizes slow changes with frequency, often associated with overall level and vocal driving-function characteristics, as well as fast changes that may reflect numerical artifacts <ref type="bibr" target="#b4">[5]</ref>.</p><p>It is helpful to view the effects of cepstral liftering in the log spectral domain. Fig. <ref type="figure" target="#fig_9">10(a</ref>) starts with the log spectrum, from a vowel implied by a truncated cepstral vector. Fig. <ref type="figure" target="#fig_9">10(b)</ref> shows the log spectrum implied after raised-sine cepstral liftering. The average level as well as slow (and fast) variations with frequency are deemphasized, leaving components that change at a moderate rate with frequency. This process emphasizes both spectral peaks and valleys.</p><p>The valleys are removed by half-wave rectifying the log spectral estimate implied after raised-sine liftering, and a final vector is obtained by transforming back to the cepstral domain. Because the half-wave rectifier is nonlinear, explicit transformation from cepstrum to log spectrum (processing through the rectifier) and then transformation back to cepstrum are required. The raised-sine lifter also affects the magnitude of the peaks. Therefore, before transforming back to the cepstrum, peaks are scaled to the level measured in the original log spectrum. The final peak-isolated estimation is shown in Fig. <ref type="figure" target="#fig_9">10(c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ROBUST RECOGNITION EVALUATION</head><p>The model was evaluated as the front end for two word recognition systems. The first is a talker-dependent dynamic time warp (DTW) system, and the second uses talker-independent hidden Markov models (HMM). The DTW system provides an initial assessment of the model and the HMM evaluations are better approximations of potential ASR applications. The vocabulary for all systems is limited to the ten digits. Finally, a comparison with RASTAbased processing is included. The inputs to all recognition experiments are corrupted with additive noise shaped to match the long-term average speech spectrum <ref type="bibr" target="#b35">[35]</ref>. Test words are embedded in (noisy) silence, so that the recognizers are required to both isolate and recognize the discrete words.</p><p>Two versions of the dynamic model were implemented: a full-rate system and a downsampled version. The fullrate system uses rounded exponential filter shapes <ref type="bibr" target="#b36">[36]</ref>, and then adapts the envelope of each filter output at the full sampling rate. The downsampled system obtains Mel-scale power spectrum estimations every 10 ms by weighting and adding power spectrum points from a fast Fourier transform (FFT), and then adapts these outputs at the downsampled rate. On an HP715 workstation, the downsampled system runs at 0.43 real time, while the full-rate implementation requires 9.4 real time. The recognition evaluations below used the downsampled implementation.</p><p>Three basic front ends are compared: linear prediction cepstral coefficients (LPCC), Mel-frequency cepstral coefficients (MFCC), and mel-frequency cepstral coefficients with adaptation (MFCCA). Each front end computes a spectral estimation every 10 ms using overlapping 30 ms Hamming windows. LPCC are computed in two stages <ref type="bibr" target="#b0">[1]</ref>: 12th-order, autocorrelation-based linear prediction provides an all-pole vocal-tract transfer function. Real cepstral coefficients are then recursively computed for this minimum-phase estimation. MFCC are computed in three stages <ref type="bibr" target="#b1">[2]</ref>. The power spectrum is computed using a zero-padded FFT. To estimate the energy at the output of each approximate auditory filter, power spectrum outputs are weighted by a triangular filter shape and then summed. The filters have a half-power bandwidth of 100 Hz up to center frequencies of 1 kHz, and a bandwidth of 0.1 times the center frequency above 1 kHz. A DCT converts the spectral estimation obtained from the logarithmic energy across filters into a final cepstral vector. Before the DCT, the logarithmic filter energies of MFCC are also processed through the dynamic stages derived in Section III to obtain the adapting spectral estimation vector MFCCA. A 13-element cepstral vector and its temporal derivative (approximated by the slope of a linear fit to seven cepstral points) are obtained for each front end, but the undifferentiated spectral level term is ignored during recognition. For the initial DTW evaluation, the peak isolation mechanism was applied only to the MFCCA to obtain MFCCAP. For subsequent HMM evaluations, all front ends were compared with and without peak isolation.</p><p>Fig. <ref type="figure" target="#fig_10">11</ref> shows spectral representations of the digits "nine six one three" from MFCC, MFCCA, and MFCCAP. The dynamic model emphasizes spectral changes in time, while peak isolation enhances spectral contrast in frequency. Together, these mechanisms highlight the spectro-temporal representation of changing frequency peaks. The second half of this picture shows representations at 5 dB signal-to-noise ratio (SNR). Onsets, transitions, and changing local spectral peaks may remain as robust cues for recognizing speech in a noisy background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DTW Evaluation</head><p>An initial evaluation with a simple dynamic programmingbased isolated word recognition system <ref type="bibr" target="#b0">[1]</ref> and a single talker was performed. A system was constructed that used an Itakura path constraint <ref type="bibr" target="#b0">[1]</ref>, and a Euclidean local distance metric excluding the undifferentiated term. Clean templates were isolated from surrounding silence, but test tokens were not. As more noise is added, word isolation, or endpoint detection, becomes more difficult. To asses the robustness of the system, it is therefore, unrealistic to assume the temporal placement of the speech within the background noise is known. Instead, dynamic programming is used to find the speech within the noise. At each time slice in the test token, a new path starts at the beginning of the template and an accumulated distance propagates through the end of the template. Accumulated distances usually reach a minimum at the end of the speech in the test token, marking the best alignment for that test token/template pair, without explicit endpoint detection. The accumulated distances are divided by their path lengths to normalize for templates of different duration. The minimum normalized distance specifies the distance to each template, and the minimum template distance determines the word recognized.</p><p>The data were digitally recorded from a single talker in a sound-isolated room using a close-talking microphone. Fig. <ref type="figure" target="#fig_11">12</ref> shows the degradation of recognition performance in background noise across the four front ends: LPCC, MFCC, dynamic model (MFCCA), and the dynamic model with peak isolation (MFCCAP). Consistent with <ref type="bibr" target="#b2">[3]</ref>, the MFCC is more robust than LPCC. However, both the dynamic model MFCCA and the dynamic model with spectral peak isolation MFCCAP are significantly more robust to background noise than MFCC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. HMM Evaluations</head><p>Using the male talkers in the TI-46 data base and the HTK-Toolkit, a series of talker-independent HMM-based robust digit isolation and recognition evaluations were also conducted. The TI-46 database is hand segmented so that words are placed in the center of each file. Before adding background noise to these files, random amounts of silence were added before and after each token. Two sets of evaluations were performed. The first used only clean data for training while the second trained both clean and noisy models.</p><p>For all models, six-states per word, simple left-to-right state transitions, continuous Gaussian densities, diagonal covariances, and fixed global variances were used. Mean feature vectors and transition probabilities for each state were trained as described below, but variances were set to the global variance estimated over all tokens in the training set. This technique is useful with limited training data and when the testing environment is significantly different from the training environment <ref type="bibr" target="#b2">[3]</ref>.</p><p>The clean models were trained in two stages. Training words were first isolated from the surrounding silence based on the total signal energy. The models were initialized assuming a uniform distribution of the words across the six states in the model. Iterative Viterbi (max-path) alignment and training was then applied until the average log probability decreased by less than a threshold. Finally, the forward-backward algorithm improved the estimate for each model using a similar convergence criterion.</p><p>When the test environment differs from the training environment, recognition performance deteriorates. A common approach to address this issue is to train models using noisy data <ref type="bibr" target="#b0">[1]</ref>. One set of clean models was built, as described above, and then a second set of "noisy models" was built using training data at an SNR of 12 dB. Both sets of models were used for recognition; the model with the highest probability (from either set) determined the word recognized. To train the noisy models, stationary background noise was added, and then forced-Viterbi alignment with the corresponding clean model was used to isolate the noisy speech from the background. The same Viterbi and forward-backward training algorithms, used for training clean models, were used to train noisy models from the isolated noisy words.</p><p>For Viterbi alignment in training and recognition, silence models were used together with a "grammar" of silenceword-silence. In a fixed-variance system, the silence models were simply the long-term moving average of the front end's response to the background noise. As the SNR changed, the silence model's mean updated to the new background noise.</p><p>Fig. <ref type="figure" target="#fig_12">13</ref>(a) shows the increasing error rate at lower SNR's for the different front ends. Each front end was evaluated with and without the peak isolation mechanism. The dynamic model MFCCA by itself shows no improvement over standard MFCC, however, adaptation improves the robustness of MFCCP, and MFCCAP remains the most robust front end. Isolating peaks is helpful, but isolating changing peaks is perhaps more helpful.</p><p>There are at least two reasons to expect the performance of the dynamic model to degrade when using HMM-based recognition in a noisy environment. The dynamic model provides a context-dependent response that may increase differences between onset responses in clean and noisy environments. However, MFCCA improves DTW performance over MFCC. The difference may be that in the DTW system, templates are continuously varying over the utterance. The HMM system requires discretizing the variation over the utterance into a finite number of states. The nonstationary response of the dynamic model (as seen in Fig. <ref type="figure" target="#fig_10">11</ref>) may not be as well-suited to segmentally stationary statistical characterization as the MFCC representations; intrasegment changes are reduced to averages. Fig. <ref type="figure" target="#fig_12">13</ref>(b) shows the evaluation using models of both clean and noisy data. Performance across all front ends improves, and MFCCAP continues to provide the most robust representation. This evaluation removes some of the context-dependent mismatch between training and testing. Fig. <ref type="figure" target="#fig_12">13(c</ref>) and (d) compare the performance using the dynamic mechanism and the perceptually motivated RASTA technique <ref type="bibr" target="#b18">[18]</ref>. RASTA involves filtering the logarithmic temporal trajectories (log energy temporal excitation patterns) with a bandpass filter that has a sharp zero at DC. By deemphasizing slow and fast changes with time, RASTA also provides an adapting response. Both front ends were evaluated with and without the peak-isolation algorithm. Fig. <ref type="figure" target="#fig_12">13(c</ref>) shows the performance with clean models, and Fig. <ref type="figure" target="#fig_12">13(d</ref>) compares the performance with clean and noisy models. Our dynamic mechanism is more robust for these tasks. In this comparison, the RASTA technique was applied directly to the logarithmic filter energies, without the perceptual linear prediction (PLP) processing used in its original optimization <ref type="bibr" target="#b18">[18]</ref>. The "standard" RASTA filter was used and performance was not compared with other RASTA variations that optimize the compressive and expansive nonlinearities for the specific acoustic environment.</p><p>Spectral estimations on a perceptual frequency scale (MFCC) are more robust than those on a linear scale (LPCC). Adaptation provides sensitivity to onsets, enhancing spectral contrast in time. Unlike the RASTA technique, which can be described as a (smoothed) first-order differentiation, the dynamic model proposed here does not provide zero output for constant input. Instead, the adaptation stages converge to static targets on the I/O curves. Also unlike the RASTA technique, recovery is roughly three times slower than attack. Finally, peak isolation enhances spectral contrast in frequency. The combination of adaptation and peak isolation provides a spectral estimation sensitive to changing local spectral peaks, enhancing the representation of speech in a noise background. The dynamic mechanism with peak isolation (MFCCAP) reduced the word recognition error in background noise by a factor of two to three over common (MFCC) front ends in each of these evaluations, and provided an improvement over the RASTA technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>Current speech recognition systems use a simplified auditory model to transform a temporal pressure wave into a sequence of spectral estimations. Specifically, ASR front ends approximate auditory frequency selectivity and magnitude compression. This paper provides two simple nonlinear mechanisms that extend the front end to include adaptation and sensitivity to the frequency location of local spectral peaks. These mechanisms impose additional computational requirements roughly equal to that of the common ASR front end. Forward-masking data parameterize the adapta-tion mechanisms. Using additive exponential adaptation after logarithmic conversion, the dynamic mechanism predicts a nearly linear decay of the amount of forward masking (in decibels) as a function of logarithmic probe delay, and faster rates of decay of forward masking from more intense forward maskers. The output is below threshold when forward masking is predicted to occur (a decision device is not used), allowing for direct connection to current recognition systems. The peak isolation mechanism is an extension of raised-sine cepstral liftering. Together with the common MFCC front end, these mechanisms imply an auditory system with frequency selectivity and magnitude compression that is highly sensitive to onsets, transitions, and changing local spectral peaks. Each of these mechanisms improves the noise-robustness of a simple word recognition system. Together they reduce the error rate by a factor of two to three over an MFCC front end.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>A</head><label></label><figDesc>Model of Dynamic Auditory Perception and Its Application to Robust Word Recognition Brian Strope and Abeer Alwan, Member, IEEE Abstract-This paper describes two mechanisms that augment the common automatic speech recognition (ASR) front end and provide adaptation and isolation of local spectral peaks. A dynamic model consisting of a linear filterbank with a novel additive logarithmic adaptation stage after each filter output is proposed. An extensive series of perceptual forward masking experiments, together with previously reported forward masking data, determine the model's dynamic parameters. Once parameterized, the simple exponential dynamic mechanism predicts the nature of forward masking data from several studies across wide ranging frequencies, input levels, and probe delay times. An initial evaluation of the dynamic model together with a local peak isolation mechanism as a front end for dynamic time warp (DTW) and hidden Markov model (HMM) word recognition systems shows an improvement in robustness to background noise when compared to Mel-frequency cepstral coefficients (MFCC), linear prediction cepstral coefficients (LPCC), and relative spectra (RASTA) based front ends. Index Terms-Dynamic auditory perception, forward masking, robust speech recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Forward-masking stimuli. (a) Large time-scale view of a single 2AFC trial. (b) Fourier transform of the probe signal (128 ms rectangular window). (c) Smaller time-scale view of the probe following the masker by 15 ms.</figDesc><graphic coords="2,113.10,206.22,373.92,120.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Average forward masking data (circles), and standard deviation (error bars), together with the model fit (lines) as a function of masker level across five octaves, with probe delay of 15, 30, 60, and 120 ms as parameter.</figDesc><graphic coords="3,117.30,55.02,365.52,441.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Average forward masking data at 1 kHz: (a) as a function of the log delay with contours for constant masker levels and (b) as the dynamic range below masker as a function of the masker level with contours for constant probe delays. The dotted line reflects the probe threshold in quiet.</figDesc><graphic coords="4,43.98,59.58,249.36,107.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) Prototypical I/O curve for a single channel in the dynamic model. Schematic output trajectories corresponding to a level change at three different rates for (b) decreasing inputs from 80 to 30 dB SPL, and (c) increasing inputs from 30 to 80 dB SPL.</figDesc><graphic coords="5,99.30,220.14,401.52,168.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Geometry to derive recovery (upward adaptation) parameters from forward-masking thresholds.</figDesc><graphic coords="6,47.52,59.58,242.16,178.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Geometry to derive attack (downward adaptation) parameters from forward-masking thresholds as a function of masker duration.</figDesc><graphic coords="6,310.80,59.58,241.68,200.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. Model's prediction of the decay of forward masking as a function of masker level at 1 kHz with (a) linear time reference and (b) logarithmic time reference.</figDesc><graphic coords="7,119.10,251.94,361.92,135.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Using the model to predict other forward masking data. (a) Wideband masker and probe [20]. (b) Wideband masker, sinusoidal probe at 1 kHz [24]. (c) Sinusoidal masker and probe at 1 kHz [19]. (d) Equation provided in [19] predicting the present data.</figDesc><graphic coords="8,237.00,172.02,126.24,90.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Peak isolation processing. Log spectrum of the vowel =i= after (a) cepstral truncation, (b) raised-sine cepstral liftering, and (c) half-wave rectification and peak normalization.</figDesc><graphic coords="10,68.16,55.02,463.92,130.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Spectrogram representations of the words "nine six one three" from MFCC analysis, the dynamic model MFCCA, and the dynamic model with peak isolation MFCCAP, at greater than 40 dB SNR and at 5 dB SNR.</figDesc><graphic coords="11,52.02,54.98,497.00,352.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Talker-dependent DTW recognition performance in noise.</figDesc><graphic coords="12,138.24,55.02,323.76,210.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Talker-independent HMM comparisons: LPCC, MFCC, and MFCCA with (a) clean models, (b) clean and noisy models. MFCCA and RASTA with (c) clean models, (d) clean and noisy models. (+) indicates with peak isolation, () indicates without peak isolation.</figDesc><graphic coords="13,51.24,249.84,497.76,180.24" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Manuscript receivedSeptember 6, 1995; revised November 12, 1996. This work was supported in part by NIDCD Grant DC02033 and by the NSF. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Dennis R. Morgan.The authors are with the Electrical Engineering Department, University of California, Los Angeles, CA 90095 USA (e-mail: bps@ucla.edu; alwan@icsl.ucla.edu).Publisher Item Identifier S 1063-6676(97)06390-6.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors thank Dr. D. Morgan and four anonymous reviewers for their helpful suggestions on a previous version of this manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Juang</surname></persName>
		</author>
		<title level="m">Fundamentals of Speech Recognition</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="357" to="366" />
			<date type="published" when="1980-08">Aug. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comparison of signal processing front ends for automatic word recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Jankowski</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoang-Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><surname>Lippman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="286" to="293" />
			<date type="published" when="1995-07">July 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Effects of masker duration in pure-tone forward masking</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kidd</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Feth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="1364" to="1386" />
			<date type="published" when="1982-11">Nov. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the use of bandpass liftering in speech recognition</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Wilpon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="947" to="954" />
			<date type="published" when="1987-07">July 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Short-term adaptation and incremental responses of single auditory-nerve fibers</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Zwislocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Cybern</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="169" to="182" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Forward masking of auditory nerve fiber responses</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dallos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neural Physiol</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1083" to="1107" />
			<date type="published" when="1979-07">July 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A fast motile response in guinea-pig outer hair cells: the cellular basis of the cochlear amplifier</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ashmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Physiol</title>
		<imprint>
			<biblScope unit="volume">388</biblScope>
			<biblScope unit="page" from="323" to="347" />
			<date type="published" when="1987-07">July 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speech coding in the auditory nerve: IV. Sounds with consonant-like dynamic characteristics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Delgutte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Y S</forename><surname>Kiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="897" to="907" />
			<date type="published" when="1984-03">Mar. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the role of spectral transition for speech perception</title>
		<author>
			<persName><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1016" to="1025" />
			<date type="published" when="1986-10">Oct. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Auditory patterns</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Rev. Modern Phys</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="47" to="65" />
			<date type="published" when="1940-01">Jan. 1940</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Application of an auditory model to speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="2623" to="2629" />
			<date type="published" when="1989-06">June 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Representation of consonants in the peripheral auditory system: A modeling study of the correspondence between response properties and phonetic features</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Goldhor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mass Inst. Technol</title>
		<imprint>
			<biblScope unit="volume">505</biblScope>
			<date type="published" when="1985">1985</date>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
	<note>RLE Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A computational model of filtering, detection, and compression in the cochlea</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Lyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1982-05">May 1982</date>
			<biblScope unit="page" from="1282" to="1285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Computational models of neural auditory processing</title>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ieee Icassp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984-03">Mar. 1984</date>
			<biblScope unit="page" from="36" to="37" />
			<pubPlace>San Diego, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A joint synchrony/mean-rate model of auditory speech processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seneff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phonet</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="55" to="76" />
			<date type="published" when="1988-01">Jan. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Noise robust speech recognition using a dynamic-cepstrum</title>
		<author>
			<persName><forename type="first">K</forename><surname>Aikaiwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICSLP</title>
		<meeting>ICSLP<address><addrLine>Yokohama, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-09">Sept. 1994</date>
			<biblScope unit="page" from="1579" to="1582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">RASTA processing of speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="578" to="589" />
			<date type="published" when="1994-10">Oct. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Forward masking as a function of frequency, masker level, and signal delay</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jesteadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="950" to="962" />
			<date type="published" when="1982-04">Apr. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rate of decay of auditory sensation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Plomp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="277" to="282" />
			<date type="published" when="1964-02">Feb. 1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analytical expressions for critical-band rate and critical bandwidth as a function of frequency</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Terhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="1523" to="1525" />
			<date type="published" when="1980-11">Nov. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transformed up-down methods in psychoacoustics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Levitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="467" to="477" />
			<date type="published" when="1971-02">Feb. 1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Basilar membrane measurements and the travelling wave</title>
		<author>
			<persName><forename type="first">B</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Patuzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hearing Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="147" to="153" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Growth of forward masking for sinusoidal and noise maskers as a function of signal delay; implications for suppression in noise</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C J</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Glasberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1249" to="1259" />
			<date type="published" when="1983-04">Apr. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling the additivity of nonsimultaneous masking</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Oxenham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C J</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hearing Res</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="105" to="118" />
			<date type="published" when="1994-10">Oct. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Peripheral origins of the upward spread of masking</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Oxenham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Plack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page">2542</biblScope>
			<date type="published" when="1996-04">Apr. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A model of temporal integration and forward masking for electrical stimulation of the auditory nerve</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Shannon</surname></persName>
		</author>
		<editor>J. M. Miller and F. A. Spelman</editor>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="187" to="205" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>in Cochlear Implants: Models of the Electrically Stimulated Ear</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A quantitative model of the &apos;effective&apos; signal processing in the auditory system, I: Model structure</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pueschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="3615" to="3622" />
			<date type="published" when="1996-06">June 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A quantitative model of the &apos;effective&apos; signal processing in the auditory system, II: Simulations and measurements</title>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="3623" to="3631" />
			<date type="published" when="1996-06">June 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Temporal summation of loudness: An analysis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Zwislocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="431" to="441" />
			<date type="published" when="1969-02">Feb. 1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-normalization and noise-robustness in early auditory representations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="412" to="435" />
			<date type="published" when="1994-07">July 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Auditory nerve representations as a basis for speech processing</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ghitza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Speech Processing</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Furui</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sondhi</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Marcel Dekker</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="453" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Speech coding in the auditory nerve: I, Vowel-like sounds</title>
		<author>
			<persName><forename type="first">B</forename><surname>Delgutte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Y S</forename><surname>Kiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="866" to="878" />
			<date type="published" when="1984-03">Mar. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Prediction of perceived phonetic distance from critical-band spectra: a first step</title>
		<author>
			<persName><forename type="first">D</forename><surname>Klatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1982-05">May 1982</date>
			<biblScope unit="page" from="1278" to="1281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The National Acoustic Laboratories&apos; (NAL) new procedure for selecting the gain and frequency response of a hearing aid</title>
		<author>
			<persName><forename type="first">D</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ear Hearing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="257" to="265" />
			<date type="published" when="1986-08">Aug. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Los Angeles (UCLA) as an Assistant Professor, where she established the Speech Processing and Auditory Perception Laboratory. In 1996, she became an Associate Professor. Her research interests include modeling speech production and perception mechanisms and applying these models in speech coding, synthesis, and recognition systems. Dr. Alwan is the recipient of the NSF Research Initiation Award (1993), the NSF Career Development Award</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Glasberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C J</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the NIH FIRST Career Development Award (1994), and the UCLA-TRW Excellence in Teaching Award</title>
		<meeting><address><addrLine>Providence, RI; Sigma Xi</addrLine></address></meeting>
		<imprint>
			<publisher>Tau Beta Pi, and the New York Academy of Sciences</publisher>
			<date type="published" when="1989">Aug. 1990. 1989. 1992. 1995. 1994</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="103" to="138" />
		</imprint>
		<respStmt>
			<orgName>Electrical Engineering Department, University of California</orgName>
		</respStmt>
	</monogr>
	<note>She is a member of Eta Kappa Nu. She is a member of the Acoustical Society of America Technical Committee on Speech Communication, and the IEEE Signal Processing Technical Committees on Audio and Electroacoustics and on Speech Processing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
