<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Incremental Learning of Concept Drifts Using Evolving Type-2 Recurrent Fuzzy Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">An Incremental Learning of Concept Drifts Using Evolving Type-2 Recurrent Fuzzy Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3BE6E4BBE1E51CA18FF3C7FB32105504</idno>
					<idno type="DOI">10.1109/TFUZZ.2016.2599855</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TFUZZ.2016.2599855, IEEE Transactions on Fuzzy Systems This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TFUZZ.2016.2599855, IEEE Transactions on Fuzzy Systems This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TFUZZ.2016.2599855, IEEE Transactions on Fuzzy Systems</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>evolving fuzzy systems</term>
					<term>fuzzy neural networks</term>
					<term>recurrent fuzzy neural networks</term>
					<term>type-2 fuzzy systems</term>
					<term>incremental learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the age of online data stream and dynamic environments result in the increasing demand of advanced machine learning techniques to deal with concept drifts in large data streams. Evolving Fuzzy Systems (EFS) are one of recent initiatives from the fuzzy system community to resolve the issue. Existing EFSs are not robust against data uncertainty, temporal system dynamics, and the absence of system order, because vast majority of EFSs are designed in the type-1 feed-forward network architecture. This paper aims to solve the issue of data uncertainty, temporal behaviour, and the absence of system order by developing a novel evolving recurrent fuzzy neural network, called Evolving Type-2 Recurrent Fuzzy Neural Network (eT2RFNN). eT2RFNN is constructed in a new recurrent network architecture, featuring double recurrent layers. The new recurrent network architecture evolves a generalized interval type-2 fuzzy rule, where the rule premise is built upon the interval type-2 multivariate Gaussian function, whereas the rule consequent is crafted by the non-linear wavelet function. The eT2RFNN adopts a holistic concept of evolving systems, where the fuzzy rule can be automatically generated, pruned, merged and recalled in the single pass learning mode. eT2RFNN is capable of coping with the problem of high dimensionality, because it is equipped with online feature selection technology. The efficacy of eT2RFNN was experimentally validated using artificial and real-world data streams and compared with prominent learning algorithms. eT2RFNN produced more reliable predictive accuracy, while retaining lower complexity than its counterparts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Online data streams and dynamic environments are the two most challenging research problems in today's real-world applications <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr">[44]</ref>, <ref type="bibr" target="#b45">[45]</ref>, <ref type="bibr" target="#b55">[55]</ref>. Every day, a massive amount of data streams are generated from sensors, the internet, etc. These data streams usually do not follow static and predictable data distributions, because they are influenced by rapidly changing statistical parameters. The Evolving Fuzzy System (EFS) <ref type="bibr" target="#b28">[29]</ref> has gained increasing popularity in the machine learning community, because it answers an urgent demand for 'learning from non-stationary data streams'. EFS answers desiderata for data stream mining for four reasons: 1) EFS works in the truly sequential (or one-pass) learning scenario <ref type="bibr" target="#b0">[1]</ref>, which is capable of processing large data streams on the fly <ref type="bibr" target="#b1">[2]</ref>; 2) EFS is capable of self-organising fuzzy rules in accordance with its relevance to the current learning environment <ref type="bibr" target="#b46">[46]</ref>; 3) EFS has a built-in rule growing procedure, which is capable of automatically generating a new fuzzy rule when encountering new knowledge <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b47">[47]</ref>. This learning module is equivalent to a drift detection scenario in conventional machine learning, because it monitors the conflict level induced by a data stream;4) EFS is equipped by rule base simplification modules: rule growing, pruning, etc, which warrants the rule base complexity at a low level and avoids the overfitting case. EFS deserves more in-depth investigation, because existing EFSs are mostly built upon the type-1 fuzzy system, which features a crisp and certain characteristic <ref type="bibr" target="#b3">[4]</ref>. This makes the EFS not robust to cope with information uncertainty as a result of noisy measurement, noisy data, and disagreement of expert knowledge <ref type="bibr" target="#b4">[5]</ref>. Furthermore, vast majority of the EFS are constructed in the feed-forward network architecture, which cannot properly handle the temporal system dynamic and requires system order to be known in order for the I/O relationship of the regression model to be defined.</p><p>We propose a novel Evolving Type-2 Recurrent Fuzzy Neural Network (eT2RFNN), which is capable of resolving all three underlying issues in an online mode: uncertainty, temporal system dynamic, and system order. eT2RFNN constitutes a fully evolving and adaptive learning algorithm, that can initiate its learning process from scratch with an empty rule base and automatically add, prune, merge its fuzzy rules from data streams afterwards. eT2RFNN borrows three learning modules proposed in <ref type="bibr" target="#b16">[17]</ref> : 1) The rule growing process is governed by the Type-2 Data Quality (T2DQ) method; 2) the parameter learning scenario relies on the Fuzzily Weighted Generalized Recursive Least Square (FWGRLS) method; 3) the rule merging process is done by virtue of the Extended Vector Similarity (EVS) concept. Meanwhile, eT2RFNN conveys four novel learning components as follows:</p><p>• A Novel Recurrent Network Architecture: eT2RFNN puts forward a new recurrent network topology, which presents a double local recurrent connection at both rule layer <ref type="bibr" target="#b11">[12]</ref> and consequent layer <ref type="bibr" target="#b17">[18]</ref>. The recurrent connection at the rule layer functions to delineate the temporal firing strength of the model by memorising the membership degree of the previous sample, whereas the recurrent link at the consequent layer aims to expedite the dynamic of the Wavelet function. To the best our knowledge, existing recurrent fuzzy neural networks still make use of a single recurrent layer in the form of local, global or interactive feedback.</p><p>• A Generalized Interval Type-2 Fuzzy Rule: A generalised interval type-2 fuzzy rule is proposed in this paper, which employs a multivariate Gaussian function in the input part and utilises a nonlinear Wavelet function in the output part. The main difference between this rule and that of <ref type="bibr" target="#b16">[17]</ref> can be seen in the rule consequent, where the nonlinear Wavelet function is exploited. This amendment is to address the demanding memory demand of the Chebyshev polynomial <ref type="bibr" target="#b17">[18]</ref>. This fuzzy rule can be perceived as an extension of <ref type="bibr" target="#b9">[10]</ref>, because it triggers arbitrarily rotated ellipsoidal rule instead of the ellipsoidal rule in the main axes.</p><p>• Type-2 Relative Mutual Information (T2RMI) method: A novel rule pruning method, namely the T2RMI method, is proposed. The T2RMI method is capable of detecting inconsequential fuzzy rules by scrutinising the correlation between the fuzzy rules and the target variables. The original version of the RMI method was originally initiated in <ref type="bibr" target="#b17">[18]</ref>. In this paper, the T2RMI method is extended to suit the working principle of a type-2 fuzzy system. Furthermore, the T2RMI method adopt a different concept from the utility method <ref type="bibr" target="#b19">[20]</ref>, popular in the literature. The significance of a rule is derived from the issue of relevance of a rule to a target variable, whereas the utility method relies on the percentage of usage of a rule during its lifespan.</p><p>• Sequential Markov Blanket Criterion (SMBC): Several online feature selection scenarios have been designed for EFS <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b48">[48]</ref>. All of which are based on the relevance of input attributes, and discount possible redundancy issues among input attributes. In this paper, a novel online feature selection, namely SMBC, is put forward, which depicts the online version of MBC <ref type="bibr" target="#b21">[22]</ref>. The SMBC takes into account both relevance and redundancy facets in determining the importance of input features.</p><p>To sum up, this paper proposes a solution to address the issue of uncertainty, temporal system dynamic, absence of system order simultaneously in learning from large data streams. Major contribution of this paper can be found in four aspects:1) a generalized recurrent network architecture with double self-feedback loops at rule and consequent layers are put forward, 2) a new type of the interval type-2 fuzzy rule is presented, where the rule premise is crafted by the interval type-2 multivariate Gaussian function with uncertain means, while the rule consequent is driven by the nonlinear Wavelet function, 3) this paper also offers two novel learning components of the EFS, the T2RMI method for the rule pruning process and the SMBC method for the online feature selection mechanism, 4) The universal approximation capability of eT2RFNN is also investigated. It is well-known that the type-2 fuzzy system is essentialy a universal approximator, because it is a modified version of the type-1 fuzzy system. An open issue is, however, in the case of the recurrent fuzzy neural network, whose intertemporal dependency is to the best of our knowledge uncharted in a standard universal approximation theorem of the fuzzy neural network. This paper discusses the universal approximation capability of the eT2RFNN by extending the result of <ref type="bibr" target="#b59">[59]</ref> for the single recurrent connection architecture to the double recurrent connection architecture. The efficacy of the eT2RFNN is experimentally validated using various realworld and synthetic data streams and are also benchmarked with state-of-the art EFSs, where eT2RFNN demonstrates the most encouraging numerical results in both complexity and accuracy. The remainder of this paper is organised as follows: Section 2 deliberates the network architecture of eT2RFNN; Section 3 elaborates the learning policy of eT2RFNN and also the complexity analysis; Section 4 details the numerical studies and the experiments based comparisons with its counterparts; Conclusions are drawn in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS A. Type-2 Fuzzy System</head><p>Real-world data, by their nature, contain bias, noise, abnormalities due to faulty sensors, etc., resulting in a high degree of uncertainty. Because training data do not truly represent system dynamic, an accurate paremeter identification strategy as necessitated in the type-1 fuzzy system becomes an extremely difficult task. In light of this issue, The type-2 fuzzy system <ref type="bibr" target="#b4">[5]</ref> characterises a fuzzy membership via a so-called fuzzy-fuzzy set, which provides some tolerance against information uncertainty. The issue of complexity hinders the viability of the type-2 fuzzy system in online real-time situations. In addition, it cannot be handled by well-known type-1 fuzzy mathematics. This drawback has led to the notion of the interval type-2 fuzzy system <ref type="bibr" target="#b5">[6]</ref>, which presents a simplified version of the type-2 fuzzy system. The interval type-2 fuzzy system assumes the secondary grade of a membership function as unity to enable the use of the type-1 fuzzy mathematics. Three configurations of non-evolving interval type-2 fuzzy system and their learning algorithms were put forward in <ref type="bibr" target="#b50">[50]</ref>. The interval type-2 fuzzy system concept has been implemented in an evolving network structure in <ref type="bibr" target="#b6">[7]</ref>. However, the initial version of the interval type-2 fuzzy neural network is overly dependent on the Karnik-Mendel (KM) type reduction method, computationaly prohibitive, because the rule consequent has to be first reordered in an ascending manner and then the KM method is carried out to find the L and R end points <ref type="bibr" target="#b7">[8]</ref>. To this end, the construct of a q coefficient was proposed in <ref type="bibr" target="#b8">[9]</ref> to perform the type reduction scenario in lieu of the KM method. This method has been incorporated into the scope of EFS in <ref type="bibr" target="#b9">[10]</ref>. The concept of interval type-2 fuzzy system has gained tremendous success in various real-world applications: edge detection <ref type="bibr" target="#b51">[51]</ref>, time-series prediction <ref type="bibr" target="#b52">[52]</ref>, intelligent agent in ambient environments <ref type="bibr" target="#b53">[53]</ref>, congestion control in the video streaming across IP networks <ref type="bibr" target="#b54">[54]</ref>. However, the interval type-2 EFS is still an open research area because of three technical flaws: 1) it is constructed in the feed-forward network architecture, which cannot properly model temporal behaviour and is sensitive to the number of delayed input attributes; 2) it endures the absence of a rule base simplification method, which may result in costly pre-and/or post-training steps; 3) vast majority of existing works have not incorporated an online feature selection scenario and thus usually treat feature selection as a pre-processing step. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Recurrent Neural Networks</head><p>In contrast to its feedforward counterpart, the recurrent network architecture is fitted out by an internal memory component in the form of a feedback or recurrent layer <ref type="bibr" target="#b10">[11]</ref>, which offers a plausible solution to surmount the temporal property of the data streams. This component can store the previous characteristics of the system dynamic, and is thereby able to handle the temporal system dynamic reliably. This appealing trait is also capable of overcoming the absence of system order, because the internal memory component can feed the past behaviour of the system in lieu of the lagged input and/or output variables in the I/O relationship of the regression model. In general, recurrent fuzzy neural networks can be classified into three types with respect to their recurrent layers: local <ref type="bibr" target="#b11">[12]</ref>, global <ref type="bibr" target="#b12">[13]</ref>, and interactive <ref type="bibr" target="#b13">[14]</ref>. In our opinion, the local recurrent layer is the most suitable one for the EFS, because it imposes the lowest complexity and is applicable in the local learning context <ref type="bibr" target="#b14">[15]</ref>. Recently, the notion of the recurrent network topology has been integrated into the realm of the interval type-2 fuzzy neural network in <ref type="bibr" target="#b15">[16]</ref>. These works, however, rely on the KM iterative procedure, which is computationally expensive Table <ref type="table" target="#tab_0">1</ref> summarises key characteristics of 9 related works, reviewed in Section 2.</p><p>III. NETWORK ARCHITECTURE OF ET2RFNN In this section, the network architecture of eT2RFNN is outlined. The eT2RFNN puts forward a novel recurrent network architecture to produce its final output. The unique property of the network topology lies on double local recurrent links at both rule and consequent layers. In essence, the recurrent property allows a model to store previous information, thus being more robust to temporal system dynamics <ref type="bibr" target="#b10">[11]</ref>. It also resolves the bottleneck in ascertaining the system order of the model. It is worth noting that, in the feed-forward fuzzy neural network, the output is often the function of previous input and/or output. On the other hand, the use of the wavelet function in the rule consequent can retard the reaction speed against rapidly changing data distributions. The decline is compensated for by the recurrent connection in the rule consequent. Several variants of recurrent networks have been published: interactive <ref type="bibr" target="#b13">[14]</ref>, global <ref type="bibr" target="#b12">[13]</ref>, and local <ref type="bibr" target="#b11">[12]</ref>. The local recurrent connection is utilised in the eT2RFNN, because it still retains the local learning trait, where each fuzzy rule is trained separately and the learning process of one rule has no effect on the stability of other rules. It is worth mentioning that a single recurrect connection is usually enough to induce an internal memory component property of the network architecture. The eT2RFNN is , however, equipped with the second recurrent link in the consequent layer to further strengthen robustness against the temporal problem and importantly to increase sensitivity of the Wavelet function against changing operating conditions <ref type="bibr" target="#b17">[18]</ref>. Fig. <ref type="figure">1</ref> shows the network architecture of the ET2RFNN.</p><p>The network architecture generates a generalized interval type-2 fuzzy rule. It utilises the interval type-2 multivariate Gaussian function with uncertain means in the hidden layer, and the nonlinear wavelet function in the consequent layer. In short, the fuzzy rule is defined as follows:</p><formula xml:id="formula_0">i R : IF X is i R ~Then o i i i o i z y , ) ( ~   , ] , [ ~, , , o i o i o i     (1) where ] , [ ~i i i R R R </formula><p>stands for a multidimensional kernel with uncertain means written as follows:</p><formula xml:id="formula_1">) ) ( ) ( exp( ~1 T i n i i n i C X C X R       ] , [ ~2 , 1 , i i i C C C  (2)</formula><p>where</p><formula xml:id="formula_2">u i i i C C C     1 ] ,</formula><p>[ ~denotes the uncertain centroids of the i-th rule and u is the number of input dimensions. Because the interval type-2 fuzzy set with uncertain means is employed, the upper centroid is set larger than the lower centroid</p><formula xml:id="formula_3">i i C C  . u u i      1</formula><p>represents a non-diagonal inverse covariance matrix, whose elements pinpoint the interrelation of input variables, and in turn govern the orientation of ellipsoids. The multivariate Gaussian function induces more reliable input space partition, because the non-axis parallel ellipsoidal cluster is capable of covering arbitrary contours of data clouds <ref type="bibr" target="#b2">[3]</ref>. This merit enables suppression of the demand of fuzzy rules in the training process. On the other hand, the multivariate Gaussian rule possesses the scale-invariant trait and sustains the interrelation among input attributes, which vanishes under the conventional fuzzy rule using the product t-norm operator.</p><p>The multivariate Gaussian rule does not possess the fuzzy set representation. This fact affects to the rule transparency, because the fuzzy rule does not exhibit the atomic clause of human-like linguistic rule. In realm of the interval type-2 fuzzy system, the fuzzy set also plays crucial role in forming the Footprint of Uncertainty (FoU). Hence, a transformation strategy must be carried out to form the fuzzy set representation of the multivariate Gaussian rule. Since the centroid of the multivariate Gaussian function has the same expression in the fuzzy set level, our only focus is to solicit the radii of the Gaussian function in the main axes. To this end, the second method in <ref type="bibr" target="#b2">[3]</ref> is generalised for the interval type-2 fuzzy rule in the eT2RFNN, because it offers an instantaneous mechanism, although it is rather inaccurate in dealing with the ellipsoidal cluster rotated at 45 degrees. In a nutshell, the radius of the nonaxis ellipsoidal cluster is defined using the average cardinality principle as follows: </p><formula xml:id="formula_4">ii i i i r r    2 ) ( <label>(3)</label></formula><formula xml:id="formula_5">1 i i i i n i i n C C C C X C X      . m o i o i o i y y y     1 ] , [</formula><p>~is an interval consequent of the i-th local sub-model and is defined as</p><formula xml:id="formula_6">o i N i o i o i N i o i X y X y     ) ( , ) (   . m u i i i        ] ,</formula><p>[ ~which denotes the interval weight vector of the i-th rule, where ] ,..., [ 1</p><formula xml:id="formula_7">u i w w   , ] ,..., [ 1 u i w w  </formula><p>and m stands for the number of output dimension.</p><p>) ( i i z  stands for the extended input vector, stemming from a nonlinear mapping of the Wavelet function. Note that the standard zero or first TSK rule consequent does not fully explore the local output approximation aptitude. This flaw is overcome by exploiting the nonlinear Wavelet function. The wavelet function puts forward the multi-resolution property, which can capture useful information on various resolution levels. As a result, it substantiates the model's generalisation and reduces the number of fuzzy rules. We make use of the dilated and translated versions of the Mexican function <ref type="bibr" target="#b22">[23]</ref> as follows: indicates the dilation and translation parameters and j i d , labels the output of the local recurrent layer. P denotes the number of rules. The dilation and translation variables are not fixed, but rather adjusted to portray the different local behaviours of the real trend of the approximation curve. We do not rely on the concept of functional link consequent via the trigonometric or Chebyshev function <ref type="bibr" target="#b23">[24]</ref>, because this strategy imposes a prohibitive memory demand due to the higher DoF and is not adaptive due to the absence of the tuning mechanism to adjust its shape.</p><formula xml:id="formula_8">     u j i j i j i j i z z z 1 2 , 2 , , ) 2 exp( ) 1 ( ) (  , ) ( i i i i B A D Z   u P u P u i a a A       ] ,..., [ 1 , 1 , 1 , u P u P u i b b B       ] ,..., [ 1 , 1 , 1 , u P u P u i D D D       ] ,..., [ 1 , 1 , 1<label>(4</label></formula><formula xml:id="formula_9">) ( ) 1 ( ) ( ) ( , , , n n n x n d j i j i j j i      (5)</formula><p>where</p><formula xml:id="formula_10">j i,</formula><p> denotes the weight of the self-feedback link, which can be seen as a storage coefficient. The efficacy of the Wavelet function usually comes at cost of the rule interpretability, because an input vector is mapped to the Wavelet domainfrequency domain. This shortcoming is, slightly, alleviated in the eT2RFNN, because it adopts the local learning scheme. The local learning scheme provides some sort of rule transparency in terms of operating region of the rule consequent, where each rule consequent represents a specific area of the output space in the Wavelet domain.</p><p>The interval type-2 fuzzy inference scheme can be committed once eliciting the spread of the multivariate Gaussian function in the main axes, because an interval-valued membership can be produced by computing the upper and lower membership degrees as follows:</p><p>) ) ( exp( ~2 . , ,</p><formula xml:id="formula_11">j i j i j j i c x      , ] , [ ~2 , 1 , i j i j i c c c  (6)         ) ; ,<label>( 1 ) ; , ( , 2 , , 1 ,</label></formula><p>,</p><formula xml:id="formula_12">j j i i j j j i i j j i x c x c N    , i j j i j j i j i j j c x c x c c x 2 , 2 , 1 , 1 ,    (7)       ) ; ,<label>( ) ; , ( , 1 , , 2 ,</label></formula><p>,</p><formula xml:id="formula_13">j j i i j j j i i j j i x c N x c N    2 ) ( 2 ) ( 2 , 1 , 2 , 1 , i j i j j i j i j j c c x c c x     (8) Henceforth, the interval firing strength ] , [ ~i i i R R R </formula><p>, describing the matching factor or the completeness of fuzzy rule, is obtained by applying the t-norm operator as follows:</p><formula xml:id="formula_14">      u j u j j i i j i i R R 1 1 , , ,   (9)</formula><p>The rule firing strength is fed to the temporal firing strength layer, which is equipped with an internal feedback loop. The temporal firing strength layer results in the temporal firing strength</p><formula xml:id="formula_15">m o P i o i o i o i ,..., 1 , ,..., 1 ], , [ ~     </formula><p>, which is not only influenced by the current spatial firing strength but also caused by the past temporal firing strength as follows:</p><formula xml:id="formula_16">) 1 ( ) 1 ( ) ( ) (     n n R n o i o i i o i o i     , ) 1 ( ) 1 ( ) ( ) (     n n R n o i o i i o i o i     (10) where ] 1 , 0 [  o i </formula><p>denotes the weight of the recurrent layer. It is observed that the temporal firing strength is unique for the i-th rule of the o-th class to foster the temporal learning property of the network architecture. Henceforth, the type reduction mechanism is undertaken to transform the interval-valued set to the crisp variable, called the type-reduced set. The q design coefficients are utilised to perform the type reduction mechanism. The underlying rationale in using the q design factor rather than the KM method is that it offers computationally lighter burden and more flexible characteristics, where it can be directly embedded in the inference process. The type-reduced set, leading to the final crisp output, is written as follows.</p><formula xml:id="formula_17">           P k k P i o l q o i y i P k k P i o l q o i y i o l y 1 1 ) 1 ( , 1 1 , ,     , ) ( 2 1 , , o r o l o y y y   ,            P i k k P i o r q o i y i P k P i o r q o i y i o r y 1 1 , 1 1 ) 1 ( , ,    <label>(11)</label></formula><p>where ] , [ r l y y stands for the type-reduced set and o y denotes the final crisp output. The design coefficients are adjusted using the T2ZEDM method to govern the proportion of the upper and lower rules adaptively. The universal approximation characteristic of the eT2RFNN is mathematically verified here.</p><p>Remark: the normalisation term of eT2RFNN is amended from those of <ref type="bibr" target="#b8">[9]</ref>, because the normalisation formula </p><formula xml:id="formula_18">  P k A A 1 in [9]</formula><formula xml:id="formula_19">    ) ( ) ( sup x T x f U x</formula><p>, where  can be any norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This theorem implies that for any given target function over any compact time interval</head><formula xml:id="formula_20">] , [ N n n o </formula><p>, the eT2RFNN can output uniformly reliable prediction with error smaller than any given value of ε.This theorem can be easily proven using the Stone-Weierstrass theorem as done in the literature <ref type="bibr">[44]</ref>. Nevertheless, the Stone-Weierstrass theorem does not yet answer the intertemporal dependencies of the recurrent network architecture to the universal approximation criterioa, because it is mainly based on a static input/output mapping. Universal approximation capability of the recurrent network architecture with a specific case of the eT2RFNN is discussed. Compute the T2DQ method ( <ref type="formula">16</ref>) End For For i=1 to P* do compute the T2DQ method for P* rules() End For Determine the winning rule</p><formula xml:id="formula_21">) ( max arg ,..., 1 X R P win i P i   </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IF (17) Then If (27)</head><p>Recall the previously pruned fuzzy rule (28)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Else</head><p>Add a new rule ( <ref type="formula">18</ref>), ( <ref type="formula">19</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End If Else</head><p>Adapt the winning rule ( <ref type="formula">22</ref>)-( <ref type="formula">24</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RULE BASE MANAGEMENT OF ET2RFNN This section details the learning policy of the eT2RFNN.</head><p>A) The Rule Growing Module: The eT2RFNN is capable of automatically evolving the fuzzy rules utilising the T2DQ method. The T2DQ method performs knowledge exploratory mechanism by incrementally computing the density of all training data seen thus far and in turn recruits samples offering high generalisation potential and summarisation power. This method relies on the density-based approach, initiated in the substactive clustering approach <ref type="bibr" target="#b56">[56]</ref> and enhanced in the mountain clustering method <ref type="bibr" target="#b57">[57]</ref>. The potential method in <ref type="bibr" target="#b0">[1]</ref> forms an online version of the density-based approach and is modified in <ref type="bibr" target="#b25">[26]</ref>. The T2DQ method is developed in <ref type="bibr" target="#b16">[17]</ref> and establishes the type-2 version of the data quality method in <ref type="bibr" target="#b25">[26]</ref>. It is formulated as follows:</p><formula xml:id="formula_22">) 2 ) 1 ( 2 ) 1 ( ( 2 1 1 1 1 N N N N N N N N N N P g h b U U g h b U U FS          (16) 1 1     N N N FS U U ,      m u j P j N c b 1 2 1 , ) ( ,      m u j P j N c b 1 2 1 , 1 ) ( ,       m u j j N P j N z c h 1 , 1 1 , ,      m u j j N P j n z c h 1 1 , 1 , 1 , j N N j j N j N U x z z , 1 1 , , 1 ,              m u j j N N j j N N U x g j g 1 , 1<label>2 1 , , 1 ) ( , where 1</label></formula><formula xml:id="formula_23">,  N j</formula><p>x is a datum in the j-th input attribute received at N-</p><formula xml:id="formula_24">1 training observation and 1 , 1 , ,   P j P j c c</formula><p>stands for the centroid of hypothetical rule (P+1 st rule), assigned as</p><formula xml:id="formula_25">x x c N j P j    1 , , x x c N j P j    1 , . x</formula><p> is an uncertainty factor, determining the footprint of uncertainty of the rule, and is fixed as 0.1 for all our simulations here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N j</head><p>x is the latest incoming datum in the j-th coordinate. We apply the average cardinality principle in <ref type="bibr" target="#b15">(16)</ref> which supposes the upper and lower rules to contribute equally q=0.5. All recursive parameters are initialised as zero. We assign the weighting factor N U to diminish the outlier's leverage as put into perspective in <ref type="bibr" target="#b26">[27]</ref>. A training sample is appended as a new rule if it complies with the following criteria.</p><formula xml:id="formula_26">P i i P P i i P FS orFS FS FS ,..., 1 1 ,..., 1 1 ) min( ) max(       (<label>17</label></formula><formula xml:id="formula_27">)</formula><p>where i FS can be gained by replacing</p><formula xml:id="formula_28">1 , 1 , ,   P j P j c c in (16) with i j i j c c , , ,</formula><p>. The condition</p><formula xml:id="formula_29">P i i P FS FS ,..., 1 , 1 ) max(   </formula><p>is deemed to be noteworthy to enhance the summarisation power of the rule base, because it captures a highly dense region. On the other hand,</p><formula xml:id="formula_30">P i i P FS FS ,..., 1 , 1 ) min(   </formula><p>plays a crucial role to discover the shift of the system dynamic, because it indicates a datum uncharted by the influence zone of the existing rules. Although</p><formula xml:id="formula_31">P i i P FS FS ,..., 1 , 1 ) min(   </formula><p>opens the likelihood for outliers to be embedded as fuzzy rules, the eT2RFNN is equipped with a rule pruning scenario to overcome the problem. It is worthmentioning that the T2DQ method is different from the potential method <ref type="bibr" target="#b0">[1]</ref> in three facets: 1) the T2DQ method is equipped by the weighting factor, which helps to reduce the impact of outliers. As found in <ref type="bibr" target="#b26">[27]</ref>, outliers cause a large pairwise distance, affecting density of next training samples; 2) the T2DQ method defines the type-2 version of the density-based method, which is fit to the working principle of the type-2 fuzzy system; 3) the T2DQ method is built upon a different kernel function, where the inverse multi-quadratic function is used as an alternate of the Cauchy kernel.</p><p>Note that the rule growing scenario functions similarly with a drift detection strategy in the realm of machine learning, because it autonomously partitions the input space with respect to true data distribution. In comparison with other knowledge exploratory modules in the IT2EFS <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, the T2DQ method is more robust against outliers. It enumerates the distance between a new datum and other data seen thus far, whereas most IT2EFSs <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref> rely on the rule firing strength to generate fuzzy rules. In addition, the T2DQ method is threshold-freeno problem-dependent threshold needs to be predefined by the user. The parameters of a new fuzzy rule are stipulated: </p><formula xml:id="formula_32">) 5 . 0 ln( 1 )) ( ), max(( ) ( , ~1 1 1 1              i i i i P N P C C C C diag X X C (18)</formula><formula xml:id="formula_33">   2 ) ( i i R R . 5 . 0  </formula><p>is selected, because as investigated in <ref type="bibr" target="#b27">[28]</ref>, the degree of confidence should not be less than 0.5. We initialise the centre of a new fuzzy rule as that of the hypothetical rule</p><formula xml:id="formula_34">X C C X C C P P P P           1 1 1 1</formula><p>, and X  is fixed as 0.1. The  completeness criterion is attained using the setting of the covariance matrix as verified in <ref type="bibr" target="#b27">[28]</ref>. The new local submodel and output covariance matrix are assigned:</p><formula xml:id="formula_35">win P     1 I P    1 ~ , ] , [ ~1 1 1        P P P , ] , [ ~1 1 1     P P P    (19)</formula><p>where  denotes a large positive constant and is fixed as .</p><formula xml:id="formula_36">u u P P P        ] , [ ~1 1 1   </formula><p>is the new output covariance matrix. Setting a new output covariance matrix with <ref type="bibr" target="#b18">(19)</ref> is desired, because it is capable of emulating a real solution as attained by the batched learning scheme instantaneously <ref type="bibr" target="#b28">[29]</ref>. The new local sub-system is akin to the output covariance matrix of the winning rule because the winning rule is adjacent to the new rule in the context of local learning, thereby potentially being pertinent to the new rule.</p><p>The winning rule is selected by the Bayesian concept, where it focuses on a fuzzy rule having a maximum posterior probability</p><formula xml:id="formula_37">) ( max arg ,..., 1 X R P win i P i   </formula><p>. The underlying reason to choose the Bayesian concept in lieu of the omnipresent compatibility measure is because it extracts the winning rule in the probabilistic fashion. When there are several candidates lying on par to a newly received datum, the prior probability will endorse the most populated rule covering more samples. In a nutshell, the posterior probability, the prior probability, and the likelihood function are respectively expressed as follows:</p><p>1063-6706 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TFUZZ.2016.2599855, IEEE Transactions on Fuzzy Systems</p><formula xml:id="formula_38">) ) ( ) ( ˆ) ( ) ( ) ( ) ( ˆ) ( ) ( ( 2 1 ) ( 1 1       P i i i k k P i i i k k k R P R X p R P R X p R P R X p R P R X p X R P       P i i i i N N R P 1 ) 1 log( ) 1 log( ) ( ˆ, ) ) ( ) ( exp( ) 2 ( 1 ) ( ˆ1 2 1 2 1 T i i i i i C X C X V R X P        (<label>20</label></formula><formula xml:id="formula_39">)</formula><p>where i N labels the number of supports of the i-th cluster. The expression of the prior probability ) ( ˆi R P is softened from its original formula with the use of the log operation. This technique is to allow a new cluster to compete with older clusters in the winning rule selection phase <ref type="bibr" target="#b25">[26]</ref>. It may happen in the training process that a data point violates the rule growing condition <ref type="bibr" target="#b16">(17)</ref>. This condition triggers the rule premise adaptation as follows: </p><p>)) . The adaptation scenario in <ref type="bibr" target="#b21">(22)</ref> allows adjusting the inverse covariance matrix directly and overrides the requirement of reinversion <ref type="bibr" target="#b49">[49]</ref>. This mechanism leads to a more stable and faster adaptation. We exploit the midpoint of the interval valued Centre or implement the so-called average cardinality principle to adjust the inverse covariance matrix. B) Rule Pruning Mechanism and Rule Recall Scenario: A novel rule pruning scenario, namely the T2RMI method, is proposed in this paper, where the key idea is to check the correlation between the fuzzy rule and the target concept. Note that the underlying difference between the T2RMI method and the RMI method in <ref type="bibr" target="#b17">[18]</ref> can be seen in its incremental working framework. Furthermore, the original RMI method has been only applied to the T1EFS. The relationship between the two variables can be analysed using either a linear or nonlinear measure. The linear measure is, however, inaccurate, because the interaction between two variables is nonlinear in nature <ref type="bibr" target="#b30">[31]</ref>. To this end, we exploit the symmetric uncertainty method, which characterises three appealing traits: simplicity, low bias for multi-valued features, and insensitivity to the order of two variables <ref type="bibr" target="#b31">[32]</ref>. The T2RMI method is formulated using the average cardinality principle:</p><formula xml:id="formula_41">1 ( 1 ) 1 ( ))( 1 ( 1 ) 1 ( ( 1 1 1 ) 1 ( 1 ) (                               (22) 1 1    N win N win N N (23) where ) 1 ( 1 1    N win N </formula><formula xml:id="formula_42">) ( ) ( ) , ( ) ( ) ( ) , ( ) , ( Y H R H Y R I Y H R H Y R I Y R RMI i i i i i     (24) where ) , ( ) ( ) ( ) , ( Y R H Y H R H Y R I i i i   </formula><p>is the information gain or the mutual information. . We take this term from the popular definition in <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b58">[58]</ref>. There are several avenues to elicit the entropy. The discretisation method and the Parzen window estimation are among the most renowned methods. Both approaches however adopt a batched learning concept, which is computationally expensive. We use the notion of differential entropy, which assumes uniformly distributed training data <ref type="bibr" target="#b32">[33]</ref>: can be enumerated recursively. The fuzzy rule is pruned given that the following condition is satisfied. The mean and standard deviation are computed with respect to the RMI during its lifespan and recursively with ease. Condition <ref type="bibr" target="#b25">(26)</ref> can also be interpreted as a measure of significant downtrend in the correlation of the i-th rule and the target feature. The T2RMI method focusses on the relevance of a fuzzy rule for the target concept. This feature is important to capture the drift of the target concept, the most common type of concept drift. Once pruned, the fuzzy rule is not permanently forgotten, and is able to be reactivated when it is again valid with the current data concept 1 * *   P P . P* denotes the number of fuzzy rules pruned by <ref type="bibr" target="#b23">(24)</ref>. This scenario is an important strategy to respond recurring concept drift, because evolving a completely new fuzzy rule to deal with the previously learned concept undermines the logic of truly adaptive online systems and catastrophically omits the learning history. In a nutshell, the rule recall criterion is committed when the following criterion is met.</p><formula xml:id="formula_43">))) var( 2 log( 1 (<label>2</label></formula><formula xml:id="formula_44">1 ) ( i i R R H    , ) 1 log( 2 1 ) , ( 2 Y R i i Y R I     , ) var( ) var( ) , cov( Y R Y R i i Y R i   (25) where Y R i i i Y R R  ), ,<label>cov</label></formula><formula xml:id="formula_45">) ( 2 ) ( i i i RMI std RMI mean RMI   , N RMI RMI mean N n n i i   <label>1 , ) ( , 1 )) ( ( ) ( 1 2 ,</label></formula><formula xml:id="formula_46">) ( max ) ( max 1 ,.., 1 * * ,.., 1 * i P i i P i FS FS    <label>(27)</label></formula><p>The pruned fuzzy rule is maintained in memory to solely quantify <ref type="bibr" target="#b15">(16)</ref> and is ruled out in any other learning scenario. Accordingly, computational load is still relieved. The pruned fuzzy rule is reactivated by setting the pruned rule as a new rule as follows:</p><formula xml:id="formula_47">* 1 , * 1 , 1 * 1 1 , * 1 i P i P i P i C P C                 (28)</formula><p>C) Rule Merging Mechanism: Several rule merging methodologies have been designed in the context of interval type-2 EFS <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, where they are constructed by a shape-1063-6706 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TFUZZ.2016.2599855, IEEE Transactions on Fuzzy Systems based or distance-based similarity measure without synergising the two approaches. These methods cannot detect the problem of a non-homogenous cluster, where merging different orientation clusters causes a blow-up effect. This phenomenon is more noticeable in a non-axis parallel ellipsoidal cluster, because it enables the cluster to arbitrarily rotate in any direction. To remedy these bottlenecks, the eT2RFNN is equipped with a multi-faceted strategy: a vector similarity measure and a blow-up check. The vector similarity measure estimates the similarity between the two clusters based on their shape and proximity, while non-homogenous clusters are solved by the blow-up check.</p><p>The vector similarity measure enumerates the similarity between two fuzzy rules more accurately with the use of their distance and shape in one joint formula as follows: is the distance-based similarity measure between the winning rule and the i-th rule in the j-th axis. We merely investigate the similarity between the winning rule and other rules to expedite the model update, because the winning rule is the sole rule to receive the rule premise adaptation, the main reason for the rule overlapping. The similarity measure in the rule level is elicited by combining the fuzzy set similarity in each input dimension with the use of the t-norm operator: <ref type="bibr" target="#b29">(30)</ref> where stands for a predefined threshold set as in all our simulations in this paper. It is worth noting that this parameter is not problem-specific and has been confirmed in <ref type="bibr" target="#b16">[17]</ref>.</p><p>Since the vector similarity method treats shape and distance as two independent issues. The alignment procedure is carried out in the shape-based similarity measure, thereby being able to compare the contour of two fuzzy sets more precisely. That is, the centroid of the winning rule j win c , and the i-th rule j i c , is aligned as</p><formula xml:id="formula_48">j i j win c c , , </formula><p>. The shape-based similarity between the two rules is quantified with an extended Jaccard similarity measure, making use of the average cardinality principle: <ref type="bibr" target="#b30">(31)</ref> where  and  respectively stand for the union and intersection of the two fuzzy sets  </p><formula xml:id="formula_49">( 2 ) ( ) (<label>2</label></formula><formula xml:id="formula_50">)) (<label>( 2 ) ( , , , , 2 , , , , 2 2 ,</label></formula><p>, j i j win j i j win j win j i</p><formula xml:id="formula_51">j i j win j i j win h h h M                  <label>(32)</label></formula><p>where . We can execute (32) by performing similar mathematical operations for the upper fuzzy set and ) ( , , j i j win M    .On the other hand, the distance-based similarity measure is committed using the extended Kernel-based metric, where the type-1 version of the kernel-based metric was developed in <ref type="bibr" target="#b2">[3]</ref>. This method can be applied for the interval type-2 fuzzy system by virtue of the average cardinality principle as follows:</p><p>(33)</p><formula xml:id="formula_52">j i j win j i c j win c A , , 1 , 1 ,       j i j win j i c j win c B , , 2 , 2 ,      </formula><p>The extended kernel-based metric features the following properties.</p><formula xml:id="formula_53">B B B A B A B A j C C C C B A S               0 1 ) , ( , 2             B A B A j C C B A S ) , ( , 2</formula><p>After eliciting both the shape-and distance-based similarities in <ref type="bibr" target="#b30">(31)</ref>, <ref type="bibr" target="#b32">(33)</ref>, we finally arrive at the vector similarity measure formula <ref type="bibr" target="#b29">(30)</ref>.</p><p>Merging two different-orientation clusters should be avoided, because it harms the local data representation which affects generalisation potential. This step incurs an over-sized cluster, which leads to a cluster delamination case, in which some local data clouds are contained by a too-large cluster. In light of this issue, the blow-up check is undertaken by examining the volume of the merged cluster in contrast with their independent volumes. The rule merging scenario proceeds if the following condition is satisfied. ) ( ) ((</p><formula xml:id="formula_54">i win i win merged merged V V V V u V V      (34)</formula><p>Equation <ref type="bibr" target="#b34">(35)</ref> illustrates that the blow-up situation is unlikely to occur because the volume of the merged cluster does not exceed their independent volumes. We involve the term u in <ref type="bibr" target="#b33">(34)</ref> to hedge the curse of dimensionality.</p><p>If a training observation complies with (30), <ref type="bibr" target="#b33">(34)</ref>, the two fuzzy rules are merged as follows: </p><formula xml:id="formula_55">N N N       ~ , ] , [ ~i i i     (37)</formula><p>The rule merging process refers to the weighted average strategy, which weights the cluster as its accumulated supports. This strategy reflects the fact that a cluster with more populations should be more influential in crafting the final shape and orientation of the merged cluster, because the merged cluster should represent the underlying data distribution and foster cluster's populations. and lead to the curse of dimensionality. Therefore, online feature selection, which is capable of discarding spurious input features in the sample-wise manner during the training process, is highly demanded in the EFS to reduce both problem and network complexities. Online feature selection has been incorporated in the EFS encompassing the input pruning scenario <ref type="bibr" target="#b19">[20]</ref> or the input weighting concept <ref type="bibr" target="#b20">[21]</ref>. Existing methods merely focus on the issue of relevance, while overlooking redundancy problems. It may happen that an input variable is inconsequential, because it shares a strong similarity with other input attributes. Such input attributes can be pruned without significant loss of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D) Online Feature Selection Mechanism: high dimensional data poses a lot of challenges for most real-world applications</head><formula xml:id="formula_56">] 1 , 0 [ ) , ( , 1  i win s j u j j v v v s S S ,.., 1 , 3 ) min( ,     ] 1 , 0 [ 3   5 . 0 3   ) ( ) ( ) ( ) ( ) , ( , ,<label>, , , , , , , 1 j</label></formula><formula xml:id="formula_57">i j win j i j win j i j win j i j win j M M M M i win s                ) ( ) ( ) ( ) ( , , , , , , j i j win j i j win j i j win M M M M            (.) M     2 ) 2 ) ( exp( ) ( 1 , 2 2 , j win j win dx c x M         ] , 0 max[ x h  ) ( , , j i j win M    2 ) exp( ) exp( ) ,</formula><p>A new online feature selection approach, namely SMBC, is mounted in the eT2RFNN and presents a sequential version of MBC. The SMBC is inspired by the Markov Blanket Criterion <ref type="bibr" target="#b21">[22]</ref>, which synergises relevance and redundancy tests in determining the significance of input features. The SMBC corrects the instability drawback of conventional input pruning scenarios <ref type="bibr" target="#b19">[20]</ref>, because it assures that once observing the Markov Blanket Criterion in the previous episodes an input feature is no longer required for model updates in the future. Note that although the input weighting approach <ref type="bibr" target="#b20">[21]</ref> does not suffer from the instability problem, it still retains the superfluous feature in the training process.</p><p>According to the SMBC, an input attribute can be classified into 4 categories according to its contribution: irrelevant, weakly relevant, weakly relevant but non-redundant, and strongly relevant. An optimal input feature subset comprises strongly relevant features and weakly relevant but nonredundant features. Hence, the underlying goal of the SMBC is to find irrelevant and weakly relevant features in order to be pruned. Two tests, namely C-correlation and F-correlation, are developed to implement the SMBC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1(C-correlation) [22]:</head><p>The correlation between any feature j x and the class T is termed as C-correlation, denoted by</p><formula xml:id="formula_58">) , ( T x SU j</formula><p>. This strategy is used to probe relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2 (F-correlation) [22]: The correlation between any pair of features</head><formula xml:id="formula_59">i j x x , ) ( j i  is termed F-correlation, labelled by ) , ( i j x x SU</formula><p>. The F-correlation analyses redundancy.</p><p>First, the C-correlation is performed to vet the relevance of input attributes to the given problem. The input feature, which happens to be irrelevant or weakly relevant</p><formula xml:id="formula_60">  ) , ( T x SU j</formula><p>, can be pruned.  is a input pruning threshold, set as 0.8 according to the recommendation of <ref type="bibr" target="#b21">[22]</ref>. The SMBC continues with the F-correlation, examining the redundancy of input variables. Let j x be an input feature of interest and let</p><formula xml:id="formula_61">X M n  ) ( n j M x </formula><p>. n M is claimed to be a Markov Blanket for j x if it summarises not only the information of j</p><p>x to the target class T , but also almost all of the input features. Therefore, the redundant input features are those of the slightly relevant features having the Markov blanket n M . Specifically, these features are those of coming through the C-correlation test</p><formula xml:id="formula_62">  ) , ( T x SU j but violating ) , ( ) , ( i j j x x SU T x SU  .</formula><p>A process is committed in the ET2RFNN to speed up the selection process, because the C-correlation is capable of eliminating the irrelevant or weakly relevant features in the first phase, thereby possibly engaging a smaller number of features in the second phase. Both the C-correlation and the F-correlation can be realised using the symmetrical uncertainty method as with the T2RMI methods. We do not detail their mathematical formulas due to space constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E) Adaptation of Network Parameters:</head><p>The adaptation of the eT2RFNN rule base parameters comprises two parts: Fuzzily Weighted Generalised Recursive Least Square (FWGRLS), Type-2 Zero Error Density Maximisation (T2ZEDM). The FWGRLS method adapts the consequent weight and presents a local learning version of the GRLS method <ref type="bibr" target="#b35">[36]</ref>. The peculiar feature of the FWGRLS method in comparison with the standard RLS method lies in the implicit weight decay term, which aims to sustain the weight vector to hover around a small bounded interval. Note that a small and bounded weight vector prevents the model to be unstable and helps to improve the model generalisation <ref type="bibr" target="#b35">[36]</ref>. Apart from substantiating the model's generalisation, this technique is able to refine the compactness of the network architecture, because an inactive fuzzy rule possesses a minor weight vector, thereby being detected by the rule pruning method more easily. The FWGRLS method is defined as follows:</p><formula xml:id="formula_63">1 )) ( ) 1 ( ) ( ) ( ~) ( )( ( ) 1 ( ) ( ~      n F n n F n R n n F n n T i i i    (<label>38</label></formula><formula xml:id="formula_64">) ) 1 ( ) ( ) ( ) 1 ( ) ( ~    n n F n n n i i i     (<label>39</label></formula><formula xml:id="formula_65">) )) ( ) ( )( ( )) 1 ( ( ) ( ) 1 ( ) ( n y n t n n n n n i i i i               (<label>40</label></formula><formula xml:id="formula_66">) ) ( ) ( n x n y i en   and en x n n y n F      ) ( ) ( ) (<label>(41) where</label></formula><formula xml:id="formula_67">] , [ ~i i i R R R </formula><p>is the firing strength of the i-th rule and</p><formula xml:id="formula_68">)] ( ), ( [ ) ( ~n n n     is the Kalman gain. )] ( ), ( [ ) ( ~n n n i i i     , u u n     ) (</formula><p>are respectively the output covariance matrix and the covariance matrix of modelling error. For simplicity, the covariance matrix of modelling error is set as the Hessian matrix as with <ref type="bibr" target="#b35">[36]</ref>.</p><formula xml:id="formula_69">)) 1 ( (    n i </formula><p>stands for the gradient of the weight decay function. Note that we extend the gradient of the weight decay function to the n-1 time step, when the solution of the gradient is too complex to be obtained. The weight decay function can be assigned as any nonlinear function, which may not be differentiable. The quadratic weight decay function is chosen here</p><formula xml:id="formula_70">2 )) 1 ( ( 2 1 )) 1 ( (      n n i i </formula><p>, because it is capable of diminishing the weight vector to its current values proportionally <ref type="bibr" target="#b35">[36]</ref>. Note that the FWGRLS method is different from a prominent FWRLS method <ref type="bibr" target="#b0">[1]</ref>, because the regularized term, namely the weigth decay term, is incorporated in the cost function of the RLS method. This modification leads to the presence of the weight decay term, which does not exist in the FWRLS method.</p><p>The T2ZEDM method adjusts the Wavelet parameters the origin by minimising the distance between the probability distribution of the system's output and the target function. This approach is deemed to be efficient to improve the prediction of high order statistical behaviour. Because the distribution of the error entropy is unknown, the cost function of the T2ZEDM is estimated using the Parzen window estimation method:</p><formula xml:id="formula_71">          N n o n N n o n e K Nh e Nh f 1 2 2 , 1 2 2 , ) 2 ( 2 1 ) 2 exp( 2 1 ) 0 ( ˆ  (42)</formula><p>where N is the number of training data seen so far and  is a smoothing parameter, simply fixed as 1.</p><p>o n e , is the system error of the o-th output in the n-th training episode. The optimisation process is carried out using the gradient descent method as follows:</p><formula xml:id="formula_72">        N n o i r l q E o n e K N q N o i r l q N o i r l q 1 , , ) 2 2 , (<label>2 1 ) 1 ( , , ) ( , ,   (43)</label></formula><formula xml:id="formula_73">        N n j i a E o n e K N a N j i a N j i a 1 ) 2 2 , (<label>2</label></formula><formula xml:id="formula_74">1 ) 1 ( ) (   (44)         N n j i b E o n e K N b N j i b N j i b 1 ) 2 2 , (<label>2</label></formula><formula xml:id="formula_75">1 ) 1 ( ) (   (45)         N n j i E o n e K N N j i N j i 1 ) 2 2 , (<label>2 1 ) 1 ( ) ( </label></formula><formula xml:id="formula_76">     (46)         N n j i E o n e K N N j i N j i 1 ) 2 2 , (<label>2 1 ) 1 ( ) ( </label></formula><formula xml:id="formula_77">    <label>(47)</label></formula><p>where</p><formula xml:id="formula_78">       , , , , b a q</formula><p>are the adaptive learning rates, determined using the Lyapunov stability criteria to warrant the convergence and E stands for the error function training data, we formulate its recursive expression as</p><formula xml:id="formula_79">        N n o N e N A N A n e 1 ) 2 2 , exp<label>( 1 ) 2 2 exp(</label></formula><p>. The gradient term</p><formula xml:id="formula_80">j i j i o i r i j i j i E E q E b E a E , ,<label>, , , , , , , ,  </label></formula><formula xml:id="formula_81">         </formula><p>can be obtained using the chain rule as follows:</p><formula xml:id="formula_82">) )( ( 1 1 , 1 1 ,              P k i P i o i i P k i P i o i i n n l o y y t y q E     , ) )( ( 1 1 , 1 1 ,              P k i P i o i i P k i P i o i i n n l o y y t y q E     (48)                                                ) ) 1 ( ( ) ) 1 ( ( )) 3 )( 2 exp( )<label>( 1 )( ( 1 1 1 1 1 1 1 1 2 , 2 , , , , P k k</label></formula><formula xml:id="formula_83">P i o r o i i P k k P i o r o i i P k k P i o i o i i P k k P i o l o i i j i j i j i j i n n j i q q q q z z z b t y a E         (49) j i j i j i j i j i a E b d z b E , , ,<label>, , ) (  </label></formula><formula xml:id="formula_84">    (50) j i j i j i a E n E , , , ) 1 (          (51) )) ) ( )<label>) 1 ( (( ) ) ( )) 1 ( )((( ( 1 , 1 , ,  </label></formula><formula xml:id="formula_85">               P k k n o o i N i i i P k k n o o i N i i i n n j i y X n R y X n R t y E       <label>(52)</label></formula><p>It is worth stressing that choosing a suitable learning rate plays a crucial role in achieving asymptotic convergence. To this end, the stable range of the learning rate is canvassed with the use of the Lyapunov stability criterion to guarantee convergence. We arrive at</p><formula xml:id="formula_86">N Wo W A P N 2 max , ) ( 2 2 0    </formula><p>. The mathematical proof can be derived following the same steps in <ref type="bibr" target="#b14">[15]</ref>. The learning rates are not fixed during the training process, rather are adapted to speed up convergence. Specifically, the learning rates are adjusted using the direction of the entropy cost function (43) </p><formula xml:id="formula_87">            1 2 1 1 ) 0 ( ) 0 ( ), 1 ( ) 0 ( ) 0 ( ), 1 ( ) ( N N W N N W W f f N f f N N     </formula><p>,where</p><formula xml:id="formula_88">1 2 1 0     <label>(53) where ] 5 . 1 , 1 ( 5   , ) 1 , 5 . 0 [ 4 </label></formula><p> are learning rate factors, which steer the dynamic of the learning rates. Because these factors are not problem-specific, they are simply set as</p><formula xml:id="formula_89">1 . 1 1   , 9 . 0 2 </formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head></head><p>. This setting is plausible, because the adaptation should be more intense when the error entropy grows to expedite the model's updates. The pseudo-code of the eT2RFNN is provided in the Algorithm 1. , which may hinder the end-user to operate the algorithm. This section aims to ensure that these parameters are not problem-dependent. Furthermore, our numerical results in Section 5 are achieved by setting them at their default values. We exclude 3  from our analysis here, because its effect has been well-studied in <ref type="bibr" target="#b16">[17]</ref> and has been shown to be not case-sensitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F) Sensitivity Analysis of Predefined</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X</head><p> is an uncertainty factor and is commonly used in the interval type-2 fuzzy neural network to determine the region of uncertainty of a new rule <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b35">[36]</ref> .This parameter is set as 0.1, adopting the same setting in the literature <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b35">[36]</ref>. The smaller the value of X  the crisper and more specific the new fuzzy set will be 1063-6706 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TFUZZ.2016.2599855, IEEE Transactions on Fuzzy Systems created, and vice versa. A too crisp fuzzy set, however, leads a fuzzy set to behave like the type-1 fuzzy set, whereas a too general fuzzy set usually causes loss of representation in a local region. On the other hand,  is an input pruning threshold, governing the C-correlation of the online feature selection process. The higher the value of this parameter the higher the number of fuzzy rules are discarded during the training process and vice versa. We assign 8 . 0   for all our simulations and adopt the same setting of <ref type="bibr" target="#b21">[22]</ref>.</p><p>The sensitivity of the two predefined paramaters is examined using a popular benchmark problem, the Mackey Glass (MG) chaotic time series problem. Our experimental procedure in this paper is akin to the standard configuration in the literature <ref type="bibr" target="#b0">[1]</ref> . Note that we keep other parameters fixed at its default setting, when a particular pameter is varied. The learning performance of the eT2RFNN is assessed in three viewpoints: NDEI, fuzzy rule, input attributes. Numerical results are reported in Table <ref type="table" target="#tab_0">1</ref>.</p><p>It is clear from Table <ref type="table" target="#tab_0">1</ref> that different values of  , X  just affect little to the eT2RFNN's learning performance. As expected, a small  results in no fuzzy rule to be eliminated during the training process, while a large X  deteriorates the accuracy as a result improper input space partion.</p><p>G) Complexity Analysis: this section discusses the computational and structural burdens of eT2RFNN, which can be understood from the resultant complexity of learning modules and the network architecture of eT2RFNN network parameters. In conjunction to state-of-the art fuzzy neural networks such as SEIT2FNN <ref type="bibr" target="#b15">[16]</ref>, SLFRWNN <ref type="bibr" target="#b17">[18]</ref>, MRIT2NFS <ref type="bibr" target="#b35">[36]</ref>, eT2Class <ref type="bibr" target="#b16">[17]</ref>, the eT2RFNN network topology produces a slightly higher number of network parameters. Nonetheless, the number of network parameters is also influenced by the number of evolved fuzzy rules in the training process, where the eT2RFNN is expected to scatter a fewer number of rules. These merits are noticeable in our numerical studies. V. PROOF OF CONCEPTS The efficacy of eT2RFNN was experimentally validated using four synthetic and real-world problems, containing various concept drifts, uncertain and noisy characteristics. The proposed algorithm was also benchmarked with 10 prominent machine learning algorithms: eT2Class <ref type="bibr" target="#b16">[17]</ref>,</p><p>eTS <ref type="bibr" target="#b0">[1]</ref>, simp_eTS <ref type="bibr" target="#b1">[2]</ref>, PANFIS <ref type="bibr" target="#b2">[3]</ref>, GENEFIS <ref type="bibr" target="#b36">[37]</ref>, DFNN <ref type="bibr" target="#b42">[43]</ref>, GDFNN <ref type="bibr" target="#b27">[28]</ref>, FAOSPFNN <ref type="bibr" target="#b42">[43]</ref>, BARTFIS <ref type="bibr" target="#b39">[40]</ref>, ANFIS <ref type="bibr" target="#b41">[42]</ref>. Learning performance of consolidated algorithms were measured against five criteria: predictive accuracy, number of fuzzy rules, number of rule base parameters number of input attributes, and execution time. All consolidated algorithms were run under MATLAB with an Intel (R) core (TM) i7-2600 CPU, 3.4 GHz processor and 8 GB memory. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Example 1: Prediction of Nox Emission of Car Engine</head><p>A real-world problem, namely prediction of Nox emission of a car engine, is put into perspective to evaluate the viability of the eT2RFNN. This problem features highly noisy and uncertain characteristic, because data were sampled from raw data of a car engine. Furthermore, this problem also contains a non-stationary characteristic because two important attributes of the engine control, namely rotation speed and torque, were varied in order for different driving behaviours in different road conditionsnormal, country, hilly -to be represented in the engine data. This case study also offers an appealing property to validate the efficacy of the online feature selection process, because 170 input attributes were extracted to guide the prediction problem. This figure results from 17 different 1063-6706 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TFUZZ.2016.2599855, IEEE Transactions on Fuzzy Systems physical variables, recorded in 10 consecutive measurements by hard sensors mounted in the car engine. In addition to comparisons against state-of the art algorithms, we also experimented using five learning configurations of eT2RFNN to demonstrate the effectiveness of each learning modules: A) This configuration reveals the learning performance of eT2RFNN without structural complexity reduction method elaborated in Section III.B and C, B) we switch off the online feature selection module outlined in Section III.D, C) the eT2RFNN is implemented using the standard recurrent interval type-2 network architecture <ref type="bibr" target="#b15">[16]</ref>, C1) this configuration is akin to C but during the training process the same number of rules is evolved to ensure fair comparision with the eT2Class, D) the eT2RFNN is structured in the type-1 network architecture of GENEFIS <ref type="bibr" target="#b36">[37]</ref>, E) The eT2RFNN utilises only the previoustime instant data n-1 in the regression model to demonstrate the spatio-temporal property of the eT2RFNN. This problem consists of 826 data points, in which 667 samples are fed in the training process, whereas the remainder is used in the testing samples. The numerical results are reported in Table <ref type="table" target="#tab_0">1</ref>. The predictive quality of benchmarked models is measured using the RMSE. From Table <ref type="table" target="#tab_12">2</ref>, the eT2RFNN(A)-(E) produced higher accuracy, while retaining lower complexity than other algorithms. The curse of dimensionality is noticeable in this problem, where all algorithms with the absence of the feature selection process experienced sluggish execution time. Each learning module contributed substantially to the resultant learning performance of eT2RFNN. It can be viewed from the deterioration of numerical results due to the absence of a particular module. Because of very noisy nature of raw engine data, the learning performance of the eT2RFNN in the type-1 fuzzy system -eT2RFNN(D)-deteriorated substantially. This fact reveals the advantage of the interval type-2 fuzzy system in processing information uncertainty. Although the eT2RFNN(E) relied on merely the latest time instant data, it still delivered comparable numerical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Example 2: Tool Wear Prediction of Ball Nose End-Milling Process</head><p>This case study presents a tool wear prediction of a ball nose end milling process (Courtesy of Dr. Li Xiang, Singapore). It is a challenging problem, because the use of multi-point cutting tools at high speed, varying machining parameters, and inconsistency and variability of cutter geometry/dimensions makes accurate prediction extremely difficult to be achieved. A CNC milling process (Rőders Tech RFM760) with a spindle rate up to 42000 RPM was used during the experiment. Raw data were captured using seven channels DAQ, where the first three channels measured the force signal in three dimensional cutting axes, the next three channels collected the vibration signal in the three dimensional cutting axes, and the last channel recorded the acoustic emission (AE) signal. The prediction of the tool wear was carried out using the force signal from two different cutter profiles, where twelve input features were extracted. Force signal was selected, because it provides the most informative features of the tool wear prediction, while the tool wear itself was determined from visual inspection of flank wear utilising Olympus SZX16 microscope. It is worth noting that the non-stationary characteristics of this problem is caused by two different cutter profiles, used during the experiment. Furthermore, surface integrity and also cutter's degradation during machining process possibly induces gradually changing data distributions. A total of 630 data points were generated and the experiment was executed using 10-fold cross validation (CV) technique. Note that although the CV procedure was utilised to produce the numerical results, the eT2RFNN fully works in the single-pass learning mode. Moreover, we extended our numerical study with the use of the periodic hold-out process to simulate the time-series environment. The numerical results can be found as a supplementary materials in our homepage 1) . The eT2RFNN was run in several learning configurations as done in example 1 but without the eT2RFNN(E), because the input-output (I/O) relationship is defined with the previous step observation only without lagged input attributes. This setting is necessary to demonstrate the efficacy of recurrent structure in overcoming over dependency on time-delayed input variables with its internal memory component. We did not perform any CV procedure to arrive at this I/O definition, because this procedure may harm the logic of online learner, supposed to run with minimum supervision and pre-processing steps. Numerical results are tabulated in Table <ref type="table" target="#tab_13">3</ref>. The eT2RFNN (A)-(E) outperformed its counterparts in attaining tradeoff between complexity and accuracy. Although the eT2RFNN is built upon a generalized interval type-2 structure, the eT2RFNN possesses comparable number of parameters with other consolidated algorithms. This aspect is evident that the eT2RFNN network architecture is capable of From Table <ref type="table" target="#tab_14">4</ref>, it can be observed that the eT2RFNN(E) demonstrated the most encouraging performance in all criteria. More interestingly, eT2RFNN(E) also incurred the least network parameters as a result of its generalised fuzzy rule, which can diminish the fuzzy rule demand, and its double recurrent links, which can overcome the absence of delayed input features. The network simplicity was achieved without any loss of accuracy and expedites the training process. The difference between the batched and sequential algorithms is also noticeable, where the batched algorithm imposes intractable computational load. In Fig. <ref type="figure" target="#fig_15">2</ref>(a), it is shown that the eT2RFNN produced reliable predictions. From Fig. <ref type="figure" target="#fig_15">2</ref>(B), it can be seen that the eT2RFNN can automatically evolve its fuzzy rules in timely manner, where new fuzzy rules are introduced, when changing system dynamics are detected around n=12000. Furthermore, obsolete fuzzy rule, which is no longer relevant to the current context, can be removed. Note that in realm of how drift is addressed, the eT2RFNN can be classified as a passive approach, because the drift is handled by automatically evolving its structures without a dedicated drift detection method. VI. CONCEPTUAL COMPARISON The efficacy of eT2RFNN has been experimentally validated using four non-stationary data streams and comparisons with prominent FNNs in Section 4. This section specifically analyses the key facets of eT2RFNN, which distinguish itself with its counterpart. To this end, eT2RFNN is conceptually contrasted with six popular RFNNs in the literature: SLFRWNN <ref type="bibr" target="#b17">[18]</ref>, TRFNS <ref type="bibr" target="#b12">[13]</ref>, RSEFNN-LF <ref type="bibr" target="#b11">[12]</ref>, RSEIT2FNN <ref type="bibr" target="#b15">[16]</ref>, MRIT2NFS <ref type="bibr" target="#b35">[36]</ref>. SLFRWNN constitutes a recurrent type-1 fuzzy wavelet neural network, which incorporates the local recurrent link in the rule consequent, while the RSEFNN-LF presents the type-1 fuzzy neural network with a local feedback loop in the rule layer. The RSEIT2FNN forms an extended version of the RSEFNN-LF in the interval type-2 fuzzy environment, while the MRIT2NFS actualizes a recurrent interval type-2 fuzzy network with the interactive recurrent network architecture. The TRFNS represents a type-1 recurrent FNN with a global recurrent type. The characteristics of consolidated algorithms are tabulated in Table <ref type="table" target="#tab_14">4</ref>.  <ref type="table" target="#tab_14">4</ref> except RSEFNN-LF either utilise the KM iterative method or the GA method. The GA method is renowned for its offline trait, because it relies on the multi-pass optimization process. Although the KM method is applicable for an online learning process, it is deemed computationally more expensive than the q-design factor of the eT2RFNN, because it requires the weight vector to be reordered in the ascending manner. The crossover points are then found using the iterative process. Other RFNNs in Table <ref type="table">7</ref> are presumed to impose more prohibitive structural complexity, because they are not fitted out by a rule base simplification procedure. Along with this paper, we also provide supplementary materials 1) , containing additional scenario of example 2. This document discloses numerical results of consolidated algorithms under the periodic hold-out process to simulate the training and testing mechanism in the online real-time environment. Furthermore, this document also discusses numerical study with a big dataset, consisting of 2+ millions data samples. That is, the prediction of household electricity power consumption <ref type="bibr" target="#b63">[63]</ref> is put forward. This numerical study is used to demonstrate the viability of the eT2RFNN in the online life-long learning situation. Furthermore, this document presents numerical study in the temporal problem, namely the SISO nonlineary dynamic system identification, which illustrates the efficacy of recurrent structure.</p><p>VII. CONCLUSION AND FURTHER STUDY A novel recurrent interval type-2 FNN, namely Evolving Type-2 Fuzzy Neural Network (eT2RFNN), is proposed in this paper aiming to handle concept drifts and uncertainties in realworld data streams. The eT2RFNN presents a fully flexible and computationally efficient working principle, which can adapt, grow, prune, and merge its fuzzy rules and even reduce the dimensionality of learning problems on the fly. The salient contributions of eT2RFNN are summed up as follows: 1) It is implemented in a novel recurrent network architecture, 2) It puts forward a generalised interval type-2 fuzzy rule, 3) the universal approximation capability of the eT2RFNN's network architecture is mathematically proven, 4) A type-2 version of the RMI method is developed, and 5) the online feature selection technique, namely the SMBC method, is proposed. Our numerical study in various synthetic and real-world data problems has demonstrated that eT2RFNN can outperform state-of-the art algorithms in both complexity and accuracy. Our future work will be devoted to designing a metacognitive scaffolding learning algorithm for eT2RFNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A. Universal Approximation Property of the eT2RFNN</head><p>The universal approximation property of the eT2RFNN can be achieved by extending the work of <ref type="bibr" target="#b59">[59]</ref> for the singlerecurrent-link network architecture to the double-recurrent-link case. An open dynamical system in discrete time can defined as a state transition and an output equation:</p><p>) , ( where s stands for state transition, a mapping from the present internal hidden state of the system and the external input un to a new system state. Note that if the state transition is discounted, we recover the expression of the feedforward network architecture, whereas without the external input un we arrive at the definition of the autonomous system. The predictive task however will be harder with the hidden state, because it takes into account inter temporal dependencies. In <ref type="bibr" target="#b59">[59]</ref>, a recurrent neural network in the state space form is developed to map an open dynamical system (A.1). The expression is amended here to suit the eT2RFNN structure as follows:  Proof : the mathematical proof can be obtained by tracing back the equations of the open dynamical system with the representation of the feedforward network structure as a result of transforming the RNN definition (A.2)-(A.3) into its feedforward network structure equivalent, using the unfolding in time and shared weigth matrices. This can be achieved in two steps: 1) a conclusion of the state space equation of the open dynamical system can be approximated by a neural network in the form of (A.2); 2) the output equation of the open dynamical system can be approximated by a neural network of the form of (A.3). Detailed mathematical derivations are left to reader and it can be achieved following the same procedure of <ref type="bibr" target="#b59">[59]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig</head><label></label><figDesc>Fig. 1 Network Architecture of eT2RFNN Algorithm 1: Learning policy of eT2RFNN Define: Training Data ) ,.., , ,.., ( ) , ( 1 1 m u n n t t x x T X </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>constant, which determines a completeness degree a new rule should have. This issue pertains to the minimum firing strength of a rule</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>entropy of Y R i , .<ref type="bibr" target="#b23">(24)</ref> revolves around [0,1], where zero indicates that the two variables are uncorrelated. It is worth noting that the term symmetric here refers to the a symmetric measure.Changing the order of the two variables will not affect the information</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>and Pearson's correlation coefficient. It is worth mentioning that the variance of i R and covariance of Y R i ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>of the fuzzy set. Because the Gaussian function has a highly nonlinear contour, its size is approximated using the triangular function. The union of two fuzzy sets is obtained as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>)</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2: (a) eT2RFNN's prediction in S&amp;P 500 time series, (b) the trace of eT2RFNN(A)'s fuzzy rule in S&amp;P 500 time series Referring to Table 4, eT2RFNN goes one step ahead of existing RFNNs by putting forward double local recurrent connections. This architecture retains the local learning trait, offering greater flexibility and robustness for the evolving system. eT2RFNN employs a more advanced type of fuzzy rule, which synergises the interval type-2 multivariate Gaussian function in the rule premise and the Wavelet function. Most other algorithms in Table4except RSEFNN-LF either utilise the KM iterative method or the GA method. The GA method is renowned for its offline trait, because it relies on the multi-pass optimization process. Although the KM method is applicable for an online learning process, it is deemed computationally more expensive than the q-design factor of the eT2RFNN, because it requires the weight vector to be reordered in the ascending manner. The crossover points are then found using the iterative process. Other RFNNs in Table7are presumed to impose more prohibitive structural complexity, because they are not fitted out by a rule base simplification procedure. Along with this paper, we also provide supplementary materials1) , containing additional scenario of example 2. This document discloses numerical results of consolidated algorithms under the periodic hold-out process to simulate the training and testing mechanism in the online real-time environment. Furthermore, this document also discusses numerical study with a big dataset, consisting of 2+ millions data samples. That is, the prediction of household electricity power consumption<ref type="bibr" target="#b63">[63]</ref> is put forward. This numerical study is used to demonstrate the viability of the eT2RFNN in the online life-long learning situation. Furthermore, this document presents numerical study in the temporal problem, namely the SISO nonlineary dynamic</figDesc><graphic coords="14,46.80,176.80,518.40,201.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>dynamical system, defined in (A.1)can be predicted by an element of the RNN (A.2)-(A.3) with an arbitrary accuracy, where()   f is an interval type-2 fuzzy set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="5,46.80,50.40,518.40,331.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Main Features of Related Works</figDesc><table><row><cell cols="2">ALGORITHMS REFERENCES</cell><cell>Type</cell><cell>Structure</cell><cell>Learning Principle</cell></row><row><cell>SEIT2FNN</cell><cell>[7]</cell><cell>Interval Type-2 Fuzzy</cell><cell>Feedforward</cell><cell>Evolving</cell></row><row><cell>T2TSKFNS</cell><cell>[9]</cell><cell>Interval Type-2 Fuzzy</cell><cell>Feedforward</cell><cell>Static</cell></row><row><cell>TSCIT2FNN</cell><cell>[10]</cell><cell>Interval Type-2 Fuzzy</cell><cell>Feeforward</cell><cell>Evolving</cell></row><row><cell>IT2FNN</cell><cell>[50]</cell><cell>Interval Type-2 Fuzzy</cell><cell>Feedforward</cell><cell>Static</cell></row><row><cell>GS-EFS</cell><cell>[48]</cell><cell>Type-1 Fuzzy</cell><cell>Feedforward</cell><cell>Evolving</cell></row><row><cell>RSONFIN</cell><cell>[11]</cell><cell>Type-1 Fuzzy</cell><cell>Single Globally Recurrent</cell><cell>Evolving</cell></row><row><cell>RSEFNN-LF</cell><cell>[12]</cell><cell>Type-1 Fuzzy</cell><cell>Single Locally Recurrent</cell><cell>Evolving</cell></row><row><cell>TRFN</cell><cell>[13]</cell><cell>Type-1 Fuzzy</cell><cell>Single Globally Recurrect</cell><cell>Evolutionary</cell></row><row><cell>IRSFNN</cell><cell>[14]</cell><cell>Type-1 Fuzzy</cell><cell>Single Interactively Recurrent</cell><cell>Evolving</cell></row><row><cell>RSEIT2FNN</cell><cell>[16]</cell><cell>Interval Type-2 Fuzzy</cell><cell>Single Locally Recurrent</cell><cell>Evolving</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>)</head><label></label><figDesc>1063-6706 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TFUZZ.2016.2599855, IEEE Transactions on Fuzzy Systems</figDesc><table><row><cell>where</cell><cell>i i b j a , ,</cell><cell>,</cell><cell>j</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Phase C: Rule Merging Mechanism/* For i=1 to P do For j=1 to u do</head><label></label><figDesc></figDesc><table><row><cell>)</cell></row><row><cell>End If</cell></row><row><cell>/*Phase B: Rule Pruning Mechanism/*</cell></row><row><cell>For i=1 to P do</cell></row><row><cell>Compute RMI (24)</cell></row><row><cell>IF (26) Then</cell></row><row><cell>Prune i-th hidden node</cell></row><row><cell>End IF</cell></row><row><cell>End For</cell></row><row><cell>/*</cell></row></table><note><p><p><p><p><p><p><p><p><p><p><p>Compute the shape-based and proximity-based similarity measures</p><ref type="bibr" target="#b30">(31)</ref></p>,</p><ref type="bibr" target="#b31">(32)</ref> </p>Quantify the vector similarity measure</p><ref type="bibr" target="#b28">(29)</ref> </p>End For IF (30),(34) Then</p>Coalesce the fuzzy rules (</p>35</p>)-</p><ref type="bibr" target="#b37">(38)</ref> </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>End IF End For /*Phase D: Feature Selection Mechanism/* For j=1 to u Do For o=1 to m Do</head><label></label><figDesc></figDesc><table><row><cell>x</cell><cell>N</cell><cell>N</cell></row><row><cell>re</cell><cell>re</cell><cell>re</cell></row></table><note><p>Compute the symmetrical uncertainty IF   ) , ( o j t x SU Then Prune the j-th feature Else IF Append j-th as relevant variables 1 ,  </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>End IF End For End For For j=1 to re N For re=1 to</head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TFUZZ.2016.2599855, IEEE Transactions on Fuzzy Systems</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">N ,</cell><cell cols="2">j </cell><cell cols="3">re</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>re</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">Analyze the redundancy</cell><cell cols="3">SU</cell><cell>(</cell><cell>x</cell><cell>, j x</cell><cell>re</cell><cell>)</cell></row><row><cell>IF</cell><cell>SU</cell><cell>(</cell><cell>x</cell><cell>,</cell><cell>T</cell><cell>)</cell><cell></cell><cell cols="2">SU</cell><cell>(</cell><cell>x</cell><cell>,</cell><cell cols="2">x</cell><cell>)</cell><cell>Then</cell></row><row><cell></cell><cell></cell><cell></cell><cell>j</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>j</cell><cell></cell><cell></cell><cell cols="2">re</cell></row><row><cell></cell><cell cols="13">Prune re-th input variable</cell><cell></cell></row><row><cell cols="2">End IF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">End For</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>End For</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="16">/*Phase E: Parameter Learning Scenario /*</cell></row><row><cell cols="4">For i=1 to P do</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="16">Adjust the fuzzy rule consequents (39)-(42)</cell></row><row><cell cols="6">For j=1 to u do</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="16">Adjust the dilation and translation parameters (45),(46)</cell></row></table><note><p><p><p><p><p><p><p><p>Adapt the input recurrent weights</p><ref type="bibr" target="#b47">(47)</ref> </p>End For For o=1 to m do</p>Fine-tune the design factors</p><ref type="bibr" target="#b42">(43)</ref></p>,</p>(44)   </p>Adjust the consequent recurrent weight (48) End For End For 1063-6706 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TFUZZ.2016.2599855, IEEE Transactions on Fuzzy Systems</figDesc><table><row><cell>S</cell><cell>2</cell><cell>,</cell><cell>j</cell><cell>(</cell><cell>win</cell><cell>i</cell><cell></cell><cell></cell><cell>A</cell><cell></cell><cell></cell><cell>B</cell></row></table><note><p>1063-6706 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>.</head><label></label><figDesc>It enhances the standard gradient descent method by modifying its cost function, where the error entropy is employed in lieu of the Mean Square Error (MSE). In principle, this method forces the error entropy to converge at 1063-6706 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TFUZZ.2016.2599855, IEEE Transactions on Fuzzy Systems</figDesc><table><row><cell>(</cell><cell>a</cell><cell>i</cell><cell>,</cell><cell>j</cell><cell>,</cell><cell>i b</cell><cell>,</cell><cell>j</cell><cell>)</cell><cell cols="9">, the design coefficients</cell><cell>(</cell><cell>q</cell><cell>, i q , l o</cell><cell>, r i</cell><cell>o</cell><cell>)</cell><cell>, and the</cell></row><row><cell cols="11">recurrent weights</cell><cell>( </cell><cell>i</cell><cell>,</cell><cell>j</cell><cell>,</cell><cell></cell><cell>i</cell><cell>,</cell><cell>j</cell><cell>)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 1 .</head><label>1</label><figDesc>Sensitivity of Predefined Parameters</figDesc><table><row><cell cols="2">Parameters EC</cell><cell>I=1</cell><cell>I=2</cell><cell cols="2">I=3 I=4</cell><cell>I=5</cell></row><row><cell>X </cell><cell cols="4">NDEI 0.33 0.33 0.3 R 5 5 3</cell><cell cols="2">0.34 0.35 3 3</cell></row><row><cell></cell><cell>Input</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell></cell><cell cols="4">NDEI 0.32 0.32 0.3</cell><cell>0.3</cell><cell>0.32</cell></row><row><cell></cell><cell>R</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>3</cell></row><row><cell></cell><cell>Input</cell><cell>4</cell><cell>4</cell><cell>3</cell><cell>2</cell><cell>1</cell></row><row><cell></cell><cell cols="5">CR= Classification rate, R=Rule, RT=Runtime</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 2 .</head><label>2</label><figDesc>Prediction of Nox emission</figDesc><table><row><cell>Model</cell><cell>Type</cell><cell cols="4">RMSE Rule Input Runtime</cell><cell>Parameters</cell></row><row><cell>eT2RFNN</cell><cell>S-2-R</cell><cell>0.03</cell><cell>2</cell><cell>1</cell><cell>0.24</cell><cell>14</cell></row><row><cell cols="2">eT2RFNN (A) S-2-R</cell><cell>0.035</cell><cell>4</cell><cell>1</cell><cell>0.27</cell><cell>28</cell></row><row><cell>eT2RFNN(B)</cell><cell>S-2-R</cell><cell>0.05</cell><cell>2</cell><cell>170</cell><cell>2.13</cell><cell>59840</cell></row><row><cell cols="2">eT2RFNN (C) S-2-R</cell><cell>0.04</cell><cell>3</cell><cell>1</cell><cell>0.24</cell><cell>18</cell></row><row><cell cols="2">eT2RFNN(C1) S-2-R</cell><cell>0.05</cell><cell>2</cell><cell>1</cell><cell>0.23</cell><cell>12</cell></row><row><cell>eT2RFNN(D)</cell><cell>S-1-R</cell><cell>0.14</cell><cell>5</cell><cell>1</cell><cell>0.27</cell><cell>30</cell></row><row><cell>eT2RFNN(E)</cell><cell>S-2-R</cell><cell>0.03</cell><cell>5</cell><cell>1</cell><cell>0.26</cell><cell>35</cell></row><row><cell>eT2Class</cell><cell>S-2-F</cell><cell>0.045</cell><cell>2</cell><cell>170</cell><cell>17.98</cell><cell>117304</cell></row><row><cell>PANFIS</cell><cell>S-1-F</cell><cell>0.052</cell><cell>5</cell><cell>170</cell><cell>3.37</cell><cell>146205</cell></row><row><cell>GENEFIS</cell><cell>S-1-F</cell><cell>0.048</cell><cell>2</cell><cell>2</cell><cell>0.41</cell><cell>18</cell></row><row><cell>Simp_eTS</cell><cell>S-1-F</cell><cell>0.14</cell><cell>5</cell><cell>170</cell><cell>5.5</cell><cell>1876</cell></row><row><cell>BARTFIS</cell><cell>S-1-F</cell><cell>0.11</cell><cell>4</cell><cell>4</cell><cell>2.55</cell><cell>52</cell></row><row><cell>DFNN</cell><cell>B-1-F</cell><cell>0.18</cell><cell>548</cell><cell>170</cell><cell>4332.9</cell><cell>280198+num of training samples</cell></row><row><cell>GDFNN</cell><cell>B-1-F</cell><cell>0.48</cell><cell>215</cell><cell>170</cell><cell>2144.1</cell><cell>109865</cell></row><row><cell>eTS</cell><cell>S-1-F</cell><cell>0.38</cell><cell>27</cell><cell>170</cell><cell>1098.4</cell><cell>13797</cell></row><row><cell>FAOS-PFNN</cell><cell>B-1-F</cell><cell>0.06</cell><cell>6</cell><cell>170</cell><cell>14.8</cell><cell>2216+num of training samples</cell></row><row><cell>ANFIS</cell><cell>B-1-F</cell><cell>0.15</cell><cell>2</cell><cell>170</cell><cell>100.41</cell><cell>17178</cell></row><row><cell cols="7">S: Sequential, B: Batch, 1: type-1, 2: Type 2, R: Recurrent, F: Feed-forward</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 3 .</head><label>3</label><figDesc>Tool condition monitoring problem of a complex manufacturing process</figDesc><table><row><cell>Model</cell><cell>Type</cell><cell>RMSE</cell><cell>Rule</cell><cell>Input</cell><cell>Runtime</cell><cell>Parameters</cell></row><row><cell>eT2RFNN</cell><cell>S-2-R</cell><cell>0.04±0.01</cell><cell>3.8±0.6</cell><cell>6.5±0.1</cell><cell>0.55±0.43</cell><cell>242.1</cell></row><row><cell cols="2">eT2RFNN (A) S-2-R</cell><cell>0.07±0.01</cell><cell>5.4±0.1</cell><cell>6.9±0.2</cell><cell>0.76±0.35</cell><cell>424.8</cell></row><row><cell>eT2RFNN(B)</cell><cell>S-2-R</cell><cell>0.06±0.03</cell><cell>3.8±0.7</cell><cell>12</cell><cell>1.1±0.8</cell><cell>820.8</cell></row><row><cell>eT2RFNN (C)</cell><cell>S-2-R</cell><cell>0.05±0.02</cell><cell>4.3±0.2</cell><cell>6.8±0.05</cell><cell>0.6±0.4</cell><cell>320.9</cell></row><row><cell cols="2">eT2RFNN(C1) S-2-R</cell><cell>0.06±0.02</cell><cell>3.8±0.8</cell><cell>6.7±0.1</cell><cell>0.58±0.2</cell><cell>263.1</cell></row><row><cell>eT2RFNN(D)</cell><cell>S-1-R</cell><cell>0.11±0.01</cell><cell>5</cell><cell>7.5±0.3</cell><cell>0.9±0.6</cell><cell>485.75</cell></row><row><cell>eT2Class</cell><cell>S-2-R</cell><cell>0.05±0.01</cell><cell>5.9±01</cell><cell>12</cell><cell>1.1±0.7</cell><cell>2065</cell></row><row><cell>PANFIS</cell><cell>S-1-F</cell><cell>0.045±0.01</cell><cell>2</cell><cell>12</cell><cell>0.98±0.11</cell><cell>636.7</cell></row><row><cell>GENEFIS</cell><cell cols="2">S-1-F 0.0418±0.008</cell><cell>4.1±0.94</cell><cell>11.1±0.31</cell><cell>1.12±0.11</cell><cell>636.7</cell></row><row><cell>Simp_eTS</cell><cell>S-1-F</cell><cell>0.26±0.08</cell><cell>5</cell><cell>12</cell><cell>1.4±0.3</cell><cell>137</cell></row><row><cell>eTS</cell><cell>S-1-F</cell><cell>0.046±0.01</cell><cell>5.1±0.32</cell><cell>12</cell><cell>1.3±0.02</cell><cell>139.5</cell></row><row><cell>BARTFIS</cell><cell>S-1-F</cell><cell>0.0632±0.01</cell><cell>20.6±4.1</cell><cell>12</cell><cell>1.43±0.06</cell><cell>762.2</cell></row><row><cell>FAOSPFNN</cell><cell>B-1-F</cell><cell>0.27±0.003</cell><cell>12.7±0.7</cell><cell>12</cell><cell>1.91±0.1</cell><cell>330.2+num of training samples</cell></row><row><cell>DFNN</cell><cell>B-1-F</cell><cell>0.12±0.1</cell><cell>101.9±2.33</cell><cell>12</cell><cell>24.3±2.43</cell><cell>3872.2+ num of training samples</cell></row><row><cell>GDFNN</cell><cell>B-1-F</cell><cell>0.05±0.06</cell><cell>5.3±1.15</cell><cell>12</cell><cell>7.25±1.54</cell><cell>137.8+num of training samples</cell></row><row><cell>ANFIS</cell><cell>B-1-F</cell><cell>0.05±0.01</cell><cell>11</cell><cell>12</cell><cell>183.05±5.36</cell><cell>407+num of training samples</cell></row><row><cell></cell><cell cols="6">S: Sequential, B: Batch, 1: type-1, 2: Type 2, R: Recurrent, F: Feed-forward</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">S&amp;P 500 Index Time Series</cell></row><row><cell></cell><cell>Model</cell><cell>Type</cell><cell cols="4">NDEI Rule Input Runtime</cell><cell>Parameters</cell></row><row><cell></cell><cell>eT2RFNN</cell><cell>S-2-R</cell><cell>0.03</cell><cell>2</cell><cell>2</cell><cell>6.22</cell><cell>32</cell></row><row><cell></cell><cell>eT2RFNN (A)</cell><cell>S-2-R</cell><cell>0.04</cell><cell>5</cell><cell>2</cell><cell>8.33</cell><cell>80</cell></row><row><cell></cell><cell>eT2RFNN(B)</cell><cell>S-2-R</cell><cell>0.04</cell><cell>2</cell><cell>5</cell><cell>7.26</cell><cell>110</cell></row><row><cell></cell><cell>eT2RFNN(C)</cell><cell>S-2-R</cell><cell>0.04</cell><cell>4</cell><cell>2</cell><cell>6.6</cell><cell>52</cell></row><row><cell></cell><cell cols="2">eT2RFNN (C1) S-2-R</cell><cell>0.05</cell><cell>2</cell><cell>2</cell><cell>6.2</cell><cell>26</cell></row><row><cell></cell><cell>eT2RFNN(D)</cell><cell>S-1-R</cell><cell>0.5</cell><cell>13</cell><cell>3</cell><cell>8.77</cell><cell>312</cell></row><row><cell></cell><cell>eT2RFNN(E)</cell><cell>S-2-R</cell><cell>0.01</cell><cell>2</cell><cell>1</cell><cell>4.82</cell><cell>14</cell></row><row><cell></cell><cell>eT2Class</cell><cell>S-2-F</cell><cell>0.05</cell><cell>5</cell><cell>5</cell><cell>21.6</cell><cell>385</cell></row><row><cell></cell><cell>PANFIS</cell><cell>S-1-F</cell><cell>0.09</cell><cell>4</cell><cell>5</cell><cell>55.3</cell><cell>144</cell></row><row><cell></cell><cell>GENEFIS</cell><cell>S-1-F</cell><cell>0.07</cell><cell>2</cell><cell>5</cell><cell>48.3</cell><cell>72</cell></row><row><cell></cell><cell>Simp_eTS</cell><cell>S-1-F</cell><cell>0.04</cell><cell>7</cell><cell>5</cell><cell>158.00</cell><cell>39</cell></row><row><cell></cell><cell>eTS</cell><cell>S-1-F</cell><cell>0.04</cell><cell>14</cell><cell>5</cell><cell>89.9</cell><cell>75</cell></row><row><cell></cell><cell>BARTFIS</cell><cell>S-1-F</cell><cell>0.02</cell><cell>8</cell><cell>5</cell><cell>12.3</cell><cell>128</cell></row><row><cell></cell><cell>DFNN</cell><cell>B-1-F</cell><cell>0.06</cell><cell>5</cell><cell>5</cell><cell>548.5</cell><cell>60+ num of training samples</cell></row><row><cell></cell><cell>GDFNN</cell><cell>B-1-F</cell><cell>0.07</cell><cell>4</cell><cell>5</cell><cell>951.4</cell><cell>64+ num of training samples</cell></row><row><cell></cell><cell>FAOSPFNN</cell><cell>B-1-F</cell><cell>0.07</cell><cell>13</cell><cell>5</cell><cell>159.8</cell><cell>91+num of training samples</cell></row><row><cell></cell><cell>ANFIS</cell><cell>B-1-F</cell><cell>0.02</cell><cell>32</cell><cell>5</cell><cell>384.9</cell><cell>222+num of training samples</cell></row><row><cell></cell><cell cols="7">S: Sequential, B: Batch, 1: type-1, 2: Type 2, R: Recurrent, F: Feed-forward</cell></row><row><cell></cell><cell></cell><cell cols="6">Table 4. Learning Characteristic of Consolidated Algorithms</cell></row><row><cell>Algorithm</cell><cell>Recurrent types</cell><cell cols="2">Premise Part</cell><cell></cell><cell cols="2">Consequent Part</cell><cell>Learning Type</cell><cell>Learning Modules</cell></row><row><cell>eT2RFNN</cell><cell>Double Local</cell><cell cols="2">Interval type-2</cell><cell></cell><cell cols="2">Nonlinear Wavelet</cell><cell>Online</cell><cell>RG+RA+RP+RM-</cell></row><row><cell></cell><cell></cell><cell cols="3">multivariate Gaussian</cell><cell></cell><cell></cell><cell>RC+FS</cell></row><row><cell>RSEIT2FNN</cell><cell>Single Local</cell><cell cols="2">Interval type-2</cell><cell></cell><cell cols="2">First order TSK</cell><cell>Online</cell><cell>RG+RA</cell></row><row><cell></cell><cell></cell><cell cols="3">univariable Gaussian</cell><cell></cell><cell></cell></row><row><cell>MRIT2NFS</cell><cell>Single Interactive</cell><cell cols="2">Interval type-2</cell><cell></cell><cell cols="2">First order TSK</cell><cell>Online</cell><cell>RG+RA</cell></row><row><cell></cell><cell></cell><cell cols="3">univariable Gaussian</cell><cell></cell><cell></cell></row><row><cell>TRFNS</cell><cell>Single Global</cell><cell cols="2">Type-1 univariable</cell><cell></cell><cell cols="2">First order TSK</cell><cell>Offline</cell><cell>RG+RA</cell></row><row><cell></cell><cell></cell><cell cols="2">Gaussian</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RSEFNN-LF</cell><cell>Single Local</cell><cell cols="2">Type-1 univariable</cell><cell></cell><cell cols="2">First order TSK</cell><cell>Online</cell><cell>RG+RA</cell></row><row><cell></cell><cell></cell><cell cols="2">Gaussian</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SLFRWNN</cell><cell>Single Local</cell><cell cols="2">Type-1 univariable</cell><cell></cell><cell cols="2">Nonlinear Wavelet</cell><cell>Offline</cell><cell>RG+RA</cell></row><row><cell></cell><cell></cell><cell cols="2">Gaussian</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="7">RG: Rule Growing, RA: Rule Adaptation, RP: Rule Pruning, RM: Rule Merging, RC: Rule Recall, FS: Feature Selection</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>1063-6706 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TFUZZ.2016.2599855, IEEE Transactions on Fuzzy Systems</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>-6706 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TFUZZ.2016.2599855, IEEE Transactions on Fuzzy Systems</figDesc><table><row><cell>1063) ( 1 s h y n </cell><cell>(output equation)</cell><cell>(A.1)</cell><cell>1 s  n</cell><cell>1 g</cell><cell>n s</cell><cell>u</cell><cell>n</cell></row></table><note><p><p></p>(state transition),</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>interval type-2 fuzzy set. The state space form eases the analysis because of explicit correspondence between equations and architecture, from which it can be directly transferred into a spatial network structure via the so-called unfolding in time and shared weight</figDesc><table><row><cell>1 n S</cell><cell>1 </cell><cell cols="2"></cell><cell cols="8">( ), 2 ~2 2 ( 2 1 1 1 n 1 n n n U B S A f S U B S A f    </cell><cell>n</cell><cell>)</cell><cell>(State Transitition)</cell></row><row><cell>Y</cell><cell cols="2"></cell><cell cols="2">C</cell><cell cols="2">C</cell><cell cols="2">S</cell><cell>n</cell><cell cols="2">1  S</cell><cell>n</cell><cell>1 </cell><cell>(output equation)</cell><cell>(A.2)</cell></row><row><cell>n</cell><cell></cell><cell></cell><cell></cell><cell cols="2">1</cell><cell cols="2">2</cell><cell cols="2">1</cell><cell></cell><cell>2</cell></row><row><cell cols="2">where</cell><cell></cell><cell></cell><cell cols="2">A</cell><cell></cell><cell>,</cell><cell cols="2">B</cell><cell></cell><cell>,</cell><cell>C</cell><cell>are weight metrices of appropriate</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>, 1</cell><cell>2</cell><cell></cell><cell></cell><cell cols="2">, 1</cell><cell>2</cell><cell>, 1</cell><cell>2</cell></row><row><cell cols="12">dimension and () f is an matrices , , C B A .</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">, 1</cell><cell>2</cell><cell></cell><cell></cell><cell cols="2">, 1</cell><cell>2</cell><cell>, 1</cell><cell>2</cell></row><row><cell cols="12">Theorem 2: let</cell><cell>g</cell><cell>:</cell><cell></cell><cell>P</cell><cell></cell><cell>u </cell><cell></cell><cell></cell><cell>m</cell><cell>be measurable and</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKOWLEDGEMENT</head><p>The work presented in this paper is supported by the Australian Research Council (ARC) under Discovery Project DP150101645 and the Latrobe university start-up grant. The third author acknowledges the support of the Austrian COMET-K2 program of the Linz Center of Mechatronics (LCM), funded by the Austrian federal government and the federal state of Upper Austria.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1063-6706 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TFUZZ.2016.2599855, IEEE Transactions on Fuzzy Systems suppressing fuzzy rule demand to low level. Furthermore, the feature selection mechanism of the eT2RFNN is capable of reducing the input dimension, thereby lowering network parameters to be saved in the memory. The efficacy of the double self-feedback loops is confirmed, where the learning performance dropped, when the second recurrent link was removed as exemplified in the eT2RFNN(C),(C1) C. Example 3: S&amp;P 500 Index Time Series This section focuses on evaluating the learning performance of eT2RFNN using the real-world financial time-series data. This case study is worth attempting, because the S&amp;P 500 index time series data characterise dynamic and volatile behaviours, making them hard to be dealt with, especially for a nonevolving algorithm. 60 years of daily index values were collected from the Yahoo finance website and refer to the period from January 3, 1950 to <ref type="bibr">March 12, 2009</ref>. In total, this corresponds to 14893 data streams, all fed in the training process. For the testing process, the same dataset as the training process was used but with reverse order <ref type="bibr" target="#b63">[63]</ref>. As the dynamic of the real-world financial data stream, this problem is highly volatile and the dynamic characteristic is evidenced by the two peaks and valley of data trend around 2000, 2003, and 2007. The S&amp;P 500 indexes are highly influenced by the US's market condition: strong growth in 2000, recession 2003, rebounded after 2003, recession in 2009. It is non-uniformly distributed in the interval of <ref type="bibr">[16.66,1565.15]</ref>. We also tested some configurations of eT2RFNN to delineate the merits of each learning constituents as done in the example 1. Fig. <ref type="figure">2</ref>(a) displays the predictive trend of eT2RFNN and the numerical results are encapsulated in Table <ref type="table">4</ref> for eT2RFNN(E) to exhibit its insensitivity to the delayed input feature, but we also provide the result exploiting all input attributes to disclose the efficacy of the online feature selection scenario. Fig. <ref type="figure">2</ref>(b) visualises the trace of fuzzy rules of eT2RFNN(C). The results are reported in Table <ref type="table">4</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An approach to online identification of Takagi-Sugeno fuzzy models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Angelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Filev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part B</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="484" to="498" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simpl_eTS: A simplified method for learning evolving Takagi-Sugeno fuzzy models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Angelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Filev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Fuzzy Systems (FUZZ)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1068" to="1073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">PANFIS: A Novel Incremental Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anavatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Angelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="68" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Type-2 fuzzy logic systems</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Karnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="643" to="658" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The concept of a linguistic variable and its application to approximate reasoning</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="249" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interval type-2 fuzzy logic systems: Theory and design</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="535" to="550" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A self-evolving interval type-2 fuzzy neural network with online structure and parameter learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tsao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1411" to="1424" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Computing derivatives in interval type-2 fuzzy logic system</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="98" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Type-2 fuzzy neural structure for identification and control of time-varying plants</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Abiyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kaynak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Electronics</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4147" to="4159" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A TSK-type based self-evolving compensatory interval type-2 fuzzy neural network (TSCIT2FNN) and its applications</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Electronics</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="447" to="459" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A recurrent self-organizing neural fuzzy inference network</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="828" to="845" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A recurrent self-evolving fuzzy neural network with local feedbacks and its application to dynamic system processing</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Fuzzy Sets and Systems</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="issue">19</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A TSK-type recurrent fuzzy network for dynamic systems processing by neural network and genetic algorithms</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="170" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identification and Prediction of Dynamic Systems Using an Interactively Recurrent Self-Evolving Fuzzy Neural Network</title>
		<author>
			<persName><forename type="first">Yang-Yin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyh-Yeong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Teng</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="310" to="321" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Recurrent Classifier based on An Incremental Meta-Cognitive-based Scaffolding Algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anavatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>online and in press</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A recurrent self-evolving interval type-2 fuzzy neural network for dynamic system processing</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1092" to="1105" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evolving Type-2 Fuzzy Classifier</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems, on line and in press</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>online and in press</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Single-hidden-layer fuzzy recurrent wavelet neural network: Applications to function approximation and system identification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ganjefar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tofighi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>online and in press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nonlinear Systems Modeling Based on Self-Organizing Fuzzy-Neural-Network with Adaptive Computation Algorithm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="554" to="564" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evolving Takagi-Sugeno Fuzzy Systems from Data Streams (eTS+)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Angelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evolving Intelligent Systems: Methodology and Applications</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Angelov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Filev</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">Kasabov )</forename><surname>Eds</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">John</forename><surname>Willey</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sons</surname></persName>
		</editor>
		<imprint>
			<publisher>IEEE Press Series on Computational Intelligence</publisher>
			<date type="published" when="2010-04">April 2010</date>
			<biblScope unit="page" from="21" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On-line incremental feature weighting in evolving fuzzy classifiers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets and Systems</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient Feature Selection via Analysis of Relevance and Redundancy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1205" to="1224" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fuzzy Wavelet Neural Networks for Identification and Control of Dynamic Plants-A Novel Structure and a Comparative Study</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Abiyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kaynak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Electronics</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3133" to="3140" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Nonlinear dynamic system identification using Chebyshev functional link artificial neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics-Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="505" to="511" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Introduction to Interval Analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Kearfott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cloud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>SIAM -Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">pClass:An Effective Classifier to Streaming Examples</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pratama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="369" to="386" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fuzzy Passive-Aggressive Classification: A Robust and Efficient Algorithm for Online Classification Problems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H-B</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="46" to="63" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A fast approach for automatic generation of fuzzy rules by generalized dynamic fuzzy neural networks</title>
		<author>
			<persName><forename type="first">S.-Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M-J</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Fuzzy System</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="578" to="594" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Evolving Fuzzy Systems ---Methodologies, Advanced Concepts and Applications</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Bayesian ARTMAP</title>
		<author>
			<persName><forename type="first">B</forename><surname>Vigdor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lerner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1628" to="1644" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection using feature similarity</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="301" to="312" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">RFCMAC: A novel reduced localized neuro-fuzzy system approach to knowledge extraction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Oentaryo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pasquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Quek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="12066" to="12084" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the entropy of continuous probability distributions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lazo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="120" to="122" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Data-Driven Interval Type-2 Neural Fuzzy System With High Learning Accuracy and Improved Model Interpretability</title>
		<author>
			<persName><forename type="first">C-F</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-Y.</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1781" to="1795" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">eT2FIS: An Evolving Type-2 Neural Fuzzy Inference System</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Quek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="124" to="148" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A Mutually Recurrent Interval Type-2 Neural Fuzzy System (MRIT2NFS) With Self-Evolving Structure and Parameters</title>
		<author>
			<persName><forename type="first">Y-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-Y.</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="492" to="509" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">GENEFIS:Towards An Effective Localist Network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Anavatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="547" to="562" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A new type of simplified fuzzy rule-based system</title>
		<author>
			<persName><forename type="first">P</forename><surname>Angelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">R</forename><surname>Yager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of General Systems</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="185" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">DENFIS: dynamic evolving neural-fuzzy inference system and its application for time series prediction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kasabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="144" to="154" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Online probabilistic learning for fuzzy inference systems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Oentaryo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5082" to="5096" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ANFIS: Adaptive-network-based fuzzy inference system</title>
		<author>
			<persName><forename type="first">J.-S</forename><forename type="middle">R</forename><surname>Jang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on System. Man. Cybernetic, part b: cybernetics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="665" to="684" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast and Accurate Self Organizing Scheme for Parsimonious Fuzzy Neural Network</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dynamic fuzzy neural networks-a novel approach to function approximation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M-J</forename><surname>Er</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Systems Man Cybernetics, part b: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="358" to="364" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Universal Approximation of a Class of Interval Type-2 Fuzzy Neural Networks in Nonlinear Identification</title>
		<author>
			<persName><forename type="first">O</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Melin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodriguez-Diaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">2013</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Knowledge Discovery from Data Streams</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Chapman &amp; Hall/CRC</publisher>
			<pubPlace>Boca Raton, Florida</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Sayed-Mouchaweh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
		<title level="m">Learning in Non-Stationary Environments: Methods and Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Self-tuning of 2 DOF control based on evolving fuzzy model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zdsar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dovzan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Skrjanc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="403" to="418" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">On-line Assurance of Interpretability Criteria in Evolving Fuzzy Systems ---Achievements, New Concepts and Open Issues</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">251</biblScope>
			<biblScope unit="page" from="22" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generalized Smart Evolving Fuzzy Systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12530-015-9132-6</idno>
	</analytic>
	<monogr>
		<title level="m">Evolving Systems, on-line and in press</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Autonomous Data Stream Clustering implementing Incremental Split-and-Merge Techniques ---Towards a Plug-and-Play Approach</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sayed-Mouchaweh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">204</biblScope>
			<biblScope unit="page" from="54" to="79" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Antonio Rodríguez Díaz: A hybrid learning algorithm for a class of interval type-2 fuzzy neural networks</title>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">R</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Melin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2175" to="2193" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Edge-Detection Method for Image Processing Based on Generalized Type-2 Fuzzy Logic</title>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Melin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><forename type="middle">I</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">R</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Castillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1515" to="1525" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Antonio Rodríguez Díaz: Application of interval type-2 fuzzy neural networks in non-linear identification and time series prediction</title>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">R</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Melin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1213" to="1224" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An Incremental Adaptive Life Long Learning Approach for Type-2 Fuzzy Embedded Agents in Ambient Intelligent Environments</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hagras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doctor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Callaghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="55" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Interval Type-2 Fuzzy Logic Congestion Control for Video Streaming Across IP Networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Jammeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fleury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hagras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghanbari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1123" to="1142" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Angelov</surname></persName>
		</author>
		<title level="m">Evolving Rule-based Models: A Tool for Design of Flexible Adaptive Systems</title>
		<meeting><address><addrLine>Heidelberg, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Fuzzy Model Identification Based on Cluster Estimation</title>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Chiu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent &amp; Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Approximate clustering via the mountain method</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Yager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Filev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1279" to="1284" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Data Mining: Practical machine learning tools and techniques</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Morgan Kaufmann</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
	<note>Third Edition</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Recurrent Neural Networks are Universal Approximators</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Schafer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Identifying Static and Dynamic Prediction Models for NOx Emissions with Evolving Fuzzy Systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Macian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guardiola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Klement</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2487" to="2500" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Data driven modeling based on dynamic parsimonious fuzzy neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pratama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="18" to="28" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Fuzzily connected multimodel systems evolving autonomously from data streams</title>
		<author>
			<persName><forename type="first">P</forename><surname>Angelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernernetics, Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="898" to="910" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A BCM Theory of Meta-Plasticity for Online Self-Reorganizing Fuzzy-Associative Learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Quek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="985" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
