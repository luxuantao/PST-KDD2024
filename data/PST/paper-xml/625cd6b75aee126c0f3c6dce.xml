<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How Different are Pre-trained Transformers for Text Ranking?</title>
				<funder ref="#_vcSs8Ea">
					<orgName type="full">Netherlands Organization for Scientific Research</orgName>
				</funder>
				<funder ref="#_t8YUnrU">
					<orgName type="full">Innovation Exchange Amsterdam</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-05">5 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">David</forename><surname>Rau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jaap</forename><surname>Kamps</surname></persName>
							<email>kamps@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">How Different are Pre-trained Transformers for Text Ranking?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-05">5 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2204.07233v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Neural IR</term>
					<term>BERT</term>
					<term>Sparse Retrieval</term>
					<term>BM25</term>
					<term>Analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, large pre-trained transformers have led to substantial gains in performance over traditional retrieval models and feedback approaches. However, these results are primarily based on the MS Marco/TREC Deep Learning Track setup, with its very particular setup, and our understanding of why and how these models work better is fragmented at best. We analyze effective BERT-based cross-encoders versus traditional BM25 ranking for the passage retrieval task where the largest gains have been observed, and investigate two main questions. On the one hand, what is similar? To what extent does the neural ranker already encompass the capacity of traditional rankers? Is the gain in performance due to a better ranking of the same documents (prioritizing precision)? On the other hand, what is different? Can it retrieve effectively documents missed by traditional systems (prioritizing recall)? We discover substantial differences in the notion of relevance identifying strengths and weaknesses of BERT that may inspire research for future improvement. Our results contribute to our understanding of (black-box) neural rankers relative to (well-understood) traditional rankers, help understand the particular experimental setting of MS-Marco-based test collections.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural information retrieval has recently experienced impressive performance gains over traditional term-based methods such as BM25 or Query-Likelihood <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>. Nevertheless, its success comes with the caveat of extremely complex models that are hard to interpret and pinpoint their effectiveness.</p><p>With the arrival of large-scale ranking dataset MS MARCO <ref type="bibr" target="#b0">[1]</ref> massive models such as BERT <ref type="bibr" target="#b4">[5]</ref> found their successful application in text ranking. Due to the large capacity of BERT (110m+ parameters), it can deal with long-range dependencies and complex sentence structures. When applied to ranking BERT can build deep interactions between query and document that allow uncovering complex relevance patterns that go beyond the simple term matching. Up to this point, the large performance gains achieved by the BERT Cross-Encoder are not well understood. Little is known about underlying matching principles that BERT bases its estimate of relevance on, what features are encoded in the model, and how the ranking relates to traditional sparse rankers such as BM25 <ref type="bibr" target="#b11">[12]</ref>. In this work, we focus on the Cross-Encoder (CE) BERT that captures relevance signals directly between query and document through term interactions between them and refer from now on to the BERT model as CE. First, we aim to gain a deeper understanding of how CE and BM25 rankings relate to each other, particularly for different levels of relevance by answering the following research questions:</p><p>RQ1: How do CE and BM25 rankings vary? RQ1.2: Does CE better rank the same documents retrieved by BM25? RQ1.3: Does CE better find documents missed by BM25? Second, we isolate and quantify the contribution of exact-and soft-term matching to the overall performance. To examine those are particularly interesting as they pose the most direct contrast between the matching paradigms of sparse-and neural retrieval. More concretely, we investigate: RQ2: Does CE incorporate "exact matching"? RQ3: Can CE still find "impossible" relevant results?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Even though little research has been done to understand the ranking mechanism of BERT previous work exists. <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b18">[19]</ref>, have undertaken initial efforts to open ranking with BERT as a black-box and empirically find evidence that exact term matching and term importance seem to play in an important role. Others have tested and defined well-known IR axioms <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b6">[7]</ref> or tried to enforced those axioms through regularization <ref type="bibr" target="#b12">[13]</ref>. Another interesting direction is to enforce sparse encoding and able to relate neural ranking to sparse retrieval <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Although related, the work in <ref type="bibr" target="#b15">[16]</ref> differs in two important aspects. First, they examine dense BERT retrievers which encode queries and documents independently. Second, they focus rather on the interpolation between BERT and BM25, whereas we specifically aim to understand how the two rankings relate to each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>The vanilla BERT Cross-Encoder (CE) encodes both queries and documents at the same time. Given input x ? {[CLS], q 1 , . . . , q n [SEP ], d 1 , . . . , d m , [SEP ]}, where q represents query tokens and d document tokens, the activations of the CLS token are fed to a binary classifier layer to classify a passage as relevant or non-relevant; the relevance probability is then used as a relevance score to re-rank the passages.</p><p>We conduct our experiments on the TREC 2020 Deep Learning Track's passage retrieval task on the MS MARCO dataset <ref type="bibr" target="#b0">[1]</ref>. For our experiments, we use the pre-trained model released by <ref type="bibr" target="#b7">[8]</ref>. To obtain the set of top-1000 documents we use anserini's <ref type="bibr" target="#b16">[17]</ref> BM25 (default parameters) without stemming, following <ref type="bibr" target="#b3">[4]</ref>. Table <ref type="table" target="#tab_0">1</ref> shows the baseline performance of BM25 and a vanilla BERT based cross-ranker (CE), re-ranking the 1,000 passages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RQ1: How do CE and BM25 rankings vary?</head><p>CE outperforms BM25 by a large margin across all metrics (see Tab. 1). To understand the different nature of the CE we trace where documents were initially ranked in the BM25 ranking. For this we split the ranking in different in four rank-ranges: 1-10, 11-100, 101-500, 501-1000 and will refer to them with ranges 10, 100, 500 and 1000 respectively from now on. We observe in which rank-range the documents were positioned with respect to the initial BM25 ranking. We show the results in form of heatmaps<ref type="foot" target="#foot_0">1</ref> in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Our initial goal is to obtain general differences between the ranking of CE and BM25 by considering all documents of the test collection (see Fig. <ref type="figure" target="#fig_0">1 (a)</ref>). First, we note that CE and BM25 vary substantially on the top of the ranking (33% CE@10), whereas at low ranks (60% CE@1000) the opposite holds. Second, we note that CE is bringing many documents up to higher ranks. Third, we observe that documents ranked high by BM25 are rarely ranked low by CE, suggesting exact matching to be a an important underlying ranking strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RQ1.2: Does CE better rank the same documents retrieved by</head><p>BM25?</p><p>To answer RQ1.2 we consider documents that were judged highly relevant or relevant according to the NIST judgments 2020. The results can be found in Fig. <ref type="figure" target="#fig_0">1</ref> (b),(c) respectively. Most strikingly, both rankers exhibit a low agreement (40%) on the documents in CE@10 for highly relevant documents hinting a substantial different notion of relevance for the top of the ranking of both methods.</p><p>For relevant documents we observe CE and BM25 overlap 46% at the top of the ranking and a large part (32%) comes from BM25@100, implying BM25 underestimated the relevance of many documents. The highest agreement between CE and BM25 here is in CE@500 (91%).  Interestingly, highly relevant documents that appear in lower ranks originate from high ranks in BM25 (CE@100: 12%, CE@500: 5%). This is an interesting finding as CE fails and underestimates the relevance of those documents, while BM25 -being a much simpler ranker -ranks them correctly. The same effect is also present for relevant documents. When considering documents that both methods ranked low we find a perfect agreement for @1000, showing that the two methods identify the same (highly-)relevant documents as irrelevant.</p><p>What about non-relevant documents that end up high in the ranking? CE brings up to CE@10 a large amount of non-relevant documents from low ranks (47% BM25@100, 23% BM25@500, and 5% BM@1000). Therewith overestimating the relevance of many documents that were correctly considered less relevant by BM25. We also note the little agreement of non-relevant documents @1000 (33%), hinting at a different notion of irrelevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">RQ1.3: Does CE better find documents missed by BM25?</head><p>To answer RQ1.3 we again consider documents that were judged (b) highly relevant and (c) relevant and refer to Fig. <ref type="figure" target="#fig_0">1</ref>, especially focusing on CE@10. The nature of CE, being too expensive for running it on the whole corpus, allows us to only study recall effects within the top-1000 documents. Hence, studying the top-10 results of CE will inform us best about the recall dynamics at high ranks. According to results in Fig. <ref type="figure" target="#fig_0">1</ref> (b) almost half (42%) of the highly relevant documents that are missed by BM25 are brought up from BM25@100, 13% from range BM25@500, and 5% from range BM25@1000. The same effect can be observed for relevant documents. This demonstrates the superior ability of CE to pull up (highly)-relevant documents that are missed by BM25 even from very low ranks. This is the domain where the true potential of the neural models over exact matching techniques lies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">RQ2: Does CE incorporate "exact matching"?</head><p>The presence of query words in the document is one of the strongest signals for relevance in ranking <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Our goal is to isolate the exact term matching effect, quantify its contribution to the performance, and relate it to sparse ranking. For this, we simply replace all non-query terms in the document with the [MASK] token leaving the model only with a skeleton of the original document and thus forcing it to rely on the exact term matches between query and document only. We do not fine-tune the model on this input. Note that there are no query document pairs within the underlying BM25 top-1000 run that have no term overlap. Results can be found in Tab. 2 under Only Q. CE with only the query words performs significantly lower than BM25 with regard to all metrics, finding clear support that CE is not leveraging exact matches sufficiently.</p><p>As in view of finding potential ways to improve CE, our results suggest that exact term matching can be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">RQ3: Can CE still find "impossible" relevant results?</head><p>While CE can leverage both, exact term-as well as "soft" matches, the biggest advantage over traditional sparse retrievers holds the ability to overcome lexical matches and to take context into account. Through "soft" matches neural models can retrieve documents that are "impossible" to retrieve using traditional potentially resulting in high recall gains. To isolate and quantify the effect of "soft matches" we follow our previous experiment but this time mask the appearance of the query words in the document. The model has now to rely on the surrounding context only. We do not fine-tune the model on this input. Note that in this setting BM25 would score randomly. Results can be found in Tab. 2 under Drop Q. We observe that CE can score documents sensibly with no overlapping query terms, largely outperforming when ranking on query terms only (Only Q). The model scores 49.89 NDCG@10 points losing only around 20 points with respect to non-manipulated input. CE might be able to fill-in the masked tokens from the context, as this makes up a main part of the Masked-Language modeling pre-training task. The model demonstrates its true potential here by drawing on its ability to understand semantics through the contextualization of query and document and to leverage its associate memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Discussion</head><p>Our experiments find evidence that documents at the top of the ranking are generally ranked very differently while a stronger agreement at the bottom of the ranking seems to be present. By investigating the rankings for different relevance levels we gain further insight. Even though, for (highly-)relevant documents there exists a bigger consensus at the top of the ranking compared to the bottom we find a discrepancy in the notion of high relevance between them for some documents, highlighting core differences between the two rankers.</p><p>We discover that CE is dramatically underestimating some of the highly relevant documents that are correctly ranked by BM25. This sheds light on the sub-optimal ranking dynamics of CE, sparking clues to overcome current issues to improve ranking in the future. Our analysis finds further evidence that the main gain in precision stems from bringing (highly-)relevant documents up from lower ranks (early precision). On the other hand, CE overestimates the relevance of many non-relevant documents where BM25 scored them correctly lower.</p><p>Through masking all but the query words within the documents we show that CE is not able to rank on the basis of only exact term matches only scoring a lot lower than BM25. By masking the query words in the document we demonstrate the ability of CE to score queries and documents without any lexical overlap with a moderate loss of performance, therefore demonstrating the true strength of neural models over traditional methods, that would completely fail in this scenario, in isolation.</p><p>We leave it to further research to qualitatively investigate the query-document pairs that BERT fails, but BM25 ranks correctly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Ranking differences between BERT Cross-Encoder (CE) and BM25:Origin of documents in CE ranking at different rank-ranges with respect to the initial BM25 ranking. More intuitively, each row indicates to what ratio documents stem from different rank-ranges. E.g., the top row can be read as the documents in rank 1-10 of the CE re-ranking originate 33% from rank 1-10, 41% from rank 11-100, 19% from rank 101-500 and 6.1% from rank 501-1000 in the initial BM25 ranking. The rank compositions are shown for (a) all, (b) highly relevant, (c) relevant, and (d) non-relevant documents according to the NIST 2020 relevant judgments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance of BM25 and crossencoder rankers on the NIST judgements of the TREC Deep Learning Task 2020.</figDesc><table><row><cell>Ranker</cell><cell>NDCG@10</cell><cell>MAP</cell><cell>MRR</cell></row><row><cell>BM25</cell><cell>49.59</cell><cell>27.47</cell><cell>67.06</cell></row><row><cell>BERT Cross-Encoder (CE)</cell><cell>69.33</cell><cell>45.99</cell><cell>80.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance of keeping only or removing the query terms from the input.</figDesc><table><row><cell>Model input</cell><cell>NDCG@10</cell><cell>MAP</cell><cell>MRR</cell></row><row><cell>Only Q</cell><cell>31.70</cell><cell>18.56</cell><cell>44.38</cell></row><row><cell>Drop Q</cell><cell>49.89</cell><cell>29.08</cell><cell>65.12</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The code for reproducing the heatmaps can be found under https://github.com/ davidmrau/transformer-vs-bm25</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments This research is funded in part by the <rs type="funder">Netherlands Organization for Scientific Research</rs> (<rs type="grantNumber">NWO CI # CISC.CC.016</rs>), and the <rs type="funder">Innovation Exchange Amsterdam</rs> (<rs type="grantName">POC grant</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vcSs8Ea">
					<idno type="grant-number">NWO CI # CISC.CC.016</idno>
				</org>
				<org type="funding" xml:id="_t8YUnrU">
					<orgName type="grant-name">POC grant</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<title level="m">MS MARCO: A human generated machine reading comprehension dataset</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diagnosing bert with retrieval heuristics</title>
		<author>
			<persName><forename type="first">A</forename><surname>C?mara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hauff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval 12035</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">605</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Overview of the TREC 2020 deep learning track</title>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
		<title level="m">Overview of the TREC 2019 deep learning track</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Splade v2: Sparse lexical and expansion model for information retrieval</title>
		<author>
			<persName><forename type="first">T</forename><surname>Formal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lassance</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piwowarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10086</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A white box analysis of colbert</title>
		<author>
			<persName><forename type="first">T</forename><surname>Formal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piwowarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Clinchant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="257" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Passage re-ranking with</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Padigela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<title level="m">Investigating the successes and failures of bert for passage re-ranking</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07531</idno>
		<title level="m">Understanding the behaviors of bert in ranking</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An axiomatic approach to diagnosing neural ir models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rennings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moraes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hauff</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-15712-8_32</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-15712-832" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="489" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR&apos;94</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An axiomatic approach to regularizing neural ranking models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 42nd international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="981" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Introduction to Modern Information Retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<ptr target="https://sigir.org/resources/museum/" />
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>McGraw-Hill, Inc., USA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Relevance: A review of and a framework for the thinking on the notion in information science</title>
		<author>
			<persName><forename type="first">T</forename><surname>Saracevic</surname></persName>
		</author>
		<idno type="DOI">10.1002/asi.4630260604</idno>
		<ptr target="https://doi.org/10.1002/asi.4630260604" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="321" to="343" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bert-based dense retrievers require interpolation with bm25 for effective passage retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zuccon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM SIGIR International Conference on Theory of Information Retrieval</title>
		<meeting>the 2021 ACM SIGIR International Conference on Theory of Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="317" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Anserini: Reproducible ranking baselines using lucene</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3239571</idno>
		<ptr target="https://doi.org/10.1145/3239571" />
	</analytic>
	<monogr>
		<title level="j">J. Data and Information Quality</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018-10">Oct 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">From neural re-ranking to neural ranking: Learning a sparse representation for inverted indexing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kamps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM international conference on information and knowledge management</title>
		<meeting>the 27th ACM international conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="497" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An analysis of bert in document ranking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1941" to="1944" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
