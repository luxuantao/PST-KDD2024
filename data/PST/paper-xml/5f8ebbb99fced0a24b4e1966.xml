<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Diversifying Search Results using Self-Attention Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xubo</forename><surname>Qin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
							<email>dou@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<address>
									<country>Renmin University of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
							<email>jirong.wen@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">Beijing Key Laboratory of Big Data Management and Analysis Methods</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Key Laboratory of Data Engineering and Knowledge Engineering</orgName>
								<address>
									<region>MOE</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Diversifying Search Results using Self-Attention Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3340531.3411914</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>CCS CONCEPTS â€¢ Information systems â†’ Information retrieval diversity Search Result Diversification; Self Attention</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Search results returned by search engines need to be diversified in order to satisfy different information needs of different users. Several supervised learning models have been proposed for diversifying search results in recent years. Most of the existing supervised methods greedily compare each candidate document with the selected document sequence and select the next local optimal document. However, the information utility of each candidate document is not independent with each other, and research has shown that the selection of a candidate document will affect the utilities of other candidate documents. As a result, the local optimal document rankings will not lead to the global optimal rankings. In this paper, we propose a new supervised diversification framework to address this issue. Based on a self-attention encoder-decoder structure, the model can take the whole candidate document sequence as input, and simultaneously leverage both the novelty and the subtopic coverage of the candidate documents. We call this framework Diversity Encoder with Self-Attention (DESA). Comparing with existing supervised methods, this framework can model the interactions between all candidate documents and return their diversification scores based on the whole candidate document sequence. Experimental results show that our proposed framework outperforms existing methods. These results confirm the effectiveness of modeling all the candidate documents for the overall novelty and subtopic coverage globally, instead of comparing every single candidate document with the selected sequence document selection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Research shows that most queries issued by users are short <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>, and these queries could be ambiguous or vague. For example, a user who issues the query "Java" may expect a result about "Java island", while another user with the same query may want information about "JAVA programming language". Even for a same user, she may also want diversified results which cover different aspects of the information she is looking for (for example, seeking for different cooking methods for "roast beef"). Search result diversification is proposed to solve the above problem by returning a diversified document list that can satisfy different information needs.</p><p>Existing search result diversification models can be divided into supervised and unsupervised models depending on whether supervised learning approaches are used. Most of the traditional approaches to search result diversification are unsupervised and they are based on handcrafted features and functions <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>. While in recent years, more and more researchers tried to use machine learning methods in search result diversification in order to learn an optimized ranking function automatically <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. To generate diversified results, these methods either explicitly model subtopic coverage of the results <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b13">14]</ref> (i.e., explicit approaches), or directly reduce result redundancy by comparing document-document similarity regardless the use of subtopics <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref> (i.e., implicit approaches).</p><p>To simplify the problem and accelerate the online ranking, existing methods usually formalize the diverse ranking process as the greedy document sequential selection. Those methods compare each of the candidate document with the selected document sequence and select the best candidate document which can provide the maximum additional information utility for the current selected document sequence. However, researchers <ref type="bibr" target="#b14">[15]</ref> have already proved that this greedy document selection mechanism may not lead to the global optimal rankings. This is because the previous methods only model the interaction between every single candidate document and the selected document sequence, ignoring the candidate document's interactions with other candidate documents. While the information utilities of all the candidate documents are not independent, when a candidate document is selected, the utilities of other documents will be affected. As a result, the sequential selection of every locally optimal document may not lead to a global optimal document ranking. This problem may be even more serious when the selected sequence is short or empty in the early stage of ranking. For example, assuming there are three candidate documents ğ‘‘ 1 , ğ‘‘ 2 , ğ‘‘ 3 , with ğ‘‘ 1 covering the subtopic ğ‘ 1 , ğ‘‘ 2 covering ğ‘ 2 , ğ‘ 3 and ğ‘‘ 3 covering ğ‘ 1 , and the three documents has got similar relevance scores to the given query. A greedy selection based model may select ğ‘‘ 1 for the first ranking position. Since the selected sequence is empty, any of the candidate documents can be seen as a "diversified document" after the empty selected sequence. However, the diverse ranking task aims to satisfy more user intents at former position, in the view of intent-based diversification metrics e.g. ğ›¼âˆ’nDCG, ğ‘‘ 2 may be a better selection comparing with ğ‘‘ 1 . In order to achieve the global optimal ranking, the model has to search all the ranking space, which is an NP-hard problem. Feng et al. <ref type="bibr" target="#b14">[15]</ref> proposed the M2DIV model with Monte-Caro Tree Search (MCTS) in order to explore a larger ranking space and raise the probability of selecting the global optimal document ranking. However, as a deep reinforced learning model with MCTS, the M2DIV is difficult to train since MCTS is so time consuming that the M2DIV propose another raw policy without MCTS <ref type="bibr" target="#b14">[15]</ref> in adaption to some online ranking tasks.</p><p>In this paper, we propose a new search result diversification framework to address the issues above. This framework can model all the candidate documents as a whole sequence, and leverage both the novelty and the subtopic coverage of every candidate document simultaneously. More specifically, we use a self-attention based encoder-decoder structure to model the interactions between candidate documents and subtopics. We call this framework Diversity Encoder with Self-Attention (DESA). The self-attention network has been widely used in order to learn the context-aware representations of words. Comparing with CNN and RNN, the self-attention network allows every item in the whole sequence to interact directly with each other simultaneously, and it can learn the long-range dependency well. In the task of search result diversification, we use the self-attention network to build an encoder-decoder framework for modeling the candidate document sequence and the subtopics. The encoder component can learn the document interactions globally in the whole candidate document sequence, indicating the novelty of every candidate document. And the decoder component can learn the matching distributions between the documents and the subtopics. Instead of comparing every single candidate document with the selected document sequence, the framework will model the whole candidate document sequence and jointly return the ranking scores of all the candidate documents in the ranking task. It doesn't depend on the greedy document selection process. We also give a theoretical analysis of how self-attention mechanism works in the task of search result diversification. Since self-attention network is suitable for parallel computing, the proposed DESA model is easy to train. Experimental results with the TREC Web Track data show that the model outperforms the state-of-the-art unsupervised or supervised diversification models significantly.</p><p>The contributions of the paper are summarized as follows:</p><p>(1) We propose a framework which can take the whole candidate document sequence as input and model the interactions between all the candidate documents for measuring their information utilities globally. Comparing with the greedy sequential selection approaches, this framework will get a higher probability of achieving the global optimal ranking.</p><p>(2) More specifically, we use a self-attention based encoderdecoder structure and model both the novelty and the subtopic coverage of the candidate documents.</p><p>(3) We theoretically analyze why self-attention is suitable to the search result diversification task. Experimental results verify the effectiveness of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Implicit and Explicit Diversification Models</head><p>Existing search result diversification models can be divided into implicit and explicit ones depending on modeling the user intents (represented as subtopics) explicitly. The implicit ranking model calculates the similarity between every candidate document and the previous selected documents, and assume that the more dissimilar the candidate document is to the selected documents, the more diversified it will be. The most typical implicit model is the MMR (Max Margin Relevance) <ref type="bibr" target="#b4">[5]</ref> model:</p><formula xml:id="formula_0">Score MMR = ğœ†score(ğ‘‘ ğ‘– , ğ‘) âˆ’ (1 âˆ’ ğœ†)max ğ‘‘ ğ‘— âˆˆğ‘† sim(ğ‘‘ ğ‘– , ğ‘‘ ğ‘— ),</formula><p>where score(ğ‘‘ ğ‘– , ğ‘) is relevance score of the current document candidate ğ‘‘ ğ‘– and the given query ğ‘, sim(ğ‘‘ ğ‘– , ğ‘‘ ğ‘— ) is the similarity between ğ‘‘ ğ‘– and the selected document ğ‘‘ ğ‘— in the selected set ğ‘†. In the view of the MMR model, the less similar the candidate document is with the selected documents, the more diversified it will be. The final ranking score of the candidate document is the linear combination of the relevance score and the novelty score. Inheriting the spirit of MMR, researchers have also proposed supervised methods, such as SVM-DIV <ref type="bibr" target="#b9">[10]</ref>, R-LTR <ref type="bibr" target="#b10">[11]</ref>), PAMM <ref type="bibr" target="#b11">[12]</ref>, and PAMM-NTN <ref type="bibr" target="#b12">[13]</ref>), for learning a better document similarity function automatically. The explicit approaches model the underlying user intents of the issued query, those intents are represented as subtopics. In the view of explicit diverse ranking, comparing with the selected document sequence, a diversified candidate document should cover as many new subtopics under the given query which has not been covered by the selected document as possible. Nowadays both unsupervised and supervised explicit approaches are proposed e.g. xQuAD <ref type="bibr" target="#b6">[7]</ref>, PM2 <ref type="bibr" target="#b7">[8]</ref>, HxQuAD/HPM2 <ref type="bibr" target="#b8">[9]</ref> and DSSA <ref type="bibr" target="#b13">[14]</ref>.</p><p>Those existing approaches used greedy document sequential selection. They compare every single candidate document with the selected document sequence, and choose the locally optimal document one-by-one to fill in the document ranking list. Since the information utilities of the candidate documents are not independent, this strategy may not lead to global optimal rankings. Based on the reinforced learning approach MDP-DIV <ref type="bibr" target="#b13">[14]</ref>, Feng <ref type="bibr" target="#b14">[15]</ref> proposed the M2DIV model with the Monte-Caro Tree Search (MCTS) to search a larger ranking space and minimize the gap between the local optimal and global optimal rankings. However, M2DIV is difficult to train since MCTS is time consuming <ref type="bibr" target="#b14">[15]</ref>, and M2DIV only models the document novelty, ignoring the subtopic coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-Attention in Information Retrieval</head><p>The self-attention mechanism is a kind of attention mechanism modeling each position in a sequence and compute the representation for each hidden state of the sequence. Recently, the models fully based on self-attention mechanism (denoted as self-attention  <ref type="bibr" target="#b15">[16]</ref> in the Neural Machine Translation (NMT) task, have achieved great successes on many NLP tasks. Researchers have used self-attention networks, e.g. GPT <ref type="bibr" target="#b16">[17]</ref>, BERT <ref type="bibr" target="#b17">[18]</ref> and ERNIE <ref type="bibr" target="#b18">[19]</ref>, as alternatives to RNNs and CNNs in many NLP tasks. However, to the best of our knowledge, only a few researchers <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> have tried to use the self-attention network in the information retrieval tasks. There are no self-attention based models designed for the search result diversification task.</p><p>In this paper, we propose using self-attention to model the interactions of candidate documents and subtopics for search result diversification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE DIVERSIFICATION FRAMEWORK</head><p>In this section, we will first describe the overall structure of the selfattention based diversification framework DESA, then introduce the details of each component and the optimization process. We also propose a theoretical analysis to explain why self-attention is suitable to the search result diversification task. Finally, we compare DESA with existing models and discuss their relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Table <ref type="table" target="#tab_0">1</ref> shows the notations and their descriptions used in this paper. Given a query ğ‘, we have ğ‘˜ subtopics I representing different user intents, and ğ‘ ğ‘– is the ğ‘–âˆ’th subtopic (ğ‘– âˆˆ [1, ğ‘˜] and ğ‘ ğ‘– âˆˆ I). Suppose D is a list of candidate documents for ğ‘, the target of search result diversification is to return a new ranked document list R based on initial ad-hoc rank list D, where diverse documents covering different subtopics are ranked higher in R and redundant documents are ranked lower.</p><p>Different from the ad-hoc retrieval task which is solely designed for returning relevant documents, search result diversification needs to consider both the relevance of each single document and the similarity between them. As introduced in Section 1, most existing diversification models used the greedy selection approach: they iteratively select the next best document by evaluating the relevance of each remaining document and its novelty bring to the results based on the list of documents which are already selected in those early iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DESA: the Overall Framework</head><p>In Figure <ref type="figure" target="#fig_0">1</ref>, we show the overall structure of our proposed DESA framework. Different from existing approaches which greedily select the next best documents and sequentially generate R, DESA calculates all diverse ranking scores for each candidate document simultaneously, then sorts the documents based on the scores and gets the diverse ranking list directly. i.e., DESA directly gets a list of ranking scores S ğ· by: S ğ· = DESA(D, ğ‘, I).</p><p>(</p><formula xml:id="formula_1">)<label>1</label></formula><p>DESA takes the whole candidate document sequence as input and models the interactions between all the candidate documents for measuring their information utilities globally. Comparing with the greedy sequential selection approaches, this framework will get a higher probability of achieving the global optimal ranking. More specifically, we use an encoder-decoder structure based on self attention to model the relationship between each document in D and each subtopic ğ‘ ğ‘– âˆˆ I. The encoder component takes the whole candidate document sequence D as input, and returns the representations of all the documents simultaneously. After interacting with every other candidate document, the document representations can indicate the novelty or dissimilarity of a document. Then the decoder component takes both document sequence and subtopics as input, returning the decoded document representations indicating the subtopic coverage of the documents. Finally, those representations will be used by a learning-to-rank function to judge the diverse ranking scores of the documents. Key components of the framework are briefly introduced as follows.</p><p>(1) Document Representations. Suppose ğ‘‘ ğ‘¡ is the ğ‘¡-th document in D, ğ’… ğ‘¡ is the initial distributed representation of the document ğ‘‘ ğ‘¡ . In order to avoid overfitting, we follow <ref type="bibr" target="#b21">[22]</ref> and use unsupervised methods doc2vec <ref type="bibr" target="#b22">[23]</ref> to generate the initial document representations instead of building the optimized document representations automatically.</p><p>(2) Self-attention Encoder. The self-attention encoder in the framework of DESA takes ğ‘« as input and returns the representations ğ‘¯ enc ğ‘« of the whole document sequence. The encoder also takes the embeddings of subtopics ğ‘° as input and returns the representations ğ‘¯ enc ğ‘° for all the subtopic. i.e., we have:</p><formula xml:id="formula_2">ğ‘¯ enc ğ· = SelfAttnEnc(ğ‘«), ğ‘¯ enc ğ¼ = SelfAttnEnc(ğ‘° ),</formula><p>where the self-attention encoder is denoted as SelfAttnEnc, which will be introduced in the next section.</p><p>(3) Self-attention Decoder. The decoder will take the encoded representation of document sequence ğ‘¯ enc ğ· and subtopics ğ‘¯ enc ğ¼ as an input, and return the decoded representations ğ‘¯ dec ğ· for all the documents. These decoded representations model the subtopic coverage of the documents. This step can be described as the following Full Paper Track CIKM '20, October 19-23, 2020, Virtual Event, Ireland equations, and the decoder is denoted as SelfAttnDec:</p><formula xml:id="formula_3">ğ‘¯ dec ğ· = SelfAttnDec(ğ‘¯ enc ğ· , ğ‘¯ enc ğ¼ ), ğ’‰ enc ğ‘¡ = ğ‘¯ enc ğ‘« [index(t)], ğ’‰ dec ğ‘¡ = ğ‘¯ dec ğ‘« [index(t)],</formula><p>where ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥ (ğ‘¡) is the operation of getting the vector at index ğ‘¡. For the ğ‘¡âˆ’th document, the encoded and decoded representations ğ’‰ enc ğ‘¡ and ğ’‰ dec ğ‘¡ are used to get the document's ranking score. (4) Subtopic Document Ranking. We use learning-to-rank to learn the relevance score of the ğ‘–-th subtopic ğ‘  ğ‘ ğ‘– through the subtopic relevance features ğ’™ ğ‘ ğ‘– :</p><formula xml:id="formula_4">ğ‘  ğ‘ ğ‘– = ğ’™ ğ‘‡ ğ‘ ğ‘– ğ’˜ ğ‘Ÿ (ğ‘– âˆˆ [1, ğ‘˜]).</formula><p>Here ğ’˜ ğ‘Ÿ is a learnable parameter. We use the same relevance features as the previous work <ref type="bibr" target="#b13">[14]</ref> for ğ’™ ğ‘ and ğ’™ ğ‘ ğ‘– , including BM25, TF-IDF, language model scores, Page Rank, the numbers of incoming links and outgoing links, et al. More details about these features can be found in <ref type="bibr" target="#b13">[14]</ref> and we omit the details due to space limitation. In the future, we plan to explore more neural-based features.</p><p>(5) The Final Ranking. The summarized document feature vectors ğ’— ğ‘‡ ğ‘‘ ğ‘¡ ,ğ‘,ğ‘ ğ‘– are concatenated by the following components: the query relevance features ğ’™ ğ‘ , the encoded document representation ğ’‰ enc ğ‘¡ and decoded document representation ğ’‰ dec ğ’• , and the relevance scores of all the ğ‘˜ subtopics [ğ‘  ğ‘ 1 , . . . , ğ‘  ğ‘ ğ‘˜ ]. Note that we use the same set of ranking features for query ğ‘ with those used for subtopics as introduced in step (4). Given the document feature vectors ğ’— ğ‘‡ ğ‘‘ ğ‘¡ ,ğ‘,ğ‘ ğ‘– , we use learning-to-rank to train the final ranking models. The ranking model then returns the ranking score ğ‘  ğ‘¡ âˆˆ S ğ· for the ğ‘¡âˆ’th document ğ‘‘ ğ‘¡ . We then generate the diversified ranking list R by sorting all the candidate documents with their ranking scores in S ğ· . Recall that different from those greedy sequential selection based models, DESA doesn't depend on the sequential selection process. This is similar to some ad-hoc ranking models such as SetRank <ref type="bibr" target="#b19">[20]</ref>.</p><p>This process is formulated as the following equations:</p><formula xml:id="formula_5">ğ’— ğ‘‘ ğ‘¡ ,ğ‘,ğ‘ ğ‘– = [ğ’™ ğ‘ ; ğ’‰ enc ğ‘¡ ; ğ’‰ dec ğ’• ; ğ‘  ğ‘ 1 , . . . , ğ‘  ğ‘ ğ‘˜ ],<label>(2)</label></formula><formula xml:id="formula_6">ğ‘  ğ‘¡ = ğ’— ğ‘‡ ğ‘‘ ğ‘¡ ,ğ‘,ğ‘ ğ‘– ğ’˜ ğ‘£ .<label>(3)</label></formula><p>where ğ’˜ ğ‘£ is a learnable parameter, [; ] means the concatenation.</p><p>In the remaining part of this section, we will introduce the components in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Self-Attention Encoder Component</head><p>The whole self-attention encoder component denoted as SelfAttnEnc takes all the candidate document embeddings as a whole document sequence ğ‘«, and returns all the document representations as a whole matrix denoted as ğ‘¯ enc ğ‘« in parallel. The representations will indicate the novelty of each document comparing with other candidate documents. In this section we will introduce the implementation of self-attention encoder in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.1</head><p>The Attention Function. In the search result diversification task, the vector representations of documents are used as input to the self-attention layer. Different from RNN, self-attention network will not model the sequence information explicitly, so the standard  Transformer structure also includes an optional component of positional encoding to incorporate the sequence information. Here we deploy an optional learnable position embeddings for capturing the sequence information of the documents and concatenate them with the document embeddings.</p><formula xml:id="formula_7">â„ ğ‘¡ ğ‘‘ğ‘’ğ‘ ] ğ» ğ· ğ‘’ğ‘›ğ‘ = [â„ 1 ğ‘’ğ‘›ğ‘ , â€¦ , â„ ğ‘› ğ‘’ğ‘›ğ‘ àµ§ ğ» ğ¼ ğ‘’ğ‘›ğ‘ = [â„ ğ‘1 ğ‘’ğ‘›ğ‘ , â€¦ , â„ ğ‘ğ‘˜ ğ‘’ğ‘›ğ‘ àµ§ ğ» ğ· ğ‘‘ğ‘’ğ‘ = [â„ 1 ğ‘‘ğ‘’ğ‘ , â€¦ , â„ ğ‘› ğ‘‘ğ‘’ğ‘ ] â„ ğ‘¡ ğ‘’ğ‘›ğ‘ = ğ» ğ· ğ‘’ğ‘›ğ‘ [ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥(ğ‘¡) àµ§ â„ ğ‘¡ ğ‘‘ğ‘’ğ‘ = ğ» ğ· ğ‘‘ğ‘’ğ‘ [ğ‘–ğ‘›ğ‘‘ğ‘’ğ‘¥(ğ‘¡)</formula><p>We use the multi-layer encoder block of the Transformer to implement DESA's self-attention component, based on the scaled dot-product attention function denoted as Attn follows:</p><formula xml:id="formula_8">Attn(ğ’’, ğ‘², ğ‘½ ) = Softmax( ğ’’ğ‘² ğ‘‡ âˆš ğ‘‘ )ğ‘½ . (<label>4</label></formula><formula xml:id="formula_9">)</formula><p>where ğ’’, ğ‘² and ğ‘½ denote the query, key and value matrices of the attention function. It should be addressed that the concept "query" here represents the query in dot-product attention, which is not the "query" in information retrieval. In search result diversification tasks, the model will take the sequence of document representations ğ‘« as an input, and the query matrix can be defined as ğ’’ = ğ‘«.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.2</head><p>The Multi-Head Attention Component. Following by some previous work e.g. SetRank <ref type="bibr" target="#b19">[20]</ref>, we use the multi-head strategy in order to learn multiple aspects of different documents. The multihead attention strategy denoted as MultiHead will first project the inputs ğ’’, ğ‘², ğ‘½ into â„ different heads with the dimension ÃŠ = ğ¸/â„:</p><formula xml:id="formula_10">MultiHead(ğ’’, ğ‘², ğ‘½ ) = [ğ’‚ 1 ; . . . ; ğ’‚ â„ ],<label>(5)</label></formula><p>where ğ’‚ ğ‘– is defined by:</p><formula xml:id="formula_11">ğ’‚ ğ‘– = Attn(ğ’’ğ‘¾ ğ‘„ ğ‘– , ğ‘²ğ‘¾ ğ¾ ğ‘– , ğ‘½ğ‘¾ ğ‘‰ ğ‘– ), ğ‘– âˆˆ [1, â„].<label>(6)</label></formula><p>Here all those ğ‘¾ parameters are learnable. Previous research <ref type="bibr" target="#b19">[20]</ref> has shown that using the multi-head strategy may help the selfattention network to learn better document similarity distribution at multi aspects. For the self-attention, ğ’’ = ğ‘² = ğ‘½ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.3</head><p>The Overall Structure of the Self-Attention Encoder. The overall structure of the encoder component is a multi-layer stack of multi-head self-attention block. Similar as the original Transformer encoder block, each of those self-attention encoder layers contains a dropout layer and a fully connected feed-forward network (denoted as FeedForward) with ReLU function as activation function.</p><p>The ğ‘–âˆ’th layer of the block is denoted as MSB ğ‘– and the encoder component SelfAttnEnc with ğ¿ layers can be described as follows:</p><formula xml:id="formula_12">SelfAttnEnc(ğ‘«) = MSB L (MSB Lâˆ’1 (. . . MSB 1 (ğ‘«)),<label>(7)</label></formula><p>MSB(ğ‘¯ prev ) = LayerNorm(ğ‘¿ + FeedForward(ğ‘¿ )), ( <ref type="formula">8</ref>)</p><formula xml:id="formula_13">ğ‘¿ = LayerNorm(ğ‘¯ prev + MultiHead(ğ‘¯ prev , ğ‘¯ prev , ğ‘¯ prev )),<label>(9)</label></formula><p>where LayerNorm denotes the layer normalization operation <ref type="bibr" target="#b23">[24]</ref>, ğ‘¯ prev is the output hidden state matrix of the previous encoder layer, and ğ‘¯ prev = ğ‘« is for the first layer.</p><p>After multi-layers of multi-head attention interactions, the output hidden state of ğ‘› input documents ğ‘¯ output = [ğ’‰ enc 1 , . . . , ğ’‰ enc ğ‘› ] can be used as the encoded document representations ğ‘¯ enc ğ· . This representation can indicate the novelty of a document, and the learning-to-rank function can take this representations to judge if a candidate document is novel or redundant comparing with other candidate documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Self-Attention Decoder Component</head><p>As we described in Section 3.2, the encoder can also take the subtopics as inputs, and return the encoded representation of the subtopics. This is because the subtopic embeddings we used are actually the document embeddings. We use the subtopic embeddings released by Jiang et al. <ref type="bibr" target="#b13">[14]</ref> based on doc2vec. The subtopic embeddings is produced from the pseudo documents of those corresponding subqueries: retrieve top Z documents with traditional IR model (e.g. BM25) first, and then concatenate these documents together to produce the pseudo documents. Those embeddings of the pseudo documents will be used as the embeddings of the subtopics.</p><p>The encoded subtopic representations are important to the decoder, since these representations include the attention distributions of the subtopics. In diverse ranking tasks, the available subtopics are mined from the query and they are usually more than the actual user intents. Comparing with the user intents, the subtopics may still contain redundancy and mislead the diversification model. And the encoded subtopic representations include the encoder attention distributions of the subtopics, these distributions can be used to leverage the subtopics' potential redundancy and minimize the misleading effect.</p><p>The decoder structure will take the representation of documents as query matrix, and subtopics as key and value matrix, returning the ğ‘¯ dec representation matrix for the documents with the multihead attention function:</p><formula xml:id="formula_14">ğ‘¯ dec = SelfAttnDec(ğ‘¯ enc ğ· , ğ‘¯ enc ğ¼ ),<label>(10)</label></formula><p>SelfAttnDec(ğ‘¯ enc ğ· , ğ‘¯ enc ğ¼ ) = MultiHead(ğ‘¯ enc ğ· , ğ‘¯ enc ğ¼ , ğ‘¯ enc ğ¼ ). (11) The output of the decoder ğ’‰ dec ğ‘¡ will be the subtopic representation of the document ğ‘‘ ğ‘¡ , this representation models the subtopics coverage of document ğ‘‘ ğ‘¡ . The rest part of the decoder component is just the same as the encoder component, including feed-forward network, ReLU activation function and layer normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training and Optimization</head><p>In this section we will introduce the training and optimization process of DESA in details. As we described above, DESA will take the document sequence and subtopics as input, and return the ranking scores of all those documents in the given document sequence. In the training phase, the score of a ranking ğ‘Ÿ is calculated by summing up all the scores of documents in ğ‘Ÿ :</p><formula xml:id="formula_15">ğ‘  ğ‘Ÿ = |ğ‘Ÿ | ğ‘–=1 ğ‘  ğ‘– .<label>(12)</label></formula><p>3.5.1 The list-pairwise sampling. Since the dataset of search result diversification task is limited, we inherit the list-pairwise sampling approach from Jiang et al. <ref type="bibr" target="#b13">[14]</ref> in order to get enough training samples. We are using pairs of training samples (ğ¶, ğ‘‘ 1 , ğ‘‘ 1 ) with common context ğ¶, appending document pair ğ‘‘ 1 and ğ‘‘ 2 to generate the document sequence pair ğ‘Ÿ 1 and ğ‘Ÿ 2 , and the metric(e.g. ğ›¼-nDCG) of positive document ranking sequence ğ‘€ (ğ‘Ÿ 1 ) should be better than the negative ranking sequence ğ‘€ (ğ‘Ÿ 2 ). The sampling process is described as follows: first the contexts ğ¶ with different lengths are obtained from both ideal rankings and random sampled rankings, then the rest of the candidate documents are traversed, sampling a pair of document (ğ‘‘ 1 , ğ‘‘ 2 ) when [ğ¶, ğ‘‘ 1 ] and [ğ¶, ğ‘‘ 2 ] are leading to different metrics. When using the listpairwise samples, the original loss function can be defined as a binary classification log-loss formation:</p><formula xml:id="formula_16">ğ¿ğ‘œğ‘ ğ‘  = ğ‘ âˆˆğ‘„ ğ‘  âˆˆğ‘† ğ‘ |Î”ğ‘€ |[ğ‘¦ ğ‘  log(ğ‘ƒ (ğ‘Ÿ 1 , ğ‘Ÿ 2 )) + (1 âˆ’ ğ‘¦ ğ‘  ) log(1 âˆ’ ğ‘ƒ (ğ‘Ÿ 1 , ğ‘Ÿ 2 ))]<label>(13</label></formula><p>) where ğ‘  is a pair of samples and ğ‘† ğ‘ is all the sample pairs of query ğ‘, ğ‘„ is the set of all the queries, ğ‘¦ ğ‘  = 1 for positive and 0 for negative, ğ‘ƒ (ğ‘Ÿ 1 , ğ‘Ÿ 2 ) = ğœ (ğ‘  ğ‘Ÿ 1 âˆ’ğ‘  ğ‘Ÿ 2 ) for the probability of being positive. Î”ğ‘€ = ğ‘€ (ğ‘Ÿ 1 )âˆ’ğ‘€ (ğ‘Ÿ 2 ) represents the weights of this sample, meaning that if the metric gap between the positive and negative rankings is larger, the sample will be more important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.5.2</head><p>The sequence mask for training. In the training phase, both the positive and the negative samples are the ground truth rankings, not the candidate document sequence. So the self-attention components are modified with a sequence mask used in the original Transformer decoder structure. Similar as the behavior of the users, the diverse ranking task is a top-down process and the evaluation metrics of the document at position ğ‘– should not be affected by the Full Paper Track CIKM '20, October 19-23, 2020, Virtual Event, Ireland document at position ğ‘— ( ğ‘— &gt; ğ‘–). The sequence mask will prevent the unexpected self-attention interactions and make sure every document will only interact with itself and those documents at former positions. The scores of documents at former position will not be affected by the documents at latter position. Notice that the sequence mask will only take effect in the training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.5.3</head><p>The context-based pairwise loss function. As we described in Equation ( <ref type="formula" target="#formula_15">12</ref>), the scores of a ranking ğ‘Ÿ is the sum of all the document ranking scores in the sequence. For the sampling pair</p><formula xml:id="formula_17">ğ‘Ÿ 1 = [ğ¶, ğ‘‘ 1 ] and ğ‘Ÿ 2 = [ğ¶, ğ‘‘ 2 ] we've got ğ‘  ğ‘Ÿ 1 = ğ‘– âˆˆğ¶ 1 ğ‘  ğ‘– + ğ‘  ğ‘‘ 1 and ğ‘  ğ‘Ÿ 2 = ğ‘– âˆˆğ¶ 2 ğ‘  ğ‘– + ğ‘  ğ‘‘ 2 .</formula><p>Here ğ¶ 1 = ğ¶ 2 = ğ¶. As we described above, ignoring the effect of dropout layers, the sequence mask in training phase will strictly ensure that ğ‘– âˆˆğ¶ 1 ğ‘  ğ‘– = ğ‘– âˆˆğ¶ 2 ğ‘  ğ‘– . So we've got:</p><formula xml:id="formula_18">ğ‘  ğ‘Ÿ 1 âˆ’ ğ‘  ğ‘Ÿ 2 = ğ‘  ğ‘‘ 1 âˆ’ ğ‘  ğ‘‘ 2 , ğ‘ƒ (ğ‘Ÿ 1 , ğ‘Ÿ 2 ) = ğ‘ƒ (ğ‘‘ 1 , ğ‘‘ 2 ). (<label>14</label></formula><formula xml:id="formula_19">)</formula><p>Denoting the binary classification log-loss function as ğ¿ğ‘œğ‘”ğ¿ğ‘œğ‘ ğ‘ , Equation ( <ref type="formula" target="#formula_16">13</ref>) can be simplified as:</p><formula xml:id="formula_20">ğ¿ğ‘œğ‘ ğ‘  = ğ‘ âˆˆğ‘„ [ğ¶,(ğ‘‘ 1 ,ğ‘‘ 2 ) ] âˆˆğ‘† ğ‘ |Î”ğ‘€ |LogLoss(P(d 1 , d 2 )).<label>(15)</label></formula><p>This is the definition of the context-based pairwise function. For search result diversification task, the scores of ğ‘‘ 1 and ğ‘‘ 2 depends on the context ğ¶, but since the metrics of the context documents should not be affected by the latter documents, the ranking scores of ğ‘– âˆˆğ¶ ğ‘  ğ‘– will not affect the loss function. This means that the contextbased pairwise loss function is actually defined in the form of a pairwise loss function for the document pair (ğ‘‘ 1 , ğ‘‘ 2 ). The target of the model optimization is to maximize the distance between positive document ğ‘‘ 1 and negative document ğ‘‘ 2 . When the model is being trained, the goal of optimization is to improve the model's ability of indicating if a single document in the candidate sequence is novel and covers more subtopics than the other candidate documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">The Ranking Process with Self-Attention</head><p>As we described above, research shows that the diversity ranking is NP-hard and for most of the previous models, greedy sequential selection is a common solution <ref type="bibr" target="#b24">[25]</ref>. Those models will compare every candidate document with the selected sequence and select the best candidate document one-by-one appending it into the selected sequence. For our self-attention based framework, when the ranking process starts, the model will take a non-diversified initial ranking sequence as input, and jointly return the diversified ranking scores of all those documents. Similar as some other selfattention based ad-hoc ranking approach e.g. SetRank <ref type="bibr" target="#b19">[20]</ref>, the model can return the ranking list with sorting all the candidate documents with their ranking scores. Different from the greedy document sequential selection models, DESA doesn't depend on the selected document sequnece.</p><p>With globally measuring all the candidate documents, DESA will outperform the previous models especially at former ranking positions. We will take the example in Section 1 to explain. Assuming there are three candidate documents ğ‘‘ 1 , ğ‘‘ 2 , ğ‘‘ 3 , with ğ‘‘ 1 covering the subtopic ğ‘ 1 , ğ‘‘ 2 covering ğ‘ 2 , ğ‘ 3 and ğ‘‘ 3 covering ğ‘ 1 , and the three documents has got similar relevance scores to the given query. Comparing with the greedy document selection models, DESA will return a higher ranking scores for ğ‘‘ 2 . since ğ‘‘ 2 is novel and covers more subtopics comparing with ğ‘‘ 1 and ğ‘‘ 3 . Then the ğ‘‘ 2 will be put on a former ranking position. This process indicates the advantage of DESA, because the goal of search result diversification is to satisfy more user intents at former ranking positions, and promoting the documents covering more subtopics to the former ranking positions will be more suitable to improve the user experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Theoretical Analysis</head><p>In this section we will analyze the effect of self-attention in the encoder-decoder structure of DESA. Here we will describe why self-attention is suitable in the diverse ranking task in details. For simplicity, we will first focus on a single-layer self-attention function in the encoder component and ignore those assist strategies e.g. positional embedding, multi-head attention or layer normalization.</p><p>The self-attention interaction of the document sequence D is calculated in parallel as a whole matrix, and the attention score can be written as the following equation focusing on the ğ‘¡-th document ğ‘‘ ğ‘¡ represented as ğ‘ ğ‘¡ , discarding the scalar factor âˆš ğ‘‘:</p><formula xml:id="formula_21">Score Attn (ğ’’ ğ’• , ğ‘² ) = Softmax(ğ’’ ğ’• ğ‘² ğ‘‡ ). (<label>16</label></formula><formula xml:id="formula_22">)</formula><p>As we described above, for self-attention, it can be approximated that ğ’’ = ğ‘² = ğ‘½ = ğ‘«, and ğ’’ ğ’• â‰ˆ ğ’… ğ’• . The ğ’’ ğ’• ğ‘² ğ‘‡ in Equation ( <ref type="formula" target="#formula_21">16</ref>) can be seen as the dot product scores between the ğ‘¡-th document and each document in the sequence including itself. With the softmax function, those scores will be converted into weights. Since the dot product of two documents can represent the similarity score between the two documents, those weights model the similarity distribution between ğ‘‘ ğ‘¡ and every document in the sequence. The self-attention output of ğ‘‘ ğ‘¡ is defined as follow:</p><formula xml:id="formula_23">â„ ğ‘¡ = Softmax([ğ‘  1 , . . . , ğ‘  ğ‘› ]) ğ‘‡ ğ‘½ = [ğ‘¤ 1 , . . . , ğ‘¤ ğ‘› ]ğ‘½ = ğ‘Š ğ‘‡ ğ‘¡ ğ‘½ .</formula><p>Here ğ‘› is the length of document sequence, ğ‘  ğ‘– is the dot product between document ğ‘‘ ğ‘¡ and ğ‘‘ ğ‘– , and ğ‘¤ ğ‘– is the similarity weight converted from ğ‘  ğ‘– . Section 3.5.1 shows the details of list-pairwise sampling for training. With shared selected context document sequence ğ¶ , positive and negative document pair ğ‘‘ pos , ğ‘‘ neg , the positive and negative samples can be written as [ğ¶, ğ‘‘ p ] and [ğ¶, ğ‘‘ n ]. Due to the property of softmax function, ğ‘› ğ‘–=1 ğ‘¤ ğ‘– = 1, for the weights distribution of document ğ‘‘ ğ‘¡ , the equation can be written as:</p><formula xml:id="formula_24">ğ‘– âˆˆğ¶ ğ‘¤ ğ‘– + ğ‘¤ ğ‘¡ = 1.</formula><p>In the view of MMR, comparing with the context ğ¶, the positive document ğ‘‘ pos should be a novel document, which means that ğ‘‘ pos should be dissimilar with the documents in the context ğ¶. The dot product scores of ğ‘‘ pos with other documents ğ‘‘ ğ‘– (ğ‘– âˆˆ ğ¶) should be significantly smaller than the scores of ğ‘‘ pos with itself, indicating ğ‘  pos &gt;&gt; ğ‘– âˆˆğ¶ ğ‘  ğ‘– (ğ‘– âˆˆ ğ¶). After the softmax function, it has got ğ‘¤ pos &gt;&gt; ğ‘– âˆˆğ¶ ğ‘¤ ğ‘– (ğ‘– âˆˆ ğ¶). For the negative document ğ‘‘ neg , since it's a redundant document, the dot product scores with the context documents will be close to the score with itself, and ğ‘¤ neg &gt;&gt; ğ‘– âˆˆğ¶ ğ‘¤ ğ‘– (ğ‘– âˆˆ ğ¶) is no longer valid. As a result, a positive document will gain an attention distribution concentrated to the document itself, while a negative document will gain an average distribution. This is identical to the spirit of MMR, since a novel document should be dissimilar with the other documents, and its similarity scores with other documents should be much smaller than the score with itself.</p><p>With the context-based pairwise optimization, the attention distribution distance gap the positive and negative documents will get bigger, and the learning-to-rank function of the model will be trained to return a ranking score of ğ‘‘ pos higher than ğ‘‘ neg . With more self-attention layers, the distribution distance between the positive and negative samples will be expanded and the learningto-rank function will be more effective to judge the novelty of a candidate document.</p><p>This analysis mainly depends on the self-attention encoder, and the principle of the decoder component is similar as the encoder:</p><formula xml:id="formula_25">ğ’‰ dec ğ‘¡ = Attn(ğ’‰ enc ğ‘¡ , ğ‘¯ enc ğ¼ , ğ‘¯ enc ğ¼ ) = [ğ‘¤ dec 1 , . . . , ğ‘¤ dec ğ‘› ]ğ‘¯ enc ğ¼ = (ğ‘Š dec ğ‘¡ ) ğ‘‡ ğ‘¯ enc ğ¼ .</formula><p>Here the ğ‘¤ dec ğ‘– is the attention weights between document ğ‘‘ ğ‘¡ and subtopic ğ‘ ğ‘– . Similar as the encoder attention distribution, the decoder attention distribution of ğ’‰ dec ğ‘¡ will be focusing on the subtopics relevant to ğ‘‘ ğ‘¡ , and the irrelevant subtopics will be ignored with lower attention weights. The decoder attention distributions of positive and negative documents will be similar as the encoder attention. For the positive document the attention distribution will be concentrated to the relevant subtopics, and for the negative document, the distribution will be average since none of the subtopics are relevant to the document. The decoder takes the encoded output representations of documents ğ‘¯ ğ‘’ğ‘›ğ‘ D and subtopics ğ‘¯ ğ‘’ğ‘›ğ‘ I as input, and a redundant document will also be affected by its encoder attention distribution, letting its decoder attention distribution more average than the decoder attention distribution for its original representation. This effect is also valid for subtopics. With a subtopic ğ‘ ğ‘– with redundant encoder attention, since ğ’‰ enc ğ‘ ğ‘– = (ğ‘¾ ğ‘’ğ‘›ğ‘ ğ‘ ğ‘– ) ğ‘‡ ğ‘° , its corresponding decoder attention weights ğ‘¤ dec ğ‘– will also be affected and weakened through the average distribution of ğ‘¾ enc ğ‘ ğ‘– . Taking ğ’‰ dec ğ‘¡ as input, the learning-to-rank function will be able to model the subtopic coverage of ğ’… ğ‘¡ together with the relevance scores of subtopics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Discussion</head><p>DESA is inspired by several existing models in IR based on selfattention e.g. SetRank <ref type="bibr" target="#b19">[20]</ref>. And the implicit implementation of DESA with no subtopics can be seen as an adaption of SetRank for search result diversification task. While the properties of diverse ranking task is significantly different from ad-hoc ranking task, and the training dataset of diverse ranking task is very limited. Comparing with SetRank, DESA has got the following differences:</p><p>(1) SetRank is not designed for search result diversification task, it's Transformer encoder structure will be unable to take the subtopics into consideration. While DESA is using a full encoderdecoder structure, and it can leverage both the document novelty and subtopic coverage.</p><p>(2) DESA takes the preliminary representations of the document sequence as input, instead of the relevance features used in Se-tRank. The self-attention networks in DESA only focus on learning the representations of the documents to indicate if a document is novel and covers more subtopics, and the framework deals with the relevance features separately.</p><p>(3) SetRank is using the attention rank loss function as a listwise function, focusing on measuring the attention distribution of the whole ranking list. And DESA is using context-based pairwise function as a pairwise function, the attention distributions stand for the similarity distribution and subtopic satisfaction distribution of every single document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL SETTINGS 4.1 Data Collections and Evaluation Metrics</head><p>4.1.1 The data collections. In the experiments we are using the same dataset as many previous diversification models(e.g.HxQuAD, PAMM-NTN, DSSA) which includes the Web Track dataset from TREC 2009 to 2012. There are in total 200 queries and 198 queries are used since query #95 and #100 have got no diversity judgements to use. Each of them includes 3 to 8 annotated subtopics, and the relevance rating is marked as relevant or irrelevant at subtopic level. We conduct all the experiments on the ClueWeb09 dataset <ref type="bibr" target="#b25">[26]</ref>.</p><p>The subtopics used by the model come from the Google query suggestions provided by Hu et al. <ref type="foot" target="#foot_0">1</ref> , and we only use the first level of the subtopics with no hierarchical subtopics. The max subtopic number of the queries is 10, and the average subtopic number is about 9.48. As those previous works do <ref type="bibr" target="#b8">[9]</ref> we treat all those subtopics with uniform weights.</p><p>For a fair comparison, we are using the document relevance features and embeddings exactly the same as the DSSA, which have been released by Jiang et al. <ref type="bibr" target="#b13">[14]</ref> in the repository on GitHub<ref type="foot" target="#foot_1">2</ref> . Those training data includes 18 relevance features for each query and subquery produced by traditional IR models e.g. BM25 and TF-IDF, and the document embeddings are generated by doc2vec with window size 5. In the future work we will try to import several deeplearning based technologies for feature extraction and document representation e.g. K-NRM <ref type="bibr" target="#b26">[27]</ref> or BERT <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.1.2</head><p>The evaluation metrics. The official diversity evaluation metrics of Web Track include ERR-IA <ref type="bibr" target="#b27">[28]</ref>, ğ›¼-nDCG <ref type="bibr" target="#b28">[29]</ref> and NRBP <ref type="bibr" target="#b29">[30]</ref>, which are used in our experiments. Besides the metrics above, we also include the metrics of Precision-IA <ref type="bibr" target="#b5">[6]</ref> (denoted as Pre-IA) and Subtopic Recall <ref type="bibr" target="#b30">[31]</ref> (denoted as S-rec). Inheriting the spirit of the previous works <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, all those metrics are computed on top 20 results of a document ranking list. Two-tailed paired t-test are used to conduct significance testing with p-value&lt;0.05. In the significance testing, DESA is compared with the DSSA as the SOTA explicit supervised model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Settings</head><p>On our GPU machine, the training phase of DESA with the training samples of 160 queries can be finished in 3 hours. We tune the layer number ğ¿ of the self-attention network in order to avoid overfitting, here ğ¿ = ğ¿ enc +ğ¿ dec , ğ¿ enc is the layer number of encoder component and ğ¿ dec is the number of decoder. We compare DESA with the undiversified baseline and those previous implicit/explicit supervised models, the detail settings of DESA will be described below. We use 5-fold cross validation to turn the parameters in all experiments with the widely used metrics ğ›¼-nDCG@20. The settings of the baseline models are described as follows:</p><p>Lemur. We use the search results produced by language model and retrieved by the Lemur service <ref type="foot" target="#foot_2">3</ref> as the non-diversified baseline. These results are released by Hu et at. <ref type="bibr" target="#b8">[9]</ref> and can be found on the website <ref type="foot" target="#foot_3">4</ref> . All those diversification approaches in our experiments are using the search results of Lemur as initial ranking sequences.</p><p>xQuAD <ref type="bibr" target="#b6">[7]</ref>, PM2 <ref type="bibr" target="#b7">[8]</ref>, HxQuAD and HPM2 <ref type="bibr" target="#b8">[9]</ref>. These are the unsupervised explicit baseline approaches for comparison. All the unsupervised methods use the parameter ğœ† to combine the relevance and diversity linearly. HxQuAD and HPM2 requires extra parameter ğ›¼ to control the weights of the hierarchical subtopic layers. The parameters are tuned with cross validation and ListMLE <ref type="bibr" target="#b9">[10]</ref> is used to learn a prior relevance function with no diversification.</p><p>R-LTR <ref type="bibr" target="#b10">[11]</ref>, PAMM <ref type="bibr" target="#b11">[12]</ref> and PAMM-NTN <ref type="bibr" target="#b12">[13]</ref>. Inspired by previous work <ref type="bibr" target="#b13">[14]</ref>, we use the metric of ğ›¼ âˆ’ ğ‘›DCG@20 to tune the parameters. The neural tensor network(NTN) is used with both R-LTR and PAMM, denoted as R-LTR-NTN and PAMM-NTN. The number of tensor slices for NTN is tuned from 1 to 10, and the number of positive ranking ğœ + and negative ranking ğœ âˆ’ are tuned per query for the PAMM. The distributed representations of documents here are 100-dimensional vectors generated by the LDA <ref type="bibr" target="#b31">[32]</ref>.</p><p>DSSA <ref type="bibr" target="#b13">[14]</ref>. We train the DSSA model with the code and data released by Jiang et al. on GitHub<ref type="foot" target="#foot_4">5</ref> , and use the following optimized settings described in the work of DSSA: LSTM cells, max-pooling on subtopic attention, hidden size 50, doc2vec embedding dimension 100 and random permutation count 10 for the list-pairwise samples. We do not use the embedding of LDA reported in the work, instead we use the doc2vec embedding released for a fair comparison. The result is denoted as DSSA (doc2vec).</p><p>Since the deep reinforced learning based models e.g. MDP-DIV <ref type="bibr" target="#b13">[14]</ref> and M2DIV <ref type="bibr" target="#b14">[15]</ref> are taking too much time to train, we do not take those models as baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overall Results</head><p>Table <ref type="table" target="#tab_2">2</ref> shows the results overall of all the models above. DESA outperforms all the baselines include the state-of-the-art implicit and explicit approaches. The performance improvement is statistically significant on all the metrics except the Pre-IA. These experimental results shows the advantage of DESA clearly. Comparing the stateof-the-art supervised approach, DESA's improvement over DSSA on ğ‘ğ‘™ğ‘â„ğ‘âˆ’nDCG is about 3%. As an explicit model, DSSA use the RNN and attention mechanism to select the best document satisfying the subtopics needed by the selected sequence. Since the RNN can't measure the interactions between each document directly, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effects of Hyperparameter Settings</head><p>We produce several experiments in order to investigate the effects of different settings to the performance of DESA. Since DESA mainly depends on the effect of self-attention, we focus on the self-attention component and deploy different experiments to test the different settings of the self-attention encoder. The baseline settings of the self-attention component include the following items: the initial document/subtopic embedding in 100 dimensions projected into 256 dimensions as the input of the self-attention network, the ğ‘‘ FF = 400 in the feed-forward network and the head number ğ» = 8 for multihead attention operation. We test the effect of different encoder layer numbers ğ¿ enc from 1 to 3 with decoder layer numbers ğ¿ dec =1, and the effect of ğ¿ dec =1 and ğ¿ dec =2 with encoder layer number ğ¿ enc =1 or ğ¿ enc =2. The effects of different settings are shown in Table <ref type="table" target="#tab_3">3</ref>. As we can see, different settings of the self-attention component may slightly influence the effect of the whole DESA framework. In our experiment, we find that the total number of self-attention layer ğ¿ should be strictly limited in order to prevent over-fitting and ensure the performance. In the diverse ranking task, ğ¿ enc =2,ğ¿ dec =1 will lead to the best performance. Since the dataset is limited, more self-attention layers will cause more computational cost and may lead to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effects of Subtopic Settings</head><p>In DESA, the decoder component takes the subtopics as the key and value matrices and returns the decoded document representations indicating the coverage of the subtopics. Here we conduct several experiments to check the effect of different decoder settings. The encoder-only framework can be seen as a simple adaption of SetRank to the diverse ranking task, so we proposed two settings in order to show the effect of the decoder components. The two settings are denoted as:"No Subtopics" and "Relevance Scores". The "No Subtopics" indicates the framework with encoder and query relevance features only, none of the subtopic information is used in ranking task. This framework can be seen as a simple adaption of SetRank to the diverse ranking task. Another expansion of the SetRank adaption is denoted as "Relevance Scores", where the relevance scores of the subtopics ğ‘  ğ‘ ğ‘– (ğ‘– âˆˆ [1, ğ‘˜]) are included, and neither decoder component nor subtopic embeddings are used in the framework. These two adaptions use the optimized settings of ğ» = 8 and ğ¿ = ğ¿ enc = 3. We also propose two settings to check the effect of the encoded subtopic representations in the decoder component. The "Encoded Subtopics" denotes using the encoded subtopic representations ğ‘¯ enc ğ¼ produced by the encoder component, and "Original Subtopics" denotes using the original subtopic embeddings after projection. Their hyperparameter settings are ğ» = 8 and ğ¿ enc = 2, ğ¿ dec = 1.</p><p>The experiment result shows that the full encoder-decoder structure of DESA outperforms the simple adaption of SetRank. The structure of SetRank is not designed for search result diversification task, and it can't take fully use of the subtopic embeddings. The results have prove the effectiveness of the full encoder-decoder structure specialized for explicit search resuld diversification. Comparing with the simple adaption of SetRank, the decoder component in DESA can measure the attention distribution of every document for the satisfaction of subtopics, leveraging both novelty and subtopic coverage together.</p><p>Besides the SetRank adaptions, the outperforming result of "Encoded Subtopics" proves that the encoded representation of the subtopics can be used to reduce the latent redundancy of the subtopics. As we described in Section 4.1.1, the average subtopic number is 9.48 among all the queries, however, the actual user intent numbers are only 3 to 8, which are smaller than the subtopic numbers in diverse ranking task. This result indicates that the subtopics used in ranking process may still contain redundancy and mislead the diversification model.</p><p>We can take the query #1 "obama family tree" in the TREC WebTrack dataset as an example. There are 10 subtopics based on Google query suggestions, while there are only 3 actual user Table <ref type="table">4</ref>: Metrics improvement per ranking position. "Total Imp." denotes the total improvement of DESA on all the 200 queries, "Avg Imp." denotes the average improvement per position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>ERR-IA ğ›¼-nDCG @5 @10 @20 @5 @10 @20 .118</p><p>intents in the TREC official subtopic annotations. And there are two subtopics ğ‘ 1 for "obama family tree pictures" and ğ‘ 2 for "obama family tree photos" in the query suggestions. While both ğ‘ 1 and ğ‘ 2 are corresponding to the same user intent, the redundant subtopics may mislead the model to select a document which covers the "different" subtopics and increase the actual redundancy.</p><p>Comparing with original subtopic embeddings, the encoded subtopic representations can integrate the subtopics' encoder attention distribution into the decoder attention. Similar as the document's attention distribution, the subtopic's attention distribution can also indicate the redundancy of a subtopic, and the decoder attention of the redundant subtopic will also be affected. As a result, the decoder attention will be adjusted to reduce the negative effect of the latent subtopic redundancy to the diverse ranking task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis of Former Ranking Position</head><p>As we described in Section 3.6, theoretically our proposed DESA framework will perform better than the greedy document sequential selection based model at former ranking positions. Here we analyze the effect of DESA at different ranking positions. We compare DESA with DSSA, the state-of-the-art greedy document sequential selection based model. In this experiment we use the ERR-IA and ğ›¼-nDCG metrics computed on top 5, top 10 and top 20 results of a document ranking list to analyse the effect of DESA at former ranking positions. Results are shown in Table <ref type="table">4</ref>. We calculate the total metric improvement of DESA comparing with DSSA. For simplicity of calculation we use the metrics sum of all the 200 queries instead of the mean metrics, denoted as Total Imp. And we calculate the average metrics improvement per position to measure the improvement of DESA at different ranking position ranges, this value is denoted as Avg Imp. For example, the ERR-IA@5 of DESA and DSSA (doc2vec) is 0.343 and 0.328, the total improvement of ERR-IA@5 is calculated as (0.343 âˆ’ 0.328) Ã— 200 â‰ˆ 2.98, and the average improvement of ERR-IA@5 is 2.98 Ã· 5 â‰ˆ 0.597.</p><p>From this table it can be seen that the average metrics improvement numbers per position of short ranking lists are larger than the numbers of longer ranking lists. The experimental results are identical to the analysis in Section 3.6, indicating that in the early stage of ranking, the sequential selection based models may fail to select the globally best candidate document after a short or empty selected sequence. DESA can measure all the candidate documents </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS</head><p>In this paper, we propose a self-attention based supervised diversification framework leveraging both document novelty and subtopic coverage. Instead of greedy document sequential selection, this framework can model all the candidate documents globally and sort those documents jointly with their ranking scores to generate the ranking list. Comparing with the previous works of search result diversification, this is the first time to model all the candidate documents simultaneously and select the best candidate document globally without greedy sequential selection. The experimental results show that modeling the candidate document interaction between each other can significantly minimize the gap between the local and global optimal rankings. In this work our model is focusing on the candidate document sequence, ignoring the selected document sequence. Simultaneously modeling every document's interaction with both the other candidate documents and the selected document sequence may be a potential future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The Overall Structure of DESA. The framework takes the whole candidate document sequence and subtopics together as input, and returns the encoded and decoded representations of every candidate document simultaneously. For the ğ‘¡âˆ’th document ğ‘‘ ğ‘¡ , the learning-to-rank function takes the query relevance features ğ‘¥ ğ‘ , the encoded and decoded representation â„ enc ğ‘¡ and â„ dec ğ‘¡ and subtopic relevance scores ğ‘  ğ‘ ğ‘– as input, and returns the ranking score ğ‘  ğ‘¡</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>20 ,</head><label>20</label><figDesc>October 19-23, 2020, Virtual Event, Ireland globally and promote the diversified documents at former positions, satisfying the user intents earlier comparing with the greedy sequential selection based models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Notations used in this paper</figDesc><table><row><cell cols="2">Notation Description</cell></row><row><cell>ğ‘</cell><cell>the input query</cell></row><row><cell>I,ğ‘ ğ‘–</cell><cell>subtopics corresponding to ğ‘, ğ‘ ğ‘– âˆˆ I</cell></row><row><cell>D</cell><cell>the candidate document sequence</cell></row><row><cell>ğ‘«</cell><cell>embeddings of the candidate document sequence</cell></row><row><cell>ğ‘°</cell><cell>embeddings of all the subtopics</cell></row><row><cell>R</cell><cell>the returned document rank list</cell></row><row><cell>ğ‘¥ ğ‘</cell><cell>document relevance features to the query</cell></row><row><cell>ğ‘¥ ğ‘ ğ‘–</cell><cell>document relevance features to the ğ‘–-th subtopics</cell></row><row><cell>ğ’… ğ‘¡</cell><cell>initial document embedding for the ğ‘¡-th document</cell></row><row><cell>ğ’’ ğ‘¡</cell><cell>initial subtopic embedding for the ğ‘¡-th subtopic</cell></row><row><cell>ğ’‰ enc ğ‘¡ ğ’‰ dec ğ‘¡</cell><cell>the encoder output for ğ‘¡-th document the decoded output for ğ‘¡-th document</cell></row><row><cell>ğ‘  ğ‘ ğ‘–</cell><cell>relevance score of the document to the ğ‘–-th subtopic</cell></row><row><cell>ğ’— ğ‘‘ ğ‘¡ ,ğ‘,ğ‘ ğ‘–</cell><cell>document vector for generating the ranking score</cell></row><row><cell>[; ]</cell><cell>concatenation operation</cell></row><row><cell cols="2">network), such as Transformer</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison for all the approaches. Best Results are in bold. â‹† indicates that the model significantly outperforms all baselines (ğ‘ &lt; 0.05 in two-tailed paired t-test).</figDesc><table><row><cell>Methods</cell><cell cols="5">ERR-IA ğ›¼-nDCG NRBP Pre-IA S-rec</cell></row><row><cell>Lemur</cell><cell>.271</cell><cell>.369</cell><cell>.232</cell><cell>.153</cell><cell>.621</cell></row><row><cell>xQuAD</cell><cell>.317</cell><cell>.413</cell><cell>.284</cell><cell>.161</cell><cell>.622</cell></row><row><cell>PM2</cell><cell>.306</cell><cell>.411</cell><cell>.267</cell><cell>.169</cell><cell>.643</cell></row><row><cell>HxQuAD</cell><cell>.326</cell><cell>.421</cell><cell>.294</cell><cell>.158</cell><cell>.629</cell></row><row><cell>HPM2</cell><cell>.317</cell><cell>.420</cell><cell>.279</cell><cell>.172</cell><cell>.645</cell></row><row><cell>R-LTR</cell><cell>.303</cell><cell>.403</cell><cell>.267</cell><cell>.164</cell><cell>.631</cell></row><row><cell>PAMM</cell><cell>.309</cell><cell>.411</cell><cell>.271</cell><cell>.168</cell><cell>.643</cell></row><row><cell>R-LTR-NTN</cell><cell>.312</cell><cell>.415</cell><cell>.272</cell><cell>.166</cell><cell>.644</cell></row><row><cell>PAMM-NTN</cell><cell>.311</cell><cell>.417</cell><cell>.272</cell><cell>.170</cell><cell>.648</cell></row><row><cell cols="2">DSSA (doc2vec) .350</cell><cell>.452</cell><cell>.318</cell><cell>.184</cell><cell>.645</cell></row><row><cell>DESA</cell><cell>.363 â‹†</cell><cell>.464 â‹†</cell><cell cols="2">.332 â‹† .184</cell><cell>.653 â‹†</cell></row><row><cell cols="6">DESA outperforms DSSA by leveraging both document novelty</cell></row><row><cell cols="6">and subtopic coverage simultaneously. And as a greedy sequential</cell></row><row><cell cols="6">selection model, DSSA may select the local optimal document at</cell></row><row><cell cols="6">each step, leading to a global suboptimal ranking. While DESA can</cell></row><row><cell cols="6">learn the interactions between all the candidate documents and</cell></row><row><cell cols="6">subtopics globally, significantly minimizing the gap between local</cell></row><row><cell cols="2">and global optimal rankings.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Effects of Different Settings</figDesc><table><row><cell>Settings</cell><cell cols="5">ERR-IA ğ›¼-nDCG NRBP Pre-IA S-rec</cell></row><row><cell>ğ¿ enc =1, ğ¿ dec =1</cell><cell>.357</cell><cell>.457</cell><cell>.324</cell><cell>.183</cell><cell>.650</cell></row><row><cell>ğ¿ enc =2, ğ¿ dec =1</cell><cell>.364</cell><cell>.464</cell><cell>.332</cell><cell>.184</cell><cell>.653</cell></row><row><cell>ğ¿ enc =3, ğ¿ dec =1</cell><cell>.355</cell><cell>.455</cell><cell>.323</cell><cell>.182</cell><cell>.654</cell></row><row><cell>ğ¿ enc =1, ğ¿ dec =2</cell><cell>.361</cell><cell>.462</cell><cell>.329</cell><cell>.182</cell><cell>.658</cell></row><row><cell>ğ¿ enc =2, ğ¿ dec =2</cell><cell>.358</cell><cell>.460</cell><cell>.324</cell><cell>.180</cell><cell>.658</cell></row><row><cell>No Subtopics</cell><cell>.344</cell><cell>.445</cell><cell>.311</cell><cell>.177</cell><cell>.648</cell></row><row><cell>Relevance Scores</cell><cell>.357</cell><cell>.458</cell><cell>.326</cell><cell>.183</cell><cell>.653</cell></row><row><cell cols="2">Encoded Subtopics .364</cell><cell>.464</cell><cell>.332</cell><cell>.184</cell><cell>.653</cell></row><row><cell cols="2">Original Subtopics .349</cell><cell>.453</cell><cell>.313</cell><cell>.180</cell><cell>.655</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://www.playbigdata.com/dou/hdiv</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/jzbjyb/dssa</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Lemur service: http://boston.lti.cs.cmu.edu/Services/clueweb09_batch/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">http://www.playbigdata.com/dou/hdiv</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://github.com/jzbjyb/dssa</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>Zhicheng Dou is the corresponding author. This work was supported by National Natural Science Foundation of China No. 61872370 and No. 61832017, and Beijing Outstanding Young Scientist Program NO. BJJWZYJH012019100020098.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysis of a very large web search engine query log</title>
		<author>
			<persName><forename type="first">Craig</forename><surname>Silverstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannes</forename><surname>Marais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monika</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Moricz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="12" />
			<date type="published" when="1999-09">September 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A large-scale evaluation and analysis of personalized search strategies</title>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihua</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on World Wide Web, WWW &apos;07</title>
				<meeting>the 16th International Conference on World Wide Web, WWW &apos;07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="581" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real life, real users, and real needs: A study and analysis of user queries on the web</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tefko</forename><surname>Spink</surname></persName>
		</author>
		<author>
			<persName><surname>Saracevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="227" />
			<date type="published" when="2000-01">January 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Identifying ambiguous queries in web search</title>
		<author>
			<persName><forename type="first">Ruihua</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenxiao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on World Wide Web, WWW &apos;07</title>
				<meeting>the 16th International Conference on World Wide Web, WWW &apos;07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1169" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The use of mmr, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual International ACM SIGIR Conference, SIGIR &apos;98</title>
				<meeting>the 21st Annual International ACM SIGIR Conference, SIGIR &apos;98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diversifying search results</title>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Gollapudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Halverson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Ieong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second ACM International Conference on Web Search and Data Mining, WSDM &apos;09</title>
				<meeting>the Second ACM International Conference on Web Search and Data Mining, WSDM &apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting query reformulations for web search result diversification</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Rodrygo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iadh</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on World Wide Web, WWW &apos;10</title>
				<meeting>the 19th International Conference on World Wide Web, WWW &apos;10</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="881" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Diversity by proportionality: An election-based approach to search result diversification</title>
		<author>
			<persName><forename type="first">Van</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International ACM SIGIR Conference, SIGIR &apos;12</title>
				<meeting>the 35th International ACM SIGIR Conference, SIGIR &apos;12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Search result diversification based on hierarchical intents</title>
		<author>
			<persName><forename type="first">Sha</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tetsuya</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, CIKM &apos;15</title>
				<meeting>the 24th ACM International on Conference on Information and Knowledge Management, CIKM &apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="63" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting diverse subsets using structural svms</title>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning, ICML &apos;08</title>
				<meeting>the 25th International Conference on Machine Learning, ICML &apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1224" to="1231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning for search result diversification</title>
		<author>
			<persName><forename type="first">Yadong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International ACM SIGIR Conference, SIGIR &apos;14</title>
				<meeting>the 37th International ACM SIGIR Conference, SIGIR &apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning maximal marginal relevance model via directly optimizing diversity evaluation measures</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference, SIGIR &apos;15</title>
				<meeting>the 38th International ACM SIGIR Conference, SIGIR &apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling document novelty with neural tensor network for search result diversification</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR Conference, SIGIR &apos;16</title>
				<meeting>the 39th International ACM SIGIR Conference, SIGIR &apos;16</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="395" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to diversify search results via subtopic attention</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference, SIGIR &apos;17</title>
				<meeting>the 40th International ACM SIGIR Conference, SIGIR &apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="545" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">From greedy selection to exploratory decision-making: Diverse ranking with policyvalue networks</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference, SIGIR &apos;18</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Å</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ERNIE: Enhanced language representation with informative entities</title>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Setrank: Learning a permutation-invariant ranking model for information retrieval</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jirong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference, SIGIR &apos;20</title>
				<meeting>the 43rd International ACM SIGIR Conference, SIGIR &apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="499" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Self-attentive document interaction networks for permutation equivariant ranking</title>
		<author>
			<persName><forename type="first">Rama</forename><surname>Kumar Pasumarthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Information &amp; Knowledge Management, CIKM &apos;13</title>
				<meeting>the 22nd ACM International Conference on Information &amp; Knowledge Management, CIKM &apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2333" to="2338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>II-1188-II-1196. JMLR.org</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on International Conference on Machine Learning</title>
				<meeting>the 31st International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>ArXiv, abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Search result diversification</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T</forename><surname>Rodrygo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iadh</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><surname>Ounis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="90" />
			<date type="published" when="2015-03">March 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Overview of the trec 2009 web track</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Charles L Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><surname>Soboroff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>DTIC Document</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">End-to-end neural ad-hoc ranking with kernel pooling</title>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17</title>
				<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metlzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM &apos;09</title>
				<meeting>the 18th ACM Conference on Information and Knowledge Management, CIKM &apos;09</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Novelty and diversity in information retrieval evaluation</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maheedhar</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><forename type="middle">V</forename><surname>Kolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azin</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>BÃ¼ttcher</surname></persName>
		</author>
		<author>
			<persName><surname>Mackinnon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st Annual International ACM SIGIR Conference, SIGIR &apos;08</title>
				<meeting>the 31st Annual International ACM SIGIR Conference, SIGIR &apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="659" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Query recommendation using query logs in search engines</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Hurtado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Mendoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Current Trends in Database Technology -EDBT 2004 Workshops</title>
				<editor>
			<persName><forename type="first">Wolfgang</forename><surname>Lindner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marco</forename><surname>Mesiti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Can</forename><surname>TÃ¼rker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yannis</forename><surname>Tzitzikas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Athena</forename><forename type="middle">I</forename><surname>Vakali</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="588" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Beyond independent relevance: Methods and evaluation metrics for subtopic retrieval</title>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGIR Forum</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="9" />
			<date type="published" when="2015-06">June 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-03">March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
