<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Identity Mappings in Deep Residual Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
							<email>kmh.kaiminghe@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Identity Mappings in Deep Residual Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/978-3-319-46493-0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62 % error) and CIFAR-100, and a 200-layer ResNet on ImageNet.</p><p>Code is available at: https://github.com/KaimingHe/resnet-1k-layers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep residual networks (ResNets) <ref type="bibr" target="#b0">[1]</ref> consist of many stacked "Residual Units". Each unit (Fig. <ref type="figure">1(a)</ref>) can be expressed in a general form: y l =h(x l ) + F(x l , W l ),</p><p>x l+1 = f (y l ), where x l and x l+1 are input and output of the l-th unit, and F is a residual function. In <ref type="bibr" target="#b0">[1]</ref>, h(x l ) = x l is an identity mapping and f is a ReLU <ref type="bibr" target="#b1">[2]</ref> function.</p><p>ResNets that are over 100-layer deep have shown state-of-the-art accuracy for several challenging recognition tasks on ImageNet <ref type="bibr" target="#b2">[3]</ref> and MS COCO <ref type="bibr" target="#b3">[4]</ref> competitions. The central idea of ResNets is to learn the additive residual function F with respect to h(x l ), with a key choice of using an identity mapping h(x l ) = x l . This is realized by attaching an identity skip connection ("shortcut").</p><p>In this paper, we analyze deep residual networks by focusing on creating a "direct" path for propagating information-not only within a residual unit, but through the entire network. Our derivations reveal that if both h(x l ) and f (y l ) are identity mappings, the signal could be directly propagated from one unit to any other units, in both forward and backward passes. Our experiments empirically show that training in general becomes easier when the architecture is closer to the above two conditions.</p><p>To understand the role of skip connections, we analyze and compare various types of h(x l ). We find that the identity mapping h(x l ) = x l chosen in <ref type="bibr" target="#b0">[1]</ref> Fig. <ref type="figure">1</ref>. Left: (a) original Residual Unit in <ref type="bibr" target="#b0">[1]</ref>; (b) proposed Residual Unit. The grey arrows indicate the easiest paths for the information to propagate, corresponding to the additive term "x l " in Eq. ( <ref type="formula" target="#formula_4">4</ref>) (forward propagation) and the additive term "1" in Eq. ( <ref type="formula" target="#formula_7">5</ref>) (backward propagation). Right: training curves on CIFAR-10 of 1001-layer ResNets. Solid lines denote test error (y-axis on the right), and dashed lines denote training loss (y-axis on the left). The proposed unit makes ResNet-1001 easier to train.</p><p>achieves the fastest error reduction and lowest training loss among all variants we investigated, whereas skip connections of scaling, gating <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>, and 1 × 1 convolutions all lead to higher training loss and error. These experiments suggest that keeping a "clean" information path (indicated by the grey arrows in Figs. <ref type="figure">1,  2 and 4</ref>) is helpful for easing optimization.</p><p>To construct an identity mapping f (y l ) = y l , we view the activation functions (ReLU and BN <ref type="bibr" target="#b7">[8]</ref>) as "pre-activation" of the weight layers, in contrast to conventional wisdom of "post-activation". This point of view leads to a new residual unit design, shown in (Fig. <ref type="figure">1(b)</ref>). Based on this unit, we present competitive results on CIFAR-10/100 with a 1001-layer ResNet, which is much easier to train and generalizes better than the original ResNet in <ref type="bibr" target="#b0">[1]</ref>. We further report improved results on ImageNet using a 200-layer ResNet, for which the counterpart of <ref type="bibr" target="#b0">[1]</ref> starts to overfit. These results suggest that there is much room to exploit the dimension of network depth, a key to the success of modern deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Analysis of Deep Residual Networks</head><p>The ResNets developed in <ref type="bibr" target="#b0">[1]</ref> are modularized architectures that stack building blocks of the same connecting shape. In this paper we call these blocks "Residual Units". The original Residual Unit in <ref type="bibr" target="#b0">[1]</ref> performs the following computation:</p><formula xml:id="formula_0">y l =h(x l ) + F(x l , W l ),<label>(1)</label></formula><formula xml:id="formula_1">x l+1 = f (y l ).<label>(2)</label></formula><p>Here x l is the input feature to the l-th Residual Unit. W l = {W l,k | 1≤k≤K } is a set of weights (and biases) associated with the l-th Residual Unit, and K is the number of layers in a Residual Unit (K is 2 or 3 in <ref type="bibr" target="#b0">[1]</ref>). F denotes the residual function, e.g., a stack of two 3×3 convolutional layers in <ref type="bibr" target="#b0">[1]</ref>. The function f is the operation after element-wise addition, and in <ref type="bibr" target="#b0">[1]</ref> f is ReLU. The function h is set as an identity mapping: h(x l ) = x l . <ref type="foot" target="#foot_0">1</ref>If f is also an identity mapping: x l+1 ≡ y l , we can put Eq. ( <ref type="formula" target="#formula_1">2</ref>) into Eq. ( <ref type="formula" target="#formula_0">1</ref>) and obtain:</p><formula xml:id="formula_2">x l+1 = x l + F(x l , W l ).<label>(3)</label></formula><p>Recursively (</p><formula xml:id="formula_3">x l+2 = x l+1 + F (x l+1 , W l+1 ) = x l + F (x l , W l ) + F (x l+1 , W l+1</formula><p>), etc.) we will have:</p><formula xml:id="formula_4">x L = x l + L−1 i=l F(x i , W i ),<label>(4)</label></formula><p>for any deeper unit L and any shallower unit l. Equation ( <ref type="formula" target="#formula_4">4</ref>) exhibits some nice properties. (i) The feature x L of any deeper unit L can be represented as the feature x l of any shallower unit l plus a residual function in a form of L−1 i=l F, indicating that the model is in a residual fashion between any units L and l. (ii) The feature</p><formula xml:id="formula_5">x L = x 0 + L−1 i=0 F(x i , W i ), of</formula><p>any deep unit L, is the summation of the outputs of all preceding residual functions (plus x 0 ). This is in contrast to a "plain network" where a feature x L is a series of matrix-vector products, say,</p><formula xml:id="formula_6">L−1 i=0 W i x 0 (ignoring BN and ReLU).</formula><p>Equation ( <ref type="formula" target="#formula_4">4</ref>) also leads to nice backward propagation properties. Denoting the loss function as E, from the chain rule of backpropagation <ref type="bibr" target="#b8">[9]</ref> we have:</p><formula xml:id="formula_7">∂E ∂x l = ∂E ∂x L ∂x L ∂x l = ∂E ∂x L 1 + ∂ ∂x l L−1 i=l F(x i , W i ) . (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>Equation <ref type="bibr" target="#b4">(5)</ref> indicates that the gradient ∂E ∂x l can be decomposed into two additive terms: a term of ∂E ∂xL that propagates information directly without concerning any weight layers, and another term of ∂E ∂x L ∂ ∂x l L−1 i=l F that propagates through the weight layers. The additive term of ∂E ∂xL ensures that information is directly propagated back to any shallower unit l. Equation ( <ref type="formula" target="#formula_7">5</ref>) also suggests that it is unlikely for the gradient ∂E ∂x l to be canceled out for a mini-batch, because in general the term ∂ ∂x l L−1 i=l F cannot be always -1 for all samples in a minibatch. This implies that the gradient of a layer does not vanish even when the weights are arbitrarily small. Discussions. Equations ( <ref type="formula" target="#formula_4">4</ref>) and <ref type="bibr" target="#b4">(5)</ref> suggest that the signal can be directly propagated from any unit to another, both forward and backward. The foundation of Eq. ( <ref type="formula" target="#formula_4">4</ref>) is two identity mappings: (i) the identity skip connection h(x l ) = x l , and (ii) the condition that f is an identity mapping.</p><p>These directly propagated information flows are represented by the grey arrows in Figs. 1, 2 and 4. And the above two conditions are true when these grey arrows cover no operations (expect addition) and thus are "clean". In the following two sections we separately investigate the impacts of the two conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">On the Importance of Identity Skip Connections</head><p>Let's consider a simple modification, h(x l ) = λ l x l , to break the identity shortcut:</p><formula xml:id="formula_9">x l+1 = λ l x l + F(x l , W l ),<label>(6)</label></formula><p>where λ l is a modulating scalar (for simplicity we still assume f is identity).</p><p>Recursively applying this formulation we obtain an equation similar to Eq. ( <ref type="formula" target="#formula_4">4</ref>):</p><formula xml:id="formula_10">x L = ( L−1 i=l λ i )x l + L−1 i=l ( L−1 j=i+1 λ j )F(x i , W i ), or simply: x L = ( L−1 i=l λ i )x l + L−1 i=l F(x i , W i ),<label>(7)</label></formula><p>where the notation F absorbs the scalars into the residual functions. Similar to Eq. ( <ref type="formula" target="#formula_7">5</ref>), we have backpropagation of the following form:</p><formula xml:id="formula_11">∂E ∂x l = ∂E ∂x L ( L−1 i=l λ i ) + ∂ ∂x l L−1 i=l F(x i , W i ) . (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>Unlike Eq. ( <ref type="formula" target="#formula_7">5</ref>), in Eq. ( <ref type="formula" target="#formula_11">8</ref>) the first additive term is modulated by a factor L−1 i=l λ i . For an extremely deep network (L is large), if λ i &gt; 1 for all i, this factor can be exponentially large; if λ i &lt; 1 for all i, this factor can be exponentially small and vanish, which blocks the backpropagated signal from the shortcut and forces it to flow through the weight layers. This results in optimization difficulties as we show by experiments.</p><p>In the above analysis, the original identity skip connection in Eq. ( <ref type="formula" target="#formula_2">3</ref>) is replaced with a simple scaling h(x l ) = λ l x l . If the skip connection h(x l ) represents more complicated transforms (such as gating and 1 × 1 convolutions), in Eq. ( <ref type="formula" target="#formula_11">8</ref>) the first term becomes L−1 i=l h i where h is the derivative of h. This product may also impede information propagation and hamper the training procedure as witnessed in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiments on Skip Connections</head><p>We experiment with the 110-layer ResNet as presented in <ref type="bibr" target="#b0">[1]</ref> on . This extremely deep ResNet-110 has 54 two-layer Residual Units (consisting of Fig. <ref type="figure">2</ref>. Various types of shortcut connections used in Table <ref type="table">1</ref>. The grey arrows indicate the easiest paths for the information to propagate. The shortcut connections in (b-f) are impeded by different components. For simplifying illustrations we do not display the BN layers, which are adopted right after the weight layers for all units here. 3×3 convolutional layers) and is challenging for optimization. Our implementation details (see appendix) are the same as <ref type="bibr" target="#b0">[1]</ref>. Throughout this paper we report the median accuracy of 5 runs for each architecture on CIFAR, reducing the impacts of random variations.</p><p>Though our above analysis is driven by identity f , the experiments in this section are all based on f = ReLU as in <ref type="bibr" target="#b0">[1]</ref>; we address identity f in the next section. Our baseline ResNet-110 has 6.61 % error on the test set. The comparisons of other variants (Fig. <ref type="figure">2</ref> and Table <ref type="table">1</ref>) are summarized as follows: Constant Scaling. We set λ = 0.5 for all shortcuts (Fig. <ref type="figure">2(b)</ref>). We further study two cases of scaling F: (i) F is not scaled; or (ii) F is scaled by a constant scalar of 1 − λ = 0.5, which is similar to the highway gating <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> but with frozen gates. The former case does not converge well; the latter is able to converge, but the test error (Table <ref type="table">1</ref>, 12.35 %) is substantially higher than the original ResNet-110. Figure <ref type="figure" target="#fig_1">3(a)</ref> shows that the training error is higher than that of the original ResNet-110, suggesting that the optimization has difficulties when the shortcut signal is scaled down.</p><p>Exclusive Gating. Following the Highway Networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> that adopt a gating mechanism <ref type="bibr" target="#b4">[5]</ref>, we consider a gating function g(x) = σ(W g x + b g ) where a transform is represented by weights W g and biases b g followed by the sigmoid Table <ref type="table">1</ref>. Classification error on the CIFAR-10 test set using ResNet-110 <ref type="bibr" target="#b0">[1]</ref>, with different types of shortcut connections applied to all Residual Units. We report "fail" when the test error is higher than 20 %.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case</head><p>Fig.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>On shortcut On F Error (%) Remark</head><p>Original <ref type="bibr" target="#b0">[1]</ref> Fig. <ref type="figure">2(a</ref> We investigate the "exclusive" gates as used in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>-the F path is scaled by g(x) and the shortcut path is scaled by 1 − g(x). See Fig. <ref type="figure">2(c</ref>). We find that the initialization of the biases b g is critical for training gated models, and following the guidelines<ref type="foot" target="#foot_1">2</ref> in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, we conduct hyper-parameter search on the initial value of b g in the range of 0 to -10 with a decrement step of -1 on the training set by cross-validation. The best value (−6 here) is then used for training on the training set, leading to a test result of 8.70 % (Table <ref type="table">1</ref>), which still lags far behind the ResNet-110 baseline. Figure <ref type="figure" target="#fig_1">3(b)</ref> shows the training curves. Table <ref type="table">1</ref> also reports the results of using other initialized values, noting that the exclusive gating network does not converge to a good solution when b g is not appropriately initialized.</p><p>The impact of the exclusive gating mechanism is two-fold. When 1 − g(x) approaches 1, the gated shortcut connections are closer to identity which helps information propagation; but in this case g(x) approaches 0 and suppresses the function F. To isolate the effects of the gating functions on the shortcut path alone, we investigate a non-exclusive gating mechanism in the next.</p><p>Shortcut-Only Gating. In this case the function F is not scaled; only the shortcut path is gated by 1 − g(x). See Fig. <ref type="figure">2(d)</ref>. The initialized value of b g is still essential in this case. When the initialized b g is 0 (so initially the expectation of 1 − g(x) is 0.5), the network converges to a poor result of 12.86 % (Table <ref type="table">1</ref>). This is also caused by higher training error (Fig. <ref type="figure" target="#fig_1">3(c)</ref>). When the initialized b g is very negatively biased (e.g., −6), the value of 1 − g(x) is closer to 1 and the shortcut connection is nearly an identity mapping. Therefore, the result (6.91 %, Table <ref type="table">1</ref>) is much closer to the ResNet-110 baseline.</p><p>1×1 Convolutional Shortcut. Next we experiment with 1 × 1 convolutional shortcut connections that replace the identity. This option has been investigated in <ref type="bibr" target="#b0">[1]</ref> (known as option C) on a 34-layer ResNet (16 Residual Units) and shows good results, suggesting that 1 × 1 shortcut connections could be useful. But we find that this is not the case when there are many Residual Units. The 110-layer ResNet has a poorer result (12.22 %, Table <ref type="table">1</ref>) when using 1 × 1 convolutional shortcuts. Again, the training error becomes higher (Fig. <ref type="figure" target="#fig_1">3(d)</ref>). When stacking so many Residual Units (54 for ResNet-110), even the shortest path may still impede signal propagation. We witnessed similar phenomena on ImageNet with ResNet-101 when using 1 × 1 convolutional shortcuts.</p><p>Dropout Shortcut. Last we experiment with dropout <ref type="bibr" target="#b10">[11]</ref> (at a ratio of 0.5) which we adopt on the output of the identity shortcut (Fig. <ref type="figure">2(f)</ref>). The network fails to converge to a good solution. Dropout statistically imposes a scale of λ with an expectation of 0.5 on the shortcut, and similar to constant scaling by 0.5, it impedes signal propagation.  <ref type="table">2</ref>. All these units consist of the same components-only the orders are different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Discussions</head><p>As indicated by the grey arrows in Fig. <ref type="figure">2</ref>, the shortcut connections are the most direct paths for the information to propagate. Multiplicative manipulations (scaling, gating, 1 × 1 convolutions, and dropout) on the shortcuts can hamper information propagation and lead to optimization problems. It is noteworthy that the gating and 1 × 1 convolutional shortcuts introduce more parameters, and should have stronger representational abilities than identity shortcuts. In fact, the shortcut-only gating and 1 × 1 convolution cover the solution space of identity shortcuts (i.e., they could be optimized as identity shortcuts). However, their training error is higher than that of identity shortcuts, indicating that the degradation of these models is caused by optimization issues, instead of representational abilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">On the Usage of Activation Functions</head><p>Experiments in the above section support the analysis in Eqs. ( <ref type="formula" target="#formula_7">5</ref>) and ( <ref type="formula" target="#formula_11">8</ref>), both being derived under the assumption that the after-addition activation f is the identity mapping. But in the above experiments f is ReLU as designed in <ref type="bibr" target="#b0">[1]</ref>, so Eqs. ( <ref type="formula" target="#formula_7">5</ref>) and ( <ref type="formula" target="#formula_11">8</ref>) are approximate in the above experiments. Next we investigate the impact of f . We want to make f an identity mapping, which is done by re-arranging the activation functions (ReLU and/or BN). The original Residual Unit in <ref type="bibr" target="#b0">[1]</ref> has a shape in Fig. <ref type="figure">4</ref>(a)-BN is used after each weight layer, and ReLU is adopted after BN except that the last ReLU in a Residual Unit is after element-wise addition (f = ReLU). Figure <ref type="figure">4(b-e</ref>) show the alternatives we investigated, explained as following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiments on Activation</head><p>In this section we experiment with ResNet-110 and a 164-layer Bottleneck <ref type="bibr" target="#b0">[1]</ref> architecture (denoted as ResNet-164). A bottleneck Residual Unit consist of a 1 × 1 layer for reducing dimension, a 3×3 layer, and a 1 × 1 layer for restoring dimension. As designed in <ref type="bibr" target="#b0">[1]</ref>, its computational complexity is similar to the two-3×3 Residual Unit. More details are in the appendix. The baseline ResNet-164 has a competitive result of 5.93 % on CIFAR-10 (Table <ref type="table">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BN After Addition.</head><p>Before turning f into an identity mapping, we go the opposite way by adopting BN after addition (Fig. <ref type="figure">4(b)</ref>). In this case f involves BN and ReLU. The results become considerably worse than the baseline (Table <ref type="table">2</ref>). Unlike the original design, now the BN layer alters the signal that passes through the shortcut and impedes information propagation, as reflected by the difficulties on reducing training loss at the beginning of training (Fig. <ref type="figure" target="#fig_4">6</ref> left).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ReLU Before Addition.</head><p>A naïve choice of making f into an identity mapping is to move the ReLU before addition (Fig. <ref type="figure">4(c)</ref>). However, this leads to a nonnegative output from the transform F, while intuitively a "residual" function should take values in (−∞, +∞). As a result, the forward propagated signal is monotonically increasing. This may impact the representational ability, and the result is worse (7.84 %, Table <ref type="table">2</ref>) than the baseline. We expect to have a residual function taking values in (−∞, +∞). This condition is satisfied by other Residual Units including the following ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-activation or Pre-activation?</head><p>In the original design (Eqs. ( <ref type="formula" target="#formula_0">1</ref>) and ( <ref type="formula" target="#formula_1">2</ref>)), the activation x l+1 = f (y l ) affects both paths in the next Residual Unit: y l+1 = f (y l )+F(f (y l ), W l+1 ). Next we develop an asymmetric form where an activation f only affects the F path: y l+1 = y l + F( f (y l ), W l+1 ), for any l (Fig. <ref type="figure" target="#fig_3">5</ref>(a) to (b)). By renaming the notations, we have the following form:</p><formula xml:id="formula_13">x l+1 = x l + F( f (x l ), W l ),<label>(9)</label></formula><p>It is easy to see that Eq. ( <ref type="formula" target="#formula_13">9</ref>) is similar to Eq. ( <ref type="formula" target="#formula_4">4</ref>), and can enable a backward formulation similar to Eq. ( <ref type="formula" target="#formula_7">5</ref>). For this new Residual Unit as in Eq. ( <ref type="formula" target="#formula_13">9</ref>), the new after-addition activation becomes an identity mapping. This design means that  if a new after-addition activation f is asymmetrically adopted, it is equivalent to recasting f as the pre-activation of the next Residual Unit. This is illustrated in Fig. <ref type="figure" target="#fig_3">5</ref>.</p><p>The distinction between post-activation/pre-activation is caused by the presence of the element-wise addition. For a plain network that has N layers, there are N − 1 activations (BN/ReLU), and it does not matter whether we think of them as post-or pre-activations. But for branched layers merged by addition, the position of activation matters.</p><p>We experiment with two such designs: (i) ReLU-only pre-activation (Fig. <ref type="figure">4(d)</ref>), and (ii) full pre-activation (Fig. <ref type="figure">4(e)</ref>) where BN and ReLU are both adopted before weight layers. Table <ref type="table">2</ref> shows that the ReLU-only pre-activation performs very similar to the baseline on ResNet-110/164. This ReLU layer is not used in conjunction with a BN layer, and may not enjoy the benefits of BN <ref type="bibr" target="#b7">[8]</ref>. Somehow surprisingly, when BN and ReLU are both used as pre-activation, the results are improved by healthy margins (Tables <ref type="table" target="#tab_0">2 and 3</ref>). In Table <ref type="table" target="#tab_0">3</ref> we report results using various architectures: (i) ResNet-110, (ii) ResNet-164, (iii) a 110-layer ResNet architecture in which each shortcut skips only 1 layer (i.e., a Residual Unit has only 1 layer), denoted as "ResNet-110(1layer)", and (iv) a 1001-layer bottleneck architecture that has 333 Residual Units (111 on each feature map size), denoted as "ResNet-1001". We also experiment on CIFAR-100. Table <ref type="table" target="#tab_0">3</ref> shows that our "pre-activation" models are consistently better than the baseline counterparts. We analyze these results in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis</head><p>We find the impact of pre-activation is twofold. First, the optimization is further eased (comparing with the baseline ResNet) because f is an identity mapping. Second, using BN as pre-activation improves regularization of the models.</p><p>Ease of Optimization. This effect is particularly obvious when training the 1001-layer ResNet. Figure <ref type="figure">1</ref> shows the curves. Using the original design in <ref type="bibr" target="#b0">[1]</ref>, the training error is reduced very slowly at the beginning of training. For f = ReLU, the signal is impacted if it is negative, and when there are many Residual Units, this effect becomes prominent and Eq. (3) (so Eq. ( <ref type="formula" target="#formula_7">5</ref>)) is not a good approximation. On the other hand, when f is an identity mapping, the signal can be propagated directly between any two units. Our 1001-layer network reduces the training loss very quickly (Fig. <ref type="figure">1</ref>). It also achieves the lowest loss among all models we investigated, suggesting the success of optimization.</p><p>We also find that the impact of f = ReLU is not severe when the ResNet has fewer layers (e.g., 164 in Fig. <ref type="figure" target="#fig_4">6</ref>(right)). The training curve seems to suffer a little bit at the beginning of training, but goes into a healthy status soon. By monitoring the responses we observe that this is because after some training, the weights are adjusted into a status such that y l in Eq. ( <ref type="formula" target="#formula_0">1</ref>) is more frequently above Table <ref type="table">4</ref>. with state-of-the-art methods on CIFAR-10 and CIFAR-100 using "moderate data augmentation" (flip/translation), except for ELU <ref type="bibr" target="#b11">[12]</ref> with no augmentation. Better results of <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> have been reported using stronger data augmentation and ensembling. For the ResNets we also report the number of parameters. Our results are the median of 5 runs with mean±std in the brackets. All ResNets results are obtained with a mini-batch size of 128 except † with a mini-batch size of 64 (code available at https://github.com/KaimingHe/resnet-1k-layers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10</head><p>Error The original ResNet-152 <ref type="bibr" target="#b0">[1]</ref> has top-1 error of 21.3 % on a 320×320 crop, and our pre-activation counterpart has 21.1 %. The gain is not big on ResNet-152 because this model has not shown severe generalization difficulties. However, the original ResNet-200 has an error rate of 21.8 %, higher than the baseline ResNet-152. But we find that the original ResNet-200 has lower training error than ResNet-152, suggesting that it suffers from overfitting.</p><p>Our pre-activation ResNet-200 has an error rate of 20.7 %, which is 1.1 % lower than the baseline ResNet-200 and also lower than the two versions of ResNet-152. When using the scale and aspect ratio augmentation of <ref type="bibr">[19,</ref><ref type="bibr" target="#b18">20]</ref>, our ResNet-200 has a result better than Inception v3 [19] (Table <ref type="table">5</ref>). Concurrent with our work, an Inception-ResNet-v2 model <ref type="bibr" target="#b19">[21]</ref> achieves a single-crop result of 19.9 %/4.9 %. We expect our observations and the proposed Residual Unit will help this type and generally other types of ResNets.</p><p>Computational Cost. Our models' computational complexity is linear on depth (so a 1001-layer net is ∼10× complex of a 100-layer net). On CIFAR, ResNet-1001 takes about 27 h to train on 2 GPUs; on ImageNet, ResNet-200 takes about 3 weeks to train on 8 GPUs (on par with VGG nets <ref type="bibr" target="#b20">[22]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Comparisons of single-crop error on the ILSVRC validation set. All ResNets are trained using the same hyper-parameters and implementations as <ref type="bibr" target="#b0">[1]</ref>). Our Residual Units are the full pre-activation version (Fig. <ref type="figure">4(e)</ref>). † : code/model available at https://github.com/facebook/fb.resnet.torch/tree/master/pretrained, using scale and aspect ratio augmentation in <ref type="bibr" target="#b18">[20]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper investigates the propagation formulations behind the connection mechanisms of deep residual networks. Our derivations imply that identity shortcut connections and identity after-addition activation are essential for making information propagation smooth. Ablation experiments demonstrate phenomena that are consistent with our derivations. We also present 1000-layer deep networks that can be easily trained and achieve improved accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Implementation Details</head><p>The implementation details and hyper-parameters are the same as those in <ref type="bibr" target="#b0">[1]</ref>.</p><p>On CIFAR we use only the translation and flipping augmentation in <ref type="bibr" target="#b0">[1]</ref> for training. The learning rate starts from 0.1, and is divided by 10 at 32k and 48k iterations. Following <ref type="bibr" target="#b0">[1]</ref>, for all CIFAR experiments we warm up the training by using a smaller learning rate of 0.01 at the beginning 400 iterations and go back to 0.1 after that, although we remark that this is not necessary for our proposed Residual Unit. The mini-batch size is 128 on 2 GPUs (64 each), the weight decay is 0.0001, the momentum is 0.9, and the weights are initialized as in <ref type="bibr" target="#b21">[23]</ref>.</p><p>On ImageNet, we train the models using the same data augmentation as in <ref type="bibr" target="#b0">[1]</ref>. The learning rate starts from 0.1 (no warming up), and is divided by 10 at 30 and 60 epochs. The mini-batch size is 256 on 8 GPUs (32 each). The weight decay, momentum, and weight initialization are the same as above.</p><p>When using the pre-activation Residual Units (Figs. <ref type="bibr">4(d)</ref>, (e) and 5), we pay special attention to the first and the last Residual Units of the entire network. For the first Residual Unit (that follows a stand-alone convolutional layer, conv 1 ), we adopt the first activation right after conv 1 and before splitting into two paths; for the last Residual Unit (followed by average pooling and a fullyconnected classifier), we adopt an extra activation right after its element-wise addition. These two special cases are the natural outcome when we obtain the pre-activation network via the modification procedure as shown in Fig. <ref type="figure" target="#fig_3">5</ref>.</p><p>The bottleneck Residual Units (for ResNet-164/1001 on CIFAR) are constructed following <ref type="bibr" target="#b0">[1]</ref>. For example, a 3 × 3, 16 For the bottleneck ResNets, when reducing the feature map size we use projection shortcuts <ref type="bibr" target="#b0">[1]</ref> for increasing dimensions, and when pre-activation is used, these projection shortcuts are also with pre-activation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>c) 1 − g(x) g(x) fail init bg=0 to −5 1 − g(x) g(x) 8.70 init bg = −6 1 − g(x) g(x) 9.81 init bg = −7 Shortcut-only gating Fig. 2(d) 1 − g(x) 1 12.86 init bg = 0 1 − g(x) 1 6.91 init bg = −6 1 × 1 conv shortcut Fig. 2(e) 1 × 1 conv 1 12.22 Dropout shortcut Fig. 2(f) dropout 0.5 1 fail function σ(x) = 1 1+e −x . In a convolutional network g(x) is realized by a 1 × 1 convolutional layer. The gating function modulates the signal by element-wise multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Training curves on CIFAR-10 of various shortcuts. Solid lines denote test error (y-axis on the right), and dashed lines denote training loss (y-axis on the left).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Table 2 .Fig. 4 .</head><label>24</label><figDesc>Fig. 4. Various usages of activation in Table2. All these units consist of the same components-only the orders are different.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Using asymmetric after-addition activation is equivalent to constructing a preactivation Residual Unit.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Training curves on CIFAR-10. Left: BN after addition (Fig. 4(b)) using ResNet-110. Right: pre-activation unit (Fig. 4(e)) on ResNet-164. Solid lines denote test error, and dashed lines denote training loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 3 .</head><label>3</label><figDesc>Classification error (%) on the CIFAR-10/100 test set using the original Residual Units and our pre-activation Residual Units.</figDesc><table><row><cell>Dataset</cell><cell>Network</cell><cell cols="2">Baseline unit Pre-activation unit</cell></row><row><cell cols="3">CIFAR-10 ResNet-110 (1layer skip) 9.90</cell><cell>8.91</cell></row><row><cell></cell><cell>ResNet-110</cell><cell>6.61</cell><cell>6.37</cell></row><row><cell></cell><cell>ResNet-164</cell><cell>5.93</cell><cell>5.46</cell></row><row><cell></cell><cell>ResNet-1001</cell><cell>7.61</cell><cell>4.92</cell></row><row><cell cols="2">CIFAR-100 ResNet-164</cell><cell>25.16</cell><cell>24.33</cell></row><row><cell></cell><cell>ResNet-1001</cell><cell>27.82</cell><cell>22.71</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">It is noteworthy that there are Residual Units for increasing dimensions and reducing feature map sizes<ref type="bibr" target="#b0">[1]</ref> in which h is not identity. In this case the following derivations do not hold strictly. But as there are only a very few such units (two on CIFAR and three on ImageNet, depending on image sizes<ref type="bibr" target="#b0">[1]</ref>), we expect that they do not have the exponential impact as we present in Sect. 3. One may also think of our derivations as applied to all Residual Units within the same feature map size.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">See also: people.idsia.ch/∼rupesh/very deep learning/ by<ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">The ResNet-200 has 16 more 3-layer bottleneck Residual Units than ResNet-152, which are added on the feature map of 28×28.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>zero and f does not truncate it (x l is always non-negative due to the previous ReLU, so y l is below zero only when the magnitude of F is very negative). The truncation, however, is more frequent when there are 1000 layers.</p><p>Reducing Overfitting. Another impact of using the proposed pre-activation unit is on regularization, as shown in Fig. <ref type="figure">6</ref> (right). The pre-activation version reaches slightly higher training loss at convergence, but produces lower test error. This phenomenon is observed on ResNet-110, ResNet-110(1-layer), and ResNet-164 on both CIFAR-10 and 100. This is presumably caused by BN's regularization effect <ref type="bibr" target="#b7">[8]</ref>. In the original Residual Unit (Fig. <ref type="figure">4(a)</ref>), although the BN normalizes the signal, this is soon added to the shortcut and thus the merged signal is not normalized. This unnormalized signal is then used as the input of the next weight layer. On the contrary, in our pre-activation version, the inputs to all weight layers have been normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Comparisons on CIFAR-10/100. Table <ref type="table">4</ref> compares the state-of-the-art methods on CIFAR-10/100, where we achieve competitive results. We note that we do not specially tailor the network width or filter sizes, nor use regularization techniques (such as dropout) which are very effective for these small datasets. We obtain these results via a simple but essential concept-going deeper. These results demonstrate the potential of pushing the limits of depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons on ImageNet.</head><p>Next we report experimental results on the 1000class ImageNet dataset <ref type="bibr" target="#b2">[3]</ref>. We have done preliminary experiments using the skip connections studied in Figs. <ref type="figure">2 and 3</ref> on ImageNet with ResNet-101 <ref type="bibr" target="#b0">[1]</ref>, and observed similar optimization difficulties. The training error of these nonidentity shortcut networks is obviously higher than the original ResNet at the first learning rate (similar to Fig. <ref type="figure">3</ref>), and we decided to halt training due to limited resources. But we did finish a "BN after addition" version (Fig. <ref type="figure">4(b)</ref>) of ResNet-101 on ImageNet and observed higher training loss and validation error. This model's single-crop (224×224) validation error is 24.6 %/7.5 %, vs. the original ResNet-101's 23.6 %/7.1 %. This is in line with the results on CIFAR in Fig. <ref type="figure">6 (left)</ref>.</p><p>Table <ref type="table">5</ref> shows the results of ResNet-152 <ref type="bibr" target="#b0">[1]</ref> and ResNet-200 3 , all trained from scratch. We notice that the original ResNet paper <ref type="bibr" target="#b0">[1]</ref> trained the models using scale jittering with shorter side s ∈ [256, 480], and so the test of a 224×224 crop on s = 256 (as did in <ref type="bibr" target="#b0">[1]</ref>) is negatively biased. Instead, we test a single 320×320 crop from s = 320, for all original and our ResNets. Even though the ResNets are trained on smaller crops, they can be easily tested on larger crops because the ResNets are fully convolutional by design. This size is also close to 299×299 used by Inception v3 [19], allowing a fairer comparison.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Microsoft COCO: objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV 2014, Part V. LNCS</title>
				<editor>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Highway networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (ELUs)</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fractional max-pooling</title>
		<author>
			<persName><forename type="first">B</forename><surname>Graham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6071</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Striving for simplicity: the all convolutional net</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fitnets: hints for thin deep nets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rethinking inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
	<note>All you need is a good init</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: surpassing humanlevel performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
