<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Themis: An I/O-Efficient MapReduce</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alexander</forename><surname>Rasmussen</surname></persName>
							<email>arasmuss@cs.ucsd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Conley</surname></persName>
							<email>mconley@cs.ucsd.edu</email>
						</author>
						<author>
							<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>San</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Diego</forename><forename type="middle">Rishi</forename><surname>Kapoor</surname></persName>
							<email>rkapoor@cs.ucsd.edu</email>
						</author>
						<author>
							<persName><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
							<email>vahdat@cs.ucsd.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">UC</orgName>
								<address>
									<addrLine>San Diego Vinh The Lam</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">UC San Diego &amp; Google, Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Themis: An I/O-Efficient MapReduce</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1A2D647B3A0C1E88A2020122A584C74B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>C.2.4 [Computer-Communication Networks]: Distributed Systems-Distributed applications Design</term>
					<term>Performance</term>
					<term>Experimentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Big Data" computing increasingly utilizes the MapReduce programming model for scalable processing of large data collections. Many MapReduce jobs are I/O-bound, and so minimizing the number of I/O operations is critical to improving their performance. In this work, we present Themis, a MapReduce implementation that reads and writes data records to disk exactly twice, which is the minimum amount possible for data sets that cannot fit in memory.</p><p>In order to minimize I/O, Themis makes fundamentally different design decisions from previous MapReduce implementations. Themis performs a wide variety of MapReduce jobs -including click log analysis, DNA read sequence alignment, and PageRank -at nearly the speed of TritonSort's record-setting sort performance <ref type="bibr" target="#b28">[29]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Building efficient and scalable data processing systems is a challenging -and increasingly important -problem. Scale-out software systems implementing the MapReduce programming model, such as Google's MapReduce <ref type="bibr" target="#b7">[8]</ref> and Apache Hadoop <ref type="bibr" target="#b37">[39]</ref>, have made great strides in providing efficient system architectures for these workloads <ref type="bibr" target="#b38">[40]</ref>. However, a significant gap remains between the delivered performance of these systems and the potential performance available from the underlying hardware <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Our recent experience building TritonSort <ref type="bibr" target="#b28">[29]</ref>, a largescale sorting system, shows that an appropriately balanced implementation can realize orders of magnitude improvement in throughput and efficiency. Translating these types of gains to more general-purpose data processing systems will help close this efficiency gap, allowing more work to be performed with the same hardware, or the same amount of work to be performed with less hardware. This improved efficiency will result in substantially lowered system cost, energy usage, and management complexity.</p><p>Given that many MapReduce jobs are I/O-bound, an efficient MapReduce system must aim to minimize the number of I/O operations it performs. Fundamentally, every MapReduce system must perform at least two I/O operations per record when the amount of data exceeds the amount of memory in the cluster <ref type="bibr" target="#b0">[1]</ref>. We refer to a system that meets this lower-bound as having the "2-IO" property. Any data processing system that does not have this property is doing more I/O than it needs to. Existing MapReduce systems incur additional I/O operations in exchange for simpler and more fine-grained fault tolerance.</p><p>In this paper, we present Themis<ref type="foot" target="#foot_0">1</ref> , an implementation of MapReduce designed to have the 2-IO property. Themis accommodates the flexibility of the MapReduce programming model while simultaneously delivering high efficiency. It does this by considering fundamentally different points in the design space than existing MapReduce implementations:</p><p>1. Eliminating task-level fault tolerance: At the scale of tens of thousands of servers, failures are common, and so MapReduce was designed with a strong task-level fault tolerance model. However, more aggressive fault tolerance gains finer-grained restart at the expense of lower overall performance. Interestingly, many Hadoop users report cluster sizes of under 100 nodes <ref type="bibr" target="#b13">[14]</ref>, much smaller than those deployed by MapReduce's early adopters. In 2011, Cloudera's VP of Technology Solutions stated that the mean size of their clients' Hadoop clusters is 200 nodes, with the median size closer to 30 <ref type="bibr" target="#b22">[23]</ref>. At this moderate scale, failures are much less common, and aggressive fault tolerance is wasteful in the common case. Foregoing tasklevel fault tolerance permits a design that achieves the 2-IO property. When a job experiences a failure, Themis simply re-executes it. This optimistic approach to fault tolerance enables Themis to aggressively pipeline record processing without unnecessarily materializing intermediate results to disk. As we will show, for moderate cluster sizes this approach has the counter-intuitive effect of improving performance despite the occasional job re-execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dynamic, adaptive memory allocation:</head><p>To maintain the 2-IO property, Themis must process a record completely once it is read from disk. This prevents Themis from putting records back on disk in response to memory pressure through swapping or writing spill files. Themis implements a policy-based, application-level memory manager that provides fine-grained sharing of memory between operators processing semi-structured, variably-sized records. This allows it to support datasets with as much as a factor of 10 7 skew between record sizes while maintaining the 2-IO property.</p><p>3. Central management of shuffle and disk I/O: Themis uses a centralized, per-node disk scheduler that ensures that records from multiple sources are written to disk in large batches to reduce disk seeks. Themis delivers nearly sequential disk I/O across a variety of MapReduce jobs, even for workloads that far exceed the size of main memory.</p><p>To validate our design, we have written a number of MapReduce programs on Themis, including a web user session tracking application, PageRank, n-gram counting, and a DNA read sequence alignment application. We found that Themis processes these jobs at nearly the per-node performance of TritonSort's record-setting sort run and nearly the maximum sequential speed of the underlying disks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MOTIVATION</head><p>Themis achieves the 2-IO property by making different design decisions than those made in Google's and Hadoop's MapReduce implementations. In this section, we discuss our motivations for making these decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Target Deployment Environments</head><p>A large number of "Big Data" clusters do not approach the size of warehouse-scale data centers like those at Google and Microsoft because moderately-sized clusters (10s of racks or fewer) are increasingly able to support important real-world problem sizes. The storage capacity and number of CPU cores in commodity servers are both increasing rapidly. In Cloudera's reference system design <ref type="bibr" target="#b6">[7]</ref>, in which each node has 16 or more disks, a petabyte worth of 1TB drives fits into just over three racks, or about 60 nodes. Coupled with the emergence of affordable 10 Gbps Ethernet at the end host and increasing bus speeds, data can be packed more densely than ever before while keeping disk I/O as the bottleneck resource. This implies that fewer servers are required for processing large amounts of data for I/O-bound workloads. We now consider the implications of this increased density on fault tolerance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fault Tolerance for "Dense" Clusters</head><p>A key principle of Themis's design is that it performs job-level, rather than task-level, fault tolerance. Job-level fault tolerance allows for much more aggressive operator pipelining than task-level fault tolerance can achieve while still maintaining the 2-IO property. However, it is not selfevident that the overhead of re-executing failed jobs does not cancel any performance gained by this aggressive pipelining. In this section, we show not only that job-level fault tolerance is a feasible approach for moderately-sized clusters, but also that there are significant potential performance benefits for using job-level fault tolerance in these environments.</p><p>Understanding the expected impact of failures is critical  <ref type="bibr" target="#b11">[12]</ref>.</p><p>to choosing the appropriate fault tolerance model. MapReduce was designed for clusters of many thousands of machines running inexpensive, failure-prone commodity hardware <ref type="bibr" target="#b7">[8]</ref>. For example, Table <ref type="table" target="#tab_0">1</ref> shows component-level meantime to failure (MTTF) statistics in one of Google's data centers <ref type="bibr" target="#b11">[12]</ref>. Google's failure statistics are corroborated by similar studies of hard drive <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b31">33]</ref> and node <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">32]</ref> failure rates. At massive scale, there is a high probability that some portion of the cluster will fail during the course of a job. To understand this probability, we employ a simple model <ref type="bibr" target="#b2">[3]</ref>, shown in Equation <ref type="formula">1</ref>, to compute the likelihood that a node in a cluster of a particular size will experience a failure during a job:</p><formula xml:id="formula_0">P (N, t, M T T F ) = 1 -e -N •t/M T T F (1)</formula><p>The probability of a failure occurring in the next t seconds is a function of (1) the number of nodes in the cluster, N , (2) t, and (3) the mean time to failure of each node, M T T F , taken from the node-level failure rates in Table <ref type="table" target="#tab_0">1</ref>. This model assumes that nodes fail with exponential (Pareto) probability, and we simplify our analysis by considering node failures only. We do this because disk failures can be made rare by using node-level mechanisms (i.e., RAID), and correlated rack failures are likely to cripple the performance of a cluster with only a few racks. Based on the above model, in a 100,000 node data center, there is a 93% chance that a node will fail during any five-minute period. On the other hand, in a moderately-sized cluster (e.g., 200 nodes, the average Hadoop cluster size as reported by Cloudera), there is only a 0.53% chance of encountering a node failure during a five-minute window, assuming the MTTF rates in Table <ref type="table" target="#tab_0">1</ref>.</p><p>This leads to the question of whether smaller deployments benefit from job-level fault tolerance, where if any node running a job fails the entire job restarts. Intuitively, this scheme will be more efficient overall when failures are rare and/or jobs are short. In fact, we can model the expected completion time of a job S(p, T ) as:</p><formula xml:id="formula_1">S(p, T ) = T p 1 -p + 1 (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where p is the probability of a node in the cluster failing, and T is the runtime of the job (a derivation of this result is given in Appendices A and B). This estimate is pessimistic, in that it assumes that jobs fail just before the end of their execution. By combining equations 1 and 2, we can compute the expected benefit-or penalty-that we get by moving to job-level fault tolerance. Modeling the expected runtime of a job with task-level fault tolerance is non-trivial, so we instead compare to an error-free baseline in which the system's performance is not affected by node failure. This comparison underestimates the benefit of job-level fault tolerance.</p><p>Figure <ref type="figure">1</ref> shows the expected performance benefits of job-level fault tolerance compared to the error-free baseline. More formally, we measure performance benefit as S(P (•), T job )/T task , where T job is the time a job on an error- The benefits of job-level fault tolerance increase as the error-free performance improvement made possible by moving to job-level fault tolerance (i.e. T task /T job ) increases. For example, if T task /T job is 4, T task is one hour and we run on a cluster of 1,000 nodes, we can expect Themis to complete the job 240% faster than the task-level fault tolerant alternative on average; this scenario is marked with a star in Figure <ref type="figure">1(b)</ref>. There are also situations in which joblevel fault tolerance will significantly under-perform tasklevel fault tolerance. For example, if T task /T job is 2, Themis will under-perform a system with task-level fault tolerance for clusters bigger than 500 nodes. From this, we make two key observations: for job-level fault tolerance to be advantageous, the cluster has to be moderately-sized, and Themis must significantly outperform the task-level alternative.</p><p>In the next section, we describe key challenges in designing a system that meets these high performance requirements while maintaining the 2-IO property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE CHALLENGE OF SKEW</head><p>One of MapReduce's attractive properties is its ability to handle semi-structured and variably-sized data. This variability makes maintaining the 2-IO property a challenge. In this section, we describe two sources of variability and the resulting requirements they place on our design.</p><p>An input dataset can exhibit several different kinds of skew, which simply refers to variability in its structure and content. These include:</p><p>Record Size Skew: In systems with semi-structured or unstructured data, some records may be much larger than others. This is called record size skew. In extreme cases, a single record may be gigabytes in size.</p><p>Partitioning Skew: Data that is not uniformly distributed across its keyspace exhibits partitioning skew. This can cause some nodes to process much more data than others if the data is naïvely partitioned across nodes, creating stragglers <ref type="bibr" target="#b8">[9]</ref>. Handling skew in MapReduce is complicated by the fact that the distribution of keys in the data produced by a map function is often not known in advance. Existing MapReduce implementations handle partitioning skew by spilling records to disk that cannot fit into memory.</p><p>Computational Skew: In a dataset exhibiting computational skew, some records take much longer than average to process. Much of the work on mitigating computational skew in MapReduce involves exploiting the nature of the particular problem and relying on a degree of user guidance <ref type="bibr" target="#b15">[16]</ref> or proactively re-partitioning the input data for a task <ref type="bibr" target="#b16">[17]</ref>. As the focus of our work is I/O-bound jobs, we do not consider computational skew in this work.</p><p>Performance Heterogeneity: The performance of a population of identical machines can vary significantly; the reasons for this heterogeneity are explored in <ref type="bibr" target="#b29">[31]</ref>. In addition, clusters are rarely made up of a homogeneous collection of machines, due both to machine failures and planned incremental upgrades. While we believe that the techniques presented in this work can be applied to heterogeneous clusters, we have not evaluated Themis in such a setting.</p><p>To handle record skew, Themis dynamically controls its memory usage, which we describe in Section 5. Themis adopts a sampling-based skew mitigation technique to minimize the effects of partitioning skew. We discuss this mitigation technique in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SYSTEM ARCHITECTURE</head><p>In this section, we describe the design of Themis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Core Architecture</head><p>Themis reuses several core runtime components that were used to build the TritonSort <ref type="bibr" target="#b28">[29]</ref> sorting system. Like Tri-tonSort, Themis is written as a sequence of phases, each of which consists of a directed dataflow graph of stages connected by FIFO queues. Each stage consists of a number of workers, each running as a separate thread.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MapReduce Overview</head><p>Unlike existing MapReduce systems, which executes map and reduce tasks concurrently in waves, Themis implements the MapReduce programming model in three phases of operation, summarized in Table <ref type="table">2</ref>. Phase zero, described in Section 6, is responsible for sampling input data to determine the distribution of record sizes as well as the distribution of keys. These distributions are used by subsequent phases to At the end of phase two, the MapReduce job is complete. Phase one reads each input record and writes each intermediate record exactly once. Phase two reads each intermediate partition and writes its corresponding output partition exactly once. Thus, Themis has the 2-IO property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Phase One: Map and Shuffle</head><p>Phase one is responsible for implementing both the map operation as well as shuffling records to their appropriate intermediate partition. Each node in parallel implements the stage graph pipeline shown in Figure <ref type="figure">2</ref>.</p><p>The Reader stage reads records from an input disk and sends them to the Mapper stage, which applies the map function to each record. As the map function produces intermediate records, each record's key is hashed to determine the node to which it should be sent and placed in a perdestination buffer that is given to the sender when it is full. The Sender sends data to remote nodes using a round-robin loop of short, non-blocking send() calls. We call the Reader to Sender part of the pipeline the "producer-side" pipeline.</p><p>The Receiver stage receives records from remote nodes over TCP using a round-robin loop of short, non-blocking recv() calls. We implemented a version of this stage that uses select() to avoid unnecessary polling, but found that its performance was too unpredictable to reliably receive all-to-all traffic at 10Gbps. The receiver places incoming records into a set of small per-source buffers, and sends those buffers to the Demux stage when they become full.</p><p>The Demux stage is responsible for assigning records to partitions. It hashes each record's key to determine the partition to which it should be written, and appends the record to a small per-partition buffer. When that buffer becomes full, it is emitted to the Chainer stage, which links buffers for each partition into separate chains. When chains exceed a pre-configured length, which we set to 4.5 MB to avoid doing small writes, it emits them to the Coalescer stage. The Coalescer stage merges chains together into a single large buffer that it sends to the Writer stage, which appends buffers to the appropriate partition file. The combination of the Chainer and Coalescer stages allows buffer memory in front of the Writer stage to be allocated to partitions in a highly dynamic and fine-grained way. We call the Receiver to Writer part of the pipeline the "consumer-side" pipeline.</p><p>A key requirement of the consumer-side pipeline is to perform large, contiguous writes to disk to minimize seeks and provide high disk bandwidth. We now describe a node-wide, application-driven disk scheduler that Themis uses to ensure that writes are large.</p><p>Each writer induces back-pressure on chainers, which causes the per-partition chains to get longer. In this way, data gets buffered within the chainer. This buffering can grow very large-to over 10GB on a machine with 24GB of memory. The longer a chain becomes, the larger the corresponding write will be. We limit the size of a chain to 14MB, to prevent very large writes from restricting pipelining. The large writes afforded by this scheduler allow Themis to write at nearly the sequential speed of the disk. <ref type="bibr" target="#b28">[29]</ref> provides a detailed evaluation of the relationship between write sizes and system throughput.</p><p>Signaling back-pressure between the chainer and the writer stage is done by means of write tokens. The presence of a write token for a writer indicates that it can accept additional buffers. When the writer receives work, it removes its token, and when it finishes, it returns the token. Tokens are also used to prevent the queues between the chainer and writer stages from growing without bound.  By the end of phase one, the map function has been applied to each input record, and the records have been grouped into partitions and stored on the appropriate node so that all records with the same key are stored in a single partition. In phase two, each partition must be sorted by key, and the reduce function must be applied to groups of records with the same key. The stages that implement phase two are shown in Figure <ref type="figure" target="#fig_0">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Phase Two: Sort and Reduce</head><p>There is no network communication in phase two, so nodes process their partitions independently. Entire partitions are read into memory at once by the Reader stage. A Sorter  stage sorts these partitions by key, keeping the result in memory. The Reducer stage applies the reduce function to all records sharing a key. Reduced records are sent to the Writer, which writes them to disk.</p><p>All records with a single key must be stored in the same partition for the reduce function to produce correct output. As a result, partitioning skew can cause some partitions to be significantly larger than others. Themis's memory management system allows phase two to process partitions that approach the size of main memory, and its optional skew mitigation phase can reduce partitioning skew without user intervention. We describe these systems in Sections 5 and 6, respectively.</p><p>A key feature of Themis's sorter stage is that it can select which sort algorithm is used to sort a buffer on a buffer-bybuffer basis. There is a pluggable sort strategy interface that lets developers use different sorting algorithms; currently quicksort and radix sort are implemented. Each sort strategy calculates the amount of scratch space it needs to sort the given buffer, depending on the buffer's contents and the sort algorithm's space complexity. For both quicksort and radix sort, this computation is deterministic. In Themis, radix sort is chosen if the keys are all the same size and the required scratch space is under a configurable threshold; otherwise, quicksort is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">MEMORY MANAGEMENT AND FLOW CONTROL</head><p>Themis relies on a dynamic and flexible memory management system to partition memory between operators. Themis's memory manager actually serves two distinct purposes: (1) it enables resource sharing between operators, and (2) it supports enforcing back-pressure and flow control. In the first case, Themis requires flexible use of memory given our desire to support large amounts of record size skew while maintaining the 2-IO property. In the second case, individual stages in the Themis pipeline naturally run at different speeds (e.g., the network is 10 Gbps, whereas the disk subsystem only supports writing at approximately 5.5 Gbps), and so back-pressure and flow control are needed to prevent faster stages from starving slower stages for resources.</p><p>Themis supports a single memory allocation interface with pluggable memory policies. We first describe the memory allocator's interface, and then describe the three policies that we've implemented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Memory allocation interface</head><p>Worker stages in Themis allocate space for buffers and other necessary scratch space using a unified and simple memory allocator interface, shown in Table <ref type="table" target="#tab_6">4</ref>.</p><p>Memory allocators can be assigned on a stage-by-stage basis, but in the current implementation we assume that memory regions are allocated and deallocated by the same allocator. The allocate call blocks until the underlying memory allocation policy satisfies the allocation, which can be an unbounded amount of time. As we will see, this simple mechanism, paired with one of three memory policies, provides for both resource sharing as well as flow control.</p><p>We now examine each of these polices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Policy 1: Pool-Based Management</head><p>The first policy we consider is a "pool" memory policy, which is inherited from TritonSort <ref type="bibr" target="#b28">[29]</ref>. A memory pool is a set of pre-allocated buffers that is filled during startup. Buffers can be checked out from a pool, and returned when they are finished being used as illustrated in Figure <ref type="figure" target="#fig_1">4</ref>. When a worker tries to check out a buffer from an empty pool, it blocks until another worker returns a buffer to that pool. The pool memory policy has the advantage that all memory allocation is done at startup, avoiding allocation during runtime. Through efficient implementation, the overhead of checking out buffers can be very small. This is especially useful for stages that require obtaining buffers with very low latency, such as the Receiver stage, which obtains buffers to use in receiving data from the network. The receiver receives uninterpreted bytes from network sockets into small, fixedsize byte buffers. These buffers are passed to a subsequent stage, which converts them into buffers containing complete records. For this reason, the receiver can use pool-based management while still supporting record-size skew.</p><p>Pools can be used to provide resource sharing between workers by giving each of those workers a reference to a single pool. The producer-consumer relationship between pairs of stages also provides a form of flow control; the upstream stage checking out buffers can only produce work at the rate at which the downstream stage can return them to the pool. However, pools have several disadvantages. First, the buffers in a pool are all fixed-size, and so the pool memory policy supports very limited amounts of data skew. By carving memory up into fixed-size pools, the maximum record size supported by this policy is limited to the size of the smallest pool. Additionally, buffer pools reserve a fixed amount of memory for a particular pair of stages. One consequence of this is a loss of flexibility; if one stage temporarily needs more memory than usual (e.g., if it is handling a large record), it cannot "borrow" that memory from another stage due to the static partitioning of memory across pools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Policy 2: Quota-Based Management</head><p>While the pool memory policy is simple, it is quite inflexible, and does not handle skewed record sizes very well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Function Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CallerID registerCaller(Worker worker)</head><p>Register worker with the allocator void* allocate(CallerID caller, uint64_t size) allocate a memory region of size bytes for caller void deallocate(void* memory) deallocate memory that was allocated by this allocator  The quota-based memory policy is designed to support more flexible memory allocation while still providing flow control. At a high level, the quota policy ensures that stages producing records do not overwhelm stages that eventually consume them. For example, most of our evaluation is writer limited, and so we want to ensure that the receiver stage, which produces records received from the network, does not overwhelm the writer stage, which is the bottleneck.</p><p>Themis has three such producer-consumer pairs: between the reader and the mapper (with the mapper acting as the consumer), between the mapper and the sender (with the mapper acting as the producer), and between the receiver and the writer. The mapper acts as both a consumer and a producer, since it is the only stage in the phase one pipeline that creates records as directed by the map function that were not read by the reader.</p><p>Quotas are enforced by the queues between stages. A quota can be viewed as the amount of memory that the pipeline between a producer and a consumer can use. When a producer stage pushes a buffer into the pipeline, the size of that buffer is debited from the quota. When a consumer stage consumes that buffer, the buffer's size is added back to the quota. If a producer is about to exceed the quota, then it blocks until the consumer has consumed sufficient memory. Quota-based allocation is illustrated in Figure <ref type="figure" target="#fig_2">5</ref>.</p><p>Quota-based memory management dramatically reduces the number of variables that need to be tuned relative to the pool-based memory policy. One need only adjust the quota allocations present between pairs of stages, rather than the capacity of a much larger number of buffer pools. Additionally, stages that are not producers and consumers do not need to do any form of coordination, which makes their memory allocations very fast.</p><p>Quota-based management assumes that any scratch space or additional memory needed by stages between the producer and consumer is accounted for in the quota. This is to prevent intermediate stages from exceeding the total amount of memory, since their memory accesses are not tracked. It also tacitly assumes that the size of a buffer being produced cannot exceed the size of the quota. This is much less re- strictive than a pool-based approach, as quotas are typically several gigabytes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Policy 3: Constraint-Based Management</head><p>In situations where the amount of memory used by stages to process records cannot be determined in advance, quota-based systems are not ideal for providing flow control. In these situations, Themis uses a more heavyweight, constraint-based memory management policy.</p><p>In the constraint-based memory policy, the total amount of memory in use by workers is tracked centrally in the memory allocator. If a worker requests memory, and enough memory is available, that request is granted immediately. Otherwise, the worker's request is added to a per-worker queue of outstanding requests and the worker sleeps on a condition variable until the request can be satisfied. Constraint-based allocation is illustrated in Figure <ref type="figure">6</ref>.</p><p>When multiple workers have outstanding unsatisfied allocation requests, the memory allocator prioritizes worker requests based on a worker's distance in the stage graph to a stage that consumes records. The producer-side pipeline measures distance to the sender stage, and the consumerside pipeline measures distance to the writer stage. The rationale behind this decision is that records that are being processed should be completely processed before more work is admitted. This decision is inspired by work on live-lock prevention in routers <ref type="bibr" target="#b21">[22]</ref>. In this way, the constraint-based allocator implements flow control based on the structure of the dataflow graph.</p><p>While this system places precise upper bounds on the amount of memory present in the system, it requires a great deal of coordination between workers, which requires significant lock contention in our implementation. In effect, the reliance on keeping the amount of available memory consistent requires that all allocation and deallocation requests are processed serially. Hence, constraint-based memory allocation is useful for situations where the number of allocation requests being made is relatively small, but the probability of exceeding available memory in common-case operation is high. Phase two in Themis uses constraint-based memory management for precisely these reasons.</p><p>In the constraint-based policy, it is possible that certain allocation interleavings can trigger deadlock. Predicting whether a general dataflow system will deadlock is undecidable <ref type="bibr" target="#b23">[24]</ref>, and deadlock prevention requires knowledge of data dependencies between stages that we deemed too heavyweight. To addressed the problem of deadlocks, Themis provides a deadlock detector. The deadlock detector periodically probes workers to see if they are waiting for a memory allocation request to complete. If multiple probe cycles pass in which all workers are waiting for an allocation or are idle, the deadlock detector informs the memory allocator that a deadlock has occurred. We have not experienced deadlock using the policy choices described in Table <ref type="table" target="#tab_5">3</ref> in any of the MapReduce jobs we have evaluated. Efficient ways of handling deadlock is the subject of ongoing work.</p><p>In summary, Themis provides a pluggable, policy-driven memory allocation subsystem that provides for flexible resource sharing between stages and workers to handle record size skew while also enabling flow control.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">SKEW MITIGATION</head><p>To satisfy the 2-IO property, Themis must ensure that every partition can be sorted in memory, since an out-of-core sort would induce additional I/Os. In addition, to support parallelism, partitions must be small enough that several partitions can be processed in parallel. Phase zero is responsible for choosing the number of partitions, and selecting a partitioning function to keep each partition roughly the same size. This task is complicated by the fact that the data to be partitioned is generated by the map function. Thus, even if the distribution of input data is known, the distribution of intermediate data may not be known. This phase is optional: if the user has knowledge of the intermediate data's distribution, they can specify a custom partitioning function, similar to techniques used in <ref type="bibr">Hadoop.</ref> Phase zero approximates the distribution of intermediate data by applying the map function to a subset of the input. If the data is homoscedastic, then a small prefix of the input is sufficient to approximate the intermediate distribution. Otherwise, more input data will need to be sampled, or phase two's performance will decrease. DeWitt et al. <ref type="bibr" target="#b9">[10]</ref> formalize the number of samples needed to achieve a given skew with high probability; typically we sample 1 GB per node of input data for nodes supporting 8 TB of input. The correctness of phase two only depends on partitions being smaller than main memory. Since our target partition size is less than 5% of main memory, this means that a substantial sampling error would have to occur to cause job failure. So although sampling does impose additional I/O over the 2-IO limit, we note that it is a small and constant overhead.</p><p>Once each node is done sampling, it transmits its sample information to a central coordinator. The coordinator uses these samples to generate a partition function, which is then re-distributed back to each node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Mechanism</head><p>On each node, Themis applies the map operation to a prefix of the records in each input file stored on that node. As the map function produces records, the node records information about the intermediate data, such as how much larger or smaller it is than the input and the number of records generated. It also stores information about each intermediate key and the associated record's size. This information varies based on the sampling policy. Once the node is done sampling, it sends that metadata to the coordinator.</p><p>The coordinator merges the metadata from each of the nodes to estimate the intermediate data size. It then uses this size, and the desired partition size, to compute the number of partitions. Then, it performs a streaming merge-sort on the samples from each node. Once all the sampled data is sorted, partition boundaries are calculated based on the desired partition sizes. The result is a list of "boundary keys" that define the edges of each partition. This list is broadcast back to each node, and forms the basis of the partitioning function used in phase one.</p><p>The choice of sampling policy depends on requirements from the user, and we now describe each policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Sampling Policies</head><p>Themis supports the following sampling policies:</p><p>(1) Range partitioning: For MapReduce jobs in which the ultimate output of all the reducers must be totally ordered (e.g., sort), Themis employs a range partitioning sampling policy. In this policy, the entire key for each sampled record is sent to the coordinator. A downside of this policy is that very large keys can limit the amount of data that can be sampled because there is only a limited amount of space to buffer sampled records.</p><p>(2) Hash partitioning: For situations in which total ordering of reduce function output is not required, Themis employs hash partitioning. In this scheme, a hash of the key is sampled, instead of the keys themselves. This has the advantage of supporting very large keys, and allowing Themis to use reservoir sampling <ref type="bibr" target="#b35">[37]</ref>, which samples data in constant space in one pass over its input. This enables more data to be sampled with a fixed amount of buffer. This approach also works well for input data that is already partially or completely sorted because adjacent keys are likely to be placed in different partitions, which spreads the data across the cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">EVALUATION</head><p>We evaluate Themis through benchmarks of several different MapReduce jobs on both synthetic and real-world data sets. A summary of our results are as follows:</p><p>• Themis is highly performant on a wide variety of MapReduce jobs, and outperforms Hadoop by 3x -16x on a variety of common jobs.</p><p>• Themis can achieve nearly the sequential speed of the disks for I/O-bound jobs, which is approximately the same rate as TritonSort's record-setting performance.</p><p>• Themis's memory subsystem is flexible, and is able to handle large amounts of data skew while ensuring efficient operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Workloads and evaluation overview</head><p>We evaluate Themis on a cluster of HP DL380G6 servers, each with two Intel E5520 CPUs (2.27 GHz), 24 GB of memory, and 16 500GB 7200 RPM 2.5" SATA drives. Each hard drive is configured with a single XFS partition. Each To evaluate Themis at scale, we often have to rely on large synthetically-generated data sets, due to the logistics of obtaining and storing freely-available, large data sets. All synthetic data sets are evaluated on 20 cluster nodes. Nonsynthetic data sets are small enough to be evaluated on a single node.</p><p>All input and output data is stored on local disks without using any distributed filesystem and without replication. We are actively exploring integrating Themis with storage systems like HDFS, but an evaluation of such integration is the subject of future work.</p><p>We evaluate Themis's performance on several different MapReduce jobs. A summary of these jobs is given in Table 5, and each job is described in more detail below.</p><p>Sort: Large-scale sorting is a useful measurement of the performance of MapReduce and of data processing systems in general. During a sort job, all cluster nodes are reading from disks, writing to disks, and doing an all-to-all network transfer simultaneously. Sorting also measures the performance of MapReduce independent of the computational complexity of the map and reduce functions themselves, since both map and reduce functions are effectively no-ops. We study the effects of both increased data density and skew on the system using sort due to the convenience with which input data that meets desired specifications can be generated. We generate skewed data with a Pareto distribution. The record size in generated datasets is limited by a fixed maximum, which is a parameter given to the job.</p><p>WordCount: Word count is a canonical MapReduce job. Given a collection of words, word count's map function emits &lt;word, 1&gt; records for each word. Word count's reduce function sums the occurrences of each word and emits a single &lt;word, N&gt; record, where N is the number of times the word occurred in the original collection.</p><p>We evaluate WordCount on the 2012-05-05 version of the Freebase Wikipedia Extraction (WEX) <ref type="bibr" target="#b36">[38]</ref>, a processed dump of the English version of Wikipedia. The complete WEX dump is approximately 62GB uncompressed, and contains both XML and text versions of each page. We run word count on the text portion of the WEX data set, which is approximately 8.2GB uncompressed.</p><p>n-Gram Count: An extension of word count, n-gram count counts the number of times each group of n words appears in a text corpus. For example, given "The quick brown fox jumped over the lazy dog", 3-gram count would count the number of occurrences of "The quick brown", "quick brown fox", "brown fox jumped", etc. We also evaluate n-gram count on the text portion of the WEX data set.</p><p>PageRank: PageRank is a graph algorithm that is widely used by search engines to rank web pages. Each node in the graph is given an initial rank. Rank propagates through the graph by each vertex contributing a fraction of its rank evenly to each of its neighbors.</p><p>PageRank's map function is given a &lt;vertex ID, adjacency list of vertex IDs|initial rank&gt; pair for each vertex in the graph. It emits &lt;adjacent vertex ID, rank contribution&gt; pairs for each adjacent vertex ID, and also re-emits the adjacency list so that the graph can be reconstructed. PageRank's reduce function adds the rank contributions for each vertex to compute that vertex's rank, and emits the vertex's existing adjacency list and new rank.</p><p>We evaluate PageRank with three different kinds of graphs.</p><p>The first (PageRank-U) is a 25M vertex synthetically-generated graph where each vertex has an edge to every other vertex with a small, constant probability. Each vertex has an expected degree of 5,000. The second (PageRank-PL) is a 250M vertex synthetically-generated graph where vertex in-degree follows a power law distribution with values between 100 and 10,000. This simulates a more realistic page graph where a relatively small number of pages are linked to frequently. The third (PageRank-WEX) is a graph derived from page links in the XML portion of the WEX data set; it is approximately 1.5GB uncompressed and has 5.3M vertices.</p><p>CloudBurst: CloudBurst <ref type="bibr" target="#b20">[21]</ref> is a MapReduce implementation of the RMAP <ref type="bibr" target="#b33">[35]</ref> algorithm for short-read gene alignment, which aligns a large collection of small "query" DNA sequences called reads with a known "reference" genome. CloudBurst performs this alignment using a standard technique called seed-and-extend. Both query and reference sequences are passed to the map function and emitted as a series of fixed-size seeds. The map function emits seeds as sequence of &lt;seed, seed metadata&gt; pairs, where the seed metadata contains information such as the seed's location in its parent sequence, whether that parent sequence was a query or a reference, and the characters in the sequence immediately before and after the seed.</p><p>CloudBurst's reduce function examines pairs of query and reference strings with the same seed. For each pair, it computes a similarity score of the DNA characters on either side of the seed using the Landau-Vishkin algorithm for approximate string matching. The reduce function emits all query/reference pairs with a similarity score above a configured threshold.</p><p>We evaluate CloudBurst on the lakewash combined v2 data set from University of Washington <ref type="bibr" target="#b14">[15]</ref>, which we preprocess using a slightly modified version of the CloudBurst input loader used in Hadoop.</p><p>Click Log Analysis: Another popular MapReduce job is analysis of click logs. Abstractly, click logs can be viewed as a collection of &lt;user ID, timestamp|URL&gt; pairs indicating which page a user loaded at which time. We chose to evaluate one particular type of log analysis task, session tracking. In this task, we seek to identify disjoint ranges of timestamps at least some number of seconds apart. For each such range of timestamps, we output &lt;user ID, start timestamp|end timestamp|start URL|end URL&gt; pairs. The map function is a pass-through; it simply groups records by user ID. The reduce function does a linear scan through records for a given user ID and reconstructs sessions. For efficiency, it assumes that these records are sorted in ascending order by timestamp. We describe the implications of this assumption in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Job Implementation Details</head><p>In this section, we briefly describe some of the implementation details necessary for running our collection of example jobs at maximum efficiency.</p><p>Combiners: A common technique for improving the performance of MapReduce jobs is employing a combiner. For example, word count can emit a single &lt;word, k&gt; pair instead of k &lt;word, 1&gt; pairs. Themis supports the use of combiner functions. We opted to implement combiners within the mapper stage on a job-by-job basis rather than adding an additional stage. Despite what conventional wisdom would suggest, we found that combiners actually decreased our performance in many cases because the computational overhead of manipulating large data structures was enough to make the mapper compute-bound. The large size of these data structures is partially due to our decision to run the combiner over an entire job's intermediate data rather than a small portion thereof to maximize its effectiveness.</p><p>In some cases, however, a small data structure that takes advantage of the semantics of the data provides a significant performance increase. For example, our word count MapReduce job uses a combiner that maintains a counter for the top 25 words in the English language. The combiner updates the appropriate counter whenever it encounters one of these words rather than creating an intermediate record for it. At the end of phase one, intermediate records are created for each of these popular words based on the counter values.</p><p>Improving Performance for Small Records: The map functions in our first implementations of word count and n-gram count emitted &lt;word/n-gram, 1&gt; pairs. Our implementations of these map functions emit &lt;hash(word), 1|word&gt; pairs instead because the resulting intermediate partitions are easier to sort quickly because the keys are all small and the same size.</p><p>Secondary Keys: A naïve implementation of the session extraction job sorts records for a given user ID by timestamp in the reduce function. We avoid performing two sorts by allowing the Sorter stage to use the first few bytes of the value, called a secondary key, to break ties when sorting. For example, in the session extraction job the secondary key is the record's timestamp.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Performance</head><p>We evaluate the performance of Themis in two ways. First, we compare performance of the benchmark applications to the cluster's hardware limits. Second, we compare the performance of Themis to that of Hadoop on two benchmark applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1">Performance Relative to Disk Speeds</head><p>The performance of Themis on the benchmark MapReduce jobs is shown in Figure <ref type="figure" target="#fig_3">7</ref> terms of MB/s/disk in order to provide a relative comparison to the hardware limitations of the cluster. The 7200 RPM drives in the cluster are capable of approximately 90 MB/s/disk of sequential write bandwidth, which is shown as a dotted line in the figure . A job running at 90 MB/s/disk is processing data as fast as it can be written to the disks. Most of the benchmark applications run at near maximum speed in both phases. CloudBurst's poor performance in phase two is due to the computationally intensive nature of its reduce function, which is unable to process records fast enough to saturate the disks. More CPU cores are needed to drive computationally intensive applications such as Cloud-Burst at maximum speed in both phases. Notice however that CloudBurst is still able to take advantage of our architecture in phase one.</p><p>We have included TritonSort's performance on the Indy 100TB sort benchmark for reference. TritonSort's 2011 Indy variant runs a much simpler code base than Themis. We highlight the fact that Themis's additional complexity and flexibility does not impact its ability to perform well on a variety of workloads. Our improved performance in phase one relative to TritonSort at scale is due to a variety of internal improvements and optimizations made to the codebase in the intervening period, as well as the improved memory utilization provided by moving from buffer pools to dynamic memory management. Performance degradation in phase two relative to TritonSort is mainly due to additional CPU and memory pressure introduced by the Reducer stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2">Comparison with Hadoop</head><p>We evaluate Hadoop version 1.0.3 on the Sort-500G and CloudBurst applications. We started with a configuration based on the configuration used by Yahoo! for their 2009 Hadoop sort record <ref type="bibr" target="#b34">[36]</ref>. We optimized Hadoop as best we could, but found it difficult to get it to run many large parallel transfers without having our nodes blacklisted for running out of memory.</p><p>The total running times for both Hadoop and Themis are given in Table <ref type="table" target="#tab_9">6</ref>. I/O-bound jobs such as sort are able to take full advantage of our architecture, which explains why Themis is more than a factor of 16 faster. As explained above, CloudBurst is fundamentally compute-bound, but the performance benefits of the 2-IO property allow the Themis implementation of CloudBurst to outperform the Hadoop implementation by a factor of 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Memory Management</head><p>In this section, we evaluate the performance of our different memory allocation policies. We also show that our allocation system is robust in the face of transient changes in individual stage throughputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.1">Memory Allocator Performance</head><p>We examine both the individual allocation times of our different memory allocation policies and their end-to-end performance. We evaluate the performance on phase one of a 200GB, 1-node sort job. Table <ref type="table" target="#tab_10">7</ref> shows that phase one's throughput is essentially unaffected by the choice of allocator policy in this particular instance. These performance numbers can be explained by looking at the mean allocation time for each worker in the system. Figure <ref type="figure">8</ref> shows that while the constraint-based allocator is more than twice as slow as the quota-based allocator, the absolute allocation times are both measured in tens of microseconds, which is negligible compared to time taken to actually do useful work. However, the results above only hold in the case where the constraint-based allocator does not deadlock. While we never experienced deadlock in phase two, we found it was quite easy to construct situations in which phase one deadlocked. For example, the exact same experiment conducted on a slightly larger data set causes deadlock in phase one with the constraint-based allocator.</p><p>The performance results in Figure <ref type="figure" target="#fig_3">7</ref> demonstrate the constraint-based allocation policy performs well in phase two. Because phase two handles entire intermediate partitions in memory, its allocations are orders of magnitude larger than those in phase one. This dramatically increases the likelihood that a single memory request is larger than one of the phase's quotas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.2">Robustness of the Quota-Based Memory Allocation Policy</head><p>We evaluate the robustness of the quota-based memory allocator by artificially slowing down the network for a period of time. We observe the effect on the total quota usage of a stage in the pipeline. Figure <ref type="figure">9</ref> shows that the Reader Converter's quota usage spikes up to its limit of 2GB in response to a slow network and then returns back to a steady state of near 0. A slow network means that stages upstream of the network are producing data faster than the network can transmit data. This imbalance leads to data backing up in front of the network. In the absence of the quota allocation policy, this data backlog grows unbounded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Skew Mitigation</head><p>Next, we evaluate Themis's ability to handle skew by observing the sizes of the intermediate data partitions created in phase one. Figure <ref type="figure">10</ref> shows the partition sizes produced by Themis on the evaluated applications. The error bars denoting the 95% confidence intervals are small, indicating that all partitions are nearly equal in size. This is unsurprising for applications with uniform data, such as sort. However, Themis also achieves even partitioning on very skewed data sets, such as Pareto-distributed sort, PageRank, and WordCount. PageRank-WEX has fairly small partitions relative to the other jobs because its intermediate data size is not large enough for phase zero to create an integer number of partitions with the desired size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Write Sizes</head><p>One of primary goals of phase one is to do large writes to each partition to avoid unnecessary disk seeks. Figure <ref type="figure" target="#fig_5">11</ref> shows the median write sizes of the various jobs we evaluated. For jobs like Sort and n-Gram where the map func- tion is extremely simple and mappers can map data as fast as readers can read it, data buffers up in the Chainer stage and all writes are large. As the amount of intermediate data per node grows, the size of a chain that can be buffered for a given partition decreases, which fundamentally limits the size of a write. For example, Sort-1.75T writes data to 2832 partitions, which means that its average chain length is not expected to be longer than about 5 MB given a receiver memory quota of 14GB; note, however, that the mean write size is above this minimum value, indicating that the writer is able to take advantage of temporary burstiness in activity for certain partitions. If the stages before the Writer stage cannot quite saturate it (such as in WordCount, CloudBurst and PageRank), chains remain fairly small. Here the minimum chain size of 4.5 MB ensures that writes are still reasonably large. In the case of PageRank-WEX, the data size is too small to cause the chains to ever become very large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">RELATED WORK</head><p>There is a large continuum of fault tolerance options between task-level restart and job-level restart, including distributed transactions <ref type="bibr" target="#b25">[26]</ref>, checkpointing and rollback <ref type="bibr" target="#b10">[11]</ref>, lineage-based recovery <ref type="bibr" target="#b39">[41]</ref> and process-pairs replication <ref type="bibr" target="#b32">[34]</ref>. Each fault tolerance approach introduces its own overheads and has its own complexities and limitations. With Themis, we choose to focus our efforts on creating a MapReduce system model that is able to handle large real-world data sets while utilizing the resources of an existing cluster as much as possible.</p><p>Recovery-Oriented Computing (ROC) [30, 5] is a research vision that focuses on efficient recovery from failure, rather than focusing exclusively on failure avoidance. This is helpful in environments where failure is inevitable, such as data centers. The design of task-level fault tolerance in existing MapReduce implementations shares similar goals with the ROC project.</p><p>Sailfish <ref type="bibr" target="#b27">[28]</ref> aims to mitigate partitioning skew in MapReduce by choosing the number of reduce tasks and intermediate data partitioning dynamically at runtime. It chooses these values using an index constructed on intermediate data. Sailfish and Themis represent two design points in a space with the similar goal of improving MapReduce's performance through more efficient disk I/O.</p><p>Several efforts aim to improve MapReduce's efficiency and performance. Some focus on runtime changes to better handle common patterns like job iteration <ref type="bibr" target="#b3">[4]</ref>, while others have extended the programming model to handle incremental updates <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26]</ref>. Work on new MapReduce scheduling disciplines <ref type="bibr" target="#b40">[42]</ref> has improved cluster utilization at a map-or reduce-task granularity by minimizing the time that a node waits for work. Tenzing <ref type="bibr" target="#b5">[6]</ref>, a SQL implementation built atop the MapReduce framework at Google, relaxes or removes the restriction that intermediate data be sorted by key in certain situations to improve performance.</p><p>Massively parallel processing (MPP) databases often perform aggregation in memory to eliminate unnecessary I/O if the output of that aggregation does not need to be sorted. Themis could skip an entire read and write pass by pipelining intermediate data through the reduce function directly if the reduce function was known to be commutative and associative. We chose not to do so to keep Themis's operational model equivalent to the model presented in the original MapReduce paper.</p><p>Characterizing input data in both centralized and distributed contexts has been studied extensively in the database systems community <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b12">13]</ref>, but many of the algorithms studied in this context assume that records have a fixed size and are hence hard to adapt to variablysized, skewed records. Themis's skew mitigation techniques bear strong resemblance to techniques used in MPP sharednothing database systems <ref type="bibr" target="#b8">[9]</ref>.</p><p>The original MapReduce paper <ref type="bibr" target="#b7">[8]</ref> acknowledges the role that imbalance can play on overall performance, which can be affected by data skew. SkewReduce <ref type="bibr" target="#b15">[16]</ref> alleviates the computational skew problem by allowing users to specify a customized cost function on input records. Partitioning across nodes relies on this cost function to optimize the distribution of data to tasks. SkewTune <ref type="bibr" target="#b16">[17]</ref> proposes a more general framework to handle skew transparently, without requiring hints from users. SkewTune is activated when a slot becomes idle in the cluster, and the task with the greatest estimated remaining time is repartitioned to take advantage of that slot. This reallocates the unprocessed input data through range-partitioning, similar to Themis's phase zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">LIMITATIONS AND FUTURE WORK</head><p>Themis's high level of performance is predicated on its ability to tightly control access to its host machine's I/O and memory. As a consequence, it is unclear how Themis would perform when sharing a cluster of machines with other applications. It is possible that some of Themis's features (such as its unified control over disk I/O) might be incorporated into a lower-level service that all processes could share, but we have not explored this approach.</p><p>At present, phase one of Themis's execution is limited by the speed of the slowest node, and is thus negatively affected by stragglers. Since Themis does not split its jobs into tasks, it is harder for it to support traditional methods of straggler mitigation such as speculative execution. Investigating alternate means of straggler mitigation is the subject of ongoing work.</p><p>Our current implementation of Themis assumes that jobs will execute serially. We realize the limitations of this restriction in real-world MapReduce deployments, and are actively developing a version of Themis that allows multiple MapReduce jobs to execute concurrently while being managed by a single instance of Themis's memory and I/O management subsystems, hence maintaining both high performance and the 2-IO property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">CONCLUSIONS</head><p>Many MapReduce jobs are I/O-bound, and so minimizing the number of I/O operations is critical to improving their performance. In this work, we present Themis, a MapReduce implementation that meets the 2-IO property, meaning that it issues the minimum number of I/O operations for jobs large enough to exceed memory. To avoid materializing intermediate results, Themis foregoes task-level fault tolerance, relying instead on job-level fault tolerance. Since the 2-IO property prohibits it from spilling records to disk, Themis must manage memory dynamically and adaptively. To ensure that writes to disk are large, Themis adopts a centralized, per-node disk scheduler that batches records produced by different mappers.</p><p>There exist a large and growing number of clusters that can process petabyte-scale jobs, yet are small enough to experience a qualitatively lower failure rate than warehousescale clusters.</p><p>We argue that these deployments are ideal candidates to adopt more efficient implementations of MapReduce, which result in higher overall performance than more pessimistic implementations. Themis has been able to implement a wide variety of MapReduce jobs at nearly the sequential speed of the underlying storage layer, and is on par with TritonSort's record sorting performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Stages of Phase Two (Sort/Reduce) in Themis</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: A diagrammatic overview of pool-based memory management. Note that memory in each pool is divided into fixed-size regions, and that any memory not allocated to pools cannot be utilized by stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: A diagrammatic overview of quota-based memory management. In this figure, QuotaA provides a memory quota between Stage 1 and Stage 4. Stages 2 and 3 use unmanaged memory created with standard malloc and free syscalls.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance of evaluated MapReduce jobs. Maximum sequential disk throughput of approximately 90 MB/s is shown as a dotted line. Our TritonSort record from 2011 is shown on the left for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Figure 9: Memory quota usage of the Reader Converter stage. The network was made artificially slow in the time period designated by the dashed lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Median write sizes for various Themis jobs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Component-level failure rates observed in a Google data center as reported in</figDesc><table><row><cell>Component</cell><cell>Failure rates</cell></row><row><cell>Node</cell><cell>4.3 months</cell></row><row><cell>Disk</cell><cell>2-4% annualized</cell></row><row><cell>Rack</cell><cell>10.2 years</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>A comparison of Themis's memory allocator implementations.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>A summary of the Themis memory allocator API</figDesc><table><row><cell>Quota A</cell><cell cols="2">Unmanaged Space</cell><cell>Free Space</cell></row><row><cell>Stage 1</cell><cell>Stage 2</cell><cell>Stage 3</cell><cell>Stage 4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>A description and table of abbreviations for the MapReduce jobs evaluated in this section. Data sizes take into account 8 bytes of metadata per record for key and value sizes</figDesc><table><row><cell>Data Size</cell></row></table><note><p>XFS partition is configured with a single allocation group to prevent file fragmentation across allocation groups, and is mounted with the noatime flag set. Each server has two HP P410 drive controllers with 512MB on-board cache, as well as a Myricom 10 Gbps network interface. All nodes are connected to a single Cisco Nexus 5596 datacenter switch. All servers run Linux 2.6.32. Our implementation of Themis is written in C++ and is compiled with g++ 4.6.2.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>. Performance is measured in Performance comparison of Hadoop and Themis.</figDesc><table><row><cell></cell><cell cols="2">Running Time</cell><cell></cell></row><row><cell cols="4">Application Hadoop Themis Improvement</cell></row><row><cell>Sort-500G</cell><cell>28881s</cell><cell>1789s</cell><cell>16.14x</cell></row><row><cell>CloudBurst</cell><cell>2878s</cell><cell>944s</cell><cell>3.05x</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Performance of allocation policies</figDesc><table><row><cell>Mean Allocation Time (µs)</cell><cell>0 10 20 30 40 50 60 70 80 90</cell><cell>Reader 0</cell><cell>Reader 1</cell><cell>Reader 2</cell><cell>Reader 3</cell><cell>Reader 4</cell><cell>Reader 5</cell><cell>Reader 6</cell><cell>Reader 7</cell><cell>Reader Converter 0</cell><cell>Reader Converter 1</cell><cell>Mapper 0</cell><cell>Mapper 1</cell><cell>Mapper 2</cell><cell>Mapper 3</cell><cell>Receiver 0</cell><cell>Receiver Converter 0</cell><cell>Receiver Converter 1</cell><cell>Demux 0 Quota-based Demux 1 Demux 2 Demux 3 Coalescer 0 Constraint-based Coalescer 1</cell></row><row><cell cols="20">Figure 8: Effects of allocation policy on mean allo-</cell></row><row><cell cols="14">cation times across workers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="19">Allocation Policy Phase One Throughput</cell></row><row><cell></cell><cell></cell><cell cols="8">Constraint-Based</cell><cell></cell><cell></cell><cell></cell><cell cols="7">84.90 MBps/disk</cell></row><row><cell></cell><cell></cell><cell cols="7">Quota-Based</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">83.11 MBps/disk</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Themis is a Titan in Greek mythology who is tasked with creating balance, order and harmony.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">ACKNOWLEDGMENTS</head><p>The authors wish to thank Kenneth Yocum for his valuable input, as well as Mehul Shah and Chris Nyberg for their input on Themis's approach to sampling. This work was sponsored in part by NSF Grants CSR-1116079 and MRI CNS-0923523, as well as through donations by Cisco Systems and a NetApp Faculty Fellowship.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A. EXPECTED RUNNING TIME OF A FAILED JOB</head><p>Let T be the job's duration and M T T F be the mean time to failure of the cluster. In our model, failure occurs as a Poisson process. We compute the expected running time of a failed job (denoted TF ) as follows:</p><p>Therefore, if the job duration T is much larger than the MTTF of the cluster (T M T T F ), Equation <ref type="formula">3</ref>implies that TF ≈ M T T F , and we expect the job to fail. On the other hand, if T M T T F , Equation 3 implies that TF ≈ T , and we expect the job to succeed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. EXPECTED COMPLETION TIME OF A THEMIS JOB</head><p>Let p denote the probability of failure in a single Themis job. Let T denote the running time of the job when there are no failures.</p><p>Consider a situation in which the job fails during the first (n -1) trials and completes in the n th trial. The probability of this event is p n-1 (1-p). Note that a successful trial takes time T and a failed trial takes time TF as in Appendix A. To simplify our notation, let α = TF /T be the fraction of its successful runtime the failed job spent running. Then the total running time in this case is (n -1)αT + T = ((n -1)α + 1)T.</p><p>By considering such an event for all possible values of n, we get the expected running time to completion for the job:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Input/Output Complexity of Sorting and Related Problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CACM</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="1988-09">Sept. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Efficiency Matters! In HotStorage</title>
		<author>
			<persName><forename type="first">E</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tucek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Practical System Reliability (pg. 226)</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kimber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Wiley-IEEE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">HaLoop: Efficient Iterative Data Processing on Large Clusters</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balazinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Ernst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Microreboot -A Technique for Cheap Recovery</title>
		<author>
			<persName><forename type="first">G</forename><surname>Candea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kawamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fujiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tenzing: A SQL Implementation On The MapReduce Framework</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Aragonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lychagina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endowment</title>
		<meeting>VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Cloudera</forename><surname>Dell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Platform</forename><surname>Hadoop</surname></persName>
		</author>
		<ptr target="http://www.cloudera.com/company/press-center/releases/dell-and-cloudera-collaborate-to-enable-large-scale-data-analysis-and-modeling-through-open-source/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified Data Processing on Large Clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Parallel Database Systems: The Future of High Performance Database Systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CACM</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1992-06">June 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parallel Sorting on a Shared-Nothing Architecture Using Probabilistic Splitting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Naughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PDIS</title>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Survey of Rollback-Recovery Protocols in Message-Passing Systems</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N M</forename><surname>Elnozahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Alvisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM CSUR</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002-09">Sept. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Availability in Globally Distributed Storage Systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Labelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">I</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stokely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V.-A</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Robust Sketching and Aggregation of Distributed Data Streams</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hadjieleftheriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Byers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kollios</surname></persName>
		</author>
		<idno>2005-011</idno>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Boston University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<ptr target="http://wiki.apache.org/hadoop/PoweredBy" />
		<title level="m">Hadoop PoweredBy Index</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">lakewash combined v2.genes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<ptr target="https://dada.cs.washington.edu/research/projects/db-data-L1_bu/escience_datasets/seq_alignment/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Skew-Resistant Parallel Processing of Feature-Extracting Scientific User-Defined Functions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balazinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rolia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SoCC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SkewTune: Mitigating Skew in MapReduce Applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balazinska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rolia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Logothetis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yocum</surname></persName>
		</author>
		<title level="m">Stateful Bulk Processing for Incremental Analytics. In SoCC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Random Sampling Techniques for Space Efficient Online Computation of Order Statistics of Large Datasets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Manku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Lindsay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data Skeletons: Simultaneous Estimation of Multiple Quantiles for Massive Streaming Datasets with Applications to Density Estimation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Liechty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2007-12">Dec. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">CloudBurst: Highly Sensitive Read Mapping with MapReduce</title>
		<author>
			<persName><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Schatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1363" to="1369" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Eliminating Receive Livelock in an Interrupt-Driven Kernel</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Mogul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOCS</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1997-08">Aug. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Monash</surname></persName>
		</author>
		<ptr target="http://www.dbms2.com/2011/07/06/petabyte-hadoop-clusters/" />
		<title level="m">Petabyte-Scale Hadoop Clusters (Dozens of Them)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Najjar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in the Dataflow Computational Model</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1907" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Subtleties in Tolerating Correlated Failures in Wide-Area Storage Systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seshan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-Scale Incremental Processing Using Distributed Transactions and Notifications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dabek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Failure Trends in a Large Disk Drive Population</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FAST</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Sailfish: A framework for large scale data processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ovsiannikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reeves</surname></persName>
		</author>
		<idno>YL-2012-002</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Yahoo! Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">TritonSort: A Balanced Large-Scale Sorting System</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Conley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fail-Stutter Fault Tolerance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Remzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotOS</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Large-Scale Study of Failures in High-Performance Computing Systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DSN</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding Disk Failure Rates: What Does an MTTF of 1,000,000 Hours Mean to You?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007-10">Oct. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Flux: An Adaptive Partitioning Operator for Continuous Query Systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chung</surname></persName>
		</author>
		<ptr target="http://rulai.cshl.edu/rmap/" />
		<title level="m">The RMAP Software for Short-Read Mapping</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<ptr target="http://sortbenchmark.org/" />
		<title level="m">Sort Benchmark</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Random Sampling with a Reservoir</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Vitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOMS</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1985-03">Mar. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<ptr target="http://wiki.freebase.com/wiki/WEX" />
		<title level="m">Freebase Wikipedia Extraction (WEX)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Apache</forename><surname>Hadoop</surname></persName>
		</author>
		<ptr target="http://hadoop.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Scaling Hadoop to 4000</title>
		<ptr target="http://developer.yahoo.net/blogs/hadoop/2008/09/scaling_hadoop_to_4000_nodes_a.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mccauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving MapReduce Performance in Heterogeneous Environments</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
