<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-03-20">March 20, 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tyna</forename><surname>Eloundou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sam</forename><surname>Manning</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
							<email>pamela@openai.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Rock</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Openai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Openresearch</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-20">March 20, 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2303.10130v1[econ.GN]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that as these models could have notable economic, social, and policy implications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As shown in Figure <ref type="figure">1</ref>, recent years, months, and weeks have seen remarkable progress in the field of generative AI and large language models (LLMs). While the public often associates LLMs with various iterations of the Generative Pre-trained Transformer (GPT), LLMs can be trained using a range of architectures, and are not limited to transformer-based models <ref type="bibr" target="#b39">(Devlin et al., 2019)</ref>. LLMs can process and produce various forms of sequential data, including assembly language, protein sequences and chess games, extending beyond natural language applications alone. In this paper, we use LLMs and GPTs somewhat interchangeably, and specify in our rubric that these should be considered similar to the GPT-family of models available via ChatGPT or the OpenAI Playground (which at the time of labeling included models in the GPT-3.5 family but not in the GPT-4 family). We examine GPTs with text-and code-generating abilities and employ the term "generative AI" to additionally include modalities such as images or audio.</p><p>Our study is motivated less by the progress of these models alone though, and more by the breadth, scale, and capabilities we've seen in the complementary technologies developed around them. The role of complementary technologies remains to be seen, but maximizing the impact of LLMs appears contingent on integrating them with larger systems <ref type="bibr" target="#b22">(Bresnahan, 2019;</ref><ref type="bibr" target="#b8">Agrawal et al., 2021)</ref>. While we focus much of this discussion on the generative capabilities of LLMs, there may be new types of software and machine communication made possible by use of LLMs for other tasks including things like embeddings which make Figure <ref type="figure">1</ref>: To get a sense of how quickly model capabilities are progressing -consider the jump in exam performance between <ref type="bibr">GPT-3.5 and GPT-4 (OpenAI, 2023b)</ref>. it possible to build custom search applications or tasks like summarization and classification where it can be unclear where to draw the distinction over what is or is not generative.</p><p>To contextualize this progression and complement labor impact forecasts of technology, we propose a new rubric for understanding LLM capabilities and their potential effects on jobs. This rubric (A.1) measures the overall exposure of tasks to GPTs, following the spirit of prior work on quantifying exposure to machine learning <ref type="bibr" target="#b30">(Brynjolfsson et al., 2018;</ref><ref type="bibr" target="#b43">Felten et al., 2018;</ref><ref type="bibr" target="#b82">Webb, 2020)</ref>. We define exposure as a proxy for potential economic impact without distinguishing between labor-augmenting or labor-displacing effects. We employ human annotators and GPT-4 itself as a classifier to apply this rubric to occupational data in the U.S. economy, primarily sourced from the O*NET database.</p><p>To construct our primary exposure dataset, we collected both human annotations and GPT-4 classifications, using a prompt tuned for agreement with a sample of labels from the authors. We observe similar agreement levels in GPT-4 responses and between human and machine evaluations, when aggregated to the task level. This measure reflects an estimate of the technical capacity to make human labor more efficient; however, social, economic, regulatory, or other determinants imply that technical feasibility does not guarantee labor productivity or automation outcomes. Our analysis indicates that approximately 19% of jobs have at least 50% of their tasks exposed when considering both current model capabilities and anticipated tools built upon them. Human assessments suggest that only 3% of U.S. workers have over half of their tasks exposed to GPT when considering existing language and code capabilities without additional software or modalities. Accounting for other generative models and complementary technologies, our human estimates indicate that This is distinct from recent social science research that makes use of advanced language models to simulate human behavior <ref type="bibr" target="#b50">(Horton, 2023;</ref><ref type="bibr" target="#b78">Sorensen et al., 2022)</ref> While our exposure rubric does not necessarily tie the concept of language models to any particular model, we were strongly motivated by our observed capabilities of GPT-4 and the suite of capabilities we saw in development with OpenAI's launch partners <ref type="bibr" target="#b67">(OpenAI, 2023b)</ref>. up to 49% of workers could have half or more of their tasks exposed to LLMs.</p><p>Our findings consistently show across both human and GPT-4 annotations that most occupations exhibit some degree of exposure to LLMs, with varying exposure levels across different types of work. Occupations with higher wages generally present with high exposure, a result contrary to similar evaluations of overall machine learning exposure <ref type="bibr" target="#b28">(Brynjolfsson et al., 2023)</ref>. When regressing exposure measures on skillsets using O*NET's skill rubric, we discover that roles heavily reliant on science and critical thinking skills show a negative correlation with exposure, while programming and writing skills are positively associated with LLM exposure. Following <ref type="bibr" target="#b10">Autor et al. (2022a)</ref>, we examine barriers to entry by "job zones" and find that occupational exposure to LLMs weakly increases with the difficulty of job preparation. In other words, workers facing higher (lower) barriers to entry in their jobs tend to experience more (less) exposure to LLMs.</p><p>We further compare our measurements to previous efforts documenting the distribution of automation exposure in the economy and find broadly consistent results. Most other technology exposure measures we examine are statistically significantly correlated with our preferred exposure measure, while measures of manual routineness and robotics exposure show negative correlations. The variance explained by these earlier efforts <ref type="bibr">(Acemoglu and Autor, 2011a;</ref><ref type="bibr" target="#b45">Frey and Osborne, 2017;</ref><ref type="bibr" target="#b30">Brynjolfsson et al., 2018;</ref><ref type="bibr" target="#b43">Felten et al., 2018;</ref><ref type="bibr" target="#b82">Webb, 2020;</ref><ref type="bibr" target="#b28">Brynjolfsson et al., 2023)</ref>, along with wage controls, ranges from 60 to 72%, indicating that 28 to 40% of the variation in our AI exposure measure remains unaccounted for by previous technology exposure measurements.</p><p>We analyze exposure by industry and discover that information processing industries (4-digit NAICS) exhibit high exposure, while manufacturing, agriculture, and mining demonstrate lower exposure. The connection between productivity growth in the past decade and overall GPT exposure appears weak, suggesting a potential optimistic case that future productivity gains from LLMs may not exacerbate possible cost disease effects <ref type="bibr" target="#b16">(Baumol, 2012)</ref>.</p><p>Our analysis indicates that the impacts of LLMs like GPT-4, are likely to be pervasive. While LLMs have consistently improved in capabilities over time, their growing economic effect is expected to persist and increase even if we halt the development of new capabilities today. We also find that the potential impact of LLMs expands significantly when we take into account the development of complementary technologies. Collectively, these characteristics imply that Generative Pre-trained Transformers (GPTs) are general-purpose technologies (GPTs). <ref type="bibr" target="#b26">(Bresnahan and Trajtenberg, 1995;</ref><ref type="bibr" target="#b59">Lipsey et al., 2005)</ref>. <ref type="bibr" target="#b46">(Goldfarb et al., 2023)</ref> argue that machine learning as a broad category is likely a general-purpose technology. Our evidence supports a wider impact, as even subsets of machine learning software meet the criteria for general-purpose technology status independently. This paper's primary contributions are to provide a set of measurements of LLM impact potential and to demonstrate the use case of applying LLMs to develop such measurements efficiently and at scale. Additionally, we showcase the general-purpose potential of LLMs. If "GPTs are GPTs," the eventual trajectory of LLM development and application may be challenging for policymakers to predict and regulate. As with other general-purpose technologies, much of these algorithms' potential will emerge across a broad range of economically valuable use cases, including the creation of new types of work <ref type="bibr" target="#b4">(Acemoglu and Restrepo, 2018;</ref><ref type="bibr" target="#b10">Autor et al., 2022a)</ref> Our research serves to measure what is technically feasible now, but necessarily will miss the evolving impact potential of the LLMs over time.</p><p>The paper is structured as follows: Section 2 reviews relevant prior work, Section 3 discusses methods and data collection, Section 4 presents summary statistics and results, Section 5 relates our measurements to earlier efforts, Section 6 explores results, and Section 7 offers concluding remarks.</p><p>Baumol's cost disease is a theory that explains why the cost of labor-intensive services, such as healthcare and education, increases over time. This happens because wages for skilled workers in other industries increase, but there is no corresponding increase in productivity or efficiency in these service industries. Therefore, the cost of labor in these industries becomes relatively more expensive compared to other goods and services in the economy.</p><p>For the remainder of the paper, we use GPT to refer to large language models generally as exemplified by those available via OpenAI, and we spell out general-purpose technologies when it is used outside of stating "GPTs are GPTs."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Literature Review</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Advancement of Large Language Models</head><p>In recent years, large language models (LLMs) have risen to prominence in the field of artificial intelligence (AI) research, showcasing their ability to tackle a wide array of complex language-based tasks. This progress has been fueled by multiple factors, including increased model parameter count, greater training data volume, and enhanced training configurations <ref type="bibr" target="#b27">(Brown et al., 2020;</ref><ref type="bibr" target="#b70">Radford et al., 2019;</ref><ref type="bibr" target="#b49">Hernandez et al., 2021;</ref><ref type="bibr" target="#b52">Kaplan et al., 2020)</ref>. Broad, state-of-the-art LLMs, such as LaMDA <ref type="bibr" target="#b79">(Thoppilan et al., 2022)</ref> and <ref type="bibr">GPT-4 (OpenAI, 2023b)</ref>, excel in diverse applications like translation, classification, creative writing, and code generation-capabilities that previously demanded specialized, task-specific models developed by expert engineers using domain-specific data.</p><p>Concurrently, researchers have improved the steerability, reliability, and utility of these models using methods like fine-tuning and reinforcement learning with human feedback <ref type="bibr" target="#b68">(Ouyang et al., 2022;</ref><ref type="bibr" target="#b15">Bai et al., 2022)</ref>. These advancements enhance the models' ability to discern user intent, rendering them more user-friendly and practical. Moreover, recent studies reveal the potential of LLMs to program and control other digital tools, such as APIs, search engines, and even other generative AI systems <ref type="bibr" target="#b73">(Schick et al., 2023;</ref><ref type="bibr" target="#b61">Mialon et al., 2023;</ref><ref type="bibr" target="#b32">Chase, 2022)</ref>. This enables seamless integration of individual components for better utility, performance, and generalization. In the long run, these trends suggest that LLMs may be capable of executing any task typically performed at a computer.</p><p>For the most part, generative AI models have predominantly been deployed as modular specialists, carrying out specific tasks, like generating images from captions or transcribing text from speech. However, we argue that it is essential to adopt a broader perspective, recognizing LLMs as crucial building blocks for additional tools. While constructing these tools and integrating them into comprehensive systems will take time and necessitate significant reconfiguration of existing processes across the economy, we already observe emerging adoption trends. Despite their limitations, LLMs are becoming increasingly integrated into specialized applications in areas such as writing assistance, coding, and legal research, paving the way for businesses and individuals to adopt GPTs more widely.</p><p>We emphasize the significance of these complementary technologies, partly because out-of-the-box general-purpose GPTs may continue to be unreliable for various tasks due to issues such as factual inaccuracies, inherent biases, privacy concerns, and disinformation risks <ref type="bibr" target="#b0">(Abid et al., 2021;</ref><ref type="bibr" target="#b74">Schramowski et al., 2022;</ref><ref type="bibr" target="#b47">Goldstein et al., 2023;</ref><ref type="bibr" target="#b66">OpenAI, 2023a)</ref>. However, specialized workflows-including tooling, software, or human-in-the-loop systems-can help address these shortcomings by incorporating domain-specific expertise. For example, Casetext offers LLM-based legal research tools that provide lawyers with quicker and more accurate legal research results, utilizing embeddings and summarization to counter the risk that GPT-4 provides innacurate details about a legal case or set of documents. GitHub Copilot is a coding assistant that employs LLMs to generate code snippets and autocomplete code, which users can then accept or reject based on their expertise. In other words, while it's true that on its own GPT-4 does not "know what time it is," it's easy enough to give it a watch.</p><p>Furthermore, a positive feedback loop may emerge as LLMs surpass a specific performance threshold, allowing them to assist in building the very tooling that enhances their usefulness and usability across various contexts. This could lower the cost and engineering expertise required to create such tools, potentially accelerating LLM adoption and integration even further. <ref type="bibr" target="#b33">(Chen et al., 2021;</ref><ref type="bibr" target="#b69">Peng et al., 2023)</ref> LLMs can also become valuable assets in machine learning model development-serving as coding assistants for researchers, data labeling services, or synthetic data generators. There is potential for such models to contribute to economic decision-making at the task level, for instance, by refining methods for task and sub-task allocation between humans and machines <ref type="bibr" target="#b76">(Singla et al., 2015;</ref><ref type="bibr" target="#b75">Shahaf and Horvitz, 2010)</ref>. As LLMs improve over time and better align with user preferences, we can anticipate a continuous enhancement in performance.</p><p>However, it is essential to recognize that these trends also bring a variety of serious risks. <ref type="bibr" target="#b54">(Khlaaf et al., 2022;</ref><ref type="bibr" target="#b84">Weidinger et al., 2022;</ref><ref type="bibr" target="#b77">Solaiman et al., 2019)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Economic Impacts of Automation Technologies</head><p>A large and growing body of literature addresses the labor market impacts of artificial intelligence and automation technologies broadly defined. The concept of skill-biased technological change and the task model of automation-often considered the standard framework for understanding technology's influence on labor-originated from research demonstrating that technological progress raises the demand for skilled workers over unskilled workers <ref type="bibr" target="#b53">(Katz and Murphy, 1992)</ref>. Numerous studies have built upon this concept, exploring the effects of technological change and automation on workers within a task-based framework <ref type="bibr" target="#b13">(Autor et al., 2003;</ref><ref type="bibr" target="#b2">Acemoglu and Autor, 2011b;</ref><ref type="bibr" target="#b4">Acemoglu and Restrepo, 2018)</ref>. This strand of research has shown that workers involved in routine and repetitive tasks are at a higher risk of technology-driven displacement, a phenomenon known as routine-biased technological change. More recent studies have distinguished between technology's task-displacement and task-reinstatement effects (where new technology increases the need for a wider array of labor-intensive tasks) <ref type="bibr" target="#b1">(Acemoglu and</ref><ref type="bibr">Restrepo, 2018, 2019)</ref>. Several studies have shown that automation technologies have resulted in wage inequality in the US, driven by relative wage declines for workers specializing in routine tasks <ref type="bibr" target="#b12">(Autor et al., 2006;</ref><ref type="bibr" target="#b81">Van Reenen, 2011;</ref><ref type="bibr" target="#b7">Acemoglu and Restrepo, 2022b)</ref>.</p><p>Prior research has employed various approaches to estimate the overlap between AI capabilities and the tasks and activities workers undertake in different occupations. These methods include mapping patent descriptions to worker task descriptions <ref type="bibr" target="#b82">(Webb, 2020;</ref><ref type="bibr" target="#b60">Meindl et al., 2021)</ref>, linking AI capabilities to occupational abilities documented in the O*NET database <ref type="bibr" target="#b43">(Felten et al., 2018</ref><ref type="bibr" target="#b42">(Felten et al., , 2023))</ref>, aligning AI task benchmark evaluations with worker tasks via cognitive abilities <ref type="bibr" target="#b80">(Tolan et al., 2021)</ref>, labeling automation potential for a subset of US occupations and using machine learning classifiers to estimate this potential for all other US occupations <ref type="bibr" target="#b45">(Frey and Osborne, 2017)</ref>, modeling task-level automation and aggregating the results to occupation-level insights <ref type="bibr" target="#b9">(Arntz et al., 2017)</ref>, expert forecasts <ref type="bibr" target="#b48">(Grace et al., 2018)</ref>, and most relevantly to this paper, devising a new rubric to assess worker activities for their suitability for machine learning <ref type="bibr" target="#b30">(Brynjolfsson et al., 2018</ref><ref type="bibr" target="#b28">(Brynjolfsson et al., , 2023))</ref>. Some of these approaches have found exposure to AI technologies at the task-level tends to be diversified within occupation. Considering each job as a bundle of tasks, it would be rare to find any occupation for which AI tools could do nearly all of the work. <ref type="bibr" target="#b10">(Autor et al., 2022a)</ref> finds as well that automation and augmentation exposures tend to be positively correlated. There is also a growing set of studies examining specific economic impacts and opportunities for LLMs <ref type="bibr" target="#b21">(Bommasani et al., 2021;</ref><ref type="bibr" target="#b42">Felten et al., 2023;</ref><ref type="bibr" target="#b57">Korinek, 2023;</ref><ref type="bibr" target="#b63">Mollick and Mollick, 2022;</ref><ref type="bibr" target="#b64">Noy and Zhang, 2023;</ref><ref type="bibr" target="#b69">Peng et al., 2023)</ref>. Alongside this work, our measurements help characterize the broader potential relevance of language models to the labor market.</p><p>General-purpose technologies (e.g. printing, the steam engine) (GPTs) are characterized by widespread proliferation, continuous improvement, and the generation of complementary innovations <ref type="bibr" target="#b26">(Bresnahan and Trajtenberg, 1995;</ref><ref type="bibr" target="#b59">Lipsey et al., 2005)</ref>. Their far-reaching consequences, which unfold over decades, are difficult to anticipate, particularly in relation to labor demand <ref type="bibr" target="#b18">(Bessen, 2018;</ref><ref type="bibr" target="#b58">Korinek and Stiglitz, 2018;</ref><ref type="bibr" target="#b3">Acemoglu et al., 2020;</ref><ref type="bibr" target="#b17">Benzell et al., 2021)</ref>. The realization of general purpose technologies' full potential requires extensive co-invention <ref type="bibr" target="#b26">(Bresnahan and Trajtenberg, 1995;</ref><ref type="bibr" target="#b23">Bresnahan et al., 1996</ref><ref type="bibr" target="#b25">Bresnahan et al., , 2002;;</ref><ref type="bibr" target="#b59">Lipsey et al., 2005;</ref><ref type="bibr" target="#b40">Dixon et al., 2021)</ref>, a costly and time-consuming process involving the discovery of new business procedures <ref type="bibr" target="#b38">(David, 1990;</ref><ref type="bibr" target="#b24">Bresnahan, 1999;</ref><ref type="bibr" target="#b44">Frey, 2019;</ref><ref type="bibr" target="#b31">Brynjolfsson et al., 2021;</ref><ref type="bibr" target="#b41">Feigenbaum and Gross, 2021)</ref>. Consequently, many studies of machine learning technologies focus on systems-level adoption, arguing that organizational systems may require redesign to effectively take advantage of novel machine learning advancements <ref type="bibr" target="#b22">(Bresnahan, 2019;</ref><ref type="bibr" target="#b8">Agrawal et al., 2021;</ref><ref type="bibr" target="#b46">Goldfarb et al., 2023)</ref>. Appropriately designed systems can yield considerable business value and improve firm performance <ref type="bibr" target="#b72">(Rock, 2019;</ref><ref type="bibr" target="#b14">Babina et al., 2021;</ref><ref type="bibr" target="#b85">Zolas et al., 2021)</ref>, with AI tools facilitating the discovery process <ref type="bibr" target="#b36">(Cockburn et al., 2018;</ref><ref type="bibr" target="#b34">Cheng et al., 2022)</ref>. By employing task-level information to assess whether LLMs fulfill GPT criteria, we seek to merge the two perspectives for understanding the technology-labor relationship.</p><p>We attempt to build on these diverse literature streams in several ways. Echoing <ref type="bibr" target="#b42">(Felten et al., 2023)</ref>, we focus our analysis on the impact of LLMs, rather than addressing machine learning or automation technologies more broadly. Additionally, we propose a novel method that employs LLMs, specifically GPT-4, to assess tasks for exposure and automation potential, thereby bolstering human scoring efforts. Subsequently, we aggregate our findings to occupations and industries, capturing the overall potential exposure in the contemporary U.S. labor market.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods and Data Collection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data on Activities and Tasks Performed by Occupation in the US</head><p>We use the O*NET 27.2 database (O*NET, 2023), which contains information on 1,016 occupations, including their respective Detailed Work Activities (DWAs) and tasks. A DWA is a comprehensive action that is part of completing task, such as "Study scripts to determine project requirements." A task, on the other hand, is an occupation-specific unit of work that may be associated with none, one, or multiple DWAs. We offer a sample of tasks and DWAs in Table <ref type="table" target="#tab_0">1</ref>. The two datasets we use consist of:</p><p>? 19,265 tasks, where each task featuring a "task description" and a corresponding occupation, and with most tasks associated with one or more DWAs</p><p>? 2,087 DWAs, where most DWAs are connected to one or more tasks, and tasks may be associated with one or more DWAs, though some tasks lack any associated DWAs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data on Wages, Employment, and Demographics</head><p>We obtain employment and wage data from the 2020 and 2021 Occupational Employment series provided by the Bureau of Labor Statistics. This dataset encompasses occupational titles, the number of workers in each occupation, and occupation-level employment projections for 2031, typical education required for entry in an occupation and on-the-job training required to attain competency in an occupation <ref type="bibr" target="#b19">(BLS, 2022)</ref>. We use the BLS-recommended crosswalk to O*NET (BLS, 2023b) to link the O*NET task and DWA dataset and the BLS Labor Force Demographics (BLS, 2023a), which is derived from the Current Population Survey (CPS). Both of these data sources are collected by the U.S. government and primarily capture workers who are not self-employed, are documented, and are working in the so-called formal economy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Exposure</head><p>We present our results based on an exposure rubric, in which we define exposure as a measure of whether access to a GPT or GPT-powered system would reduce the time required for a human to perform a specific DWA or complete a task by at least 50 percent. We provide a summary of our rubric below, while the complete rubric can be found in A.1. When we have labels for DWAs, we first aggregate at the task level before aggregating at the occupation level.</p><p>No exposure (E0) if:</p><p>? there is no or minimal reduction in the time required to complete the activity or task while maintaining equivalent quality or ? using any combination of the capabilities described in accordance with the below criteria would decrease the quality of the activity/task output. Direct exposure (E1) if:</p><p>? using solely the theoretical LLM or GPT-4 described via ChatGPT or the OpenAI playground can decrease the time required to complete the DWA or task by at least half (50%). LLM+ Exposed (E2) if:</p><p>? access to the LLM alone would not reduce the time required to complete the activity/task by at least half, but ? additional software could be developed on to the LLM that could reduce the time it takes to complete the specific activity/task with quality by at least half. Among these systems, we count access to image generation systems. a a In practice, as can be seen in the full rubric in A.1, we categorize access to image capabilities separately (E3) to facilitate annotation, though we combine E2 and E3 for all analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exposure overview</head><p>We set the exposure threshold at a potential 50 % reduction in time required to complete a specific DWA or task while maintaining consistent quality. We anticipate that adoption will be highest and most immediate for applications that realize a considerable increase in productivity. Although this threshold is somewhat arbitrary, it was selected for ease of interpretation by annotators.</p><p>We then collected both human and GPT-4-generated annotations using the exposure rubric, which underlie the bulk of the analyses in this paper.</p><p>? Human Ratings: We obtained human annotations by applying the rubric to each O*NET Detailed Worker Activity (DWA) and a subset of all O*NET tasks and then aggregated those DWA and task Moreover, regardless of the chosen threshold, we guessed that the real-world reduction in task time would likely be slightly or significantly lower than our estimates, leading us to opt for a relatively high threshold. In our own validation labeling, we found that this corresponded closely to whether GPT or GPT-powered applications could perform the core part of a task or nearly the entire task. scores at the task and occupation levels. To ensure the quality of these annotations, the authors personally labeled a large sample of tasks and DWAs and enlisted experienced human annotators who have extensively reviewed GPT outputs as part of OpenAI's alignment work <ref type="bibr" target="#b68">(Ouyang et al., 2022)</ref>.</p><p>? GPT-4 Ratings: We administered a similar rubric to an early version of <ref type="bibr">GPT-4 (OpenAI, 2023b)</ref> but on all task/occupation pairs rather than DWAs. We made slight modifications to the rubric (which was used as a "prompt" to the model in this case) to enhance agreement with a set of human labels. Full agreement rates are given in Table <ref type="table" target="#tab_1">2</ref>.</p><p>We construct three primary measures for our dependent variable of interest: (i) ?, corresponding to E1 in the exposure rubric above, anticipated to represent the lower bound of the proportion of exposed tasks within an occupation, (ii) ?, which is the sum of E1 and 0.5*E2, where the 0.5 weight on E2 is intended to account for exposure when deploying the technology via complementary tools and applications necessitates additional investment, and (iii) ?, the sum of E1 and E2, an upper bound of exposure that provides an assessment of maximal exposure to GPT and GPT-powered software. We summarize agreement between annotation groups and measures in Table <ref type="table" target="#tab_1">2</ref>. For the remainder of the analysis, if not specified, the reader may assume that we refer to ? exposure -meaning all tasks directly exposed via tools like ChatGPT or the OpenAI Playground are considered twice as exposed as tasks requiring some complementary innovation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Limitations of our methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Subjective human judgments</head><p>A fundamental limitation of our approach lies in the subjectivity of the labeling. In our study, we employ annotators who are familiar with the GPT models' capabilities. However, this group is not occupationally diverse, potentially leading to biased judgments regarding GPTs' reliability and effectiveness in performing tasks within unfamiliar occupations. We acknowledge that obtaining high-quality labels for each task in an occupation requires workers engaged in those occupations or, at a minimum, possessing in-depth knowledge</p><p>The authors annotated DWAs that clearly required a high degree of physicality or manual dexterity, and the contracted annotators labeled the remaining activities, along with a subset of tasks including those without associated DWAs and those for which there was no clear task-level annotation after aggregating the DWA annotations.</p><p>Despite recent advances of multimodal GPT models (OpenAI, 2023b), vision capabilities were not included in the assessment of ? exposure.</p><p>Figure <ref type="figure">2</ref>: Human raters (x-axis) and GPT-4 ratings (y-axis) show a high degree of agreement about GPT exposure by occupation. Near the highest levels of exposure following the ? method of aggregating exposure scores to occupations, GPT-4 ratings tend to be lower than Human ratings. We present the raw scatter and the binscatter. Near the top end of exposure ratings, humans are on average more likely to rate an occupation as exposed.</p><p>of the diverse tasks within those occupations. This represents an important area for future work in validating these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Measuring GPTs with GPT-4</head><p>Recent research indicates that GPT-4 serves as an effective discriminator, capable of applying intricate taxonomies and responding to changes in wording and emphasis. (OpenAI, 2023b) The outcomes of GPT-4 task classification are sensitive to alterations in the rubric's wording, the prompt's order and composition, the presence or absence of specific examples in the rubric, the level of detail provided, and key term definitions. Iterating on the prompt, based on observed outcomes in a small validation set, can enhance the agreement between model outputs and the rubric's intent. Consequently, there are slight differences between the rubric presented to humans and the one used for GPT-4. This decision was made deliberately to guide the model towards reasonable labels without excessively influencing human annotators. As a result, we use multiple annotation sources, but none should be considered the definitive ground truth relative to the others. In the analysis, we will present results from human annotators as our primary results. Further improvement and innovation in crafting effective rubrics for LLM classification remain possible. Still, we observe a high degree of agreement between human ratings and GPT-4 ratings at the occupation level concerning overall exposure to GPT systems (see Table <ref type="table" target="#tab_1">2</ref>, Figure <ref type="figure">??</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Additional Weaknesses</head><p>? Validity of task-based framework. It is unclear to what extent occupations can be entirely broken down into tasks, and whether this approach systemtically omits certain categories of skills or tasks that are tacitly required for competent performance of a job. Additionally, tasks can be composed of sub-tasks, some of which are more automatable than others. Some tasks may function as pre-cursor to other tasks, such that the completion of downstream tasks is dependent on precursor tasks. If indeed, the task-based breakdown is not a valid representation of how most work in an occupation is performed, our exposure analysis would largely be invalidated.</p><p>? Relative vs. absolute measures. It is likely best to interpret these measures as relative measures, e.g. an occupation with an estimated 0.6 exposure should likely be interpreted as just much more exposed than one with 0.1 exposure.</p><p>? Lack of expertise and task interpretation. Human annotators were mostly unaware of the specific occupations mapped to each DWA during the labeling process. This led to unclear logic for aggregating tasks and occupations, as well as some evident discrepancies in labels, demonstrated in Table <ref type="table" target="#tab_0">1</ref>. We experimented with various aggregation methods and discovered that even with a maximum-matching approach (taking the matching human&lt;&gt;model label if one existed), the agreement remained relatively consistent. Ultimately, we collected additional labels for task/occupation pairs where there was significant disagreement.</p><p>? Forward-looking and subject to change, with some early evidence. Accurately predicting future LLM applications remains a significant challenge, even for experts <ref type="bibr" target="#b67">(OpenAI, 2023b)</ref>. Emergent capabilities, human perception biases, and technological development shifts can all affect the accuracy and reliability of predictions regarding LLMs' potential impact on worker tasks. Our projections are inherently forward-looking and based on current trends, evidence, and perceptions of technological possibilities. As a result, they may change as new advancements arise in the field. For example, some tasks that seem unlikely for LLMs to impact today might change with the introduction of new model capabilities. Conversely, tasks that appear exposed might face unforeseen challenges limiting language model applications.</p><p>? Sources of disagreement. While we did not rigorously examine sources of disagreement, we found a few places where humans and the model tended to get "stuck" in their assessments:</p><p>-Tasks or activities where while an LLM could theoretically help or accomplish the task, adopting it to do so would require multiple people to change their habits or expectations (e.g. meetings, negotiations)</p><p>-Tasks or activities where there is currently some regulation that requires human oversight or norm that suggests human judgment or empathy (e.g. making decisions, counseling), and -Tasks or activities where there already exists a technology that can reasonably automate the task (e.g. making reservations).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>General-purpose technologies are relatively rare and characterized by their pervasiveness, improvement over time, and the development of significant co-invention and spillovers <ref type="bibr" target="#b59">(Lipsey et al., 2005)</ref>. Our assessment of GPTs' (Generative Pre-trained Transformers) impact on the labor market is limited since it does not consider total factor productivity or capital input potential. In addition to their influence on labor, GPTs may also influence these dimensions. At this stage, certain GPT criteria are easier to evaluate than others. For example, assessing the long-term impact of these models' capabilities and the growth of complementary applications and systems is more feasible in the long run. Our primary focus at this early stage is to test the hypothesis that GPT language models have a pervasive influence on the economy, similar to <ref type="bibr" target="#b46">(Goldfarb et al., 2023)</ref>'s analysis of machine learning diffusion through job postings to assess machine learning's GPT potential as an algorithmic category. Rather than using job postings or studying machine learning in general, examining the task evaluation approach with both human and GPT annotations may reveal whether GPT impacts are limited to a small set of similar tasks or occupations.</p><p>Our findings suggest that, based on their task-level capabilities, GPTs have the potential to significantly affect a diverse range of occupations within the U.S. economy, demonstrating a key attribute of general-purpose technologies. In the following sections, we discuss results across various roles and wage structures. Additional results on the relative exposure of industries within the U.S. economy can be found in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Summary Statistics</head><p>Summary statistics for these measures can be found in Table <ref type="table" target="#tab_2">3</ref>. Both human and GPT-4 annotations indicate that average occupation-level ? values fall between 0.14 and 0.15, suggesting that, for the median occupation, approximately 15% of tasks are directly exposed to GPTs. This figure increases to over 30% for ? and surpasses 50% for ?. Coincidentally, human and GPT-4 annotations also tag between 15% and 14% of total tasks in the dataset as being exposed to GPTs.</p><p>Based on the ? values, we estimate that 80% of workers belong to an occupation with at least one task exposed to GPTs, while 19% of workers are in an occupation where over half the tasks are labeled as exposed.</p><p>Although the potential for tasks to be affected is extensive, GPTs must be incorporated into broader systems to realize this potential fully. As is common with general-purpose technologies, such co-invention barriers may impede the rapid diffusion of GPTs into economic applications. Additionally, predicting the need for human oversight is challenging, especially for tasks where model capabilities equal or surpass human levels. While the requirement for human supervision may initially slow down the adoption and diffusion rate, users of GPTs and GPT-powered systems are likely to become increasingly acquainted with the technology over time, particularly in terms of understanding when and how to trust its outputs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Wages and Employment</head><p>In Figure <ref type="figure">3</ref>, we present the exposure intensity across the economy. The first plot displays exposure in terms of total workers, while the second plot shows exposure in terms of total occupations. Each point on the graph represents the estimated percentage of workers (and occupations) on the y-axis with an exposure level (?, ?, and ?) indicated on the x-axis. For example, human annotators determined that 2.4% of workers are ? 50 -exposed, 18.6% are ? 50 -exposed, and 49.6% are ? 50 -exposed, where the threshold of 50% comes from the x-axis and the percentage of workers comes from the y axis in the right plot of Figure <ref type="figure">2</ref>. At any given point on the x-axis, the vertical distance between the ? and the ? represents the exposure potential attributable to tools and applications beyond direct exposure to GPTs. The distribution of exposure is similar for both workers and Figure <ref type="figure">3</ref>: Exposure intensity across the economy, displayed on the left in terms of percent of affected occupations and on the right as percent of affected workers. The distribution of exposure is similar across occupations and across workers, suggesting that worker concentration in occupations is not highly correlated with occupational exposure to GPTs or GPT-powered software. We do however expect that it could be more highly correlated with investment in developing GPT-powered software for particular domains.</p><p>occupations, suggesting that worker concentration in occupations does not have a strong correlation with occupational exposure to GPT or GPT-powered software.</p><p>Aggregated at the occupation level, human and GPT-4 annotations exhibit qualitative similarities and tend to correlate, as demonstrated in Figure <ref type="figure">4</ref>. Human annotations estimate marginally lower exposure for high-wage occupations compared to GPT-4 annotations. While there are numerous low-wage occupations with high exposure and high-wage occupations with low exposure, the overall trend in the binscatter plot reveals that higher wages are associated with increased exposure to GPT.</p><p>The potential exposure to GPTs seems to have little correlation with current employment levels. In Figure <ref type="figure">4</ref>, both human and GPT-4 ratings of overall exposure are aggregated to the occupation-level (y-axis) and compared with the log of total employment (x-axis). Neither plot reveals significant differences in GPT exposure across varying employment levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Skill Importance</head><p>In this section, we investigate the relationship between the importance of a skill for an occupation (as annotated in the O*NET dataset) and our exposure measures. We first take the Basic Skills provided by O*NET (skill definitions can be found in Appendix B) and normalize the measure of skill importance for each occupation to enhance interpretability. Then we perform a regression analysis on our exposure measures (?, ?, ?) to examine the strength of associations between skill importance and exposure.</p><p>Our findings indicate that the importance of science and critical thinking skills are strongly negatively associated with exposure, suggesting that occupations requiring these skills are less likely to be impacted by current language models. Conversely, programming and writing skills show a strong positive association with exposure, implying that occupations involving these skills are more susceptible to being influenced by language models (see Table <ref type="table" target="#tab_4">5</ref> for detailed results).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Barriers to Entry</head><p>Next, we examine barriers to entry to better understand if there is differentiation in exposure due to types of jobs. One such proxy is an O*NET occupation-level descriptor called the "Job Zone." A job zone groups occupations that are similar in (a) the level of education needed to get a job in the occupation, (b) the amount Figure <ref type="figure">4</ref>: The binscatter plots depict the exposure to language models (LLMs) in various occupations, as assessed by both human evaluators and GPT-4. These plots compare the exposure to GPT (?) at the occupation level against the log of total employment within an occupation and log of the median annual wage for occupations. While some discrepancies exist, both human and GPT-4 assessments indicate that higher wage occupations tend to be more exposed to LLMs. Additionally, numerous lower wage occupations demonstrate high exposure based on our rubrics. Core tasks receive twice the weight of supplemental tasks within occupations when calculating average exposure scores. Employment and wage data are sourced from the BLS-OES survey conducted in May 2021. In the ONET database, there are 5 Job Zones, with Job Zone 1 requiring the least amount of preparation (3 months) and Job Zone 5 requiring the most extensive amount of preparation, 4 or more years. We observe that median income increases monotonically across job zones as the level of preparation needed also increases, with the median worker in Job Zone 1 earning $30, 230 and the median worker in Job Zone 5 earning $80, 980.</p><p>All of our measures (?, ?, and ?) show an identical pattern, that is, exposure increases from Job Zone 1 to Job Zone 4, and either remains similar or decreases at Job Zone 5. Similar to Figure <ref type="figure">3</ref> in 5, we plot the percentage of workers at every threshold of exposure. We find that, on average, the percentage of workers in occupations with greater than 50% ? exposure in Job Zones 1 through 5 have ? at 0.00% (Job Zone 1), 6.11% (Job Zone 2), 10.57% (Job Zone 3), 34.5% (Job Zone 4), and 26.45% (Job Zone 5), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Typical Education Needed for Entry</head><p>Since inclusion in a job zone accounts for both the education required-which itself is a proxy for skill acquisition-and the preparation required, we seek data to disentangle these variables. We use two variables from the Bureau of Labor Statistics' Occupational data: "Typical Education Needed for Entry" and "On-the-job Training Required to Attain Competency" in an occupation. By examining these factors, we aim to uncover trends with potential implications for the workforce. There are 3,504,000 workers for whom we lack data on education and on-the-job training requirements, and they are therefore excluded from the summary tables.</p><p>Our analysis suggests that individuals holding Bachelor's, Master's, and professional degrees are more exposed to GPTs and GPT-powered software than those without formal educational credentials (see Table <ref type="table" target="#tab_6">7</ref>). Interestingly, we also find that individuals with some college education but no degree exhibit a high level of exposure to GPTs and GPT-powered software. Upon examining the table displaying barriers to entry, we observe that the jobs with the least exposure require the longest training, potentially offering a lower payoff (in terms of median income) once competency is achieved. Conversely, jobs with no on-the-job training required or only internship/residency required appear to yield higher income but are more exposed to GPT. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Validation of Measures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison to Earlier Efforts</head><p>This paper aims to build on a number of previous empirical studies examining the occupational exposure to advances in AI and/or automation. Previous studies have used a variety of methods, including:</p><p>? Using occupational taxonomies like O*NET to characterize which occupations have routine vs. non-routine and manual vs. cognitive task content <ref type="bibr" target="#b13">(Autor et al., 2003;</ref><ref type="bibr">Acemoglu and Autor, 2011a)</ref>.</p><p>? Mapping text descriptions of tasks to descriptions of technological advances in patents. <ref type="bibr" target="#b56">(Kogan et al., 2021;</ref><ref type="bibr" target="#b82">Webb, 2020)</ref> ? Linking capabilities of AI systems to occupational abilities and aggregating exposure estimates to the occupations where those abilities are required. <ref type="bibr" target="#b43">(Felten et al., 2018</ref><ref type="bibr" target="#b42">(Felten et al., , 2023) )</ref> ? Mapping the results of AI task benchmark evaluations (ImageNet, Robocup, etc.) to 59 worker tasks through a set of 14 cognitive abilities drawn from the cognitive science literature. <ref type="bibr" target="#b80">(Tolan et al., 2021)</ref> ? Expert labeling of automation potential for a set of O*NET occupations where experts had high confidence, combined with a probabilistic classifier to estimate automation potential for the remainder of O*NET occupations. <ref type="bibr" target="#b45">(Frey and Osborne, 2017)</ref> ? Developing a rubric for evaluating the "suitability for machine learning" (SML) of activities that workers are completing in the economy <ref type="bibr" target="#b29">(Brynjolfsson and Mitchell, 2017;</ref><ref type="bibr" target="#b30">Brynjolfsson et al., 2018</ref><ref type="bibr" target="#b28">Brynjolfsson et al., , 2023))</ref>.</p><p>We provide a set of summary statistics on many of these prior efforts in Table <ref type="table" target="#tab_7">8</ref>. This paper's methodology primarily builds upon the SML approach by developing a rubric to evaluate the overlap between LLM capabilities and worker tasks as reported in the O*NET database. Table <ref type="table">9</ref> presents the results of OLS regressions of our new LLM exposure measurements on occupation-level exposure measures from <ref type="bibr" target="#b43">(Felten et al., 2018)</ref> ("AI Occupational Exposure Score" in the table), <ref type="bibr" target="#b45">(Frey and Osborne, 2017)</ref>  <ref type="bibr">(Frey &amp; Osborne Automation)</ref>, scores from all three technologies in <ref type="bibr" target="#b82">(Webb, 2020)</ref>, normalized routine manual and cognitive scores from <ref type="bibr">(Acemoglu and Autor, 2011a)</ref>, and <ref type="bibr" target="#b30">(Brynjolfsson et al., 2018</ref><ref type="bibr">, 2023) (SML)</ref>. We also use annualized occupational salaries from the most recent BLS Occupational Employment Survey as a control. There are four separate output variables representing new scores in this paper that are predicted by earlier efforts.</p><p>GPT-4 Exposure Rating 1 corresponds to our overall exposure rubric as evaluated by GPT-4, where full exposure potential is coded as 1, no exposure potential is coded as 0, and partial exposure (E2 in our labeling scheme) is coded as 0.5. GPT-4 Exposure Rating 2 is scored similarly for overall exposure, but with a slightly different prompt. The results are very similar across the two prompts. GPT-4 Automation Rating applies our "T" rubric, coding no automation exposure from LLMs as 0, full automation exposure as 1, and levels 2, 3, and 4 as 0.25, 0.5, and 0.75, respectively. Finally, Human Exposure Rating represents the same rubric as in GPT-4 Exposure Rating 1 but is scored by humans, as discussed in an earlier section of the paper. These results correspond to the ? set of statistics presented above.</p><p>The results across each type of measurement are consistent. We find generally positive and statistically significant correlations between our LLM exposure measures and previous measurements targeting software and AI. Encouragingly, the SML exposure scores by occupation show significant and positive associations with the exposure scores we develop in this paper, demonstrating a level of cohesion between the two studies with similar approaches. The Webb software and AI patent-based measures, SML, and normalized (demeaned We have also included summary statistics for measurements newly presented in this work. We include all measures from <ref type="bibr" target="#b82">(Webb, 2020)</ref>, normalized routine cognitive and manual scores from <ref type="bibr">(Acemoglu and Autor, 2011a)</ref> (means may deviate slightly from 0 due to imperfect matching of occupational groups), Suitability for Machine Learning from <ref type="bibr" target="#b29">(Brynjolfsson and Mitchell, 2017;</ref><ref type="bibr" target="#b30">Brynjolfsson et al., 2018</ref><ref type="bibr" target="#b28">Brynjolfsson et al., , 2023))</ref>, AI Occupational Exposure from <ref type="bibr" target="#b43">(Felten et al., 2018)</ref>, and Automation exposure from <ref type="bibr" target="#b45">(Frey and Osborne, 2017)</ref>. We include as many occupations as we can match, but since O*NET taxonomies have changed as these measures have been developed, some of the roles may be missing from the most recent version of O*NET 6-digit occupations.</p><p>and divided by standard deviation) routine cognitive scores all exhibit positive associations with some of our measures. Software, SML, and routine cognitive scores all show positive and statistically significant associations with LLM exposure scores at a 1% level. Coefficients on AI scores from <ref type="bibr" target="#b82">(Webb, 2020)</ref> are also positive and statistically significant at a 5% level, but our secondary prompt on overall exposure to LLMs in columns 3 and 4 does not exhibit a statistically significant relationship. For the most part, the AI Occupational Exposure Score is not correlated with our exposure measures. Webb's Robot exposure scores, routine manual task content, and the overall Automation metric from <ref type="bibr" target="#b45">(Frey and Osborne, 2017</ref>) are all negatively correlated with our primary GPT-4 and human-assessed overall exposure ratings, conditional on the other measurements. This negative correlation reflects the limited exposure of physical tasks to LLMs. Manual work is not exposed to LLMs or even LLMs with additional systems integration for the time being. Our automation rubric results are also uncorrelated with <ref type="bibr" target="#b45">(Frey and Osborne, 2017)</ref> measures.</p><p>Low correlations with <ref type="bibr" target="#b43">(Felten et al., 2018)</ref> and <ref type="bibr" target="#b45">(Frey and Osborne, 2017)</ref> could potentially be explained by differences in approaches. Linking AI capabilities to worker abilities or scoring exposure directly based on the occupation's characteristics, rather than aggregating up to the occupation from DWA or task-level scoring (as in the SML paper and our own), offer a slightly different perspective on the content of occupations.</p><p>In all regressions, the ? 2 ranges between 60.7% (column 3) and 72.8% (column 5). This suggests that our measure, which explicitly focuses on LLM capabilities, has between 28 and 40% unexplained variance compared to other measurements. Particularly in the case of AI-related exposure scores, we anticipate that a combination of other measurements would have a strong correlation with our scores. However, earlier efforts had limited information about the future progress of LLM technologies. We expect that our understanding of future machine learning technologies is similarly imperfectly captured by our rubric today. Table <ref type="table">9</ref>: Regression of GPT-exposure cores on prior efforts. Regression coefficients from exposure measures from our rubrics on earlier efforts to quantify occupational exposure to AI and automation. We also include annualized wages from the BLS-OES survey in May 2021. Each measure is kept in its original scale, with the exception of routine cognitive and routine manual scores from <ref type="bibr">(Acemoglu and Autor, 2011a)</ref>. Those two scores are standardized to mean zero and variance 1. Generally we find strong positive associations with previous efforts, though large residual variance to still be explained by our new measures. Columns 1 and 2 are based on our main ? exposure measure from GPT-4 ratings. Columns 3 and 4 are based on a similar slightly different exposure rubric also rated by GPT-4 for robustness. Columns 5 and 6 reflect human ratings on the same rubric as columns 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">GPTs as a General-Purpose Technology</head><p>Earlier in this paper we discuss the possibility that GPTs could be classified as a general-purpose technology. This classification requires GPTs to meet three core criteria: improvement over time, pervasiveness throughout the economy, and the ability to spawn complementary innovations <ref type="bibr" target="#b59">(Lipsey et al., 2005)</ref>. Evidence from the AI and machine learning literature thoroughly demonstrates that GPTs meet the first criteria -they are improving in capabilities over time with the ability to complete or be helpful for an increasingly complex set of tasks and use-cases (see 2.1). This paper presents evidence to support the latter two criteria, finding that GPTs on their own can have pervasive impacts across the economy, and that complementary innovations enabled by GPTsparticularly via software and digital tools -can have widespread application to economic activity. Figure <ref type="figure">3</ref> offers one illustration of the potential economic impact of complementary software built on top of LLMs. Taking the difference in the y-axis (the share of all occupations) between ? and ? at a given point along the x-axis (the share of tasks within an occupation that are exposed) gives the aggregate within-occupation exposure potential attributable to tools and software over and above direct exposure from LLMs on their own. The difference in means across all tasks between ? and ? of 0.42 using the GPT-4 annotations and 0.32 using the human annotations (see Figure <ref type="figure">3</ref>), suggests that the average impact of GPT-powered software on task-exposure may be more than twice as large as the mean exposure from LLMs on their own (mean ? of 0.14 based on both human annotations and GPT-4 annotations). While our findings suggest that out-of-the-box these models are relevant to a meaningful share of workers and tasks, they also suggest that the software innovations they spawn could drive a much broader impact.</p><p>One component of the pervasiveness of a technology is its level of adoption by businesses and users. This paper does not systematically analyze adoption of these models, however, there is early qualitative evidence that adoption and use of LLMs is becoming increasingly widespread. The power of relatively simple UI improvements on top of LLMs was evident in the rollout of ChatGPT -wherein versions of the underlying model had been previously available via API, but usage skyrocketed after the release of the ChatGPT interface. <ref type="bibr" target="#b35">(Chow, 2023;</ref><ref type="bibr">OpenAI, 2022)</ref> Following this release, a number of commercial surveys indicate that firm and worker adoption of LLMs has increased over the past several months. <ref type="bibr" target="#b37">(Constantz, 2023;</ref><ref type="bibr" target="#b71">ResumeBuilder.com, 2023)</ref> Widespread adoption of these models, however, necessitates the identification of existing bottlenecks. A key determinant of their utility is the level of confidence humans place in them, as well as habits. For instance, in the legal profession, the models' usefulness hinges upon whether legal professionals can trust their output without resorting to verifying original documents or conducting independent research. The cost and flexibility of the technology, worker and firm preferences, and incentives also play a significant role in the adoption of tools built on top of LLMs. In this way, adoption may be driven by progress on some of the ethical and safety risks associated with LLMs: bias, making up facts, and misalignment to name a few OpenAI (2023a).</p><p>Moreover, the adoption of LLMs will vary across different economic sectors due to factors such as data availability, regulatory quality, innovation culture, and the distribution of power and interests. Consequently, a comprehensive understanding of the adoption and of large language models by workers and firms requires a more in-depth exploration of these intricacies.</p><p>One possibility is that time savings and seamless application will hold greater importance than quality improvement for the majority of tasks. Another is that the initial focus will be on augmentation, followed by automation <ref type="bibr" target="#b51">(Huang and Rust, 2018)</ref>. One way this might take shape is that an augmentation phase where jobs first become more precarious (writers become freelancers) could play out prior to full automation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Implications for US Public Policy</head><p>The introduction of automation technologies, including LLMs, has previously been linked to heightened economic disparity and labor disruption, which may give rise to adverse downstream effects. <ref type="bibr">(Acemoglu and Restrepo, 2022a;</ref><ref type="bibr" target="#b1">Acemoglu, 2002;</ref><ref type="bibr" target="#b62">Moll et al., 2021;</ref><ref type="bibr" target="#b55">Klinova and Korinek, 2021;</ref><ref type="bibr" target="#b83">Weidinger et al., 2021</ref><ref type="bibr" target="#b84">Weidinger et al., , 2022) )</ref> Our results examining worker exposure in the United States underscore the need for societal and policy preparedness to the potential economic disruption posed by LLMs and the complementary technologies that they spawn. While it is outside the scope of this paper to recommend specific policy prescriptions to smooth the transition to an economy with increasingly widespread LLM adoption, prior work such as <ref type="bibr" target="#b11">(Autor et al., 2022b)</ref> has articulated several important directions for US policy related to education, worker training, reforms to safety net programs, and more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Limitations and Future Work</head><p>This study possesses several limitations that warrant further investigation. Primarily, our focus on the United States restricts the generalizability of our findings to other nations where the adoption and impact of generative models may differ due to factors such as industrial organization, technological infrastructure, regulatory frameworks, linguistic diversity, and cultural contexts. We hope to address this limitation by extending the study's scope and by sharing our methods so other researchers can build on them.</p><p>Subsequent research efforts should consider two additional studies: one exploring GPT adoption patterns across various sectors and occupations, and another scrutinizing the actual capabilities and limitations of state-of-the-art models in relation to worker activities beyond the scope of our exposure scores. For example, despite recent advances in multimodal capabilities with GPT-4, we did not consider vision capabilities in the ? ratings on direct GPT-exposure. (OpenAI, 2023b) Future work should consider the impact of such capability advances as they unfold. We acknowledge that there may be discrepancies between theoretical and practical performance, particularly in complex, open-ended, and domain-specific tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In conclusion, this study offers an examination of the potential impact of LLMs, specifically GPTs, on various occupations and industries within the U.S. economy. By applying a new rubric for understanding LLM capabilities and their potential effects on jobs, we have observed that most occupations exhibit some degree of exposure to GPTs, with higher-wage occupations generally presenting more tasks with high exposure. Our analysis indicates that approximately 19 % of jobs have at least 50% of their tasks exposed to GPTs when considering both current model capabilities and anticipated GPT-powered software.</p><p>Our research aims to highlight the general-purpose potential of GPTs and their possible implications for US workers. Previous literature demonstrates the impressive improvements of GPTs to date (see 2.1). Our findings confirm the hypothesis that these technologies can have pervasive impacts across a wide swath of occupations in the US, and that additional advancements supported by GPTs, mainly through software and digital tools, can have significant effects on a range of economic activities. However, while the technical capacity for GPTs to make human labor more efficient appears evident, it is important to recognize that social, economic, regulatory, and other factors may influence actual labor productivity outcomes. As capabilities continue to evolve, the impact of GPTs on the economy will likely persist and increase, posing challenges for policymakers in predicting and regulating their trajectory.</p><p>Further research is necessary to explore the broader implications of GPT advancements, including their potential to augment or displace human labor, their impact on job quality, impacts on inequality, skill development, and numerous other outcomes. By seeking to understand the capabilities and potential effects of GPTs on the workforce, policymakers and stakeholders can make more informed decisions to navigate the complex landscape of AI and its role in shaping the future of work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">GPT Conclusion (GPT-4's Version)</head><p>Generative Pre-trained Transformers (GPTs) generate profound transformations, garnering potential technological growth, permeating tasks, greatly impacting professions. This study probes GPTs' potential trajectories, presenting a groundbreaking rubric to gauge tasks' GPT exposure, particularly in the U.S. labor market.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">GPT Conclusion (Author-Augmented Version)</head><p>Generative Pre-trained Transformers (GPTs) generate profound transformations, garnering potential technological growth, permeating tasks, gutting professional management. Gauging possible trajectories? Generate pioneering taxonomies, gather policymakers together, generalize past today.</p><p>Equivalent quality means someone reviewing the work would not be able to tell whether a human completed it on their own or with assistance from the LLM.</p><p>If you aren't sure how to judge the amount of time a task takes, consider whether the tools described exposed the majority of subtasks associated with the task.</p><p>## E1 -Direct exposure Label tasks E1 if direct access to the LLM through an interface like ChatGPT or the OpenAI playground alone can reduce the time it takes to complete the task with equivalent quality by at least half. This includes tasks that can be reduced to: -Writing and transforming text and code according to complex instructions, -Providing edits to existing text or code following specifications, -Writing code that can help perform a task that used to be done by hand, -Translating text between languages, -Summarizing medium-length documents, -Providing feedback on documents, -Answering questions about a document, -Generating questions a user might want to ask about a document, -Writing questions for an interview or assessment, -Writing and responding to emails, including ones that involve refuting information or engaging in a negotiation (but only if the negotiation is via written correspondence), -Maintain records of written data, -Prepare training materials based on general knowledge, or -Inform anyone of any information via any written or spoken medium.</p><p>## E2 -Exposure by LLM-powered applications Label tasks E2 if having access to the LLM alone may not reduce the time it takes to complete the task by at least half, but it is easy to imagine additional software that could be developed on top of the LLM that would reduce the time it takes to complete the task by half. This software may include capabilities such as: -Summarizing documents longer than 2000 words and answering questions about those documents, -Retrieving up-to-date facts from the Internet and using those facts in combination with the LLM capabilities, -Searching over an organization's existing knowledge, data, or documents and retreiving information, -Retrieving highly specialized domain knowledge, -Make recommendations given data or written input, -Analyze written information to inform decisions, -Prepare training materials based on highly specialized knowledge, -Provide counsel on issues, and -Maintain complex databases.</p><p>## E3 -Exposure given image capabilities Suppose you had access to both the LLM and a system that could view, caption, and create images as well as any systems powered by the LLM (those in E2 above). This system cannot take video as an input and it cannot produce video as an output. This system cannot accurately retrieve very detailed information from image inputs, such as measurements of dimensions within an image. Label tasks as E3 if there is a significant reduction in the time it takes to complete the task given access to a LLM and these image capabilities: -Reading text from PDFs, -Scanning images, or -Creating or editing digital images according to instructions.</p><p>The images can be realistic but they should not be detailed. The model can identify objects in the image but not relationships between those options.</p><p>## E0 -No exposure Label tasks E0 if none of the above clearly decrease the time it takes for an experienced worker to complete the task with high quality by at least half. Some examples: -If a task requires a high degree of human interaction (for example, in-person demonstrations) then it should be classified as E0. -If a task requires precise measurements then it should be classified as E0. -If a task requires reviewing visuals in detail then it should be classified as E0. -If a task requires any use of a hand or walking then it should be classified as E0. -Tools built on top of the LLM cannot make any decisions that might impact human livelihood (e.g. hiring, grading, etc.). If any part of the task involves collecting inputs to make a final decision (as opposed to analyzing data to inform a decision or make a recommendation) then it should be classified as E0. The LLM can make recommendations. -Even if tools built on top of the LLM can do a task, if using those tools would not save an experienced worker significant time completing the task, then it should be classified as E0. -The LLM and systems built on top of it cannot do anything that legally requires a human to perform the task. -If there is existing technology not powered by an LLM that is commonly used and can complete the task then you should mark the task E0 if using an LLM or LLM-powered tool will not further reduce the time to complete the task.</p><p>When in doubt, you should default to E0. ## Annotation examples: Occupation: Inspectors, Testers, Sorters, Samplers, and Weighers Task: Adjust, clean, or repair products or processing equipment to correct defects found during inspections. Label (E0/E1/E2/E3): E0 Explanation: The model does not have access to any kind of physicality, and more than half of the task (adjusting, cleaning and repairing equipment) described requires hands or other embodiment.</p><p>Occupation: Computer and Information Research Scientists Task: Apply theoretical expertise and innovation to create or apply new technology, such as adapting principles for applying computers to new uses. Label (E0/E1/E2/E3): E1 Explanation: The model can learn theoretical expertise during training as part of its general knowledge base, and the principles to adapt can be captured in the text input to the model.</p><p>Activity: Schedule dining reservations. Label (E0/E1/E2/E3): E2 Explanation: Automation technology already exists for this (e.g. Resy) and it's unclear what an LLM offers on top of using that technology (no-diff). That said, you could build something that allows you to ask the LLM to make a reservation on Resy for you.</p><p>-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ONET Basic Skills Definitions Basic Skills</head><p>Developed capacities that facilitate learning or the more rapid acquisition of knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Content</head><p>Background structures needed to work with and acquire more specific skills in a variety of different domains.</p><p>? Reading Comprehension -Understanding written sentences and paragraphs in work-related documents.</p><p>? Active Listening -Giving full attention to what other people are saying, taking time to understand the points being made, asking questions as appropriate, and not interrupting at inappropriate times.</p><p>? Writing -Communicating effectively in writing as appropriate for the needs of the audience.</p><p>? Speaking -Talking to others to convey information effectively.</p><p>? Mathematics -Using mathematics to solve problems.</p><p>? Science -Using scientific rules and methods to solve problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Process</head><p>Procedures that contribute to the more rapid acquisition of knowledge and skill across a variety of domains</p><p>? Critical Thinking -Using logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems.</p><p>? Active Learning -Understanding the implications of new information for both current and future problem-solving and decision-making.</p><p>? Learning Strategies -Selecting and using training/instructional methods and procedures appropriate for the situation when learning or teaching new things.</p><p>? Monitoring -Monitoring/Assessing performance of yourself, other individuals, or organizations to make improvements or take corrective action. Alongside exposure scores, we display the median of median annual income for each occupation, as well as the total number of workers in each group, in thousands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Education</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Regional, Industrial, and Productivity Exposure</head><p>Which regions are most exposed (map) to automation and augmentation Figures <ref type="figure">6</ref> and<ref type="figure">7</ref> show the overall employment-weighted relative exposure of 3-digit NAICS industries according to human raters and our algorithmic exposure rubric respectively. The impact potential is present across nearly all industries, with wide heterogeneity. Table XX (PUT A TABLE SHOWING RELATIVE EXPOSURES) describes the relative exposures according to different evaluation regimes. Both methods agree generally on relative exposures: data processing, information processing, and hospitals all have high exposure.</p><p>Figure <ref type="figure">6</ref> Figure <ref type="figure">7</ref> Recent productivity growth (both total factor and labor) appears uncorrelated with exposure as well. <ref type="bibr">Figures D and D</ref> show little relationship between productivity growth since 2012 and current exposure to LLMs as rated by the model. A high correlation between already fast-growing productive industries and exposure might mean an exacerbation of Baumol's cost disease. In other words, if LLMs are likely to increase productivity differentially across industries, one concern is that the most productive would become even more productive. With inelastic demand for the production of those industries, the most productive sectors would shrink as a proportion of inputs in the economy. We see little to suggest this will be the case. Productivity growth since 2012 and exposure to LLM technologies appear unrelated. From the table above, we see that the proportion of women employed in an occupation is positively and significantly associated with an occupation's exposure to GPTs. Across all measures, we see the proportion of Asian people in an occupation to be positively correlated with GPT-exposure and that of Latino people to be negatively correlated. Demographic groups are unevenly distributed across occupations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Demographic Variation in</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: ? exposure ratings of occupations in the five Job Zones, which are groups of similar occupations that are classified according to the level of education, experience, and on-the-job training needed to perform them.</figDesc><graphic url="image-9.png" coords="14,83.70,72.00,444.59,228.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="2,95.40,72.00,421.19,266.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-4.png" coords="12,72.00,72.00,468.00,147.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-10.png" coords="26,88.94,500.91,234.00,213.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-11.png" coords="26,324.57,500.91,234.00,213.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Sample of occupations, tasks, and Detailed Work Activities from the O*NET database. We see that aggregating over activities alone is imprecise, as evidenced by the fact that we'd expect Gambling Cage Workers to complete the given DWA in person, using some physicality while we'd expect Online Merchants to complete the same activity solely with a computer.</figDesc><table><row><cell>Task ID</cell><cell>Occupation Title</cell><cell>DWAs</cell><cell>Task Description</cell></row><row><cell cols="2">14675 Computer Systems</cell><cell>Monitor computer system performance</cell><cell>Monitor system operation to detect potential</cell></row><row><cell></cell><cell>Engineers/Architects</cell><cell>to ensure proper operation.</cell><cell>problems.</cell></row><row><cell>18310</cell><cell>Acute Care Nurses</cell><cell>Operate diagnostic or therapeutic</cell><cell>Set up, operate, or monitor invasive</cell></row><row><cell></cell><cell></cell><cell>medical instruments or equipment.</cell><cell>equipment and devices, such as colostomy or</cell></row><row><cell></cell><cell></cell><cell>Prepare medical supplies or equipment</cell><cell>tracheotomy equipment, mechanical</cell></row><row><cell></cell><cell></cell><cell>for use.</cell><cell>ventilators, catheters, gastrointestinal tubes,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>and central lines.</cell></row><row><cell cols="2">4668.0 Gambling Cage</cell><cell>Execute sales or other financial</cell><cell>Cash checks and process credit card advances</cell></row><row><cell></cell><cell>Workers</cell><cell>transactions.</cell><cell>for patrons.</cell></row><row><cell>15709</cell><cell>Online Merchants</cell><cell>Execute sales or other financial</cell><cell>Deliver e-mail confirmation of completed</cell></row><row><cell></cell><cell></cell><cell>transactions.</cell><cell>transactions and shipment.</cell></row><row><cell cols="2">6529 Kindergarten</cell><cell>-</cell><cell>Involve parent volunteers and older students in</cell></row><row><cell></cell><cell>Teachers, Except</cell><cell></cell><cell>children's activities to facilitate involvement</cell></row><row><cell></cell><cell>Special Education</cell><cell></cell><cell>in focused, complex play.</cell></row><row><cell>6568</cell><cell>Elementary School</cell><cell>-</cell><cell>Involve parent volunteers and older students in</cell></row><row><cell></cell><cell>Teachers, Except</cell><cell></cell><cell>children's activities to facilitate involvement</cell></row><row><cell></cell><cell>Special Education</cell><cell></cell><cell>in focused, complex play.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Model and human comparison of agreement and Pearson's correlation scores. The agreement score is determined by looking at how often the two groups agree on the annotation (e.g. E0, E1 or E2). In the paper we use GPT-4, Rubric 1.</figDesc><table><row><cell>Comparison</cell><cell cols="3">? Weighting Agreement Pearson's</cell></row><row><cell>GPT-4, Rubric 1; Human</cell><cell>? E1</cell><cell>80.8%</cell><cell>0.223</cell></row><row><cell></cell><cell>? E1 + .5*E2</cell><cell>65.6%</cell><cell>0.591</cell></row><row><cell></cell><cell>? E1 + E2</cell><cell>82.1%</cell><cell>0.654</cell></row><row><cell>GPT-4, Rubric 2; Human</cell><cell>? E1</cell><cell>81.8%</cell><cell>0.221</cell></row><row><cell></cell><cell>? E1 + .5*E2</cell><cell>65.6%</cell><cell>0.538</cell></row><row><cell></cell><cell>? E1 + E2</cell><cell>79.5%</cell><cell>0.589</cell></row><row><cell cols="2">GPT-4, Rubric 1; GPT-4, Rubric 2 ? E1</cell><cell>91.1%</cell><cell>0.611</cell></row><row><cell></cell><cell>? E1 + .5*E2</cell><cell>76.0%</cell><cell>0.705</cell></row><row><cell></cell><cell>? E1 + E2</cell><cell>82.4%</cell><cell>0.680</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Summary statistics of our human and model exposure data.</figDesc><table><row><cell></cell><cell cols="2">Occupation Level Exposure</cell></row><row><cell></cell><cell>Human</cell><cell>GPT-4</cell></row><row><cell></cell><cell cols="2">mean std mean std</cell></row><row><cell cols="3">? ? ? 0.14 0.14 0.14 0.16</cell></row><row><cell cols="3">? ? ? 0.30 0.21 0.34 0.22</cell></row><row><cell>? ? ?</cell><cell cols="2">0.46 0.30 0.55 0.34</cell></row><row><cell></cell><cell cols="2">Task Level Exposure</cell></row><row><cell></cell><cell>Human</cell><cell>GPT-4</cell></row><row><cell></cell><cell cols="2">mean std mean std</cell></row><row><cell cols="3">? ? ? 0.15 0.36 0.14 0.35</cell></row><row><cell cols="3">? ? ? 0.31 0.37 0.35 0.35</cell></row><row><cell>? ? ?</cell><cell cols="2">0.47 0.50 0.56 0.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Occupations with the highest exposure according to each measurement. The final row lists the occupations with the highest ? 2 value, indicating that they had the most variability in vulnerability-prediction. Exposure percentages indicate the share of an occupation's task that are exposed to GPTs (? ? ?) or GPT-powered software (? ? ? and ? ? ?), where exposure is defined as driving a reduction in time it takes to complete the task by at least 50% (see exposure rubric A.1. As such, occupations listed in this table are those where we estimate</figDesc><table><row><cell>Group</cell><cell>Occupations with highest exposure</cell><cell>% Exposure</cell></row><row><cell>Human ? ? ?</cell><cell>Interpreters and Translators</cell><cell>76.5</cell></row><row><cell></cell><cell>Survey Researchers</cell><cell>75.0</cell></row><row><cell></cell><cell>Poets, Lyricists and Creative Writers</cell><cell>68.8</cell></row><row><cell></cell><cell>Animal Scientists</cell><cell>66.7</cell></row><row><cell></cell><cell>Public Relations Specialists</cell><cell>66.7</cell></row><row><cell>Human ? ? ?</cell><cell>Survey Researchers</cell><cell>84.4</cell></row><row><cell></cell><cell>Writers and Authors</cell><cell>82.5</cell></row><row><cell></cell><cell>Interpreters and Translators</cell><cell>82.4</cell></row><row><cell></cell><cell>Public Relations Specialists</cell><cell>80.6</cell></row><row><cell></cell><cell>Animal Scientists</cell><cell>77.8</cell></row><row><cell>Human ? ? ?</cell><cell>Mathematicians</cell><cell>100.0</cell></row><row><cell></cell><cell>Tax Preparers</cell><cell>100.0</cell></row><row><cell></cell><cell>Financial Quantitative Analysts</cell><cell>100.0</cell></row><row><cell></cell><cell>Writers and Authors</cell><cell>100.0</cell></row><row><cell></cell><cell>Web and Digital Interface Designers</cell><cell>100.0</cell></row><row><cell></cell><cell cols="2">Humans labeled 15 occupations as "fully exposed."</cell></row><row><cell>Model ? ? ?</cell><cell>Mathematicians</cell><cell>100.0</cell></row><row><cell></cell><cell>Correspondence Clerks</cell><cell>95.2</cell></row><row><cell></cell><cell>Blockchain Engineers</cell><cell>94.1</cell></row><row><cell></cell><cell>Court Reporters and Simultaneous Captioners</cell><cell>92.9</cell></row><row><cell></cell><cell>Proofreaders and Copy Markers</cell><cell>90.9</cell></row><row><cell>Model ? ? ?</cell><cell>Mathematicians</cell><cell>100.0</cell></row><row><cell></cell><cell>Blockchain Engineers</cell><cell>97.1</cell></row><row><cell></cell><cell>Court Reporters and Simultaneous Captioners</cell><cell>96.4</cell></row><row><cell></cell><cell>Proofreaders and Copy Markers</cell><cell>95.5</cell></row><row><cell></cell><cell>Correspondence Clerks</cell><cell>95.2</cell></row><row><cell>Model ? ? ?</cell><cell>Accountants and Auditors</cell><cell>100.0</cell></row><row><cell></cell><cell>News Analysts, Reporters, and Journalists</cell><cell>100.0</cell></row><row><cell></cell><cell>Legal Secretaries and Administrative Assistants</cell><cell>100.0</cell></row><row><cell></cell><cell>Clinical Data Managers</cell><cell>100.0</cell></row><row><cell></cell><cell>Climate Change Policy Analysts</cell><cell>100.0</cell></row><row><cell></cell><cell cols="2">The model labeled 86 occupations as "fully exposed."</cell></row><row><cell cols="2">Highest variance Search Marketing Strategists</cell><cell>14.5</cell></row><row><cell></cell><cell>Graphic Designers</cell><cell>13.4</cell></row><row><cell></cell><cell>Investment Fund Managers</cell><cell>13.0</cell></row><row><cell></cell><cell>Financial Managers</cell><cell>13.0</cell></row><row><cell></cell><cell>Insurance Appraisers, Auto Damage</cell><cell>12.6</cell></row></table><note><p>that GPTs and GPT-powered software are able to save workers a significant amount of time completing a large share of their tasks, but it does not necessarily suggest that their tasks can be fully automated by these technologies.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>OLS Regression Results of Exposure Measures on O*NET Skills</figDesc><table><row><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell>(std err)</cell><cell>(std err)</cell><cell>(std err)</cell></row><row><cell>Constant</cell><cell>0.082***</cell><cell>-0.112***</cell><cell>0.300***</cell></row><row><cell></cell><cell>(0.011)</cell><cell>(0.011)</cell><cell>(0.057)</cell></row><row><cell>Active Listening</cell><cell>0.128**</cell><cell>0.214***</cell><cell>0.449***</cell></row><row><cell></cell><cell>(0.047)</cell><cell>(0.043)</cell><cell>(0.027)</cell></row><row><cell>Mathematics</cell><cell>-0.127***</cell><cell>0.161***</cell><cell>0.787***</cell></row><row><cell></cell><cell>(0.026)</cell><cell>(0.021)</cell><cell>(0.049)</cell></row><row><cell>Reading Comprehension</cell><cell>0.153***</cell><cell>0.470***</cell><cell>-0.346***</cell></row><row><cell></cell><cell>(0.041)</cell><cell>(0.037)</cell><cell>(0.017)</cell></row><row><cell>Science</cell><cell>-0.114***</cell><cell>-0.230***</cell><cell>-0.346***</cell></row><row><cell></cell><cell>(0.014)</cell><cell>(0.012)</cell><cell>(0.017)</cell></row><row><cell>Speaking</cell><cell>-0.028</cell><cell>0.133***</cell><cell>0.294***</cell></row><row><cell></cell><cell>(0.039)</cell><cell>(0.033)</cell><cell>(0.042)</cell></row><row><cell>Writing</cell><cell>0.368***</cell><cell>0.467***</cell><cell>0.566***</cell></row><row><cell></cell><cell>(0.042)</cell><cell>(0.037)</cell><cell>(0.047)</cell></row><row><cell>Active Learning</cell><cell>-0.157***</cell><cell>-0.065**</cell><cell>0.028</cell></row><row><cell></cell><cell>(0.027)</cell><cell>(0.024)</cell><cell>(0.032)</cell></row><row><cell>Critical Thinking</cell><cell>-0.264***</cell><cell>-0.196***</cell><cell>-0.129**</cell></row><row><cell></cell><cell>(0.036)</cell><cell>(0.033)</cell><cell>(0.042)</cell></row><row><cell>Learning Strategies</cell><cell>-0.072*</cell><cell>-0.209***</cell><cell>-0.346***</cell></row><row><cell></cell><cell>(0.028)</cell><cell>(0.025)</cell><cell>(0.034)</cell></row><row><cell>Monitoring</cell><cell>-0.067**</cell><cell>-0.149***</cell><cell>-0.232***</cell></row><row><cell></cell><cell>(0.023)</cell><cell>0.020)</cell><cell>(0.026)</cell></row><row><cell>Programming</cell><cell>0.637***</cell><cell>0.623***</cell><cell>0.609***</cell></row><row><cell></cell><cell>(0.030)</cell><cell>(0.022)</cell><cell>(0.024)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Exposure to GPTs by Job Zone</figDesc><table><row><cell></cell><cell cols="2">Med(Med) Inc Emp (000s)</cell><cell>H ? ? ?</cell><cell>M ? ? ?</cell><cell>H ? ? ?</cell><cell>M ? ? ?</cell><cell>H ? ? ?</cell><cell>M ? ? ?</cell></row><row><cell>None</cell><cell>$77,440</cell><cell>90,776</cell><cell>0.20</cell><cell>0.16</cell><cell>0.42</cell><cell>0.46</cell><cell>0.63</cell><cell>0.76</cell></row><row><cell>Apprenticeship</cell><cell>$55,995</cell><cell>3,066</cell><cell>0.01</cell><cell>0.02</cell><cell>0.04</cell><cell>0.06</cell><cell>0.07</cell><cell>0.10</cell></row><row><cell>Internship/residency</cell><cell>$77,110</cell><cell>3,063</cell><cell>0.16</cell><cell>0.06</cell><cell>0.36</cell><cell>0.38</cell><cell>0.55</cell><cell>0.71</cell></row><row><cell>Short-term on-the-job training</cell><cell>$33,370</cell><cell>66,234</cell><cell>0.11</cell><cell>0.15</cell><cell>0.21</cell><cell>0.25</cell><cell>0.32</cell><cell>0.34</cell></row><row><cell>Moderate-term on-the-job training</cell><cell>$46,880</cell><cell>31,285</cell><cell>0.09</cell><cell>0.12</cell><cell>0.21</cell><cell>0.25</cell><cell>0.32</cell><cell>0.38</cell></row><row><cell>Long-term on-the-job training</cell><cell>$48,925</cell><cell>5,070</cell><cell>0.08</cell><cell>0.10</cell><cell>0.18</cell><cell>0.22</cell><cell>0.28</cell><cell>0.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Mean exposure scores for occupations, grouped by level of on-the-job training required to attain competency in the job. Alongside exposure scores, we display the median of median annual income for each occupation, as well as the total number of workers in each group, in thousands.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Summary statistics for a suite of prior efforts to measure occupational exposure to AI and automation.</figDesc><table><row><cell></cell><cell cols="4">Min 25th Perc. Median 75th Perc</cell><cell>Max</cell><cell cols="3">Mean Std. Dev. Count</cell></row><row><cell>GPT-4 Exposure Rating 1</cell><cell>0.00</cell><cell>0.13</cell><cell>0.34</cell><cell>0.50</cell><cell>1.00</cell><cell>0.33</cell><cell>0.22</cell><cell>750</cell></row><row><cell>GPT-4 Exposure Rating 2</cell><cell>0.00</cell><cell>0.09</cell><cell>0.24</cell><cell>0.40</cell><cell>0.98</cell><cell>0.26</cell><cell>0.20</cell><cell>750</cell></row><row><cell>Human Exposure Rating</cell><cell>0.00</cell><cell>0.09</cell><cell>0.29</cell><cell>0.47</cell><cell>0.84</cell><cell>0.29</cell><cell>0.21</cell><cell>750</cell></row><row><cell>Software (Webb)</cell><cell>1.00</cell><cell>25.00</cell><cell>50.00</cell><cell>75.00</cell><cell cols="2">100.00 50.69</cell><cell>30.05</cell><cell>750</cell></row><row><cell>Robot (Webb)</cell><cell>1.00</cell><cell>22.00</cell><cell>52.00</cell><cell>69.00</cell><cell cols="2">100.00 48.61</cell><cell>28.61</cell><cell>750</cell></row><row><cell>AI (Webb)</cell><cell>1.00</cell><cell>28.00</cell><cell>55.00</cell><cell>82.00</cell><cell cols="2">100.00 54.53</cell><cell>29.65</cell><cell>750</cell></row><row><cell cols="2">Suitability for Machine Learning 2.60</cell><cell>2.84</cell><cell>2.95</cell><cell>3.12</cell><cell>3.55</cell><cell>2.99</cell><cell>0.18</cell><cell>750</cell></row><row><cell>Normalized Routine Cognitive</cell><cell>-3.05</cell><cell>-0.46</cell><cell>0.10</cell><cell>0.63</cell><cell>3.42</cell><cell>0.07</cell><cell>0.86</cell><cell>750</cell></row><row><cell>Normalized Routine Manual</cell><cell>-1.81</cell><cell>-0.81</cell><cell>-0.11</cell><cell>0.73</cell><cell>2.96</cell><cell>0.05</cell><cell>1.01</cell><cell>750</cell></row><row><cell cols="2">AI Occupational Exposure Score 1.42</cell><cell>3.09</cell><cell>3.56</cell><cell>4.04</cell><cell>6.54</cell><cell>3.56</cell><cell>0.70</cell><cell>750</cell></row><row><cell>Frey &amp; Osborne Automation</cell><cell>0.00</cell><cell>0.07</cell><cell>0.59</cell><cell>0.88</cell><cell>0.99</cell><cell>0.50</cell><cell>0.38</cell><cell>681</cell></row><row><cell>Log Avg. Salary</cell><cell>10.13</cell><cell>10.67</cell><cell>11.00</cell><cell>11.34</cell><cell cols="2">12.65 11.02</cell><cell>0.45</cell><cell>749</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Mean exposure scores for occupations, grouped by typical education needed for entry into the occupation.</figDesc><table><row><cell></cell><cell cols="2">Median Income Emp (000s)</cell><cell>H ? ? ?</cell><cell>M ? ? ?</cell><cell>H ? ? ?</cell><cell>M ? ? ?</cell><cell>H ? ? ?</cell><cell>M ? ? ?</cell></row><row><cell>No formal educational credential</cell><cell>$31,900</cell><cell>36,187</cell><cell>0.05</cell><cell>0.06</cell><cell>0.10</cell><cell>0.10</cell><cell>0.15</cell><cell>0.15</cell></row><row><cell>High school diploma or equivalent</cell><cell>$45,470</cell><cell>67,033</cell><cell>0.09</cell><cell>0.13</cell><cell>0.20</cell><cell>0.25</cell><cell>0.31</cell><cell>0.37</cell></row><row><cell>Postsecondary nondegree award</cell><cell>$48,315</cell><cell>9,636</cell><cell>0.07</cell><cell>0.15</cell><cell>0.19</cell><cell>0.28</cell><cell>0.31</cell><cell>0.41</cell></row><row><cell>Some college, no degree</cell><cell>$40,970</cell><cell>2,898</cell><cell>0.23</cell><cell>0.34</cell><cell>0.39</cell><cell>0.53</cell><cell>0.55</cell><cell>0.72</cell></row><row><cell>Associate's degree</cell><cell>$60,360</cell><cell>3,537</cell><cell>0.12</cell><cell>0.14</cell><cell>0.31</cell><cell>0.36</cell><cell>0.49</cell><cell>0.59</cell></row><row><cell>Bachelor's degree</cell><cell>$78,375</cell><cell>71,698</cell><cell>0.23</cell><cell>0.17</cell><cell>0.47</cell><cell>0.51</cell><cell>0.70</cell><cell>0.84</cell></row><row><cell>Master's degree</cell><cell>$79,605</cell><cell>3,216</cell><cell>0.26</cell><cell>0.14</cell><cell>0.46</cell><cell>0.44</cell><cell>0.66</cell><cell>0.74</cell></row><row><cell>Doctoral or professional degree</cell><cell>$82,420</cell><cell>5,290</cell><cell>0.21</cell><cell>0.13</cell><cell>0.41</cell><cell>0.43</cell><cell>0.60</cell><cell>0.74</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Demographic Differences in Exposure</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Exposure</cell></row><row><cell></cell><cell>coef</cell><cell>std err z</cell><cell>P&gt; |z| [0.025 0.975]</cell></row><row><cell>const</cell><cell>0.1814</cell><cell cols="2">0.014 12.509 0.000 0.153 0.210</cell></row><row><cell>women</cell><cell>0.0961</cell><cell>0.016</cell><cell>6.017 0.000 0.065 0.127</cell></row><row><cell>black</cell><cell>-0.0794</cell><cell cols="2">0.066 -1.204 0.229 -0.209 0.050</cell></row><row><cell>asian</cell><cell>0.2231</cell><cell>0.083</cell><cell>2.674 0.008 0.060 0.387</cell></row><row><cell cols="2">hispanic -0.3934</cell><cell cols="2">0.040 -9.811 0.000 -0.472 -0.315</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Thank you to the group of annotators who helped us annotate task exposure, including <rs type="person">Muhammad Ahmed Saeed</rs>, <rs type="person">Bongane Zitha</rs>, <rs type="person">Merve ?zen ?enen</rs>, <rs type="person">J.J.</rs>, and <rs type="person">Peter Hoeschele</rs>. We also thank <rs type="person">Lauryn Fuld</rs>, <rs type="person">Ashley Glat</rs>, <rs type="person">Michael Lampe</rs>, and <rs type="person">Julia Susser</rs> for excellent research assistance. We thank <rs type="person">Miles Brundage</rs> for significant feedback on this paper.</p><p>We thank <rs type="person">Todor Markov</rs> and <rs type="person">Vik Goel</rs> for setting up the infrastructure to run our taxonomies against GPT-4. We thank <rs type="person">Lama Ahmad</rs>, <rs type="person">Donald Bakong</rs>, <rs type="person">Seth Benzell</rs>, <rs type="person">Erik Brynjolfsson</rs>, <rs type="person">Carl Frey</rs>, <rs type="person">Sarah Giroux</rs>, <rs type="person">Gillian Hadfield</rs>, <rs type="person">Johannes Heidecke</rs>, <rs type="person">Shengli Hu</rs>, <rs type="person">Alan Hickey</rs>, <rs type="person">Eric Horvitz</rs>, <rs type="person">Ashyana Kachra</rs>, <rs type="person">Daniel Kokotajlo</rs>, <rs type="person">Christina Kim</rs>, <rs type="person">Katya Klinova Gretchen Krueger</rs>, <rs type="person">Michael Lampe</rs>, <rs type="person">Aalok Mehta</rs>, <rs type="person">Larissa Schiavo</rs>, <rs type="person">Daniel Selsam</rs>, <rs type="person">Sarah Shoker</rs>, <rs type="person">Prasanna Tambe</rs>, and <rs type="person">Jeff Wu</rs> for feedback and edits at various stages of the project.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LLM assistance statement</head><p>GPT-4 and ChatGPT were used for writing, coding, and formatting assistance in this project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Taxonomies</head><p>A.1 Exposure</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># E Exposure Rubric</head><p>Consider the most powerful OpenAI large language model (LLM). This model can complete many tasks that can be formulated as having text input and text output where the context for the input can be captured in 2000 words. The model also cannot draw up-to-date facts (those from &lt;1 year ago) unless they are captured in the input.</p><p>Assume you are a worker with an average level of expertise in your role trying to complete the given task. You have access to the LLM as well as any other existing software or computer hardware tools mentioned in the task. You also have access to any commonly available technical tools accessible via a laptop (e.g. a microphone, speakers, etc.). You do not have access to any other physical tools or materials.</p><p>Please label the given task according to the rubric below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Occupations Without Any Exposed Tasks</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Persistent anti-muslim bias in large language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farooqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, AIES &apos;21</title>
		<meeting>the 2021 AAAI/ACM Conference on AI, Ethics, and Society, AIES &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="298" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Skills, tasks and technologies: Implications for employment and earnings</title>
		<author>
			<persName><forename type="first">D</forename><surname>Acemoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of labor economics</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Acemoglu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Autor</surname></persName>
		</editor>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2002">2002. 2011</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1043" to="1171" />
		</imprint>
	</monogr>
	<note>Technical change, inequality, and the labor market</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Skills, Tasks and Technologies: Implications for Employment and Earnings</title>
		<author>
			<persName><forename type="first">D</forename><surname>Acemoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Autor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Labor Economics</title>
		<editor>
			<persName><forename type="first">O</forename><surname>Ashenfelter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Card</surname></persName>
		</editor>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2011">2011b</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1043" to="1171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Ai and jobs: Evidence from online vacancies</title>
		<author>
			<persName><forename type="first">D</forename><surname>Acemoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Autor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hazell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Restrepo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>National Bureau of Economic Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The race between man and machine: Implications of technology for growth, factor shares, and employment</title>
		<author>
			<persName><forename type="first">D</forename><surname>Acemoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Restrepo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American economic review</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1488" to="1542" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automation and new tasks: How technology displaces and reinstates labor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Acemoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Restrepo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Perspectives</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3" to="30" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Demographics and automation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Acemoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Restrepo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Review of Economic Studies</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<author>
			<persName><forename type="first">D</forename><surname>Acemoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Restrepo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tasks, automation, and the rise in us wage inequality</title>
		<imprint>
			<date type="published" when="2022">2022b</date>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="1973" to="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Ai adoption and system-wide change</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Gans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goldfarb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>National Bureau of Economic Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Revisiting the risk of automation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arntz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Zierahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economics Letters</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="157" to="160" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">New frontiers: The origins and content of new work</title>
		<author>
			<persName><forename type="first">D</forename><surname>Autor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Salomons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Seegmiller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022a</date>
			<publisher>National Bureau of Economic Research</publisher>
			<biblScope unit="page" from="1940" to="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Autor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Mindell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Reynolds</surname></persName>
		</author>
		<title level="m">The Work of the Future: Building Better Jobs in an Age of Intelligent Machines</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2022">2022b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The polarization of the us labor market</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Autor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kearney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American economic review</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="194" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The skill content of recent technological change: An empirical exploration</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Autor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Murnane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly journal of economics</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1279" to="1333" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Artificial intelligence, firm growth, and product innovation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Babina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hodson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Firm Growth, and Product Innovation</title>
		<imprint>
			<date type="published" when="2021-11-09">2021. November 9, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>El-Showk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kravec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.05862</idno>
		<title level="m">Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The cost disease: Why computers get cheaper and health care doesn&apos;t</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Baumol</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Yale university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Simulating endogenous global automation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Benzell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Kotlikoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lagarda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>National Bureau of Economic Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Working Paper 29220</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Artificial intelligence and jobs: The role of demand</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bessen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The economics of artificial intelligence: an agenda</title>
		<imprint>
			<publisher>University of Chicago Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="291" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Employment by detailed occupation. BLS (2023a). Demographic characteristics (cps)</title>
		<author>
			<persName><surname>Bls</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Occupational outlook handbook a-z index</title>
		<author>
			<persName><surname>Bls</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<title level="m">On the opportunities and risks of foundation models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Artificial intelligence technologies and aggregate growth prospects</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bresnahan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Technical progress and co-invention in computing and in the uses of computers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bresnahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Greenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brownstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Flamm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brookings Papers on Economic Activity. Microeconomics</title>
		<imprint>
			<biblScope unit="page" from="1" to="83" />
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Computerisation and wage dispersion: an analytical reinterpretation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Bresnahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The economic journal</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">456</biblScope>
			<biblScope unit="page" from="390" to="415" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Information technology, workplace organization, and the demand for skilled labor: Firm-level evidence</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Bresnahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brynjolfsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Hitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The quarterly journal of economics</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="339" to="376" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">General purpose technologies &apos;engines of growth</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Bresnahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trajtenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of econometrics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="108" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Quantifying the Distribution of Machine Learning&apos;s Impact on Work</title>
		<author>
			<persName><forename type="first">E</forename><surname>Brynjolfsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rahwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">What can machine learning do? workforce implications</title>
		<author>
			<persName><forename type="first">E</forename><surname>Brynjolfsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">358</biblScope>
			<biblScope unit="issue">6370</biblScope>
			<biblScope unit="page" from="1530" to="1534" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What can machines learn, and what does it mean for occupations and the economy?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Brynjolfsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AEA Papers and Proceedings</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="43" to="47" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The productivity j-curve: How intangibles complement general purpose technologies</title>
		<author>
			<persName><forename type="first">E</forename><surname>Brynjolfsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Syverson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Journal: Macroeconomics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="333" to="372" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Chase</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>LangChain</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P D O</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03374</idno>
		<title level="m">Evaluating large language models trained on code</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Innovae: Generative ai for understanding patents and innovation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tambe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Why ChatGPT Is the Fastest Growing Web Platform Ever | Time</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Chow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The impact of artificial intelligence on innovation: An exploratory analysis</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Cockburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The economics of artificial intelligence: An agenda</title>
		<imprint>
			<publisher>University of Chicago Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="115" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Nearly a third of white collar workers have tried chatgpt or other ai programs, according to a new survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Constantz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The dynamo and the computer: an historical perspective on the modern productivity paradox</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Economic Review</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="355" to="361" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>ArXiv, abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The robot revolution: Managerial and employment consequences for firms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5586" to="5605" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Organizational frictions and increasing returns to automation: Lessons from at&amp;t in the twentieth century</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Feigenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Gross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>National Bureau of Economic Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Felten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Seamans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.01157</idno>
		<title level="m">How will language modelers like chatgpt affect occupations and industries? arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A method to link advances in artificial intelligence to occupational abilities</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Felten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Seamans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AEA Papers and Proceedings</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="54" to="57" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The technology trap</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Technology Trap</title>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The future of employment: How susceptible are jobs to computerisation?</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technological Forecasting and Social Change</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="254" to="280" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Could machine learning be a general purpose technology? a comparison of emerging technologies using data from online job postings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Teodoridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research Policy</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">104653</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Musser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Diresta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gentzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sedova</surname></persName>
		</author>
		<title level="m">Generative language models and automated influence operations: Emerging threats and potential mitigations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">When will ai exceed human performance? evidence from ai experts</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salvatier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dafoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="729" to="754" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01293</idno>
		<title level="m">Scaling laws for transfer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Large language models as simulated economic agents</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Horton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.07543</idno>
	</analytic>
	<monogr>
		<title level="m">What can we learn from homo silicus? arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Artificial intelligence in service</title>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of service research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="172" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Changes in relative wages, 1963-1987: supply and demand factors</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The quarterly journal of economics</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="78" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">A hazard analysis framework for code synthesis large language models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brundage</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Ai and shared prosperity</title>
		<author>
			<persName><forename type="first">K</forename><surname>Klinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIES 2021 -Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Technology, vintage-specific human capital, and labor displacement: Evidence from linking patents with occupations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Papanikolaou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D W</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Seegmiller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>National Bureau of Economic Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Working Paper 29552</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Language models and cognitive automation for economic research</title>
		<author>
			<persName><forename type="first">A</forename><surname>Korinek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>National Bureau of Economic Research</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Artificial intelligence and its implications for income distribution and unemployment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Korinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Stiglitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The economics of artificial intelligence: An agenda</title>
		<imprint>
			<publisher>University of Chicago Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="349" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Economic transformations: general purpose technologies and long-term economic growth</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Lipsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Carlaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Bekar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Oup</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Meindl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mendon?a</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.13317</idno>
		<title level="m">Exposure of occupations to technologies of the fourth industrial revolution</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dess?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nalmpantis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pasunuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raileanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.07842</idno>
		<title level="m">Augmented language models: a survey</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Uneven growth: Automation&apos;s impact on income and wealth inequality</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rachel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Restrepo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SSRN Electronic Journal</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">New modes of learning enabled by ai chatbots: Three methods and assignments</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Mollick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mollick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Experimental evidence on the productivity effects of generative artificial intelligence</title>
		<author>
			<persName><forename type="first">S</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<idno>SSRN 4375283</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">O*net 27.2 database. OpenAI (2022)</title>
		<author>
			<persName><forename type="first">O*</forename><surname>Net</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Introducing chatgpt</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Gpt-4 system card</title>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023a</date>
		</imprint>
		<respStmt>
			<orgName>OpenAI</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<title level="m">Gpt-4 technical report</title>
		<imprint>
			<date type="published" when="2023">2023b</date>
		</imprint>
		<respStmt>
			<orgName>OpenAI</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.02155</idno>
		<title level="m">Training language models to follow instructions with human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kalliamvakou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cihon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Demirer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.06590</idno>
		<title level="m">The impact of ai on developer productivity: Evidence from github copilot</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">1 in 4 companies have already replaced workers with chatgpt</title>
		<author>
			<persName><surname>Resumebuilder</surname></persName>
		</author>
		<author>
			<persName><surname>Com</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Engineering value: The returns to technological talent and investments in artificial intelligence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rock</surname></persName>
		</author>
		<idno>SSRN 3427412</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dess?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raileanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cancedda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scialom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.04761</idno>
		<title level="m">Toolformer: Language models can teach themselves to use tools</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Large pre-trained language models contain human-like biases of what is right and wrong to do</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schramowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Turan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Rothkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="258" to="268" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Generalized task markets for human and machine computation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shahaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning to hire teams</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Human Computation &amp; Crowdsourcing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Solaiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kreps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mccain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Newhouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blazakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcguffie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Release strategies and the social impacts of language models</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">An information-theoretic approach to prompt engineering without ground truth labels</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rytting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Delorey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fulda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wingate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08239</idno>
		<title level="m">Lamda: Language models for dialog applications</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Measuring the occupational impact of ai: tasks, cognitive abilities and ai benchmarks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pesole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mart?nez-Plumed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fern?ndez-Mac?as</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hern?ndez-Orallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>G?mez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="191" to="236" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Wage inequality, technology and trade: 21st century evidence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van Reenen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Labour economics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="730" to="741" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">The impact of artificial intelligence on the labor market. Working paper</title>
		<author>
			<persName><forename type="first">M</forename><surname>Webb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<pubPlace>Stanford University</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Ethical and social risks of harm from language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Weidinger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04359</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>cs</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Taxonomy of risks posed by language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kasirzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Biles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stepleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Legassick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gabriel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;22</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="214" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Advanced technologies adoption and use by us firms: Evidence from the annual business survey</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zolas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brynjolfsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mcelheran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Beede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buffington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goldschlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dinlersoz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>National Bureau of Economic Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
