<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CIF: CONTINUOUS INTEGRATE-AND-FIRE FOR END-TO-END SPEECH RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Linhao</forename><surname>Dong</surname></persName>
							<email>donglinhao2015@ia.ac.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
							<email>xubo@ia.ac.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CIF: CONTINUOUS INTEGRATE-AND-FIRE FOR END-TO-END SPEECH RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>continuous integrate-and-fire</term>
					<term>end-to-end model</term>
					<term>soft and monotonic alignment</term>
					<term>online speech recognition</term>
					<term>acoustic boundary positioning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel soft and monotonic alignment mechanism used for sequence transduction. It is inspired by the integrate-and-fire model in spiking neural networks and employed in the encoder-decoder framework consists of continuous functions, thus being named as: Continuous Integrate-and-Fire (CIF). Applied to the ASR task, CIF not only shows a concise calculation, but also supports online recognition and acoustic boundary positioning, thus suitable for various ASR scenarios. Several support strategies are also proposed to alleviate the unique problems of CIF-based model. With the joint action of these methods, the CIF-based model shows competitive performance. Notably, it achieves a word error rate (WER) of 2.86% on the test-clean of Librispeech and creates new state-of-the-art result on Mandarin telephone ASR benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Automatic speech recognition (ASR) system is undergoing an exciting pathway to be more simplified and accurate with the spring up of various end-to-end models. Among them, the attention-based model <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, which builds a soft alignment between each decoder step and every encoder step, not only shows a performance advantage in comparison with other end-to-end models <ref type="bibr" target="#b2">[3]</ref>, but also successfully challenges the dominance of HMM-LSTM Hybrid system in ASR <ref type="bibr" target="#b3">[4]</ref>. However, despite the superiority of accuracy, such attention-based model often encounters incompetent scenarios in real ASR application: 1) it cannot support online (or streaming) recognition since it need refer to the entire encoded sequence; 2) it cannot well timestamp the recognition result since it's not frame-synchronous. Besides, attending to every encoder steps is bound to bring a mass of unnecessary computations on steps that are acoustically irrelevant to the decoding step. Focusing on solving above problems, we aim at seeking a soft alignment which not only performs an efficient monotonic calculation but also locates acoustic boundaries. And we find inspirations from the integrate-and-fire model <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>Integrate-and-fire is one of the earliest models in spiking neural networks (SNNs), which are more bio-plausible and known as the next generation of neural networks <ref type="bibr" target="#b6">[7]</ref>. The integrate-and-fire neuron operates using spikes, which are discrete events that take place at points in time. Specifically, it forwardly integrates the stimulations in the input signal (e.g. spike train), and its membrane potential changes accordingly. When the potential reaches a specific threshold, it fires a spike that will stimulate other neurons, and its potential is reset. It's not hard to find that: 1) such integrate-and-fire process is strictly monotonic; 2) the fired spikes could be used to represent the events that locate an acoustic boundary. By transferring the idea of integrate-and-fire to the end-to-end ASR, we could imagine such an alignment mechanism: it forwardly integrates the information in acoustic signals, once a boundary is located, it instantly fires the integrated acoustic information for further recognition. And the difficulty of achieving it lies in how to simulate the process of integrateand-fire using continuous functions that support back-propagation.</p><p>In this paper, we propose Continuous Integrate-and-Fire (CIF), a novel soft and monotonic alignment employed in the encoderdecoder framework. At each encoder step, it receives the vector representation of current encoder step and a corresponding weight that scales the amount of information contained in the vector. Then, it forwardly accumulates the weights and integrates the vector information until the accumulated weight reaches a threshold, which means an acoustic boundary is located. At this point, the acoustic information of current encoder step is shared by two adjacent labels, thus CIF divides the information into two part: the one for completing the integration of current label and the other for the next integration, which mimics the processing of the integrate-and-fire model when it fires at some point during the period of a encoder step. Then, it fires the integrated acoustic information to the decoder to predict current label. Such process is sketched in Fig. <ref type="figure" target="#fig_0">1 (b</ref>) and is performed till to the end of the encoded sequence.</p><p>We also present several supporting strategies to refine the performance of CIF-based model, including: 1) a scaling strategy to solve the problem of unequal length between the predicted labels and the targeted labels in the cross-entropy training; 2) a quantity loss to supervise the model to predict the quantity of labels closer to the targets; 3) a tail handling method to process the residual information at the end of inference. With the joint action of these methods, our CIF-based model shows impressive performance on multiple ASR datasets covering different languages and speech types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATION TO PRIOR WORK</head><p>Several prior works have also studied the soft and monotonic alignment in end-to-end ASR models. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>  to be a forward-moving window that fits gaussian distribution <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> or even heuristic rule <ref type="bibr" target="#b9">[10]</ref>, where the center and width of the window are predicted by its decoder state. Comparing with them, CIF neither follows a given assumption nor uses the state of the decoder, thus encouraging more pattern learning from the acoustic data.</p><p>Besides, CIF provides a concise calculation process by conducting the locating and integrating at the same time, rather than <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> which need two separate steps of first using a hard monotonic attention to decide when to stop and then performing soft attention to calculate, also rather than <ref type="bibr" target="#b12">[13]</ref> which needs a CTC trained model to conduct pre-partition before the attention decoding.</p><p>In <ref type="bibr" target="#b13">[14]</ref>, Li al. present the important Adaptive Computation Steps (ACS) algorithm whose motivation is to dynamically decide a block of frames to predict a linguistic output. In comparison, CIF holds a different motivated perspective -'integrate-and-fire', and models at a finer time granularity to process the firing phenomenon widely existed inside the encoded frames. Besides, the processing of CIF ensures the full utilization of acoustic information and lays a foundation for the effective application of its supporting strategies, which are what the ACS (that has a huge performance gap from a HMM-DNN model) lacks of.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Continuous Integrate-and-Fire</head><p>Continuous Integrate-and-Fire (CIF) is a soft and monotonic alignment employed in the encoder-decoder framework. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, CIF connects the encoder and decoder. At each encoder step u, it receives two inputs: 1) current output (state) of encoder: hu; 2) current weight: αu, which scales the amount of information contained in hu. Then, it forwardly accumulates the received weights and integrates the received states (using the form of 'weighted sum') until the accumulated weight reaches a given threshold β, which means an acoustic boundary is located. At this point, the information of current encoder step is shared by current label yi and the next label, thus CIF divides current weight αu into two part: the one is used to fulfill the integration of current label yi by building a com-</p><formula xml:id="formula_0">α 1 ℎ 1 α 2 ℎ 2 α 3 ℎ 3 α 4 ℎ 4 α 5 ℎ 5 × α 21 α 22 × × × × α 41 α 42 × × ＋ ＋ ＋ ＋ … … … 𝑐 1 𝑐 2</formula><p>Fig. <ref type="figure">3</ref>. Illustration of the calculation of CIF on an encoded sequence h = (h1, h2, h3, h4, h5, . . .) with predicted weights α = (0.2, 0.9, 0.6, 0.6, 0.1, . . .). The integrated embedding</p><formula xml:id="formula_1">c1 = 0.2 * h1 + 0.8 * h2, c2 = 0.1 * h2 + 0.6 * h3 + 0.3 * h4.</formula><p>plete distribution (whose sum of weights is 1.0) on relevant encoder steps, the other is used for the integration of next label. After that, it fires the integrated embedding ci (as well as the context vector) to the decoder to predict the corresponding label yi. The above process is roughly presented in Fig. <ref type="figure">3</ref> and is performed till to the end of encoded sequence. The complete algorithm is detailed in Algorithm 1, where the threshold β is recommended to be 1.0.</p><p>Algorithm 1: Continuous Integrate-and-Fire (CIF) Input: The outputs of encoder h = (h1, . . . , hu, . . . , hU ) and the corresponding weights α = (α1, . . . , αu, . . . , αU ), the threshold β; Output: The integrated embeddings (corresponding to the output labels): c = (c1, . . . , ci, . . . , cS); 17 return c = (c1, . . . , ci, . . . , cS);</p><formula xml:id="formula_2">1 Initialize i = 1, initial accumulated weight α a 0 = 0, initial accumulated state h a 0 = 0; 2 for u = 1; u &lt;= U ; u + + do 3 //</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Supporting Strategies for CIF-based Model</head><p>We also present some support strategies for the CIF-based model to alleviate its unique problems during training and inference: Scaling Strategy: In the training, the length S of the produced integrated embeddings c may differ from the length S of targets ỹ, thus bringing difficulties to the cross-entropy training that better to be 'one-to-one'. To solve it, we propose a scaling strategy, which multiplies the calculated weights α = (α1, α2, ..., αU ) by a scalar S U u=1 αu to generate the scaled weights α = (α1 , α2 , ..., αU ) whose sum is equal to S, thus teacher-forcing CIF to produce c with length S and driving more effective training.</p><p>Quantity Loss: We also present an optional loss function to supervise the CIF-based model to predict quantity of integrated embeddings closer to the quantity of targeted labels. We term it as quantity loss LQUA, which is defined as U u=1 αu − S , where S is the length of the targets ỹ. By providing the quantity constraints, this loss not only promotes the learning of acoustic boundary positioning, but also alleviates the performance degradation after removing the scaling strategy in the inference.</p><p>Tail Handling: In the inference, the tail leaves some useful information that is not enough to trigger one firing. Directly discarding this information causes the incomplete results (e.g. incomplete words in ASR) at the tail. To alleviate such problem, we utilize a rounding method which makes an additional firing if the residual weight is greater than 0.5 during inference. Besides, we also introduce a label &lt;EOS&gt; to the tail of target sequence to teach the model to predict the end of sentence and more importantly, provide cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Structure</head><p>Fig. <ref type="figure" target="#fig_1">2</ref> shows the architecture of our CIF-based model used for ASR but lacks some details of the model structure. Here, we give these details and introduce some new characteristics of the CIF-based model:</p><p>Encoder: Our encoder follows the encoder structure in <ref type="bibr" target="#b14">[15]</ref>, which uses a two-layer convolutional front-end followed by a pyramid self-attention networks (SANs) and reduces the time resolution to 1/8. Forward encoding for online recognition is achieved by applying the chunk-hopping mechanism in <ref type="bibr" target="#b14">[15]</ref>. As an aside, adjusting the encoding resolution enables CIF suitable for various tasks, e.g. we could use up-sampling to generate longer encoded sequence than outputs to make CIF apply to text-to-speech (TTS), etc.</p><p>To calculate the weight αu corresponding to each encoded output hu, we pass a window centered at hu (e.g. [hu−1, hu, hu+1]) to a 1-dimensional convolutional layer and then a fully connected layer with one output unit and a sigmoid activation, where the convolutions can be replaced by other neural networks.</p><p>Decoder: Two versions of decoder are introduced in this work: the one an autoregressive decoder, which follows the decoder structure in <ref type="bibr" target="#b14">[15]</ref>. Specifically, it first projects the concatenation of the embedding (ei−1) of the previous label and the previously integrated embedding (ci−1) as the input of SANs. Then, it concatenates the output of SANs (oi) and currently integrated embedding (ci) and then projects the concatenation to obtain the logit.</p><p>The other is a non-autoregressive decoder, which directly passes the currently integrated embedding (ci) to the SANs to get the output (oi) that is then projected to get the logit. Compared with the autoregressive decoder, it has higher computational parallelization and could provide inference speedups for the offline ASR where the integrated embeddings can be calculated by CIF at once.</p><p>Loss Functions: In the training, the SAN-based encoder and decoder provide high parallelization to the teacher-forcing learning of CIF-based model, where the encoding, the CIF calculation (which is lightweight since it just performs the weighted calculation and has no trainable parameters) and the decoding are performed in order. To further boost the model learning, in addition to the cross-entropy loss LCE, two optional auxiliary loss functions are applied: one of them is the quantity loss LQUA in section 3.2, the other is the CTC loss LCT C , which is applied on the encoder (similar to <ref type="bibr" target="#b15">[16]</ref>) and addresses the left-to-right acoustic encoding. When using these two optional loss, our model is trained under the loss L as follows:</p><formula xml:id="formula_3">L = LCE + λ1LCT C + λ2LQUA<label>(1)</label></formula><p>where λ1 and λ2 are tunable hyper-parameters. The importance of the two optional loss functions are explored in section 5.2. LM Incorporation: In the inference, we first perform beam search on the output distributions predicted by the decoder, then use a SAN-based language model (LM) to perform second-pass rescor-ing as <ref type="bibr" target="#b3">[4]</ref>, which determines the final transcript y * as follows:</p><formula xml:id="formula_4">y * = arg max y∈NBest(x,N ) ( log P (y|x) + γ log PLM (y)) (2)</formula><p>where γ is a tunable hyper-parameter, NBest(x, N ) is the hypotheses produced by the CIF-based model via beam search with size N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL SETUP</head><p>We experiment on three public ASR datasets including the popular English read-speech corpus (Librispeech <ref type="bibr" target="#b16">[17]</ref>), current largest Mandarin read-speech corpus (AISHELL-2 <ref type="bibr" target="#b17">[18]</ref>) and the Mandarin telephone ASR benchmark (HKUST <ref type="bibr" target="#b18">[19]</ref>). For Librispeech, we use all the train data (960 hours) for training, mix the two development sets for validation, use the two test sets for evaluation, and use the separately prepared language model (LM) data for the training of LM.</p><p>For AISHELL-2, we use all the train data (1000 hours) for training, mix the three development sets for validation and use the three test sets for evaluation. For HKUST, we use the same training (∼168 hours), validation and evaluation set as <ref type="bibr" target="#b14">[15]</ref>. The training of LM on AISHELL-2 and HKUST uses the text from respective training set. We extract input features using the same setup as <ref type="bibr" target="#b14">[15]</ref> for all datasets. Speed perturbation <ref type="bibr" target="#b19">[20]</ref> with fixed ± 10% is applied for all training datasets. The frequency masking and time masking in <ref type="bibr" target="#b20">[21]</ref> with F = 8, mF = 2, T = 70, mT = 2, p = 0.2 are applied to all models except the base model on Librisppech. We use the BPE <ref type="bibr" target="#b21">[22]</ref> toolkit generating 3722 word pieces for Librispeech by merging 7500 times on its training text, plus three special labels: the blank &lt;BLK&gt;, the end of sentence &lt;EOS&gt; and the pad &lt;PAD&gt;, the number of output labels is 3725 for Librispeech. We collect the characters and markers from the training text of AISHELL-2 and HKUST, respectively. Plus the three special labels, we generate 5230 output labels for AISHELL-2 and 3674 output labels for HKUST .</p><p>We implement our model on TensorFlow <ref type="bibr" target="#b22">[23]</ref>. For the selfattention networks (SANs) in our model, we use the structure in <ref type="bibr" target="#b14">[15]</ref> and set h = 4, d model = 640, d f f = 2560 for the two Mandarin datasets, and change (d model , d f f ) to (512, 2048), (1024, 4096) for the base, big model on Librispeech, respectively. For the encoder, we use the same configures as <ref type="bibr" target="#b14">[15]</ref>, where n in the pyramid structure is all set to 5. The chunk-hopping <ref type="bibr" target="#b14">[15]</ref> for forward encoding uses the chunk size of 256 (frames) and the hop size of 128 (frames). For the 1-dimensional convolutional layer that predicts weights, the number of filters is set to d model , and the window width is all set to 3 except the base model on Librispeech is set to 5. Besides, layer normalization <ref type="bibr" target="#b23">[24]</ref> and a ReLU activation are applied after this convolution. For CIF, we set the threshold β to 0.9 for all models to allow the possible firing after a single step. But it may produce few negative weight after dividing weight, which is non-intuitive in weighted sum calculation. Here, we recommend β = 1.0, which calculates intuitively and performs slightly better than β = 0.9 in our later experiments. For the decoder, the number of SANs is all set to 2 except the base model on Librispeech is set to 3. The loss hyper-parameter λ1 is set to 0.5 for two Mandarin datasets and to 0.25 for Librispeech, λ2 is all set to 1.0. The LM uses SANs with h = 4, d model = 512, d f f = 2048, and the number of SAN layers is set to 3, 6, 20 for HKUST, AISHELL-2 and Librispeech, respectively.</p><p>In the training, we only apply dropout to the SANs, whose attention dropout and residual dropout are all set to 0.2 except the base model on Librispeech that is set to 0.1. We use the uniform label smoothing in <ref type="bibr" target="#b24">[25]</ref> and set it to 0.2 for both of the CIF-based model and the LM. Scheduled Sampling <ref type="bibr" target="#b25">[26]</ref> with a constant sampling rate of 0.5 is applied on two Mandarin datasets. In the inference, we use beam search with size 10. The hyper-parameter γ for LM rescoring is set to 0.1, 0.2, 0.9 for HKUST, AISHELL-2 and Librispeech, respectively. All experimental results are averaged over 3 runs.</p><p>We display the aligned results (the located boundaries) of CIF on https://linhodong.github.io/cif_alignment/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results on Librispeech</head><p>On the Librispeech dataset, we use the word pieces as the output labels. Since the word pieces are obtained without referring to any acoustic knowledge, the acoustic boundary between adjacent labels may be blurred. Even so, our big CIF-based model still achieves a word error rate (WER) of 2.86% on test-clean and 8.08% on testother (as shown in Table <ref type="table" target="#tab_1">1</ref>), which not only shows a clear performance advantage than other soft and monotonic models (e.g. triggered attention <ref type="bibr" target="#b12">[13]</ref>), but also matches or surpasses most of the published results of end-to-end models.</p><p>By fine-tuning the trained big model via the chunk-hopping [15] mechanism, we enables our big model that uses a SAN encoder to support online recognition. As shown in Table <ref type="table" target="#tab_1">1</ref>, the online model obtains a WER of 3.25% on test-clean and 9.63% on test-other. Besides, the above CIF-based models all apply a very low encoded frame rate (12.5 Hz) for reducing the computational burden. Switching to a higher frame rate may further improve their performance. As shown in Table <ref type="table" target="#tab_2">2</ref>, ablating the auto-regression in the decoder causes the largest performance degradation. To further verify this phenomenon, we compare the models with/without auto-regression on the Mandarin dataset of AISHELL-2 but find they achieve comparable performance. Since the acoustic boundaries between Mandarin characters are much more clear, we suspect the importance of auto-regression is related to the clearness of acoustic boundary between output labels. Besides, the proposed support strategies (scaling strategy, quantity loss and tail handling) for the CIF-based model all provide clear improvements. Among them, the quantity loss is the most important since ablating it causes the largest performance loss and brings large learning instability. The introduced CTC loss also benefits to the CIF-based model but not as important as others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results on AISHELL-2</head><p>On the AISHELL-2 dataset, we use the characters of Mandarin as the output labels. Since every character of Mandarin is single syllable and AISHELL-2 is a read-speech dataset, the acoustic boundary between labels are clear. Consistent with our expectations, the CIFbased model performs very competitive on all test sets and significantly improves the results achieved by the Chain model <ref type="bibr" target="#b32">[33]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model CER</head><p>Chain-TDNN <ref type="bibr" target="#b32">[33]</ref> 23.7 Self-attention Aligner <ref type="bibr" target="#b14">[15]</ref> 24.1 Transformer <ref type="bibr" target="#b33">[34]</ref> 26.6 Extended-RNA <ref type="bibr" target="#b34">[35]</ref> 26.8 Joint CTC-attention model / ESPNet <ref type="bibr" target="#b15">[16]</ref> 27.4 Triggered Attention <ref type="bibr" target="#b12">[13]</ref> 30 where the membrane potential Um is constantly simulated by the input spikes I in the period of dt, C is a constant. Similarly, the dynamics of CIF can be described as f (h) = dα a /dt, which follows the basic dynamic form of the IF model but differs at one aspect: CIF regards the information in the period of dt as a whole and uses continuous values to represent and process. Specifically, CIF uses a vector h to directly represent the inputs in dt and a continuous function f() to directly calculate the change of α a brought by the inputs. This coarse-grained dynamics determines CIF needs to divide the information when it produces a firing in the period of a encoder step (dt).</p><p>In the future, mimicking the dynamics of other models in spiking neural networks may be a way to improve CIF. At the application level, CIF not only shows competitive performance on popular ASR benchmarks, but also could extract acoustic embeddings (which may be useful in multimodal tasks, etc.) in a concise way. In addition, CIF could support various sequence transduction tasks (e.g. TTS) by using a suitable encoding resolution. In the future, we will further verify the performance of CIF-based model on larger-scale ASR datasets and other tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the attention alignment and our proposed CIF alignment on an encoded utterance of length 5 and labelled as "CAT". The shade of gray in each square represents the weight of each encoder step involved in the calculation of decoding labels. The vertically dashed line in (b) represents the located acoustic boundary, and the weight of the boundary step is divided into two parts used for the calculation of the two adjacent labels, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The architecture of our CIF-based model used for the ASR task. Operations in the dashed rectangles are only applied in the training stage. The switch (S) before the CIF module connects the left in the training stage and the right in the inference stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>AND CONCLUSION At the theoretical level, CIF simulates the dynamic characteristics of the integrate-and-fire (IF) model on artificial neural networks. The IF model has the dynamics of I = C * (dUm/dt),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>calculate current accumulated weight;</figDesc><table><row><cell>4 5</cell><cell>α a u = α a u−1 + αu; if α a u &lt; β then</cell></row><row><cell>6 7</cell><cell>// no boundary is located; h a u = h a u−1 + αu  *  hu;</cell></row><row><cell>8</cell><cell>else</cell></row><row><cell>9</cell><cell>// a boundary is located;</cell></row></table><note>10// αu is divided into two part, the first part αu1 is used to fulfill the integration of current label yi;11 αu1 = 1 − α a u−1 ; 12 ci = h a u−1 + αu1 * hu;13 i++; 14 // The other part αu2 is used for the next integration; 15 α a u = αu2 = αu − αu1; 16 h a u = αu2 * hu;</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparison with other end-to-end models on Librispeech, word error rate (WER) (%)</figDesc><table><row><cell></cell><cell cols="2">test-clean</cell><cell cols="2">test-other</cell></row><row><cell>Model</cell><cell cols="4">w/o LM w/ LM w/o LM w/ LM</cell></row><row><cell>LAS + SpecAugment [21]</cell><cell>2.8</cell><cell>2.5</cell><cell>6.8</cell><cell>5.8</cell></row><row><cell>Attention + Tsf LM [27]</cell><cell>4.4</cell><cell>2.8</cell><cell>13.5</cell><cell>9.3</cell></row><row><cell>Jasper [28]</cell><cell>3.86</cell><cell cols="3">2.95 11.95 8.79</cell></row><row><cell>wav2letter++ [29]</cell><cell>-</cell><cell>3.44</cell><cell>-</cell><cell>11.24</cell></row><row><cell>Cnv Cxt Tsf [30]</cell><cell>4.7</cell><cell>-</cell><cell>12.9</cell><cell>-</cell></row><row><cell>CTC + SAN [31]</cell><cell>-</cell><cell>4.8</cell><cell>-</cell><cell>13.1</cell></row><row><cell>CTC + Policy [32]</cell><cell>-</cell><cell>5.42</cell><cell>-</cell><cell>14.70</cell></row><row><cell>Triggered Attention [13]</cell><cell>7.4</cell><cell>5.7</cell><cell>19.2</cell><cell>16.1</cell></row><row><cell>CIF + SAN (base)</cell><cell>4.48</cell><cell cols="3">3.68 12.62 10.89</cell></row><row><cell>CIF + SAN (big)</cell><cell>3.41</cell><cell>2.86</cell><cell>9.28</cell><cell>8.08</cell></row><row><cell cols="2">+ Chunk-hopping (online) 3.96</cell><cell cols="3">3.25 11.19 9.63</cell></row><row><cell cols="2">5.2. Ablation Study on Librispeech</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">In this section, we use ablation study to evaluate the importance of</cell></row><row><cell cols="4">different methods applied to the CIF-based model.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on Librispeech, where the full model is our base CIF-based model (without LM) in Table.1, WER (%)</figDesc><table><row><cell></cell><cell cols="2">test-clean test-other</cell></row><row><cell>without scaling strategy</cell><cell>6.03</cell><cell>14.98</cell></row><row><cell>without quantity loss</cell><cell>8.84</cell><cell>15.49</cell></row><row><cell>without tail handling</cell><cell>6.04</cell><cell>14.11</cell></row><row><cell>without CTC loss</cell><cell>4.96</cell><cell>13.27</cell></row><row><cell>without autoregressive</cell><cell>9.27</cell><cell>21.56</cell></row><row><cell>Full Model</cell><cell>4.48</cell><cell>12.62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison with the previously published results on AISHELL-2, character error rate (CER) (%) On the HKUST dataset, the speech are all Mandarin telephone conversations, which are more challenging to recognize than readspeech due to the spontaneous and informal speaking style. Besides, the amount of training data on HKUST is smaller. Nevertheless, the CIF-based model still shows good generalization and creates new state-of-the-art result on this benchmark dataset.</figDesc><table><row><cell>Model</cell><cell cols="3">test android test ios test mic</cell></row><row><cell>Chain-TDNN [33]</cell><cell>9.59</cell><cell>8.81</cell><cell>10.87</cell></row><row><cell>CIF + SAN</cell><cell>6.17</cell><cell>5.78</cell><cell>6.34</cell></row><row><cell>+ Chunk-hopping (online)</cell><cell>6.52</cell><cell>6.04</cell><cell>6.68</cell></row><row><cell>5.4. Results on HKUST</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison with the previously published results on HKUST, CER (%)</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A comparison of sequence-tosequence models for speech recognition</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leif</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">State-of-theart speech recognition with sequence-to-sequence models</title>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Gonina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recherches quantitatives sur l&apos;excitation electrique des nerfs traitee comme une polarization</title>
		<author>
			<persName><forename type="first">Louis</forename><surname>Lapicque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal de Physiologie et de Pathologie Generalej</title>
		<imprint>
			<date type="published" when="1907">1907</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lapicque&apos;s introduction of the integrate-andfire model neuron (1907)</title>
		<author>
			<persName><forename type="first">Abbott</forename><surname>Larry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain research bulletin</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Networks of spiking neurons: The third generation of neural network models</title>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Maass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Gaussian prediction based attention for online end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Junfeng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Rong</forename><surname>Dai</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Local monotonic attention mechanism for end-to-end speech and language processing</title>
		<author>
			<persName><forename type="first">Andros</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sakriani</forename><surname>Sakti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCNLP</title>
				<meeting>the IJCNLP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An analysis of local monotonic attention variants</title>
		<author>
			<persName><forename type="first">Andr</forename><surname>Merboldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Schlter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Monotonic chunkwise attention</title>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An online attention-based model for speech recognition</title>
		<author>
			<persName><forename type="first">Ruchao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>INTER-SPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Triggered attention for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Niko</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end speech recognition with adaptive computation steps</title>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hattori</forename><surname>Masanori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-attention aligner: A latency-control end-to-end model for asr using self-attention network and chunk-hopping</title>
		<author>
			<persName><forename type="first">Linhao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint ctcattention based end-to-end speech recognition using multi-task learning</title>
		<author>
			<persName><forename type="first">Suyoun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Aishell-2: Transforming mandarin asr research into industrial scale</title>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuechen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Bu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hkust/mts: A very large scale mandarin telephone speech corpus</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongsheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Cieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shudong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Spoken Language Processing</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the ISCA</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName><forename type="first">William</forename><surname>Daniel S Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>INTERSPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
				<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Tensorflow: Largescale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Towards better decoding and language model integration in sequence to sequence models</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>INTERSPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Rwth asr systems for librispeech: Hybrid vs attention</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Lüscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugen</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Kitza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilfried</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>INTERSPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Jasper: An end-to-end convolutional neural acoustic model</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Lavrukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huyen</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Teja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gadde</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>INTERSPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">wav2letter++: The fastest open-source speech recognition system</title>
		<author>
			<persName><forename type="first">Awni</forename><surname>Vineel Pratap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiantong</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaliy</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName><surname>Collobert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Transformers with convolutional context for asr</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Selfattention networks for connectionist temporal classification in speech recognition</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving end-to-end speech recognition with policy learning</title>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Purely sequence-trained neural networks for asr based on lattice-free mmi</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pegah</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vimal</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
		<editor>INTERSPEECH</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A comparison of modeling units in sequence-to-sequence speech recognition with the transformer on mandarin chinese</title>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linhao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Extending recurrent neural aligner for streaming end-to-end speech recognition in mandarin</title>
		<author>
			<persName><forename type="first">Linhao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>INTERSPEECH</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
