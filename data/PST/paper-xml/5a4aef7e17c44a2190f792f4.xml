<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Unified Spatiotemporal Prior based on Geodesic Distance for Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
							<email>wenguanwang@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">• W. Wang and J. Shen are with Beijing Laboratory of Intelligent Information Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
							<email>shenj@vision.ee.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">• W. Wang and J. Shen are with Beijing Laboratory of Intelligent Information Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
							<email>ryang@cs.uky.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">• W. Wang and J. Shen are with Beijing Laboratory of Intelligent Information Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
							<email>fatih.porikli@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">• W. Wang and J. Shen are with Beijing Laboratory of Intelligent Information Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Research School of Engineering</orgName>
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">•</forename><forename type="middle">R</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Kentucky</orgName>
								<address>
									<postCode>40507</postCode>
									<settlement>Lexington</settlement>
									<region>KY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Unified Spatiotemporal Prior based on Geodesic Distance for Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AF7528A129309D11C750C983410D0B9D</idno>
					<idno type="DOI">10.1109/TPAMI.2017.2662005</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2017.2662005, IEEE Transactions on Pattern Analysis and Machine Intelligence</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video saliency</term>
					<term>video object segmentation</term>
					<term>geodesic distance</term>
					<term>spatiotemporal prior</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video saliency, aiming for estimation of a single dominant object in a sequence, offers strong object-level cues for unsupervised video object segmentation. In this paper, we present a geodesic distance based technique that provides reliable and temporally consistent saliency measurement of superpixels as a prior for pixel-wise labeling. Using undirected intra-frame and inter-frame graphs constructed from spatiotemporal edges or appearance and motion, and a skeleton abstraction step to further enhance saliency estimates, our method formulates the pixel-wise segmentation task as an energy minimization problem on a function that consists of unary terms of global foreground and background models, dynamic location models, and pairwise terms of label smoothness potentials. We perform extensive quantitative and qualitative experiments on benchmark datasets. Our method achieves superior performance in comparison to the current state-of-the-art in terms of accuracy and speed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>U NSUPERVISED video object segmentation, a key challenge in computer vision, aims at partitioning multiple video frames into objects and background regions. Such an automatic segmentation has been shown to benefit a variety of applications such as video summarization, video compression, content based video retrieval and human-computer interaction, to name a few.</p><p>Traditionally, video object segmentation task is performed with motion and appearance information represented by motion vectors, feature point trajectories, color descriptors, and boundary indicators. Depending on the availability and quality of these inputs, object regions are usually obtained after complicated and fragile inference procedures often with preset assumptions of object and camera motion. In simple scenarios where the foreground object moves distinctly from its background, grouping of motion vectors and feature point trajectories generates semantically meaningful segments. Several works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> analyzed point trajectories to leverage the motion information. But, what about if a part of the object remains static? In typical complex videos, the assumption of motion consistency may result in oversegmentation, thus failing to extract entire object regions. Utilizing both motion and appearance cues seems to be a better choice as it was adopted by many methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. Specially, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> generate a large number of object proposals <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> in every frame using these cues, and cast the task of video object segmentation as the problem of inferring and selecting the most relevant object proposal. In the selection process, both motion and appearance information are combined in measuring the objectness of proposals where various assessment strategies are introduced.</p><p>However, all these approaches still face many difficulties. On one hand, they all require complicated object inference techniques, which comes with a high computational expense. On the other hand, they impose heuristically chosen cues which may not be the right choice for a general class of objects. Besides, proposal based methods sustain the disadvantage that correct proposals are often few or do not exist at all when the foreground object is small or similar to the background.</p><p>We can ask whether there is any reliable object-level descriptor that can be employed for a general class of video objects. This calls for introducing a robust object-level cue as an indicator of the object in terms of interesting regions in a scene. We address this challenge by giving emphasis to the value of spatiotemporal saliency to automatically identify visually prominent object regions in a video sequence. Our intuition is that potentially discriminative yet confined motion and appearance cues should be combined with more comprehensive spatiotemporal saliency cues in order to generate a reliable object prior. Once a reliable saliency prior is built, estimating refined appearance models and then in turn generating accurate object segments becomes feasible. This motivates us to decompose the automatic segmentation problem into two stages: video saliency detection and video object partitioning.</p><p>For an effective solution to unsupervised video segmentation, we need the capability to detect salient regions in a video. While salient object detection in still images has been exploited in the past, computing spatiotemporal saliency in videos is still an active area of research since extending image based algorithms to video is nontrivial. Temporal coherence yields significant information, nevertheless, it is inevitably susceptible to noise due to nonuniform background motions and well-known motion estimation errors. Moreover, most video saliency methods simply treat the motion Fig. <ref type="figure">1</ref>. Overview of our video object segmentation framework. Input frame is over-segmented into superpixels and a spatiotemporal edge map is produced by the combination of static edge probability and optical flow gradient magnitude. For each superpixel, we compute its object-probability and the refined saliency estimate via intra-frame graph and inter-frame graph, respectively. An object skeleton abstraction method is further derived for obtaining final saliency estimates via biasing the central skeleton regions with higher saliency values. Finally, we combine the spatiotemporal saliency priors, global appearance models and dynamic location models using motion information among few subsequent frames for producing correct pixel-wise segmentation.</p><p>feature as another cue within their image saliency models <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, lacking an elegant framework to incorporate intra-frame and inter-frame information in a unified fashion.</p><p>In this paper, we aim to partition the foreground objects from their backgrounds in all frames of a given video sequence without any user assistance or contextual assumptions. To this end, we propose a video object segmentation method that consists of a superpixel based spatiotemporal saliency prior detection stage and pixel based binary labeling stage that runs in a recursive fashion.</p><p>Our proposed video segmentation framework is depicted in Figure <ref type="figure">1</ref>. We first introduce a unified spatiotemporal saliency prior that combines motion boundaries and spatial edges into a unified model that is designed to align with object boundaries for a simple yet powerful representation. Our model offers a reliable and temporally consistent region-level prior for object segmentation by employing psychophysically motivated low-level features that incorporate spatial stimulus and temporal coherence. Saliency of a region is measured by its shortest geodesic distance to background regions in two inter-frame and intra-frame graphs, which are constructed from the intensity edge and motion boundary cues, as well as the background information across adjacent frames. The geodesic distance has the power of abstracting object structure to efficiently determine its central regions by assigning higher saliency values to more representative regions. It has been shown to be effective for supervised segmentation where user provides seeds <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Such a user interaction, however, is impractical in streaming video applications. Our method extends the geodesic distance into unsupervised video segmentation. Hence, we design a skeleton abstraction method that explicitly incorporates weak object structure and emphasizes the saliency values of the central skeleton regions based on geodesic distances. After obtaining the spatiotemporal saliency prior, we integrate saliency cues, dynamic location models as well as global appearance models into an energy minimization that is optimized via graph-cuts to generate highly accurate foreground object boundaries in entire video segment.</p><p>To summarize, our main contributions are:</p><p>• A unified framework that incorporates video saliency for unsupervised pixel-wise labeling of foreground objects using an energy function that contains three unary and two pair-wise terms (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>A new formulation for spatiotemporal saliency by exploiting intra-frame and inter-frame relevancy via undirected graphs on superpixels. For the intra-frame stimulus, we employ geodesic distance on spatiotemporal edges within a single frame. We construct the inter-frame graph for temporal coherence between consecutive frames (Section 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>A geodesic distance based weighting of intra-frame and inter-frame graphs based on the observation that salient regions have higher geodesic distances to background regions (Section 3.1 and 3.3).</p><p>• A greedy skeleton abstraction scheme for iteratively selecting confident foreground regions (Section 3.4).</p><p>Our method achieves the state-of-art performance on four large benchmarks. This paper builds upon and extends our recent work in <ref type="bibr" target="#b19">[20]</ref> with a more in depth discussion of the algorithm and expanded evaluation results. We further introduce a new geodesic distance based skeleton regions abstraction method that regularizes the original regions of object with higher saliency. The remainder of this paper is organized as follows: An overview of the related work is given in Section 2. The spatiotemporal saliency stage is explained in Section 3. Intermediate processes of the video segmentation method are articulated in Section 4. Experimental evaluations are presented in Section 5. Discussions and limitations are given in Section 6. Concluding remarks can be found in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we give a brief overview of recent works in unsupervised video segmentation and saliency detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Unsupervised Video Segmentation</head><p>A variety of techniques have been proposed for unsupervised video segmentation in the past decade. Most approaches are based on bottom-up models using low-level features such as motion, color, and edge orientation. In particular, the importance of the motion information was emphasized in many works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. While the use of short duration motion boundaries in pairs of subsequent frames is not uncommon <ref type="bibr" target="#b21">[22]</ref>, several methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref> argued that motion should be analyzed over longer periods, as such long term analysis is able to decrease the intra-object variance of motion relative to the interobject variance and propagate motion information to frames in which the object remains static. For this, <ref type="bibr" target="#b1">[2]</ref> grouped pixels with coherent motion computed via long-range motion vectors from the past and future frames. Similarly, the work in <ref type="bibr" target="#b0">[1]</ref> offered a framework for trajectory-based video segmentation through building affinity matrix between pairs of trajectories. In <ref type="bibr" target="#b2">[3]</ref>, discontinuities of embedding density between spatially neighboring trajectories were detected. Incorporating higher order motion models, a clustering method for point tracks was proposed in <ref type="bibr" target="#b22">[23]</ref>. In general, motion based methods suffer difficulties when different parts of an object exhibit nonhomogeneous motion patterns. This problem is exacerbated further with the absence of a strong prior for object. Moreover, these approaches require careful selection of a suitable model especially for the trajectory clustering process, which often comes with a high computation complexity, as <ref type="bibr" target="#b6">[7]</ref> pointed out.</p><p>There were previous efforts <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b23">[24]</ref> that presented optimization frameworks for bottom-up segmentation employing both appearance and motion cues. Several methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> proposed to select primary object regions in object proposal domain based on the notion of what a generic object looks like. These approaches benefit from the work of object hypotheses proposals <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> that offer a large number of object candidates in every frame. Therefore, segmenting video object is transformed into an object region selection problem. In this selection process, both motion and appearance cues are used to measure the objectness of a proposal. More specifically, a clustering process was introduced for finding objects by <ref type="bibr" target="#b6">[7]</ref>, a constrained maximum weight cliques technique to model the selection process was imposed <ref type="bibr" target="#b7">[8]</ref>, and a layered directed acyclic graph based framework was presented by <ref type="bibr" target="#b8">[9]</ref>. Work of <ref type="bibr" target="#b24">[25]</ref> segmented moving objects by ranking spatiotemporal segment proposals with moving objectness detector trained on image and motion fields. In <ref type="bibr" target="#b25">[26]</ref>, tracking and segmentation were integrated into a unified framework to detect the primary object proposal and handle the video segmentation task. The main drawbacks of the proposal based algorithms are their high computational cost associated with proposal generation and complicated object inference schemes. Recently, some methods <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref> were proposed to exploit temporal correlations over the entire video, which produced global optimal segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Saliency Detection for Image and Video</head><p>The human visual system is remarkably effective in localizing the regions in a scene that stand out from their neighbors. Saliency detection <ref type="bibr" target="#b28">[29]</ref> is originally a task of simulating the human visual system for predicting scene locations where a human observer may fixate. Recent research has shown that extracting salient objects or regions is more useful and beneficial to a wide range of computer vision applications. The output of salient object detection is usually a saliency map where the intensity of each pixel represents the probability of that pixel belonging to the salient object.</p><p>Saliency detection methods in general can be categorized as either bottom-up or top-down approaches. Top-down approaches <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> are goal-directed and require an explicit understanding of the context of the image. Supervised learning with a specific class is therefore a frequently adopted principle. Most of the saliency detection methods are based on bottom-up visual attention mechanisms, which are independent of the knowledge of the content in the image and utilize various low level features, such as intensity, color and orientation.</p><p>Inspired by visual perception studies that indicate contrast is a major factor in visual attention mechanisms, numerous bottomup models have been proposed based on different mathematical formulations of contrast. Many methods assumed that globally infrequent features are more salient, and frequency analysis is carried out in the spectral domain. For example, <ref type="bibr" target="#b31">[32]</ref> proposed a saliency detection algorithm using spectral residuals based on the log spectra representation for images. The phase spectrum of the Fourier transform is considered to be the key element in obtaining the location of salient regions in <ref type="bibr" target="#b32">[33]</ref>. Later, <ref type="bibr" target="#b33">[34]</ref> introduced a frequency-tuned approach to estimate center-surround contrast using color and luminance features. Other methods attempted to detect saliency in the spatial domain, usually adopting several visual cues. A graph-based dissimilarity measure was used in <ref type="bibr" target="#b29">[30]</ref>. In <ref type="bibr" target="#b34">[35]</ref>, a content-aware saliency detection with the consideration of the contrast from both local and global perspectives was built. <ref type="bibr" target="#b35">[36]</ref> presented a framework for saliency detection based on the efficient fusion of different feature channels and the local center-surround hypothesis. In <ref type="bibr" target="#b36">[37]</ref>, two saliency indicators, global appearance contrast and spatially compact distribution, were considered. Recently, several approaches <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref> exploited background information, called boundary prior. These methods use image boundaries as background, further enhancing saliency computation.</p><p>While image saliency detection has been extensively studied, computing spatiotemporal saliency for videos is a relatively new problem. Different from image saliency detection, moving objects catch more attention of human beings than static ones, even if the static objects have large contrast to their neighbors. In other words, motion is the most important cue for video saliency detection, which makes deeper exploration of the inter-frame information crucial. Gao et al. <ref type="bibr" target="#b12">[13]</ref> extended their image saliency model <ref type="bibr" target="#b40">[41]</ref> by adding the motion channel for prediction of human eye fixations in dynamic scenes based on the center-surround hypothesis. Similarly, Mahadevan et al. <ref type="bibr" target="#b13">[14]</ref> combined center-surround saliency with the dynamic textures for spatiotemporal saliency using the saliency model in <ref type="bibr" target="#b40">[41]</ref>. In <ref type="bibr" target="#b14">[15]</ref>, Seo et al. computed the so-called local regression kernels from the given video, measuring the likeness of a pixel (or voxel) to its surrounding. They extended their model for video saliency detection straightforwardly by extracting a feature vector from each spatiotemporal 3-D cube. Recently, <ref type="bibr" target="#b4">[5]</ref> used a statistical framework and local feature contrast in illumination, color, and motion for formulating final saliency maps. <ref type="bibr" target="#b41">[42]</ref> proposed a cluster-based saliency method, where three visual attention cues, contrast, spatial, and global correspondence, are devised to measure the cluster saliency. <ref type="bibr" target="#b42">[43]</ref> adopted spacetime saliency to generate a low-frame-rate video from a highframe-rate input using various low-level features and region-based contrast analysis.</p><p>It can be seen that video saliency detection is still an emerging and challenging research problem to be further investigated. The existing methods, however, usually build their system with a simple combination of image saliency models with motion cues, lacking an efficient framework to fully explore intra-frame and inter-frame information together. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SPATIOTEMPORAL SALIENCY PRIOR</head><p>Our video object segmentation method consists of two stages: superpixel based spatiotemporal saliency prior detection and pixel based binary labeling. Here, we explain the saliency stage first.</p><p>To achieve reliable saliency estimation, our method combines psychophysically motivated low-level features, such as color, edge, and motion boundary in a unified geodesic distance based framework. Figure <ref type="figure" target="#fig_0">2</ref> shows the intermediate stages of our video saliency prior. First, input frames are partitioned into superpixels for computational and memory efficiency (Figure <ref type="figure" target="#fig_0">2-b</ref>). We then extract two types of edges: spatial edges (Figure <ref type="figure" target="#fig_0">2-c</ref>) within the same frame, and motion boundary edges (Figure <ref type="figure" target="#fig_0">2-d</ref>) across the neighboring frames. These two features are explicitly integrated into a single spatiotemporal edge map (Figure <ref type="figure" target="#fig_0">2</ref>-e) as described in Section 3.1.</p><p>Geodesic distance is adopted in an intra-frame graph for computing the object probability of each superpixel as given Figure <ref type="figure" target="#fig_0">2</ref>-f. The object probability is computed as the shortest geodesic distance to the frame boundaries and it is based on the observation that salient object areas are often surrounded by the regions with high spatiotemporal edge value. Details of this part can be found in Section 3.2.</p><p>To improve the saliency estimation, a coarse separation of foreground and background is obtained via a self-adaptive thresholding. Then, an inter-frame graph is constructed by computing geodesic distance to the estimated background regions of two adjacent frames as explained in Section 3.3. This graph is used to produce an initial spatiotemporal saliency map as shown in Figure <ref type="figure" target="#fig_0">2-g</ref>.</p><p>We also apply a novel skeleton abstraction method that amplifies the saliency values of the central skeleton regions based on geodesic distances to incorporate weak object structure. As explained in Section 3.4, we assign such central regions higher saliency values in the final spatiotemporal saliency prior, which can be seen in Figure <ref type="figure" target="#fig_0">2</ref>-h.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Spatiotemporal Edge Generation</head><p>Human visual perception <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref> suggest that basic visual features such as motion and edges are processed at the human pre-attentive stage for visual attention, which motivates us to combine spatial edge and motion boundary cues into a coalescent spatiotemporal edge map. Both color and motion discontinuities provide valuable evidence in predicting object boundaries. As shown in Figure <ref type="figure" target="#fig_0">2</ref>, spatial color discontinuities in a single frame and optical flow field estimated from two consecutive frames reveal the important regions of the video frames. We build our approach on these two indicators.</p><p>Given an input video sequence {F 1 , F 2 , • • •}, we compute a spatial edge probability map E k c of k-th frame F k using <ref type="bibr" target="#b45">[46]</ref>. The value of E k c (x), normalized to [0, 1], represents the probability of edge at the corresponding pixel x. The optical flow between the pairs of subsequent frames are obtained by the large displacement motion estimation algorithm <ref type="bibr" target="#b46">[47]</ref>. Let V k be the optical flow field of frame F k . We compute the motion gradient magnitude</p><formula xml:id="formula_0">E k o of V k as E k o (x) = ∇V k (x)</formula><p>. We oversegment each frame into superpixels using SLIC <ref type="bibr" target="#b47">[48]</ref>. In our implements, we set the number of sunperpixels per frame K=1000.</p><formula xml:id="formula_1">Let Y k = {y k 1 , y k 2 , • • •} be the superpixel set of frame F k .</formula><p>Given the pixel edge map E k c , the edge probability of superpixel y k n is computed as the average value of the pixels with 10 largest edge probabilities within y k n . This generates a superpixelwise edge map E k c . Similarly, the optical flow magnitude map E k o is re-computed on superpixel level. Then, we generate a spatiotemporal edge map E k as:</p><formula xml:id="formula_2">E k = E k c • E k o .<label>(1)</label></formula><p>The intuition behind the design of Equation 1 is that, distinct motion patterns and spatial gradients are indicators of the location of salient foreground object. This can be easily observed in Figure <ref type="figure" target="#fig_0">2</ref>e where object superpixels either have high spatiotemporal edge map values or are surrounded by those high valued superpixels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Intra-frame Graph Construction</head><p>To highlight the foreground regions that have high spatiotemporal edge values or are surrounded by regions with high spatiotemporal edge values, we employ the geodesic distance to compute a probability map. The geodesic distance d geo (v 1 , v 2 , G) between any two nodes v 1 , v 2 in graph G is the smallest integral of a weight function W over all possible paths between v 1 and v 2 :</p><formula xml:id="formula_3">d geo (v 1 , v 2 , G) = min Cv 1 ,v 2 v1 v2 |W (m) • Ċv1,v2 (m)|dm, (2) where C v1,v2 (m) is a path connecting the nodes v 1 , v 2 .</formula><p>For frame F k , we construct an undirected weighted graph G k = {V k , E k } with superpixels Y k as nodes V k and the links between adjacent nodes as edges E k . Based on the graph structure, we derive a</p><formula xml:id="formula_4">|V k | × |V k | weight matrix W k , where |V k | is the number of nodes in V k . The (m, n)-th element of W k indicates the weight of edge e k mn ∈ E k between adjacent superpixels Y k m and Y k n : W k mn = E k (y k m ) -E k (y k n ) ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">E k (Y k m ) and E k (Y k n ) correspond to the spatiotemporal boundary probability of superpixels Y k</formula><p>m and Y k n , separately. For superpixel y k n , the probability P k (y k n ) of being foreground is computed by the shortest geodesic distance to the image boundaries using</p><formula xml:id="formula_6">P k (y k n ) = min q∈Q k d geo (y k n , q, G k ),<label>(4)</label></formula><p>where Q k indicate the superpixels along the four boundaries of frame</p><formula xml:id="formula_7">F k . The geodesic distance d geo (v 1 , v 2 , G k ) between any two superpixels v 1 , v 2 ∈ V k in graph G k can be computed in discrete form: d geo (v 1 , v 2 , G k ) = min Cv 1 ,v 2 m,n W k mn , m, n ∈ C v1,v2 . (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>which can be seen as the accumulated edge weights along their shortest path on graph G k . If a superpixel is outside the desired object, its probability value is small because there exists a pathway to image boundaries that does not pass the regions with high spatiotemporal edge value. Whereas, if a superpixel is inside the object, this superpixel is surrounded by the regions with large probabilities of edges, which increases the geodesic distance to image boundaries. We normalize the probability map P k to [0, 1] for each frame. Since our graph is very sparse, the shortest paths of all superpixels are efficiently computed by the Johnson algorithm <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inter-frame Graph Construction</head><p>The foreground probability map P k reveals the foreground object region but it is not complete and precise. In particular, probability values of the true background regions near the object boundary may have high values due to the oversegmentation process. Besides, inaccurate optical flow estimation may result in erroneous values. By the definition of saliency, foreground and background regions should be visually different, and object regions should be temporally continuous between consecutive frames. These motivate us to estimate saliency between pairs of adjacent frames.</p><p>For each pair of adjacent frames F k and F k+1 , we construct an undirected weighted graph</p><formula xml:id="formula_9">G k = {V k , E k }. The nodes V k consist of the superpixels Y k of frame F k</formula><p>and the superpixels Y k+1 of frame F k+1 . There are two types of edges: intra-frame edges that link spatially adjacent superpixels and inter-frame edges that connect temporally adjacent superpixels. The superpixels are spatially connected if they are adjacent in the same frame.</p><p>Temporally adjacent superpixels refer to the superpixels which belong to different frames but have overlap. We assign the edge weight as the Euclidean distance between their mean colors in the CIE-Lab color space. For each frame, we use a self-adaptive threshold to decompose frame F k into background regions B k and object-like regions U k through the probability map P k . The threshold σ k for frame F k is computed as</p><formula xml:id="formula_10">σ k = µ(P k ),<label>(6)</label></formula><p>where µ(•) is the mean probability of all pixels within the frame F k . We assign the object-like regions U k and the background regions B k of k-th frame as</p><formula xml:id="formula_11">U k = {y k n |P k (y k n ) &gt; σ k } ∪ {y k n |y k n is temporally connected to U k-1 }, B k = Y k -U k . (7)</formula><p>In a causal system, previously determined object regions offer valuable information to eliminate artifacts due to inaccurate optical flow estimation. Therefore, we project object-like regions of prior frame F k-1 onto frame F k . Our motivation can be observed in Figure <ref type="figure" target="#fig_1">3</ref>. The object estimation result of frame F k (Figure <ref type="figure" target="#fig_1">3c</ref>) is not ideal, due to the incorrect optical flow estimation (Figure <ref type="figure" target="#fig_1">3-b</ref>). If F k is segmented using only the self-adaptive threshold T k defined in Equation <ref type="formula">7</ref>, an inferior decomposition is generated (Figure <ref type="figure" target="#fig_1">3-d</ref>), further leading into incorrect saliency result (Figure <ref type="figure" target="#fig_1">3-g</ref>). When the previous estimation is projected, a more correct decomposition can be obtained (Figure <ref type="figure" target="#fig_1">3</ref>-f), and more consistent saliency can be attained (Figure <ref type="figure" target="#fig_1">3-h</ref>).</p><p>Based on the graph G k , we compute saliency map S k (S k+1 ) for frame F k (F k+1 ) as follows:</p><formula xml:id="formula_12">S k (y k n ) = min b∈B k ∪B k+1 d geo (y k n , b, G k ), S k+1 (y k+1 n ) = min b∈B k ∪B k+1 d geo (y k+1 n , b, G k ).<label>(8)</label></formula><p>The rationale behind Equation 8 is that the saliency value of a superpixel is measured by its shortest path to background regions in color space considering both spatial and temporal information. We update P k and P k+1 for frame F k and F k+1 with S k and S k+1 , and keep iterating this process for the following two adjacent frames F k+1 and F k+2 until the final frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Skeleton Abstraction</head><p>To further refine the saliency estimates above, we use a geodesic distance based abstraction scheme that augments core regions with higher saliency values. We decompose (Figure <ref type="figure" target="#fig_2">4-c</ref>) frame F k into two parts: background regions B k and object-like regions U k using a threshold similar to the one in Equation 6 yet computed by the saliency result S k as</p><formula xml:id="formula_13">σ k = µ(S k ), U k = {y k n |S k (y k n ) &gt; σ k }, B k = Y k -U k . (<label>9</label></formula><formula xml:id="formula_14">)</formula><p>As the saliency result S k is more accurate than P k (this is quantitatively verified in our experiment part), we decompose frame F k through an efficient thresholding strategy.</p><p>The skeleton region abstraction is an iterative process based on the undirected weighted graph G k defined in Section 3.2. The base skeleton region should have two properties. First, this region should be as far away from background regions B k as possible; second, it should be close to foreground regions U k . Based on this conditions, the base skeleton region is selected by After obtaining the base skeleton region (Figure <ref type="figure" target="#fig_2">4</ref>-d), we select the other skeleton regions. These regions are as far away from background regions B k and previous skeleton regions as possible. This induces the skeleton regions to cover object regions that may have different appearances. Therefore, the skeleton regions are selected in a greedy fashion:</p><formula xml:id="formula_15">O k ← argmin o∈U k max u ∈U k d geo (o, u , G k ) min b ∈B k d geo (o, b , G k ) . (<label>10</label></formula><formula xml:id="formula_16">)</formula><formula xml:id="formula_17">O k ← O k ∪ argmax o∈U k min o ∈O k d geo (o, o , G k )•min b ∈B k d geo (o, b , G k ) . (11)</formula><p>As shown in Figure <ref type="figure" target="#fig_2">4</ref>-e, each of the subsequent skeleton regions is selected to maximize its geodesic distance to background and previously selected skeleton regions. This process continues until a small percentage (10%) of the object-like regions U k are selected as skeleton. All object-like regions that lie on the shortest geodesic path between the base and subsequently chosen skeleton regions are also selected as skeleton regions. Finally, we increase the saliency values of the skeleton regions (in all experiments 2×) as in Figure <ref type="figure" target="#fig_2">4-h</ref>). A quantitative evaluation of the effectiveness and improvement of each step of our saliency scheme is presented in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PIXEL LABELING ENERGY FUNCTION</head><p>In the second stage of our segmentation method, we perform binary video segmentation based on the saliency results from Section 3. Separate global appearance models for foreground and background are established using the saliency results. Dynamic location model for each frame is estimated from motion information extracted from subsequent frames. Finally, the spatiotemporal saliency maps, global appearance models and dynamic location model are combined into an energy function for binary segmentation.</p><p>We formulate the segmentation task as a pixel labeling problem. Each pixel x k i in frame F k can take a label l k i ∈ {0, 1}, where 0 corresponds to background and 1 corresponds to foreground. A labeling L = {l k i } k,i of pixels from all frames represents a partitioning of the entire video. Similar to other segmentation works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b49">[50]</ref>, we define an energy function for labeling L of all the pixels as</p><formula xml:id="formula_18">F(L) = k,i U k (l k i ) + λ 1 k,i A k (l k i ) + λ 2 k,i L k (l k i ) + λ 3 (i,j)∈Ns V k (l k i , l k j ) + λ 4 (i,j)∈Nt W k (l k i , l k+1 j ),<label>(12)</label></formula><p>where the spatial pixel neighborhood N s consists of 8 neighboring pixels within the same frame, the temporal pixel neighborhood N t consists of the forward-backward 9 neighbors in adjacent frames, and i, j are indices of pixels. This energy function consists of three unary terms, U k , A k and L k , and two pairwise terms V k and W k , which depend on the labels of spatially and temporally neighboring pixels. The purpose of U k is to evaluate how likely a pixel is foreground according to the spatiotemporal saliency maps computed in the previous step. The unary appearance term A k encourages labeling pixels that have similar colors according to their global appearance models. The third unary term L k is for labeling pixels according to the location priors estimated from the dynamic location models. The pairwise terms V k and W k encourage spatial and temporal smoothness, respectively. All the terms are described in detail next. The scalar parameters λ weight the various terms, which can be set according to the characteristic of the video content. In our implements, we empirically set λ 1 = 0.5, λ 2 = 0.2, λ 3 = λ 4 = 100.</p><p>Having described the separate terms of the complete energy function F below, we use graph-cuts <ref type="bibr" target="#b50">[51]</ref> to compute the optimal binary labeling and obtain the final segmentation (Figure <ref type="figure" target="#fig_3">5-h</ref>).</p><p>Saliency term U k : The unary saliency term U k is based on the saliency estimation results and penalizes assignments of pixels with low saliency to the foreground. The term U k has the following form</p><formula xml:id="formula_19">U k (l k i ) = -log(1 -S k (x k i )) if l k i = 0; -log(S k (x k i )) if l k i = 1.<label>(13)</label></formula><p>Appearance term A k : To model the foreground and background appearance, two weighted color histograms H f and H b are computed in RGB color space. Each color channel is uniformly quantized into 10 bins, and there is a total of 10 3 bins in the joint histogram. Each pixel contributes into these histograms H f and H b according to its color values with weights S k (x) and 1 -S k (x), respectively.</p><p>To construct the foreground (background) histogram, we only use pixels from the superpixels spatially connected to the former foreground (background) superpixels and have saliency values larger (smaller) than the adaptive threshold, which is defined as the mean value of spatiotemporal saliency map. This strategy makes better use of the information of spatiotemporal saliency results and minimizes adverse effects of background regions with similar color to the foreground contaminating the foreground histogram (Figure <ref type="figure" target="#fig_3">5-c,e</ref>). Finally, the histograms are normalized. Denoting c(x k i ) as the histogram bin index of RGB color value at pixel x k i , the unary appearance term A k is defined as:</p><formula xml:id="formula_20">A k (l k i ) =          -log( H b (c(x k i )) H f (c(x k i )) + H b (c(x k i )) ) if l k i = 0; -log( H f (c(x k i )) H f (c(x k i )) + H b (c(x k i )) ) if l k i = 1.<label>(14)</label></formula><p>Location term L k : For the cases of cluttered scenes and background regions having similar appearance models with the foreground, the object motion consistency provides a valuable prior to locate the areas likely to contain the object. Thus, we estimate the location of foreground object with respect to motion information from a small number of neighboring frames. For k-th frame, we accumulate the optical flow gradient magnitudes within a temporal window of ±t frames to obtain relatively longer term motion information of the foreground regions as</p><formula xml:id="formula_21">E k = k+t i=k-t E i o = k+t i=k-t ∇V i . (<label>15</label></formula><formula xml:id="formula_22">)</formula><p>Having a larger temporal window provides some robustness to individual pixel-wise unreliable optical estimates. However, this may also cause E k loses its discriminative power since motion cue spans out on too many frames. In our experiments, we set t = 5. We use the intra-frame graph construction described in Section 3.1 to compute a dynamic location model for each frame (Figure <ref type="figure" target="#fig_3">5</ref>-f,g). Finally, we determine the location prior L k i for pixel x k i and the unary location term L k as</p><formula xml:id="formula_23">L k (l k i ) = -log(1 -L k (x k i )) if l k i = 0; -log(L k (x k i )) if l k i = 1.<label>(16)</label></formula><p>Pairwise terms V k , W k : These terms impose label smoothness by constraining the segmentation labels to be both spatially and temporally consistent. They are contrast-modulated Potts potentials <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b49">[50]</ref>, which favor assigning the same label to neighboring pixels that have similar color. The spatial consistency term V k computed between spatially adjacent pixels x i and x j is defined as</p><formula xml:id="formula_24">V k (l k i , l k j ) = δ(l k i , l k j ) exp -θ||C(x k i )-C(x k j )|| 2 , (<label>17</label></formula><formula xml:id="formula_25">)</formula><p>where C(x k i ) is the color vector of the pixel x k i and δ(•) denotes the Dirac delta function, which is 0 when l k i = l k j . The constant φ is chosen <ref type="bibr" target="#b49">[50]</ref> to be θ = (2</p><formula xml:id="formula_26">(i,j)∈Ns ||C(x k i ) -C(x k j )|| 2 ) -1 ,<label>(18)</label></formula><p>to ensure the exponential term in Equation <ref type="formula" target="#formula_24">17</ref>switches appropriately between high and low contrast. Similarly, the temporal consistency term W k is defined as</p><formula xml:id="formula_27">W k (l k i , l k+1 j ) = δ(l k i , l k+1 j ) exp -θ||C(x k i )-C(x k+1 j )|| 2 . (<label>19</label></formula><formula xml:id="formula_28">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL EVALUATIONS</head><p>Even though it is not the ultimate goal of the proposed algorithm, we first evaluate the effectiveness of our spatiotemporal saliency estimation method by comparing against some state-of-the-art saliency methods (in Section 5.1). After that, in Section 5.2, we compare both quantitatively and qualitatively our overall segmentation method with serveral well-known video segmentation approaches. Then we offer more detailed exploration and dissect various parts of our approach. In Section 5.3, we assess its computational load. In Section 5.4, we investigate the impact of important parameters, verify basic assumptions of the proposed algorithm and evaluate the effectiveness of each step of the proposed framework. In our comparisons, we use the implementations provided by the respective authors and set their free parameters to maximize their performance.</p><p>For quantitative and qualitative analyses, we use four benchmark datasets, the SegTrack <ref type="bibr" target="#b51">[52]</ref>, the extended SegTrack <ref type="bibr" target="#b52">[53]</ref>, Freiburg-Berkeley Motion Segmentation Dataset (FBMS) <ref type="bibr" target="#b0">[1]</ref> and the DAVIS <ref type="bibr" target="#b53">[54]</ref>. The SegTrack dataset contains 6 videos in total where full pixel-level segmentation ground-truth for each frame is available. We follow the common protocol <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b21">[22]</ref> and use 5 video sequences (Birdfall, Cheetah, Girl, Monkeydog and Parachute) for evaluations (the last video, Penguin, is not usable for saliency since only a single penguin is labeled in a colony of penguins).</p><p>While the SegTrack dataset is widely popular, the extended SegTrack dataset is more challenging. It was originally introduced for evaluating object tracking algorithms, yet it is also suitable for video object segmentation. The extended SegTrack dataset consists of 8 additional sequences, which have complex backgrounds and varying object motion patterns. We select five sequences (Bird of Paradise, Frog, Monkey, Soldier and Worm), each of which contains a single dominant object.</p><p>The FBMS dataset, containing 59 video sequences, is widely used for video segmentation and covers various challenges such as large foreground and background appearance variation, significant shape deformation, and large camera motion.</p><p>We also report our performance on the newly developed DAVIS dataset, which is one of the most challenging video segmentation benchmarks. It comprises a total of 50 high-resolution sequences spanning a wide degree of difficulty, such as occlusions, fast-motion and appearance changes. Our method achieved more than 75% improvement over the best previous method in terms of MAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation of Spatiotemporal Saliency</head><p>Since spatiotemporal saliency detection is an important step of our video segmentation approach, we assess its performance against the existing saliency methods. Using the original implementations obtained from the corresponding authors, we make comparisons between 6 alternative approaches including manifold ranking saliency model (MR) <ref type="bibr" target="#b38">[39]</ref>, saliency filter (SF) <ref type="bibr" target="#b54">[55]</ref>, selfresemblance based saliency (SS) <ref type="bibr" target="#b14">[15]</ref>, saliency via quaternion Fourier transform (QS) <ref type="bibr" target="#b32">[33]</ref>, cluster-based co-saliency (CS) <ref type="bibr" target="#b41">[42]</ref>, and space-time saliency for time-mapping (TS) <ref type="bibr" target="#b42">[43]</ref>. The former two of these methods aim at image saliency while the latter four are designed for video saliency.</p><p>We report results on three widely used performance measures including precision-recall (PR) curve, F-score <ref type="bibr" target="#b33">[34]</ref>, and MAE (mean absolute errors). Precision is the fraction of the correctly labeled foreground pixels among the all pixels labeled as foreground by the algorithm, while recall is the fraction of correctly labeled foreground pixels among the ground-truth foreground pixels. We generate binary saliency maps from each method and plot the corresponding PR curves by varying the operating point threshold.</p><p>In general, a high recall response may come at the expense of reduced precision, and vice versa. Therefore, we also estimate F-score for evaluating precision and recall simultaneously. F-score evaluates precision and recall is defined as</p><formula xml:id="formula_29">F-score = (1 + β 2 ) × precision × recall β 2 × precision + recall , (<label>20</label></formula><formula xml:id="formula_30">)</formula><p>Fig. <ref type="figure">7</ref>. Qualitative comparison against the state-of-the-art methods on the SegTrack benchmark videos <ref type="bibr" target="#b51">[52]</ref>, the extended SegTrack <ref type="bibr" target="#b52">[53]</ref> sequences and the famous FBMS dataset <ref type="bibr" target="#b0">[1]</ref> with pixel-level ground-truth labels. Our saliency method renders the entire objects as salient in complex scenarios, yielding continuous saliency maps that are most similar to the ground-truth.</p><p>where we set β 2 to 0.3 to assign a higher importance to precision as suggested in <ref type="bibr" target="#b33">[34]</ref>. For a complete analysis, we follow <ref type="bibr" target="#b54">[55]</ref> to evaluate the mean absolute error (MAE) between a real-valued saliency map S and a binary ground-truth G for all image pixels:</p><formula xml:id="formula_31">MAE = |S -G| N , (<label>21</label></formula><formula xml:id="formula_32">)</formula><p>where N is the number of pixels. The MAE estimates the approximation degree between the saliency map and the ground-truth map, and it is normalized to [0, 1]. The MAE provides a better estimate of conformity between estimated and ground-truth maps.</p><p>The precision-recall curves of all methods are reported in Figure <ref type="figure" target="#fig_4">6</ref>-a. As shown, our method significantly outperforms the state-of-the-art. The minimum recall value in these curves can also be regarded as an indicator of robustness. A high precision score at the minimum recall value means a good separation between the foreground and background confidence values, as most of the high confidence saliency values (close to 1) are correctly estimated the foreground object.</p><p>As can be seen, when the threshold is close to 255, the recall scores of other saliency models become very small, and the recall scores of SS <ref type="bibr" target="#b14">[15]</ref> and QS <ref type="bibr" target="#b32">[33]</ref> shrinks to 0. This is a result of those saliency maps do not correspond to the ground-truth objects. To our advantage, the minimum recall of the our method does not drop to 0. This demonstrates our saliency maps align better with the correct objects. In addition, our saliency method achieves the best precision rates above 0.9, which shows it is more precise and responsive to the actual salient information. Similar conclusions can be drawn from the F-score, as shown in Figure <ref type="figure" target="#fig_4">6</ref>-b. Our F-score is well above the performance of other methods. The MAE results are presented in Figure <ref type="figure" target="#fig_4">6-c</ref>. As shown, our saliency maps successfully reduce the MAE almost by 75% compared to the second best method (which is SF <ref type="bibr" target="#b54">[55]</ref>). In summary, our method consistently produces superior results.</p><p>Figure <ref type="figure">7</ref> shows a qualitative comparison of different methods, where brighter pixels indicate higher saliency probabilities. It is observed that image saliency methods (MR <ref type="bibr" target="#b38">[39]</ref>, SF <ref type="bibr" target="#b54">[55]</ref>) applied independently to each frame produce unstable outputs, some saliency maps even completely miss the foreground object, mainly because temporal coherence in video can convey important information for identifying salient objects. In contrast, video saliency methods SS <ref type="bibr" target="#b14">[15]</ref>, QS <ref type="bibr" target="#b32">[33]</ref>, CS <ref type="bibr" target="#b41">[42]</ref>, and TS <ref type="bibr" target="#b42">[43]</ref> perform relatively better as they utilize motion information. However, saliency maps from previous video saliency models are often generated in lower pixel precision and tend to assign lower foreground probabilities to pixels inside the salient objects. This is due to the fact that optical flow estimations are unreliable.</p><p>Based on above, we draw two important conclusions: (1) motion information gives effective guidance for detecting foreground object; (2) making methods rely heavily on motion information is not the optimal choice. Comprehensive utilization of various features in spatial and temporal space (color, edges, motion, etc.) produces more satisfactory segmentation results. Our model is able to estimate more accurate saliency maps within and on the boundaries of the target objects in cluttered backgrounds.</p><p>In addition, the assigned saliency values have higher confidence values, which also reflects in the quantitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation of Pixel Labeling</head><p>Our framework produces both spatially and temporally coherent video object segmentation results in a fully unsupervised way. We use the average per-frame pixel error rate (APFPER) introduced by <ref type="bibr" target="#b51">[52]</ref> for evaluating the performance on the SegTrack dataset. This error rate measures the number of misclassified pixels and used in <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b21">[22]</ref>. As discussed in <ref type="bibr" target="#b52">[53]</ref>, the intersection-over-union overlap (IoU) metric 1 , which is the intersection over union of the estimated and ground-truth segmentation maps, is an informative indicator of the performance. This metric is also widely used for evaluating the segmentation performance. Therefore, we report our performance on the IoU metric for the SegTrack <ref type="bibr" target="#b51">[52]</ref>, extended SegTrack <ref type="bibr" target="#b52">[53]</ref>, FBMS <ref type="bibr" target="#b0">[1]</ref> and the DAVIS <ref type="bibr" target="#b53">[54]</ref> by computing the score for each frame and then averaging it over all frames.</p><p>The APFPER results of ours and <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b56">[57]</ref> on the SegTrack are shown in Table <ref type="table" target="#tab_0">1</ref>. The segmentation methods in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b55">[56]</ref> and our method are unsupervised, while other methods in <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b56">[57]</ref> are supervised. As seen, our method outperforms all existing unsupervised algorithms on most video sequences. Furthermore, our algorithm is better or on a par with the supervised approaches <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b56">[57]</ref>, which indicates the robustness of the proposed approach.</p><p>Table <ref type="table" target="#tab_1">2</ref> presents the IoU scores of our method and <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref> on the SegTrack and 1. Note the IoU score is equivalent to the region similarity metric (Jaccard metric) used in DAVIS benchmark. We analyze the effectiveness of our approach on the FBMS dataset and DAVIS dataset using IoU score. The IoU scores for representative sequences of the FBMS dataset and the average performance over the entire dataset (59 videos) are demonstrated in Table <ref type="table" target="#tab_2">3</ref>. The proposed method achieves the best score on most of the videos and the best average score overall. Table <ref type="table" target="#tab_3">4</ref> reports the IoU scores of our method and other methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b26">[27]</ref> on the DAVIS datasets, some results are borrowed from <ref type="bibr" target="#b53">[54]</ref>. The average IoU score is computed over the 50 video sequences of DAVIS datasets. As may be seen in Table <ref type="table" target="#tab_3">4</ref>, our method still performs comparably or better than other concurrent approaches.</p><p>Representative pixel labeling results are shown in Figure <ref type="figure" target="#fig_5">8</ref> (SegTrack, the extended SegTrack) and Figure <ref type="figure" target="#fig_6">9</ref> (FBMS, DAVIS). Our method has the ability to segment objects with fast motion patterns (Cheetah and Horse) or large shape deformation (Parkour). It produces accurate segmentation maps even when foreground undergoes appearance changes (Mallard-fly), contains various motion patterns (Soldier), or has similar color cues with the background (Monkey). In contrast, existing approaches <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b21">[22]</ref> either mislabel background pixels as foreground or miss foreground pixels. In our we observed that target foregrounds in various scenarios can be segmented accurately by our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Computational Load</head><p>Our method is tested on a Dell T5610 workstation with an Intel Xeon E5 CPU of 2.50 with unoptimized MATLAB implementation. We analyze the computational load of the steps in the proposed pipeline. We also include 4 video saliency methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref> and 3 video segmentation methods <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b21">[22]</ref> for providing a comprehensive view of execution times of existing approaches.</p><p>The execution times are presented in Figure <ref type="figure" target="#fig_7">10</ref> (excluding optical flow computations for all algorithms). Figure <ref type="figure" target="#fig_7">10-a</ref> shows the execution time comparisons of our and other saliency methods. It is clear that our saliency method is one of the fastest solutions and only slower than the frequency domain based method <ref type="bibr" target="#b32">[33]</ref>. Figure <ref type="figure" target="#fig_7">10</ref>-b reports the per-frame processing times of the overall segmentation procedures. All solutions use the optical flow estimation method of <ref type="bibr" target="#b46">[47]</ref>. Our method (3.5 seconds per frame) is much faster than <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref> but only slower than <ref type="bibr" target="#b21">[22]</ref>.</p><p>The object proposal based segmentation methods of <ref type="bibr" target="#b6">[7]</ref>, [9] require computationally expensive and complex object proposal generation and inference stage <ref type="bibr" target="#b9">[10]</ref> costing 43.5 seconds additional time per frame. Clearly, running time efficiency is the major bottleneck for the usability of those video segmentation algorithms, as a substantial amount of time is spent preprocessing image frames to generate object proposals. The execution time of each part of our whole scheme is shown in Figure <ref type="figure" target="#fig_7">10-c</ref>. The whole segmentation pipeline takes about 3.5 seconds for each frame, where over 60% of the runtime is spent on the edge generation <ref type="bibr" target="#b45">[46]</ref>. Saliency detection takes a total of 1.2 seconds: 0.38 seconds for computing the saliency via intraframe graph (Step1), 0.59 seconds for improving saliency results via inter-frame graph (Step2), and 0.23 seconds for generating final saliency via abstracting skeleton regions (Step3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Validation of the Proposed Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Parameter Selection</head><p>In this section, we investigate the impact of important parameters and verify basic assumptions of the proposed algorithm. We carry out the evaluation of our system on the SegTrack dataset <ref type="bibr" target="#b51">[52]</ref> and the extended SegTrack dataset <ref type="bibr" target="#b52">[53]</ref>.</p><p>We first study the influence of the number of superpixels K per frame. We report the performance by varying K = {500, 600, • • •, 1400, 1500}. We plot the MAE value of the spatiotemporal saliency estimates and the IoU score of the segmentation results as functions of a variety of Ks in Figure <ref type="figure" target="#fig_8">11</ref>. For both datasets, we can observe that the performance of both saliency estimates and segments increase when more superpixels (K ↑) are oversegmented. However, when we further increase the number of superpixels (K &gt; 900), the final performance does not change obviously. As the computational expense would increase with the lager number of superpixels, we set K = 1000 in our implements. Additionally, we can find that the segmentation performance gets better with more accurate saliency estimates.</p><p>In Section 3.1, we fuse the spatial edge E c and motion edge E o together via Equation 1. It is necessary to explore different combination strategies for generating spatiotemporal edges. We consider four extra combination strategies and report their corresponding performance of saliency estimates and segmentation  results in Table <ref type="table" target="#tab_4">5</ref>. As can be seen, the combination strategy in Equation 1 achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Validation of Spatiotemporal Saliency Steps</head><p>The proposed spatiotemporal saliency method has intra-frame graph and inter-frame graph saliency steps and also the skeleton abstraction step. The intra-frame graph provides an initial estimation of the salient region and the background in the first step (Section 3.2). Based on this initial estimation, the inter-frame graph further improves the saliency estimations of the superpixels in the second step (Section 3.3). The salient object regions are compared with each other and highly-confident foreground regions are strengthened into the final saliency map in the third step (Section 3.4).</p><p>To exhibit more details of our algorithm and objectively evaluate the contribution of different parts in the proposed saliency model to the saliency detection performance, we report the evaluation of each stage of our algorithm on the SegTrack <ref type="bibr" target="#b51">[52]</ref>, the Step1 and Step2 refer to saliency via intra-frame and inter-frame graphs, respectively. Step3 is the skeleton abstraction. Top: evaluation results on the SegTrack <ref type="bibr" target="#b51">[52]</ref>. Middle: evaluation results on the extended SegTrack <ref type="bibr" target="#b52">[53]</ref>. Bottom: evaluation results on the FBMS <ref type="bibr" target="#b0">[1]</ref>.</p><p>extended SegTrack <ref type="bibr" target="#b52">[53]</ref> and the FBMS <ref type="bibr" target="#b0">[1]</ref> datasets. We report the performance improvement of each step in Figure <ref type="figure" target="#fig_9">12</ref>. Step1 and Step2 refer to the initial saliency via the intra-frame graph (Section 3.2) and the refined saliency via the inter-frame graph (Section 3.3). Step3 corresponds to our final saliency results (Section 3.4). As shown, compared to the PR curve for initial saliency map Step1, the performance of the refined saliency Step2 is elevated and final saliency estimates Step3 achieve the best performance. This demonstrates the contribution of our saliency refinement via inter-frame graph and object skeleton abstraction scheme based saliency optimization for improving the saliency detection performance. The results for the MAE measure show similar conclusions. Overall, the performance of each step improves progressively, which demonstrates that the combination of all steps is effective for improving the overall performance. Some qualitative comparison results can be observed in Figure <ref type="figure" target="#fig_0">2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSIONS AND LIMITATIONS</head><p>The proposed algorithm has a few limitations. The performance of our algorithm is limited by the accuracy of spatiotemporal saliency estimation. Saliency estimation is the cornerstone of our method to determine where the primary object it is. If importance analysis was misleading, it might negatively affect segmentation results. For example, our spatiotemporal saliency method may not be well suited for scenes that have multiple salient objects or have a primary foreground object that occupies large portion of the image. In these scenarios, it is likely to produce suboptimal results as the potential assumption for saliency detection is that only a part of scene attracts human attention mostly. In our approach, we formulate the local dynamic location prior and the global appearance information in the proposed segmentation energy function (Equation <ref type="formula" target="#formula_19">13</ref>), which would alleviate this problem.</p><p>Another difficulty for the current method is handling objects with occlusion, which is the common challenge in video segmentation problem. As the proposed spatiotemporal saliency prior relies on the object continuity between adjacent frames, it is able to handle common scenarios with small or short occlusions in a bottom-up fashion (Figure <ref type="figure" target="#fig_10">13-a</ref>). As for some extremely difficult scenarios with complete occlusions, such as the bmx in Figure <ref type="figure" target="#fig_10">13</ref>-b, the proposed method may still locate a part of scene as salient region, even the object has been occluded. That is followed by the basic assumption of saliency detection that important object should exist. One promising direction to improve the segmentation is the use of long range connectivity of objects such as motion trajectories. Other advances may come from adopting some occlusion-aware tracking techniques or the development of more powerful representations beyond regions, such as supervoxel and video object proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS AND FUTURE WORK</head><p>We have presented an unsupervised approach that incorporates geodesic distance into saliency-aware video object segmentation.</p><p>As opposed to the traditional video segmentation methods that heavily rely on cumbersome object inference and motion analysis, our method emphasizes the importance of video saliency, which offers strong and reliable cues for pixel labeling of foreground video objects.</p><p>The proposed method incorporates intra-graph edge and intergraph motion boundary information into a spatiotemporal edge map. It uses the geodesic distance on these graphs to measure the saliency score of each superpixel. In intra-frame graph, the geodesic distance between the superpixel and frame boundary is exploited to estimate the foreground probability. In inter-frame graph, geodesic distance to the estimated background is utilized to update the spatiotemporal saliency map for each pair of adjacent frames. The geodesic distance is also employed to extract the base and supporting foreground superpixels in the skeleton abstraction step to further enhance the saliency scores. In the pixel labeling stage, an energy function that combines global appearance models, dynamic location models and spatiotemporal saliency maps is defined and efficiently minimized via graph-cuts to obtain the final segmentation results.</p><p>We have evaluated our methods on four benchmarks, namely SegTrack <ref type="bibr" target="#b51">[52]</ref>, extended SegTrack <ref type="bibr" target="#b52">[53]</ref>, FBMS <ref type="bibr" target="#b0">[1]</ref> and the DAVIS <ref type="bibr" target="#b53">[54]</ref>. The extensive experimental evaluations show that our approach can generate high quality saliency maps in relatively short time and achieve consistently higher performance scores than many other existing methods. Comparing with other video segmentation methods, our approach generates both quantitatively and qualitatively superior segmentation results.</p><p>For future work, we will apply the proposed approach to other applications, such as video resizing, video summarization, and video compression. Additionally, our work provides important hints toward combining spatiotemporal saliency prior with more effective video representations, such as trajectory and supervoxel.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of our geodesic distance based spatiotemporal saliency prior. (a) Input frame F k . (b) Oversegmentation of F k into superpixels Y k . (c) Spatial edge probability map E k c of F k . (d) Gradient magnitude E k o of optical flow of F k . (e) Superpixel-wise spatiotemporal edge map E k computed via Equation 1. (f) Object estimation result P k via intra-frame graph. (g) Saliency result S k via inter-frame graph. (h) Final video saliency via the proposed skeleton abstraction method.</figDesc><graphic coords="4,50.58,43.70,510.89,224.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of our inter-frame graph construction. (a) Frame F k . (b) Optical flow flied V k . (c) When the optical flow estimation is not accurate (which is unfortunately the common case) object probabilities P k are degraded. (d) Frame F k is decomposed into background regions B k and object-like regions U k by self-adaptive threshold σ k defined in Equation 6. The black regions indicate the background regions B k , while the bright regions indicate the object-like regions U k . (e) The decomposition of prior frame F k-1 . (f) The object-like regions U k-1 of frame F k-1 are projected onto frame F k . (g) Spatiotemporal saliency result S k for frame F k with consideration of (d) and (e). (h) Spatiotemporal saliency result S k for frame F k with consideration of (e) and (f).</figDesc><graphic coords="5,50.58,43.70,510.89,224.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of the skeleton abstraction process. (a) Frame F k . (b) Saliency results S k of (a) obtained via Equation 8. (c) Frame F k is decomposed into background regions B k and object-like regions U k by self-adaptive threshold σ k defined in Equation 9. The black regions indicate the background regions B k , while the bright regions indicate the object-like regions U k . (d) The red region corresponds to the base skeleton region, which is the first selected skeleton region through Equation10. (e) The three yellow regions correspond to the subsequently selected skeleton regions through Equation11. (f) We iteratively find the add skeleton regions until the number of selected skeleton regions reaches 10% of the number of object-like regions U k . (g) The blue regions are the other skeleton regions that lie on the shortest geodesic path between the base and the selected skeleton regions. (h) The saliency values of the skeleton regions are enhanced.</figDesc><graphic coords="6,48.00,43.70,516.05,228.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Illustration of pixel labeling. (a) Input frame F k . (b) Spatiotemporal saliency map S k . (c) The regions within the red boundaries are the superpixels with the saliency value larger than the adaptive threshold, which are used for establishing the foreground histogram model. The regions between the green boundaries and red boundaries are for building background histogram model. (d) Global appearance models with color histograms H f and H b for foreground and background via (c). (e) Probability map for foreground computed via the global appearance model. (f) Accumulated optical flow gradient magnitude E k for frame F k yields trajectory of the object within few subsequent frames. (g) Dynamic location prior L k is obtained via intra-frame graph construction method described in Section 3.2. (h) Final segmentation results by Equation13, which consists of the saliency term (b), the appearance term (e), and the location term (g), and two pairwise terms.</figDesc><graphic coords="7,50.58,43.70,510.86,226.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Comparison with 6 alternative saliency detection methods using SegTrack dataset<ref type="bibr" target="#b51">[52]</ref> (top), extended SegTrack dataset<ref type="bibr" target="#b52">[53]</ref> (middle) and FBMS dataset<ref type="bibr" target="#b0">[1]</ref> (bottom) with pixel-level ground-truth: (a) average precision recall curve by segmenting saliency maps using fixed thresholds, (b) F-score, (c) average MAE. Notice that, our algorithm significantly outperforms other methods in terms of the precision-recall curve and F-score. Our method achieved more than 75% improvement over the best previous method in terms of MAE.</figDesc><graphic coords="9,50.58,43.70,510.84,474.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Our segmentation results on the SegTrack [52] (Cheetah and Girl) and the extended SegTrack dataset [53] (Bird of Paradise, Monkey and Soldier ) with pixel-level ground-truth masks. The pixels within the green boundaries are segmented as foreground.</figDesc><graphic coords="11,50.58,43.69,510.88,561.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Our segmentation results on the FBMS [1] (Horse and Camel) and the DAVIS dataset [54] (Kite-walk, Mallard-fly and Parkour ) with pixel-level ground-truth masks. The pixels within the green boundaries are segmented as foreground.</figDesc><graphic coords="12,50.58,43.70,510.88,551.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Computational load of our method and the state-of-the-art for 340×240 video. (a) Execution time of video saliency estimation stage compared against other video saliency methods [15], [33], [42], [43]. (b) Execution time of overall method compared against other video segmentation methods [7], [9], [22]. (c) Execution time of each intermediate steps. Step1 and Step2 are saliency estimations via intra-frame graph and inter-frame graph, respectively. Step3 is the final saliency step.</figDesc><graphic coords="14,313.26,43.70,249.49,224.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Parameter selection for the number of superpixel K using (a) the SegTrack database and (b) the extended SegTrack dataset. The MAE of the saliency results and the IoU score of the segmentation results are plotted as functions of a variety of Ks.</figDesc><graphic coords="15,49.26,43.70,249.48,303.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Assessment of individual steps of our saliency estimation by (a) precision-recall curves, and (b) MAE scores.Step1 and Step2 refer to saliency via intra-frame and inter-frame graphs, respectively. Step3 is the skeleton abstraction. Top: evaluation results on the SegTrack<ref type="bibr" target="#b51">[52]</ref>. Middle: evaluation results on the extended SegTrack<ref type="bibr" target="#b52">[53]</ref>. Bottom: evaluation results on the FBMS<ref type="bibr" target="#b0">[1]</ref>.</figDesc><graphic coords="15,313.26,43.70,249.48,416.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>Fig.<ref type="bibr" target="#b12">13</ref>. Video object segmentation and salient object detection results for object occlusion. (a) For object with part occlusions, the proposed method can still produce reliable spatiotemporal saliency prior and generate accurate segments. (b) When heavy object occlusions occur, the proposed method may suffer difficulties since it sticks to find a salient object followed by the basic assumption of saliency detection.</figDesc><graphic coords="16,50.58,43.70,510.88,225.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,50.58,43.70,510.82,159.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>APFPER results on SegTrack dataset<ref type="bibr" target="#b51">[52]</ref> compared to the ground-truth. Lower values are better. The best and the second best results are boldfaced and underlined, respectively.</figDesc><table><row><cell></cell><cell>video</cell><cell>frames</cell><cell>Ours</cell><cell>[1]</cell><cell>[7]</cell><cell>[8]</cell><cell cols="2">unsupervised [9] [23]</cell><cell>[22]</cell><cell>[28]</cell><cell>[53]</cell><cell>[56]</cell><cell cols="2">supervised [52] [57]</cell></row><row><cell></cell><cell>Birdfall</cell><cell>30</cell><cell>140</cell><cell>217</cell><cell>288</cell><cell>468</cell><cell>155</cell><cell>606</cell><cell>189</cell><cell>144</cell><cell>199</cell><cell>468</cell><cell>252</cell><cell>454</cell></row><row><cell></cell><cell>Cheetah</cell><cell>29</cell><cell>622</cell><cell>890</cell><cell>905</cell><cell>1175</cell><cell>633</cell><cell>11210</cell><cell>806</cell><cell>617</cell><cell>599</cell><cell>1968</cell><cell cols="2">1142 1217</cell></row><row><cell>SegTrack</cell><cell>Girl Monkeydog</cell><cell>21 71</cell><cell>991 350</cell><cell cols="9">3859 1785 5683 1488 26409 1698 1195 1164 7595 284 521 1434 365 12662 472 354 322 1434</cell><cell cols="2">1304 1755 563 683</cell></row><row><cell></cell><cell>Parachute</cell><cell>51</cell><cell>195</cell><cell>855</cell><cell>201</cell><cell>1595</cell><cell>220</cell><cell>40251</cell><cell>221</cell><cell>200</cell><cell>242</cell><cell>1113</cell><cell>235</cell><cell>502</cell></row><row><cell></cell><cell>Avg.</cell><cell>-</cell><cell>459</cell><cell>1221</cell><cell>740</cell><cell>2071</cell><cell>572</cell><cell>18228</cell><cell>677</cell><cell>502</cell><cell>505</cell><cell>2516</cell><cell>699</cell><cell>922</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>IoU scores on SegTrack dataset<ref type="bibr" target="#b51">[52]</ref> and extended SegTrack dataset<ref type="bibr" target="#b52">[53]</ref> compared to the ground-truth. Higher values are better. The best and the second best results are boldfaced and underlined, respectively.</figDesc><table><row><cell></cell><cell>video</cell><cell>frames</cell><cell>Ours</cell><cell>[7]</cell><cell>unsupervised [9] [22] [26]</cell><cell>[28]</cell><cell>[24]</cell><cell>[58]</cell><cell cols="2">supervised [59] [60]</cell><cell>[61]</cell></row><row><cell></cell><cell>Birdfall</cell><cell>30</cell><cell>74.5</cell><cell cols="4">48.7 71.4 37.4 72.5 73.2 57.4</cell><cell cols="3">78.7 57.4 56.0 32.5</cell></row><row><cell></cell><cell>Cheetah</cell><cell>29</cell><cell>64.3</cell><cell cols="4">43.4 58.8 40.9 61.2 64.2 24.4</cell><cell cols="3">66.1 33.8 46.1 33.1</cell></row><row><cell>SegTrack</cell><cell>Girl</cell><cell>21</cell><cell>88.7</cell><cell cols="4">77.5 81.9 71.2 86.4 86.7 31.9</cell><cell cols="3">84.6 87.9 53.6 52.4</cell></row><row><cell></cell><cell>Monkeydog</cell><cell>71</cell><cell>78.0</cell><cell cols="4">64.3 74.2 73.6 74.0 76.1 68.3</cell><cell cols="3">82.2 54.4 61.0 22.1</cell></row><row><cell></cell><cell>Parachute</cell><cell>51</cell><cell>94.8</cell><cell cols="4">94.3 93.9 88.1 95.9 94.6 69.1</cell><cell cols="3">94.4 94.5 85.6 69.9</cell></row><row><cell></cell><cell>Bird of Paradise</cell><cell>98</cell><cell>94.5</cell><cell cols="4">22.3 35.2 85.4 90.0 93.9 86.8</cell><cell cols="2">93.0 95.2</cell><cell>5.1</cell><cell>44.3</cell></row><row><cell></cell><cell>Frog</cell><cell>279</cell><cell>83.3</cell><cell cols="4">71.0 76.3 69.4 80.2 81.5 67.1</cell><cell cols="3">56.3 81.4 14.5 45.2</cell></row><row><cell>Extended</cell><cell>Monkey</cell><cell>31</cell><cell>84.1</cell><cell cols="4">38.6 61.4 69.6 83.1 63.9 61.9</cell><cell cols="3">86.0 88.6 73.1 61.7</cell></row><row><cell>SegTrack</cell><cell>Soldier</cell><cell>32</cell><cell>79.2</cell><cell cols="4">10.0 51.4 47.4 76.3 36.8 66.5</cell><cell cols="3">81.1 86.4 70.7 43.0</cell></row><row><cell></cell><cell>Worm</cell><cell>243</cell><cell>74.8</cell><cell cols="4">40.5 53.9 73.0 82.4 61.7 34.7</cell><cell cols="3">79.3 89.6 36.8 27.4</cell></row><row><cell></cell><cell>Avg.</cell><cell>-</cell><cell>81.6</cell><cell cols="4">51.1 65.8 65.6 80.2 73.3 56.8</cell><cell cols="3">80.1 76.9 50.2 43.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>IoU scores on a representative subset of the FBMS dataset<ref type="bibr" target="#b0">[1]</ref>, and the average computed over the 59 video sequences. Higher values are better. The best and the second best results are boldfaced and underlined, respectively.</figDesc><table><row><cell></cell><cell>video</cell><cell>Ours</cell><cell>[7]</cell><cell>[9]</cell><cell>[22]</cell></row><row><cell></cell><cell>Bear2</cell><cell>70.1</cell><cell cols="3">87.5 21.0 86.8</cell></row><row><cell></cell><cell>Cars5</cell><cell>38.5</cell><cell cols="3">10.7 38.7 17.4</cell></row><row><cell></cell><cell>Cars9</cell><cell>60.0</cell><cell cols="3">19.5 28.9 52.4</cell></row><row><cell></cell><cell>Cars10</cell><cell>55.9</cell><cell cols="3">65.7 74.9 79.0</cell></row><row><cell></cell><cell>Cats1</cell><cell>85.7</cell><cell cols="3">19.8 81.5 83.1</cell></row><row><cell></cell><cell>Dogs2</cell><cell>91.7</cell><cell cols="3">90.8 83.7 86.3</cell></row><row><cell></cell><cell>Horses1</cell><cell>89.4</cell><cell cols="3">77.6 83.5 77.5</cell></row><row><cell>FBMS</cell><cell>Horses2</cell><cell>92.7</cell><cell cols="3">13.5 86.7 91.5</cell></row><row><cell></cell><cell>People1</cell><cell>68.1</cell><cell cols="3">56.0 64.8 53.3</cell></row><row><cell></cell><cell>People2</cell><cell>68.3</cell><cell cols="3">47.1 56.5 48.0</cell></row><row><cell></cell><cell>People4</cell><cell>86.4</cell><cell cols="3">82.1 83.8 79.4</cell></row><row><cell></cell><cell>People5</cell><cell>56.4</cell><cell cols="3">10.7 84.4 51.8</cell></row><row><cell></cell><cell>Rabbits1</cell><cell>90.8</cell><cell cols="3">92.4 91.6 92.9</cell></row><row><cell></cell><cell>Rabbits2</cell><cell>71.0</cell><cell cols="3">20.4 47.8 28.3</cell></row><row><cell></cell><cell>Rabbits5</cell><cell>88.1</cell><cell cols="3">55.1 84.7 90.1</cell></row><row><cell></cell><cell>Avg.</cell><cell>63.3</cell><cell cols="3">52.3 54.3 47.7</cell></row><row><cell cols="6">the extended SegTrack. Our approach outperforms the state-of-</cell></row><row><cell cols="6">the-art most videos and achieves the highest overall IoU score</cell></row><row><cell>(81.6).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4</head><label>4</label><figDesc>IoU scores on a representative subset of the DAVIS dataset<ref type="bibr" target="#b53">[54]</ref>, and the average computed over the 50 video sequences. Higher values are better. The best and the second best results are boldfaced and underlined, respectively.</figDesc><table><row><cell></cell><cell>video</cell><cell>Ours [3] [7] [22] [21] [62] [27]</cell></row><row><cell></cell><cell>Bmx-bumps</cell><cell>40.9 35.0 30.9 24.0 35.2 36.8 63.5</cell></row><row><cell></cell><cell>Bmx-trees</cell><cell>26.3 16.2 19.3 18.00 18.8 12.1 21.2</cell></row><row><cell></cell><cell>Boat</cell><cell>21.0 12.9 6.48 36.1 14.3 5.6 0.7</cell></row><row><cell></cell><cell>Bus</cell><cell>77.5 68.3 78.5 82.4 88.4 66.4 62.9</cell></row><row><cell></cell><cell>Camel</cell><cell>75.9 77.8 57.8 56.2 75.5 84.9 76.8</cell></row><row><cell></cell><cell>Car-roundabout</cell><cell>76.8 55.2 64.0 80.8 63.0 87.1 50.8</cell></row><row><cell></cell><cell>Dance-twirl</cell><cell>36.8 36.5 38.0 45.3 36.6 45.2 34.7</cell></row><row><cell>DAVIS</cell><cell>Elephant</cell><cell>67.3 75.9 67.5 82.3 68.8 49.4 51.7</cell></row><row><cell></cell><cell>Horsejump-high</cell><cell>65.5 36.4 37.0 57.8 73.4 83.0 83.4</cell></row><row><cell></cell><cell>Lucia</cell><cell>89.0 66.9 84.7 64.3 41.7 84.0 87.6</cell></row><row><cell></cell><cell>Mallard-fly</cell><cell>71.9 29.3 58.4 60.1 3.3 38.0 61.7</cell></row><row><cell></cell><cell cols="2">Motocross-bumps 55.3 50.1 68.9 61.7 46.5 60.3 61.4</cell></row><row><cell></cell><cell cols="2">Paragliding-launch 65.6 55.4 55.9 50.6 51.2 59.1 62.8</cell></row><row><cell></cell><cell>Soccerball</cell><cell>85.4 35.0 87.8 84.3 37.0 24.2 82.9</cell></row><row><cell></cell><cell>Swing</cell><cell>71.3 41.2 70.9 43.1 62.2 53.3 85.1</cell></row><row><cell></cell><cell>Avg.</cell><cell>64.7 50.1 56.9 57.4 54.3 51.4 64.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5 Validation</head><label>5</label><figDesc></figDesc><table><row><cell>spatiotemporal edge E</cell><cell cols="2">SegTrack MAE IoU</cell><cell cols="2">Extended SegTrack MAE IoU</cell></row><row><cell>Ec • Eo</cell><cell>0.011</cell><cell>80.05</cell><cell>0.049</cell><cell>82.82</cell></row><row><cell>Ec</cell><cell>0.147</cell><cell>32.43</cell><cell>0.143</cell><cell>24.93</cell></row><row><cell>Eo</cell><cell>0.023</cell><cell>72.54</cell><cell>0.084</cell><cell>63.29</cell></row><row><cell>Ec + Eo</cell><cell>0.045</cell><cell>64.18</cell><cell>0.091</cell><cell>60.30</cell></row><row><cell>exp(Ec + Eo)</cell><cell>0.060</cell><cell>61.31</cell><cell>0.107</cell><cell>55.46</cell></row></table><note><p><p><p><p><p>of spatiotemporal edge generation on the SegTrack dataset</p><ref type="bibr" target="#b51">[52]</ref> </p>and extended SegTrack dataset</p><ref type="bibr" target="#b52">[53]</ref></p>.</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Track to the future: Spatiotemporal video segmentation with long-range motion cues</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lezama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video segmentation by tracing discontinuities in a trajectory embedding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video object segmentation by tracking regions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Segmenting salient objects from images and videos</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heikkilä</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Streaming hierarchical video segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Maximum weight cliques with mutex constraints for video object segmentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Video object segmentation through spatially accurate and temporally dense extraction of primary object regions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Category independent object proposals</title>
		<author>
			<persName><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What is an object?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Constrained parametric min-cuts for automatic object segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The discriminant center-surround hypothesis for bottom-up saliency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vijay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nuno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="497" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatiotemporal saliency in dynamic scenes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="177" />
			<date type="published" when="2010-01">Jan 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Static and space-time visual saliency detection by self-resemblance</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A geodesic framework for fast interactive image and video segmentation and matting</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Geodesic graph cut for interactive image segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geos: geodesic image segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Toby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Andrew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Geodesic image and video editing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Saliency-aware geodesic video object segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object segmentation in video: a hierarchical variational approach for turning point trajectories into dense regions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Higher order motion models and spectral clustering</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to segment moving objects in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Track and segment: An iterative unsupervised approach for video object proposals</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Primary object segmentation in videos via alternate convex optimization of foreground and background distributions</title>
		<author>
			<persName><forename type="first">W.-D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
	<note>Graph-based visual saliency</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Probabilistic learning of task-specific visual attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Saliency detection: a spectral residual approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatio-temporal saliency detection using phase spectrum of quaternion fourier transform</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Context-aware saliency detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goferman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Center-surround divergence of feature statistics for salient object detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Frintrop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient salient region detection with soft image abstraction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Warrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Geodesic saliency using background priors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Saliency optimization from robust background detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bottom-up saliency is a discriminant process</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cluster-based co-saliency detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3766" to="3778" />
			<date type="published" when="2013-10">Oct 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Time-mapping using space-time saliency</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A feature-integration theory of attention</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Treisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gelade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="136" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Do low-level visual features have a causal influence on gaze during dynamic scene viewing?</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mital</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="144" to="144" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient closedform solution to generalized boundary detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: Descriptor matching in variational motion estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="500" to="513" />
			<date type="published" when="2011-03">March 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Efficient algorithms for shortest paths in sparse networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="1977-01">Jan. 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="309" to="314" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1222" to="1239" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Motion coherent tracking using multi-label MRF optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Flagg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="202" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Video segmentation by tracking many figure-ground segments</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Vibe: A universal background subtraction algorithm for video sequences</title>
		<author>
			<persName><forename type="first">O</forename><surname>Barnich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Droogenbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1709" to="1724" />
			<date type="published" when="2011-06">June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Adaptive fragmentsbased tracking of non-rigid objects using level sets</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chockalingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Jots: Joint online tracking and segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Hough-based tracking of nonrigid objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Godec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Superpixel tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Causal video object segmentation from persistence of occlusions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soattoc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
