<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bayesian Graph Neural Networks with Adaptive Connection Sampling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-30">30 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Arman</forename><surname>Hasanzadeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Electrical and Computer Engineering De-partment</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<settlement>College Station</settlement>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ehsan</forename><surname>Hajiramezanali</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Electrical and Computer Engineering De-partment</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<settlement>College Station</settlement>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shahin</forename><surname>Boluki</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Electrical and Computer Engineering De-partment</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<settlement>College Station</settlement>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingyuan</forename><surname>Zhou</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">McCombs School of Business</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
								<address>
									<settlement>Austin</settlement>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nick</forename><surname>Duffield</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Electrical and Computer Engineering De-partment</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<settlement>College Station</settlement>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Krishna</forename><surname>Narayanan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Electrical and Computer Engineering De-partment</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<settlement>College Station</settlement>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoning</forename><surname>Qian</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Electrical and Computer Engineering De-partment</orgName>
								<orgName type="institution">Texas A&amp;M University</orgName>
								<address>
									<settlement>College Station</settlement>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bayesian Graph Neural Networks with Adaptive Connection Sampling</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-30">30 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2006.04064v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a unified framework for adaptive connection sampling in graph neural networks (GNNs) that generalizes existing stochastic regularization methods for training GNNs. The proposed framework not only alleviates oversmoothing and over-fitting tendencies of deep GNNs, but also enables learning with uncertainty in graph analytic tasks with GNNs. Instead of using fixed sampling rates or hand-tuning them as model hyperparameters as in existing stochastic regularization methods, our adaptive connection sampling can be trained jointly with GNN model parameters in both global and local fashions. GNN training with adaptive connection sampling is shown to be mathematically equivalent to an efficient approximation of training Bayesian GNNs. Experimental results with ablation studies on benchmark datasets validate that adaptively learning the sampling rate given graph training data is the key to boosting the performance of GNNs in semi-supervised node classification, making them less prone to over-smoothing and over-fitting with more robust prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph neural networks (GNNs), and its numerous variants, have shown to be successful in graph representation learning by extracting high-level features for nodes from their topological neighborhoods. GNNs have boosted the state-ofthe-art performance in a variety of graph analytic tasks, such as semi-supervised node classification and link prediction <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017;</ref><ref type="bibr">2016;</ref><ref type="bibr" target="#b17">Hasanzadeh et al., 2019;</ref><ref type="bibr" target="#b14">Hajiramezanali et al., 2019)</ref>. Despite their successes, GNNs have There exist a variety of methods to address these problems. For example, DropOut <ref type="bibr" target="#b36">(Srivastava et al., 2014)</ref> is a popular regularisation technique with deep neural networks (DNNs) to avoid over-fitting, where network units are randomly masked during training. In GNNs, DropOut is realized by randomly removing the node features during training <ref type="bibr" target="#b35">(Rong et al., 2019)</ref>. Often, the procedure is independent of the graph topology. However, empirical results have shown that, due to the nature of Laplacian smoothing in GNNs, graph convolutions have the over-smoothing tendency of mixing representations of adjacent nodes so that, when increasing the number of GNN layers, all nodes representations will converge to a stationary point, making them unrelated to node features <ref type="bibr" target="#b26">(Li et al., 2018)</ref>. While it has been shown in <ref type="bibr" target="#b24">Kipf &amp; Welling (2017)</ref> that DropOut alone is ineffectual in preventing over-fitting, partially due to over-smoothing, the combination of DropEdge, in which a set of edges are randomly removed from the graph, with DropOut has recently shown potential to alleviate these problems <ref type="bibr" target="#b35">(Rong et al., 2019)</ref>.</p><p>On the other hand, with the development of efficient posterior computation algorithms, there have been successes in learning with uncertainty by Bayesian extensions of traditional deep network architectures, including convolutional neural networks (CNNs). However, for GNNs, deriving their Bayesian extensions is more challenging due to their irregular neighborhood connection structures. In order to account for uncertainty in GNNs, <ref type="bibr" target="#b40">Zhang et al. (2019)</ref> present a Bayesian framework where the observed graph is viewed as a realization from a parametric family of random graphs. This allows joint inference of the graph and the GNN weights, leading to resilience to noise or adversarial attacks. Besides its prohibitive computational cost, the choice of the random graph model is important and can be inconsistent for different problems and datasets. Furthermore, the posterior inference in the current implementation only depends on the graph topology, but cannot consider node features.</p><p>In this paper, we introduce a general stochastic regularization technique for GNNs by adaptive connection sampling-Graph DropConnect (GDC). We show that existing GNN regularization techniques such as DropOut <ref type="bibr" target="#b36">(Srivastava et al., 2014)</ref>, DropEdge <ref type="bibr" target="#b35">(Rong et al., 2019)</ref>, and node sampling <ref type="bibr" target="#b3">(Chen et al., 2018)</ref> are special cases of GDC. GDC regularizes neighborhood aggregation in GNNs at each channel, separately. This prevents connected nodes in graph from having the same learned representations in GNN layers; hence better improvement without serious over-smoothing can be achieved. Furthermore, adaptively learning the connection sampling or drop rate in GDC enables better stochastic regularization given graph data for target graph analytic tasks. In fact, our ablation studies show that only learning the DropEdge rate, without any DropOut, already substantially improves the performance in semi-supervised node classification with GNNs. By probabilistic modeling of the connection drop rate, we propose a hierarchical beta-Bernoulli construction for Bayesian learnable GDC, and derive the solution with both continuous relaxation and direct optimization with Augment-REINFORCE-Merge (ARM) gradient estimates. With the naturally enabled UQ and regularization capability, our learnable GDC can help address both over-smoothing and UQ challenges to further push the frontier of GNN research.</p><p>We further prove that adaptive connection sampling of GDC at each channel can be considered as random aggregation and diffusion in GNNs, with a similar Bayesian approximation interpretation as in Bayesian DropOut for CNNs <ref type="bibr" target="#b8">(Gal &amp; Ghahramani, 2015)</ref>. Specifically, Monte Carlo estimation of GNN outputs can be used to evaluate the predictive posterior uncertainty. An important corollary of this formulation is that any GNN with neighborhood sampling, such as Graph-SAGE <ref type="bibr" target="#b16">(Hamilton et al., 2017)</ref>, could be considered as its corresponding Bayesian approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Bayesian Neural Networks</head><p>Bayesian neural networks (BNNs) aim to capture model uncertainty of DNNs by placing prior distributions over the model parameters to enable posterior updates during DNN training. It has been shown that these Bayesian extensions of traditional DNNs can be robust to over-fitting and provide appropriate prediction uncertainty estimation <ref type="bibr" target="#b9">(Gal &amp; Ghahramani, 2016;</ref><ref type="bibr" target="#b1">Boluki et al., 2020)</ref>. Often, the standard Gaussian prior distribution is placed over the weights. With random weights {W (l) } L l=1 , the output prediction given an input x can be denoted by f x, {W (l) } L l=1 , which is now a random variable in BNNs, enabling uncertainty quantification (UQ).</p><p>The key difficulty in using BNNs is that Bayesian inference is computationally intractable. There exist various methods that approximate BNN inference, such as Laplace approximation <ref type="bibr" target="#b29">(MacKay, 1992)</ref>, sampling-based and stochastic variational inference <ref type="bibr" target="#b32">(Paisley et al., 2012;</ref><ref type="bibr" target="#b34">Rezende et al., 2014;</ref><ref type="bibr" target="#b15">Hajiramezanali et al., 2020;</ref><ref type="bibr" target="#b5">Dadaneh et al., 2020a)</ref>, Markov chain Monte Carlo (MCMC) <ref type="bibr" target="#b31">(Neal, 2012)</ref>, and stochastic gradient MCMC <ref type="bibr" target="#b28">(Ma et al., 2015)</ref>. However, their computational cost is still much higher than the non-Bayesian methods, due to the increased model complexity and slow convergence <ref type="bibr" target="#b9">(Gal &amp; Ghahramani, 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">DropOut as Bayesian Approximation</head><p>Dropout is commonly used in training many deep learning models as a way to avoid over-fitting. Using dropout at test time enables UQ with Bayesian interpretation of the network outputs as Monte Carlo samples of its predictive distribution <ref type="bibr" target="#b9">(Gal &amp; Ghahramani, 2016)</ref>. Various dropout methods have been proposed to multiply the output of each neuron by a random mask drawn from a desired distribution, such as Bernoulli <ref type="bibr" target="#b18">(Hinton et al., 2012;</ref><ref type="bibr" target="#b36">Srivastava et al., 2014)</ref> and Gaussian <ref type="bibr" target="#b22">(Kingma et al., 2015;</ref><ref type="bibr" target="#b36">Srivastava et al., 2014)</ref>. Bernoulli dropout and its extensions are the most commonly used in practice due to their ease of implementation and computational efficiency in existing deep architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Over-smoothing &amp; Over-fitting in GNNs</head><p>It has been shown that graph convolution in graph convolutional neural networks (GCNs) <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017)</ref> is simply a special form of Laplacian smoothing, which mixes the features of a node and its nearby neighbors. Such diffusion operations lead to similar learned representations when the corresponding nodes are close topologically with similar features, thus greatly improving node classification performance. However, it also brings potential concerns of over-smoothing <ref type="bibr" target="#b26">(Li et al., 2018)</ref>. If a GCN is deep with many convolutional layers, the learned representations may be over-smoothed and nodes with different topological and feature characteristics may become indistinguishable. More specifically, by repeatedly applying Laplacian smoothing many times, the node representations within each connected component of the graph will converge to the same values.</p><p>Moreover, GCNs, like other deep models, may suffer from over-fitting when we utilize an over-parameterized model to fit a distribution with limited training data, where the model we learn fits the training data very well but generalizes poorly to the testing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Stochastic Regularization &amp; Reduction for GNNs</head><p>Quickly increasing model complexity and possible overfitting and over-smoothing when modeling large graphs, as empirically observed in the GNN literature, have been conjectured for the main reason of limited performance from deep GNNs <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b35">Rong et al., 2019)</ref>. Several stochastic regularization and reduction methods in GNNs have been proposed to improve the deep GNN performance. For example, stochastic regularization techniques, such as DropOut <ref type="bibr" target="#b36">(Srivastava et al., 2014)</ref> and DropEdge <ref type="bibr" target="#b35">(Rong et al., 2019)</ref>, have been used to prevent over-fitting and over-smoothing in GNNs. Sampling-based stochastic reduction by random walk neighborhood sampling <ref type="bibr" target="#b16">(Hamilton et al., 2017)</ref> and node sampling <ref type="bibr" target="#b3">(Chen et al., 2018)</ref> has been deployed in GNNs to reduce the size of input data and thereafter model complexity. Next, we review each of these methods and show that they can be formulated in our proposed adaptive connection sampling framework.</p><p>Denote the output of the lth hidden layer in GNNs by</p><formula xml:id="formula_0">H (l) = [h (l) 0 , . . . , h (l)</formula><p>n ] T ∈ R n×f l with n being the number of nodes and f l being the number of output features at the lth layer. Assume H (0) = X ∈ R n×f0 is the input matrix of node attributes, where f 0 is the number of nodes features. Also, assume that W (l) ∈ R f l ×f l+1 and σ( • ) are the GNN parameters at the lth layer and the corresponding activation function, respectively. Moreover, N (v) denotes the neighborhood of node v; N (v) = N (v) ∪ {v}; and N(.) is the normalizing operator, i.e., N(A) = I N + D −1/2 A D −1/2 . Finally, represents the Hadamard product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1.">DROPOUT (SRIVASTAVA ET AL., 2014)</head><p>In a GNN layer, DropOut randomly removes output elements of its previous hidden layer H (l) based on independent Bernoulli random draws with a constant success rate at each training iteration. This can be formulated as follows:</p><formula xml:id="formula_1">H (l+1) = σ N(A)(Z (l) H (l) ) W (l) ,<label>(1)</label></formula><p>where Z (l) is a random binary matrix, with the same dimensions as H (l) , whose elements are samples of Bernoulli(π). Despite its success in fully connected and convolutional neural networks, DropOut has shown to be ineffectual in GNNs for preventing over-fitting and over-smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2.">DROPEDGE (RONG ET AL., 2019)</head><p>DropEdge randomly removes edges from the graph by drawing independent Bernoulli random variables (with a constant rate) at each iteration. More specifically, a GNN layer with DropEdge can be written as follows:</p><formula xml:id="formula_2">H (l+1) = σ N(A Z (l) ) H (l) W (l) ,<label>(2)</label></formula><p>Note that here, the random binary mask, i.e. Z (l) , has the same dimensions as A. Its elements are random samples of Bernoulli(π) where their corresponding elements in A are non-zero and zero everywhere else. It has been shown that the combination of DropOut and DropEdge reaches the best performance in terms of mitigating overfitting in GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3.">NODE SAMPLING (CHEN ET AL., 2018)</head><p>To reduce expensive computation in batch training of GNNs, due to the recursive expansion of neighborhoods across layers, <ref type="bibr" target="#b3">Chen et al. (2018)</ref> propose to relax the requirement of simultaneous availability of test data. Considering graph convolutions as integral transforms of embedding functions under probability measures allows for the use of Monte Carlo approaches to consistently estimate the integrals. This leads to an optimal node sampling strategy, FastGCN, which can be formulated as</p><formula xml:id="formula_3">H (l+1) = σ N(A) diag(z (l) )H (l) W (l) ,<label>(3)</label></formula><p>where z (l) is a random vector whose elements are drawn from Bernoulli(π). This, indeed, is a special case of DropOut, as all of the output features for a node are either completely kept or dropped while DropOut randomly removes some of these related output elements associated with the node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Graph DropConnect</head><p>We propose a general stochastic regularization technique for GNNs-Graph DropConnect (GDC)-by adaptive connection sampling, which can be interpreted as an approximation of Bayesian GNNs.</p><p>In GDC, we allow GNNs to draw different random masks for each channel and edge independently. More specifically, the operation of a GNN layer with GDC is defined as follows:</p><formula xml:id="formula_4">H (l+1) [:, j] = σ f l i=1 N(A Z (l) i,j ) H (l) [:, i] W (l) [i, j] , for j = 1, . . . , f l+1<label>(4)</label></formula><p>where f l and f l+1 are the number of features at layers l and l + 1, respectively, and</p><formula xml:id="formula_5">Z (l)</formula><p>i,j is a sparse random matrix (with the same sparsity as A) whose non-zero elements are randomly drawn by Bernoulli(π l ). Note that π l can be different for each layer for GDC instead of assuming the same constant drop rate for all layers in previous methods.</p><p>As shown in (1), (2), and (3), DropOut <ref type="bibr" target="#b36">(Srivastava et al., 2014)</ref>, DropEdge <ref type="bibr" target="#b35">(Rong et al., 2019)</ref>, and Node Sampling <ref type="bibr" target="#b3">(Chen et al., 2018)</ref> have different sampling assumptions on channels, edges, or nodes, yet there is no clear evidence to favor one over the other in terms of consequent graph analytic performance. In the proposed GDC approach, there is a free parameter {Z (l) i,j ∈ {0, 1} n×n } f l i=1 to adjust the binary mask for the edges, nodes and channels. Thus the proposed GDC model has one extra degree of freedom to incorporate flexible connection sampling.</p><p>The previous stochastic regularization techniques can be considered as special cases of GDC. To illustrate that, we assume Z (l) i,j are the same for all j ∈ {1, 2, . . . , f l+1 }, thus we can omit the indices of the output elements at layer l + 1 and rewrite (4) as <ref type="table">and diag(z   (l)</ref> N S ) ∈ {0, 1} n×n be the random binary matrices corresponding to the ones adopted in DropOut <ref type="bibr" target="#b36">(Srivastava et al., 2014)</ref>, DropEdge <ref type="bibr" target="#b35">(Rong et al., 2019)</ref>, and Node Sampling <ref type="bibr" target="#b3">(Chen et al., 2018)</ref>, respectively. The random mask {Z</p><formula xml:id="formula_6">H (l+1) = σ f l i=1 N(A Z (l) i ) H (l) [:, i] W (l) [i, :] (5) Define J n as a n×n all-one matrix. Let Z (l) DO ∈ {0, 1} n×f l , Z (l) DE ∈ {0, 1} n×n ,</formula><formula xml:id="formula_7">(l) i ∈ {0, 1} n×n } f l</formula><p>i=1 in GDC become the same as those of the DropOut when</p><formula xml:id="formula_8">Z (l) i = J n diag(Z (l) DO [:, i]), the same as those of DropE- dge when {Z (l) i } f l i=1 = Z (l)</formula><p>DE , and the same as those of node sampling when {Z</p><formula xml:id="formula_9">(l) i } f l i=1 = J n diag(z<label>(l)</label></formula><p>N S ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">GDC as Bayesian Approximation</head><p>In GDC, random masking is applied to the adjacency matrix of the graph to regularize the aggregation steps at each layer of GNNs. In existing Bayesian neural networks, the model parameters, i.e. W (l) , are considered random to enable Bayesian inference based on predictive posterior given training data <ref type="bibr" target="#b10">(Gal et al., 2017;</ref><ref type="bibr" target="#b1">Boluki et al., 2020)</ref>.</p><p>Here, we show that connection sampling in GDC can be transformed from the output feature space to the parameter space so that it can be considered as appropriate Bayesian extensions of GNNs.</p><p>First, we rewrite (5) to have a node-wise view of a GNN layer with GDC. More specifically,</p><formula xml:id="formula_10">h (l+1) v = σ   1 c v u∈ N (v) z (l) vu h (l) u W (l)   ,<label>(6)</label></formula><p>where c v is a constant derived from the degree of node v, and z</p><formula xml:id="formula_11">(l)</formula><p>vu ∈ {0, 1} 1×f l is the mask row vector corresponding to connection between nodes v and u in three dimensional tensor</p><formula xml:id="formula_12">Z (l) = [Z (l) 1 , . . . , Z (l) f l ].</formula><p>For brevity and without loss of generality, we ignore the constant c v in the rest of this section. We can rewrite and reorganize (6) to transform the randomness from sampling to the parameter space as</p><formula xml:id="formula_13">h (l+1) v = σ   u∈ N (v) h (l) u diag(z (l) vu ) W (l)   = σ   u∈ N (v) h (l) u diag(z (l) vu ) W (l)   . (7) Define W (l) vu := z (l)</formula><p>vu W (l) . We have:</p><formula xml:id="formula_14">h (l+1) v = σ   u∈ N (v) h (l) u W (l) vu   . (8) W (l)</formula><p>vu , which pairs the corresponding weight parameter with the edge in the given graph. The operation with GDC in (8) can be interpreted as learning different weights for each of the message passing along edges e = (u, v) ∈ E where E is the union of edge set of the input graph and self-loops for all nodes.</p><p>Following the variational interpretation in <ref type="bibr" target="#b10">Gal et al. (2017)</ref>, GDC can be seen as an approximating distribution q θ (ω) for the posterior p(ω | A, X) when considering a set of random weight matrices ω = {ω e } |E| e=1 in the Bayesian framework, where ω e = {W (l) e } L l=1 is the set of random weights for the eth edge, |E| is the number of edges in the input graph, and θ is the set of variational parameters. The Kullback-Leibler (KL) divergence KL(q θ (ω)||p(ω)) is considered in training as a regularisation term, which ensures that the approximating q θ (ω) does not deviate too far from the prior distribution. To be able to evaluate the KL term analytically, the discrete quantised Gaussian can be adopted as the prior distribution as in <ref type="bibr" target="#b10">Gal et al. (2017)</ref>. Further with the factorization q θ (ω) over L layers and |E| edges such that q θ (ω) = l e q θ l (W (l) e ) and letting</p><formula xml:id="formula_15">q θ l (W (l) e ) = π l δ(W (l) e − 0) + (1 − π l )δ(W (l) e − M (l) ),</formula><p>where θ l = {M (l) , π l }, the KL term can be written as</p><formula xml:id="formula_16">L l=1 |E| e=1 KL(q θ l (W (l) e ) || p(W (l)</formula><p>e )) and approximately</p><formula xml:id="formula_17">KL(q θ l (W (l) e ) || p(W (l) e )) ∝ (1 − π l ) 2 || M (l) || 2 − H(π l ),</formula><p>where H(π l ) is the entropy of a Bernoulli random variable with the success rate π l .</p><p>Since the entropy term does not depend on network weight parameters M (l) , it can be omitted when π l is not optimized. But we learn π l in GDC, thus the entropy term is important.</p><p>Minimizing the KL divergence with respect to the drop rate π l is equivalent to maximizing the entropy of a Bernoulli random variable with probability 1 − π l . This pushes the drop rate towards 0.5, which may not be desired in some cases where higher/lower drop rate probabilities are more appreciated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Variational Inference for GDC</head><p>We consider z</p><formula xml:id="formula_18">(l)</formula><p>e and W (l) as local and global random variables, respectively, and denote</p><formula xml:id="formula_19">Z (l) = {z (l) e } |E| e=1 and ω (l) = {W (l) e } |E| e=1</formula><p>. For inference of this approximating model with GDC, we assume a factorized variational distribution q(ω (l) , Z (l) ) = q(ω (l) ) q(Z (l) ). Let the prior distribution p(W (l) e ) be a discrete quantised Gaussian and p(ω</p><formula xml:id="formula_20">(l) ) = E e=1 p(W (l)</formula><p>e ).Therefore, the KL term can be written as</p><formula xml:id="formula_21">L l=1 KL(q(ω (l) , Z (l) ) || p(ω (l) , Z (l) )), with KL q(ω (l) , Z (l) ) || p(ω (l) , Z (l) ) ∝ |E|(1 − π l ) 2 ||M (l) || 2 + |E| e=1 KL q(z (l) e ) || p(z (l) e ) .</formula><p>The KL term consists of the common weight decay in the non-Bayesian GNNs with the additional KL term</p><formula xml:id="formula_22">|E| e=1 KL(q(z (l) e ) || p(z (l)</formula><p>e )) that acts as a regularization term for z (l) e . In this GDC framework, the variational inference loss, for node classification for example, can be written as</p><formula xml:id="formula_23">L({M (l) , π l } L l=1 ) = E q({ω (l) ,Z (l) } L l=1 ) [logP (Y o |X, {ω (l) , Z (l) } L l=1 )] − L l=1 KL(q(ω (l) , Z (l) ) || p(ω (l) , Z (l) )),<label>(9)</label></formula><p>where Y o denotes the collection of the available labels for the observed nodes. The optimization of ( <ref type="formula" target="#formula_23">9</ref>) with respect to the weight matrices can be done by a Monte Carlo sample, i.e. sampling a random GDC mask and calculating the gradients with respect to {M (l) } L l=1 with stochastic gradient descent. It is easy to see that if {π l } L l=1 are fixed, implementing our GDC is as simple as using common regularization terms on the neural network weights.</p><p>We aim to optimize the drop rates {π l } L l=1 jointly with the weight matrices. This clearly provides more flexibility as all the parameters of the approximating posterior will be learned from the data instead of being fixed a priori or treated as hyper-parameters, often difficult to tune. However, the optimization of (9) with respect to the drop rates is challenging. Although the KL term is not a function of the random masks, the commonly adopted reparameterization techniques <ref type="bibr" target="#b34">(Rezende et al., 2014;</ref><ref type="bibr" target="#b21">Kingma &amp; Welling, 2013)</ref> are not directly applicable here for computing the expectation in the first term since the drop masks are binary. Moreover, score-function gradient estimators, such as REIN-FORCE <ref type="bibr" target="#b38">(Williams, 1992;</ref><ref type="bibr" target="#b7">Fu, 2006)</ref>, possess high variance. One potential solution is continuous relaxation of the drop masks. This approach has lower variance at the expense of introducing bias. Another solution is the direct optimization with respect to the discrete variables by the recently developed Augment-REINFORCE-Merge (ARM) method <ref type="bibr" target="#b39">(Yin &amp; Zhou, 2019)</ref>, which has been used in BNNs <ref type="bibr" target="#b1">(Boluki et al., 2020)</ref> and information retrieval <ref type="bibr" target="#b6">(Dadaneh et al., 2020b;</ref><ref type="bibr">a)</ref>. In the next section, we will discuss in detail about our GDC formulation with more flexible beta-Bernoulli prior construction for adaptive connection sampling and how we solve the joint optimization problem for training GNNs with adaptive connection sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Variational Beta-Bernoulli GDC</head><p>The sampling or drop rate in GDC can be set as a constant hyperparameter as commonly done in other stochastic regularization techniques. In this work, we further enrich GDC with an adaptive sampling mechanism, where the drop rate is directly learned together with GNN parameters given graph data. In fact, in the Bayesian framework, such a hierarchical construct may increase the model expressiveness to further improve prediction and uncertainty estimation performance, as we will show empirically in Section 7.</p><p>Note that in this section, for brevity and simplicity we do the derivations for one feature dimension only, i.e. f l = 1. Extending to multi-dimensional features is straightforward as we assume the drop masks are independent across features. Therefore, we drop the feature index in our notations. Inspired by the beta-Bernoulli process <ref type="bibr" target="#b37">(Thibaux &amp; Jordan, 2007)</ref>, whose marginal representation is also known as the Indian Buffet Process (IBP) <ref type="bibr" target="#b11">(Ghahramani &amp; Griffiths, 2006)</ref>, we impose a beta-Bernoulli prior to the binary random masks as</p><formula xml:id="formula_24">a (l) e = z (l)</formula><p>e a e , z (l) e ∼ Bernoulli(π l ),</p><formula xml:id="formula_25">π l ∼ Beta(c/L, c(L − 1)/L),<label>(10)</label></formula><p>where a e denotes an element of the adjacency matrix A corresponding to an edge e, and â(l) e an element of the matrix Â(l) = A Z (l) . Such a formulation is known to be capable of enforcing sparsity in random masks <ref type="bibr" target="#b41">(Zhou et al., 2009;</ref><ref type="bibr" target="#b13">Hajiramezanali et al., 2018)</ref>, which has been shown to be necessary for regularizing deep GNNs as discussed in DropEdge <ref type="bibr" target="#b35">(Rong et al., 2019)</ref>.</p><p>With this hierarchical beta-Bernoulli GDC formulation, inference based on Gibbs sampling can be computationally demanding for large datasets, including graph data <ref type="bibr" target="#b17">(Hasanzadeh et al., 2019)</ref>. In the following, we derive efficient variational inference algorithm(s) for learnable GDC.</p><p>To perform variational inference for GDC random masks and the corresponding drop rate at each GNN layer together with weight parameters, we define the variational distribution as q(Z (l) , π l ) = q(Z (l) | π l )q(π l ). We define q(π l ) to be Kumaraswamy distribution <ref type="bibr" target="#b25">(Kumaraswamy, 1980)</ref>; as an alternative to the beta prior factorized over lth layer</p><formula xml:id="formula_26">q(π l ; a l , b l ) = a l b l π a l −1 l (1 − π a l l ) b l −1 ,<label>(11)</label></formula><p>where a l and b l are greater than zero. Knowing π l the edges are independent, thus we can rewrite q(Z (l</p><formula xml:id="formula_27">) | π l ) = |E| e=1 q(z (l)</formula><p>e | π l ). We further put a Bernoulli distribution with parameter π l over q(z (l) e |π l ). The KL divergence term can be written as</p><formula xml:id="formula_28">KL q(Z (l) , π l ) || p(Z (l) , π l ) = |E| e=1 KL q(z (l) e | π l ) || p(z (l) e | π l ) + KL (q(π l ) || p(π l )) .</formula><p>While the first term is zero due to the identical distributions, the second term can be computed in closed-form as</p><formula xml:id="formula_29">KL (q(π l ) || p(π l )) = a l − c/L a l −γ − Ψ(b l ) − 1 b l + log a l b l c/L − b l − 1 b l ,</formula><p>where γ is the Euler-Mascheroni constant and Ψ(•) is the digamma function.</p><p>The gradient of the KL term in (9) can easily be calculated with respect to the drop parameters. However, as mentioned in the previous section, due to the discrete nature of the random masks, we cannot directly apply reparameterization technique to calculate the gradient of the first term in ( <ref type="formula" target="#formula_23">9</ref>) with respect to the drop rates (parameters). One way to address this issue is to replace the discrete variables with a continuous approximation. We impose a concrete distribution relaxation <ref type="bibr" target="#b19">(Jang et al., 2016;</ref><ref type="bibr" target="#b10">Gal et al., 2017)</ref> for the Bernoulli random variable z</p><formula xml:id="formula_30">(l)</formula><p>uv , leading to an efficient optimization by sampling from simple sigmoid distribution which has a convenient parametrization</p><formula xml:id="formula_31">z(l) e = sigmoid 1 t log π l 1 − π l + log u 1 − u ,<label>(12)</label></formula><p>where u ∼ Unif[0, 1] and t is temperature parameter of relaxation. We can then use stochastic gradient variational Bayes to optimize the variational parameters a l and b l .</p><p>Although this approach is simple, the relaxation introduces bias. Our other approach is to directly optimize the variational parameters using the original Bernoulli distribution in the formulation as in <ref type="bibr" target="#b1">Boluki et al. (2020)</ref>. We can calculate the gradient of the variational loss with respect to α = {logit(1 − π l )} L l=1 using ARM estimator , which is unbiased and has low variance, by performing two forward passes as</p><formula xml:id="formula_32">∇ u L(α) = E u∼ L l=1 |E| e=1 Unif[0,1](u (l) e ) L({M (l) } L l=1 , 1 [u&gt;σ(−α)] ) − L({M (l) } L l=1 , 1 [u&lt;σ(α)] ) u − 1 2 , where L({M (l) } L l=1 , 1 [u&lt;σ(α)]</formula><p>) denotes the loss obtained by setting</p><formula xml:id="formula_33">Z (l) = 1 [u (l) &lt;σ(α l )] := 1 [u (l) 1 &lt;σ(α l )] , . . . , 1 [u (l)</formula><p>|E| &lt;σ(α l )] for l = 1, . . . , L. The gradient with respect to {a l , b l } L l=1 can then be calculated by using the chain rule and the reparameterization for</p><formula xml:id="formula_34">π l = (1 − u 1 b l ) 1 a l , u ∼ Unif[0, 1].</formula><p>It is worth noting that although the beta-Bernoulli DropConnect with ARM is expected to provide better performance due to the more accurate gradient estimates, it has slightly higher computational complexity as it requires two forward passes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Connection to Random Walk Sampling</head><p>Various types of random walk have been used in graph representation learning literature to reduce the size of input graphs. In GNNs, specifically in GraphSAGE <ref type="bibr" target="#b16">(Hamilton et al., 2017)</ref>, random walk sampling has been deployed to reduce the model complexity for very large graphs. One can formulate a GNN layer with random walk sampling as follows:</p><formula xml:id="formula_35">h (l+1) v = σ   ( u∈ N (v) (z (l) vu | Z (l−1) ) h (l) u ) W (l)   . (13)</formula><p>Here, Z (l) is the same as the one in DropEdge except that it is dependent on the masks from the previous layer. This is due to the fact that random walk samples for each node are connected subgraphs.</p><p>In this setup, we can decompose the variational distribution of the GDC formulation in an autoregressive way.</p><p>Specifically, here we have q(z</p><formula xml:id="formula_36">(l) uv |Z (l−1) ) = Bernoulli(π l )1 u∈ N (v) z (l−1) vu &gt;0 .</formula><p>With fixed Bernoulli parameters, we can calculate the gradients for the weight matrices with Monte Carlo integration. Learning Bernoulli parameters is challenging and does not allow direct application of ARM due to the autoregressive structure of the variational posterior. We leave sequential ARM for future study. Corollary 1 Any graph neural network with random walk sampling, such as GraphSAGE, is an approximation of a Bayesian graph neural network as long as outputs are calculated using Monte Carlo sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Sampling Complexity</head><p>The number of random samples needed for variational inference in GDC, (4), at each layer of a GNN is |E| × f l × f l+1 . This number would reduce to |E| × f l in the constrained version of GDC as shown in (5). These numbers, potentially, could be very high specially if the size of the graph or the number of filters are large, which could increase the space complexity and computation time. To circumvent this issue, we propose to draw a single sample for a block of features as oppose to drawing a new sample for every single feature. This would reduce the number of required samples to |E| × nb with nb being the number of blocks. In our experiments, we have one block in the first layer and two blocks in layers after that. In our experiments, we keep While in our GDC formulation, as shown in ( <ref type="formula" target="#formula_4">4</ref>) and ( <ref type="formula">5</ref>), the normalization N(•) is applied after masking, one can multiply the randomly drawn mask with the pre-computed normalized adjacency matrix. This relaxation reduces the computation time and has negligible effect on the performance based on our experiments. An extension to the GDC sampling strategy is asymmetric sampling where the mask matrix Z could be asymmetric. This would increase the number of samples by a factor of two; however it increases the model flexibility. In our experiments, we have used asymmetric masks and multiplied the mask with the normalized adjacency matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Numerical Results</head><p>We test the performance of our adaptive connection sampling framework, learnable GDC, on semi-supervised node classification using real-world citation graphs. In addition, we compare the uncertainty estimates of predictions by Monte Carlo beta-Bernoulli GDC and Monte Carlo Dropout. We also show the performance of GDC compared to existing methods in alleviating the issue of over-smoothing in GNNs. Furthermore, we investigate the effect of the number of blocks on the performance of GDC. We have also investigated learning separate drop rates for every edge in the network, i.e. local GDC, which is included in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Semi-supervised Node Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1.">DATASETS AND IMPLEMENTATION DETAILS</head><p>We conducted extensive experiments for semi-supervised node classification with real-world citation datasets. We consider Cora, Citeseer and Cora-ML datasets, and preprocess and split them same as <ref type="bibr" target="#b24">Kipf &amp; Welling (2017)</ref> and <ref type="bibr" target="#b0">Bojchevski &amp; Gunnemann (2018)</ref>. We train beta-Bernoulli GDC (BBGDC) models for 2000 epochs with a learning rate of 0.005 and a validation set used for early stopping. All of the hidden layers in our implemented GCNs have 128 dimen-sional output features. We use 5 × 10 −3 , 10 −2 , and 10 −3 as L2 regularization factor for Cora, Citeseer, and Cora-ML, respectively. For the GCNs with more than 2 layers, we use warm-up during the first 50 training epochs to gradually impose the beta-Bernoulli KL term in the objective function. The temperature in the concrete distribution is set to 0.67. For a fair comparison, the number of hidden units are the same in the baselines and their hyper-parameters are hand-tuned to achieve their best performance. Performance is reported by the average accuracy with standard deviation based on 5 runs on the test set. The dataset statistics as well as more implementation details are included in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2.">DISCUSSION</head><p>Table <ref type="table" target="#tab_0">1</ref> shows that BBGDC outperforms the state-of-the-art stochastic regularization techniques in terms of accuracy in all benchmark datasets. DO and DE in the table stand for DropOut and DropEdge, respectively. Comparing GCN-DO and GCN-DE, we can see that DropEdge alone is less effective than DropOut in overcoming over-smoothing and over-fitting in GCNs. The difference between accuracy of GCN-DO and GCN-DE is more substantial in deeper networks (5% in Cora, 7% in Citeseer, and 13% in Cora-ML), which further proves the limitations of DE. Among the baselines, combination of DO and DE shows the best performance allowing to have deeper models. However, this is not always true. For example in Citeseer, 4-layer GCN shows significant decrease in performance compared to 2-layer GCN.</p><p>To show the advantages of learning the drop rates as well as the effect of hierarchical beta-Bernoulli construction, we have also evaluated beta-Bernoulli DropEdge (BBDE) with the concrete approximation, in which the edge drop rate at each layer is learned using the same beta-Bernoulli hierarchical construction as GDC. We see that GCN with BBDE, without any DropOut, performs better than both GCNs with DE and DO-DE. By comparing GCN with BBDE and GCN with BBGDC, it is clear that the improvement is not only due to learnable sampling rate but also the increased flexibility of GDC compared to DropEdge. We note that GCN-BBGDC is the only method for which the accuracy improves by increasing the number of layers except in Citeseer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3.">CONCRETE RELAXATION VERSUS ARM</head><p>To investigate the effect of direct optimization of the variational loss with respect to the drop parameters with ARM vs relaxation of the discrete random variables with concrete, we construct three ARM optimization-based variants of our method: Learnable Bernoulli DropEdge with ARM gradient estimator (BDE-ARM) where the edge drop rate of the Bernoulli mask at each layer is directly optimized; beta-Bernoulli DropEdge with ARM (BBDE-ARM); and beta-Bernoulli GDC with ARM (BBGDC-ARM). We evaluate these methods on the 4-layer GCN setups where significant performance improvement compared with the baselines has been achieved by BBDE and GDC with concrete relaxation.</p><p>Comparing the performance of BBDE-ARM and BBGDC-ARM in Table <ref type="table" target="#tab_1">2</ref> with the corresponding models with concrete relaxation, suggests further improvement when the drop parameters are directly optimized. Moreover, BDE-ARM, which optimizes the parameters of the Bernoulli drop rates, performs better than DO, DE, and DO-DE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Uncertainty Quantification</head><p>To evaluate the quality of uncertainty estimates obtained by our model, we use the Patch Accuracy vs Patch Uncertainty (PAvPU) metric introduced in <ref type="bibr" target="#b30">(Mukhoti &amp; Gal, 2018)</ref>. PAvPU combines p(accurate|certain), i.e. the probability that the model is accurate on its output given that it is confident on the same, p(certain|inaccurate), i.e. the probability that the model is uncertain about its output given that it has made a mistake in its prediction, into a single metric. More specifically, it is defined as PAvPU = (n ac + n iu )/(n ac + n au + n ic + n iu ), where n ac is the number of accurate and certain predictions, n au is the number of accurate and uncertain predictions, n ic is the number of inaccurate and certain predictions, and n iu is the number of inaccurate and uncertain predictions.</p><p>Higher PAvPU means that certain predictions are accurate and inaccurate predictions are uncertain.</p><p>We here demonstrate the results for uncertainty estimates for a 4-layer GCN-DO and a 4-layer GCN-BBGDC with random initialization for semi-supervised node classification on Cora. We have evaluated PAvPU using 20 Monte Carlo sam- ples for the test set where we use predictive entropy as the uncertainty metric. The results are shown in Figure <ref type="figure" target="#fig_0">1</ref>. It can be seen that our proposed model consistently outperforms GCN-DO on every uncertainty threshold ranging from 0.5 to 1 of the maximum predictive uncertainty. While Figure <ref type="figure" target="#fig_0">1</ref> depicts the results based on one random initialization, other initializations show the same trend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Over-smoothing and Over-fitting</head><p>To check how GDC helps alleviate over-smoothing in GCNs, we have tracked the total variation (TV) of the outputs of hidden layers during training. TV is a metric used in the graph signal processing literature to measure the smoothness of a signal defined over nodes of a graph <ref type="bibr" target="#b4">(Chen et al., 2015)</ref>. More specifically, given a graph with the adjacency matrix A and a signal x defined over its nodes, TV is defined as</p><formula xml:id="formula_37">TV(x) = x − (1/|λ max |)A x 2 2</formula><p>, where λ max denotes the eigenvalue of A with largest magnitude. Lower TV shows that the signal on adjacent nodes are closer to each other, indicating possible over-smoothing.</p><p>We have compared the TV trajectories of the hidden layer outputs in a 4-layer GCN-BBGDC and a 4-layer GCN-DO normalized by their Frobenius norm, depicted in Figure <ref type="figure" target="#fig_1">2(a)</ref>. It can be seen that, in GCN-DO, while the TV of the first layer is slightly increasing at each training epoch, the TV of the second hidden layer decreases during training. This, indeed, contributed to the poor performance of GCN-DO. On the contrary, the TVs of both first and second layers in GCN-BBGDC is increasing during training. Not only this robustness is due to the dropping connections in GDC framework, but also is related to its learnable drop rates.</p><p>With such promising results showing less over-smoothing with BBGDC, we further investigate how our proposed method works in deeper networks. We have checked the accuracy of GCN-BBGDC with a various number of 128- dimensional hidden layers ranging from 2 to 16. The results are shown in Figure <ref type="figure" target="#fig_1">2</ref>(b). The performance improves up to the GCN with 4 hidden layers and decreases after that. It is important to note that even though the performance drops by adding the 5-th layer, the degree to which it decreases is far less than competing methods. For example, the node classification accuracy with GCN-DO quickly drops to 69.50% and 64.5% with 8 and 16 layers. In addition, we should mention that the performance of GCN-DO only improves from two to three layers. This, indeed, proves GDC is a better stochastic regularization framework for GNNs in alleviating over-fitting and over-smoothing, enabling possible directions to develop deeper GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Effect of Number of Blocks</head><p>In GDC for every pair of input and output features, a separate mask for the adjacency matrix should be drawn. However, as we discussed in Section 6, this demands large memory space. We circumvented this problem by drawing a single mask for a block of features. While we used only two blocks in our experiments presented so far, we here investigate the effect of the number of blocks on the node classification accuracy. The performance of 128-dimensional 4-layer GCN-BBGDC with 2, 16, and 32 blocks are shown in Table <ref type="table" target="#tab_2">3</ref>. As can be seen, the accuracy improves as the number of blocks increases. This is due to the fact that increasing the number of blocks increases the flexibility of GDC. The choice of the number of blocks is a factor to consider for the trade off between the performance and memory </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this paper, we proposed a unified framework for adaptive connection sampling in GNNs that generalizes existing stochastic regularization techniques for training GNNs. Our proposed method, Graph DropConnect (GDC), not only alleviates over-smoothing and over-fitting tendencies of deep GNNs, but also enables learning with uncertainty in graph analytic tasks with GNNs. Instead of using fixed sampling rates, our GDC technique parameters can be trained jointly with GNN model parameters. We further show that training a GNN with GDC is equivalent to an approximation of training Bayesian GNNs. Our experimental results shows that GDC boosts the performance of GNNs in semi-supervised classification task by alleviating over-smoothing and overfitting. We further show that the quality of uncertainty derived by GDC is better than DropOut in GNNs.</p><p>Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Datasets and Implementation Details</head><p>All of the models are implemented in PyTorch <ref type="bibr" target="#b33">(Paszke et al., 2017)</ref>. All of the simulations are conducted on a single NVIDIA GeForce RTX 2080 GPU node. We evaluate our proposed methods, GCN-BBDE and GCN-BBGDC, and baselines on three standard citation network benchmark datasets. We preprocess and split the dataset as done in <ref type="bibr" target="#b24">(Kipf &amp; Welling, 2017)</ref> and <ref type="bibr" target="#b0">(Bojchevski &amp; Gunnemann, 2018)</ref>.</p><p>For Cora and Cora-ML, we use 140 nodes for training, 500 nodes for validation and 1000 nodes for testing. For Citeseer, we use 120 nodes for training and the same number of nodes as Cora for validation and testing.  <ref type="bibr" target="#b20">(Kingma &amp; Ba, 2014)</ref> in all of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. GDC versus Other Stochastic Regularization Techniques</head><p>To further clarify the differences of our proposed GDC from existing stochastic regularization techniques, we draw the schematics of a GCN layer to which DropOut, DropEdge, Node Sampling, and our GDC are applied; shown in figures below. The input graph topology for the GCN layer is depicted in 3. The number of input and output features are both two in this toy example.     <ref type="bibr" target="#b35">(Rong et al., 2019)</ref>. Each circle is a feature and each square represents a node. DropEdge drops edges between nodes hence all of the connections between their corresponding channels are dropped. Note that the mask in DropEdge is symmetric. In this example, the edge between nodes 1 and 2 as well as the edge between nodes 1 and 4 are dropped. The dashed lines show dropped connections and the gray ones show the kept ones.  <ref type="bibr" target="#b3">(Chen et al., 2018)</ref>. Each circle is a feature and each square represents a node. FastGCN drops nodes hence all of the connections to that node are dropped. The faded nodes represents the dropped nodes. The dashed lines show dropped connections and the gray ones show the kept ones.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Comparison of uncertainty estimates in PAvPU by a 4layer GCN-BBGDC with 128-dimensional hidden layers and a 4-layer GCN-DO 128-dimensional hidden layers on Cora.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. From left to right: a) Total variation of the hidden layer outputs during training in a 4-layer GCN-BBGDC with 128-dimensional hidden layers and a 4-layer GCN-DO 128-dimensional hidden layers on Cora; b) Comparison of node classification accuracy for GCNs with a different number of hidden layers using different stochastic regularization methods. All of the hidden layers are 128 dimensional.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure3. Top: Schematic of a GCN layer on a graph with 4 nodes. Number of both input and output features are two. The connections are localized as explicitly depicted for node 2. Bottom: The same GCN layer shown in a more conventional way, i.e. each layer is a vector of neurons or features. Each circle is a feature and each square represents a node. The connections are sparse and the sparsity is based on the input graph topology. The connections for node 2 in layer l + 1 are highlighted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Schematic of our proposed GDC. Each circle is a feature and each square represents a node. GDC drops connections independently across layers. The dashed lines show dropped connections and the gray ones show the kept connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. Schematic of DropOut<ref type="bibr" target="#b36">(Srivastava et al., 2014)</ref>. Each circle is a feature and each square represents a node. DropOut drops features at each layer. The faded circles represent dropped features while the other ones are kept. The dashed lines show dropped connections and the gray ones show the kept ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure6. Schematic of DropEdge<ref type="bibr" target="#b35">(Rong et al., 2019)</ref>. Each circle is a feature and each square represents a node. DropEdge drops edges between nodes hence all of the connections between their corresponding channels are dropped. Note that the mask in DropEdge is symmetric. In this example, the edge between nodes 1 and 2 as well as the edge between nodes 1 and 4 are dropped. The dashed lines show dropped connections and the gray ones show the kept ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure7. Schematic of the node sampling strategy in FastGCN<ref type="bibr" target="#b3">(Chen et al., 2018)</ref>. Each circle is a feature and each square represents a node. FastGCN drops nodes hence all of the connections to that node are dropped. The faded nodes represents the dropped nodes. The dashed lines show dropped connections and the gray ones show the kept ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Semi-supervised node classification accuracy of GCNs with our adaptive connection sampling and baseline methods. ± 0.49 80.42 ± 0.25 71.46 ± 0.55 68.58 ± 0.88 84.62 ± 1.70 84.73 ± 0.52 GCN-BBGDC 81.80 ± 0.99 82.20 ± 0.92 71.72 ± 0.48 70.00 ± 0.36 85.43 ± 0.70 85.52 ± 0.83 the order of features the same as the original input files, and divide them into nb groups with the equal number of features.</figDesc><table><row><cell>Method</cell><cell cols="2">Cora</cell><cell>Citeseer</cell><cell></cell><cell cols="2">Cora-ML</cell></row><row><cell></cell><cell>2 layers</cell><cell>4 layers</cell><cell>2 layers</cell><cell>4 layers</cell><cell>2 layers</cell><cell>4 layers</cell></row><row><cell>GCN-DO</cell><cell>80.98 ± 0.48</cell><cell>78.24 ± 2.4</cell><cell cols="4">70.44 ± 0.39 64.38 ± 0.90 83.45 ± 0.73 81.51 ± 1.01</cell></row><row><cell>GCN-DE</cell><cell cols="6">78.36 ± 0.92 73.40 ± 2.07 70.52 ± 0.75 57.14 ± 0.90 83.30 ± 1.37 68.89 ± 3.37</cell></row><row><cell>GCN-DO-DE</cell><cell cols="6">80.58 ± 1.19 79.20 ± 1.07 70.74 ± 1.23 64.84 ± 0.98 83.61 ± 0.83 81.21 ± 1.53</cell></row><row><cell>GCN-BBDE</cell><cell>81.58</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Accuracy of ARM optimization-based variants of our proposed method in semi-supervised node classification.</figDesc><table><row><cell>Method</cell><cell cols="2">Cora (4 layers) Citeseer (4 layers)</cell></row><row><cell>GCN-BDE-ARM</cell><cell>79.95 ± 0.79</cell><cell>67.90 ± 0.15</cell></row><row><cell>GCN-BBDE-ARM</cell><cell>81.78 ± 0.81</cell><cell>69.43 ± 0.45</cell></row><row><cell>GCN-BBGDC-ARM</cell><cell>82.40 ± 0.60</cell><cell>70.25 ± 0.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Accuracy of 128-dimensional 4-layer GCN-BBGDC with different number of blocks on Cora in semi-supervised node classification.</figDesc><table><row><cell>Method</cell><cell cols="3">2 blocks 16 blocks 32 blocks</cell></row><row><cell>GCN-BBGDC</cell><cell>82.2</cell><cell>83.0</cell><cell>83.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Graph dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell cols="4"># Classes # Nodes # Edges # Features</cell></row><row><cell>Cora</cell><cell>7</cell><cell>2,708</cell><cell>5,429</cell><cell>1,433</cell></row><row><cell>Citeseer</cell><cell>3</cell><cell>3,327</cell><cell>4,732</cell><cell>3,703</cell></row><row><cell>Cora-ML</cell><cell>7</cell><cell>2,995</cell><cell>8,416</cell><cell>2,879</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table 4 provides the detailed statistics of the graph datasets used in our experiments. The warm-up factor used in GCN-BBGDC with more than 2 layers for Cora and Cora-ML is min({1, epoch/20}), and for Citeseer is min({1, epoch/40}). We have deployed Adam optimizer</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">* Equal contribution 1 Electrical and Computer Engineering Department, Texas A&amp;M University, College Station, Texas, USA 2 McCombs School of Business, The University of Texas at Austin, Austin, Texas, USA. Correspondence to: Arman Hasanzadeh &lt;armanihm@tamu.edu&gt;.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The presented materials are based upon the work supported by the National Science Foundation under Grants ENG-1839816, IIS-1848596, CCF-1553281, IIS-1812641, IIS-1812699, and CCF-1934904.   </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ablation Study: Global versus Local</head><p>We further investigate our learnable GDC, in which for each edge at each layer a different connection sampling distribution is learned. We refer to this scenario as the local learnable GDC. This, indeed, is a more general case than learning a single distribution for all edges in a layer. Expanding the variational beta-Bernoulli GDC to local learnable GDC is straightforward. Note that the KL term in the loss function can be derived in the same manner as in the global learnable GDC -as described in Section 4 of the paper -except that it will include the sum of num layers × num edges terms as opposed to the num layers terms in the global GDC.</p><p>By training the aforementioned model on the citation datasets, we find that the accuracy degrades and the KL divergence reduces to zero for every choice of prior. This phenomenon, which is known as posterior collapse or KL vanishing, is a common problem in variational autoencoders for language modeling <ref type="bibr" target="#b2">(Bowman et al., 2015;</ref><ref type="bibr" target="#b12">Goyal et al., 2017;</ref><ref type="bibr" target="#b27">Liu et al., 2019)</ref>. It is often due to over-parametrization in the model, which is indeed the case in the local learnable GDC. A solution to this issue could be making the parameters of the distribution dependent on the graph topology and/or node attributes. We leave this for future studies.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gunnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boluki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ardywibowo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Dadaneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05155</idno>
		<title level="m">Learnable Bernoulli dropout for Bayesian deep learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06349</idno>
		<title level="m">Generating sentences from a continuous space</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><surname>Fastgcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Signal recovery on graphs: Variation minimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kovačević</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="4609" to="4624" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pairwise supervised hashing with Bernoulli variational auto-encoder and self-control gradient estimator</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Dadaneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boluki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10477</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Arsm gradient estimator for supervised learning to rank</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Dadaneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boluki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="3157" to="3161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Gradient estimation. Handbooks in operations research and management science</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="575" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bayesian convolutional neural networks with bernoulli approximate variational inference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02158</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1050" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Concrete dropout</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3581" to="3590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Infinite latent feature models and the Indian buffet process</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="475" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Z-forcing: Training stochastic recurrent networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G A P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6713" to="6723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bayesian multi-domain learning for cancer subtype discovery from next-generation sequencing count data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hajiramezanali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Dadaneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karbalayghareh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9115" to="9124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Variational graph recurrent neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hajiramezanali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hasanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semiimplicit stochastic recurrent neural networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hajiramezanali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hasanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3342" to="3346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-implicit graph variational auto-encoders</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hasanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hajiramezanali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Auto-encoding variational Bayes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2575" to="2583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A generalized probability density function for double-bounded random processes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kumaraswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Hydrology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="79" to="88" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Cyclical annealing schedule: A simple approach to mitigating kl vanishing</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10145</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A complete recipe for stochastic gradient MCMC</title>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2917" to="2925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Bayesian methods for adaptive models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Mackay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Evaluating bayesian deep learning methods for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mukhoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12709</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">118</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6430</idno>
		<title level="m">Variational Bayesian inference with stochastic search</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">DropEdge: Towards the very deep graph convolutional networks for node classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hierarchical beta processes and the Indian buffet process</title>
		<author>
			<persName><forename type="first">R</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ARM: Augment-REINFORCEmerge gradient for stochastic binary networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bayesian graph convolutional neural networks for semi-supervised classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ustebay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5829" to="5836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Non-parametric Bayesian dictionary learning for sparse image representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2295" to="2303" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
