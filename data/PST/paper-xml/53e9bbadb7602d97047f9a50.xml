<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image-based Monte Carlo localisation with omnidirectional images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Emanuele</forename><surname>Menegatti</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Information Engineering (DEI)</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="laboratory">Intelligent Autonomous Systems Laboratory</orgName>
								<orgName type="institution">The University of Padua</orgName>
								<address>
									<addrLine>Via Gradenigo 6/a</addrLine>
									<postCode>I-35131</postCode>
									<settlement>Padova</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mauro</forename><surname>Zoccarato</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Information Engineering (DEI)</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="laboratory">Intelligent Autonomous Systems Laboratory</orgName>
								<orgName type="institution">The University of Padua</orgName>
								<address>
									<addrLine>Via Gradenigo 6/a</addrLine>
									<postCode>I-35131</postCode>
									<settlement>Padova</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Enrico</forename><surname>Pagello</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Information Engineering (DEI)</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="laboratory">Intelligent Autonomous Systems Laboratory</orgName>
								<orgName type="institution">The University of Padua</orgName>
								<address>
									<addrLine>Via Gradenigo 6/a</addrLine>
									<postCode>I-35131</postCode>
									<settlement>Padova</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Institute ISIB of CNR</orgName>
								<address>
									<settlement>Padua</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hiroshi</forename><surname>Ishiguro</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Adaptive Machine Systems</orgName>
								<orgName type="institution">Osaka University</orgName>
								<address>
									<postCode>565-0871</postCode>
									<settlement>Suita</settlement>
									<region>Osaka</region>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image-based Monte Carlo localisation with omnidirectional images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DEB69D3671EEF92E0AA3EBFAE92DADAD</idno>
					<idno type="DOI">10.1016/j.robot.2004.05.003</idno>
					<note type="submission">Received 5 May 2004; accepted 27 May 2004</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Omnidirectional vision</term>
					<term>Image-based navigation</term>
					<term>Fourier transform</term>
					<term>Fourier signature</term>
					<term>Monte Carlo localisation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Monte Carlo localisation generally requires a metrical map of the environment to calculate a robots position from the posterior probability density of a set of weighted samples. Image-based localisation, which matches a robots current view of the environment with reference views, fails in environments with perceptual aliasing. The method we present in this paper is experimentally demonstrated to overcome these disadvantages in a large indoor environment by combining Monte Carlo and image-based localisation. It exploits the properties of the Fourier transform of omnidirectional images, while weighting the samples according to the similarity among images. We also introduce a novel strategy for solving the "kidnapped robot problem".</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In mobile robotics the localisation problem is fundamental. In several successful experiments, the robot is provided with a geometrical map of the environment, and it uses some kind of sensors to locate itself in this map <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref>. However, it is not always possible (or convenient) to build a geometrical map of the environment. In the image-based localisation approach <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b7">8]</ref>, a map is not used and the agent uses a set of view" previously taken at different locations to locate itself. These locations are called reference locations. The corresponding images are called reference images. When the robot moves, it can compare the current view with the reference images stored in its visual memory. In the image-based localisation, the problem of finding the position of the robot in the environment is reduced to the problem of finding the best match for the current image among the reference images. Most of the cited works on image-based localisation just stop here. In this work, we enhance image-based localisation with a statistical approach in a way similar to the work reported in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9]</ref> (Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>Classical image-based localisation does not work in environments with periodical structures (for instance, The plan of the building where the omnidirectional image data set was built. This is a large environment in which the length of the longer corridor is 50 m. The thick line represents the robot's path. Details of the map are unimportant; it is presented just to convey the complexity of the environment. a corridor with a set of doors equally spaced or a set of junctions with the same appearance). In these environments, the appearance of the world is the same at different places, so the current view will match not only the corresponding reference image, but also all reference images similar to the current image. This is a case of perceptual aliasing, that is, the reading of the sensor is the same at different locations. So, the vision sensor is not able to discriminate these locations. A robot could use additional sensors to discriminate between two points, for example, a GPS sensor or other non-vision sensors. But what if these additional sensors are not available? The robot needs to manage situations, may be transitory situations, in which it has evidence of being located at two distinct points at the same time. The solution we adopted here is to use a Monte Carlo localisation process to manage the uncertainty about its position <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">21]</ref>. This technique is able to manage a multi-modal probability density, thus, allowing the robot to estimate its position correctly when the current image matches more than one reference image.</p><p>The environment in which we tested our system presents severe perceptual aliasing. We show in Fig. <ref type="figure">2</ref> some sample images from the reference image data set to show that the environment looks the same also at locations more than 5 m away from one another.</p><p>To have a general feeling of how severe the perceptual aliasing is in this environment, one might consider Fig. <ref type="figure" target="#fig_1">3</ref>, in which the similarity<ref type="foot" target="#foot_0">1</ref> of the images grabbed by the robot in different positions is plotted against all reference images (in Fig. <ref type="figure" target="#fig_1">3</ref>(a) the input image is number 10, in Fig. <ref type="figure" target="#fig_1">3</ref>(b) it is number 18, in Fig. <ref type="figure" target="#fig_1">3(c</ref>) it is number 54, in Fig. <ref type="figure" target="#fig_1">3</ref>(d) it is number 59). These plots show that the similarity of the current input image is high (higher peaks in the plots) not only for the correct reference image, but also for other reference images in locations far away from the actual position of the robot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Image matching</head><p>In the image-based localisation approach, the main problems are how to store in a memory-saving way Fig. <ref type="figure">2</ref>. Some panoramic images taken at different points in the environment. These images show that the environment appears very similar even at locations more than 5 m away one from the other. The arrows show the actual position in which every image was grabbed.  the reference images and how to efficiently compare all of them with the current input image, considering that for a wide environment the number of reference images can be very large (e.g. in our 50 m × 25 m environment, we have about 500 reference images).</p><p>In this paper we have fully developed a method we proposed in previous works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. The robot is equipped with an omnidirectional camera <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>. At the set-up stage, the robot takes a set of omnidirectional images at reference locations. At the running stage, while it moves in the environment, the robot compares the current omnidirectional image with the reference images. In order to store and match efficiently a large number of images, we transform each omnidirectional image into a compact representation. Fig. <ref type="figure" target="#fig_2">4</ref> sketches the steps of this transformation. From a colour omnidirectional image (Fig. <ref type="figure" target="#fig_2">4(a)</ref>) we create a gray-scale panoramic cylinder, that is a new image obtained by unwarping the original omnidirectional image, Fig. <ref type="figure" target="#fig_2">4(b)</ref>. The panoramic cylinder is expanded row by row into its Fourier series. The robot memorises each view by storing the Fourier coefficients of the low frequency components, Fig. <ref type="figure" target="#fig_2">4(c</ref>). We called the set of the stored coefficients the Fourier signature of the omnidirectional image. We showed in <ref type="bibr" target="#b14">[15]</ref> that the first 15 Fourier components are a compact representation for the omnidirectional images that enable to ef-fectively assess the similarity of the input images with respect to the reference images. The Fourier signature drastically reduces the amount of memory required to store a reference image, as reported in Table <ref type="table" target="#tab_0">1</ref>. For a typical omnidirectional image of 640 × 480 pixels, we build a 512 × 80 pixels panoramic image, which Fourier coefficients can be stored in a 15 × 80 × 2 array (i.e. 15 Fourier coefficients, 80 rows, two for the phase and the magnitude of the Fourier transform). So, compared to other techniques that stores the original omnidirectional colour image, the memory requirements decreases from 7.3 Mbit to 19 kbit, as shown in <ref type="bibr" target="#b16">[17]</ref>.</p><p>With this approach matching the current view against the visual memory is also computationally inexpensive. In fact, we defined a simple dissimilarity function that takes as input the Fourier signatures of two images and gives a value proportional to the amount of "dissimilarity" of the two images. In other words, the less similar the two images are, the higher the value of the dissimilarity function. In Eq. ( <ref type="formula" target="#formula_0">1</ref>) we defined the dissimilarity function as the L1 norm of the Fourier signatures of the two images O i and O j : here k indicates the frequency and y is the index of the row of the panoramic cylinder. For more details on this procedure, please refer to <ref type="bibr" target="#b14">[15]</ref>:</p><formula xml:id="formula_0">Dis(O i , O j ) = l-1 y=0 m-1 k=0 |F iy (k) -F jy (k)|. (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>To asses the computational burden of our system, we performed the off-line experiments on a 1400 MHz AMD Athlon with 512 MB of RAM. On this machine, once grabbed the omnidirectional image, to unwarp it, to calculate the Fourier signature, and to calculate the similarity with all the reference images takes less than 30 ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Related works</head><p>Let us highlight the advantages of our approach with respect to other ways of storing and comparing the images. First, using an omnidirectional camera reduces the number of images required to fully describe the environment. In fact, if a perspective camera is used, the view of the environment from a certain location changes with the direction of gaze. A solution can be to constrain the movements of the robot in order to have the perspective camera always pointing in the same direction <ref type="bibr" target="#b3">[4]</ref>, but this strongly limits the motion of the robot. An alternative can be to extract from the perspective images some features that reduce the amount of required memory while retaining a rich description of the image. A good example of this is reported in <ref type="bibr" target="#b21">[22]</ref>, where 936 images were stored in less than 4 MB. Nonetheless, collecting such a large number of images is tedious and time consuming.</p><p>Using omnidirectional cameras, we have only one omnidirectional image for every reference location. The omnidirectional image contains the appearance of the environment in all possible gazing directions. Several authors, exploited omnidirectional images for image-based localisation. The most common approach is to extract a set of eigenimages from the set of reference images and project the reference images and the current image into eigenspaces. Unfortunately, this approach does not lead to rotationally invariant images. Usually, the images are preprocessed in order to obtain the rotational invariance as in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8]</ref> or the heading of the sensor is constrained as in <ref type="bibr" target="#b12">[13]</ref>. On the contrary, our technique, which uses the Fourier transform of the panoramic cylinder (i.e. the Fourier signature), is a natural representation for implementing rotational invariance, as detailed in <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Monte Carlo localisation</head><p>As we stated in Section 1, the image-based localisation approach is misleading in situation in which the appearance of the environment is the same at two different locations. In this work, we overcame this problem by exploiting a well-known probabilistic approach in order to estimate the correct position of the robot. The general method, known as Bayesian filtering (also known as Markov localisation in robotics) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21]</ref>, recursively updates the probability density of the robot's position (the belief) using motion and perception information. In the Monte Carlo method one represents the posterior probability density of the robot's pose with a set of discrete points in the configuration space of the robot. To better understand the rest of the paper, we need to briefly review the theory behind the Monte Carlo localisation approach (MCL). In the Bayesian Filtering problem one has to calculate the probability density of the robot's position Bel(s t ) = p(s t |O t , a t ) over time. What is known is the prior probability density Bel(s 0 ), which describes the initial robot uncertainty about its position; the prediction model p(s t |s t -1 , a t -1 ) that applies motion data a t -1 to actual state s t -1 obtaining a new state s t ; the observation model p(O t |s t ) that represents the probability of making observation O t from state s t .</p><p>Using the Bayes formula and Markov assumption about the state space the equation to calculate the belief is</p><formula xml:id="formula_2">Bel(s t ) = ηp(O t |s t ) × p(s t |s t-1 , a t-1 ) Bel(s t-1 ) ds t-1 , (2)</formula><p>where η is a normalisation factor. In Monte Carlo localisation, the posterior probability density of the robot's position (i.e. the belief) is approximated with a set of weighted samples {s i t , ω i t } i=1,... ,N . The weight associated with every sample is proportional to the likelihood that the robot is occupying that position. The samples are updated recursively with a procedure called sampling-importance-resampling <ref type="bibr" target="#b2">[3]</ref>. During resampling the samples are drawn with probability proportional to their weights. As a result, unlikely-samples die out, while samples with high weights are replicated.</p><p>In our implementation, the motion and perception data used to update the samples come respectively from the odometry and from the omnidirectional camera. The weight w i associated with every sample s i is proportional to the similarity of the reference images closer to that sample with respect to the current input image, as shown in Eq. ( <ref type="formula" target="#formula_3">3</ref>)</p><formula xml:id="formula_3">w i = 1 |C| g j ∈C S j (D -dist(s i , g j )), (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where C is the set of the reference position g j at a distance less than D from the sample s i , and S j the similarity value associated with the reference image at position g j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Related works</head><p>The approach we used was inspired by the work of Wolf et al. <ref type="bibr" target="#b21">[22]</ref> and turned out to be very similar to the concurrent work of Gross et al. <ref type="bibr" target="#b8">[9]</ref>, which we were unaware of. One might think these approaches are the same of the one presented by Dellaert et al. <ref type="bibr" target="#b4">[5]</ref>. Actually, they are different in that Dellaert et al., instead of using single reference images, constructed a complex global visual map of the environment by mosaicing all the images of the ceiling.</p><p>The main differences between our system and the work of Wolf et al. are (i) they used a perspective camera while we used an omnidirectional camera (reducing the number of images to be stored); (ii) they associated a visibility region with every reference image in the memory database to reduce the search space for image matching. The use of a visibility region implies the system needs a metrical map of the environment and increases the complexity of the algorithm. We do not need to use the visibility region because our omnidirectional camera supplies a 360 • view of the environment; this also means that we rely only on the similarity computation to distinguish various localisation hypothesis with a gain in minor complexity of the algorithm.</p><p>The differences with Gross et al.'s work are mainly in the way they calculate the image similarity. Basically, for every omnidirectional image they extract a feature vector from the colour panoramic cylinder. This feature vector is calculated dividing the colour panorama into 10 sections and taking the average RGB values of each of these sections. They defined a image similarity function that takes as arguments these features vectors and that implements a rotational invariance. The colour information available in our test environment is very poor, the predominant colours are gray and white, so a method based only on colours to match the images would not work. Capturing the brightness patterns in the environment as provided by the Fourier signature proved to be very effective.</p><p>Concerning, the Monte Carlo localisation methods used in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9]</ref>, they are very similar to the one we used. The main difference is in the way the three works approaches the kidnapped robot problem. In the next section, we present our original contribution to the kidnapped robot problem <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The proposed kidnapping strategy</head><p>In the kidnapped robot problem, the robot is lifted and moved to a different location, so it does not have any odometric feedback on this motion. This problem aims to test the system in situations in which the odometric information of the robot is totally wrong or the robot has to recover from an incorrect localisation.</p><p>In the Monte Carlo localisation methods when the robot has a good localisation, the samples are generated only in a tight cloud close to the estimated position of the robot. This makes it difficult to solve the kidnapped robot problem. In fact, if the robot is moved to a new position without perceiving this motion (kidnapping), it will continue to generate the samples around the previous position, without generating any samples around the new unknown position. If the robot does not have any samples around the new position, it will never recover from the localisation error. The standard approach replaces a certain number of samples with others randomly drawn in the entire environment <ref type="bibr" target="#b6">[7]</ref>. This approach relies on the hope that some samples of this subset will be generated by chance close to the position where the robot has been moved. In this case, these samples will have large weights in the next measurements and will survive and cluster the other samples close to them. The drawback is that it takes several iterations for the samples to cluster around the correct position and sometimes it might takes some iterations even to generate a sample close to the position where the robot was kidnapped. This was the approach used also by Wolf et al. <ref type="bibr" target="#b21">[22]</ref>. Gross et al. used an alternative approach, instead of generating a certain amount of samples at random position, they have a certain number of samples generated at fixed important positions in the environment (about 3% of the total number of samples) <ref type="bibr" target="#b8">[9]</ref>.</p><p>We propose a new solution: instead of randomly drawing the sample, the new samples should be generated only around the reference images that best match the current input image. Each time the samples are generated a number of samples (10% of samples) are replaced with new samples drawn around the positions of the reference images most similar to the current input image. This assures that the newly generated samples are concentrated around possible locations only. This approach is made possible by the technique we use to match the input image with the reference images. Usually, all reference images matching the input image are close to each other, so the new samples concentrate in a region around the correct reference image. In case of kidnapping or perceptual aliasing, the reference images that are similar to the input image can be far away one from the other and so the new samples are generated at two different positions Fig. <ref type="figure">6</ref>. The kidnapped robot problem solved with the standard approach. Note the robot needs many steps in order to recover from the kidnapping. The small dots are the samples generated by the Monte Carlo filter, the line with the dots is the actual path of the robot (the dots are the position at which the robot takes the input images), and the thick curve is the estimated path of the robot. The estimated position of the robot is calculated as the average position of the samples and is marked with a cross. in the environment, like in Fig. <ref type="figure" target="#fig_3">5</ref>. In other words, the samples are taking into account alternative possibilities for the actual location of the robot. In the case of kidnapping, the new location is stable and the samples will quickly cluster in the new location. In the case of perceptual aliasing, it is just a transient situation lasting for one or two steps that will not spoil the correct estimation of the robot's pose. In both cases, our system is able to quickly calculate the correct position.</p><p>We compared our proposed technique of generating new samples with the standard technique used in <ref type="bibr" target="#b21">[22]</ref>. Fig. <ref type="figure">6</ref> shows the solution to the kidnapped robot problem with the standard approach of drawing 10% of new samples uniformly distributed in the environment at random positions. One can see that the system needs many steps to recover from the kidnapping and even after five steps the localisation error is still very high. In Fig. <ref type="figure">6</ref>, the system is performing the same test using our technique, in just four steps the robot has fully recover from the localisation error. As we will Fig. <ref type="figure">7</ref>. The kidnapped robot problem with our newly proposed kidnapped strategy. Note the robot needs only three to four steps to recover the correct localisation. see in Section 4, our system proved to give better performances in all the test we performed. The proposed kidnapped strategy has also been applied in the global localisation with a significative speed-up in the convergence of the estimated position to the real position (almost twice as fast) (Fig. <ref type="figure">7</ref>).</p><p>We did not compared our approach to the sensor resetting localisation (SRL) approach of Lenser and Veloso <ref type="bibr" target="#b13">[14]</ref>. Their approach is well suited in situations were the kidnapping of the robot is frequent (i.e. the robot is frequently lifted and moved away, like in the four-legged league RoboCup competition) and when there are landmarks with known positions. In this case, one can trust more the vision sensor than the localisation method. However, in typical indoor applications kidnapping is not so frequent and our system does not use fix landmarks, but relies only on the appearance of the environment. This appearance might temporary change due to occlusion from people walking by. In the case of temporary occlusion, we do not want all the sample to move away to the next most similar reference image, as it would happened with SRL. Every time there are more than one reference image similar to the current image, we want to take into account all new possible localisations, but with a bias towards the previous location. In other words, we want to trust less the vision sensor than the localisation algorithm.</p><p>A possible extension of our approach is to use the idea of "hierarchical localisation", introduced in <ref type="bibr" target="#b14">[15]</ref>. The hierarchical localisation is the capability of the robot to calculate its position with a tight or broach accuracy. In <ref type="bibr" target="#b14">[15]</ref>, we showed that the hierarchical localisation can save computational time when a precise localisation is not needed. Here we suggest the hierarchi-cal localisation can be used also to select if the Monte Carlo filter has to take into account a smaller or a larger number of alternative localisation hypotheses. In our approach, the hierarchical localisation is obtained by controlling the number of Fourier components used to calculate the similarity between the images, therefore controlling the number of reference images that match the current input image. This idea is not discussed further, because it is out of the scope of the this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The system was tested along the corridors marked in the map of Fig. <ref type="figure" target="#fig_0">1</ref> (right). This is a large office building composed of a long corridor (about 50 m) and four shorter corridors forming a loop (the loop is about 20 m in diameter). The corridors are about 3 m wide. The total path of the robot is about 100 m long. Some panoramic snapshots are shown in Fig. <ref type="figure">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(left).</head><p>As stated before, this environment is particularly challenging because of its high perceptual aliasing, that is all corridors look very similar. They have white walls, gray doors and little colour information is available. Along the path, the robot faces many false localisation hypothesis, but the correct localisation is never lost.</p><p>In this test, the reference images were taken every 20 cm on a line approximately in the center of the corridor. Every reference image was labelled with the ground-truth position of the robot. In a second run the robot acquired the input images at different positions along the corridors and the actual positions of the robot were recorded to calculate the localisation error at run time. The tests were performed on the system off-line. The inputs are the sequences of the current images presented one by one every time the robot takes a new step, and the recorded odometry shown in Fig. <ref type="figure" target="#fig_4">8</ref>. The aim of the experiments is to demonstrate the system is able to reliably localise the robot and to recover from localisation errors. We carried out several experiments on global localisation, position tracking and robot kidnapping, introducing different amount of noise to the odometers' data. The system is able to successfully localise the robot in any situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Global localisation and position tracking</head><p>The screenshots of Fig. <ref type="figure" target="#fig_5">9</ref> show an experiment of global localisation and position tracking. The small (red) dots are the samples generated by the Monte Carlo filter, the bigger (blue) dots are the ground-truth positions of the input images taken by the robot, the thinner (blue) line is the ground-truth path of the robot, and the thicker black curve is the estimated path of the robot. The estimated position of the robot is calculated as the average position of the samples and is marked with a black cross. Our system is able to localise the robot, without any prior information on the robot's position, after processing about 5-6 images. The correct localisation is achieved even if we use a very low number of samples. Experimentally, we observed that the minimal number of samples to obtain a reliable localisation is about the same as the number of reference images. In few iterations, the localisation error sets be-low the distance of the reference images (i.e. 20 cm). In Fig. <ref type="figure" target="#fig_5">9 (</ref>Step 00) the robot does not have any information on its position, so it generates samples uniformly distributed in the environment. Note the robot generates samples also in forbidden positions (that is outside of free space and inside wall or obstacles). This is because the robot does not have any information on the geometrical map of the environment. The only information the robot has is the relative position of the images one with respect to the others. This proves our image-based Monte Carlo approach works without the need of a geometrical map of the environment. The rough map of the environment presented in this paper is needed only for displaying purposes. In Fig. <ref type="figure" target="#fig_5">9 (</ref>Step 01), the robot grabs the first omnidirectional image and after the resampling stage almost all samples are already concentrated around the correct position. All samples inside the obstacles disappeared. Only few samples are left along the corridors in locations that have a certain amount of similarity with the current input image. In Step 02, after grabbing the second image, samples are even more concentrated and survive only in the first corridor. In Fig. <ref type="figure" target="#fig_5">9</ref> (Steps 09, 19, and 26), the position tracking experiment is presented. It is not shown in the screenshots, but the system is able to keep track of the robot's position when the robot takes long steps (about 200 cm) in dead-reckoning mode (i.e. it travels for a while without tacking any picture) with large odometric errors. In this conditions, as soon as a new image is grabbed, the system is able to correct the errors caused by odometry. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Kidnapped robot</head><p>In Fig. <ref type="figure" target="#fig_6">10</ref>, we compare again the standard strategy of drawing samples randomly distributed in the environment (left column) with our new kidnapped robot strategy (right column). The test is performed on a much larger environment than the one in Figs. <ref type="figure">6</ref> and<ref type="figure">7</ref>. The result is that the proposed kidnapped strategy both allows a quick recovery from the localisation error due to the kidnapping and also improves the global localisation (less observations are necessary to cluster all the samples around the actual robot's position). In Fig. <ref type="figure" target="#fig_7">11</ref> a statistical analysis of the performance of the two methods is plotted. We repeated the experiment of Fig. <ref type="figure" target="#fig_6">10</ref> twenty times and we calculated the average error in the case of the standard strategy (red dashed line) and our new strategy (blue solid line). The lo-calisation error decreases toward lower values much faster with our strategy both in the global localisation phase and in the recovering phase after kidnapping. Also in the position tracking phase the localisation error obtained using our randomly is smaller, because with the standard strategy some of the samples are randomly spread out in the environment and so they bias the estimation of the actual position of the robot, generating a sensible error (see Fig. <ref type="figure" target="#fig_7">11</ref> (right), where the plot is zoomed on the residual error).</p><p>The Monte Carlo localisation software is the most computationally intensive routine in our system. It requires about 170 ms per cycle using 1000 samples. Even if the software was not optimised for computational speed, it could offer real-time performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we presented a robust image-based localisation system that can operate in every type of environment. We presented our solution to the problem of lowering the computational and memory requirements posed by image-based localisation. This solution uses the Fourier transform of omnidirectional images grabbed by the robot. We discussed the advantages of this solution with respect to the solutions devised by other authors. To overcome the limitation of the image-based localisation approach, namely the lack of robustness in the case of perceptual aliasing, we used a Monte Carlo localisation technique. We showed that this system is able to track the position of the robot while moving and it is able to estimate the position of the robot without any prior knowledge on the real position. Moreover, we showed that our image-based Monte Carlo approach does not require a geometrical map of the environment, but just the position of reference images with respect to each other (that is a kind of topological map). A new approach in solving the kidnapped robot problem was proposed: we generated a certain number of samples (here 10%) close to all reference images that match the current input image. This approach was shown to outperform the standard approach of generating samples randomly distributed in the environment.</p><p>on July 2003. He is a President of the Intelligent Autonomous Society, and a Vice President of the RoboCup International Federation. He is also a Vice President of the Italian Association of Robotics and Automation. His current research interests are on applying Artificial Intelligence to Robotics with particular regards to the Multi-robot Systems field.</p><p>Hiroshi Ishiguro received his D.Eng. degree from Osaka University, Japan, in 1991. In 1991, he started working as a Research Assistant in Department of Electrical Engineering and Computer Science, Yamanashi University, Japan. Then, he moved to Department of Systems Engineering, Osaka University, Japan, as a re-search assistant in 1992. In 1994, he was an Associate Professor of Department of Information Science, Kyoto University, Japan, and started research of distributed vision using omnidirectional cameras. From 1998 to 1999, he worked in Department of Electrical and Computer Engineering, University of California, San Diego, USA, as a visiting scholar. From 1999, he is a visiting researcher in ATR Media Information Science Laboratories and he has developed interactive humanoid robots, Robovie. In 2000, he moved to Department of Computer and Communication Sciences, Wakayama University, Japan, as an Associate Professor and then he became a professor in 2001. Now he is a Professor of Department of Adaptive Machine Systems, Osaka University, Japan.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) A picture of the robot used in the experiments. (b) The plan of the building where the omnidirectional image data set was built. This is a large environment in which the length of the longer corridor is 50 m. The thick line represents the robot's path. Details of the map are unimportant; it is presented just to convey the complexity of the environment.</figDesc><graphic coords="2,109.40,90.13,330.12,215.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Some examples of perceptual aliasing in our test environment. The height of the picks is proportional to the amount of similarity of the different images. (a) Input Image 10 matches not only the correct reference image at 1380 cm on the x-axis, but also reference images at 320, 620, and 1060 cm. The same happens in plots (b)-(d).</figDesc><graphic coords="3,49.16,327.23,440.64,292.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The process of generation of the Fourier signature. The omnidirectional image (top left) is unwarped in a panoramic cylinder (bottom) and the Fourier coefficient of every row of the panoramic cylinder are calculated (top right).</figDesc><graphic coords="4,102.61,90.13,343.80,195.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. An example of some samples being generated close to two different reference images matching the current input image. On the left the cluster corresponding to the correct reference image and on the right some samples corresponding to a reference image very similar to the current input image.</figDesc><graphic coords="7,281.03,90.14,216.00,64.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The odometric data used in the experiments. Note the odometric information used by the robot is really inaccurate.</figDesc><graphic coords="9,127.57,90.13,284.04,150.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. An example of Monte Carlo localisation. These are some snapshots of our system while it is performing a global localisation (Steps 00, 01, and 02) and then position tracking (Steps 09, 19, and 26). For colour see online version.</figDesc><graphic coords="10,72.44,90.14,403.92,310.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Fig.10. Here you can see some snapshots in our test environment. The standard kidnapped robot strategy (left series of images) is compared with our new strategy (right series). The dotted (blue) line is the actual path of the robot and the solid (black) curve is the estimated path of the robot. The snapshots show that with our strategy it is possible to speed-up global localisation and also to quickly relocalise after the kidnapping. For colour see online version.</figDesc><graphic coords="11,68.17,90.13,402.84,516.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Average localisation error during the experiment of Fig. 10. The strategy we proposed outperforms the uniform strategy in robustness and speed in recovering the correct position. On the right, a zoom of the plot on the left to show the amount of residual error for the two approaches.</figDesc><graphic coords="12,73.09,90.13,402.84,159.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="3">The different memory requirements illustrating the memory savings</cell></row><row><cell cols="2">introduced by the Fourier signature</cell><cell></cell></row><row><cell>Image</cell><cell>Memory required (bit)</cell><cell>Memory</cell></row><row><cell></cell><cell></cell><cell>required</cell></row><row><cell>Omnidirectional</cell><cell>640 × 480 × 24</cell><cell>7.3 Mbits</cell></row><row><cell>Panoramic cylinder</cell><cell>512 × 80 × 8</cell><cell>328 kbits</cell></row><row><cell>Fourier signature</cell><cell>80 × 15 × 2 × 8</cell><cell>19 kbits</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The similarity between the current image and a reference image is calculated with Eq. (1).</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Memory-based self-localisation using omnidirectional images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Aihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Iwasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yokoya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Takemura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Pattern Recognition</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lovell</surname></persName>
		</editor>
		<meeting>the 14th International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1799" to="1803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A tutorial on particle filters for on-line non-linear/non-Gaussian Bayesian tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arulampalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maskell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Clapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="174" to="188" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An improved particle filter for non-linear problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fearnhead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Proceedings of the Radar, Sonar and Navigation</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">146</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised matching of visual landmarks for robotic homing using Fourier-Mellin transform</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cassinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Inelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using the condensation algorithm for robust, vision-based mobile robot localization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;99)</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;99)</meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Monte Carlo localization for mobile robots</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA&apos;99)</title>
		<meeting>the IEEE International Conference on Robotics and Automation (ICRA&apos;99)</meeting>
		<imprint>
			<date type="published" when="1999-05">May 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Monte Carlo localization: efficient position estimation for mobile robots</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial intelligence (AAAI&apos;99)</title>
		<meeting>the National Conference on Artificial intelligence (AAAI&apos;99)</meeting>
		<imprint>
			<date type="published" when="1999-07">July 1999</date>
			<biblScope unit="page" from="343" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Vision-based navigation and environmental representations with an omnidirectional camera</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gaspar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Winters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Santos-Victor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Omnivision-based probabilistic self-localization for a mobile shopping assistant continued</title>
		<author>
			<persName><forename type="first">H.-M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schroeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Boehme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2003)</title>
		<meeting><address><addrLine>Las Vegas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-10">October 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Development of low-coast compact omnidirectional vision sensors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ishiguro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Panoramic Vision</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="23" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image-based memory of environment</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS-96)</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS-96)</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="634" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust localization using panoramic view-based recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Pattern Recognition (ICPR&apos;00)</title>
		<meeting>the 15th International Conference on Pattern Recognition (ICPR&apos;00)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2000-09">September 2000</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="136" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A probabilistic model for appearance-based robot localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kröse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vlassis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bunschoten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Motomura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="391" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sensor resetting localization for poorly modelled mobile robots</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lenser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>the IEEE International Conference on Robotics and Automation (ICRA)<address><addrLine>San Francisco, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1225" to="1232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image-based memory for robot navigation using properties of the omnidirectional images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Menegatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ishiguro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="251" to="267" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Designing an omnidirectional vision system for a goalkeeper robot</title>
		<author>
			<persName><forename type="first">E</forename><surname>Menegatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pagello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pellizzari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Spagnoli</surname></persName>
		</author>
		<editor>A. Birk, S. Coradeschi, S. Tadokoro</editor>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="78" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical image-based localisation for mobile robots with Monte Carlo locations</title>
		<author>
			<persName><forename type="first">E</forename><surname>Menegatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zoccarato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pagello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ishiguro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Mobile Robots (ECMR&apos;03)</title>
		<meeting>European Conference on Mobile Robots (ECMR&apos;03)</meeting>
		<imprint>
			<date type="published" when="2003-09">September 2003</date>
			<biblScope unit="page" from="13" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image-based localisation Monte Carlo localisation without a map</title>
		<author>
			<persName><forename type="first">E</forename><surname>Menegatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zoccarato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pagello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ishiguro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Congress of the Italian Association for Artificial Intelligence</title>
		<meeting>the 8th Congress of the Italian Association for Artificial Intelligence</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003-09">September 2003</date>
			<biblScope unit="page" from="423" to="435" />
		</imprint>
	</monogr>
	<note>AI * IA 2003: Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Zero phase representation of panoramic images for image based localization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hlaváč</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Computer Analyses of Images and Patterns</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Solina</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</editor>
		<meeting>the 8th International Conference on Computer Analyses of Images and Patterns<address><addrLine>Ljubljana, Slovenia</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1999-09">September 1999</date>
			<biblScope unit="volume">1689</biblScope>
			<biblScope unit="page" from="550" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic algorithms and the interactive museum tour-guide robot minerva</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennewitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haehnel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="972" to="999" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust Monte Carlo localization for mobile robots</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Journal</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust vision-based localization for mobile robots using an image retrieval system based on invariant features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Burkhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Map-based navigation for a mobile robot with omnidirectional image sensor copis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nishizawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yachida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="634" to="648" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
