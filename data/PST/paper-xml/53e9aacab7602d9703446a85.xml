<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">New Parameter-Free Simplified Swarm Optimization for Artificial Neural Network Training and Its Application in the Prediction of Time Series</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Wei-Chang</forename><surname>Yeh</surname></persName>
						</author>
						<title level="a" type="main">New Parameter-Free Simplified Swarm Optimization for Artificial Neural Network Training and Its Application in the Prediction of Time Series</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">98572226E7327DA37C21E4FFAFD0CFBE</idno>
					<idno type="DOI">10.1109/TNNLS.2012.2232678</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Artificial intelligence</term>
					<term>evolutionary computation</term>
					<term>machine learning</term>
					<term>neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A new soft computing method called the parameterfree simplified swarm optimization (SSO)-based artificial neural network (ANN), or improved SSO for short, is proposed to adjust the weights in ANNs. The method is a modification of the SSO, and seeks to overcome some of the drawbacks of SSO. In the experiments, the iSSO is compared with five other famous soft computing methods, including the backpropagation algorithm, the genetic algorithm, the particle swarm optimization (PSO) algorithm, cooperative random learning PSO, and the SSO, and its performance is tested on five famous time-series benchmark data to adjust the weights of two ANN models (multilayer perceptron and single multiplicative neuron model). The experimental results demonstrate that iSSO is robust and more efficient than the other five algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Artificial neural network (ANN) is a soft computing method which has been utilized to obtain optimal or good-quality solutions to difficult optimization problems in a number of fields. It is a mathematical or computational model motivated by analogy with the brain as having a natural propensity for storing experiential knowledge and making that knowledge available for use. An ANN is capable of learning to detect and extract nonlinear relationships and interactions from predictor variables. During the learning technique, the ANN is trained on the basis of information flow-through without the need for the distribution of variables to be known <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b8">[9]</ref>.</p><p>The learning technique is the most vital element in justifying all connection weights and bias values in ANNs. Currently, many learning techniques are being used to train ANNs to minimize cost, such as the backpropagation (BP) algorithm <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b8">[9]</ref>, genetic algorithm (GA) <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>, particle swarm optimization (PSO) <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>, and simplified swarm optimization (SSO) <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b20">[21]</ref>.</p><p>The SSO was first proposed by Yeh in <ref type="bibr" target="#b16">[17]</ref>. It is an emerging population-based stochastic optimization method that belongs Manuscript received February 29, 2012; revised November 19, 2012; accepted November 23, 2012. Date of publication January 14, 2013; date of current version <ref type="bibr">February 13, 2013</ref>. This work was supported in part by the National Science Council of Taiwan, under Grant NSC 98-2221-E-007-051-MY3.</p><p>The author is with the Advanced Analytics Institute, University of Technology Sydney, Sydney NSW 2007, Australia, and also with the Department of Industrial Engineering and Engineering Management, National Tsing Hua University, Hsinchu 300, Taiwan (e-mail: yeh@ieee.org).</p><p>Digital Object Identifier 10.1109/TNNLS.2012.2232678</p><p>to the categories of both swarm intelligence and evolutionary computation <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b19">[20]</ref>. The SSO has been proven to be more straightforward, effective, efficient, and flexible in solving discrete optimization problems than PSO <ref type="bibr" target="#b18">[19]</ref> and can achieve better solutions to the disassembly problem than GA <ref type="bibr" target="#b19">[20]</ref>.</p><p>In this brief, an improved SSO called improved SSO is proposed as a supervised learning technique for training the ANN by improving the update mechanism (UM) in the SSO. A review of the literature suggests that ANN is a promising technique for forecasting problems <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b8">[9]</ref>. The proposed iSSO is therefore implemented on the two ANN models most frequently used on time-series data: the multilayer perceptron (MLP) model <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b8">[9]</ref> and the single multiplicative neuron (SMN) model <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>. To illustrate its performance, the proposed iSSO is compared with five other popular methods: BP, which is a gradient-based method, GA <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>, PSO <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>, cooperative random learning PSO (CRPSO), which is a special PSO <ref type="bibr" target="#b14">[15]</ref>, and the traditional SSO <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b20">[21]</ref>.</p><p>The rest of this brief is organized as follows. Section II briefly introduces the ANN models used including the MLP and the SMN, together with the original SSO. The proposed iSSO is presented in Section III. In Section IV, the proposed iSSO is tested on five time-series experiments and compared with another five popular methods to demonstrate its performance. Conclusion and future works are included in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MLP, SMN, AND SSO</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. MLP</head><p>Let ξ ki be the input of the i th input (node/neuron) of the kth sample, h j be the output of the j th hidden node, w i j be the connection weight from the i th input node to the j th hidden node, b 0 andb 1 be biases in the input layer and the output layer, ω j k be the connection weight from the j th hidden node to the kth sample,y k be the output of the kth sample, and y target,k be the real value of the kth sample, where i = 1,2,…,n, j = 1,2,…,m, k = 1,2,…,N, n is the number of inputs, m is the number of hidden nodes, and k is the sample size.</p><p>The MLP attempts to minimize the global error meansquared error (MSE) via the training algorithm to find the set of weight values that will cause the output from the neural network to match the actual target values in this brief as closely as possible</p><formula xml:id="formula_0">MSE = 1 N N k=1 (y k -y target, k ) 2<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">y k = m j =1 ω j k 1 + exp - n i=1 w i j ξ ki + b 0 + b 1 . (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>In MLP, all connection weights (i.e., w i j and ω j k for i = 1, 2, …, n, j = 1, 2, …, m, and k = 1, 2, …, N) and bias values (i.e., b 0 and b 1 ) are initially assigned random values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. SMN</head><p>Let ξ ki , w i , and b i be the input, weight, and bias of the i th input of the kth sample for i = 1, 2, …,n and k = 1, 2, …,N, respectively. The goal of the SMN training process is to minimize <ref type="bibr" target="#b0">(1)</ref>, as in the MLP, except that (2) is replaced by</p><formula xml:id="formula_3">yk = 1 1 + exp - n i=1 (w i ξ ki + b i ) . (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>As in the MLP, all weights (i.e., w i for i = 1,2,…,n) and bias values (i.e., b i for i = 1,2,…,n) are initially assigned random values in the SMN.</p><p>The differences between the SMN and the MLP are the bias terms, the operator in the hidden layer, and the number of hidden nodes. In the MLP, there is only one bias term in the input layer. However, there is a bias term for each input node in the SMN. Second, in the hidden layer, the MLP is based on the addition operator, whereas the SMN uses the multiplication operator. Third, there are numerous hidden nodes in the MLP, whereas there is only one hidden node in the SMN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SSO</head><p>X i,t = (x t i1 , x t i2 , . . . , x t i,DIM ) be the i th solution in the tth generation, GEN be the generation number, and package on package (POP) be the number of populations, where DIM is the number of variables and x t i j is the value of the j th variable of X i,t . The pBest P i = ( p i 1, p i 2,…, p i , DIM) is the i th solution with the best solution fitness it has achieved so far, i.e., the best in {X i , k |k = 1,2,…,t}, and the gBest G = (g 1 ,g 2 ,…,g DIM ) is the best among P i , where i = 1,2,…,POP. The fundamental concept of SSO is that each selected variable value may be generated from the current solution, pBest, gBest, or a random number according to the specifics in SSO. Based upon the above concept, a simple UM of SSO after being given c w , c p , and c g is defined as</p><formula xml:id="formula_5">x t +1 i j = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ x t i j , if ρ ∈ [0, C w = c w ) p i j , if ρ ∈ [C w , C p = C w + c p ) g j , if ρ ∈ [C p , C g = C p + c g ) x, if ρ ∈ [C g , 1)<label>(4)</label></formula><p>where ρ is a uniform random number generated in the range [01) and xis a random number between the lower and upper bounds of the i th variable. Note that c w , c p , c g , and 1-(c w +c p +c g ) represent the probabilities of the new variable value generated from the current solution, pBest, gBest, and a random number in SSO, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED iSSO</head><p>The proposed iSSO treats the parameters of SSO (e.g., c w and c g ) and weights (including bias terms) of ANN (i.e., w i j , ω j k , b 0 , and b 1 in MLP; w i and b i in SMN for i = 1, 2, …, n, j = 1, 2, …, m, and k = 1, 2, …, N) as variables. Hence, DIM = 2 + mn + mN + 2 in MLP and DIM = 2 + 2n in SMN. For ease of explanation, weights in ANN are called ANN variables and parameters in SSO are called SSO variables. Three different UMs are proposed in iSSO as follows.</p><p>1) The normal UM (nUM): The nUM is used to update one randomly selected ANN variable of each solution.</p><p>2) The parameter UM (pUM): The pUM is used to update one randomly selected SSO variable for each solution.</p><p>3) The reborn UM (rUM): The rUM is used to update a solution when it is unimproved for a special period. The details are discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. nUM</head><p>In the proposed nUM, there are five main improvements on the traditional SSO <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b19">[20]</ref> to update X i , t , as follows.</p><p>1) The role of pBest is eliminated.</p><p>2) One variable only is selected randomly to update rather than all variables. 3) X i, t has its own SSO variable, i.e, c ig, t and c iw, t . 4) C ig, t = c ig, t and C iw, t = C ig, t +c iw, t are used instead of C w = c w ,C p = C w + c p , and C g = C p + c g in the traditional SSO. 5) A new control parameter called age (a i ) is defined as the generation number that the fitness value of the i th solution is unimproved. Equation ( <ref type="formula">8</ref>) is revised by considering the above five situations as</p><formula xml:id="formula_6">x t +1 i j = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ g j + σ/a i , if ρ ∈ [0, C ig,t = c ig,t ) x t i j + σ (g j -x t i j ), if ρ ∈ [C ig,t , C iw,t = C ig,t + c iw,t ) x, if ρ ∈ [C iw,t , 1)<label>(5)</label></formula><p>where x t i j , g j , x, and ρ are defined as in <ref type="bibr" target="#b7">(8)</ref>, and σ is a uniform random number generated in the range [-1, 1].</p><p>In <ref type="bibr" target="#b4">(5)</ref>, the larger the value of a i , the smaller the search range around gBest if ρ ∈ [0,C ig ,t). The solution X i ,t is updated by searching around the present position toward gBest if σ &gt; 0 or away from gBest if σ &lt; 0 for ρ ∈ [C ig ,t,C iw ,t). The solution X i ,t is updated by randomly generating a new value to the i th variable for keeping the diversity of solutions if ρ ∈ [C iw ,t,1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. pUM</head><p>The SSO parameters of the i th solution in the tth generation, i.e., c ig , t and c iw ,t, are treated as two variables in this brief. Like the ANN variables, only one is selected randomly to be updated by the proposed pUM as follows:</p><formula xml:id="formula_7">c ik,t +1 = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ g SSO,k + σ/a i , if ρ ∈ [0, C ig,t = c ig,t ) + 0.01σ c ik,t , if ρ ∈ [C ig,t , C iw,t (g SSO,k -c ik,t ), = C ig,t + c iw,t ) c, if ρ ∈ [C iw,t , 1)<label>(6</label></formula><p>) where g SSO,k is the related SSO variables of gbest,k ∈ {g,w}, σ is a random number in [-0.10.1], and c and ρ are uniform random numbers generated in <ref type="bibr">[01]</ref>. Note that c ig , t and c iw , t need to be swapped if c ig , t &lt;c iw ,t after using the pUM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. rUM</head><p>The rUM is used to update a solution, say X i ,t, only if its age is too great, i.e., a i ≥ AGE, where AGE is a predefined positive integer number. In the rUM, a whole new solution is generated by using nUM to each ANN variable based on (5) after resetting a i = 1. One of the SSO variables is selected randomly to be updated using pUM based on <ref type="bibr" target="#b5">(6)</ref>. Note that the nUM mentioned in Section III-A is implemented on a randomly selected variable only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Proposed iSSO</head><p>The overall procedure of the proposed iSSO is summarized as follows.</p><p>Step 1: Generate X i , 1, let t = 1, calculate F(X i , 1) based on the related ANN model, and find a gBest</p><formula xml:id="formula_8">G ∈ {X i , 1 | i = 1, 2, …, POP} such that F(G)</formula><p>is at least as good as F(X i , 1), where i = 1, 2, …, POP.</p><p>Step 2: Let i = 1.</p><p>Step 3: If a i &lt; AGE, then update X it to X it +1 using the nUM and the pUM based on ( <ref type="formula" target="#formula_6">5</ref>) and ( <ref type="formula" target="#formula_7">6</ref>), calculate F(X it ), and go to Step 7.</p><p>Step 4: Update X it to X it +1 using the rUM mentioned in Section III-C and calculate F(X it ).</p><p>Step 5: If F(X it ) is improved, then let a i = 1. Otherwise, go to Step 7.</p><p>Step 6: If F(X it ) is better than F(G), then let G = X it .</p><p>Step 7: If i &lt; POP, let i = i + 1 and go to Step 3.</p><p>Step 8: If t = GEN and/or CPU time are met, then halt; otherwise let t = t + 1 and go back to Step 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. FORECAST TESTS ON TIME-SERIES BENCHMARK</head><p>In this section, five benchmark time series, namely, Mackey-Glass chaotic time series (MG), Box-Jenkins Gas Furnace (BJ), Electroencephalogram data (EEG), dataset A: laser generated data (LGD), and data set D: computer generated series (CGS), are used to test the performance of the proposed iSSO. These time series are widely used as benchmark datasets in testing the performance of ANNs <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b24">[27]</ref>. All values of these five time series are normalized between 0.1 and 0.9.</p><p>To evaluate the average MSE (i.e., solution quality), the standard deviation of MSE (i.e., robustness), and the efficiency (i.e., the running time) of the proposed iSSO, five different algorithms were also coded in C++ programming language and carried out on an Intel CoreTM2 2.1-GHz PC with 2-GB memory: BP [1]- <ref type="bibr" target="#b8">[9]</ref>, GA <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>, PSO <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>, CRPSO <ref type="bibr" target="#b14">[15]</ref>, and SSO <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b20">[21]</ref>. All experiments were implemented for 50 runs. The number of generations was 1000 and the number of particles/solutions/chromosomes was 30 in each algorithm. The learning rate of BP was 0.7 and the generation number was 30 × 1000 = 30 000; the crossover rate and mutation rate were 1.0, and the roulette wheel selection was used to select the population of the next generation in GA; c 1 = c 2 = 2 and the upper and lower bounds of each position were -15 and 15, the inertia weight was linearly decreased from 0.9 to 0.5 in PSO and CRPSO; C w = 0.15, C p = 0.5, C g = 0.75 in SSO. There were two experiments in these time series, namely, Ex1 and Ex2 are as follows.</p><p>Ex1: Test all six algorithms on the five datasets based on the SMN.</p><p>Ex2: Test all six algorithms on the five datasets based on the MLP by trying different hidden nodes (from one to six).</p><p>The results obtained from Ex1 and Ex2 were used to determine the best ANN structure. All experimental results were categorized into training MSE, testing MSE, and CPU time (in seconds, represented by T ), and there were statistics in each category that included the average mean and standard deviations of the MSE of the training datasets and test datasets represented by MSEtr/STDtr and MSEte/STDte, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Five Benchmark Time Series</head><p>This subsection gives a brief introduction to the five benchmark time series prior to running the experiments. The MG is generated from the Mackey-Glass time-delay ordinary differential equation <ref type="bibr" target="#b21">[22]</ref>. We predict the value of the time series at the point y(t + 1) from the earlier points y(t), y(t -6), y(t -12), and y(t -18) as in <ref type="bibr" target="#b14">[15]</ref>. The training data consists of 450 samples, and 500 samples are used to test the performance of the algorithm.</p><p>The BJ was recorded from the combustion process of a methane-air mixture <ref type="bibr">[23]</ref>. The original data included 296 samples of the CO 2 concentration output y(t), and gas flow rate input u(t). The best input variables, selected after much trial and error in <ref type="bibr" target="#b14">[15]</ref>, are y(t) and u(t -3) for estimating y(t + 1). The training data consists of 140 samples, and 150 samples are used to test performance.</p><p>In each of the following three datasets, we predict the value at point y(t + 1) with four input variables y(t), y(t -1), y(t -3), and y(t -7).</p><p>The EEG was recorded by Zak Keirn for his thesis work on the M.S. degree in the Electrical Engineering Department at Purdue University <ref type="bibr" target="#b22">[24]</ref>. This problem is intentionally selected because it is observed that it cannot be predicted by linear models. The training data consists of 150 samples, and 150 samples are used to test performance.</p><p>The LGD is one of six datasets prepared for the Santa Fe Time Series Competition <ref type="bibr">[25]</ref>. A detailed description of the measurements can be found in <ref type="bibr" target="#b23">[26]</ref>. The training data consists of 400 samples, and 400 samples are used to test performance.</p><p>The CGS is another of the six time-series datasets presented at the Santa Fe competition <ref type="bibr">[25]</ref>. The principal distinctive feature of these data is that they are a very long time series consisting of 100 000 points. Since it was artificially generated, the series has the further feature that it contains no errors such as would inevitably be present in measured data. A detailed description of the measurements can be found in <ref type="bibr" target="#b24">[27]</ref>. The training data consists of 450 samples, and 500 samples are used to test performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ex1</head><p>Table <ref type="table">I</ref> shows the computational results of the six algorithms based on the SMN. In general, iSSO outperforms the other five algorithms in the mean of MSE, robustness, and CPU time for forecasting the MG, BJ, EGG, and CGS in the SMN. The only exception is BP, which performs better in forecasting the LGD; however, it is less efficient than the other algorithms. GA performs worst in predicting the MG, BJ, EGG, and LGD overall from the perspectives of mean of MSE, robustness, and run time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ex2</head><p>Table <ref type="table">II</ref> shows the MLP of each algorithm with the best number of hidden nodes (denoted by N) obtained by trial and error. Since N varies with the algorithm and generation numbers, we record the corresponding fitness function values and runtimes in the 250th, 500th, 750th, and 1000th generations in MLP. In general, iSSO outperforms the other five algorithms in the mean of MSE, robustness, and runtime. From the computational results in Table <ref type="table">II</ref>, it is clear that all values of the training MSE and testing MSE increase relative to the numerical increment of hidden nodes. In addition, the computation time also increases due to the complexity of the structure. Note that the best hidden node number is always 1 in iSSO, with the exception of the LGD dataset, which is the main reason why iSSO is more efficient than other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparisons Between SMN and MLP</head><p>The results shown in Tables I and II roughly suggest that the MLP is more effective than the SMN, but also that the SMN is more efficient than the MLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>Parameter settings always play an important role in the exchange of information among solutions for effective and efficient exploration in soft computing. A new soft computing method called iSSO was proposed by improving the efficiency and effectiveness of the SSO without giving the initial values to all parameters to obtain all weights in the ANN. From computational experiments in predicting the benchmark time series MG, BJ, EGG, LGD, and CGS, the proposed iSSO was shown to outperform BP, PSO, CRPSO, GA, and SSO in both the MLP and the SMN, which are two common models in ANN. The basic MLP with one or two hidden nodes was sufficient for time-series forecasting problems according to the experimental results. In addition, the effectiveness of the MLP was always superior to that of the SMN (in the minimal MSE). On the other hand, the computation time of the SMN is better than that of the MLP.</p><p>This brief is a pilot work in developing parameter-free soft computing. Future research may compare iSSO with other evolutionary metaheuristics, such as bare-bones PSO, differential evolution, and Tribes, and may involve other performance measures such as the impact on the performance of nUM, pUM, and rUM. Also, it is useful and significant to carry out a theoretical analysis to show the benefits of the proposed iSSO and all other soft computing methods.</p></div>		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The author would like to thank Y.-C. Ke for all the programming and simulations and Y.-M. Yeh for helpful remarks. The author would also like to thank the anonymous referees for their useful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Minimum complexity echo state network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rodan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="144" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recursive Bayesian recurrent neural networks for time-series modeling</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Mirikitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nikolaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="262" to="274" />
			<date type="published" when="2010-02">Feb. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fundamentals of Neural Networks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Fausett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Prentice Hall</publisher>
			<pubPlace>Upper Saddle River, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Toward automatic time-series forecasting using neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1028" to="1039" />
			<date type="published" when="2012-07">Jul. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reinforced two-step-ahead weight adjustment technique for online training of recurrent neural networks</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-J</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1269" to="1278" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Trend time-series modeling and forecasting with neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="808" to="816" />
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust synchronization of an array of coupled stochastic discrete-time delayed neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1910" to="1921" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Time series modeling and forecasting using memetic algorithms for regime-switching models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bergmeir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Triguero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Aznarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Benitez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1841" to="1847" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A solution for the N-bit parity problem using a single multiplicative neuron</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Iyoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hirota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Process. Lett</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="233" to="238" />
			<date type="published" when="2003-12">Dec. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<title level="m">Genetic Algorithms in Search, Optimization and Machine Learning</title>
		<meeting><address><addrLine>Reading, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison Wesley</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Combining volatility and smoothing forecasts of UK demand for international tourism</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Coshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tourism Manag</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="495" to="511" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-time bias-adjusted O3 and PM2.5 air quality index forecasts and their performance evaluations over the continental United States</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Atmosp. Environ</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="2203" to="2212" />
			<date type="published" when="2010-04">Apr. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Neural Netw</title>
		<meeting>IEEE Int. Conf. Neural Netw</meeting>
		<imprint>
			<date type="published" when="1995-12">Dec. 1995</date>
			<biblScope unit="page" from="1942" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Particle swarm optimization-based support vector machine for forecasting dissolved gases content in power transformer oil</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-B</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy Convers. Manage</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1604" to="1609" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">PSO-based single multiplicative neuron model for time series prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2805" to="2816" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combined modeling for electric load forecasting with adaptive particle swarm optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1671" to="1678" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A two-stage discrete particle swarm optimization for the problem of multiple multi-level redundancy allocation in series systems</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="9192" to="9200" />
			<date type="published" when="2009-07">Jul. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simplified swarm optimization in disassembly sequencing problems with learning effects</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Operat. Res</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2168" to="2177" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A new hybrid approach for mining breast cancer pattern using discrete particle swarm optimization and statistical method</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="8204" to="8211" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimization of the end-of-life disassembly sequencing problem based on the use of a novel method based on self-adaptive simplified swarm optimization and precedence preservative operator</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. Part A</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="250" to="261" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Novel swarm optimization for mining classification rules on thyroid gland data</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="page" from="65" to="76" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Oscillation and chaos in physiological control systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mackey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">197</biblScope>
			<biblScope unit="issue">4300</biblScope>
			<biblScope unit="page" from="287" to="289" />
			<date type="published" when="1977-07">Jul. 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title/>
		<ptr target="http://www.cs.colostate.edu" />
	</analytic>
	<monogr>
		<title level="j">Computer Science News</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dimensions and entropies of chaotic intensity pulsations in a single-mode far-infrared NH 3 laser</title>
		<author>
			<persName><forename type="first">U</forename><surname>Hübner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. A</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="6354" to="6365" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Future of Time Series</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Gershenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Weigend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Time Series Prediction: Forecasting the Future and Understanding the Past</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Weigend</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Gershenfeld</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
