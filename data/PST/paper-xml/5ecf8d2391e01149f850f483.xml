<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On Mutual Information in Contrastive Learning for Visual Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-05-27">27 May 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mike</forename><surname>Wu</surname></persName>
							<email>wumike@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
							<email>chengxuz@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Milan</forename><surname>Mossé</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
							<email>yamins@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Noah</forename><surname>Goodman</surname></persName>
							<email>ngoodman@stanford.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">On Mutual Information in Contrastive Learning for Visual Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-05-27">27 May 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2005.13149v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Philosophy ( ), and Psychology ( )</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While supervised learning algorithms have given rise to human-level performance in several visual tasks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7]</ref>, they require exhaustive labelled data, posing a barrier to widespread adoption. In recent years, we have seen the growth of several approaches to un-supervised learning from the vision community <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref> where the aim is to uncover vector representations that are "semantically" meaningful as measured by performance on a variety of downstream visual tasks e.g., classification or object detection. In the last two years, this class of algorithms has already achieved remarkable results, quickly closing the gap to supervised methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>The core machinery behind these unsupervised algorithms is a basic concept: treat every example as its own label and perform classification as in the usual setting, the intuition being that a good representation should be able to discriminate between different examples. Later algorithms build on this basic concept either through (1) technical innovations to circumvent numerical instability <ref type="bibr" target="#b17">[18]</ref>, (2) storage innovations to hold a large number of examples in memory <ref type="bibr" target="#b5">[6]</ref>, (3) choices of data augmentation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b15">16]</ref>, or (4) improvements in compute or hyperparameter choices <ref type="bibr" target="#b0">[1]</ref>.</p><p>However, as a reader it is surprisingly difficult to compare these algorithms beyond high level intuition. As we get into the details of each algorithm, it is hard to rigorously justify the design. For instance, why does additional clustering of a small neighborhood around every example <ref type="bibr" target="#b18">[19]</ref> improve performance? Why does introducing CIELAB image augmentations <ref type="bibr" target="#b15">[16]</ref> yield better representations? More generally, why is per-example classification a good idea? From a practitioners point of view, the choices made by each new algorithm may appear arbitrary. Furthermore, it is unclear which directions are more promising for the next generation of algorithms. Ideally, we wish to have a theoretical framework that provides a systematic understanding of the full class of algorithms and can suggest new directions of research.</p><p>In this paper, we describe such a framework based on mutual information between "views." In doing so, we find several insights regarding the individual algorithms. Specifically, our contributions are:</p><p>• We present an information-theoretic description that can characterize IR <ref type="bibr" target="#b17">[18]</ref>, LA <ref type="bibr" target="#b18">[19]</ref>,</p><p>CMC <ref type="bibr" target="#b15">[16]</ref>, and more. To do so, we derive a new lower bound on mutual information that supports sampling examples from a restricted variational distribution. • We identify two fundamental choices in this class of algorithms: <ref type="bibr" target="#b0">(1)</ref> how to choose data augmentations (or "views") and (2) how to choose negative samples. Together these two are the crux of why instance-classification yields useful representations. • By formulating them as bounds on mutual information, we simplify and stabilize existing contrastive learning objectives.</p><p>• By varying how we choose negative examples (a previously unexplored direction), we find consistent improvements in multiple transfer tasks, outperforming IR, LA, and CMC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>We provide a review of four representation learning algorithms as described by their respective authors. We will revisit several of these with the lens of mutual information in Sec. 3.2.</p><p>Instance Discrimination (IR) IR, introduced by Wu et. al. <ref type="bibr" target="#b17">[18]</ref>, was the first algorithm to treat each example as its own class. Let x i for i in [N ] enumerate the images in a training dataset. The function g θ is a neural network mapping images to vectors of reals. The IR objective is given by</p><formula xml:id="formula_0">L IR (x i , M ) = log p(i|x i , M ), where p(i|x, M ) = e g θ (xi) T M [i]/ω N j=1 e g θ (xi) T M [j]/ω .<label>(1)</label></formula><p>Here, ω is a hyperparameter used to smooth the softmax, which otherwise may have issues with gradient saturation. The denominator in p(i|x, M ) requires computing a forward pass with g θ for N data points which can be prohibitively expensive. <ref type="bibr">Wu et. al.</ref> suggest two approaches to ameliorate the cost. First, they use a memory bank M to store the representations for every image. The i-th representation is updated using a linear combination of the stored representation and a new representation every epoch to prevent them from growing stale:</p><formula xml:id="formula_1">M [i] = α * M [i] + (1 − α) * g θ (x i )</formula><p>where α ∈ [0, 1). The notation M [i] retrieves the representation for the i-th element from the memory bank. Second, the authors approximate N j=1 e g θ (xi) T M [j]/ω ≈ κ n j=1 e g θ (xi) T M [j]/ω where n N and κ is a user-specified constant to adjust the approximation.</p><p>Local Aggregation (LA) The optimum for IR spreads representations uniformly across the surface of a sphere since then it is equally easy to discriminate any example from the others. However, such uniformity may be undesirable: images of the same class should intuitively be closer in representation than images of other classes. With this as motivation, LA <ref type="bibr" target="#b18">[19]</ref> biases a representation that pulls "nearby" images closer while "pushing" other images away. Its objective is</p><formula xml:id="formula_2">L LA (x i , M ) = log p(C i ∩ B i |x i , M ) p(B i |x i , M ) where p(I|x, M ) = i∈I p(i|x, M )<label>(2)</label></formula><p>where I is a set of indices containing i. Given the i-th image x i , the background neighbor set B i contains the k closest examples to M [i] in embedding space. Second, the close neighbor set C i contains elements that belong to the same cluster as M [i] where clusters are defined by k-nearest neighbors. Intuitively, the elements of C i should be "closer" to x i than the elements of B i , which acts as a baseline level of "closeness." Throughout training, the elements of B i and C i change. In practice, LA outperforms IR by 6% on the transfer task of ImageNet classification.</p><p>Contrastive Multiview Coding (CMC) CMC <ref type="bibr" target="#b15">[16]</ref> adapts IR to decompose an input image into the luminance (L) and AB-color channels. Then, CMC is the sum of two IR objectives where the memory banks for each modality are swapped, encouraging the representation of the luminance of an image to be "close" to the representation of the AB-color of that image, and vice versa:</p><formula xml:id="formula_3">L CMC (x i , M ) = L IR (x i,lum , M ab ) + L IR (x i,ab , M lum )<label>(3)</label></formula><p>where M modality represents the memory bank storing representations for a single modality. In practice, CMC outperforms IR by almost 10% in ImageNet classification.</p><p>A Simple Framework for Contrastive Learning (SimCLR) SimCLR <ref type="bibr" target="#b0">[1]</ref> performs an expansive experimental study of how data augmentation, architectures, and computational resources effect the IR objective. In particular, SimCLR finds better performance without a memory bank and by adding a nonlinear transformation on the representation before the dot product. That is,</p><formula xml:id="formula_4">L SimCLR (x i ) = log p(i|x i ), p(i|x i ) = e h ψ (g θ (xi)) T h ψ (g θ (xi))/ω N j=1 e h ψ (g θ (xi)) T h ψ (g θ (xj ))/ω<label>(4)</label></formula><p>where h ψ : R d → R d is usually an MLP. Note no memory banks are used in Eq. 4. Instead, the other elements in the same minibatch are used as negative samples. Combining these insights with significantly more compute, SimCLR achieves the state-of-the-art on transfer to ImageNet classification. In this work, we find similar takaways as <ref type="bibr" target="#b0">[1]</ref> and offer some theoretical support. </p><formula xml:id="formula_5">(d) CMC (CIFAR10)</formula><p>Figure <ref type="figure">1</ref>: Effect of view set choice on representation quality for IR and CMC on ImageNet and CIFAR10. Empty or trivial view sets (e.g. flipping) consistently lead to poor performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">An Unexpected Reliance on Data Augmentation</head><p>As is standard practice, IR, LA, and CMC preprocess images with a composition of random cropping, noise jitter, horizontal flipping, and grayscale conversion. In their original papers, these algorithms do not emphasize the importance of data augmentation -and understandably so, as there is no apriori good reason to do otherwise. Indeed, in the objectives from Eq. 1-3, data augmentation does not explicitly appear; the encoder g θ is instead assumed to act on the image x i directly, although in reality it acts on a transformed image. With Thm. 2.1, we make the conjecture that without data augmentation, contrastive learning would not enjoy the success it has found in practice. In fact, without the inductive bias of neural networks, instance discrimination without augmentation is trivial.</p><p>For intuition, consider that in the IR objective, which for a L 2 normalized encoder g θ , pushes points g θ (x i ) towards a uniform distribution on the surface of a sphere. We note that IR without data augmentation does not specify where exactly to place the i-th data point. So, one can place the i-th point anywhere on the sphere as long as points are equidistant from one another and still maximize Eq. 1. That is, IR is "permutation-invariant". Next, notice that the performance of a representation on a transfer task depends on the task itself. For example, in vision, features describing objects in an image are more useful to classification, detection, etc. There certainly exists a permutation such that examples of the same class are placed next to each other on the sphere. But if the optimal g θ (with respect to IR) is invariant under any permutation of the encoded points g θ (x i ), then IR cannot possibly know which permutation to select for given that it does not know the transfer task. We show that in this sense, the data augmentations we choose inject our own prior knowledge into training, helping to break this invariance to permutations in a desirable manner. In the following theorem, we express this intuition more formally, assuming an idealized setting where there is no inductive bias introduced by the optimizer nor the neural network parameterizing g θ : Theorem 2.1. Given a dataset of N realizations of a random variable X, denoted D = {x i } N i=1 , we define a set of data augmentation functions V i = {ν(x i , a) : a ∈ A} where ν : X × A → X and A is a set of indices, including an identity index 0 such that ν(x i , 0) = x i for all i ∈ [N ]. In other words, x i ∈ V i . Fix any parameters θ which minimize the Instance Discrimination objective with respect to the data, E x∼p D (x) L IR (x, M ) , and fix any permutation π of [N ].</p><p>Then optimal solutions are invariant under the permutation π if and only if augmentations applied to distinct tasks can't collide, and in particular if no augmentations are used at all. That is, for the optimal g θ , an alternative encoder g theta,π , which maps (x π(i) , a) to g θ (x i ). is also optimal if and only if V i ∩ V j = ∅ for all i, j ∈ [N ]. In particular, this holds if A = {0}. Thm. 2.1 implies that in the idealized setting, data augmentation is the crux of learning a good representation. Moreover, not all augmentations are equally good: only those that collide for different examples contribute to reducing the permutation invariance. In practice, the augmentations we choose to use bias the objective towards a subset of the optimal minima, in particular the ones good for visual transfer tasks. In the non-idealized setting, optimizing IR with no data augmentations may already produce non-trivial representations since neural network architecture and implicit regularization from SGD bias towards certain optima. But, we would still expect to see much worse representations than if we had used data augmentation to begin with. Proving Thm. 2.1 requires machinery that we develop in the reminder of the paper. Its proof and most others are in the supplement. But to begin, we can find experimental evidence to support our conjecture. We train IR and CMC on both the ImageNet and CIFAR datasets with different subsets of augmentations. For instance, we may optimize IR but only use horizontal flips as augmentation. Or, we may use no augmentations at all. For each image in the test set, we can measure the quality of the representation learned by predicting the label of its closest image in the training set by L 2 distance in the representation space, as done in <ref type="bibr" target="#b18">[19]</ref>. A better representation would properly place unseen images of a class nearby to ones seen in training, thereby classifying the unseen image correctly. Fig. <ref type="figure">1</ref> clearly shows that without augmentations (the blue line), the representations learned by IR and CMC are significantly worse (though not trivial) in terms of classification accuracy than with all augmentations (the brown line). Further, we confirm that not all augmentations lead to good representations. In particular, view sets defined by horizontal flipping and/or grayscale are disjoint for any two images. As we would expect from Thm. 2.1, the representations learned do not perform differently than using no augmentations at all. On the other hand, cropping, which certainly introduces collisions, accounts for most of the benefit amongst the basic image augmentations.</p><p>Having discovered the importance of image transformations, we restate IR to explicitly include data augmentation. Define a view function, ν : X × A → X that maps the realization of a random variable and an index to another realization. In standard contrastive learning, each index a in A is associated with a composition of augmentations (e.g. cropping with scale 0.2 plus jitter with level 0.4 plus rotation with degree 45, etc.), and a uniform distribution p(a) over A. The IR objective becomes</p><formula xml:id="formula_6">L IR (x i , M ) = E p(a) log e g θ (ν(xi,a)) T M [i]/ω N j=1 e g θ (ν(xi,a)) T M [j]/ω ,<label>(5)</label></formula><p>where the representation M [i] of the i-th image x i is updated at the n-th epoch by the equation</p><formula xml:id="formula_7">M [i] = α * M [i] + (1 − α) * g θ (ν(x i , a m )) for a m ∼ p(a).</formula><p>We will show that Eq. 5 is equivalent to a bound on mutual information between views, explaining the importance of data augmentations. To do so, we first rehearse a commonly used lower bound on MI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Equivalence of Instance Discrimination and Mutual Information</head><p>Similar connections between mutual information and representation learning have been suggested for a family of masked language models <ref type="bibr" target="#b9">[10]</ref>. However, the connection has not been deeply explored and a closer look in the visual domain uncovers several insights surrounding contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Lower Bound on Mutual Information</head><p>Mutual information (MI) measures the statistical dependence between two random variables, X and Y . Formally, we write I(X; Y ) = E X=x,y∼p log p(x,y) p(x)p(y) where p(x, y) is the joint distribution over X and Y . In general, mutual information is computationally intractable to compute. This is especially true in machine learning settings as X and Y are frequently high dimensional. In lieu of infeasible integrals, there have been several approaches to lower bound mutual information <ref type="bibr" target="#b11">[12]</ref>, of which a popular one is noise-contrastive estimation <ref type="bibr" target="#b4">[5]</ref>, also called InfoNCE <ref type="bibr" target="#b10">[11]</ref>:</p><formula xml:id="formula_8">I NCE (X; Y ) = E p(x,y1) f θ,φ (x, y 1 ) − E p(y 2:K ) log 1 K K i=1 e f θ,φ (x,yi)<label>(6)</label></formula><p>where f θ,φ (x, y) = g θ (x) T g φ (y) is a witness function representing the "compatibility" of two vectors. We use g θ and g φ to designate encoders that map realizations of X and Y to vectors. We call the output of each encoder a representation. The value of the witness function alone is an unnormalized quantity. Given x, y ∼ p(x, y), the second term in Eq. 6 serves to normalize f θ,φ (x, y) with respect to other plausible realizations of Y . We use y 2:K = {y 2 , . . . , y K } to denote a set of K − 1 negative samples used to estimate the marginal probability. A common choice for the distribution over sets is p(y 2:K ) = K i=2 p(y i ). That is, i.i.d. samples drawn from the marginal distribution p(y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Equivalence of IR, CMC, and SimCLR to InfoNCE</head><p>Consider lower bounding the mutual information between two weighted view sets with InfoNCE.</p><p>Given the i-th image x i and the memory bank M with update rate α, we estimate the i-th entry as</p><formula xml:id="formula_9">M [i] = |A| n=1 α n−1 g θ (ν(x i , a n )) ≈ nα m=1 α m−1 g θ (ν(x i , a m ))<label>(7)</label></formula><p>The left expression enumerates over all elements of the index set A while the right expression sums over a m ∼ p(a), the m-th sampled index from A, over n α epochs of training. Because the contribution of any view to the memory bank entry exponentially decays, the first sum can be tightly approximated by the second where n α is a function of α. If α = 0, then n α = 1.</p><p>To generalize the above sum to views ν * :</p><formula xml:id="formula_10">X × 2 A → 2 X of index sets A ⊆ A, define g θ (v * (x i , A), w) = 1 |A| xm∈v * (xi,A) w[m] • g θ (x m ),</formula><p>for a set of weights w. We can associate an index set A i ⊆ A with each example x i as the indices a sampled from p(a) over n α epochs of training. Fixing weights w s = {1} and w t = {1, α, . . . , α nα−1 }, any index a, and an index set A j for each x j (with |A j | = n α ), we use the above to define the witness function by f θ (ν * (x i , {a}), ν * (x j , A j )) = g θ (ν * (x i , {a}), w s ) T g θ (ν * (x j , A j ), w t )/ω. We thus express the InfoNCE bound</p><formula xml:id="formula_11">I NCE (X; X) = E p(x 1:N ) E p(a) log e f θ (ν * (xi,{a}),ν * (xi,Ai))) 1 N N j=1 e f θ (ν * (xi,{a}),ν * (xj ,Aj )))<label>(8)</label></formula><p>If we squint our eyes, Eq. 8 and Eq. 5 share similar structure. The next lemma formalizes this intuition to show an equivalence of InfoNCE with IR, CMC and SimCLR.</p><p>Lemma 3.1. Let N be the size of the dataset, p(x) a marginal distribution over images x, and M a memory bank. Then the following are equivalent:</p><formula xml:id="formula_12">I NCE (X; X) − log N , E p(x) L IR (x, M ) , E p(x) L SimCLR (x) , E p(x) L CMC (x, M ) /2.</formula><p>The memory bank makes Eq. 8 quite involved, and the comparatively simple MI formulation above leads us to question its value: are weighted views more useful than the individual views? We consider the special case when α = 0. Here, the i-th entry M [i] stores only the encoding of the single view chosen in the last epoch. As such, we can simplify and remove the memory bank altogether:</p><formula xml:id="formula_13">I NCE (X; X) = E p(x 1:N ) E a 1:N ,b,b ∼p(a) log e f θ (ν(x1,b),ν(x1,b )) 1 N N j=1 e f θ (ν(x1,b),ν(xj ,aj )) . (<label>9</label></formula><formula xml:id="formula_14">)</formula><p>We will show in our experiments that this simplified form of IR performs as well as the original.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mutual Information and Data Augmentation</head><p>In Thm. 2.1, we discussed the notion that not all the optima of IR are good for transfer learning.</p><p>Having drawn connections between IR and MI, we can revisit this statement from an informationtheoretic angle. Lemma 3.1 shows that contrastive learning is the same as maximizing MI for the special case where X and Y are the same random variable. As I(X; X) is trivial, optimizing it should not be fruitful. But we can understand "views" as a device for information bottleneck that force the encoders to estimate MI under missing information: the stronger the bottleneck, the more robust the representation as it learns to be invariant. Critically, if the data augmentations are lossy, then the mutual information between two views of an image is not obvious, making the objective nontrivial. At the same time, if we were to train IR with lossless data augmentations, we should not expect any learning beyond inductive biases introduced by the neural network g θ .    View sets that hide too little (e.g. grayscale, flip) or too much information (e.g. crops that preserve too little) result in poor transfer performance. Subfigure (e) combines (a) through (d) for scale.</p><p>Fig. <ref type="figure" target="#fig_3">2</ref> depicts a more careful experiment studying the relationship been views and MI. Each point in Fig. <ref type="figure" target="#fig_3">2</ref> represents an IR model trained on CIFAR10 with different views: no augmentations (black point), grayscale images (gray point), flipped images (brown point), L-ab filters (red point), color jitter (green line) where Noise=x means adding higher levels of noise for larger x, and cropping (blue and purple lines) where the bounds [a, b] represent the minimum and maximum crop sizes with 0 being no image at all and 1 retaining the full image. By Lemma 3.1, we estimate MI (x-axis of Fig. <ref type="figure" target="#fig_3">2</ref>) using L IR plus a log N constant. We see a parabolic relationship between MI and transfer performance: views that preserve more information lead to both a higher MI (trivially) and poorer representations. Similarly, views that hide too much information lead to very low MI and again, poor representations. It is a sweet spot in between where we find good classification performance.</p><p>Finally, we note that Lemma 3.1 provides a more critical comparison between IR and CMC: as the two are functionally identical, the only differences are in how each defines their views. We make three observations: First, the view set for CMC is partitioned into two disjoint sets with a one-to-one correspondence between elements of each set (since an image is decomposed into an L and AB filter) -further, as L and AB capture almost disjoint information, CMC imposes a strong information bottleneck between any two views. In fact, Fig. <ref type="figure" target="#fig_3">2e</ref> shows the L-ab view set to be at the apex of the curve between MI and accuracy. Second, the notion of "view" as an L or AB filter of an image versus "view" as cropping or adding jitter are one and the same. Whereas the original paper <ref type="bibr" target="#b15">[16]</ref> focuses on the former exclusively, we find this more general interpretation to be useful -as Fig. <ref type="figure">1c and d</ref> show, performance still dramatically varies without cropping. Third, Fig. <ref type="figure">1c</ref> and d also show that without any augmentations, CMC still maintains a baseline performance much higher than IR does. The MI framework makes it easy to understand why this might be: even with no augmentations, the view set for CMC is nontrivial as it at least contains L and AB filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Simplifying Contrastive Learning</head><p>Showing an equivalence to mutual information can help us (1) pick hyperparameters and (2) simplify the contrastive learning pipeline altogether. We give two examples below.</p><p>The memory bank is not critical to representation learning. The mutual information framework in Sec. 3.2 suggested a simpler version of IR with α = 0 in which the memory bank can be replaced by sampling another random view. We can compare these formulations experimentally by measuring transfer accuracy on classification. Fig. <ref type="figure" target="#fig_4">3e</ref>-g shows results varying α from 0 (no bank) to near 1 (very slowly update). We find performance when α = 0 and when α = 0.5 (the standard approach) are equal across algorithms and datasets. This suggests that we do not require a memory bank but merely  can choose two random views every iteration. SimCLR <ref type="bibr" target="#b0">[1]</ref> has suggested a similar finding. Our contribution is to show that the MI framework made such a simplification obvious.</p><p>Softmax "hacks" are unnecessary. Because the original IR paper was a million category classification problem, its implementation required innovations to be tractable. For instance, IR approximates the denominator in Eq. 1 with a Monte Carlo estimate scaled κ = 2876934.2/1281167. Such practices were propagated to LA and CMC in the official implementations online. While this setup works well for ImageNet, it is not clear how to set the constants for other datasets. For small datasets like CIFAR10, such large constants introduce numerical instability in themselves. However, once we draw the relationship between IR and MI, we immediately see that there is no need to actually compute the softmax (and no need for κ) -rather InfoNCE only requires logsumexp, a much more stable operation. Fig. <ref type="figure" target="#fig_4">3c and d</ref> show the effect of switching from the original IR code to InfoNCE. While we expected to find the large impact in CIFAR10, we also find that even in ImageNet (for which those constants were chosen), InfoNCE improves performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Equivalence of Local Aggregation and Mutual Information</head><p>Having shown an equivalence between IR, SimCLR, CMC and InfoNCE, we might wish to do the same for LA. However, LA has two distinguishing features -close and background neighbor sets -that are not obviously related to MI. In the next few paragraphs, we show how to describe LA with MI, uncovering several insights and new algorithms along the way. Namely, we introduce a generalization of InfoNCE that supports sampling from a restricted variational distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A Variational Lower Bound on Mutual Information</head><p>Recall that InfoNCE draws negative samples independently from p(y) but this choice may not be desirable. We may wish to choose negative samples from a different distribution over sets q(y 2:K ) or we may even wish to conditionally sample negatives, q(y 2:K |y 1 ) where y 1 ∼ p(y). While previous literature presents InfoNCE with an arbitrary variational distribution <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref> that would justify either of these choices, we have not found a proof supporting this. One of the contributions of this paper is to formally define a class of variational distributions q(y 2:K |y 1 ) such that Eq. 6 remains a valid lower bound if we replace p(y 2:K ) with q(y 2:K |y 1 ). We begin with the following theorem: x) and suppose that g is P-integrable with mean c. Picking two thresholds</p><formula xml:id="formula_15">Theorem 4.1. Fix a distribution P over (R d , B R d ). Fix any x * in R d and any f : R d × R d → R. Define g(x) = e f (x * ,</formula><formula xml:id="formula_16">γ ≥ τ &gt; log c in R, let S τ,γ = {x | τ ≤ f (x * , x) ≤ γ}.</formula><p>Suppose that P(S τ,γ ) &gt; 0, and define Q τ,γ (A) = P(A|S τ,γ ) for any Borel A.</p><formula xml:id="formula_17">Then E P [g(x)] &lt; E Qτ,γ [g(x)].</formula><p>Proof. It suffices to show the inequalities c &lt; e τ ≤ E Qτ,γ [g(x)]. The first holds since log c &lt; τ . The second holds since 1 Sτ,σ e τ ≤ 1 Sτ,σ g(x), and taking the expectation E Qτ,γ of both sides of this inequality gives the desired result upon observing that E Qτ,γ [1 Sτ,γ e τ ] = Q τ,γ (S τ,γ )e τ = e τ and that</p><formula xml:id="formula_18">E Qτ,γ [1 Sτ,σ g(x)] = E Qτ,γ [g(x)].</formula><p>Next, we apply Thm. </p><formula xml:id="formula_19">Y i , Y j ) → R define S τ,γ = {y|τ ≤ f (y 1 , y) ≤ γ} with τ &gt; log E p(y) [e f (y1,y) ]. For any Borel A = A 2 × .... × A K , define a variational distribution over Y 2:K = (Y 2 , ..., Y K ) by q τ,γ (Y 2:K ∈ A) = K j=2 p(A j |S τ,γ ). Define I VINCE (X; Y 1 ) = E p(x,y1) E y 2:K ∼qτ,γ log e f (x,y 1 ) 1 K K j=1 e f (x,y j ) . Then I VINCE ≤ I NCE .</formula><p>Proof. Let h be any monotonic increasing function. Note that each e f (x,yj ) for j in [K] satisfies the conditions on g in Thm 4.1. Since τ &gt; log E p(y) [e f (y1,y) ], by Thm 4.1,</p><formula xml:id="formula_20">E p(y 2:K ) h( 1 K K j=1 e f (x,yj ) ) ≤ E qτ,γ (y 2:K ) h( 1 K K j=1 e f (x,yj ) ) . If h = log, then I VINCE (X; Y 1 ) ≤ I NCE (X; Y 1 ). As −τ → ∞ = γ, the bound is tight.</formula><p>A Toy Example Thm. 4.1 and Coro. 4.1.1 imply an ordering to the MI bounds considered. We next explore the tightness of these bounds in a toy example with known MI. Consider two random variables Z and distributed such that we can pick independent samples z i ∼ N (0, Σ Z ) and i ∼ N (0, Σ )</p><p>where Σ Z = 1 −0.5 −0.5 1 and Σ = 1 0.9 0.9 1 Then, let (X, Y ) = Z + . That is, introduce a new random variable X as the first dimension of the sum and Y as the second. The mutual information between X and Y can be analytically computed as  <ref type="table" target="#tab_3">1</ref>, we find that a higher τ results in a looser estimate (as expected from Coro. 4.1.1). It might strike the reader as peculiar to use VINCE for representation learningit is a looser bound than InfoNCE, and in general we seek tightness with bounds. However, we will argue that learning a good representation and tightening the bound are two subtly related but fundamentally distinct problems.</p><formula xml:id="formula_21">I(X; Y ) = − 1 2 log(1 − Σ[1,2]Σ[2,1] Σ[1,1]Σ[2,2] ) as (X, Y ) is jointly Gaussian with covariance Σ = Σ Z + Σ .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Equivalence of LA and VINCE</head><p>Focusing first on the background neighbor set in LA, we can connect it to MI through VINCE. Consider sampling from q τ (x|x i )<ref type="foot" target="#foot_0">1</ref> as defined in Thm 4.1 with x i being the i-th image. Assuming a larger threshold τ , we are forced to sample negatives that more closely resemble g θ (x i ), the representation of the i-th image. With Fig. <ref type="figure" target="#fig_3">2</ref>, we have already seen that a tighter bound (i.e. lossless views) does not mean better representations. Similarly, by using VINCE, we trade a looser bound for a more challenging problem: encoders must now distinguish between more similar objects, forcing the representation to be more semantically meaningful. Replacing p(x i ) with q τ (x|x i ) immediately suggests new contrastive learning algorithms that "interpolate" between IR and LA.</p><p>Lemma 5.1. Let B i and C i refer to the background and close neighbor sets, respectively. We define the Ball Discrimination (BALL) objective as L BALL (x i , M ) = log p(i|xi,M ) p(Bi|xi,M ) . Similarly, define the Annulus Discrimination (ANN) objective as L ANN (x i , M ) = log p(i|xi,M ) p(Bi\Ci|xi,M ) . Then, BALL and ANN both lower bound MI between weighted view sets. That is,</p><formula xml:id="formula_22">E p(xi) [L BALL (x i , M ) + log |B i |] and E p(xi) [L ANN (x i , M ) + log |B i \ C i |]</formula><p>are both equivalent to I VINCE (X; X). In the former, we draw elements of B i from a variational distribution q τ (x|x i ). In the latter, we sample elements of</p><formula xml:id="formula_23">B i \ C i sampled from different q τ,γ (x|x i ) with finite γ.</formula><p>The primary difference between BALL and IR is that negative samples are drawn from a restricted domain of the marginal distribution "centered" at the current data point x i . Thus, we cannot equate the BALL estimator to InfoNCE; we must rely on VINCE, which provides the machinery to describe using a conditional distribution with smaller support. While the BALL and ANN objectives are both equivalent to VINCE, only the latter has a finite threshold γ used to define the close neighbor set C i . Further, while ANN and LA both use a close neighbor set, they differ in that LA pulls the representations of elements in C i closer whereas ANN does not. Yet, BALL and LA are quite similar as they sample negatives from the same distribution. Next, consider a variation of BALL where the background neighbor B iter i is dependent on the training iteration. We initialize B 0 i to the entire marginal distribution and anneal it throughout training to a smaller subset -this is equivalent to initializing the threshold τ to ∞ and increasing it every gradient step. Doing so is well-defined as the variational distribution implied by B iter i is static at any iteration. We call this BALL+. Similarly, we define ANN+ where the close neighbor set is annealed as well.</p><formula xml:id="formula_24">v(x ,a) i v(x ,b) i v(x ,c) j (a) IR v(x ,a) i v(x ,c) j B i v(x ,b) i (b) BALL v(x ,a) i v(x ,c) j B i C i v(x ,b) i (c) ANN v(x ,a) i v(x ,a) i v(x ,a) k v(x ,c) j B i C i v(x ,b) i (d) LA v(x ,a) i v(x ,c) j B i t Bi t-1 Bi t-2 Bi t-3 v(x ,b) i (e) BALL+ v(x ,a) i v(x ,c) j B i t C i t Bi t-1 Bi t-2 Bi t-3 v(x ,b) i (f) ANN+</formula><p>It remains to show that LA lower bounds mutual information. First, consider a simplified version of LA where we assume that the close neighbor set contains only the current image. That is, C i = {i}. We call this LA 0 . It is straightforward to show that LA 0 is equivalent to BALL. Lemma 5.2. Fix C i = {i}. Then L LA0 (X; X) = L BALL (X; X). Now, we pose LA as a generalization of LA 0 where elements of the close neighbor set C i can be thought of as views of x i , the current image. To start, define the view set of x i as {ν(x i , a) : a ∈ A}. Consider an enlarged view set j∈Ci {ν(x j , a) : a ∈ A}, recalling i ∈ C i i.e. the image x i is a member of its own close neighbor set. So the enlarged view set contains all views of x i and all views of images in the close neighbor set of x i . Then, optimizing L LA is akin to optimizing L LA0 where We can now say something concrete about IR and LA. Namely, both are lower bounds on MI between weighted view sets. However, LA makes two different choices. First, LA expands the view set for each image to include neighboring images in representation space. For a good representation (hence why LA needs to initialized from IR), images in the close neighborhood make for semantically meaningful views. A larger view set then contributes to a more abstract representation. Second, LA chooses more difficult negative samples, again requiring a stronger representation to properly differentiate examples. Fig. <ref type="figure" target="#fig_6">5</ref> shows a summary of the many algorithms presented in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>The theory suggests that representation quality in increasing order to be IR, BALL, ANN, then LA.</p><p>To verify this, we fit each of these algorithms on ImageNet and CIFAR10. Fig. <ref type="figure" target="#fig_5">4</ref> show the nearest neighbor classification accuracy on a test set throughout training. Table <ref type="table" target="#tab_4">2</ref> shows transfer classification performance: accuracy of logistic regression trained using the frozen representations learned by each of the unsupervised algorithms. We follow the training paradigms in prior works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b0">1]</ref> and standardize hyperparameters across models (see Sec. B.3).</p><p>Takeaway #1 We confirm that the ordering of performance for IR, BALL, ANN, and LA is as expected for ImageNet and CIFAR10. In particular, the performance curves in Fig. <ref type="figure" target="#fig_5">4</ref> show that BALL and ANN account for half the performance gains of LA over IR, agreeing with our analysis that both of the choices of negative sampling and view set are important for building strong representations.</p><p>Takeaway #2 To show that the mutual information framework generalizes to other contrastive learning models, we compare CMC to ball and annulus extensions of CMC, denoted by CMC-BALL and CMC-ANN. As Fig. <ref type="figure" target="#fig_5">4(c,d</ref>) and Table <ref type="table" target="#tab_4">2</ref> find similar patterns to Takeaway #1 and for CMC-based models. In particular, we can surpass the performance of CMC (and IR) by choosing harder negative samples with VINCE in both ImageNet and CIFAR10.</p><p>Takeaway #3 BALL+ and ANN+ out-perform all other algorithms. For example, on CIFAR10, ANN+ surpasses LA by 3% while CMC-ANN+ surpasses CMC by over 2%. We see similar gains in ImageNet where CMC-ANN+ achieves an accuracy of 50.5%, a difference of 2% with CMC and LA. These positive results exemplify the power of carefully choosing negative samples.</p><p>Other Transfer Tasks The representations learned through contrastive learning are posited to be general and useful for many transfer tasks. Inspired by <ref type="bibr" target="#b5">[6]</ref>, we test our algorithms on several downstream visual tasks other than classification: object detection, instance segmentation, and keypoint detection. Like <ref type="bibr" target="#b5">[6]</ref>, we use the Detectron2 <ref type="bibr" target="#b16">[17]</ref> pipeline. However, we use a frozen ResNet18 backbone<ref type="foot" target="#foot_1">2</ref> to focus on representation quality (that is, parameters are not finetuned). Table <ref type="table" target="#tab_4">2</ref> show commonly reported metrics for the COCO'17, Pascal VOC '07 <ref type="bibr" target="#b2">[3]</ref>, and LVIS <ref type="bibr" target="#b3">[4]</ref> datasets. The results uncover the same pattern: ANN+ consistently performs better than other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have presented an interpretation of representation learning based on mutual information between image views. This formulation led to more systematic understanding of a family of existing approaches. It further enabled simplifications of these approaches and new, better performing techniques. In particular, we uncovered that the choices of views and negative sample distribution strongly influence the performance of contrastive learning. By choosing more difficult negative samples, we surpassed high-performing algorithms like LA and CMC across several popular visual tasks.</p><p>This framework suggests several new directions. Future research could investigate automatically generating views or learning the parameters of the variational distributions. Visual algorithms like IR and LA no longer look very different from masked language modeling, as both families are unified under mutual information. Future work could pursue generalizations to multimodal domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>Contrastive algorithms are becoming a popular method for unsupervised learning, as its reach spreads into multiple domains beyond vision e.g. audio. A good theoretical framework from which to understand and compare variations in contrastive learning is useful to validate experimental results and guide future directions. As the field continues to move, we hope the framework presented in this paper serves as a useful perspective. There are several outstanding questions that this paper does not address: by exposing the importance of negative sampling and views, there are questions surrounding how to automatically make these choices. Being able to do so would lead to impact in generalizing contrastive learning to other domains like language and audio where the term "augmentation" is not as well-defined. We hope this work serves as a motivator for new research in this direction.</p><p>But as noted in the main text, there is a simpler formulation of IR equivalent to InfoNCE where we replace the memory bank with the drawing of a second random view of x i . As SimCLR draws negative samples from the same minibatch as x i , which are chosen i.i.d., the equivalence holds.</p><p>A.2 Proof of Lemma 5.1</p><p>Proof. We begin with the BALL objective. Expand L BALL and cancel denominators.</p><formula xml:id="formula_25">E p(xi) [L BALL (x i , M ) + log |B i |] = E p(xi)p(a)   log    e g θ (v(x i ,a)) T M [i]/ω N j=1 e g θ (v(x i ,a)) T M [j]/ω k∈Bi ( e g θ (v(x i ,a)) T M [k]/ω N j=1 e g θ (v(x i ,a)) T M [j]/ω )    + log |B i |    = E p(xi)p(a) log e g θ (v(xi,a)) T M [i]/ω 1 |Bi| k∈Bi e g θ (v(xi,a)) T M [k]/ω = E p(xi) E qτ (x j =i |xi) E p(a)   log e f θ (v * (xi,{a}),v * (xi,Ai)) 1 |Bi| |Bi| j=1 e f θ (v * (xi,a),v * (xj ,Aj ))   = I VINCE (X; X)</formula><p>where q τ (x j =i |x i ) = |Bi| j=1,j =i q τ (x j |x i ). Define index sets A i and a view function v * over sets as in Sec. 3.2. The third equality holds by rewriting a memory bank entry as a weighted sum. The proof for ANN follows identically but the variational distribution q τ,γ (x j |x i ) must be a function of γ, which is used to exclude the close neighbor set C i from the distribution of valid negative samples.</p><formula xml:id="formula_26">A.3 Proof of Lemma 5.2 Proof. L LA0 (x i , M ) = log p(Ci∩Bi|xi,M ) p(Bi|xi,M ) = log p(i|xi,M ) p(Bi|xi,M ) = L BALL (x i , M ).</formula><p>A.4 Proof of Lemma 5.3</p><p>Proof. Recall that the images in the set {x k |k ∈ C i } are the most semantically close to the current image x i (by construction). As such, we view each x k as x i with some (semantic) noise added. For example, if x i is an image of a dog, x k may be another dog with similar visual features.</p><p>More formally, fix an index i (in the training dataset). Then for all k ∈ C i , there exists some noise ε such that some x k ≈ x ε i where x ε i = x i + ε, the current image with some noise added. Then,</p><formula xml:id="formula_27">E p(x ε i ) g θ (ν(x ε i , a)) T M [i]/ω ≈ 1 |C i ∩ B i | k∈Ci∩Bi g θ (ν(x k , a)) T M [i]/ω with |C i ∩ B i |</formula><p>samples to approximate the expectation. That is, the elements of C i can be seen as a Monte Carlo approximation of an expectation with respect to the marginal distribution over noisy images of x i , denoted p(x ε i ). Now,</p><formula xml:id="formula_28">L LA (x i , M ) = log p(C i ∩ B i |x i , M ) p(B i |x i , M ) ∝ log E p(a) 1 |Ci∩Bi| k∈Ci∩Bi e (g θ (ν(x k ,a)) T M [i]/ω) /Z p(B i |x i , M ) ≈ log E p(x ε i )p(a) [e (g θ (v(x i ,a)) T M [i]/ω) /Z] p(B i |x i , M ) ≤ log E p(xi)p(a) [e (g θ (ν(xi,a)) T M [i]/ω) /Z] p(B i |x i , M ) = log p(i|x i , M ) p(B i |x i , M )</formula><p>where we use the equivalence introduced above in line 2. We use Z to abbreviate the denominator of the IR objective. Finally, the inequality in the second to last line holds since adding noise to a vector decreases its correlation with any other vector.</p><p>A.5 Proof of Theorem 2.1</p><p>Proof. Throughout, we assume the equivalence between IR and InfoNCE shown in Sec. 3.2 and 4. Consider first the case where A = {0}. The empirical estimate of InfoNCE uses the same distribution p(a) over indices a ∈ A, now sampling data points x i from the dataset distribution p D (x) in place of the prior p(x):</p><formula xml:id="formula_29">ÎNCE (X; X) = E x 1:N ∼p D (x) E a 1:N ,b,b ∼p(a) log e g θ (v(x1,b)) T g θ (v(x1,b )) 1 N N i=1 e g θ (v(x1,b)) T g θ (v(xi,ai))</formula><p>Since there is no augmentation (i.e., only one index a = 0), any g θ optimizing the above is a one-to-one map from image-index pairs (x i , a) to images x j , so there exists a map g theta,π sending (x π(i) , a) to x j . Since the x i are drawn i.i.d., the above expectation doesn't change its value if we substitute x π(i) for x i , so that g theta,π is optimal as well.</p><p>We move now to the more general case in which the views of distinct x i , x j do not collide, i.e., in which V i ∩ V j = ∅. To show g theta,π is still optimal in this setting, consider an augmented set D = i∈[N ] V i . Note that by construction D ⊆ D. If we let p(a) be the uniform distribution over indices and p(ã) be a trivial distribution over the indices Ã = {0}, then</p><formula xml:id="formula_30">E p D (x) E p(a) [• • • ] = E p D (x) E p(ã) [• • • ]</formula><p>In English, maximizing InfoNCE using the dataset D and the (disjoint) view set V i for example x i is equivalent to maximizing InfoNCE with the augmented dataset D and trivial view set V i = {v 0 } for all i. This case thus reduces to the previous one, in which A = {0}. This shows that non-colliding augmentations lead to permutation invariance. To show the reverse implication, consider distinct x i , x j such that v m (x i ) = v l (x j ) for some k, l in [M ]. In other words, V i ∩V j = ∅. Fix θ as the optimal parameters which minimize InfoNCE. Then</p><formula xml:id="formula_31">g θ (v m (x i )) T g θ (v l (x j ))</formula><p>is minimized as a consequence. Now consider a permuted encoder g theta,π . We can see that</p><formula xml:id="formula_32">g θ (v m (x i )) T g θ (v l (x j )) &lt; g theta,π (v m (x i )) T g theta,π (v l (x j ))</formula><p>That is, any permuted encoder will bring the embedding of x i further from the embedding of x j . However, with intersecting view sets, this implies that the loss with respect to g theta,π is higher than with g θ . So theta, π is not optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Proof of InfoNCE</head><p>Lemmas A.1, A.2 and Thm. A.3 are largely taken from <ref type="bibr" target="#b11">[12]</ref>. We will start with a simpler estimator of mutual information, known as the Barber-Agakov estimator, and work our way to InfoNCE. Lemma A.1. Define the (normalized) Barber-Agakov bound as I BA (X; Y ) = E p(x,y) [log q(x|y) − log p(x)] where q(x|y) is a variational distribution approximating p(x|y).</p><formula xml:id="formula_33">Then I(X; Y ) ≥ I BA (X; Y ). If we let q(x|y) = p(x)e f (x,y) Z(y)</formula><p>where f (x, y) is the witness function and Z(y) = E p(x) [e f (x,y) ] is the partition, define the unnormalized Barber-Agakov bound as</p><formula xml:id="formula_34">I UBA (X; Y ) = E p(x,y) [f (x, y) − log Z(y)] Then, I(X; Y ) ≥ I UBA (X; Y ). Proof. We first observe that I(X; Y ) = E p(x,y) log p(x|y) p(x) = E p(x,y) log q(x|y)p(x|y) p(x)q(x|y) = E p(x,y) log q(x|y) p(x) + D KL (p(x|y)||q(x|y)) ≥ E p(x,y) log q(x|y) p(x)</formula><p>= I BA (X; Y ). For the unnormalized bound, use the definition for q(x|y). Lastly, I(X; Y ) ≥ E p(x,y) log q(x|y)</p><formula xml:id="formula_35">p(x) = E p(x,y) log p(x)e f (x,y) p(x)Z(y) = E p(x,y) [f (x, y) − log Z(y)] = I UBA (X; Y ).</formula><p>The log-partition log Z(y) is difficult to evaluate. The next estimator bounds it instead. Lemma A.2. Define the Nguyen-Wainwright-Jordan bound as</p><formula xml:id="formula_36">I NWJ (X; Y ) = E p(x,y) [f (x, y)] − E p(y)</formula><p>1 e Z(y) where Z(y) is defined as in Lemma A.1. Then I(X; Y ) ≥ I NWJ (X; Y ).</p><p>Proof. For a(y) positive we use the bound log Z(y) ≤ Z(y) a(y) + log a(y) − 1, tight when a(y) = Z(y). Use this on I UBA (X; Y ).</p><formula xml:id="formula_37">I(X; Y ) ≥ I UBA (X; Y ) = E p(x,y) [f (x, y)] − E p(y) [log Z(y)] ≥ E p(x,y) [f (x, y)] − E p(y) Z(y) a(y) + log a(y) − 1 = E p(x,y) [f (x, y)] − E p(y) 1 a(y) E p(x) [e f (x,y) ] + log a(y) − 1 . Now, choose a(y) = e. Then, I(X; Y ) ≥ E p(x,y) [f (x, y)] − E p(y) 1 e E p(x) [e f (x,y) ] + log e − 1 = E p(x,y) [f (x, y)] − E p(y) Z(y) e = I NWJ (X; Y )</formula><p>Finally, we are ready to show that the InfoNCE estimator lower bounds mutual information. One of primary differences between the NWJ and the InfoNCE estimators is that the latter reuses the sample y from the joint p(x, y) in estimating the partition.</p><p>Theorem A.3. Let X, Y 1 , ..., Y K be two random variables with the Y j i.i.d. for j = 1, ..., K. Let K − 1 be the number of negative samples. Define p(x, y 1 ) as the joint distribution for X and Y 1 . Define p(y 2:K ) = K j=2 p(y j ) as the distribution over negative samples. Fix p(y j ) = p(y 1 ), the marginal distribution for Y 1 . Recall the InfoNCE estimator</p><formula xml:id="formula_38">I NCE (X; Y 1 ) = E p(x,y1)   f θ,φ (x, y 1 ) − E p(y 2:K )   log 1 K K j=1 e f θ,φ (x,yj )    <label>(10)</label></formula><p>. Then, I(X; Y 1 ) ≥ I NCE (X, Y 1 ). That is, the InfoNCE objective is a lower bound for the mutual information between X and Y 1 .</p><p>Proof. We will show that the InfoNCE bound arises from augmenting y 1 with a set of auxiliary samples y 2 , ..., y K and employ the Nguyen-Wainwright-Jordan bound.</p><p>Let Y 2:K be a set of random variables distributed independently according to p(y 1 ). As Y 1 and Y 2:K are independent, I(X; Y 1 ) = I(X; Y 1 , Y 2:K ). Now assume a witness function: f (x, y 1:K ) = 1 + log e f (x,y 1 ) a(x,y 1:K ) , where a(x, y 1:K ) = 1 K K i=1 e f (x,yi) , a Monte Carlo approximation of the partition Z(y). Apply the Nguyen-Wainwright-Jordan bound. I(X; Y 1 ) = I(X; Y 1 , Y 2:K ) ≥ E p(x,y 1:K ) 1 + log e f (x,y 1 ) a(x,y 1:K ) − E p(x)p(y 1:K ) 1 e • e 1+log e f (x,y 1 ) a(x,y 1:K ) = 1 + E p(x,y 1:K ) log e f (x,y 1 ) a(x,y 1:K ) − E p(x)p(y 1:K ) e f (x,y 1 ) a(x,y 1:K ) Because p(y 1:K ) = i p(y i ) is invariant under permutation of indices 1 : K, we have E p(x)p(y 1:K ) e f (x,y 1 ) a(x,y 1:K ) = E p(x)p(y 1:K ) e f (x,y i ) a(x,y 1:K ) for any i. Use this and the definition of a: E p(x)p(y 1:K ) e f (x,y 1 ) a(x,y 1:K )</p><formula xml:id="formula_39">= 1 K K i=1 E p(x)p(y 1:K ) e f (x,y i ) a(x,y 1:K ) = E p(x)p(y 1:K ) 1 K K i=1 e f (x,y i ) a(x,y 1:K ) = 1</formula><p>We now substitute this result into full equation above:</p><formula xml:id="formula_40">I(X; Y 1 ) ≥ 1 + E p(x,y 1:K ) log e f (x,y 1 ) a(x,y 1:K ) − 1 = E p(x,y 1:K ) f (x, y 1 ) − log 1 K K i=1 e f (x,yi) = E p(x,y1) E p(y 2:K ) f (x, y 1 ) − log 1 K K i=1 e f (x,yi) = I NCE (X; Y 1 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training Details B.1 Section 4.1: A Toy Example</head><p>The encoders are 5-layer MLPs with 10 hidden dimensions and ReLU nonlinearities. To build the dataset, we sample 2000 points and optimize the InfoNCE objective with Adam with a learning rate of 0.03, batch size 128, and no weight decay for 100 epochs. Given a percentage for VINCE, we compute distances between all elements in the memory bank and the representation the current image -we only sample 100 negatives from the top p percent. We conduct the experiment with 5 different random seeds to estimate the variance.</p><p>B.2 Experiment shown in Fig. <ref type="figure" target="#fig_3">2</ref> For each point, we train using the IR objective (t = 0.5, τ = 0.07) for 100 epochs (no finetuning) with SGD (momentum 0.9, batch size 256, weight decay 1e-4, and learning rate 0.03). We use 4096 negative samples train from the marginal distribution (training dataset). To estimate transfer accuracy, for each image in the test set, we embed it using our learned encode and then find the label of the nearest neighbor in the training dataset with the memory bank. To vary the crop and color jitter hyperparameters we use the functions in torchvision.transforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Section 6 Experiments</head><p>For these experiments, we follow the training practices in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b15">16]</ref>. In short, we used a temperature τ = 0.07 and memory bank update rate t = 0.5. For optimization, we used SGD with momentum of 0.9, batch size 256, weight decay of 1e-4, learning rate 0.03. For pretraining, the learning rate was dropped twice by a factor of 10 once learning "converged," defined as no improvment in nearest neighbor classification accuracy (computed on validation set after every training epoch) for 10 epochs.</p><p>To define the background neighbors, we used 4096 of the nearest points. To define the close neighbors, we used 10 kNN with different initializations where k = 30000 for ImageNet and k = 300 for CIFAR10. To train kNNs, we use the FAISS libary <ref type="bibr" target="#b8">[9]</ref>. For the image model architecture, we used ResNet18 <ref type="bibr" target="#b7">[8]</ref>  Once training the representation is complete, we freeze the parameters and begin the transfer phase by fitting a logistic regression on top of the learned representations to predict the labels (this is the only time the labels are required). We optimize with SGD with a learning rate of 0.01, momentum 0.9, weight decay 1e-5 and batch size of 256 for 30 epochs. Like in pretraining, we drop the learning rate twice with a patience of 10. In transfer learning, the same augmentations are used as in pretraining.</p><p>The performance from fitting logistic regression is always higher than the nearest neighbor estimate, which is nonetheless still useful as a cheaper surrogate metric.</p><p>For CIFAR, the same hyperparameters are used as above except we must use the InfoNCE formulation of IR, etc. as otherwise we face numerical instability. Images in CIFAR as reshaped to be 256 by 256 pixels to use the exact same encoder architectures as with ImageNet. When fitting LA or ANN on CIFAR10, we set k = 300 in clustering with KNN.</p><p>For CMC experiments, the hyperparameters are again consistent above with a few exceptions. First, we exclude grayscale conversion from the data augmentation list since it does not make sense for CMC. Second, the batch size was halved to 128 to fit two ResNet18 models in memory (one for L and one for AB). Thus, one should not compare the learning curves of IR against CMC (which is never done in the main text) but one can compare learning curves of CMC models to each other and IR models to each other. When training to completion (as in Table <ref type="table">.</ref> 2), it is fair to compare CMC and IR as both models have converged. Implementation wise, we adapted the official PyTorch implementation on Github.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Section 3.4: Experiments</head><p>All hyperparameters are as described in Sec. B.3 which the exception of the particular hyperparameter we are varying for the experiment. To compare InfoNCE and the original IR formulation, we adapted the public PyTorch implementation found at https://github.com/neuroailab/ LocalAggregation-Pytorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Detectron2 Experiments</head><p>We make heavy usage of the Detectron2 code found at https://github.com/ facebookresearch/detectron2. In particular, the script https://github.com/ facebookresearch/detectron2/blob/master/tools/convert-torchvision-to-d2.py allows us to convert a trained ResNet18 model from torchvision to the format needed for Detectron2. The repository has default configuration files for all the experiments in this section. We change the following fields to support using a frozen ResNet18: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Properties of a Good View Set</head><p>Recall Fig. <ref type="figure">1</ref>, which measured the quality of different representations learned under different view sets. We continue our analysis to summarize the results with a few properties that differentiate "good" and "bad" views (data augmentation techniques). Property 1 was mentioned in the main text.</p><p>Property #1: (Lossy-ness) We can frame views as a device for information bottleneck that force the encoders to estimate mutual information under missing information: the stronger the bottleneck, the more robust the representation. Further, if the view functions are lossy, then the mutual information I(X; X) is not obvious, making the objective nontrivial. Some evidence of this as a desirable property can be seen in Fig. <ref type="figure">1</ref>: lossless view functions (such as horizontal flipping or none at all) result in poor representations where as more lossy view functions (such as cropping and noise addition) form much stronger representations than lossless and less lossy view functions (e.g. grayscale).</p><p>Property #2: (Completeness) Different views should differ in the information they contain (otherwise the objective is again trivial). Consider the training paradigm for IR: every epoch, for a given x, we must choose a random view. In the limit, this paradigm would amortize the parameters of the encoders over all views in v x . This compounds the amount of information that must be compressed in the encoded representation, encouraging abstraction and invariance. With this interpretation, completeness is important as we want to be invariant to the full spectrum of views -otherwise, the learned representation would contain "holes" along certain transformations that may appear in unseen images. For instance, consider a view set of center crops versus a view set of all random crops. Property #3: (Uniqueness) Given an image, imagine we scramble all pixels to random values. This would clearly make it difficult for the witness function to properly access similarity (and be maximally lossy) but so much so that no meaningful representation can be learned. To avoid this, we demand the view set of a datum to be unique to that datum alone. In other words, the elements of view set v x for a data point x should not appear in the view set v y for any y. For instance, cropping an image or adding color jitter does not fully obscur the identity of the original image.</p><p>Property #4: (Consistency) For a datum x, a view v(x, a) for any a ∈ A must be in the same domain as x. For instance, the view of an image cannot be a piece of text. In this sense, we can consider x to be a priviledged view (of itself). Because amortizing over views increases the generalization capability of our encoders to elements of v x (and since x has membership in v x ), we expect amortization to result in a good representation for the priviledged view x. The same-domain assumption is worth highlighting: if we were to choose views of a different dimensionality or modality, we would not be able to encode (untransformed) images from a test set into our representation.</p><p>A Second Toy Example To showcase these properties, consider two interweaved spirals, one spinning clockwise and one counter-clockwise, composed of 2D coordinates: (c x , c y ). Define the elements of the view set v x for x as (c x + ε x , c y + ε y ) where ε x , ε y ∼ U (0, η). We vary η from 0 (only the priviledged view) to 5 (every point possible in the domain).  Consider the transfer task of predicting which spiral each point belongs to (using logistic regression on the representations). Thus, the information required to disentangle the spirals must be linearly separable in the representations. From Fig. <ref type="figure" target="#fig_9">6</ref>, we see that with η = 0 (lossless views), we can maximize MI (low training loss) but the transfer accuracy is near chance. As noise approaches 0.4 (lossy and unique), we find steady improvements to accuracy. From Fig. <ref type="figure" target="#fig_9">6b</ref>, only η = 0.4 has a (mostly) linear separation. However, as noise surpasses 0.4 (and views lose uniqueness), transfer accuracy recedes to chance. These results exemplify the importance of lossy-ness and uniqueness.</p><p>Training Details The encoders are 5-layer MLPs with 128 hidden dimensions and map inputs x to a 2D representation. We use 4096 negative samples drawn from the training dataset to optimize InfoNCE with SGD using momentum 0.9, weight decay 1e-5, batch size 128, and learning rate 0.03 for 10k iterations. The memory bank update parameter α is set to 0.5 with a temperature of 0.07.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Discussion</head><p>We discuss a few minor points regarding MI and contrastive learning not included in the main text. We also include some additional figures in Fig. <ref type="figure" target="#fig_10">7</ref> for memory bank experiments.</p><p>The witness function should be under-parameterized. Since f θ (x, y) encodes x and y independently before the dot product, the encoders bear the near-full burden of estimating mutual information.  However, could we parameterize f θ more heavily (e.g. perceptron)? Consider the alternative design: f θ,ψ (x, y) = h ψ (g θ (x), g θ (y)) where h ψ is a neural network. At one extreme, g θ and g θ mimic identity functions whereas h ψ assumes the burden of learning. Representations learned in this setting should prove far less useful. To test this hypothesis, we compare the following designs: (1) a dot product g θ (x) T g θ (y); (2) a bilinear transform g θ (x) T W g θ (y); (3) concatenate g T θ (x) and g θ (y) and pass it into a linear layer projecting to 1 dimension; (4,5) concatenate g T θ (x) and g θ (y) and pass it through a series of nonlinear layers with 128 hidden nodes before projecting to 1 dimension. In order from (1) to (5), the models increase in expressivity. Fig. <ref type="figure" target="#fig_10">7a and b</ref> show classification performance on ImageNet and CIFAR10. We find that a more expressive h ψ results in lower accuracy, symbolizing a weaker representation. We highlight that these experiments differ from <ref type="bibr" target="#b0">[1]</ref> where the authors found f θ,ψ (x, y) = (h ψ • g θ )(x) T (h ψ • g θ )(y) to improve performance.</p><p>Harder negative samples is not always good. In Fig. <ref type="figure" target="#fig_5">4</ref> and Table <ref type="table" target="#tab_4">2</ref>, we see that slowly increasing the threshold τ significantly improves performance (see BALL+/ANN+). This implies that using harder negative samples may not be beneficial at all points in training. Early in training, it could that difficult negatives bias towards a local optima. To test this, we can train BALL models with varied τ whose parameters are initialized with a IR model at different points of training (e.g. epoch 1 versus epoch 100). Fig. <ref type="figure">8</ref> shows exactly this, additionally varying the number of total negatives drawn from 100 to 4096. Recall from the first toy experiment that a smaller percentage p represents a larger τ (with p = 100% equaling IR). We make two observations: First, at the beginning of training, difficult negatives hurt the representation. Second, with too few samples, difficult negatives are too close to the current image, stunting training. But with too many samples, difficult negatives are "drowned out" by others. If we balance the difficulty of the negatives with the number of samples, we find improved performance with larger τ (see Fig. <ref type="figure">8b</ref>). In summary, always using harder negative samples is not a bulletproof strategy. In practice, we find slowly introducing them throughout training to work well.</p><p>The temperature parameter is important. In simplifying contrastive learning, we might wonder if we could remove the temperature parameter ω. Fig. <ref type="figure">9</ref> suggests that tuning ω is important for at least the speed of convergence. We find the best ω to be dataset dependent.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Nearest neighbor classification accuracy versus MI on CIFAR10 under different view sets.View sets that hide too little (e.g. grayscale, flip) or too much information (e.g. crops that preserve too little) result in poor transfer performance. Subfigure (e) combines (a) through (d) for scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: NN classification accuracy comparing the IR objective with InfoNCE in stability (a,b), and different α for the memory bank of IR, LA (e-h) on Imagenet ( †) and CIFAR10 ( ‡).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Nearest neighbor (NN) classification accuracy throughout training: (a,b) show the IR family of models on ImageNet and CIFAR10; (c,d) show similar results for the CMC family.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of five different contrastive learning algorithms. Black points represent the current image; gray points represent other views; red points represent negative samples; blue points represent elements of an extended view set for LA. The dark gray areas represents a valid negative sampling distribution. BALL+ and ANN+ shrink the dark gray area throughout training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Consider points along two intertwined spirals. (a) We vary the amount of noise added to the points. (b) We visualize the learned 2D representation. (c) Training losses over time. With more noise, MI is lower. (d) The transfer task is to predict which spiral a point belongs to.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Nearest neighbor classification accuracy for various experiments: (a, b) compare different architectures for f θ (x, y); (c, d) compare the original IR objective with InfoNCE in stability; (e-h) compare different update parameters α for the memory bank of IR, LA, and CMC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Relationship between the total number of negative samples drawn and its distribution: (a,b ,c) show the nearest neighbor classification accuracy when using various values for τ to define q τ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>4.1 to InfoNCE to construct the variational InfoNCE, or VINCE, estimator and prove that for large enough τ , it is a lower bound on InfoNCE and thus, mutual information. Corollary 4.1.1. Let X be independent from Y 1 , ..., Y k , the latter i.i.d. according to a distribution p. For f : (</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Looseness of VINCEWe fit InfoNCE and VINCE with varying values for τ and measure the estimated mutual information. Table1shows the results. For rows with VINCE, the percentage shown in the left column represents the percentage of training examples (sorted by L 2 distance in representation space to y 1 ) that are "valid" negative samples. For instance, 50% would indicate that negative samples can only be drawn from the 50% of examples whose representation is closest to g θ (y 1 ) in distance. Therefore, a smaller percentage is a higher τ . From Table</figDesc><table><row><cell>Method</cell><cell>Estimate of MI</cell></row><row><cell>True</cell><cell>0.02041</cell></row><row><cell>InfoNCE</cell><cell>0.01345 ± 0.001</cell></row><row><cell cols="2">VINCE (90%) 0.01241 ± 3e−4</cell></row><row><cell cols="2">VINCE (75%) 0.00220 ± 1e−4</cell></row><row><cell cols="2">VINCE (50%) 7.29e−5 ± 9e−6</cell></row><row><cell cols="2">VINCE (25%) 1.67e−5 ± 2e−6</cell></row><row><cell cols="2">VINCE (10%) 5.87e−6 ± 1e−6</cell></row><row><cell>VINCE (5%)</cell><cell>1.97e−6 ± 4e−6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Evaluation</figDesc><table><row><cell>Model</cell><cell>Top1</cell><cell>Top5</cell><cell></cell><cell></cell><cell>Model</cell><cell></cell><cell>Top1</cell><cell>Top5</cell><cell>Model</cell><cell>Top1</cell><cell>Top5</cell><cell></cell><cell></cell><cell>Model</cell><cell>Top1</cell><cell>Top5</cell></row><row><cell>IR</cell><cell>81.2</cell><cell>98.9</cell><cell></cell><cell></cell><cell>CMC</cell><cell></cell><cell>85.6</cell><cell>99.4</cell><cell>IR</cell><cell>43.2</cell><cell>67.0</cell><cell></cell><cell></cell><cell>CMC</cell><cell>48.2</cell><cell>72.4</cell></row><row><cell>BALL</cell><cell>81.4</cell><cell>99.0</cell><cell></cell><cell cols="3">CMC-BALL</cell><cell>85.7</cell><cell>99.5</cell><cell>BALL</cell><cell>45.3</cell><cell>68.6</cell><cell></cell><cell cols="2">CMC-BALL</cell><cell>48.9</cell><cell>73.1</cell></row><row><cell>ANN</cell><cell>81.4</cell><cell>99.0</cell><cell></cell><cell cols="2">CMC-ANN</cell><cell></cell><cell>86.1</cell><cell>99.5</cell><cell>ANN</cell><cell>46.6</cell><cell>70.4</cell><cell></cell><cell cols="2">CMC-ANN</cell><cell>49.2</cell><cell>73.3</cell></row><row><cell>LA</cell><cell>81.8</cell><cell>99.1</cell><cell></cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>LA</cell><cell>48.0</cell><cell>72.4</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BALL+</cell><cell>82.1</cell><cell>99.1</cell><cell></cell><cell cols="3">CMC-BALL+</cell><cell>86.8</cell><cell>99.5</cell><cell>BALL+</cell><cell>47.3</cell><cell>71.1</cell><cell></cell><cell cols="2">CMC-BALL+</cell><cell>49.7</cell><cell>73.6</cell></row><row><cell>ANN+</cell><cell>84.8</cell><cell>99.3</cell><cell></cell><cell cols="3">CMC-ANN+</cell><cell>87.8</cell><cell>99.5</cell><cell>ANN+</cell><cell>48.4</cell><cell>72.5</cell><cell></cell><cell cols="2">CMC-ANN+</cell><cell>50.5</cell><cell>74.0</cell></row><row><cell cols="4">(a) CIFAR10: Pred. Acc</cell><cell cols="5">(b) CIFAR10: Pred. Acc</cell><cell cols="4">(c) ImageNet: Pred. Acc</cell><cell cols="2">(d) ImageNet: Pred. Acc</cell></row><row><cell>Model</cell><cell>AP bb</cell><cell>AP bb 50</cell><cell cols="2">AP bb 75</cell><cell>AP mk</cell><cell></cell><cell>AP mk 50</cell><cell>AP mk 75</cell><cell>Model</cell><cell>AP bb</cell><cell>AP bb 50</cell><cell cols="2">AP bb 75</cell><cell>AP mk</cell><cell>AP mk 50</cell><cell>AP mk 75</cell></row><row><cell>IR</cell><cell>6.1</cell><cell>13.9</cell><cell cols="2">4.4</cell><cell>6.0</cell><cell></cell><cell>12.7</cell><cell>5.1</cell><cell>IR</cell><cell>8.6</cell><cell>19.0</cell><cell cols="2">6.6</cell><cell>8.5</cell><cell>17.4</cell><cell>7.4</cell></row><row><cell>BALL</cell><cell>6.3</cell><cell>14.3</cell><cell cols="2">4.9</cell><cell>6.4</cell><cell></cell><cell>13.2</cell><cell>5.6</cell><cell>BALL</cell><cell>9.4</cell><cell>20.3</cell><cell cols="2">7.3</cell><cell>9.3</cell><cell>18.8</cell><cell>8.4</cell></row><row><cell>ANN</cell><cell>6.7</cell><cell>14.8</cell><cell cols="2">5.3</cell><cell>6.7</cell><cell></cell><cell>13.9</cell><cell>5.9</cell><cell>ANN</cell><cell>9.9</cell><cell>21.2</cell><cell cols="2">7.8</cell><cell>9.7</cell><cell>19.7</cell><cell>8.6</cell></row><row><cell>LA</cell><cell>7.4</cell><cell>16.1</cell><cell cols="2">5.7</cell><cell>7.4</cell><cell></cell><cell>15.2</cell><cell>6.4</cell><cell>LA</cell><cell>10.2</cell><cell>22.0</cell><cell cols="2">8.1</cell><cell>10.0</cell><cell>20.3</cell><cell>9.0</cell></row><row><cell>BALL+</cell><cell>7.2</cell><cell>15.5</cell><cell cols="2">5.6</cell><cell>7.3</cell><cell></cell><cell>14.4</cell><cell>6.2</cell><cell>BALL+</cell><cell>9.8</cell><cell>21.0</cell><cell cols="2">7.7</cell><cell>9.8</cell><cell>19.8</cell><cell>8.8</cell></row><row><cell>ANN+</cell><cell>7.7</cell><cell>16.5</cell><cell cols="2">6.0</cell><cell>7.8</cell><cell></cell><cell>15.5</cell><cell>6.7</cell><cell>ANN+</cell><cell>10.7</cell><cell>22.8</cell><cell cols="2">8.5</cell><cell>10.7</cell><cell>20.9</cell><cell>9.4</cell></row><row><cell cols="9">(e) COCO: Mask R-CNN, R18-C4, 1x schedule</cell><cell cols="6">(f) COCO: Mask R-CNN, R18-FPN, 1x schedule</cell></row><row><cell>Model</cell><cell>AP kp</cell><cell>AP kp 50</cell><cell></cell><cell>AP</cell><cell>kp 75</cell><cell></cell><cell>Model</cell><cell>AP bb</cell><cell>AP bb 50</cell><cell>AP bb 75</cell><cell cols="2">Model</cell><cell></cell><cell>AP mk</cell><cell>AP mk 50</cell><cell>AP mk 75</cell></row><row><cell>IR</cell><cell>34.6</cell><cell>63.0</cell><cell></cell><cell cols="2">32.9</cell><cell></cell><cell>IR</cell><cell>5.5</cell><cell>14.5</cell><cell>3.3</cell><cell>IR</cell><cell></cell><cell></cell><cell>3.0</cell><cell>5.9</cell><cell>2.8</cell></row><row><cell>BALL</cell><cell>34.9</cell><cell>63.6</cell><cell></cell><cell cols="2">33.8</cell><cell></cell><cell>BALL</cell><cell>6.0</cell><cell>15.7</cell><cell>3.5</cell><cell cols="2">BALL</cell><cell></cell><cell>3.4</cell><cell>6.5</cell><cell>3.3</cell></row><row><cell>ANN</cell><cell>35.9</cell><cell>64.9</cell><cell></cell><cell cols="2">34.2</cell><cell></cell><cell>ANN</cell><cell>6.6</cell><cell>17.6</cell><cell>4.1</cell><cell cols="2">ANN</cell><cell></cell><cell>3.8</cell><cell>7.3</cell><cell>3.5</cell></row><row><cell>LA</cell><cell>36.3</cell><cell>65.3</cell><cell></cell><cell cols="2">35.1</cell><cell></cell><cell>LA</cell><cell>7.6</cell><cell>20.0</cell><cell>4.3</cell><cell>LA</cell><cell></cell><cell></cell><cell>3.8</cell><cell>7.2</cell><cell>3.5</cell></row><row><cell>BALL+</cell><cell>36.3</cell><cell>65.2</cell><cell></cell><cell cols="2">35.0</cell><cell></cell><cell>BALL+</cell><cell>7.1</cell><cell>18.3</cell><cell>4.1</cell><cell cols="2">BALL+</cell><cell></cell><cell>4.0</cell><cell>7.7</cell><cell>3.8</cell></row><row><cell>ANN+</cell><cell>37.0</cell><cell>66.1</cell><cell></cell><cell cols="2">35.6</cell><cell></cell><cell>ANN+</cell><cell>7.5</cell><cell>20.1</cell><cell>4.3</cell><cell cols="2">ANN+</cell><cell></cell><cell>4.3</cell><cell>8.2</cell><cell>4.0</cell></row><row><cell cols="6">(g) COCO: Keypt R-CNN, R18-FPN</cell><cell cols="5">(h) VOC: Faster R-CNN, R18-C4</cell><cell cols="4">(i) LVIS: Mask R-CNN, R18-FPN</cell></row></table><note>of the representations using six visual transfer tasks: object classification on ImageNet and CIFAR10 (a-d); object detection and instance segmentation on COCO (e,f); keypoint detection on COCO (g); object detection on Pascal VOC 2007 (h); and instance segmentation on LVIS (i). In all cases, the backbone network is a frozen pretrained ResNet-18 (R 18 ). views for image x i are sampled from the enlarged view set. The proof of the next Lemma follows a similar intuition to bound LA by LA 0 . Lemma 5.3. For any C i containing i, we have E p(x) [L LA0 (x, M )] ≥ E p(x) [L LA (x, M )].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>to prioritize efficiency; future work can investigate larger architectures. The representation dimension was fixed to 128. For image augmentations during training, we use a random resized crop to 224 by 224, random grayscale with probability 0.2, random color jitter, random horizontal flip, and normalize pixels using ImageNet statistics. When training LA on ImageNet, we initialize ResNet and the memory bank using 10 epochs of IR -we found this initialization to be crucial to finding a good optima with LA.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>The double spiral dataset contains 10k points contained within a 2D box of [−2, 2] 2 .</figDesc><table><row><cell></cell><cell>0.040</cell><cell>dot-product</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.035</cell><cell>bilinear linear</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.030</cell><cell>mlp (3) mlp (6)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Test Accuracy</cell><cell>0.015 0.020 0.025</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.010</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.005</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.000</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>Epoch</cell><cell>30</cell><cell>40</cell><cell>50</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">For γ infinite we drop the parameter, writing qτ instead of qτ,γ.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">As ResNet50 and larger are the commonly used backbones, our results will not be state-of-the-art. However, the ordering in performance between algorithms is meaningful and our primary interest.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs</head><p>A.1 Proof of Lemma 3.1</p><p>Proof. We first show an equivalence between IR and InfoNCE. As CMC and SimCLR are closely derived from IR, these equivalences follow in a straightforward manner. Define the index sets A j for each x j as in Sec. 3.2. Then for arbitrary i ∈</p><p>The second equality holds by construction, given definitions of InfoNCE for weighted sets of views.</p><p>Given this, the equivalence of InfoNCE to CMC follows from (1) that CMC is the sum of IR terms and (2) that being a bound is independent of the choice of view set. Similarly, the equivalence of InfoNCE to SimCLR follows from the fact that the bound holds regardless of how g θ is parameterized. For instance, let g θ = h θ1 • u θ2 where θ = θ 1 ∪ θ 2 . That is, the encoder is a composition of functions. Then SimCLR (with the computational changes like large batch sizes abstracted away) is equivalent to IR and thus to InfoNCE. We recognize that SimCLR, unlike IR, does not use a memory bank.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<title level="m">A simple framework for contrastive learning of visual representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A mutual information maximization perspective of language representation learning</title>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.08350</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><surname>Tucker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06922</idno>
		<title level="m">On variational bounds of mutual information</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
