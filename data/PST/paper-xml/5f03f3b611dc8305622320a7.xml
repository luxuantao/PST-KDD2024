<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Grale: Designing Networks for Graph Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-23">23 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Halcrow</surname></persName>
							<email>halcrow@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Alexandru</forename><surname>Moşoi</surname></persName>
							<email>mosoi@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Sam</forename><surname>Ruth</surname></persName>
							<email>samruth@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
							<email>bperozzi@acm.org</email>
						</author>
						<title level="a" type="main">Grale: Designing Networks for Graph Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-23">23 Jul 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3394486.3403302</idno>
					<idno type="arXiv">arXiv:2007.12002v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>How can we find the right graph for semi-supervised learning? In real world applications, the choice of which edges to use for computation is the first step in any graph learning process. Interestingly, there are often many types of similarity available to choose as the edges between nodes, and the choice of edges can drastically affect the performance of downstream semi-supervised learning systems. However, despite the importance of graph design, most of the literature assumes that the graph is static.</p><p>In this work, we present Grale, a scalable method we have developed to address the problem of graph design for graphs with billions of nodes. Grale operates by fusing together different measures of (potentially weak) similarity to create a graph which exhibits high task-specific homophily between its nodes. Grale is designed for running on large datasets. We have deployed Grale in more than 20 different industrial settings at Google, including datasets which have tens of billions of nodes, and hundreds of trillions of potential edges to score. By employing locality sensitive hashing techniques, we greatly reduce the number of pairs that need to be scored, allowing us to learn a task specific model and build the associated nearest neighbor graph for such datasets in hours, rather than the days or even weeks that might be required otherwise.</p><p>We illustrate this through a case study where we examine the application of Grale to an abuse classification problem on YouTube with hundreds of million of items. In this application, we find that Grale detects a large number of malicious actors on top of hardcoded rules and content classifiers, increasing the total recall by 89% over those approaches alone.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure <ref type="figure">1</ref>: The Graph Design Problem. On the left is an example of a typical product graph with several different kinds of possible similarity (edge types) connecting nodes of two types (green,red). While informative, this graph can be challenging for semi-supervised learning methods -even though the products share some facets of similarity, they are not informative for the final task. On the right, is the ideal graph similarity which would maximize label classification performance on this dataset. In this work, we present Grale, a scalable method for extracting graphs from very large datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As the size and scope of unlabeled data has grown, there has been a corresponding surge of interest in graph-based methods for Semi-Supervised Learning (SSL) (e.g. <ref type="bibr">[3][33]</ref>[30] <ref type="bibr" target="#b4">[5]</ref>) which make use of both the labeled data items and also of the structure present across the whole dataset. An essential component of such SSL algorithms is the manifold assumption -the assumption that the labels vary smoothly over the underlying graph structure (i.e. that the graph possesses homophily). As such, the datasets that are typically used to evaluate these methods are small datasets that exhibit this property (e.g. citation networks). However, there is relatively little guidance on how to apply these methods when there is not an obvious source of similarity that's useful for the problem <ref type="bibr" target="#b28">[29]</ref>.</p><p>Real-world applications tend to be multi-modal, and so this lack of guidance is especially problematic. Rather than there being just one simple feature space to choose a similarity on, we have a wealth of different modes to compare similarities in (each of which may be suboptimal in some way <ref type="bibr" target="#b27">[28]</ref>). For example, video data comes with visual signals, audio signals, text signals, and other kinds of metadata -each with their own ways of measuring similarity. Through work on many applications at Google we have found that the right answer is to choose a combination of these. But how should we choose this combination?</p><p>In this paper we present Grale, a method we have developed to design graphs for SSL. Our work on Grale was motivated by practical problems faced while applying SSL to rich heterogeneous datasets in an industrial setting. Grale is capable of learning similarity models and building graph for datasets with billions of nodes in a matter of hours. It is widely used inside Google, with more than 20 deployments, where it acts as a critical component of semisupervised learning systems. At its core, Grale provides the answer to a very straightforward question -given a semi-supervised learning task -"do we want this edge in the graph?".</p><p>We illustrate the benefits of Grale with a case study of a real anti-abuse detection system at YouTube, a large video sharing site. Fighting abuse on the web can be a challenging problem area because it falls into an area where features that one might have about abusive actors are quite sparse compared to the number of labeled examples available at training time. Abuse on the web is also often done at scale, inevitably leaving some trace community structure that can be exploited to fight it.</p><p>Standard supervised approaches often fail in this type of setting since there are not enough labeled examples to adequately fit a model with capacity large enough to capture the complexity of the problem.</p><p>Specifically, our contributions are the following:</p><p>(1) An algorithm and model for learning a task specific graph for semi-supervised applications.</p><p>(2) An efficient algorithm for building a nearest neighbor graph with such a model.</p><p>(3) A demonstration of the effectiveness of this approach for fighting abuse on YouTube, with hundreds of millions of items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES 2.1 Notation</head><p>We consider a general multiclass learning setting where we have a partially labeled set of points X = {x 1 , x 2 , . . . , x V } where the first L points have class labels Y = {y 1 , y 2 , . . . , y L }, each y k being a one-hot vector of dimension C, indicating which of the C classes the associated point belongs to. Further, we assume each point x i ∈ X has a multi-modal representation over d modalities,</p><formula xml:id="formula_0">x i = {x i,1 , x i,2 , . . . x i,d }. Each sub-representation x i,d has its own natural distance measure κ d .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Loss functions</head><p>Ordinarily one might try to fit a model to make predictions of y k = ŷ(x k ) by selecting a family of functions and finding a member of it which minimizes the cross-entropy loss between y and ŷ:</p><formula xml:id="formula_1">L = − i ∈Y c ∈ C y i,c log ŷi,c ,<label>(1)</label></formula><p>where y i,c and ŷi,c indicate the ground truth and prediction scores for point i with respect to class c. Given some graph G = (V , E) with edge weights w i j on our data, SSL graph algorithms <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b17">18]</ref> nearly universally seek to minimize a Potts model type loss function (either explicitly or implicitly) similar to:</p><formula xml:id="formula_2">L = i, j ∈E w i, j c ∈ C | ŷi,c − ŷj,c | + i ∈V ,c ∈ C | ŷi,c − y i,c |. (2)</formula><p>A classic way to minimize this type of loss is to iteratively apply a label update rule of the form:</p><formula xml:id="formula_3">ŷ(n+1) i,c = αy i,c + β j ∈N i w i, j ŷ(n) j,c j ∈N i w i, j ,<label>(3)</label></formula><p>where N i is the neighborhood of point i, ŷ(n) i,c is the predicted score for point i with respect to class c after n iterations, and α and β are hyperparameters. Here, we consider a greedy solution which seeks to minimize Eq. ( <ref type="formula" target="#formula_1">1</ref>) using a single iteration of label updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The Graph Design Problem</head><p>In Equations ( <ref type="formula">2</ref>) and (3) it is assumed the edge weights w i, j are given to us. However in the multi-modal setting we describe here it is not the case that there is a single obvious choice. This is a critical decision when building such a system, which can strongly impact the overall accuracy <ref type="bibr" target="#b27">[28]</ref>. In many cases one might heuristically choose some similarity measure, performing some hyperparameter optimization routine to select the number of neighbors to compare to or some ϵ similarity threshold. When one includes ways of selecting or combining various similarity measures, the parameter space can become too large to feasibly handle with a simple grid search -especially when the dataset being operated on becomes large.</p><p>Instead we tackle this problem head on, framing the problem as one of graph design rather than graph selection. The Graph Design problem is as follows: Given:</p><p>• A multi-modal feature space X (as in Subsection 2.1)</p><p>• A partial labeling on this feature space Y • A learning algorithm A which is a function of some graph G having vertex set equal to the elements of X Find: An edge weighting function w(x i , x j ) that allows us to construct a graph G which optimizes the performance of A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GRALE</head><p>Grale is built to efficiently solve this type of problem -designing task specific graphs by reducing the original task to training a classifier on pairs of points. We assume the existence of some oracle y(x i , x j ) which gives a binary valued ground truth, defined for some subset of unordered pairs (x i , x j ). The oracle is chosen to produce a graph amenable to the primary task, A; in the multiclass learning setting described above, the relevant oracle would return true if and only if both points are labeled and share the same class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Pairwise Loss for Graph Design</head><p>We start with the nearest neighbor update rule from Eq. (3) and let the weights be the log of some function of our 'natural' metrics in each mode: w i j := log G(x i , x j ), for some G : R d xR d → R, which only depends on the d modality distances:</p><formula xml:id="formula_4">G(x i , x j ) = f (κ 1 (x i , x j ), κ 2 (x i , x j ), . . . , κ d (x i , x j )).<label>(4</label></formula><p>) Next, we choose a relaxed form of Eq. ( <ref type="formula" target="#formula_3">3</ref>), where we drop the normalization factor and instead impose a constraint w i j &lt; 0 ŷi,c =</p><formula xml:id="formula_5">j ∈N i,c G(x i , x j )<label>(5)</label></formula><p>with N i,c being the set of neighbors of node i with y j,c = 1.</p><p>Removing the normalization factor greatly simplifies the optimization problem. Applied to G, our new constraint is transformed to requiring that 0 &lt; G &lt; 1. This can be achieved simply by choosing G to belong to some bounded family of functions (such as applying a sigmoid transform to the output of a neural network). Note also that since each κ is a metric and thus symmetric in its arguments this also gives us that w i j = w ji , ensuring that a nearest neighbor graph we build with this edge weighting function will be undirected. Putting this back into the full multi-class cross-entropy loss on the node labels Eq. (1), we recover the loss of a somewhat simpler problem to solve: the binary classification problem of deciding whether two nodes are both members of the same class.</p><formula xml:id="formula_6">L = − c ∈ C i ∈X j ∈X y i,c , y j,c log G(x i , x j ).<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Locality Sensitive Hashing</head><p>A key requirement for Grale is that it must scale to datasets containing billions of nodes, making an all-pairs search infeasible. Instead we rely on approximate similarity search using locality sensitive hashing (LSH). Generally speaking LSH works by selecting a family of hash functions H with the property that two points which hash to the same value are likely to be 'close'. Then one compares all such points which share a hash value. In our case, however, the notion of similarity is the output of a model. So we don't have an obvious choice of LSH function.</p><p>Here we analyze two different ways to construct such a LSH function for our input distances: AND-amplification and OR-amplification.</p><p>Recall that above we assume G(x i , x j ) is purely a function of distances between x i and x j in D subspaces. Let us define a metric µ on X × X as a sum over our individual metrics κ d :</p><formula xml:id="formula_7">µ (x 1 , x 2 ), (x 3 , x 4 ) = d κ d (x 1 , x 3 ) + κ d (x 2 , x 4 ).<label>(7)</label></formula><p>Let us also assume that our similarity model G is Lipschitz continuous on X × X, namely that there exists some K ∈ R such that:</p><formula xml:id="formula_8">|G(x i , x j ) − G(x k , x l )| ≤ K µ (x i , x j ), (x k , x l ) . (8) This implies that |G(x i , x i ) − G(x i , x j )| ≤ K d κ d (x i , x j ).<label>(9)</label></formula><p>Next, let ∆(x i , x j ) = 1 − G(x i , x j ). We note that ∆(x i , x j ) is not precisely a metric, but it is bounded to the unit interval. Applying this to Eq. ( <ref type="formula" target="#formula_8">9</ref>) yields:</p><formula xml:id="formula_9">|∆(x i , x i ) − ∆(x i , x j )| ≤ K d κ d (x i , x j ).<label>(10)</label></formula><p>In practice ∆(x i , x i ) is quite small (a point always has the same label as itself!), but a learned model may not evaluate to exactly 0. Let ϵ = sup x ∈X ∆(x, x), either ∆(x i , x j ) &lt; ϵ or we have</p><formula xml:id="formula_10">∆(x i , x j ) − ϵ ≤ ∆(x i , x j ) − ∆(x i , x i ) ≤ K d κ d (x i , x j ).<label>(11)</label></formula><p>In either case,</p><formula xml:id="formula_11">∆(x i , x j ) ≤ K d κ d (x i , x j ) + ϵ.<label>(12)</label></formula><p>Next let H n be an (r, cr, p, q) sensitive family of locality sensitive hash functions for κ n . That is, two points x i and x j with κ n (x i , x j ) ≤ r have probability at least p of sharing the same hash value for some hash function h randomly chosen from H n . Further two points x i and x j with κ n (x i , x j ) ≥ cr have probability of at most q of sharing the same hash value for a similarly chosen h.</p><p>We construct a new 'AND' family H ⊗ of hash functions by concatenating hash functions from each of the H n , with each being (r, c n r , p n , q n ) sensitive for the associated space. Then H ⊗ is (r ⊗ , cr ⊗ , p ⊗ , q ⊗ )-sensitive for G, where</p><formula xml:id="formula_12">r ⊗ = KrD + Dϵ p ⊗ = d p d q ⊗ = d q d .<label>(13)</label></formula><p>Alternatively we may also employ an 'OR'-style construction of an LSH function, taking H ⊕ as the union of the H n . If we assume that there exists some correlation between the subspaces of X such that two points having κ m (x i , x j ) ≤ r also have κ n (x i , x j ) ≤ r with probability p mn (and likewise for distance ≥ cr with probability q mn ) then H m acts as a locality sensitive hash function for</p><formula xml:id="formula_13">H n . Under this assumption H ⊕ is (r ⊕ , cr ⊕ , p ⊕ , q ⊕ )-sensitive for G, where r ⊕ = KrD + Dϵ p ⊕ = min d 1 − (1 − p d ) l d (1 − p dl ) q ⊕ = max d 1 − (1 − q d ) l d (1 − q dl ) .<label>(14)</label></formula><p>In practice, we employ a mixture of the AND and OR forms of locality sensitive hash functions when applying Grale, tuning choices of functions and hyper-parameters by grid search (see Table <ref type="table">2</ref> for an evaluation of the performance on a real world dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph building algorithm</head><p>Our graph building algorithm works by employing the LSH function constructed for our similarity measure to produce candidate pairs, then scoring the candidate pairs with the model. More explicitly, we start by sampling S hash functions from our chosen family H s (constructed from the set of LSH families for our features). Then for each point p i ∈ X we compute a set of hashes, bucketing by hash value. Finally, we find it practical to cap the size of each bucket (typically this is chosen to be 100), randomly subdividing any bucket which is larger. Without this step in the worse case we could still be computing all O(N 2 ) comparisons. An additional practical step we take in some cases is to simply drop buckets which are too large, since this is usually indicative of hashing on a 'noise' feature and this buckets tend not to contain many desirable pairs, and often lead to false positives from the model. For each pair in our subdivided buckets we compare all pairs with our scoring function G (learned using the training procedure described in the next section). Finally we output the top k values (potentially subject to some ϵ threshold on the scores).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training algorithm for Grale</head><p>During the learning phase, we follow the same algorithm described in the previous section, but instead of applying the model we instead consult the user supplied oracle y(p i , p j ). Of course, the oracle will </p><formula xml:id="formula_14">w i j = ŷ(p i , p j ) ; if w i j &gt; ϵ then Emit edge (p i , p j , w i j ) ; end end end return G = ∪ edges</formula><p>Algorithm 2: Graph Building Algorithm not be able to supply a value for every pair -if so there would be no need for a model! In the cases where the oracle does have some ground truth we save the pairwise feature vector δ i j and the value returned by the oracle, to generate training and hold-out sets ∆ 1 , ∆ 2 . This algorithm is detailed in the Appendix.</p><p>We then train the model on the saved pairs, holding out some of the pairs for model evaluation. Note: it is important to perform the holdout by splitting the set of points rather than pairs so that data from the same point does not appear in both the training set and holdout set to avoid over-fitting. Our model is optimized for logloss, namely we seek to minimize the following quantity, with ŷij being the confidence score of our model (described in section 3.5):</p><formula xml:id="formula_15">L = − 1 |∆| i, j ∈∆ y i, j log ŷ i j + (1 − y i j ) log (1 − ŷij ).<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Jointly learned node embeddings</head><p>So far we have focused on the setting where we only use per-mode distance scores as features, but this approach can be extended to also jointly learn embeddings of the nodes themselves. In the extension we divide the model into two components: the node embedding component and the similarity learning component. The distances input to the similarity learning component are augmented with per-dimension distances from the embeddings. By training the embedding model with the similarity model, we can backpropagate the similarity loss to the embedding layers obtaining an embedding tailored for the task.</p><p>Augmenting the model in this way allows us to employ 'twotower' style networks <ref type="bibr" target="#b3">[4]</ref> to learn similarity for features where that technique works well (for example image data <ref type="bibr" target="#b12">[13]</ref>), but use pure distances for sparser features where the embedding approach does not work as well. In some cases we may even rely entirely on distances derived from the embeddings (as is the case for some of the experiments in Section 4). We note that the LSH family used above can still be applied in this setting assuming that the embedding function is Lipschitz (following a similar argument as in Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Model Structure</head><p>The structure of the model used by Grale can in general vary from application to application. In practice we have used Grale with both neural networks and gradient boosted decision tree models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. Here we will describe the neural network approach in more detail.</p><p>The architecture of the neural net that we use is given in Figure <ref type="figure">2</ref>. The inputs to the neural net are split into two types: pairwise features which are functions of the features of both nodes (δ i j above), and singleton features -features of each node by itself. Examples of pairwise features might include the Jaccard similarity of tokens associated with two nodes. We use a standard 'two-tower' architecture for the individual node features, sharing weights between each tower, to build an embedding of each node. These embeddings are multiplied pointwise and concatenated with the vector of distance features κ 1 (x i , x j ), . . . , κ d (x i , x j ) to generate a combined pairwise set of features. We choose a pairwise multiplication here because it guarantees that the model is symmetric in its arguments (i.e. ŷij = ŷji ). Note that since the rest of the model takes distances as inputs, it is symmetric by construction.</p><p>Note that here we have extended G to be a function of not just the original distances but also of the input points themselves. However, if we consider the embeddings learned as an augmentation of the data itself, then we are back to the case of a pure function of distances, where we have also included coordinate-wise distances in the embedded space. We can also argue that the locality sensitive hash functions for distances in the un-embedded versions of these spaces cover as LSH functions in the embedded space due to the correlation between the two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Complexity</head><p>Without the LSH process, the complexity of graph building on a set of N points would be O(N 2 ) (since we compare each point to every other). However instead, employing LSH we compute S hashes per point. Grouping points by hash value is done by sorting, add a factor of O(SN log N ) To cap the amount of work done per bucket, we further subdivide these buckets into smaller subbuckets of maximum size B (100 is a typical value). Only then do we compare all points within the same subbucket, giving us a complexity of O(SN B) for this step. Putting this together and doing a fixed amount of work per comparison, giving us a final complexity of O(SN (B + log N )) work to score all of the edges.</p><formula xml:id="formula_16">x i,1</formula><p>x i,2 . . .</p><formula xml:id="formula_17">x i,d x j,1 x j,2 . . . x j,d κ 1 κ 2 . . . κ d</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding Towers</head><p>Hadamard Product</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concatenated Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hidden layers</head><p>Output Similarity (G)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input distances</head><p>Point i Point j</p><p>Figure <ref type="figure">2</ref>: The Grale Neural Network model. The gray nodes are inputs, the blue are hidden layers, and red is the output. The network architecture combines a standard two-tower model with natural distances in the input feature spaces. Weights are shared between the two towers. The Hadamard product (pointwise multiplication) of the two towers is used to give us a pairwise embedding. We treat this as an additional set of distance features to augment the input distance features κ 1 (x i , x j ), . . . , κ d (x i , x j ). This combined set acts as an input to the second part of the model which computes a single similarity score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION ON PUBLIC DATASETS 4.1 Experimental Design</head><p>There are many papers describing improved techniques for semisupervised learning. We focus on <ref type="bibr" target="#b28">[29]</ref> as a comparison point since it also describes an approach to learning a task specific similarity measure, called PG-Learn. PG-Learn learns a set of per dimension bandwidths for a generalized version of the RBF kernel</p><formula xml:id="formula_18">K(x i , x j ) = exp − m (x im − x jm ) 2 σ m . (<label>16</label></formula><formula xml:id="formula_19">)</formula><p>Rather than directly optimizing this as we do in this paper, PG-Learn sets out to optimize it as a similarity kernel for the Local and Global Consistency (LGC) algorithm by Zhou et al <ref type="bibr" target="#b32">[33]</ref>. We compare on 2 of the same datasets used by <ref type="bibr" target="#b28">[29]</ref>. USPS<ref type="foot" target="#foot_0">1</ref> is a handwritten digit set, scanned from envelopes by the U.S. postal service and represented as numeric pixel values. MNIST<ref type="foot" target="#foot_1">2</ref> is another popular handwritten digit dataset, where the images have been size-normalized and centered. Both tasks use a small sample of the labels to learn from, to more realistically simulate the SSL setting. We list the size, dimension, and number of classes for these datasets in the appendix.</p><p>To build a similarity model with Grale, we select an oracle function that, for a pair of training nodes, the edge weight between them is 1 if they are in the same class and 0 otherwise. Once trained, we use this model to compute an edge weight for every pair of vertices. Since the number of nodes in these datasets is small compared to the number of nodes in a typical Grale application, we computed the edge weights for all possible pairs of nodes; we did not use locality sensitive hashing.</p><p>We then use the graph as input to Equation 2 and label nodes by computing a score for each class as weighted nearest neighbors score, weighting by the log of the similarity scores. We choose the class with the highest score as the predicted label. That is,</p><formula xml:id="formula_20">max c ∈ C exp j ∈N c (i) log G i j ,<label>(17)</label></formula><p>where N c (i) is the set of nodes in the neighborhood of node i that appear in our training set with class c, and G i j is our model's similarity score for the pair. Many of these other approaches also build a k-NN graph, with distance defined by a kernel:</p><p>In Table <ref type="table" target="#tab_1">1</ref>, we list the results of using Grale for these classification tasks alongside the results from <ref type="bibr" target="#b28">[29]</ref>, which include PG-Learn, alongside some other approaches. • AEW: Adaptive Edge Weighting by Karasuyama et al. that estimates the bandwidths through local linear reconstruction <ref type="bibr" target="#b10">[11]</ref>.</p><p>For every result in this table, 10% of the data was used for training. For all results except Grale, there was 15 minutes of automatic hyperparameter tuning. For Grale, we manually tested different edge weight models for each dataset. For MNIST we used a twotower DNN with two fully connected hidden layers. For USPS we used CNN towers.</p><p>In Table <ref type="table" target="#tab_1">1</ref>  other methods. In this case, the number of labels is so small that we found it quite easy to overfit in similarity model training. It is likely that in this case, all methods are learning roughly the same thing. This is especially true given how competitive Grid is, where the similarity has a single free parameter. On the other hand, MNIST provides quite a bit more to work with, so Grale is able to learn quite a bit and strongly outperforms the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CASE STUDY: YOUTUBE ABUSE DETECTION</head><p>The comparisons in Section 4 give some sense for how Grale performs; however, the datasets considered are much smaller than what Grale was designed to work with. They are also single mode problems, where the features belong to a single dense feature space.</p><p>We provide a case study here on how Grale was applied to abuse on YouTube, which is much more on the scale of problem that Grale was built to solve. YouTube is a video sharing platform with hundreds of millions of active users <ref type="bibr" target="#b30">[31]</ref>. Its popularity makes it a common target <ref type="bibr" target="#b9">[10]</ref> for organizations seeking to post to spam <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b15">16]</ref>. Between April 2019 and June 2019, YouTube removed 4M channels for violating its Community Guidelines <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>We trained Grale with ground truth given by Equation <ref type="formula">18</ref>, treating items as either belong to the class of abusive items (removed for Community Guidelines violations) or the class of safe (active, nonabusive) items. We also ignore the connections between safe (nonabusive) pairs during training since these are not important for propagating abusive labels on the graph.</p><formula xml:id="formula_21">y i j =</formula><p>1 if items i and j are abusive 0 otherwise <ref type="bibr" target="#b17">(18)</ref> Experiments were done using both the neural network and treebased formulations of Grale, but we found better performance with the tree version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">LSH Evaluation.</head><p>In our many applications of Grale, we have observed that the choice of locality sensitive hash functions is critical to the overall performance of the system. There is some trade-off between p, the floor on the likelihood of producing close pairs, and q, the ceiling of the likelihood of producing far pairs. Having too large q results in graph building times that are too long to be useful, resulting in too much delay before items could be taken down. Alternatively, if p is too small we miss too many connections, hurting the overall recall of the entire system.</p><p>Here we consider two different thresholds in evaluating our LSH scheme. The first (r in Section 3.2) is a threshold chosen such that a connection at that strength is strongly indicative of an abusive item. The second ('cr ') is a more moderate threshold, chosen such that the connections are still interesting and useful when combined with some other information in determining if an item is abusive. % strong ties (p) % weak ties (q) baseline (random pairs) 0.0653% 77.4% tuned LSH 52.3% 22.7% Table <ref type="table">2</ref>: A comparison of the LSH function used for YouTube and a naive baseline. The parameters p and q are the same as defined in Section 3.2: % of pairs returned by LSH with distance less than r where r is chosen to be a high precision decision threshold of the model and % of pairs returned by LSH with distance further than a moderate precision decision threshold, respectively. degree 0 degree &gt;0 high-precision degree &gt;0 safe (100%) 87.80% 12.20% 2.32% abusive (100%) 51.87% 48.13% 36.96% Table <ref type="table">3</ref>: Percentage of items safe and abusive that have a node in the graph, considering only high-precision edges. Abusive nodes are 6-16x more likely to be connected in the graph.</p><p>The final numbers we arrived at are given in Table <ref type="table">2</ref>. In order to find the same number of high weight edges using random sampling, we would need to evaluate 10.5x more edges than we currently do (note this is assuming sampling without replacement, whereas LSH finds duplicates).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Graph Analysis</head><p>After finding a suitable hashing scheme and model for the dataset, we can materialize the graph. Here we provide a quantitative analysis of the global properties of the graph. In Section 5.4, we visualize the graph for additional insights.</p><p>We set a threshold on edge weights to guarantee a very high level of precision. This means that many nodes end up without any neighbors in the graph. Table <ref type="table">3</ref> shows that after pruning 0degree nodes (i.e. those without a very high confidence neighbor) the graph covers 36.96% of the abusive nodes, but only 2.31% of the safe nodes.</p><p>Figure <ref type="figure" target="#fig_3">3a</ref> shows the distribution of node degrees, separated into safe and abusive items. Abusive nodes generally have more neighbors than safe nodes. However simply having related items doesn't make an item abusive. Safe nodes may have other safe neighbors for legitimate reasons. So it is important to take into account more than just the blind structure of a node's neighborhood.</p><p>Similarly, Figure <ref type="figure" target="#fig_3">3b</ref> shows the degree of safe and abusive items, but only when the neighbors are restricted to abusive nodes. Safe nodes can have abusive neighbors if the edge evaluation model is imprecise or the labels are noisy. However there is a clear drop-off in the frequency of high-degree safe nodes from Figure <ref type="figure" target="#fig_3">3a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Classification Results</head><p>Next, we performed a graph classification experiment using Grale. To show the utility of graphs designed by our method, we built and evaluated a simple single nearest neighbor classifier. <ref type="foot" target="#foot_2">3</ref> We selected 25% of the abusive nodes as seeds, and for every other node in the graph we assigned a score equal to the maximum weight of  In 3a, the total degree distribution for nodes, broken down by abusive status. In 3b, the number of abusive neighbors for safe (green) and abusive (red) nodes. We see that safe nodes have far fewer abusive neighbors on average. the edges connecting the node to a seed. Also, to simulate more real-world conditions we split our training and evaluation sets by time, training the model on older data but evaluating on newer data. This gives us a more accurate assessment of how the system would function after deployment, since we will always be operating on data generated after the model was trained. The precision-recall curve of this classifier can be observed in Figure <ref type="figure" target="#fig_4">4</ref>.</p><p>We compared Grale+Label Propagation, content classifiers, and some heuristics against a subset of YouTube abusive items. Content classifiers were trained using the same training data to predict abusive/safe. The results in Table <ref type="table" target="#tab_2">4</ref> show that Grale was responsible for half of the recall. While content classifiers were given the first chance to detect an item (for technical reasons), Figure <ref type="figure" target="#fig_6">5</ref> shows that content classifiers miss a significant part of recall when faced with new trends which the classifiers have not been trained on. With Grale, the labels can be propagated to older items as well without requiring new model retraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Graph Visualization</head><p>Finally, to give a sense of how well the scope and variety of structure present in graphs designed by Grale, here we provide two kinds of visualizations of the local and global structure of the YouTube graph.  For instance in Figure <ref type="figure" target="#fig_8">5a</ref>, we see that clear community structure is visible -there is one tight cluster of abusive nodes (marked in red) and one tight cluster of non-abusive (in green). We note how this figure (along with Fig. <ref type="figure" target="#fig_8">5e</ref>) illustrates the earlier point that existing in a dense neighborhood in the graph, while potentially suspicious, is not necessarily an indicator of abuse. Simultaneously, we can see that the resulting graph can contain components with relatively small or large diameter (Fig. <ref type="figure" target="#fig_8">5c</ref> and Fig. <ref type="figure" target="#fig_8">5d respectively</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Global structure.</head><p>In order to visualize the global structure of the YouTube case study, we trained a random walk embedding <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b0">1]</ref> on the YouTube graph learned through Grale. Figure <ref type="figure" target="#fig_9">6</ref> shows a two-dimensional t-SNE <ref type="bibr" target="#b14">[15]</ref> projection of this embedding. We observe that, in addition to the rich local structure highlighted above, the Grale graph contains interesting global structure. In order to understand the global structure of the graph better, we performed a manual inspection of dense regions of the embedding space, and found they corresponded to known abusive behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>We begin by noting that the literature on label inference is vastly larger than that of graph construction. In this section, we briefly   discuss the relevant literature in graph construction, which we divide into unsupervised and supervised approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Unsupervised Graph Construction</head><p>Perhaps the most natural form of graph construction is to take nearest neighbors in the underlying metric space. The two straightforward approaches to generate the neighborhood for each data item v either connect it to the k-nearest data items, or to all neighbors within an ϵ-ball. While such proximity graphs have some desirable properties, such methods have difficulty adapting to multimodal data.</p><p>For large datasets, the naive approach of comparing each point to every other point to find nearest neighbors becomes intractably slow. Locality sensitive hashing is a classic technique for doing approximate nearest neighbor search in metric spaces, speeding up the process significantly. The simplest version of this algorithm would be to compare all pairs which share a hash value. However this still has worst-case complexity of O(N 2 ) (N being the number of points in question). Zhang et al <ref type="bibr" target="#b31">[32]</ref> propose a modification of this algorithm which reduces the complexity to O(N log N ) by sorting points by hash value and executing a brute force search within fixed sized windows in this ordered list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Supervised Graph Construction</head><p>A second branch of the literature seeks to use training labels to learn similarity scores between a set of nodes <ref type="bibr" target="#b1">[2]</ref>. Motivated by tasks like item recommendation <ref type="bibr" target="#b7">[8]</ref>, these methods aim to model the relationship between data items <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9]</ref>, connecting new nodes or even removing incorrect edges from an existing graph <ref type="bibr" target="#b20">[21]</ref>. While Grale could be thought of as a link prediction system, our motivations are substantially different. Instead of finding new connections between data items, we seek to accurately characterize the many possible interactions into the most useful form of similarity for a given problem domain. Other work focuses on modeling the joint distribution between the data items and their label for better semi-supervised learning <ref type="bibr" target="#b11">[12]</ref>. Salimans et al. <ref type="bibr" target="#b25">[26]</ref> use a generative adversarial network (GAN) to model this distribution. These approaches typically have good performance on small-scale SSL tasks, but are not scalable enough for the size or scale of real datasets, which may contain millions of distinct points. Unlike this work, Grale is designed to be scalable and therefore utilizes a conditional distribution between the data and labels.</p><p>Perhaps the most relevant related work comes from Wu et al. <ref type="bibr" target="#b28">[29]</ref>, who propose a kernel for graph construction which reweights the columns of the feature matrix. Unlike this approach, Grale is capable of modeling arbitrary transformations over the feature space, operating on complex multi-modal features (like video and audio), and running on billions of data items. Similarly, work on clustering attributed graphs have explored attributed subspaces <ref type="bibr" target="#b16">[17]</ref> as a form of graph similarity, for example, using distance metric learning to learn edge weights <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper we have demonstrated Grale, a system for learning task specific similarity functions and building the associated graphs in multi-modal settings with limited data. Grale scales up to handling a graph with billions of nodes thanks to the use of locality sensitive hashing, greatly reducing the number of pairs that need to be considered.</p><p>We have also demonstrated the performance of Grale in two ways. First, on smaller academic datasets, we have shown that it is capable of producing competitive results in settings with limited amounts of labeling. Second, we have demonstrated the capability and effectiveness of Grale on a real YouTube dataset with hundreds of millions of data items.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>•</head><label></label><figDesc>Grid search (GS): k-NN graph with RBF kernel where k and bandwidth σ are chosen via grid search, • Rand d search (RS): k-NN with the generalized RBF kernel where k and different bandwidths are randomly chosen, • MinEnt: Minimum Entropy based tuning of the bandwidths as proposed by Zhu et al [34],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Node degree distribution in Grale YouTube. (b) Abusive-degree distribution in Grale YouTube.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Degree distributions in the learned YouTube graph.In 3a, the total degree distribution for nodes, broken down by abusive status. In 3b, the number of abusive neighbors for safe (green) and abusive (red) nodes. We see that safe nodes have far fewer abusive neighbors on average.</figDesc><graphic url="image-3.png" coords="7,85.65,404.69,176.54,122.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Precision-recall curve of single nearest-neighbor classification on our graph. We selected 25% of the abusive items as node seeds, and propagated the label to neighboring nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>5. 4 . 1</head><label>41</label><figDesc>Local structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5</head><label>5</label><figDesc>shows visualization of several subgraphs extracted from the YouTube graph. The resulting visualizations illustrate the variety of different structure present in the data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) Intermixed dense clusters. (b) Sparse abusive cluster. (c) Dense abusive cluster. (d) Sparse non-abusive subgraph. (e) Dense non-abusive clusters. (f) Small abusive clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Different subgraphs extracted from the YouTube case study. Here we show a small sample of the rich patterns Grale is able to capture when applied to real data. Colors correspond to the abuse status of the nodes.</figDesc><graphic url="image-10.png" coords="8,2.97,293.47,321.23,312.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of global structure. In order to visualize the global structure of the YouTube case study, we learned a random walk embedding [20, 1] of the highest weight edges in the graph. Here we show a t-SNE projection of a sample of these embeddings. Clusters correspond to known abusive behavior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>we see that while Grale does not always give the best performance it produces results that are fairly similar to several Test Accuracy of Various Methods</figDesc><table><row><cell cols="6">Dataset Grale PGLrn MinEnt AEW Grid Rand d</cell></row><row><cell>USPS</cell><cell>0.892</cell><cell>0.906</cell><cell>0.908</cell><cell>0.895 0.873</cell><cell>0.816</cell></row><row><cell cols="2">MNIST 0.927</cell><cell>0.824</cell><cell>0.816</cell><cell>0.782 0.755</cell><cell>0.732</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Abuse identified from different techniques on a subset of YouTube abusive items. Adding Grale to the existing system increased recall by 89%.</figDesc><table><row><cell>Algorithm</cell><cell></cell><cell>% of items</cell></row><row><cell>Content Classifiers</cell><cell></cell><cell>47.7%</cell></row><row><cell>Heuristics</cell><cell></cell><cell>5.3%</cell></row><row><cell cols="2">Total (Heuristics + Content Classifiers)</cell><cell>53%</cell></row><row><cell cols="2">Grale+Label Propagation</cell><cell>47.0% (+89%)</cell></row><row><cell>Algorithm</cell><cell cols="2">New items Old items</cell></row><row><cell>Grale+Label Propagation</cell><cell>25.8%</cell><cell>74.2%</cell></row><row><cell>Content Classifiers</cell><cell>71.6%</cell><cell>28.4%</cell></row><row><cell>Heuristics</cell><cell>33.7%</cell><cell>66.3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Breakdown by item age at the time of classification as abusive across various methods. 'Grale + Label Propagation is able to identify additional items initially missed by content classifiers.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">http://www.cs.huji.ac.il/~shais/datasets/ClassificationDatasets.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">http://yann.lecun.com/exdb/mnist/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">We note that in practice, a more complex system is employed, but the details of this are beyond the scope of this paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank Warren Schudy, Vahab Mirrokni, Natalia Ponomareva, Peter Englert, Filipe Miguel Gonçalves de Almeida, and Anton Tsitsulin.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX Training Procedure</head><p>Algorithm 3 shows how the sketching function is used in the train/test split.</p><p>Input : A set of points X and oracle function Y Output : A similarity model on approximating Y on X Split X into training and hold-out sets, X t r ain , X t est ; Buckets = NNSketching(X) ; </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning edge representations via low-rank asymmetric projections</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CIKM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Link prediction using supervised learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Al</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chaoji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">SDM Workshops</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data using graph mincuts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Signature verification using a&quot; siamese&quot; time delay neural</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<idno>network. NIPS</idno>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Chami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03675</idno>
		<title level="m">Machine learning on graphs: a model and comprehensive taxonomy</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.02590</idno>
		<title level="m">A tutorial on network embeddings</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Enhanced network embeddings via exploiting edge labels</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CIKM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Link prediction approach to collaborative filtering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>JCDL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A survey on network embedding</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>TKDE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Show me the money: characterizing spam-advertised revenue</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kanich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>SEC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive edge weighting for graphbased learning algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karasuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mamitsuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>ICML Workshops</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Click trajectories: end-to-end analysis of the spam value chain</title>
		<author>
			<persName><forename type="first">K</forename><surname>Levchenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>S&amp;P</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pharmaleaks: understanding the business of online pharmaceutical affiliate programs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mccoy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>SEC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Evaluating clustering in subspace projections of high dimensional data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Assent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seidl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>VLDB</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On potts model clustering, kernel k-means and density estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Murua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Stanberry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Stuetzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Focused clustering and outlier detection in large attributed graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Iglesias</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>KDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deepwalk: online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>KDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">When recommendation goes wrong: anomalous link discovery in recommendation networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schueppert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saalweachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thakur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>KDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Tf boosted trees: a scalable tensorflow based framework for gradient boosting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ponomareva</surname></persName>
		</author>
		<editor>ECML/PKDD. Y. Altun et al.</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Compact multi-class boosted trees</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ponomareva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Big Data</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large scale distributed semi-supervised learning using streaming approximation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Diao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="519" to="528" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">T</forename></persName>
		</author>
		<ptr target="https://transparencyreport.google.com/youtube-policy/removals" />
		<imprint/>
	</monogr>
	<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improved techniques for training gans</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The partnerka-what is it, and why should you care</title>
		<author>
			<persName><forename type="first">D</forename><surname>Samosseiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Virus Bulletin Conference</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Influence of graph construction on semi-supervised learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A R</forename><surname>De Sousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Batista</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A quest for structure: jointly learning the graph structure and semi-supervised classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CIKM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><surname>Youtube</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/intl/en-GB/about/press/.(" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fast knn graph construction with locality sensitive hashing</title>
		<author>
			<persName><forename type="first">Y.-M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ECML/PKKD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
