<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable Attack on Graph Data by Injecting Vicious Nodes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-04-22">22 Apr 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jihong</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fnu</forename><surname>Suya</surname></persName>
							<email>suya@virginia.edu</email>
						</author>
						<author>
							<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
							<email>jundong@virginia.edu</email>
						</author>
						<author>
							<persName><forename type="first">Zijiang</forename><surname>Yang</surname></persName>
							<email>zijiang.yang@wmich.edu</email>
						</author>
						<author>
							<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Xian Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Virginia</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Western Michigan University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scalable Attack on Graph Data by Injecting Vicious Nodes</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-04-22">22 Apr 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2004.13825v1[cs.CR]</idno>
					<note type="submission">Received: date / Accepted: date</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies have shown that graph convolution networks (GCNs) are vulnerable to carefully designed attacks, which aim to cause misclassification of a specific node on the graph with unnoticeable perturbations. However, a vast majority of existing works cannot handle large-scale graphs because of their high time complexity. Additionally, existing works mainly focus on manipulating existing nodes on the graph, while in practice, attackers usually do not have the privilege to modify information of existing nodes. In this paper, we develop a more scalable framework named Approximate Fast Gradient Sign Method (AFGSM) which considers a more practical attack scenario where adversaries can only inject new vicious nodes to the graph while having no control over the original graph. Methodologically, we provide an approximation strategy to linearize the model we attack and then derive an approximate closed-from solution with a lower time cost. To have a fair comparison with existing attack methods that manipulate the original graph, we adapt them to the new attack scenario by injecting vicious nodes. Empirical experimental results show that our proposed attack method can significantly reduce the classification accuracy of GCNs and is much faster than existing methods without jeopardizing the attack performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphs are widely used to model various types of real-world data and many canonical learning tasks such as classification, clustering, and anomaly detection have been widely investigated for the graph-structured data <ref type="bibr" target="#b2">(Bhagat et al., 2011;</ref><ref type="bibr" target="#b28">Tian et al., 2014;</ref><ref type="bibr" target="#b21">Perozzi et al., 2014a;</ref><ref type="bibr" target="#b27">Tang et al., 2016)</ref>. In this paper, we focus on the task of node classification. Recently, to solve the node classification problem, graph convolution networks (GCNs) have gained a surge of research interests in the data mining and machine learning community because of their superior prediction performance <ref type="bibr" target="#b23">(Pham et al., 2017;</ref><ref type="bibr" target="#b6">Cai et al., 2018;</ref><ref type="bibr" target="#b20">Monti et al., 2017)</ref>. However, recent research efforts showed that various graph mining algorithms (e.g., GCNs) are vulnerable to carefully crafted adversarial examples, which are "unnoticeable" to humans but can cause the learning models to misclassify some target nodes <ref type="bibr" target="#b34">(Zügner et al., 2018;</ref><ref type="bibr" target="#b12">Dai et al., 2018;</ref><ref type="bibr" target="#b4">Bojchevski and Günnemann, 2018)</ref>. The vulnerabilities of these learning algorithms can lead to severe consequences in security-sensitive applications. For example, GCNs are often used in the risk management area to evaluate the credit level of users <ref type="bibr" target="#b12">(Dai et al., 2018;</ref><ref type="bibr" target="#b0">Akoglu et al., 2015;</ref><ref type="bibr" target="#b5">Bolton et al., 2001)</ref>, as user-user information is often used in this context, it provides ample opportunities for criminals to increase their credit score by connecting to high-credit users. In this paper, we focus on assessing the robustness of graph convolution networks against adversarial attacks. Different from existing efforts that directly manipulate the original graph, we investigate a more realistic attack scenario of injecting vicious nodes and develop a scalable solution to tackle the problem.</p><p>Limitations of Current Approaches. There are two common issues of existing works on attacking GCNs <ref type="bibr" target="#b34">(Zügner et al., 2018;</ref><ref type="bibr" target="#b12">Dai et al., 2018;</ref><ref type="bibr" target="#b3">Bojcheski and Günnemann, 2018)</ref>: the scalability issue and the explicit assumption that adversaries can easily manipulate existing nodes on the graph. First, GCNs are usually applied in large-scale graphs in various domains <ref type="bibr" target="#b30">(Ying et al., 2018;</ref><ref type="bibr" target="#b15">Hamilton et al., 2017;</ref><ref type="bibr" target="#b9">Chen et al., 2018)</ref>, which puts a high demand on the scalability of the underlying attack models. However, existing efforts <ref type="bibr" target="#b34">(Zügner et al., 2018;</ref><ref type="bibr" target="#b12">Dai et al., 2018;</ref><ref type="bibr" target="#b3">Bojcheski and Günnemann, 2018</ref>) cannot be easily generalized to handle large-scale graphs. Second, in actual life, attackers may not be able to manipulate existing nodes in a graph. For example, GCNs are often used to classify users on social websites like Twitter or Weibo for content recommendation by exploring the friendship graph of users. But attackers usually have no ability to manipulate existing users on these websites. To achieve the purpose of attack, a simple way is to register some new accounts on these websites and enable these accounts to establish connections with existing users, e.g., following other users or making comments on the same posts. Despite that, existing attack models seldomly consider this new attack scenario. The aforementioned two limitations motivate us to investigate the following research problem: how to effectively and efficiently manipulate the prediction results of GCNs on a specific node by injecting vicious nodes to a large graph?</p><p>Challenges. There are three challenges in the new attack scenario, where the first two are general challenges for devising efficient attacks on GCNs while the third one is a unique challenge of the new attack scenario we consider.</p><p>-Discreteness. Different from images which can be approximately regarded as a continuous field as the value of pixels can be any integers between 0 and 255, graph-structured data is often depicted in the discrete domains. Thus existing attack <ref type="bibr" target="#b13">(Dong et al., 2018;</ref><ref type="bibr" target="#b26">Szegedy et al., 2013)</ref>strategies that are widely used in other domains (e.g., computer vision) cannot be directly applied. The proposed solution needs to well handle the potential combinatorial problem efficiently. -Poisoning Attack. Unlike the clear separation between training and test data in other domains, the node classification task is often conducted in a Fig. <ref type="figure">1</ref>: Comparison between the attack scenario in existing literature and the new attack scenario considered in this paper.</p><p>transductive setting, where the test data (without ground-truth labels) is also considered in the training phase. Because of this, when the test data is manipulated, the graph is also dynamically updated. Therefore, it is important to propose attack strategies that remain effective after the model is retrained on the manipulated data. This leads to a bi-level optimization problem which is often computationally expensive to solve. -High complexity of Existing methods. We can adapt existing methods to the new attack scenario. However, they all fail to scale to large graphs as their time complexities are very high (the complexity analysis is shown in Section 4.3). Considering that the node classification task is usually conducted on large graphs in practice, it is important to develop an efficient method that can be scaled to real-world large graphs.</p><p>Contributions. With the above-mentioned challenges, we propose a novel Approximate Fast Gradient Sign Method (AFGSM), which can modify and inject vicious nodes efficiently in the new attack setting. Specifically, our contributions can be summarized as follows:</p><p>-A New Attack Scenario. We consider a more practical attack scenario where adversaries can only inject vicious nodes to the graph while the original nodes on the graph remain unchanged. -Adapting Existing Attacks to New Scenario. We adapt and carefully tune the exiting attacks to our new attack scenario and adopt these attacks as the baselines for comparison. -A More Efficient and Effective Algorithm. We propose a new attack strategy named Approximate Fast Gradient Sign Method (AFGSM), which can generate adversarial perturbations much more efficiently than the baseline attacks while maintaining similar attack performance. -Extensive Evaluation. We empirically illustrate the effectiveness of our method on five benchmark datasets and also test on two state-of-the-art graph neural networks and one unsupervised network embedding model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Adversarial examples are extensively studied in the image classification task and recently researchers also show its existence in graph-related problems. Therefore, we will discuss related works in both the image domain and the graph domain.</p><p>Attack on Images. <ref type="bibr" target="#b26">Szegedy et al. (2013)</ref> first demonstrate the vulnerability of deep learning models to adversarial examples using L-BFGS method and attributes the existence of adversarial examples to high non-linearity of deep models. Later on, <ref type="bibr" target="#b14">Goodfellow et al. (2014)</ref> propose an efficient Fast Gradient Sign Method (FGSM) and instead demonstrates that adversarial examples exist because deep learning models are linear in nature. <ref type="bibr" target="#b7">Carlini et al. (Carlini and Wagner, 2017)</ref> propose a stronger C&amp;W attack that breaks heuristic defenses that are effective against adversarial examples generated using L-BFGS method. Madry et al. <ref type="bibr" target="#b18">(Madry et al., 2017)</ref> propose the Projected Gradient Attack (PGD) and successfully break defenses that are effective against FGSM attacks. C&amp;W and PGD attacks are commonly adopted as benchmarks for evaluating the robustness of new defenses as non-certified defenses can be easily evaded by considering some variants of the two attacks <ref type="bibr" target="#b1">(Athalye et al., 2018)</ref>. However, these attacks cannot be applied to our setting because of the discrete nature of graph data.</p><p>Attack on Graphs. Some earlier attacks on graphs focus on modifying the graph structure. <ref type="bibr" target="#b8">Chaoji et al. (2012)</ref> add edges to maximize the content spreading in social network platforms such as Twitter. Researchers also reveal that the shortest path of a graph can be changed by slightly perturbing its structure <ref type="bibr" target="#b16">(Israeli and Wood, 2002;</ref><ref type="bibr">Phillips, 1993)</ref>. <ref type="bibr" target="#b11">Csji et al. (2014)</ref> aim to maximize the PageRank score of a target node in a network with structure manipulation. However, all these attacks are not designed for graph learning algorithms (e.g., graph neural networks or node embedding algorithms such as DeepWalk).</p><p>Recently, researchers also demonstrate the vulnerability of graph learning algorithms. <ref type="bibr" target="#b10">Chen et al. (2017)</ref> inject noise to a bipartite graph that represents DNS queries to mislead the result of graph clustering. However, their attack is generated through manual effort based on attacker's domain knowledge. <ref type="bibr" target="#b32">Zhao et al. (2018)</ref> study the poisoning attacks on multi-task relationship learning, but based on an assumption that the sampled nodes are i.i.d. within each task, which does not hold for the node classification task. <ref type="bibr" target="#b12">Dai et al. (2018)</ref> study evasion attacks on node classification and graph classification problems. However, their perturbation is only limited to the edges of the graph, while attackers can benefit more by additionally manipulating features of nodes (shown in Section 5.4). <ref type="bibr" target="#b34">Zügner et al. (2018)</ref> propose Nettack, which manipulates both edges and features of the graph with a greedy approach. In addition, the authors propose an efficient method to calculate the constraint condition on the perturbations to ensure the generated perturbations are "unnoticeable". <ref type="bibr" target="#b3">Bojcheski and Günnemann (2018)</ref> study poisoning attack on unsupervised node embeddings by borrowing ideas from matrix perturbation theory to maximize the loss of DeepWalk <ref type="bibr" target="#b22">(Perozzi et al., 2014b)</ref> and change the embedding outcome. <ref type="bibr" target="#b25">Sun et al. (2019)</ref> study the new attack scenario by injecting vicious nodes to perturb the graph using a reinforcement learning strategy. However, the complexity of reinforcement learning (Watkins and Dayan, 1992) is pretty high thus cannot scale to large graphs. <ref type="bibr" target="#b33">Zügner and Günnemann (2019)</ref> propose a data poisoning attack named Meta-attack based on meta-learning, which can reduce the overall classification accuracy of the GCN by only perturbing small fraction of the training data.</p><p>Note that, all the aforementioned attacks except <ref type="bibr" target="#b25">(Sun et al., 2019)</ref> on graph learning algorithms focus on changing edges or features of existing nodes and are not designed for injecting vicious nodes to the graph. As shown in Section 5, directly adapting these attacks to the new attack scenario is not promising due to high complexity and we are motivated to devise a new attack strategy tailored for the vicious node setting with a low computational cost.</p><p>3 Problem Definition</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation and Preliminary</head><p>In this section, we first introduce the notations used throughout this paper and preliminaries of GCNs. Here, following the standard notations in the literature <ref type="bibr" target="#b17">(Kipf and Welling, 2016;</ref><ref type="bibr" target="#b32">Zhou et al., 2018;</ref><ref type="bibr" target="#b29">Wu et al., 2019)</ref>, we assume that G = (V, E, F) denotes an undirected attributed network (graph) with n nodes (e.g., v i ∈ V), m edges (e.g., e ij = (v i , v j ) ∈ E), and d attributes (features) (e.g., f i ∈ F ). The features of these n nodes are given by X</p><formula xml:id="formula_0">= [x 1 , x 2 , • • • , x n ] ∈ {0, 1} n×d</formula><p>, where x i ∈ {0, 1} d denotes the feature for the i-th node v i . The adjacency matrix A ∈ {0, 1} n×n contains the information of node connections, where each component A ij denotes whether the edge e ij exists in the graph. Here, we represent the graph as G = (A, X) for simplicity. Note that only a limited number of nodes possess label information in many real-world scenarios and we denote these nodes as V L , where each node v i ∈ V L is affiliated with the label c v ∈ C.</p><p>Following the well-established work on node classification <ref type="bibr" target="#b17">(Kipf and Welling, 2016)</ref>, the probability of classification with GCN is formulated as:</p><formula xml:id="formula_1">Z = f θ (G) = sof tmax Âσ ÂXW (1) W (2) ,<label>(1)</label></formula><p>where Z vc denotes the probability of assigning node v to the class c; 2) collects all the trainable parameters; σ(•) is an activation function (ReLU is used in this paper). In the framework of semi-supervised classification, the optimal parameter θ * is learned by minimizing the cross-entropy loss on the labeled nodes V L , i.e.,</p><formula xml:id="formula_2">Â = D− 1 2 Ã D− 1 2 is calculated by Ã = A + I and the diagonal matrix D with diagonal element Dii = j Ãij for i = 1, 2, • • • , n; θ = W (1) , W<label>(</label></formula><formula xml:id="formula_3">min θ L train (f θ (G)) = − v∈V L lnZ vc v .</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem Definition and Methodology</head><p>Different from previous works <ref type="bibr" target="#b12">(Dai et al., 2018;</ref><ref type="bibr" target="#b34">Zügner et al., 2018;</ref><ref type="bibr" target="#b33">Zügner and Günnemann, 2019)</ref>, in this paper, we consider a new scenario: attacking a specific target node v 0 ∈ V to change its prediction by injecting n in vicious nodes that are not on the original graph, denoted by</p><formula xml:id="formula_4">V in with |V in | = n in and V in ∩ V = ∅.</formula><p>Formally, let G = A , X be the new graph after performing small perturbations on the original graph G, then we have</p><formula xml:id="formula_5">A = A E E O , X = X X in .<label>(3)</label></formula><p>Here E ∈ {0, 1} n×n in denotes the relationship matrix between original nodes and vicious nodes. Specifically, E ij = 1 if the original node v i ∈ V is connected to the vicious node v j ∈ V in , and E ij = 0 otherwise. Symmetric matrix O ∈ {0, 1} n in ×n in represents the relationships between vicious nodes in V in . The edge information denoted by E and O are called vicious edges in this paper. X in ∈ {0, 1} n in ×d is the feature matrix of vicious nodes. It is noteworthy that the perturbations can only be performed on E, O and X in while A and X remain unchanged.</p><p>Formally, the problem of adversarial attacks on graph G in the new scenario is typically formulated as the following bi-level optimization problem<ref type="foot" target="#foot_0">1</ref> : min</p><formula xml:id="formula_6">{E,O,X in }∈Φ(G ) L atk f θ * G = Z v 0 c v 0 − max c new =c v 0 Z v 0 c new (4) s.t. θ * = arg min θ L train f θ G .</formula><p>where Z = f θ * (G ) denotes the prediction confidence scores for all classes on the perturbated graph G , L atk is the loss function during attack, Φ(G ) is the constraints that E, O and X in should meet on the perturbed graph G . In the inner optimization problem, we get the optimal weights θ * of the model f (i.e., GCN) on the current perturbed graph G and in the outer optimization problem, we get the optimal perturbation(i.e., E, O and X in ) on the current model f θ * . Subsequently, we will introduce the loss function on attack and constraint conditions in details.</p><p>Loss function of attacks. The loss function of attacks L atk aims to find a perturbed graph G that classifies the target node v 0 as c new and has maximal distance to the ground truth label c v 0 in terms of log-probabilities/logits. Thus, the smaller L atk is, the worse the classification performs on the target node v 0 .</p><p>It is noteworthy that the surrogate model proposed in <ref type="bibr" target="#b34">(Zügner et al., 2018)</ref> is typically used to generate perturbations instead of using Eq. ( <ref type="formula" target="#formula_1">1</ref>) such that:</p><formula xml:id="formula_7">Z = f θ (G) = Â2 XW . (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>The simplification is necessary for efficiency since it enables us to derive an approximate optimal solution (shown in section 4). In this sense, the loss function on attack turns to</p><formula xml:id="formula_9">L atk f θ * G = Â 2 X W v 0 c v 0 − max c new =c v 0 Â 2 X W v 0 c new . (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>Constraint conditions. There should be some constraint conditions to ensure unnoticeable perturbations, i.e., the definition of Φ(G ). One of the most widely used constraints for the adversarial attack is 0 -norm constraint. Specifically, the number of vicious edges by a budget ∆ e should be sparse, i.e.,</p><formula xml:id="formula_11">E 0 + 0.5 O 0 ≤ ∆ e ,<label>(7)</label></formula><p>where • 0 denotes the 0 -norm of a matrix (i.e., the number of non-zero elements of a matrix). Additionally, if we inject vicious nodes with strange pairs of features (e.g., mutually exclusive features), it will be easily detected. For this issue, the work in <ref type="bibr" target="#b34">(Zügner et al., 2018)</ref> proposes a statistical test based on the co-occurrence graph of features to decide that a feature is unnoticeable if it occurs together with a node's original features. However, note that there are no original features for vicious nodes in our new scenario. Indeed, we can design the features arbitrarily.</p><p>In this paper, we take a more practical solution by modifying the constraint such that the vicious nodes cannot import co-occurrence pairs of features that do not exist in the original graph. Formally:</p><formula xml:id="formula_12">∃ f i , f j ∈ F , ∃ u in ∈ V in , [X in ] u in ,i = 1 ∧ [X in ] u in ,j = 1 only if ∃u ∈ V, X u,i = 1 ∧ X u,j = 1 (8)</formula><p>This constraint means that if feature i and feature j occur together on the vicious node u in , they must have occurred together in an original node u. In other words, no new co-occurrence pairs of features will be imported on the vicious nodes. Moreover, considering that vicious nodes with too few or too many features may be noticeable, we constrain the 0 -norm of vicious nodes to be equal to the mean of 0 -norm of original nodes, i.e., ∀</p><formula xml:id="formula_13">u in ∈ V in , [X in ] u in • 0 = X 0 /n where [X in ] u in • is the u in -th row of matrix X in (i.e.</formula><p>, the feature vector of vicious node u in ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Proposed Framework</head><p>It is a very challenging problem to solve the proposed optimization problem in Eq. ( <ref type="formula">4</ref>) due to the discreteness of the graph-structured data. In this section, we will first discuss how to adapt existing methods to the new attack scenario: injecting vicious nodes to perturb graphs. Moreover, considering the high computational complexity of existing methods, we propose a novel method (AFGSM) to speed up the computation and hence the developed method is scalable to large-scale graphs. Note that although we only focus on undirected graphs, all algorithms in this paper can be easily generalized to directed graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Adapting Existing Methods for the New Scenario</head><p>In this paper, we consider three methods designed for the traditional scenario, including Nettack <ref type="bibr" target="#b34">(Zügner et al., 2018)</ref>, FGSM <ref type="bibr" target="#b34">(Zügner et al., 2018)</ref> and Metaattack <ref type="bibr" target="#b33">(Zügner and Günnemann, 2019)</ref>. To adapt them to the new attack scenario, we generate vicious nodes under the constraint conditions in Section 3.2 which is designed specifically for the new attack scenario instead of the constraints in <ref type="bibr" target="#b34">(Zügner et al., 2018)</ref>.</p><p>There are two strategies to adapt existing methods to the new scenario. First, we can inject all vicious nodes to the graph, and then consider the vicious nodes as special original ones. We call this strategy as one-time injection. Second, we can inject vicious nodes one by one and once we inject a vicious node, we optimize the corresponding edges and features. We call this strategy sequential injection.</p><p>Nettack for the new scenario. Nettack <ref type="bibr" target="#b34">(Zügner et al., 2018)</ref> addresses the bilevel optimization problem following a greedy strategy. Specifically, we initialize the vicious edges in the graph G with E, O, and set the initial X in by randomly sampling from the original graph G. Then we apply Nettack on the graph G and constrain that perturbations are performed on E, O and X in . In each iteration, Nettack assigns a score for each potential edge that satisfies the constraints and choose the edge with the maximum score to flip. The main cost of Nettack is the calculations of the scores, thus it derives an incremental update method that can get the new scores from the old scores after each iteration in constant time.</p><p>According to the analysis in <ref type="bibr" target="#b34">(Zügner et al., 2018)</ref>, the time complexity of Nettack in terms of n in vicious nodes is</p><formula xml:id="formula_14">O (∆ e • n inj • (n • N ei v 0 + f d))</formula><p>, where N ei v 0 is the number of the one-order and second-order neighbors of the target node v 0 , f is the number of non-zero features in each vicious node's feature vector(i.e., X 0 /n). Specifically, there are ∆ e iterations. In each iteration, n scores for each vicious node should be calculated by considering all non-zero elements in [ Â 2 ] v 0 • (N ei v 0 edges average). And for each vicious node, once we select a feature (f at most) for it, we need to check whether the candidate features (d at most) of the node is co-occurring with it. Note that the strategies we adopt won't affect the computational complexity.</p><p>Meta-attack for the new scenario. Meta-attack <ref type="bibr" target="#b33">(Zügner and Günnemann, 2019)</ref> utilizes the idea of meta-learning to optimize the perturbations generated on graphs. Note that Meta-attack is originally designed to attack the whole graph rather than attacking a specific target node v 0 . To apply Meta-attack to the new attack scenario, we replace the loss function L atk with Eq. ( <ref type="formula" target="#formula_9">6</ref>) and update the meta-gradients by</p><formula xml:id="formula_15">∇ meta G = ∇ G L atk (f θ * (G )) s.t. θ * = opt θ (L train (f θ (G ))) (9)</formula><p>where opt(•) is a differentiable optimization procedure (e.g., gradient descent or its variants). In each iteration, Meta-attack picks the edge and the feature value with the maximum meta-gradient to flip.</p><p>According to <ref type="bibr" target="#b33">(Zügner and Günnemann, 2019)</ref>, the time complexity of Metaattack for the new scenario is O ∆ e T n 2 + n in f d , where T is the number of iterations. For each iteration, the second-order gradient (Hessian matrix) should be calculated with a computational complexity of O n 2 . And for computation of features, it needs O (∆ e n in f d) just like Nettack.</p><p>FGSM for the new scenario. FGSM <ref type="bibr" target="#b34">(Zügner et al., 2018)</ref> computes the gradients of L atk w.r.t. edges and features and then it chooses the edge with the maximum revised gradient (i.e., multiply −1 if the edge exists) to flip and optimize the features with the signs of gradients.</p><p>The time complexity of FGSM is O ∆ e n 2 + n in f d . For each iteration, the computation of gradients needs O n 2 . And like Nettack, the computation of features needs O (∆ e n in f d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Approximate Fast Gradient Sign Method (AFGSM)</head><p>Although existing methods can be adapted to the new attack scenario, their computational complexity is often too high to allow them to scale up in large-scale graphs in practice. To address this problem, we propose an efficient algorithm in this section, named Approximate Fast Gradient Sign Method (AFGSM) which injects vicious nodes one by one (i.e., the sequential injection strategy).</p><p>Specifically, we inject vicious node v in with edge information e in ∈ {0, 1} n and feature vector x in ∈ {0, 1} d to attack the target node v 0 in graph G = (A, X), and denote the new graph by G = (A , X ), where</p><formula xml:id="formula_16">A = A e in e in 0 ∈ {0, 1} (n+1)×(n+1) , X = X x in ∈ {0, 1} (n+1)×d . (<label>10</label></formula><formula xml:id="formula_17">)</formula><p>It is noteworthy that there are two possible values for the v 0 -th component of edge information e in , i.e., [e in ] v 0 = 0 or 1, where [•] k refers to the k-th component of a vector. [e in ] v 0 = 1 indicates that the vicious node v in is allowed to connect to the target node v 0 directly, namely direct attack; otherwise, we call it indirect attack, which is more difficult to be detected by intelligent defense algorithms since the vicious node is not connected to the target node directly.</p><p>Here we assume that in large-scale graphs, the changes of degrees of original nodes can be ignored after injecting only one vicious node, thus we can approximate the self-loop degree matrix D as</p><formula xml:id="formula_18">D ≈ D dv in ∈ R (n+1)×(n+1) ,<label>(11)</label></formula><p>where dv in = d v in + 1; d v in refers to the predefined degree of the vicious node v in (i.e., how many connections it will build with existing nodes). In this sense, the Laplacian matrix of graph G is calculated by n+1) ,</p><formula xml:id="formula_19">Â = D − 1 2 (A + I) D − 1 2 ≈ Â êin ê in d−1 v in ∈ R (n+1)×(</formula><formula xml:id="formula_20">where êin = ( dv in ) − 1 2 D− 1 2 e in .</formula><p>As a result, the probability of classification Z after perturbation is derived as</p><formula xml:id="formula_21">Z = Â 2 X W ≈ Â2 X + êin ê in X + Âê in x in + d−1 v in êin x in ê in ÂX + d−1 v in ê in X + ê in êin x in + d−2 v in x in W .<label>(12)</label></formula><p>Specifically, the probability of classification for the target node v 0 turns to</p><formula xml:id="formula_22">Z v 0 j ≈ [ Â2 XW ] v 0 j + [ê in ]v 0 ê in X[W ] •j + ê in [ Â]•v 0 + d−1 v in [ê in ]v 0 x in [W ] •j (13) for j = 1, 2, • • • , |C|, where [•] i• and [•]</formula><p>•j denote the i-th row and j-th column of a matrix, respectively. [•] ij refers to the i-th row and j-th component in a matrix.</p><p>Based on the approximation above, we observe that Z v 0 j turns out to be linear with feature vector x in ∈ {0, 1} d and edge information e in ∈ {0, 1} n . Since the loss function L atk formulated in Eq. ( <ref type="formula" target="#formula_9">6</ref>) is also linear with Z v 0 j , we can obtain the optimal closed-form solutions of x in and e in which minimize loss function L atk by their gradients.</p><p>An approximate closed-form solution of x in . From Eq. ( <ref type="formula">13</ref>), the output Z is linear with respect to the variable x in . Therefore, a closed-form solution of x in w.r.t. the optimization problem in Eq. ( <ref type="formula">4</ref>) can be obtained as follows</p><formula xml:id="formula_23">x * in = −0.5sign ∂L atk ∂x in + 0.5,<label>(14)</label></formula><p>where sign(•) is the element-wise function that takes the sign of a value. In other words, the features are set to 1 if the corresponding gradients in ∂L atk ∂x in are negative and 0 otherwise. The gradients of loss function L atk w.r.t. the variable x in can be calculated as follows</p><formula xml:id="formula_24">∂L atk ∂x in ≈ d−1 v in e in D− 1 2 [ Â] •v 0 + d−1 v in d− 1 2 v 0 [e in ] v 0 [W ] •c v 0 − [W ] •c new , (<label>15</label></formula><formula xml:id="formula_25">)</formula><formula xml:id="formula_26">where d−1 v in (e in D− 1 2 [ Â] •v 0 + d−1 v in d− 1 2 v 0 [e in ] v 0</formula><p>) is always positive, and thus can be ignored since we only care about the signs of the gradients.</p><p>Algorithm 1 The proposed AFGSM algorithm.</p><p>Require: Graph G = (A, X), the target node v 0 , the number of vicious nodes n in , the budget of edge perturbations ∆e; 1: Train the surrogate model on the original graph G and get its weight matrix W ; 2: Randomly assign the degrees of vicious nodes d</p><formula xml:id="formula_27">(0) v in , d (1) v in , • • • , d (n in −1) v in to satisfy the budget constraint n in −1 k=0 d (k) v in = ∆e; 3: G (0) ← G, A (0) ← A, X (0) ← X; 4: Calculate x *</formula><p>in according to Eq. ( <ref type="formula" target="#formula_23">14</ref>); 5:</p><formula xml:id="formula_28">for t = 0, • • • , n in − 1 do 6: Initialize e (t) in = 0, x<label>(t)</label></formula><p>in = 0; 7:</p><formula xml:id="formula_29">x (t)</formula><p>in ← Sample X 0 /n features from x * in under the constraint condition Φ(G); 8: e (t) ← Calculate e * in according to Eq. ( <ref type="formula" target="#formula_36">16</ref>); 9: t) and X (t) according to Eq. ( <ref type="formula" target="#formula_16">10</ref>); 10: end for</p><formula xml:id="formula_30">G (t+1) = A (t+1) , X (t+1) ← Update A (</formula><formula xml:id="formula_31">11: return G (n in ) = A (n in ) , X (n in ) ;</formula><p>Algorithm 2 The proposed AFGSM-ada algorotihm.</p><p>Require: Graph G = (A, X), the target node v 0 , the number of vicious nodes n in , the budget of edge perturbations ∆e; 1: Randomly assign the degrees of vicious nodes d</p><formula xml:id="formula_32">(0) v in , d (1) v in , • • • , d (n in −1) v in to satisfy the budget constraint n in −1 k=0 d (k) v in = ∆e; 2: G (0) ← G, A (0) ← A, X (0) ← X; 3: for t = 0, • • • , n in − 1 do 4:</formula><p>Train the surrogate model on the perturbed graph G (t) and get its weight matrix W ; 5:</p><p>Calculate x * in according to Eq. ( <ref type="formula" target="#formula_23">14</ref>); 6:</p><p>Initialize e</p><formula xml:id="formula_33">(t) in = 0, x<label>(t)</label></formula><p>in = 0; 7:</p><formula xml:id="formula_34">x (t)</formula><p>in ← Sample X 0 /n features from x * in under the constraint condition Φ(G); 8: e (t) ← Calculate e * in according to Eq. ( <ref type="formula" target="#formula_36">16</ref>); 9: t) and X (t) according to Eq. ( <ref type="formula" target="#formula_16">10</ref>); 10: end for 11: return G (n in ) = (A (n in ) , X (n in ) );</p><formula xml:id="formula_35">G (t+1) = A (t+1) , X (t+1) ← Update A (</formula><p>An approximate closed-form solution of e in . Without loss of generality, we assume that the vicious node v in connects to a fixed number of nodes in the original graph G, and therefore d v in = e in 0 holds. Since the output Z is linear w.r.t. the variable e in , we achieve a closed-form solution of e in with constraint in Eq. ( <ref type="formula" target="#formula_11">7</ref>), i.e., e * in = −0.5sign</p><formula xml:id="formula_36">∂L atk ∂e in − g d in 1 + 0.5,<label>(16)</label></formula><p>where g d in is the d in -th smallest element of ∂L atk ∂e in . In other words, the i-th component of e * in is set to 1 if the corresponding gradients are less than the d in -th smallest gradient in ∂L atk ∂e in . The gradients of loss function L atk w.r.t. the variable e in is derived as follows</p><formula xml:id="formula_37">∂L atk ∂e in ≈ dv in dv 0 − 1 2 d− 1 2 v in [e in ] v 0 D− 1 2 X + D−1 [ Ã]•v 0 x in [W ]•c v 0 − [W ]•c new ,<label>(17)</label></formula><p>where ( dv in dv 0 ) − 1 2 is always positive, and thus can be ignored. We summarize the procedure of the proposed AFGSM in Algorithm 1. Note that the approximate closed-form solution of feature vector x * in is calculated for once as it only depends on the weights W . However, the approximate closed-form solution of edge information e * in relies on variables A and X and thus should be updated as the vicious nodes are injected one by one in a sequential manner.</p><p>Note that Algorithm 1 treats the model weight matrix W as static. Alternatively, we also develop an adaptive version of AFGSM, namely AFGSM-ada by retraining the surrogate model once we inject a vicious node using the AFGSM. We summarize the procedure of AFGSM-ada in Alghritm 2. Similarly, we also develop the adaptive version of Nettack and FGSM correspondingly, namely Nettack-ada and FGSM-ada. As for Meta-attack, it updates the model weights dynamically, so there is no need to develop its variant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complexity of AFGSM. The time complexity of the proposed AFGSM algo</head><formula xml:id="formula_38">- rithm is O (n in (n + f d))</formula><p>. This is because that the gradients in Eq. ( <ref type="formula" target="#formula_24">15</ref>) and Eq. ( <ref type="formula" target="#formula_37">17</ref>) is concise enough such that they can be calculated by several vectors and no matrix multiplication is involved. And for each vicious node, the computation of features needs O (f d) just like analyzed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison of Complexity</head><p>Comparing the time complexity of different methods, we observe that</p><formula xml:id="formula_39">AFGSM:O (n in (n + f d)) &lt; Nettack:O (∆ e • n inj • (n • N ei v 0 + f d)) &lt; FGSM:O ∆ e n 2 + n in f d &lt; Meta-attack:O ∆ e T n 2 + n in f d</formula><p>As a result, the proposed AFGSM algorithm is the most efficient one in terms of time complexity and then followed by Nettack, FGSM and Meta-attack. Experimental results also substantiate the conclusion in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we evaluate the effectiveness and efficiency of our attack in the new attack scenario. We first provide the experimental setup (Section 5.1). Then we show the results of adapting existing methods to the new attack scenario following two different strategies: one-time injection and sequential injection (Section 5.2). Next, we explore the performance of our methods on large graphs and analyze the time cost (Section 5.3). Finally, we consider two stricter scenarios where attackers have limited capability and show the performance of the AFGSM method in these restricted cases (Section 5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>We conduct our experiments on five well-known public datasets: Citeseer <ref type="bibr" target="#b24">(Sen et al., 2008</ref><ref type="bibr">), Cora (McCallum et al., 2000)</ref>, Pubmed <ref type="bibr" target="#b24">(Sen et al., 2008)</ref>, DBLP <ref type="bibr" target="#b31">(Zhang et al., 2019)</ref> and Reddit <ref type="bibr" target="#b15">(Hamilton et al., 2017)</ref>. The first four are citation networks where nodes are documents, edges are citation links and features are selected as the words in the document after filtering out the stop words. And the last one is a post-to-post graph where nodes are posts and edges denote these posts are from the same user. Due to the high cost of training models (e.g., GCN and Deepwalk) on the original Reddit graph(around 230k nodes), we randomly sample a subgraph with nearly 150K nodes. The detailed statistics of these datasets are shown in Table 1. Following the same attack setting in <ref type="bibr" target="#b34">(Zügner et al., 2018)</ref>, we only consider the largest connected component for convenience.</p><p>In the experiments, we split the datasets into training set (10%), validation set (10%), and test set (80%). Note that in practice, attackers rarely can manipulate the training data. Therefore, we only inject vicious nodes to the test set (without labels). We first train a surrogate model on the training set, and then among all nodes that are correctly classified, we select (i) the 10 nodes with the highest margin of classification, (ii) the 10 nodes with the lowest margin of classification, (iii) 30 nodes selected randomly as our target nodes to be attacked. Since we focus on transductive classification in this paper, the model is then retrained on the mixture of clean training and perturbed test data. For each target node, we repeat the retraining process 5 times with different random seeds to stabilize the results and the average performance is reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Adapting Existing Methods to the New Scenario</head><p>As mentioned in the previous section, the node injection process can be performed in two ways: inject nodes all at once (one-time injection) and inject nodes sequentially (sequential injection). In this section, we compare the performance of different attacks with these two different node injection strategies.</p><p>One-time injection. The one-time injection can be roughly treated as the special case of the attack scenario considered in <ref type="bibr" target="#b34">(Zügner et al., 2018)</ref> as nodes are added in advance and perturbations are generated using existing attacks 3 . Onetime injection proceeds by first connecting a fixed number of vicious nodes to the target node directly and then treat the newly added nodes as existing nodes and apply the attacks proposed in <ref type="bibr" target="#b34">(Zügner et al., 2018;</ref><ref type="bibr" target="#b33">Zügner and Günnemann, 2019)</ref>. We choose to connect the vicious nodes directly because connections to the target node usually lead to better attack performance <ref type="bibr" target="#b34">(Zügner et al., 2018)</ref>. However, we do not know the optimal number of vicious nodes that should be connected and testing all combinations of vicious nodes is not practical. Therefore, we randomly connect 0%, 50% and 100% of the vicious nodes to the target node. Although this simple approach cannot cover all the cases, as shown below, connecting all vicious nodes (to the target node) gives the best attack performance.</p><p>The results of different attacks under different number of initial connections (to the target node) are shown in Table <ref type="table" target="#tab_1">2</ref>. Note that we enforce the same perturbation budget (10 nodes and 20 edges) for all methods in Table <ref type="table" target="#tab_1">2</ref> for a fair comparison. First, we observe that all attacks get the best performance when we connect 100% of vicious nodes to the target node, which is also consistent with our intuition and the results in <ref type="bibr" target="#b34">(Zügner et al., 2018</ref>) that (more) connections with target node gives better attack results. Second, we find that Meta-attack performs the best in the one-time injection. The reason is that Meta-attack generates the perturbation on edges and features utilizing the second-order gradients which can provide more information by considering the model weights dynamically. However, the complexity of Meta-attack is often very high because of the higher-order gradients and hence, cannot scale to large graphs such as DBLP and Pumbed used in the paper. Third, interestingly, we observe that FGSM performs better than Nettack in the new scenario which is quite different from the results in <ref type="bibr" target="#b34">(Zügner et al., 2018)</ref>. We hypothesize that it may be due to the limited search space of Nettack in the new scenario. More specifically, the initial values of E, O are extremely sparse (i.e., only a small number of connected edges) compared to the number of existing nodes and edges in the graph, even if we connect 100% of vicious nodes to the target node initially. Therefore, the search space for Nettack is severely limited.</p><p>Sequential injection. Next, we explore the performance of different attacks using the sequential injection strategy. Following the same setting in the one-time injection, we still connect the vicious nodes to the target node, but in a sequential manner. The results are shown in Table <ref type="table">3</ref>.</p><p>We find that, in the sequential addition scenario, FGSM performs better while Meta-attack performs worse compared to their counterpart in the one-time injection scenario. For example, on the Cora dataset, FGSM-one-time only lowers the GCN accuracy from 92.8% to 29.2% while FGSM-sequential lowers the accuracy to 25.6%. Differently, still on the Cora dataset and with the GCN model, Meta-attackone-time can lower the accuracy from 92.8% to 18.8% while Meta-attack-sequential can only lower it to 24.8%. As for Nettack, there is no significant difference in the performance by following different node injection strategies. Nettack-one-time can 3 There is a difference in the constraint for feature perturbations. As explained in Section 3.2, we do not have specific feature constraints (however, we do not allow the occurrence of pairs of features that do not exist in the original nodes) for the vicious nodes while in the original scenario, the number of feature perturbations cannot exceed a certain threshold.</p><p>Table <ref type="table">3</ref>: Accuracy of victim learning models against different attacks with two different strategies. Attacks with a postfix of one-time are attacks with one-time node injection strategy. Attacks with a postfix sequential are attacks with sequential node injection strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Citeseer Cora GCN GAT Deepwalk GCN GAT Deepwalk Clean 0.892 ± 0.010 0.804 ± 0.008 0.736 ± 0.054 0.928 ± 0.016 0.788 ± 0.027 0.840 ± 0.028 Nettack-one-time 0.248 ± 0.010 0.244 ± 0.015 0.604 ± 0.102 0.324 ± 0.023 0.356 ± 0.022 0.651 ± 0.032 Nettack-sequential 0.252 ± 0.024 0.228 ± 0.010 0.644 ± 0.066 0.316 ± 0.015 0.356 ± 0.020 0.784 ± 0.065 Meta-attack-one-time 0.104 ± 0.008 0.164 ± 0.015 0.352 ± 0.081 0.188 ± 0.020 0.196 ± 0.023 0.452 ± 0.047 Meta-attack-sequential 0.120 ± 0.012 0.228 ± 0.016 0.392 ± 0.071 0.248 ± 0.024 0.240 ± 0.013 0.420 ± 0.046 FGSM-one-time 0.216 ± 0.023 0.216 ± 0.150 0.504 ± 0.066 0.292 ± 0.010 0.252 ± 0.016 0.488 ± 0.032 FGSM-sequential 0.156 ± 0.020 0.144 ± 0.015 0.408 ± 0.061 0.256 ± 0.008 0.316 ± 0.008 0.456 ± 0.023 lower the accuracy on Cora to 32.4% while Nettack-sequential lowers it to 31.6%.</p><p>For GCN on the Citeseer, Nettack-one-time lowers the accuracy to 24.8% while Nettack-sequential lowers it to 25.2%, which has a negligible difference. We hypothesize that different performance of Meta-attack and FGSM under the two injection strategies is related to the search space identified by the node injection strategies and the nature of the attacks.</p><p>The main difference between the two node injection strategies is the degree of freedom in their valid search space. For the sequential injection, attacks are limited to manipulate a limited number of edges for each vicious node as we have a constraint on the degree of each vicious node. Therefore, for attacks that utilize limited information (e.g., first-order gradient for FGSM), this limitation helps to prevent attacks from manipulating too many sub-optimal edges for a single node and hence avoids getting stuck into bad solutions. In contrast, for the one-time injection, attacks are only constrained by the total number of vicious edges and thus have higher freedom when perturbing edges. For attacks with limited information, a higher degree of freedom can easily lead to suboptimal solutions while with more information (e.g., second-order derivative for Meta-attack), a higher degree of freedom leads to better attack results. For Nettack, it still suffers from the limited search space with sequential injection (similar to the analysis in onetime injection) and hence, doesn't show significant difference under different node injection strategies. Moreover, we observe that FGSM with sequential injection performs relatively closely to the costly Meta-attack with the one-time injection and the gap can be further reduced by adaptively retraining the model during attack process and more details can be found in Table <ref type="table" target="#tab_2">4</ref>. However, in comparison to the complicated second order in Meta-attack, the first-order gradient in FGSM can be easily approximated and hence more efficient AFGSM is proposed in this paper.</p><p>For experiments presented in the rest part of the paper, when choosing the baseline methods, for each method, we select the best one under the two node injection strategies. Specifically, we select FGSM-sequential, Meta-attack-one-time, and Nettack-one-time. For convenience, we still denote them as FGSM, Metaattack and Nettack, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Attack by AFGSM and its adaptive variant.</head><p>We conduct experiments on small and large-scale graphs to show the effectiveness and efficiency of our attack. For each attack in this section, we additionally consider an adaptive variant of the attack, which trains the model dynamically during the attack process. Therefore, both AFGSM and FGSM have two attack forms: one with fixed model during the attack and one with dynamically retrained model during the attack. Meta-attack does not have its adaptive variant because the original attack already retrains the model during the attack. We denote the attack with adaptive training by adding a postfix "ada".</p><p>On small graphs. First, we conduct experiments on small graphs (e.g., Citeseer and Cora). The results are shown in Table <ref type="table" target="#tab_2">4</ref>. We find that adaptively training models during the attack process helps FGSM and AFGSM achieve better performance. For example, AFGSM for GCN on the Citeseer dataset only lowers the accuracy from 89.2% to 22.4% while AFGSM-ada lowers it to 10.4%. Although Meta-attack still performs the best among all methods, FGSM-ada and AFGSMada get a pretty close performance to Meta-attack. AFGSM performs similar to FGSM because our approximation technique preserves the attack effectiveness while improves the efficiency significantly. To show that the good performance of AFGSM and AFGSM-ada does not depend on a specific set of hyperparameters, we also compare all the attacks using different constraint budgets. The results are shown in Figure <ref type="figure" target="#fig_0">2</ref>. We can easily find that under different sets of attack hyperparameters, AFGSM is still very effective and AFGSM-ada performs close to the best performing Meta-attack. We emphasize that AFGSM achieves the comparably good performance with much lower computational complexity.</p><p>On large graphs. To show the scalability of our algorithm, we first conduct experiments on large DBLP and Pubmed datasets (still with 10 vicious nodes and 20 edges). For graphs with 10K+ nodes, we can still obtain results for Nettack, FGSM, and AFGSM and their adaptive variants. Meta-attack cannot scale to these graphs and hence, we do not include the results of Meta-attack. Details of the experiments can be found in Table <ref type="table" target="#tab_3">5</ref>. We observe similar results as of small graphs: AFGSM performs closely to FGSM, and their adaptive variants provide better performance. One exception happens for Deepwalk on DBLP, which might be because of the poor transferability of the adaptive attack on GCN to Deepwalk (attacks on GAT and Deepwalk are all transferred from the attacks on GCN). Among all methods, FGSM-ada performs the best and AFGSM-ada is close to FGSM-ada. However, due to high complexity, FGSM or FGSM-ada cannot scale to graphs with more than 30K nodes (tested on our machine<ref type="foot" target="#foot_3">4</ref> using sampled subgraphs from Reddit).</p><p>We further test the performance of our attack on larger graphs, where none of the baseline attacks can scale. We construct the large graph by subsampling from the Reddit dataset with 150K nodes. Note that we only perturb edges because the features of Reddit are preprocessed and it is impractical to directly manipulate the preprocessed features. Considering that the results of target nodes with higher degrees are harder to be mislead <ref type="bibr" target="#b34">(Zügner et al., 2018)</ref>, we attack the target nodes with d v 0 /2 vicious nodes and d 0 edges. The results are shown in Table <ref type="table" target="#tab_4">6</ref>. Our AFGSM can still significantly reduce the accuracy on the GCN model. The transferability of the attack on GCN to Deepwalk is relatively low and we leave it as future work to improve the transferability of AFGSM on extremely large graphs. We also note that, when evaluating our attack on extremely large graphs, the main bottleneck will be in the model training instead of the attack process. Therefore, as long as there are efficient methods to train models on extremely large graphs, our attack can always scale and (highly probably) work.    Time cost analysis. To compare the computational cost of different methods, we conduct experiments (on the same machine mentioned above) on subgraphs of DBLP with different sizes (i.e., varying from 1K to 15K nodes with a step size of 1K nodes). The results of time cost are shown in Figure <ref type="figure" target="#fig_2">3</ref>. It is obvious that AFGSM is much more efficient than other baseline attacks, and the order of time cost aligns well with the complexity analysis in Section 4.3 (i.e., AFGSM is the most efficient and followed by Nettack, FGSM, and Meta-attack). Furthermore, the execution time of AFGSM remains relatively stable when the size of the graph grows, while the run time of other attacks grow much faster when the graph size increases, especially for Meta-attack (time cost is 3,000 times higher than AFGSM and cannot scale to the DBLP dataset with more than 13K nodes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Edge-only perturbation and Indirect perturbation</head><p>In this section, we explore two additional restricted perturbations: edges-only perturbation and indirect perturbation to verify the robustness of our attack algorithm in more practical and restricted settings.</p><p>Edge-only perturbation. From the practical point of view, manipulation of features can be hard since attackers may not have the knowledge of how features in the graph are selected and preprocessed. Therefore, we study the performance AFGSM when attackers are only allowed to change edges of vicious nodes. To do so, we inject vicious nodes with random features sampled from the original graph instead of some well-designed features to get rid of the impact of features and only focus on the edges. We denote our attack in the restricted setting as AFGSM-edges  since we only optimize edges during the attack process. The results are shown in Figure <ref type="figure" target="#fig_3">4</ref>. We observe that AFGSM in the restricted setting still succeeds. We further verify whether features and edges are equally important in the new attack scenario. By comparing the performance of AFGSM and AFGSM-edges as well as AFGSM-ada and AFGSM-ada-edges, we observe that perturbing only the edges limits the attack performance significantly. This is in contrast to the findings in the attack scenario of <ref type="bibr" target="#b34">(Zügner et al., 2018)</ref>, where the authors observe that manip-ulating features are not very important for successful attacks. Such contrast exists because, in the scenario of <ref type="bibr" target="#b34">(Zügner et al., 2018)</ref>, the attacker can only perturb features of the original nodes slightly (to remain unnoticeable) while in the new attack scenario, original features of vicious nodes can be rather arbitrary (but perturbations still follow the constraint in Eq. ( <ref type="formula" target="#formula_11">7</ref>)). Hence, the new attack scenario grants the attacker more freedom in designing the perturbed features.</p><p>Indirect perturbation. In some cases, attackers may not be able to build connections with the target node directly. For example, some Facebook users can change their privacy settings and do not allow friend requests from unknown users who do share any mutual friends. Therefore, we evaluate the performance of our attack when attackers are not allowed to build direct connections to the target node. We denote the variant of our attack as AFGSM-in in the restricted setting. Results are shown in Figure <ref type="figure" target="#fig_4">5</ref>. We observe that even in the restricted setting, the attack still succeeds in fooling the victim learning model. Unsurprisingly, we also observe that attacks in the current setting are much less successful than the attack in the case where vicious nodes can be directly connected to the target node. This observation also highlights the importance of building direct connections to the target node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we consider a practical attack scenario against GCN models, where attackers are only allowed to inject vicious nodes to the graph. We then propose a new attack algorithm named AFGSM to generate reliable adversarial perturbations efficiently. Through extensive experimental evaluations, we verify that GCN models are still vulnerable in the new attack setting and further show that our proposed AFGSM method outperforms the baselines significantly in terms of attack effectiveness and efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Accuracy on GCN with different numbers of vicious nodes and edges. (a) and (c) denote the performance of GCN with different numbers of vicious edges and 10 nodes on Citeseer and Cora, respectively. (b) and (d) denote the performance of GCN with different numbers of vicious nodes and 10 nodes on Citeseer and Cora, respectively.</figDesc><graphic url="image-2.png" coords="16,66.33,89.45,345.71,261.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>± 0.025 0.808 ± 0.010 0.856 ± 0.023 0.904 ± 0.022 0.848 ± 0.020 0.832 ± 0.020 Random 0.712 ± 0.020 0.724 ± 0.015 0.832 ± 0.047 0.848 ± 0.016 0.796 ± 0.023 0.792 ± 0.032 Nettack 0.268 ± 0.016 0.420 ± 0.021 0.712 ± 0.035 0.152 ± 0.027 0.252 ± 0.010 0.796 ± 0.046 Nettack-ada 0.272 ± 0.016 0.436 ± 0.019 0.808 ± 0.016 0.156 ± 0.023 0.240 ± 0.000 0.824 ± 0.030 FGSM 0.260 ± 0.000 0.644 ± 0.015 0.652 ± 0.032 0.120 ± 0.013 0.220 ± 0.000 0.516 ± 0.037 FGSM-ada 0.240 ± 0.000 0.592 ± 0.016 0.664 ± 0.034 0.116 ± 0.015 0.220 ± 0.000 0.504 ± 0.029 AFGSM 0.252 ± 0.016 0.528 ± 0.016 0.604 ± 0.034 0.156 ± 0.008 0.224 ± 0.008 0.728 ± 0.030 AFGSM-ada 0.216 ± 0.013 0.460 ± 0.013 0.656 ± 0.028 0.136 ± 0.020 0.224 ± 0.023 0.648 ± 0.030</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Run time comparison on the GCN model.</figDesc><graphic url="image-3.png" coords="17,213.10,225.34,192.75,111.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Accuracy of victim learning model against edge-only attacks.</figDesc><graphic url="image-4.png" coords="18,89.29,89.45,311.14,207.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Accuracy of victim learning model against indirect attacks.</figDesc><graphic url="image-5.png" coords="18,89.29,334.22,311.14,208.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The detailed statistics of the used datasets. N LCC and E LCC are the numbers of nodes and edges in the largest connected component, d is the dimension of features and C is the number of classes.</figDesc><table><row><cell>Dataset</cell><cell>N LCC</cell><cell>E LCC</cell><cell>d</cell><cell>C</cell><cell>frequency of classes</cell></row><row><cell>Citeseer</cell><cell>2,110</cell><cell cols="2">3,668 3,703</cell><cell>6</cell><cell>532, 463, 388, 308, 304, 115</cell></row><row><cell>Cora</cell><cell>2,485</cell><cell cols="2">5,069 1,433</cell><cell>7</cell><cell>726, 406, 379, 344, 285, 214, 131</cell></row><row><cell>DBLP</cell><cell>16,766</cell><cell cols="2">44,422 2,476</cell><cell>4</cell><cell>6935, 6532, 1777, 1522</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>44,324</cell><cell>500</cell><cell>3</cell><cell>7875, 7739, 4103</cell></row><row><cell>Reddit</cell><cell cols="2">149,177 5,215,380</cell><cell cols="3">602 41 28163, 15163, 13963, 13065, 12742, ... 2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accuracy of victim learning models against different attacks with one-time injection strategy w.r.t. different number of initial connections between vicious nodes and target node. Clean denotes the model without any attacks. Random denotes the model in which the vicious nodes connect to existing nodes randomly and their features are also sampled randomly according to those of existing nodes.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Citeseer</cell><cell></cell><cell></cell><cell>Cora</cell></row><row><cell></cell><cell>GCN</cell><cell>GAT</cell><cell>Deepwalk</cell><cell>GCN</cell><cell>GAT</cell><cell>Deepwalk</cell></row><row><cell>Clean</cell><cell cols="6">0.892 ± 0.010 0.804 ± 0.008 0.736 ± 0.054 0.928 ± 0.016 0.788 ± 0.027 0.840 ± 0.028</cell></row><row><cell>Random</cell><cell cols="6">0.772 ± 0.020 0.708 ± 0.016 0.648 ± 0.041 0.868 ± 0.029 0.700 ± 0.022 0.728 ± 0.063</cell></row><row><cell>Nettack-0%</cell><cell cols="6">0.480 ± 0.025 0.460 ± 0.022 0.652 ± 0.041 0.424 ± 0.008 0.516 ± 0.023 0.744 ± 0.029</cell></row><row><cell>Nettack-50%</cell><cell cols="6">0.336 ± 0.029 0.304 ± 0.015 0.592 ± 0.081 0.352 ± 0.016 0.428 ± 0.016 0.656 ± 0.064</cell></row><row><cell>Nettack-100%</cell><cell cols="6">0.248 ± 0.010 0.244 ± 0.015 0.604 ± 0.102 0.324 ± 0.023 0.356 ± 0.022 0.651 ± 0.032</cell></row><row><cell>Meta-attack-0%</cell><cell cols="6">0.148 ± 0.020 0.156 ± 0.015 0.460 ± 0.046 0.204 ± 0.015 0.272 ± 0.020 0.484 ± 0.061</cell></row><row><cell cols="7">Meta-attack-50% 0.112 ± 0.027 0.140 ± 0.028 0.412 ± 0.059 0.224 ± 0.039 0.232 ± 0.010 0.520 ± 0.073</cell></row><row><cell cols="7">Meta-attack-100% 0.104 ± 0.008 0.164 ± 0.015 0.352 ± 0.081 0.188 ± 0.020 0.196 ± 0.023 0.452 ± 0.047</cell></row><row><cell>FGSM-0%</cell><cell cols="6">0.208 ± 0.020 0.300 ± 0.033 0.524 ± 0.041 0.336 ± 0.015 0.436 ± 0.057 0.596 ± 0.085</cell></row><row><cell>FGSM-50%</cell><cell cols="6">0.200 ± 0.013 0.216 ± 0.023 0.504 ± 0.034 0.260 ± 0.013 0.304 ± 0.015 0.536 ± 0.048</cell></row><row><cell>FGSM-100%</cell><cell cols="6">0.216 ± 0.023 0.216 ± 0.150 0.504 ± 0.066 0.292 ± 0.010 0.252 ± 0.016 0.488 ± 0.032</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Accuracy of victim learning models against different attacks and adaptive variants. Attacks with a postfix of ada are adaptive attacks with victim learning models retrained during the attack process.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Citeseer</cell><cell></cell><cell></cell><cell>Cora</cell></row><row><cell></cell><cell>GCN</cell><cell>GAT</cell><cell>Deepwalk</cell><cell>GCN</cell><cell>GAT</cell><cell>Deepwalk</cell></row><row><cell>Clean</cell><cell cols="6">0.892 ± 0.010 0.804 ± 0.008 0.736 ± 0.054 0.928 ± 0.016 0.788 ± 0.027 0.840 ± 0.028</cell></row><row><cell>Random</cell><cell cols="6">0.772 ± 0.020 0.708 ± 0.016 0.648 ± 0.041 0.868 ± 0.029 0.700 ± 0.022 0.728 ± 0.063</cell></row><row><cell>Nettack</cell><cell cols="6">0.248 ± 0.010 0.244 ± 0.015 0.604 ± 0.102 0.324 ± 0.023 0.356 ± 0.022 0.651 ± 0.032</cell></row><row><cell cols="7">Nettack-ada 0.256 ± 0.008 0.224 ± 0.023 0.556 ± 0.069 0.304 ± 0.023 0.304 ± 0.019 0.656 ± 0.064</cell></row><row><cell cols="7">Meta-attack 0.104 ± 0.008 0.164 ± 0.015 0.352 ± 0.081 0.188 ± 0.020 0.196 ± 0.023 0.452 ± 0.047</cell></row><row><cell>FGSM</cell><cell cols="6">0.156 ± 0.020 0.144 ± 0.015 0.408 ± 0.061 0.256 ± 0.008 0.316 ± 0.008 0.456 ± 0.023</cell></row><row><cell>FGSM-ada</cell><cell cols="6">0.112 ± 0.010 0.136 ± 0.015 0.444 ± 0.057 0.208 ± 0.020 0.364 ± 0.015 0.472 ± 0.056</cell></row><row><cell>AFGSM</cell><cell cols="6">0.224 ± 0.023 0.192 ± 0.016 0.532 ± 0.060 0.304 ± 0.015 0.404 ± 0.041 0.580 ± 0.051</cell></row><row><cell cols="7">AFGSM-ada 0.104 ± 0.027 0.128 ± 0.020 0.484 ± 0.055 0.212 ± 0.008 0.388 ± 0.036 0.588 ± 0.056</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Accuracy of victim learning models against different attacks on large graphs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Accuracy on Reddit Random 0.860 ± 0.042 0.944 ± 0.008 AFGSM 0.572 ± 0.027 0.892 ± 0.020</figDesc><table><row><cell>Method</cell><cell cols="2">Reddit</cell></row><row><cell></cell><cell>GCN</cell><cell>Deepwalk</cell></row><row><cell>Clean</cell><cell>0.860 ± 0.010</cell><cell>0.96 ± 0.000</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The problem is formulated as bi-level optimization because the perturbed test input is also used in the training procedure and the model weight is dependent on perturbed test data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The frequency of classes are:28163, 15163, 13963, 13065, 12742, 12041, 11149, 10239, 7915,  5863, 5087, 5048, 4937, 4898, 4849, 4668, 4547, 4212, 4188, 4184, 4161, 4040,  </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3930" xml:id="foot_2">, 3588, 3538,  3422, 3279, 2970, 2960, 2792, 2687, 2630, 2304, 2232, 2115, 1696, 1645, 1588, 1554, 991, 328   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">We record the actual run time on the same machine with configuration: CPU (i9-7900X, 3.30GHz), 128GB RAM.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph based anomaly detection and description: a survey</title>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data mining and knowledge discovery</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="626" to="688" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<idno>arXiv:180200420</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Node classification in social networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social network data analytics</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="115" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Adversarial attacks on node embeddings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bojcheski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<idno>arXiv:180901093</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adversarial attacks on node embeddings via graph poisoning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<idno>arXiv:180901093</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised profiling methods for fraud detection. Credit scoring and credit control VII pp</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="235" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A comprehensive survey of graph embedding: Problems, techniques, and applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kcc</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1616" to="1637" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Recommendations to boost content spread in social networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chaoji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ranu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bhatt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<idno>arXiv:180110247</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Practical attacks against graph-based clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nadji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kountouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monrose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasiloglou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pagerank optimization by edge selection</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Csji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Jungers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Blondel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">169</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="73" to="87" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno>arXiv:180602371</idno>
		<title level="m">Adversarial attack on graph structured data</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9185" to="9193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>arXiv:14126572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shortest-path network interdiction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Israeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Networks: An International Journal</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="111" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>arXiv:160902907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">InInterna-tional Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Focused clustering and outlier detection in large attributed graphs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iglesias</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014a</date>
			<biblScope unit="page" from="1346" to="1355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014b</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Column networks for collective classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence Phillips CA</title>
				<imprint>
			<date type="published" when="1993">2017. 1993</date>
		</imprint>
	</monogr>
	<note>Acm Symposium on Theory of Computing</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Node injection attacks on graphs via reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Honavar</surname></persName>
		</author>
		<idno>arXiv:190906543</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fergus</forename><forename type="middle">R</forename></persName>
		</author>
		<idno>arXiv:13126199</idno>
		<title level="m">triguing properties of neural networks</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Node classification in signed social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 SIAM international conference on data mining</title>
				<meeting>the 2016 SIAM international conference on data mining<address><addrLine>SIAM</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="54" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning deep representations for graph clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Eighth AAAI Conference on Artificial Intelligence Watkins CJ, Dayan P (1992) Q-learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="279" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>arXiv:190100596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno>arXiv:190104095</idno>
		<title level="m">Attributed network embedding via subspace discovery</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Data poisoning attacks on multi-task relationship learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><forename type="middle">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sj ;</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<idno>arXiv:181208434</idno>
	</analytic>
	<monogr>
		<title level="m">Graph neural networks: A review of methods and applications</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Thirty-Second AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Adversarial attacks on graph neural networks via meta learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<idno>arXiv:190208412</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, ACM</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, ACM</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2847" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
