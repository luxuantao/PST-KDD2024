<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geometric Transformers for Protein Interface Contact Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-06">6 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alex</forename><surname>Morehead</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering &amp; Computer Science</orgName>
								<orgName type="institution">University of Missouri Columbia</orgName>
								<address>
									<postCode>65211</postCode>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
							<email>chen.chen@missouri.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering &amp; Computer Science</orgName>
								<orgName type="institution">University of Missouri Columbia</orgName>
								<address>
									<postCode>65211</postCode>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianlin</forename><surname>Cheng</surname></persName>
							<email>chengji@missouri.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering &amp; Computer Science</orgName>
								<orgName type="institution">University of Missouri Columbia</orgName>
								<address>
									<postCode>65211</postCode>
									<region>MO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Geometric Transformers for Protein Interface Contact Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-06">6 Oct 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2110.02423v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Computational methods for predicting the interface contacts between proteins come highly sought after for drug discovery as they can significantly advance the accuracy of alternative approaches, such as protein-protein docking, protein function analysis tools, and other computational methods for protein bioinformatics. In this work, we present the Geometric Transformer, a novel geometryevolving graph transformer for rotation and translation-invariant protein interface contact prediction, packaged within DeepInteract, an end-to-end prediction pipeline. DeepInteract predicts partner-specific protein interface contacts (i.e., inter-protein residue-residue contacts) given the 3D tertiary structures of two proteins as input. In rigorous benchmarks, DeepInteract, on challenging protein complex targets from the new Enhanced Database of Interacting Protein Structures (DIPS-Plus) and the 13th and 14th CASP-CAPRI experiments, achieves 17% and 13% top L/5 precision (L: length of a protein unit in a complex), respectively. In doing so, DeepInteract, with the Geometric Transformer as its graph-based backbone, outperforms existing methods for interface contact prediction in addition to other graph-based neural network backbones compatible with DeepInteract, thereby validating the effectiveness of the Geometric Transformer for learning rich relational-geometric features for downstream tasks on 3D protein structures. 1</p><p>1 Training and inference code as well as pre-trained models are available at https://github.com/BioinfoMachineLearning/DeepInteract Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Interactions of proteins often reflect and directly influence their functions in molecular processes, so understanding the relationship between protein interaction and protein function is of utmost importance to biologists and other life scientists. Here, we study the residue-residue interaction between two protein structures that bind together to form a binary protein complex (i.e., dimer), to better understand how these coupled proteins will function in vivo. Predicting where two proteins will interface in silico has become an appealing method for measuring the interactions between proteins since a computational approach saves time, energy, and resources compared to traditional methods for experimentally measuring such interfaces <ref type="bibr" target="#b33">(Wells &amp; McClendon (2007)</ref>). A key motivation for determining these interface contacts is to decrease the time required to discover new drugs and to advance the study of newly designed proteins <ref type="bibr" target="#b23">(Murakami et al. (2017)</ref>).</p><p>Existing approaches to interface contact prediction include classical machine learning and deep learning-based methods. These methods traditionally use hand-crafted features to predict which inter-chain pairs of amino acid residues will interact with one another upon the binding of the two protein chains, treating each of their residue pairs as being independent of one another. Recent work on interface prediction <ref type="bibr" target="#b20">(Liu et al. (2020)</ref>), however, considers the biological insight that the interaction between two inter-chain residue pairs depends not only on the pairs' features themselves but also on other residue pairs ordinally nearby in terms of the protein complex's sequence. As such, the problem of interface contact prediction became framed as one akin to image segmentation or object detection, opening the door to innovations in interface contact prediction by incorporating the latest techniques from computer vision.</p><p>Nonetheless, up to now, no works on partner-specific protein interface contact prediction have leveraged two recent innovations to better capture geometric shapes of protein structures and long-range interactions between amino acids important for accurate prediction of protein-protein interface contacts: (1) geometric deep learning for evolving proteins' geometric representations and (2) graphbased self-attention similar to that of <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref>. Towards this end, we introduce Deep-Interact, an end-to-end deep learning pipeline for protein interface prediction. DeepInteract houses the Geometric Transformer, a new graph transformer designed to exploit protein structure-specific geometric properties, as well as a dilated convolution-based interaction module adapted from <ref type="bibr" target="#b2">Chen et al. (2021)</ref> to predict which inter-chain residue pairs comprise the interface between the two protein chains. In response to the exponential rate of progress being made in predicting protein structures in silico, we trained DeepInteract end-to-end using DIPS-Plus <ref type="bibr" target="#b22">(Morehead et al. (2021)</ref>), to date the largest feature-rich dataset of protein complex structures for machine learning of protein interfaces, to close the gap on a proper solution to this fundamental problem in structural biology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Over the past several years, geometric deep learning has become an effective means of automatically learning useful feature representations from structured data <ref type="bibr" target="#b1">(Bronstein et al. (2021)</ref>). Previously, geometric learning algorithms like convolutional neural networks (CNNs) and graph neural networks (GNNs) have been used to predict protein interface contacts. <ref type="bibr" target="#b6">Fout et al. (2017)</ref> designed a siamese GNN architecture to learn weight-tied feature representations of residue pairs. This approach, in essence, processes subgraphs for each residue in each complex and aggregates node-level features locally using a nearest-neighbors approach. Since this partner-specific method derives its training dataset from Docking Benchmark 5 (DB5) <ref type="bibr" target="#b32">Vreven et al. (2015)</ref>, it is ultimately data-limited. <ref type="bibr" target="#b29">Townshend et al. (2019)</ref> represent interacting protein complexes by voxelizing each residue into a 3D grid and encoding in each grid entry the presence and type of the residue's underlying atoms. This partner-specific encoding scheme captures static geometric features of interacting complexes, but it is not able to scale well due to its requiring a computationally-expensive spatial resolution of the residue voxels to achieve good results.</p><p>Continuing the trend of applying geometric learning to protein structures, <ref type="bibr" target="#b7">Gainza et al. (2020)</ref> developed MaSIF to perform partner-independent interface region prediction. Likewise, <ref type="bibr" target="#b4">Dai &amp; Bailey-Kellogg (2021)</ref> do so with an attention-based GNN. These methods learn to perform binary classification of the residues in both complex structures to identify regions where residues from both complexes are likely to interact with one another. However, because these approaches pre-dict partner-independent interface regions, they are less likely to be useful in helping solve related tasks such as drug-protein interaction prediction and protein-protein docking <ref type="bibr" target="#b0">(Ahmad &amp; Mizuguchi (2011))</ref>. To date, one of the best result sets obtained by any model for protein interface contact prediction comes from <ref type="bibr" target="#b20">Liu et al. (2020)</ref> where high-order (i.e. sequential and coevolution-based) interactions between residues are learned and preserved throughout the network in addition to static geometric features initially embedded in the protein complexes. However, this work, like many of those preceding it, undesirably maintains the trend of reporting model performance in terms of the median area under the receiver operating characteristic which is not robust to extreme class imbalances as often occur in interface contact prediction. In addition, this approach is data-limited as it uses the DB5 dataset and its predecessors to derive both its training data and makes use of only each residue's carbon-alpha (Cα) atom in deriving its geometric features, ignoring important geometric details provided by an all-atom view of protein structures.</p><p>Our work builds on top of prior works by making the following contributions:</p><p>• We provide the first example of graph self-attention applied to protein interface contact prediction, showcasing its effective use in learning representations of protein geometries to be exploited in downstream tasks.</p><p>• We propose the new Geometric Transformer which can be used for tasks on 3D protein structures and other 3D graphs. For the problem of interface contact prediction, we train the Geometric Transformer to evolve a geometric representation of protein structures simultaneously with protein sequence and coevolutionary features for the prediction of interchain residue-residue contacts. In doing so, we also demonstrate the merit of the recentlyreleased Enhanced Database of Interacting Protein Structures (DIPS-Plus) for interface prediction <ref type="bibr" target="#b22">(Morehead et al. (2021)</ref>).</p><p>• Our experiments on challenging protein complex targets demonstrate that our proposed method, DeepInteract, achieves state-of-the-art results for interface contact prediction.</p><p>Through these experiments, we also highlight the need for more transparent and problemappropriate metrics as well as more stringent sequence-based filtering on dataset crossvalidation partitions to assess the real-world performance of machine learning models for proteins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Datasets</head><p>The current opinion in the bioinformatics community is that protein sequence features still carry important higher-order information concerning residue-residue interactions <ref type="bibr" target="#b20">(Liu et al. (2020)</ref>). In particular, the residue-residue coevolution and residue conservation information obtained through multiple sequence alignments (MSAs) has been shown to contain powerful information concerning intra-chain and even inter-chain residue-residue interactions as they yield a compact representation of residues' coevolutionary relationships <ref type="bibr" target="#b15">(Jumper et al. (2021)</ref>).</p><p>Keeping this in mind, for our training and validation datasets, we chose to use DIPS-Plus (Morehead et al. ( <ref type="formula">2021</ref>)), which is to our knowledge the largest feature-rich dataset of protein complexes for protein interface contact prediction to date. In total, DIPS-Plus contains 42,112 binary protein complexes with positive labels (i.e., 1) for each inter-chain residue pair that are found within 6 Å of each other in the complex's bound (i.e., structurally-conformed) state. The dataset contains a variety of rich residue-level features: (1) an 8-state one-hot encoding of the secondary structure in which the residue is found;</p><p>(2) a scalar solvent accessibility;</p><p>(3) a scalar residue depth; (4) a 1 × 6 vector detailing each residue's protrusion concerning its side chain; (5) a 1 × 42 vector describing the composition of amino acids towards and away from each residue's side chain; (6) each residue's coordinate number conveying how many residues to which the residue meets a significance threshold, (7) a 1 × 27 vector giving residues' emission and transition probabilities derived from HH-suite3 <ref type="bibr" target="#b28">(Steinegger et al. (2019)</ref>) profile hidden Markov models constructed using MSAs; and (8) amide plane normal vectors for downstream calculation of the angle between each intra-chain residue pair's amide planes.</p><p>To compare the performance of DeepInteract with that of state-of-the-art methods, we select 32 challenging and well-diversified homodimers and heterodimers from the test partition of DIPS-Plus to benchmark each method for its competency in predicting interface contacts. In addition, we evaluate  <ref type="formula">2021</ref>)) since these targets are considered by the bioinformatics community to be challenging for existing interface predictors. For any test complexes derived from multimers (i.e., protein complexes that can contain more than two chains), to represent the complex we chose the pair of chains with the largest number of interface contacts.</p><p>To expedite training and validation and to constrain memory usage, beginning with all remaining complexes not chosen for testing, we filtered out all complexes where either chain contains fewer than 20 residues and where the number of possible interface contacts is more than 256 2 , leaving us with an intermediate total of 26,504 complexes for training and validation. In DIPS-Plus, binary protein complexes are grouped into shared directories according to whether they are derived from the same parent complex. As such, using a per-directory strategy, we randomly designate 80% of these complexes for training and 20% for validation to restrict overlap between our cross-validation datasets. After choosing these targets for testing, we then filter out complexes from our training and validation partitions of DIPS-Plus that contain any chain with over 30% sequence identity to any chain in any complex in our test datasets. This threshold of 30% sequence identity is commonly used in the bioinformatics literature <ref type="bibr" target="#b14">(Jordan et al. (2012)</ref>, <ref type="bibr" target="#b36">Yang et al. (2013)</ref>) to prevent large evolutionary overlap between a dataset's cross-validation partitions. However, most existing works for interface contact prediction do not employ such filtering criteria, so the results reported in these works may be over-optimistic by nature. In performing such sequence-based filtering, we are left with 15,618 and 3,548 binary complexes for training and validation, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem Formulation</head><p>Summarized in Figure <ref type="figure" target="#fig_1">2</ref>, we designed DeepInteract, our proposed pipeline for interface contact prediction, to frame the problem of predicting interface contacts in silico as a two-part task: The first part is to use attentive graph representation learning to inductively learn new node-level representations from a pair of graphs representing two protein structures. The second part is to interleave the new node representations for each graph into a single interaction tensor, an M × N tensor where M and N are the numbers of amino acid residues in the first and second input protein chains, respectively. We use such interaction tensors as input to an interaction module, a convolution-based dense predictor of node-node (i.e., residue-residue) interactions. We denote each protein chain in the complexes as a graph, with nodes corresponding to the chain's amino acid residues represented by their Cα atoms and edges corresponding to the residues' k-nearest neighbors. In this setting, we let k = 20 as we observed favorable cross entropy loss on our validation dataset with this level of connectivity. We note that this level of connectivity has also proven to be advantageous for prior works </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Geometric Transformer Architecture</head><p>Hypothesizing that a self-attention mechanism that evolves proteins' physical geometries is a key component missing from existing interface contact predictors, we propose the Geometric Transformer, a graph neural network explicitly designed for capturing and iteratively evolving protein geometric features. As seen in Figure <ref type="figure" target="#fig_2">3</ref>, the Geometric Transformer builds upon the existing Graph Transformer architecture <ref type="bibr" target="#b5">(Dwivedi &amp; Bresson (2021)</ref>) to include an edge initialization module and geometry-evolving conformation module as well as subtler architectural enhancements such as moving the network's first normalization layer to precede any affinity score computations for improved training stability (Hussain et al. ( <ref type="formula">2021</ref>)). To our knowledge, the Geometric Transformer is the first deep learning model that applies multi-head attention to the task of partner-specific protein interface contact prediction. The new modules proposed with the Geometric Transformer are described in detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Edge Representation Initialization</head><p>The Geometric Transformer first creates an initial embedding for each edge given a set of learnable vectors and precomputed features derived from each protein chain's 3D tertiary structure. Formally, we embed each edge e's source and destination nodes n i and n j as trainable one-hot vectors indexed by P i and P j , the position of node i and j in the chain's underlying amino acid sequence. In the spirit of <ref type="bibr" target="#b12">Ingraham et al. (2019)</ref>, we then concatenate an edge-wise sinusoidal positional encoding sin(P i − P j ) for e with an edge weight (i.e., the normalized Euclidean distance between node i and j) to serve as e's original message, providing the network with a directional notion of residue-toresidue distances in protein chains' underlying sequences.</p><p>For the edge initializer module's remaining four features, we sought to include enough geometric information for the network to be able to uniquely determine the Euclidean positions of each node's neighboring nodes. For this reason, we adopt similar distance, direction, and orientation descriptors as <ref type="bibr" target="#b12">Ingraham et al. (2019)</ref>. Finally, we complement the protein backbone-geometric features provided by inter-residue distances, directions, and orientations with the angles between each residue pair's amide plane normal vectors. Once all seven edge initial features are constructed, we then concatenate them together and feed them through a multilayer perceptron (MLP). After doing so, we apply gating to the edges' messages, distances, directions, orientations, and amide angles separately to encourage the network to learn the importance of specific channels in each of these input features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Conformation Module</head><p>The role of the Geometric Transformer's conformation module, as illustrated in Figure <ref type="figure" target="#fig_3">4</ref>, is for it to learn how to iteratively evolve a geometric representation of protein structures by applying repeated gating to our initial geometric edge features. Gating is a technique that has previously been shown to encourage neural networks to not become over-reliant on any particular input feature <ref type="bibr" target="#b8">(Gu et al. (2020)</ref>) and, as such, in the Geometric Transformer can be seen as a form of channel-wise dropout for single feature sets. By also employing residual connections from original edge representations to gating-learned edge representations, the conformation module can operate more stably in the presence of multiple neural network layers <ref type="bibr" target="#b9">(He et al. (2016)</ref>).</p><p>In locating edges with which to update e ij , the Geometric Transformer introduces the notion of a geometric neighborhood for edge e ij , treating each edge as a pseudo-node. Precisely, an edge geometric neighborhood, with n ∈ Z + , is defined as the 2n incoming edges e ni , e nj for e ij 's source and destination nodes, respectively. The intuition behind updating each edge according to its 2n nearest edge neighbors is that the geometric relationship between a residue pair, described by their mutual edge's features, can be influenced by the physical constraints imposed by proximal residueresidue geometries. As such, we use these nearby edges during geometric feature updates.</p><p>The conformation module's design was inspired by SphereNet <ref type="bibr" target="#b21">(Liu et al. (2021)</ref>) and similar graph neural network architectures designed for learning on 3D graphs. What distinguishes the conformation module from the works of others is its incorporation of geometric insights specific to large biomolecules such as proteins. Namely, by including the residue-residue distances, residue-residue local reference frame directions and (quaternion) orientations, and amide plane-amide plane angles, the network is provided with enough information to ascertain the relative coordinates of each neighboring residue from a given residue's local reference frame <ref type="bibr" target="#b21">(Liu et al. (2021)</ref>), thereby ensuring the network's capability of adequately learning from 3D structures.</p><p>By way of their construction, as described in Section A.2, each of our selected geometric edge features is invariant to rotations and translations of the network's input space. Further, since each node and edge-wise operation we apply to such features is also geometrically invariant, we observe that the Geometric Transformer operates invariantly with respect to translations and rotations in R 3 , a desirable property to have when using deep learning architectures to process 3D objects such as proteins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Node Representation Initialization</head><p>For the node features we used with the Geometric Transformer, we chose to use each of the residuelevel features contained in DIPS-Plus that are described succinctly in Section 3. In addition to these node-level features in our protein chain graphs, we include node-wise positional encodings to initialize the Geometric Transformer with information concerning the residue ordering of the chain's underlying sequence as such ordering is important to understanding downstream protein structural, interactional, and functional properties of each residue. This positional encoding is calculated for each node by applying a min-max normalization of each chain sequence's residue indices P i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Interaction Module</head><p>Upon applying multiple layers of the Geometric Transformer to each pair of input protein chains, we then feed the Geometric Transformer's learned node representations after channel-wise interleaving into our interaction module, consisting of a dilated ResNet module adapted from the intra-chain contact predictor of <ref type="bibr" target="#b2">Chen et al. (2021)</ref>. The core residual network component in this interaction module consists of four residual blocks differing in the number of internal layers. Each residual block is comprised of several consecutive instance normalization layers and convolutional layers with 64 kernels of size 3 × 3. The number of layers in each block represents the number of 2D convolution layers in the corresponding component. The final values of the last convolutional layer are added to the output of a shortcut block, which is a convolutional layer with 64 kernels of size 1 × 1. A squeeze-and-excitation (SE) block <ref type="bibr" target="#b10">(Hu et al. (2018)</ref>) is added at the end of each residual block.</p><p>The SE operation weights each of its channels differently by a trainable 2-layer dense network when creating the output feature maps, so that channel-wise feature responses can be adaptively recalibrated. Ultimately, the output of the interaction module is a probability-valued M x N matrix that can be viewed as a heatmap describing which inter-chain residue pairs are likely to be in contact with one another upon binding of the two chains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>For all experiments conducted with DeepInteract, we used 2 layers of the graph neural network chosen for the experiment and 128 intermediate GNN and CNN channels to restrict the time required to train each model. For the Geometric Transformer, we used an edge geometric neighborhood of size n = 2 for each edge such that each edge's geometric features are updated by their 4-nearest incoming edges. In addition, we used the Adam optimizer (Kingma &amp; Ba ( <ref type="formula">2014</ref>)), a learning rate of 1e −3 , a weight decay rate of 1e −2 , a dropout (i.e., forget) rate of 0.2, and a batch size of 1. We also employed 0.5-threshold gradient value clipping and stochastic weight averaging <ref type="bibr" target="#b13">(Izmailov et al. (2018)</ref>). With an early-stopping patience period of 5 epochs, we observed most models converging after approximately 30 training epochs on DIPS-Plus. For our loss function, we used weighted cross entropy with a positive class weight of 5 to help the network overcome the large class imbalance present in interface prediction. All DeepInteract models employed 14 layers of our dilated ResNet architecture described in Section 4.3 and had their top-k precisions averaged over three separate runs, each with a different random seed (standard deviation of top-k precisions in parentheses).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hyperparameter Search</head><p>The identify our optimal set of model hyperparameters, we performed a manual hyperparameter search over the ranges of [1e −1 , 1e −2 , 1e −3 , 1e −4 , 1e −5 , 1e −6 ] and [1e −1 , 1e −2 , 1e −3 , 1e −4 ] for the learning rate and weight decay rate, respectively. In doing so, we found a learning rate of 1e −3 and a weight decay rate of 1e −2 to provide the lowest loss and the highest metric values on our validation dataset. We restricted our hyperparameter search to the learning rate and weight decay rate of our models due to the large computational and environmental costs associated with training each model, however, this suggests that further improvements to our models could be found with a more extensive hyperparameter search such as a grid search over, for example, the models' dropout rate and number of intermediate GNN and CNN channels.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Selection of Baselines</head><p>In choosing the baseline state-of-the-art methods against which we would compare DeepInteract with various graph-based backbones, we considered the reproducibility and accessibility of the method as the most important factor for its inclusion in our benchmarks to encourage adoption of accessible and transparent benchmarks for future works. That is, we only selected baseline methods that are easy to reproduce or instead simple for the general public to use to make predictions.</p><p>Since two of the three baseline methods we chose to include, DeepHomo and ComplexContact, do not make both their feature generation and model training source code publicly available, we were restricted to using the version of their public web servers available on 9/20/2021 to make inter-chain interface contact predictions. As such, our selection criterion for each baseline method consequently determined the number of complexes against which we could feasibly test each method, thereby restricting the size of our test datasets to 51 complexes in total. In addition, not all baselines chosen were originally trained for both types of protein complexes (i.e., homodimers and heterodimers), so for these baselines we do not include their results for the type of complex for which they are not respectively designed. To assess models' ability to accurately select residue pairs likely to be in interaction upon binding of two given chains, all methods are scored using the top-k precision metric commonly used for intra-chain contact prediction <ref type="bibr" target="#b2">(Chen et al. (2021))</ref>, where k ∈ {10, L/10, L/5} with L being the length of the shortest chain in a given complex. Formally, our definition of a model's top-k precision prec k , where T pos k represents the number of true positive residue pairs selected from the top-k most probable pairs, is</p><formula xml:id="formula_0">prec k = T pos k k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Discussion</head><p>For brevity, in all experiments, we refer to BIPSPI (Sanchez-Garcia et al. Table <ref type="table" target="#tab_0">1</ref> demonstrates that DeepInteract outperforms existing state-of-the-art methods for interface contact prediction on DIPS-Plus with both types of protein complexes, homodimers (homo) where the two chains are of the same protein and heterodimers (hetero) where the two chains are from  Since future users of DeepInteract may want to predict interface contacts for either type of complex, we consider a method's type-averaged top-k precision as an important metric for which to optimize.</p><p>Likewise, Tables <ref type="table" target="#tab_3">3 and 4</ref> present the average top-k precision of DeepInteract on 19 challenging protein complexes (14 homodimers and 5 heterodimers) from the 13th and 14th rounds of the joint CASP-CAPRI meeting. In them, we once again see DeepInteract exceed the precision of stateof-the-art interface contact predictors for both complex types. In addition, we see that combining DeepInteract with the Geometric Transformer offers improvements to top-k precision for both homodimers and heterodimers compared to using either a GCN or a Graph Transformer-based GNN backbone. Such a result confirms our hypothesis that the Geometric Transformer's geometric selfattention mechanism can enable enhanced prediction performance for downstream tasks on 3D protein structures, using interface contact prediction as a case study.</p><p>An interesting future experiment to run involves using the structures of chains generated by Al-phaFold2 <ref type="bibr" target="#b15">(Jumper et al. (2021)</ref>) as the inputs to our DeepInteract models while benchmarking on publicly available CASP-CAPRI 13-14 targets. Doing so, however, would require extensive finetuning of AlphaFold2's input feature generation to ensure that no CASP-CAPRI target overlaps widely with a target included in AlphaFold2's training dataset. As such, we defer an exploration of this idea to future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we presented a DeepInteract which introduces the geometry-evolving Geometric Transformer for protein structures and showcases its use in predicting inter-chain residue-residue interactions in protein complexes. We foresee several other uses of the Geometric Transformer in geometric deep learning of protein structures such as protein tertiary and quaternary structure quality assessment and refinement as well as residue disorder prediction, simply to name a few. One limitation of the existing design for the Geometric Transformer is the O(n 2 ) computational complexity associated with its dot product self-attention mechanism. With the advent of more efficient alternatives to self-attention, we hope to overcome this limitation with sparse attention variants like those of the Nyströmformer <ref type="bibr" target="#b34">(Xiong et al. (2021)</ref>) in future works. In addition, for future work, we plan to investigate how to integrate the Geometric Transformer's conformation module for evolving geometric representations into equivariant machine learning architectures such as the Equivariant Graph Neural Network (EGNN) by <ref type="bibr" target="#b26">Satorras et al. (2021)</ref>. Equivariant updates to rich geometric features is an interesting avenue for future research that we hope to explore further. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Sample Interface Contact Predictions</head><p>In the first row of Figure <ref type="figure" target="#fig_5">5</ref>, we see predictions made by DeepInteract for a homodimer complex from our test partition of DIPS-Plus (i.e., PDB ID: 4HEQ). The leftmost image represents the softmax contact probability map. The center image corresponds to the same contact map after having a 0.5 probability threshold applied to it such that residue pairs with at least a 50% probability of being in interaction with each other have their interaction probabilities rounded up to 1.0. The rightmost image is the ground-truth contact map. Similarly, in the second row of Figure <ref type="figure" target="#fig_5">5</ref>, we are shown the cropped predictions made by DeepInteract for a CASP-CAPRI test heterodimer (i.e., PDB ID: 6TRI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Definition of Edge Geometric Features</head><p>Similar to <ref type="bibr" target="#b12">Ingraham et al. (2019)</ref>, we construct a local reference frame (i.e., an orientation O i ) for each protein chain graph's residues. Representing each residue by its Cartesian coordinates x i , we formally define</p><formula xml:id="formula_1">u i = x i − x i−1 x i − x i−1 , n i = u i × u i+1 u i × u i+1 , b i = u i − u i+1 u i − u i+1 .</formula><p>with n i being the unit vector normal to the plane formed by the rays (x i−1 − x i ) and (x i+1 − x i ) and b i being the negative bisector of this plane. We then define O i as</p><formula xml:id="formula_2">O i = [b i n i b i × n i ].</formula><p>Having defined the orientation O i for each residue that describes the local reference frame (x i , O i ).</p><p>To provide the Geometric Transformer with an alternative notion of residue-residue orientations, we define the unit vector normal to the amide plane for residue i as</p><formula xml:id="formula_3">U i = (x Cαi − x Cβi ) × (x Cβi − x Ni )</formula><p>where x Cαi , x Cβi , and x N i are the Cartesian coordinates of the residue's carbon-alpha (Cα), carbon-beta (Cβ), and nitrogen (N ) atoms, respectively. Finally, we relate the reference frames for residues i and j by describing their edge geometric features as</p><formula xml:id="formula_4">r( x j − x i ), O T i x j − x i x j − x i , q(O T i O j ), a(U i , U j )</formula><p>with the first term r() being a distance encoding of 16 Gaussian RBFs spaced isotropically from 0 to 20 Å, the second term describing the relative direction of x j with respect to reference frame (x i , O i ), the third term detailing an orientation encoding q() of the quaternion representation of the rotation matrix O T i O j , representing each quaternion with respect to its vector of real coefficients, and the fourth term a() representing the angle between the amide plane normal vectors U i and U j .</p><p>Our definition of these edge geometric features makes use of the backbone atoms for each residue. As such, the graph representation of protein chains we use with the Geometric Transformer encodes not only residue-level geometric features but also those derived from an atomic view of protein structures. We hypothesized this hybrid approach to modeling protein structure geometries would have a noticeable downstream effect on interface contact prediction precision via the node and edge representations learned by the Geometric Transformer. This hypothesis is confirmed in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Protein Complexes Selected for Testing</head><p>Table <ref type="table" target="#tab_4">5</ref> displays the PDB and chain IDs of DIPS-Plus protein complexes chosen for testing. Likewise, in Table <ref type="table">6</ref>, we see the PDB and chain IDs of CASP-CAPRI 13-14 targets chosen for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Alternative Networks for the Interaction Module</head><p>We, like <ref type="bibr" target="#b20">Liu et al. (2020)</ref>, note that the task of interface prediction bears striking similarities to dense prediction tasks in computer vision (e.g., semantic segmentation). In this train of thought, we experimented with several semantic segmentation models as replacements for our interaction module's dilated ResNet, one namely being DeepLabV3Plus <ref type="bibr" target="#b3">(Chen et al. (2018)</ref>). We observed a strong propensity of such semantic segmentation models to identify interaction regions well but to do so with low pixel-wise precision. We hypothesize this is due to the downsampling and upsampling methods often employed within such architectures that invariably degrade the original input tensor's representation resolution. We also experimented with several state-of-the-art Vision Transformer and MLP-based models for computer vision but ultimately found their algorithmic complexity, memory usage, or input shape requirements to be prohibitive for this task, since our test datasets' input protein complexes can vary greatly in size to contain between 20 residues and over</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A Mol* (Sehnal et al. (2021)) visualization of interacting protein chains (PDB ID: 3H11).</figDesc><graphic url="image-1.png" coords="2,207.00,72.00,197.97,150.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: DeepInteract overview. Our proposed pipeline separates interface contact prediction into two tasks: (1) learning new node representations from pairs of residue protein graphs inductively and (2) convolving over the new node representations interleaved together to predict pairwise contact probabilities.</figDesc><graphic url="image-2.png" coords="4,108.00,72.00,396.00,132.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Geometric Transformer overview. The Geometric Transformer uses the Graph Transformer as its foundational architecture. Notably, our final layer of the Geometric Transformer removes the edge update path in the above figure since, in our formulation of the interface prediction problem, only node representations are directly used for the final interface contact prediction.</figDesc><graphic url="image-3.png" coords="5,108.00,72.00,396.00,218.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Conformation module overview. The Geometric Transformer uses a conformation module in each layer for evolving protein structure information via repeated gating and finally a series of residual connection blocks.</figDesc><graphic url="image-4.png" coords="6,108.00,72.00,396.00,140.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(2018)), DeepHomo<ref type="bibr" target="#b35">(Yan &amp; Huang (2021)</ref>), ComplexContact<ref type="bibr" target="#b37">(Zeng et al. (2018)</ref>), and DeepInteract as BI, DH, CC, and DI, respectively. In addition, we refer to the Graph Convolutional Network of<ref type="bibr" target="#b17">Kipf &amp; Welling (2016)</ref>, the Graph Transformer of<ref type="bibr" target="#b5">Dwivedi &amp; Bresson (2021)</ref>, and the Geometric Transformer as GCN, GraphTran, and GeoTran, respectively. Each method predicts interfacing residue pairs subject to the (on average) 1:1000 positive-negative class imbalance imposed by the biological sparsity of true interface contacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: The network's softmax contact probabilities (leftmost column), 0.5 positive probabilitythresholded predictions (middle column), and ground-truth labels (rightmost column), respectively, for PDB ID: 4HEQ (first row) and 6TRI (second row), two of the complexes in our test datasets.</figDesc><graphic url="image-8.png" coords="14,155.01,170.20,99.01,97.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The average interface prediction precision on two types of DIPS-Plus test targets.</figDesc><table><row><cell></cell><cell></cell><cell>16 (Homo)</cell><cell></cell><cell></cell><cell>16 (Hetero)</cell><cell></cell></row><row><cell>Method</cell><cell>10</cell><cell>L/10</cell><cell>L/5</cell><cell>10</cell><cell>L/10</cell><cell>L/5</cell></row><row><cell>BI</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.02</cell><cell>0.02</cell><cell>0.02</cell></row><row><cell>DH</cell><cell>0.13</cell><cell>0.12</cell><cell>0.09</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CC</cell><cell></cell><cell></cell><cell></cell><cell>0.18</cell><cell>0.16</cell><cell>0.16</cell></row><row><cell>DI (GCN)</cell><cell>0.23 (0.03)</cell><cell>0.21 (0.03)</cell><cell>0.21 (0.01)</cell><cell>0.1 (0.03)</cell><cell>0.1 (0.02)</cell><cell>0.1 (0.02)</cell></row><row><cell>DI (GraphTran)</cell><cell>0.25 (0.06)</cell><cell>0.25 (0.04)</cell><cell>0.21 (0.05)</cell><cell>0.12 (0.03)</cell><cell>0.11 (0.03)</cell><cell>0.09 (0.02)</cell></row><row><cell>DI (GeoTran)</cell><cell>0.25 (0.03)</cell><cell>0.24 (0.03)</cell><cell>0.21 (0.01)</cell><cell>0.19 (0.04)</cell><cell>0.17 (0.04)</cell><cell>0.16 (0.03)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The average interface prediction precision on DIPS-Plus test targets of both types.</figDesc><table><row><cell></cell><cell></cell><cell>32 (Both Types)</cell><cell></cell></row><row><cell>Method</cell><cell>10</cell><cell>L/10</cell><cell>L/5</cell></row><row><cell>BI</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>DI (GCN)</cell><cell>0.18 (0.005)</cell><cell>0.17 (0.004)</cell><cell>0.16 (0.01)</cell></row><row><cell>DI (GraphTran)</cell><cell>0.2 (0.02)</cell><cell>0.18 (0.02)</cell><cell>0.15 (0.02)</cell></row><row><cell>DI (GeoTran)</cell><cell>0.22 (0.02)</cell><cell>0.2 (0.02)</cell><cell>0.17 (0.02)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The average interface prediction precision on dimers from CASP-CAPRI 13 &amp; 14.</figDesc><table><row><cell></cell><cell></cell><cell>14 (Homo)</cell><cell></cell><cell></cell><cell>5 (Hetero)</cell><cell></cell></row><row><cell>Method</cell><cell>10</cell><cell>L/10</cell><cell>L/5</cell><cell>10</cell><cell>L/10</cell><cell>L/5</cell></row><row><cell>BI</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.04</cell><cell>0</cell><cell>0.03</cell></row><row><cell>DH</cell><cell>0.02</cell><cell>0.02</cell><cell>0.02</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CC</cell><cell></cell><cell></cell><cell></cell><cell>0.06</cell><cell>0.08</cell><cell>0.05</cell></row><row><cell>DI (GCN)</cell><cell>0.12 (0.01)</cell><cell>0.1 (0.02)</cell><cell>0.09 (0.03)</cell><cell>0.1 (0.04)</cell><cell>0.1 (0.03)</cell><cell>0.09 (0.01)</cell></row><row><cell>DI (GraphTran)</cell><cell>0.07 (0.04)</cell><cell>0.07 (0.04)</cell><cell>0.07 (0.03)</cell><cell>0.2 (0.02)</cell><cell>0.2 (0.02)</cell><cell>0.17 (0.02)</cell></row><row><cell>DI (GeoTran)</cell><cell>0.14 (0.03)</cell><cell>0.13 (0.01)</cell><cell>0.11 (0.02)</cell><cell>0.23 (0.04)</cell><cell>0.23 (0.03)</cell><cell>0.20 (0.04)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The average interface prediction precision across all targets from CASP-CAPRI 13 &amp; 14. Table2shows the average precision of DeepInteract across both complex types.</figDesc><table><row><cell></cell><cell></cell><cell>19 (Both Types)</cell><cell></cell></row><row><cell>Method</cell><cell>10</cell><cell>L/10</cell><cell>L/5</cell></row><row><cell>BI</cell><cell>0.01</cell><cell>0</cell><cell>0.01</cell></row><row><cell>DI (GCN)</cell><cell>0.11 (0.02)</cell><cell>0.1 (0.03)</cell><cell>0.09 (0.02)</cell></row><row><cell cols="2">DI (GraphTran) 0.11 (0.04)</cell><cell>0.1 (0.04)</cell><cell>0.1 (0.03)</cell></row><row><cell>DI (GeoTran)</cell><cell>0.17 (0.03)</cell><cell>0.16 (0.02)</cell><cell>0.13 (0.01)</cell></row><row><cell>different proteins.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The protein complexes selected from DIPS-Plus for testing interface contact predictors.</figDesc><table><row><cell cols="7">PDB ID Chain 1 Chain 2 Type PDB ID Chain 1 Chain 2</cell><cell>Type</cell></row><row><cell>1BHN</cell><cell>B</cell><cell>D</cell><cell>Homo</cell><cell>1AON</cell><cell>R</cell><cell>S</cell><cell>Hetero</cell></row><row><cell>1KPT</cell><cell>A</cell><cell>B</cell><cell>Homo</cell><cell>1BE3</cell><cell>D</cell><cell>E</cell><cell>Hetero</cell></row><row><cell>1SDU</cell><cell>A</cell><cell>B</cell><cell>Homo</cell><cell>1GK8</cell><cell>K</cell><cell>M</cell><cell>Hetero</cell></row><row><cell>1UZN</cell><cell>A</cell><cell>B</cell><cell>Homo</cell><cell>1OCZ</cell><cell>R</cell><cell>V</cell><cell>Hetero</cell></row><row><cell>2B4H</cell><cell>A</cell><cell>B</cell><cell cols="2">Homo 1UWA</cell><cell>A</cell><cell>I</cell><cell>Hetero</cell></row><row><cell>2G30</cell><cell>C</cell><cell>E</cell><cell>Homo</cell><cell>3A6N</cell><cell>A</cell><cell>E</cell><cell>Hetero</cell></row><row><cell>2GLM</cell><cell>E</cell><cell>F</cell><cell cols="2">Homo 3ABM</cell><cell>D</cell><cell>K</cell><cell>Hetero</cell></row><row><cell>2IUO</cell><cell>D</cell><cell>J</cell><cell>Homo</cell><cell>3JRM</cell><cell>H</cell><cell>I</cell><cell>Hetero</cell></row><row><cell>3BXS</cell><cell>A</cell><cell>B</cell><cell>Homo</cell><cell>3MG6</cell><cell>D</cell><cell>E</cell><cell>Hetero</cell></row><row><cell>3CT7</cell><cell>B</cell><cell>E</cell><cell cols="2">Homo 3MNN</cell><cell>C</cell><cell>F</cell><cell>Hetero</cell></row><row><cell>3NUT</cell><cell>A</cell><cell>D</cell><cell>Homo</cell><cell>3T1Y</cell><cell>E</cell><cell>H</cell><cell>Hetero</cell></row><row><cell>3RE3</cell><cell>B</cell><cell>C</cell><cell>Homo</cell><cell>3TUY</cell><cell>D</cell><cell>E</cell><cell>Hetero</cell></row><row><cell>4HEQ</cell><cell>A</cell><cell>B</cell><cell cols="2">Homo 3VYG</cell><cell>G</cell><cell>H</cell><cell>Hetero</cell></row><row><cell>4LIW</cell><cell>A</cell><cell>B</cell><cell>Homo</cell><cell>4A3D</cell><cell>C</cell><cell>L</cell><cell>Hetero</cell></row><row><cell>4OTA</cell><cell>D</cell><cell>F</cell><cell>Homo</cell><cell>4CW7</cell><cell>G</cell><cell>H</cell><cell>Hetero</cell></row><row><cell>4TO9</cell><cell>B</cell><cell>D</cell><cell>Homo</cell><cell>4DR5</cell><cell>G</cell><cell>I</cell><cell>Hetero</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The project is partially supported by two NSF grants (DBI 1759934 and IIS 1763246), one NIH grant (GM093123), three DOE grants (DE-SC0020400, DE-AR0001213, and DE-SC0021303), and the computing allocation on the Summit compute cluster provided by Oak Ridge Leadership Computing Facility (Contract No. DE-AC05-00OR22725).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>DeepInteract is designed to be used for machine learning of protein molecular data. It makes use of only publicly available information concerning biomolecular structures and their interactions. Consequently, all data used to create DeepInteract models do not contain any personally identifiable information or offensive content. As such, we do not foresee negative societal impacts as a consequence of DeepInteract or the Geometric Transformer being made publicly available. Furthermore, future adaptions or enhancements to DeepInteract may benefit the machine learning community and, more broadly, the scientific community by providing meaningful refinements to a transparent and extensible pipeline for geometric deep learning of protein-protein interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reproducibility Statement</head><p>To enable this work to be reproducible by others, we have thoroughly documented two methods for running predictions with DeepInteract using a provided PyTorch Lightning model checkpoint (Falcon ( <ref type="formula">2019</ref>)) and one method for training DeepInteract models in an associated GitHub repository. The most convenient prediction method describes how to run DeepInteract as a platform-agnostic Docker container. The second method, for both training and prediction, details how users can piecewise install DeepInteract and its data and software dependencies on a Linux-based operating system. An anonymized version of this GitHub repository, with version history and personally identifiable artifacts removed, has been uploaded alongside this manuscript at the time of submission. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Hardware Used</head><p>The Oak Ridge Leadership Facility (OLCF) at the Oak Ridge National Laboratory (ORNL) is an open science computing facility that supports HPC research. The OLCF houses the Summit compute cluster. Summit, launched in 2018, delivers 8 times the computational performance of Titan's 18,688 nodes, using only 4,608 nodes. Like Titan, Summit has a hybrid architecture, and each node contains multiple IBM POWER9 CPUs and NVIDIA Volta GPUs all connected with NVIDIA's high-speed NVLink. Each node has over half a terabyte of coherent memory (high bandwidth memory + DDR4) addressable by all CPUs and GPUs plus 800GB of non-volatile RAM that can be used as a burst buffer or as extended memory. To provide a high rate of I/O throughput, the nodes are connected in a non-blocking fat-tree using a dual-rail Mellanox EDR InfiniBand interconnect. We used the Summit compute cluster to train all our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Software Used</head><p>In addition, we used Python 3. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Partner-aware prediction of interacting residues in proteinprotein complexes from sequence data</title>
		<author>
			<persName><forename type="first">Shandar</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Mizuguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">e29104</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Geometric deep learning: Grids, groups, graphs, geodesics, and gauges</title>
		<author>
			<persName><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><surname>Veličković</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combination of deep neural network with attention mechanism enhances the explainability of protein contact prediction</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiye</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.26052</idno>
		<ptr target="https://doi.org/10.1002/prot.26052.URLhttps://onlinelibrary.wiley.com/doi/abs/10.1002/prot.26052" />
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="697" to="707" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
				<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Protein Interaction Interface Region Prediction by Geometric Deep Learning</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Bailey-Kellogg</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btab154</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btab154.btab154" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<idno type="ISSN">1367-4803</idno>
		<imprint>
			<date type="published" when="2021">03 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A generalization of transformer networks to graphs</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dwivedi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<ptr target="https://github.com/PyTorchLightning/pytorch-lightning" />
		<imprint>
			<date type="published" when="2019">2021. 2019</date>
			<biblScope unit="volume">3</biblScope>
			<pubPlace>Falcon, WA</pubPlace>
		</imprint>
	</monogr>
	<note>Pytorch lightning. GitHub. Note</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Protein interface prediction using graph convolutional networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basir</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asa</forename><surname>Ben-Hur</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/f507783927f2ec2737ba40afbd17efb5-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6530" to="6539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Gainza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Freyr</forename><surname>Sverrisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><surname>Correia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="184" to="192" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving the gating mechanism of recurrent neural networks</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3800" to="3809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Edge-augmented graph transformers: Global self-attention is enough for graphs</title>
		<author>
			<persName><forename type="first">Md</forename><surname>Shamim Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharmashankar</forename><surname>Subramanian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.03348</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative models for graphbased protein design</title>
		<author>
			<persName><forename type="first">John</forename><surname>Ingraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/f3a4ff4839c56a5f460c88cce3666a2b-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilson</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05407</idno>
		<title level="m">Averaging weights leads to wider optima and better generalization</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting proteinprotein interface residues using local surface structural similarity</title>
		<author>
			<persName><forename type="first">El-Manzalawy</forename><surname>Rafael A Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drena</forename><surname>Yasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasant</forename><surname>Dobbs</surname></persName>
		</author>
		<author>
			<persName><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with alphafold</title>
		<author>
			<persName><forename type="first">John</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustin</forename><surname>Žídek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Blind prediction of homo-and hetero-protein complexes: The casp13-capri experiment</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Marc F Lensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nurul</forename><surname>Brysbaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Nadzirin</surname></persName>
		</author>
		<author>
			<persName><surname>Velankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Raphaël</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tereza</forename><surname>Chaleil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">A</forename><surname>Gerguri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elodie</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandra</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Carbone</surname></persName>
		</author>
		<author>
			<persName><surname>Grudinin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1200" to="1221" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Prediction of protein assemblies, the next frontier: The casp14-capri experiment</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Marc F Lensink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Théo</forename><surname>Brysbaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nurul</forename><surname>Mauri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Nadzirin</surname></persName>
		</author>
		<author>
			<persName><surname>Velankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Raphael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tereza</forename><surname>Chaleil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">A</forename><surname>Clarence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning of high-order interactions for protein interface prediction</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="679" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Spherical message passing for 3d graph networks</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bora</forename><surname>Oztekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05013</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Dips-plus: The enhanced database of interacting protein structures for interface prediction</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Morehead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ada</forename><surname>Sedova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Network analysis and in silico prediction of protein-protein interactions with applications in drug discovery</title>
		<author>
			<persName><forename type="first">Yoichi</forename><surname>Murakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lokesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Prathipati</surname></persName>
		</author>
		<author>
			<persName><surname>Mizuguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current opinion in structural biology</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="134" to="142" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BIPSPI: a method for the prediction of partner-specific protein-protein interfaces</title>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Sanchez-Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>O S Sorzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J M</forename><surname>Carazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Segura</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/bty647</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/bty647" />
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<idno type="ISSN">1367-4803</idno>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="470" to="477" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">E(n) equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">Garcia</forename><surname>Víctor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v139/satorras21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07-24">18-24 Jul 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="9323" to="9332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mol* Viewer: modern web app for 3D visualization and analysis of large biomolecular structures</title>
		<author>
			<persName><forename type="first">David</forename><surname>Sehnal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Bittrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radka</forename><surname>Svobodová</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Berka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Václav</forename><surname>Bazgier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Velankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">K</forename><surname>Burley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaroslav</forename><surname>Koča</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Rose</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gkab314</idno>
		<ptr target="https://doi.org/10.1093/nar/gkab314" />
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<idno type="ISSN">0305-1048</idno>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">W1</biblScope>
			<biblScope unit="page" from="W431" to="W437" />
			<date type="published" when="2021-05">05 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hh-suite3 for fast remote homology detection and deep protein annotation</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milot</forename><surname>Mirdita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harald</forename><surname>Vöhringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><forename type="middle">J</forename><surname>Haunsberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Söding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end learning on 3d protein structure for interface prediction</title>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Townshend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Suriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Dror</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/6c7de1f27f7de61a6daddfffbe05c058-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="15642" to="15651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Python 3 Reference Manual</title>
		<author>
			<persName><forename type="first">Guido</forename><surname>Van Rossum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><forename type="middle">L</forename><surname>Drake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>CreateSpace, Scotts Valley, CA</pubPlace>
		</imprint>
	</monogr>
	<note>ISBN 1441412697</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Updates to the integrated protein-protein interaction benchmarks: docking benchmark version 5 and affinity benchmark version 2</title>
		<author>
			<persName><forename type="first">Thom</forename><surname>Vreven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><forename type="middle">H</forename><surname>Moal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Vangone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">G</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Panagiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mieczyslaw</forename><surname>Kastritis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Torchala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Chaleil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">A</forename><surname>Jiménez-García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><surname>Fernandez-Recio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">427</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="3031" to="3041" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reaching for high-hanging fruit in drug discovery at protein-protein interfaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">L</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><surname>Mcclendon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">450</biblScope>
			<biblScope unit="issue">7172</biblScope>
			<biblScope unit="page" from="1001" to="1009" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">omformer: A nystr\&quot; om-based algorithm for approximating self-attention</title>
		<author>
			<persName><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanpeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rudrasis</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Nystr\</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03902</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Accurate prediction of inter-protein residue-residue contacts for homo-oligomeric protein complexes</title>
		<author>
			<persName><forename type="first">Yumeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng-You</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Protein-ligand binding site recognition using complementary binding-specific substructure comparison and sequence profile alignment</title>
		<author>
			<persName><forename type="first">Jianyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrish</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="2588" to="2595" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Complexcontact: a web server for inter-protein contact prediction using deep learning</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feifeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiufeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">W1</biblScope>
			<biblScope unit="page" from="W432" to="W437" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
