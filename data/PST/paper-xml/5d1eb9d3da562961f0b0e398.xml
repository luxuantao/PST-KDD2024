<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Learning of Object Structure and Dynamics from Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Google AI Resident 33rd Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2019)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
							<email>chensun@google.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Google AI Resident 33rd Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2019)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Google AI Resident 33rd Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2019)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Forrester</forename><surname>Cole</surname></persName>
							<email>fcole@google.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Google AI Resident 33rd Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2019)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
							<email>kpmurphy@google.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Google AI Resident 33rd Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2019)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
							<email>honglak@google.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Google AI Resident 33rd Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2019)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Learning of Object Structure and Dynamics from Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4B745391CD764DE1DEC9C9BDE81D6EFE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Extracting and predicting object structure and dynamics from videos without supervision is a major challenge in machine learning. To address this challenge, we adopt a keypoint-based image representation and learn a stochastic dynamics model of the keypoints. Future frames are reconstructed from the keypoints and a reference frame. By modeling dynamics in the keypoint coordinate space, we achieve stable learning and avoid compounding of errors in pixel space. Our method improves upon unstructured representations both for pixel-level video prediction and for downstream tasks requiring object-level understanding of motion dynamics. We evaluate our model on diverse datasets: a multi-agent sports dataset, the Human3.6M dataset, and datasets based on continuous control tasks from the DeepMind Control Suite. The spatially structured representation outperforms unstructured representations on a range of motion-related tasks such as object tracking, action recognition and reward prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Videos provide rich visual information to understand the dynamics of the world. However, extracting a useful representation from videos (e.g. detection and tracking of objects) remains challenging and typically requires expensive human annotations. In this work, we focus on unsupervised learning of object structure and dynamics from videos.</p><p>One approach for unsupervised video understanding is to learn to predict future frames <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14]</ref>. Based on this body of work, we identify two main challenges: First, it is hard to make pixel-level predictions because motion in videos becomes highly stochastic for horizons beyond about a second. Since semantically insignificant deviations can lead to large error in pixel space, it is often difficult to distinguish good from bad predictions based on pixel losses. Second, even if good pixel-level prediction is achieved, this is rarely the desired final task. The representations of a model trained for pixel-level reconstruction are not guaranteed to be useful for downstream tasks such as tracking, motion prediction and control.</p><p>Here, we address both of these challenges by using an explicit, interpretable keypoint-based representation of object structure as the core of our model. Keypoints are a natural representation of dynamic objects, commonly used for face and pose tracking. Training keypoint detectors, however, generally requires supervision. We learn the keypoint-based representation directly from video, without any supervision beyond the pixel data, in two steps: first encode individual frames to keypoints, then model the dynamics of those points. As a result, the representation of the dynamics model is spatially structured, though the model is trained only with a pixel reconstruction loss. We show that enforcing spatial structure significantly improves video prediction quality and performance for tasks such as action recognition and reward prediction.</p><p>By decoupling pixel generation from dynamics prediction, we avoid compounding errors in pixel space because we never condition on predicted pixels. This approach has been shown to be beneficial for supervised video prediction <ref type="bibr" target="#b24">[25]</ref>. Furthermore, modeling dynamics in keypoint coordinate space allows us to sample and evaluate predictions efficiently. Errors in coordinate space are more meaningful than in pixel space, since distance between keypoints is more closely related to semantically relevant differences than pixel-space distance. We exploit this by using a best-of-manysamples objective <ref type="bibr" target="#b3">[4]</ref> during training to achieve stochastic predictions that are both highly diverse and of high quality, outperforming the predictions of models lacking spatial structure.</p><p>Finally, because we build spatial structure into our model a priori, its internal representation is biased to contain object-level information that is useful for downstream applications. This bias leads to better results on tasks such as trajectory prediction, action recognition and reward prediction.</p><p>Our contributions are: (1) a novel architecture and optimization techniques for unsupervised video prediction with a structured internal representation; (2) a model that outperforms recent work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28]</ref> and our unstructured baseline in pixel-level video prediction; (3) improved performance vs. unstructured models on downstream tasks requiring object-level understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Unsupervised learning of keypoints. Previous work explores learning to find keypoints in an image by applying an autoencoding architecture with keypoint-coordinates as a representational bottleneck <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">33]</ref>. The bottleneck forces the image to be encoded in a small number of points. We build on these methods by extending them to the video setting.</p><p>Stochastic sequence prediction. Successful video prediction requires modeling uncertainty. We adopt the VRNN <ref type="bibr" target="#b5">[6]</ref> architecture, which adds latent random variables to the standard RNN architecture, to sample from possible futures. More sophisticated approaches to stochastic prediction of keypoints have been recently explored <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b20">21]</ref>, but we find the basic VRNN architecture sufficient for our applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised video prediction.</head><p>A large body of work explores learning to predict video frames using only a pixel-reconstruction loss <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b6">7]</ref>. Most similar to our work are approaches that perform deterministic image generation from a latent sample produced by stochastic sampling from a prior conditioned on previous timesteps <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14]</ref>. Our approach replaces the unstructured image representation with a structured set of keypoints, improving performance on video prediction and downstream tasks compared with SVG <ref type="bibr" target="#b7">[8]</ref> (Section 5).</p><p>Recent methods also apply adversarial training to improve prediction quality and diversity of samples <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b13">14]</ref>. EPVA <ref type="bibr" target="#b27">[28]</ref> predicts dynamics in a high-level feature space and applies an adversarial loss to the predicted features. We compare against EPVA and show improvement without adversarial training, but adversarial training is compatible with our method and is a promising future direction.</p><p>Video prediction with spatially structured representations. Like our approach, several recent methods explore explicit, spatially structured representations for video prediction. Xu et al. <ref type="bibr" target="#b28">[29]</ref> proposed to discover object parts and structure by watching how they move in videos. Vid2Vid <ref type="bibr" target="#b26">[27]</ref> proposed a video-to-video translation network from segmentation masks, edge masks and human pose. The method is also used for predicting a few frames into the future by predicting the structure representations first. Villegas et al. <ref type="bibr" target="#b24">[25]</ref> proposed to train a human pose predictor and then use the predicted pose to generate future frames of human motion. In <ref type="bibr" target="#b25">[26]</ref>, a method is proposed where future human pose is predicted using a stochastic network and the pose is then used to generate future frames. Recent methods on video generation have used spatially structured representations for video motion transfer between humans <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>. In contrast, our model is able to find spatially structured representation without supervision while using video frames as the only learning signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Architecture</head><p>Our model is composed of two parts: a keypoint detector that encodes each frame into a lowdimensional, keypoint-based representation, and a dynamics model that predicts dynamics in the keypoint space (Figure <ref type="figure">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stopgradient</head><p>Figure <ref type="figure">1</ref>: Architecture of our model. Variables are black, functions blue, losses red. Some arrows are omitted for clarity, see Equations 1 to 4 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unsupervised keypoint detector</head><p>The keypoint detection architecture is inspired by <ref type="bibr" target="#b11">[12]</ref>, which we adapt for the video setting. Let v 1:T ∈ R H×W ×C be a video sequence of length T . Our goal is to learn a keypoint detector ϕ det (v t ) = x t that captures the spatial structure of the objects in each frame in a set of keypoints x t .</p><p>The detector ϕ det is a convolutional neural network that produces K feature maps, one for each keypoint. Each feature map is normalized and condensed into a single (x, y)-coordinate by computing the spatial expectation of the map. The number of heatmaps K is a hyperparameter that represents the maximum expected number of keypoints necessary to model the data.</p><p>For image reconstruction, we learn a generator ϕ rec that reconstructs frame v t from its keypoint representation. The generator also receives the first frame of the sequence v 1 to capture the static appearance of the scene: v t = ϕ rec (v 1 , x t ). Together, the keypoint detector ϕ det and generator ϕ rec form an autoencoder architecture with a representational bottleneck that forces the structure of each frame to be encoded in a keypoint representation <ref type="bibr" target="#b11">[12]</ref>.</p><p>The generator is also a convolutional neural network. To supply the keypoints to the network, each point is converted into a heatmap with a Gaussian-shaped blob at the keypoint location. The K heatmaps are concatenated with feature maps from the first frame v 1 . We also concatenate the keypoint-heatmaps for the first frame v 1 to the decoder input for subsequent frames v t , to help the decoder to "inpaint" background regions that were occluded in the first frame. The resulting tensor forms the input to the generator. We add skip connections from the first frame of the sequence to the generator output such that the actual task of the generator is to predict v t -v 1 .</p><p>We use the mean intensity µ k of each keypoint feature map returned by the detector as a continuousvalued indicator of the presence of the modeled object. When converting keypoints back into heatmaps, each map is scaled by the corresponding µ k . The model can use µ k to encode the presence or absence of individual objects on a frame-by-frame basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Stochastic dynamics model</head><p>To model the dynamics in the video, we use a variational recurrent neural network (VRNN) <ref type="bibr" target="#b5">[6]</ref>. The core of the dynamics model is a latent belief z over keypoint locations x. In the VRNN architecture, the prior belief is conditioned on all previous timesteps through the hidden state h t-1 of an RNN, and thus represents a prediction of the current keypoint locations before observing the image:</p><formula xml:id="formula_0">p(z t |x &lt;t , z &lt;t ) = ϕ prior (h t-1 )<label>(1)</label></formula><p>We obtain the posterior belief by combining the previous hidden state with the unsupervised keypoint coordinates x t = ϕ det (v t ) detected in the current frame:</p><formula xml:id="formula_1">q(z t |x ≤t , z &lt;t ) = ϕ enc (h t-1 , x t )<label>(2)</label></formula><p>Predictions are made by decoding the latent belief:</p><formula xml:id="formula_2">p(x t |z ≤t , x &lt;t ) = ϕ dec (z t , h t-1 )<label>(3)</label></formula><p>Finally, the RNN is updated to pass information forward in time:</p><formula xml:id="formula_3">h t = ϕ RNN (x t , z t , h t-1 ).<label>(4)</label></formula><p>Note that to compute the posterior (Eq. 2), we obtain x t from the keypoint detector, but for the recurrence in Eq. 4, we obtain x t by decoding the latent belief. We can therefore predict into the future without observing images by decoding x t from the prior belief. Because the model has both deterministic and stochastic pathways across time, predictions can account for long-term dependencies as well as future uncertainty <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b5">6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Keypoint detector</head><p>The keypoint detector is trained with a simple L2 image reconstruction loss</p><formula xml:id="formula_4">L image = t ||v -v|| 2 2</formula><p>, where v is the true and v is the reconstructed image. Errors from the dynamics model are not backpropagated into the keypoint detector. <ref type="foot" target="#foot_0">2</ref>Ideally, the representation should use as few keypoints as possible to encode each object. To encourage such parsimony, we add two additional losses to the keypoint detector:</p><p>Temporal separation loss. Image features whose motion is highly correlated are likely to belong to the same object and should ideally be represented jointly by a single keypoint. We therefore add a separation loss that encourages keypoint trajectories to be decorrelated in time. The loss penalizes "overlap" between trajectories within a Gaussian radius σ sep :</p><formula xml:id="formula_5">L sep = k k exp(- d kk 2σ 2 sep )<label>(5)</label></formula><p>where</p><formula xml:id="formula_6">d kk = 1 T t ||(x t,k -x k ) -(x t,k -x k )|| 2 2</formula><p>is the distance between the trajectories of keypoints k and k , computed after subtracting the temporal mean x from each trajectory. || • || 2  2 denotes the squared Euclidean norm. Keypoint sparsity loss. For similar reasons, we add an L1 penalty L sparse = k |µ k | on the keypoint scales µ to encourage keypoints to be sparsely active.</p><p>In Section 5.3, we show that both L sep and L sparse contribute to stable keypoint detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dynamics model</head><p>The standard VRNN <ref type="bibr" target="#b5">[6]</ref> is trained to encode the detected keypoints by maximizing the evidence lower bound (ELBO), which is composed of a reconstruction loss and a KL term between the Gaussian prior N prior t = N (z t |ϕ prior (h t-1 )) and posterior distribution</p><formula xml:id="formula_7">N enc t = N (z t |ϕ enc (h t-1 , x t )): L VRNN = - T t=1 E log p(x t |z ≤t , x &lt;t ) -βKL(N enc t N prior t )<label>(6)</label></formula><p>The KL term regularizes the latent representation. In the VRNN architecture, it is also responsible for training the RNN, since it encourages the prior to predict the posterior based on past information. To balance reliance on predictions with fidelity to observations, we add the hyperparameter β (see also <ref type="bibr" target="#b1">[2]</ref>). We found it essential to tune β for each dataset to achieve a balance between reconstruction quality (lower β) and prediction diversity.</p><p>The KL term only trains the dynamics model for single-step predictions because the model receives observations after each step <ref type="bibr" target="#b9">[10]</ref>. To encourage learning of long-term dependencies, we add a pure reconstruction loss, without the KL term, for multiple future timesteps: The standard approach to estimate log p(x t |z ≤t , x ≤t ) in Eq. 6 and 7 is to sample a single z t . To further encourage diverse predictions, we instead use the best of a number of samples <ref type="bibr" target="#b3">[4]</ref> at each timestep during training:</p><formula xml:id="formula_8">L future = - T +∆T t=T +1 E [log p(x t |z ≤t , x ≤T )]<label>(7)</label></formula><formula xml:id="formula_9">max i log p(x t |z i,t , z &lt;t , x &lt;t ) ,<label>(8)</label></formula><p>where z i,t ∼ N enc t for observed steps and z i,t ∼ N prior t for predicted steps. By giving the model several chances to make a good prediction, it is encouraged to cover a range of likely data modes, rather than just the most likely. Sampling and evaluating several predictions at each timestep would be expensive in pixel space. However, since we learn the dynamics in the low-dimensional keypoint space, we can evaluate sampled predictions without reconstructing pixels. Due to the keypoint structure, the L2 distance of samples from the observed keypoints meaningfully captures sample quality. This would not be guaranteed for an unstructured latent representation. As shown in Section 5, the best-of-many objective is crucial to the performance of our model.</p><p>The combined loss of the whole model is:</p><formula xml:id="formula_10">L image + λ sep L sep + λ sparse L sparse + L VRNN + L future ,<label>(9)</label></formula><p>where λ sep and λ sparse are scale parameters for the keypoint separation and sparsity losses. See Section S1 for implementation details, including a list of hyperparameters and tuning ranges (Table <ref type="table">S1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We first show that the structured representation of our model improves prediction quality on two video datasets, and then show that it is more useful than unstructured representations for downstream tasks that require object-level information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Structured representation improves video prediction</head><p>We evaluate frame prediction on two video datasets (Figure <ref type="figure" target="#fig_0">2</ref>). The Basketball dataset consists of a synthetic top-down view of a basketball court containing five offensive players and the ball, all drawn as colored dots. The videos are generated from real basketball player trajectories <ref type="bibr" target="#b31">[32]</ref>, testing the ability of our model to detect and stably represent multiple objects with complex dynamics. The dataset contains 107,146 training and 13,845 test sequences. The Human3.6 dataset <ref type="bibr" target="#b10">[11]</ref> contains video sequences of human actors performing various actions. We use subjects S1, S5, S6, S7, and S9 for training (600 videos), and subjects S9 and S11 for evaluation (239 videos). For both datasets, ground truth object coordinates are available for evaluation, but are not used by the model. The Basketball dataset contains the coordinates of each of the 5 players and the ball. The Human dataset contains 32 motion capture points, of which we select 12 for evaluation.</p><p>We compare the full model (Struct-VRNN) to a series of baselines and ablations: the Struct-VRNN (no BoM) model was trained without the best-of-many objective; the Struct-RNN is deterministic; the CNN-VRNN architecture uses the same stochastic dynamics model as the Struct-VRNN, but uses an unstructured deep feature vector as its internal representation instead of structured keypoints. All structured models use K = 12 for Basketball, and K = 48 for Human3.6M, and were conditioned on  Example sequences are the closest or furthest samples from ground truth according to VGG cosine similarity, as indicated. Note that for Struct-VRNN, even the samples furthest from ground truth are of high visual quality. Bottom left: Mean VGG cosine similarity of the the samples closest to ground truth (left) and furthest from ground truth (right). Higher is better. Plots show mean performance across 5 model initializations, with the 95% confidence interval shaded. Bottom right: Fréchet Video Distance <ref type="bibr" target="#b22">[23]</ref>, using all samples. Lower is better. Dots represents separate model initializations. EPVA <ref type="bibr" target="#b27">[28]</ref> is not stochastic, so we compare performance with a single sample from our method on their test set.</p><p>8 frames and trained to predict 8 future frames. For the CNN-VRNN, which lacks keypoint structure, we use a latent representation with 3K elements, such that its capacity is at least as large as that of the Struct-VRNN representation. Finally, we compare to three published models: SVG <ref type="bibr" target="#b7">[8]</ref>, SAVP <ref type="bibr" target="#b13">[14]</ref> and EPVA <ref type="bibr" target="#b27">[28]</ref> (Figure <ref type="figure" target="#fig_2">3</ref>).</p><p>The Struct-VRNN model matches or outperforms the other models in perceptual image and video quality as measured by VGG <ref type="bibr" target="#b18">[19]</ref> feature cosine similarity and Fréchet Video Distance <ref type="bibr" target="#b22">[23]</ref> (Figure <ref type="figure" target="#fig_2">3</ref>). Results for the lower-level metrics SSIM and PSNR are similar (see supplemental material).</p><p>The ablations suggest that the structured representation, the stochastic belief, and the best-of-many objective all contribute to model performance. The full Struct-VRNN model generates the best reconstructions of ground truth, and also generates the most diverse samples (i.e., samples that are furthest from ground truth; Figure <ref type="figure" target="#fig_2">3</ref> bottom left). In contrast, the ablated models and SVG show both lower best-case accuracy and smaller differences between closest and furthest samples, indicating less diverse samples. SAVP is closer, performing well on the single-frame metric (VGG cosine sim.), but still worse on FVD than the structured model. Qualitatively, Struct-VRNN exhibits sharper images and longer object permanence than the unstructured models (Figure <ref type="figure" target="#fig_2">3</ref>, top; note limb detail and dynamics). This is true even for the samples that are far from ground truth (Figure <ref type="figure" target="#fig_2">3</ref> top, row "Struct-VRNN (furthest)"), which suggests that our model produces diverse high-quality samples, rather than just a few good samples among many diverse but unrealistic ones. This conclusion is  backed up by the FVD (Figure <ref type="figure" target="#fig_2">3</ref> bottom right), which measures the overall quality of a distribution of videos <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">The learned keypoints track objects</head><p>We now examine how well the learned keypoints track the location of objects. Since we do not expect the keypoints to align exactly with human-labeled objects, we fit a linear regression from the keypoints to the ground truth object positions and measure trajectory prediction error on held-out sequences (Figure <ref type="figure" target="#fig_3">4</ref>). The trajectory error is the average distance between true and predicted coordinates at each timestep. To account for stochasticity, we sample 50 predictions and report the error of the best. <ref type="foot" target="#foot_1">3</ref>As a baseline, we train Struct-VRNN and CNN-VRNN models with additional supervision that forces the learned keypoints to match the ground-truth keypoints. The keypoints learned by the unsupervised Struct-VRNN model are nearly as predictive as those trained with supervision, indicating that the learned keypoints represent useful spatial information. In contrast, prediction from the internal representation of the unsupervised CNN-VRNN is poor. When trained with supervision, however, the CNN-VRNN reaches similar performance as the supervised Struct-VRNN. In other words, both the Struct-VRNN and the CNN-VRNN can learn a spatial internal representation, but the Struct-VRNN learns it without supervision.</p><p>As expected, the less diverse predictions of the Struct-VRNN (no BoM) and Struct-RNN perform worse on the coordinate regression task. Finally, for comparison, we remove the dynamics model entirely and simply predict the last observed keypoint locations for all future timepoints. All models except unsupervised CNN-VRNN outperform this baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Simple inductive biases improve object tracking</head><p>In Section 4.1, we described losses intended to add inductive biases such as keypoint sparsity and uncorrelated object trajectories to the keypoint detector. We find that these losses improve object tracking performance and stability. Figure <ref type="figure" target="#fig_4">5</ref> shows that models without L sep and L sparse show reduced video prediction and tracking performance. The increased variability between model initializations without L sep and L sparse suggests that these losses improve the learnability of a stable keypoint  structure (also see Figure <ref type="figure" target="#fig_6">S6</ref>). In summary, we find that training and final performance is most stable if K is chosen to be larger than the expected number of objects, such that the model can use µ in combination with L sparse and L sep to activate the optimal number of keypoints.  Since the learned keypoints track objects, the model's predictions can be intuitively manipulated by directly adjusting the keypoints.</p><p>On the Basketball dataset, we can explore counterfactual scenarios such as predicting how the other players react if one player moves left as opposed to right (Figure <ref type="figure" target="#fig_6">6</ref>). We simply manipulate the sequence observed keypoint locations before they are passed to the RNN, thus conditioning the RNN states and predictions on the manipulated observations.</p><p>For the Human3.6M dataset, we can independently manipulate body parts and generate poses that are not present in the training set (Figure <ref type="figure" target="#fig_8">7</ref>; please see https://mjlm.github.io/ video_structure/for videos). The model learns to associate keypoints with local areas of the body, such that moving keypoints near an arm moves the arm without changing the rest of the image. The learned keypoints are also useful for downstream tasks such as action recognition and reward prediction in reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Structured representation retains more semantic information</head><formula xml:id="formula_11">S t r u c t -V R N N ( s u p . ) S t r u c t -V R N N S t r u c t -R N N C N N -V R N N ( s u p . ) C N N -V R N N 0.00 0.25 0.50</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Action recognition accuracy</head><p>To test action recognition performance, we train a simple 3-layer RNN to classify Human3.6M actions from a sequence of keypoints (see Section S2.2 for model details).</p><p>The keypoints learned by the structured models perform better than the unstructured features learned by the CNN-VRNN (Figure <ref type="figure" target="#fig_9">8</ref>). Future prediction is not needed, so the RNN and VRNN models perform similarly.</p><p>One major application we anticipate for our model is planning and reinforcement learning of spatially defined tasks. As a first step, we trained our model on a dataset collected from six tasks in the DeepMind Control Suite (DMCS), a set of simulated continuous control environments (Figure <ref type="figure" target="#fig_10">9</ref>). Image observations and rewards were collected from the DMCS environments using random actions, and we modified our model to condition predictions on the agent's actions by feeding the actions as an additional input to the RNN. Models were trained without access to the task reward function. We used the latent state of the dynamics model as an input to a separate reward prediction model for each task (see Section S2.3 for details). The dynamics learned by the Struct-VRNN give better reward prediction performance than the unstructured CNN-VRNN baseline, suggesting our architecture may be a useful addition to planning and reinforcement learning models. Concurrent work that applies a similar keypoint-structured model to control tasks confirms these results <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>A major question in machine learning is to what degree prior knowledge should be built into a model, as opposed to learning it from the data. This question is especially important for unsupervised vision models trained on raw pixels, which are typically far removed from the information that is of interest for downstream tasks. We propose a model with a spatial inductive bias, resulting in a structured, keypoint-based internal representation. We show that this structure leads to superior results on downstream tasks compared to a representation derived from a CNN without a keypoint-based representational bottleneck.</p><p>The proposed spatial prior keypoints represents a middle ground between unstructured representations and an explicitly object-centric approach. For example, we do not explicitly model object masks, occlusions, or depth. Our architecture either leaves these phenomena unmodeled, or learns them from the data. By choosing to not build this kind of structure into the architecture, we keep our model simple and achieve stable training (see variability across initializations in Figures <ref type="figure" target="#fig_7">3, 4,</ref> and<ref type="figure" target="#fig_4">5</ref>) on diverse datasets, including multiple objects and complex, articulated human shapes.</p><p>We also note the importance of stochasticity for the prediction of videos and object trajectories. In natural videos, any sequence of conditioning frames is consistent with an astronomical number of plausible future frames. We found that methods that increase sample diversity (e.g. the best-of-many objective <ref type="bibr" target="#b3">[4]</ref>) led to large gains in FVD, which measures the similarity of real and predicted videos on the level of distributions over entire videos. Conversely, due to the diversity of plausible futures, frame-wise measures of similarity to ground truth (e.g. VGG cosine similarity, PSNR, and SSIM) are near-meaningless for measuring long-term video prediction quality.</p><p>Beyond image-based measures, the most meaningful evaluation of a predictive model is to apply it to downstream tasks of interest, such as planning and reinforcement learning for control tasks. Because of its simplicity, our architecture is straightforward to combine with existing architectures for tasks that may benefit from spatial structure. Applying our model to such tasks is an important future direction of this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Main datasets used in our experiments. First row: Ground truth images. Second row: Decoded coordinates (black dots; xt in Figure 1) and past trajectories (gray lines). Third row: Reconstructed image. Green borders indicate observed frames, red indicate predicted frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Video generation quality on Human3.6M. Our stochastic structured model (Struct-VRNN) outperforms our deterministic baseline (Struct-RNN), our unstructured baseline (CNN-VRNN), and the SVG<ref type="bibr" target="#b7">[8]</ref> and SAVP<ref type="bibr" target="#b13">[14]</ref> models. Top: Example observed (green borders) and predicted (red borders) frames (best viewed as video: https://mjlm.github.io/video_structure/). Example sequences are the closest or furthest samples from ground truth according to VGG cosine similarity, as indicated. Note that for Struct-VRNN, even the samples furthest from ground truth are of high visual quality. Bottom left: Mean VGG cosine similarity of the the samples closest to ground truth (left) and furthest from ground truth (right). Higher is better. Plots show mean performance across 5 model initializations, with the 95% confidence interval shaded. Bottom right: Fréchet Video Distance<ref type="bibr" target="#b22">[23]</ref>, using all samples. Lower is better. Dots represents separate model initializations. EPVA<ref type="bibr" target="#b27">[28]</ref> is not stochastic, so we compare performance with a single sample from our method on their test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Prediction error for ground-truth trajectories by linear regression from predicted keypoints. (sup.) indicates supervised baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Ablating either the temporal separation loss or the keypoint sparsity loss reduces model performance and stability. In the FVD plots, each dot corresponds to a different model initialization. Coordinate error plots show the prediction error when regressing the ground-truth object coordinates on the discovered keypoints. Lines show the mean of five model initializations, with the 95% confidence intervals shaded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Manipulation</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Unsupervised keypoints allow human-guided exploration of object dynamics. We manipulated the observed coordinates for Player 1 (black arrow) to change the original (blue) trajectory. The other players were not manipulated. The dynamics were then rolled out into the future to predict how the players will behave in the manipulated (red) scenario. Black crosses mark initial player positions. Light-colored parts of the trajectories are observed, dark-colored parts are predicted. Dots indicate final position. Lines of the same color indicate different samples conditioned on the same observed coordinates.</figDesc><graphic coords="8,164.10,82.78,370.05,93.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>5. 4</head><label>4</label><figDesc>Manipulation of keypoints allows interaction with the model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Keypoints learned by our method may be manipulated to change peoples' poses. Note that both manipulations and effects are spatially local. Best viewed in video (https://mjlm.github. io/video_structure/).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Action recognition on the Human3.6M dataset. Solid line: null model (predict the most frequent action). Dashed line: prediction from groundtruth coordinates. Sup., supervised.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Predicting rewards on the DeepMind Control Suite continuous control domains. We chose domains with dense rewards to ensure the random policy would provide a sufficient reward signal for this analysis. To make scales comparable across domains, errors are normalized to a null model which predicts the mean training-set-reward at all timesteps. Lines show the mean across test-set examples and 5 random model initializations, with the 95% confidence interval shaded.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We found this to be necessary to maintain a keypoint-structured representation. If the image model is trained based on errors from the dynamics model, the image model may adopt the poorly structured code of an incompletely trained dynamics model, rather than the dynamics model adopting the keypoint-structured code.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>For Human3.6M, we choose the best sample based on the average error of all coordinates. For Basketball, we choose the best sample separately for each player.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning Character-Agnostic Motion for Motion Retargeting in 2D</title>
		<author>
			<persName><forename type="first">K</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fixing a Broken ELBO</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stochastic variational video prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumitru An Campbell</surname></persName>
		</author>
		<author>
			<persName><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accurate and Diverse Sampling of Sequences based on a &quot;Best of Many&quot; Sample Objective</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Everybody Dance Now</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ginosar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>abs/1808.07371</idno>
	</analytic>
	<monogr>
		<title level="m">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A Recurrent Latent Variable Model for Sequential Data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Disentangled Representations from Video</title>
		<author>
			<persName><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Birodkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Stochastic Video Generation with a Learned Prior</title>
		<author>
			<persName><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning Latent Dynamics for Planning from Pixels</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAMI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Conditional Image Generation for Learning the Structure of Visual Objects</title>
		<author>
			<persName><forename type="first">T</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unsupervised learning of object keypoints for perception and control</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.11883</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stochastic Adversarial Video Prediction</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno>abs/1804.01523</idno>
	</analytic>
	<monogr>
		<title level="m">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Action-conditional video prediction using deep networks in atari games</title>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Video (language) modeling: a baseline for generative models of natural videos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<idno>arXiv preprint:1412.6604</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Video Representations using LSTMs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stochastic Prediction of Multi-Agent Interactions From Partial Observations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mocogan: Decomposing motion and content for video generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Towards Accurate Generative Models of Video: A New Metric &amp; Challenges</title>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decomposing Motion and Content for Natural Video Sequence Prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to Generate Long-term Future via Hierarchical Prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The Pose Knows: Video Forecasting by Generating Pose Futures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Video-to-Video Synthesis</title>
		<author>
			<persName><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical Long-term Video Prediction without Supervision</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wichers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Unsupervised Discovery of Parts, Structure, and Dynamics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Visual dynamics: Probabilistic future frame synthesis via cross convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mt-vae: Learning motion transformations to generate multimodal human dynamics</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generating Multi-Agent Trajectories using Programmatic Weak Supervision</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised Discovery of Object Landmarks as Structural Representations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
