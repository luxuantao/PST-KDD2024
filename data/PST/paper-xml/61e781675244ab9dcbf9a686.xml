<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Unsupervised Deep Graph Structure Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-17">17 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
							<email>yixin.liu@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
							<email>yu.zheng@latrobe.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">La Trobe University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">La Trobe University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daokun</forename><surname>Zhang</surname></persName>
							<email>daokun.zhang@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Monash Suzhou Research Institute</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Monash Suzhou Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongxu</forename><surname>Chen</surname></persName>
							<email>hongxu.chen@uts.edu.au</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
							<email>penghao@buaa.edu.cn</email>
							<affiliation key="aff4">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
							<email>shirui.pan@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Unsupervised Deep Graph Structure Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-17">17 Jan 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/XXXXXX.XXXXXX</idno>
					<idno type="arXiv">arXiv:2201.06367v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>graph neural networks</term>
					<term>graph structure learning</term>
					<term>unsupervised learning</term>
					<term>contrastive learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, graph neural networks (GNNs) have emerged as a successful tool in a variety of graph-related applications. However, the performance of GNNs can be deteriorated when noisy connections occur in the original graph structures; besides, the dependence on explicit structures prevents GNNs from being applied to general unstructured scenarios. To address these issues, recently emerged deep graph structure learning (GSL) methods propose to jointly optimize the graph structure along with GNN under the supervision of a node classification task. Nonetheless, these methods focus on a supervised learning scenario, which leads to several problems, i.e., the reliance on labels, the bias of edge distribution, and the limitation on application tasks. In this paper, we propose a more practical GSL paradigm, unsupervised graph structure learning, where the learned graph topology is optimized by data itself without any external guidance (i.e., labels). To solve the unsupervised GSL problem, we propose a novel StrUcture Bootstrapping contrastive LearnIng fraMEwork (SUBLIME for abbreviation) with the aid of self-supervised contrastive learning. Specifically, we generate a learning target from the original data as an "anchor graph", and use a contrastive loss to maximize the agreement between the anchor graph and the learned graph. To provide persistent guidance, we design a novel bootstrapping mechanism that upgrades the anchor graph with learned structures during model learning. We also design a series of graph learners and post-processing schemes to model the structures to learn. Extensive experiments on eight benchmark datasets demonstrate the significant effectiveness of our proposed SUBLIME and high quality of the optimized graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Mathematics of computing → Graph algorithms; • Computing methodologies → Neural networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent years have witnessed the prosperous development of graphbased applications in numerous domains, such as chemistry, bioinformatics and cybersecurity. As a powerful deep learning tool to model graph-structured data, graph neural networks (GNNs) have drawn increasing attention and achieved state-of-the-art performance in various graph analytical tasks, including node classification <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b39">40]</ref>, link prediction <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32]</ref>, and node clustering <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b54">55]</ref>. GNNs usually follow a message-passing scheme, where node representations are learned by aggregating information from the neighbors on an observed topology (i.e., the original graph structure).</p><p>Most GNNs rely on a fundamental assumption that the original structure is credible enough to be viewed as ground-truth information for model training. Such assumption, unfortunately, is usually violated in real-world scenarios, since graph structures are usually extracted from complex interaction systems which inevitably contain uncertain, redundant, wrong and missing connections <ref type="bibr" target="#b44">[45]</ref>. Such noisy information in original topology can seriously damage the performance of GNNs. Besides, the reliance on explicit structures hinders GNNs' broad applicability. If GNNs are capable of uncovering the implicit relations between samples, e.g., two images containing the same object, they can be applied to more general domains like vision and language.</p><p>To tackle the aforementioned problems, deep graph structure learning (GSL) is a promising solution that constructs and improves the graph topology with GNNs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b57">58]</ref>. Concretely, these methods parameterize the adjacency matrix with a probabilistic model <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b44">45]</ref>, full parameterization <ref type="bibr" target="#b19">[20]</ref> or metric learning model <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b52">53]</ref>, and jointly optimize the parameters of the adjacency matrix and GNNs by solving a downstream task (i.e., node classification) <ref type="bibr" target="#b57">[58]</ref>. However, existing methods learn graph structures in a supervised scenario, which brings the following issues: <ref type="bibr" target="#b0">(1)</ref> The reliance on label information. In supervised GSL methods, humanannotated labels play an important role in providing supervision signal for structure improvement. Such reliance on labels limits the application of supervised GSL on more general cases where annotation is unavailable. <ref type="bibr" target="#b1">(2)</ref> The bias of learned edge distribution. Node classification usually follows a semi-supervised setting, where only a small fraction of nodes (e.g., 140/2708 in Cora dataset) are under the supervision of labels. As a result, the connections among these nodes and their neighbors would receive more guidance in  structure learning, while the relations between nodes far away from them are rarely discovered by GSL <ref type="bibr" target="#b10">[11]</ref>. Such imbalance leads to the bias of edge distribution, affecting the quality of the learned structures.</p><p>(3) The limitation on downstream tasks. In existing methods, the structure is specifically learned for node classification, so it may contain more task-specific information rather than general knowledge. Consequently, the refined topology may not benefit other downstream tasks like link prediction or node clustering, indicating the poor generalization ability of the learned structures.</p><p>To address these issues, in this paper, we investigate a novel unsupervised learning paradigm for GSL, namely unsupervised graph structure learning. As compared in Fig. <ref type="figure" target="#fig_1">1</ref>, in our learning paradigm, structures are learned by data itself without any external guidance (i.e., labels), and the acquired universal, edge-unbiased topology can be freely applied to various downstream tasks. In this case, one natural question can be raised: how to provide sufficient supervision signal for unsupervised GSL? To answer this, we propose a novel StrUcture Bootstrapping contrastive LearnIng fraMEwork (SUBLIME for abbreviation) to learn graph structures with the aid of self-supervised contrastive learning <ref type="bibr" target="#b24">[25]</ref>. Concretely, our method constructs an "anchor graph" from the original data to guide structure optimization, with a contrastive loss to maximize the mutual information (MI) between anchor graph and the learned structure. Through maximizing their consistency, informative hidden connections can be discovered, which well respects the node proximity conveyed by the original features and structures. Meanwhile, as we optimize the contrastive loss on the representations of every node, all potential edge candidates will receive the essential supervision, which promotes a balanced edge distribution in the inferred topology. Furthermore, we design a bootstrapping mechanism to update anchor graph with the learned edges, which provides a self-enhanced supervision signal for GSL. Besides, we carefully design multiple graph learners and post-processing schemes to model graph topology for diverse data. In summary, our core contributions are three-fold:</p><p>• Problem. We propose a novel unsupervised learning paradigm for graph structure learning, which is more practical and challenging than the existing supervised counterpart. To the best of our knowledge, this is the first attempt to learn graph structures with GNNs in an unsupervised setting. • Algorithm. We propose a novel unsupervised GSL method SUBLIME, which guides structure optimization by maximizing the agreement between the learned structure and a crafted self-enhanced learning target with contrastive learning.</p><p>• Evaluations. We perform extensive experiments to corroborate the effectiveness and analyze the properties of SUBLIME via thorough comparisons with state-of-the-art methods on eight benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Graph Neural Networks</head><p>Graph neural networks (GNNs) are a type of deep neural networks aiming to learn low-dimensional representations for graphstructure data <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b47">48]</ref>. Modern GNNs can be categorized into two types: spectral and spatial methods. The spectral methods perform convolution operation to graph domain using spectral graph filter <ref type="bibr" target="#b2">[3]</ref> and its simplified variants, e.g., Chebyshev polynomials filter <ref type="bibr" target="#b8">[9]</ref> and the first-order approximation of Chebyshev filter <ref type="bibr" target="#b21">[22]</ref>. The spatial methods perform convolution operation by propagating and aggregating local information along edges in a graph <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b49">50]</ref>. In spatial GNNs, different aggregation functions are designed to learn node representations, including mean/max pooling <ref type="bibr" target="#b14">[15]</ref>, LSTM <ref type="bibr" target="#b14">[15]</ref>, self-attention <ref type="bibr" target="#b39">[40]</ref>, and summation <ref type="bibr" target="#b49">[50]</ref>. Readers may refer to the elaborate survey <ref type="bibr" target="#b47">[48]</ref> for a thorough review.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep Graph Structure Learning</head><p>Graph structure learning (GSL) problem has been investigated by conventional machine learning techniques in graph signal processing <ref type="bibr" target="#b9">[10]</ref>, spectral clustering <ref type="bibr" target="#b1">[2]</ref>, and network science <ref type="bibr" target="#b25">[26]</ref>. However, these methods are not capable of handling graph data with highdimensional features, so they are not further discussed in our paper. Very recently, there thrives a branch of research that investigates GSL for GNNs with the aim to boost their performance on downstream tasks, which is named deep graph structure learning <ref type="bibr" target="#b57">[58]</ref>. These methods follow a general pipeline: the graph adjacency matrix is modeled with learnable parameters, and then jointly optimized along with GNN under the supervision of a downstream node classification task. In these methods, various techniques are leveraged to parameterize the adjacency matrix. Considering the discrete nature of graph structures, one type of methods adopts probabilistic models, such as Bernoulli probability model <ref type="bibr" target="#b11">[12]</ref> and stochastic block model <ref type="bibr" target="#b44">[45]</ref>. Another type of methods models structures with node-wise similarity computed by metric learning functions like cosine similarity <ref type="bibr" target="#b6">[7]</ref> and dot production <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b52">53]</ref>. Besides, directly treating each element in adjacency matrix as a learnable parameter is also an effective solution <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>. Nevertheless, the existing deep GSL approaches follow a supervised scenario where node labels are always required to refine the graph structures. In this paper, differently, we advocate a more practical unsupervised learning paradigm where no extra information is needed for GSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Contrastive Learning on Graphs</head><p>After achieving significant performance in visual <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref> and linguistic <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13]</ref> domains, contrastive learning has shown competitive performance and become increasingly popular in graph representation learning <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b58">59]</ref>. Graph contrastive learning obeys the principle of mutual information (MI) maximization, which pulls the representations of samples with shared semantic information closer while pushing the representations of irrelevant samples away <ref type="bibr" target="#b24">[25]</ref>. In graph data, the MI maximization can be carried out to samples in the same scale (i.e., node-level <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b58">59</ref>] and graph-level <ref type="bibr" target="#b51">[52]</ref>) or different scales (i.e., node v.s. graph <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b56">57]</ref> and node v.s. subgraph <ref type="bibr" target="#b31">[32]</ref>). Graph contrastive learning also benefits diverse applications, such as chemical prediction <ref type="bibr" target="#b46">[47]</ref>, anomaly detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref>, federated learning <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b55">56]</ref>, and recommendation <ref type="bibr" target="#b53">[54]</ref>. However, it still remains unclear how to effectively improve GSL using contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM DEFINITION</head><p>Before we make the problem statement of unsupervised GSL, we first introduce the definition of graphs. An attributed graph can be represented by G = (V, E, X) = (A, X), where V is the set of</p><formula xml:id="formula_0">𝑛 = |V | nodes, E is the set of 𝑚 = |E | edges, X ∈ R 𝑛×𝑑</formula><p>is the node feature matrix (where the 𝑖-th row x 𝑖 is the feature vector of node 𝑣 𝑖 ), and A ∈ [0, 1] 𝑛×𝑛 is the weighted adjacency matrix (where 𝑎 𝑖 𝑗 is the weight of the edge connecting 𝑣 𝑖 and 𝑣 𝑗 ). Frequently used notations are summarized in Appendix A.</p><p>In this paper, we consider two unsupervised GSL tasks, i.e., structure inference and structure refinement. The former is applicable to general datasets where graph structures are not predefined or are unavailable. The latter, differently, aims to modify the given noisy topology and produce a more informative graph. Node labels are unavailable for structure optimization in both tasks. Definition 3.1 (Structure inference). Given a feature matrix X ∈ R 𝑛×𝑑 , the target of structure inference is to automatically learn a graph topology S ∈ [0, 1] 𝑛×𝑛 , which reflects the underlying correlations among data samples. In particular, S 𝑖 𝑗 ∈ [0, 1] indicates whether there is an edge between two samples (nodes) x 𝑖 and x 𝑗 . Definition 3.2 (Structure refinement). Given a graph G = (A, X) with a noisy graph structure A, the target of structure refinement is to refine A to be the optimized adjacency matrix S ∈ [0, 1] 𝑛×𝑛 to better capture the underlying dependency between nodes.</p><p>With the graph topology S which is either learned automatically from data or refined from an existing graph structure, the hypothesis is that the model performance on downstream tasks can be essentially improved with G 𝑙 = (S, X) as the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>This section elaborates our proposed SUBLIME, a novel unsupervised GSL framework. As shown in Fig. <ref type="figure" target="#fig_2">2</ref>, SUBLIME on the highest level consists of two components: the graph structure learning module that models and regularizes the learned graph topology and the structure bootstrapping contrastive learning module that provides a self-optimized supervision signal for GSL. In the graph structure learning module, a sketched adjacency matrix is first parameterized by a graph learner, and then refined by a post-processor to be the learned adjacency matrix. Afterwards, in the structure bootstrapping contrastive learning module, we first establish two different views to contrast: learner view that discovers graph structure and anchor view that provides guidance for structure learning. Then, after data augmentation, the agreement between two views is maximized by a node-level contrastive learning. Specially, we design a structure bootstrapping mechanism to update anchor view with learned structures. The following subsections illustrate these crucial components respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Graph Learner</head><p>As a key component of GSL, the graph learner generates a sketched adjacency matrix S ∈ R 𝑛×𝑛 with a parameterized model. Most existing methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20]</ref> adopt a single strategy to model graph structure, which cannot adapt to data with different unique properties. To find optimal structures for various data, we consider four types of graph learners, including a full graph parameterization (FGP) learner and three metric learning-based learners (i.e., Attentive, MLP, and GNN learner). In general, we formulate a graph learner as 𝑝 𝜔 (•), where 𝜔 is the learnable parameters.</p><p>FGP learner directly models each element of the adjacency matrix by an independent parameter <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20]</ref> without any extra input. Formally, FGP learner is defined as:</p><formula xml:id="formula_1">S = 𝑝 𝐹𝐺𝑃 𝜔 = 𝜎 (Ω),<label>(1)</label></formula><p>where 𝜔 = Ω ∈ R 𝑛×𝑛 is a parameter matrix and 𝜎 (•) is a non-linear function that makes training more stable. The assumption behind FGP learner is that each edge exists independently in the graph. Different from the FGP learner, metric learning-based learners <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b57">58]</ref> first acquire node embeddings E ∈ R 𝑛×𝑑 from the input data, and then model S with pair-wise similarity of the node embeddings:</p><formula xml:id="formula_2">S = 𝑝 𝑀𝐿 𝜔 (X, A) = 𝜙 (ℎ 𝜔 (X, A)) = 𝜙 (E),<label>(2)</label></formula><p>where ℎ 𝜔 (•) is a neural network-based embedding function (a.k.a. embedding network) with parameter 𝜔, and 𝜙 (•) is a non-parametric metric function (e.g., cosine similarity or Minkowski distance) that calculates pair-wise similarity. For different ℎ 𝜔 (•), we provide three specific instances of metric learning-based learners: Attentive, MLP, and GNN learners.</p><p>Attentive Learner employs a GAT-like <ref type="bibr" target="#b39">[40]</ref> attentive network as its embedding network, where each layer compute the Hadamard production of input feature vector and the parameter vector:</p><formula xml:id="formula_3">E (𝑙) = ℎ (𝑙) 𝑤 (E (𝑙−1) ) = 𝜎 ([e (𝑙−1) 1 ⊙ 𝜔 (𝑙) , • • • , e (𝑙−1) 𝑛 ⊙ 𝜔 (𝑙) ] ⊺ ),<label>(3)</label></formula><p>in which E (𝑙) is the output matrix of the 𝑙-th layer of embedding network, e (𝑙−1) 𝑖 ∈ R 𝑑 is the transpose of the 𝑖-th row vector of E (𝑙−1) , 𝜔 (𝑙) ∈ R 𝑑 is the parameter vector of the 𝑙-th layer, ⊙ is the Hadamard operation, (•) ⊺ is the transposition operation, and 𝜎 (•) is a non-linear operation. The input of the first layer E (0) is the feature matrix X, and the output of the final layer E (𝐿) (𝐿 is the layer number of embedding network) is the embedding matrix E. Attentive learner assumes that each feature has different contribution to the existence of edge, but there is no significant correlation between features.</p><p>MLP Learner uses a Multi-Layer Perception (MLP) as its embedding network, where a single layer can be written by:</p><formula xml:id="formula_4">E (𝑙) = ℎ (𝑙) 𝑤 (E (𝑙−1) ) = 𝜎 (E (𝑙−1) Ω (𝑙) ),<label>(4)</label></formula><p>where Ω (𝑙) ∈ R 𝑑×𝑑 is the parameter martix of the 𝑙-th layer, and the other notations are similar to Eq. (3). Compared to attentive learner, MLP learner further considers the correlation and combination of features, generating more informative embeddings for downstream similarity metric learning.</p><p>GNN Learner integrates features X and original structure A into node embeddings E via GNN-based embedding network. Due to the reliance on original topology, GNN learner is only used for the structure refinement task. For simplicity, we take GCN layers <ref type="bibr" target="#b21">[22]</ref> to form embedded network:</p><formula xml:id="formula_5">! !" , ! #$ ! !" , ! #$ Anchor View Learner View Graph Learner # % GNN Encoder $ &amp; MLP Projector % ' GNN Encoder $ &amp; MLP Projector % ' &amp; ( ̅ &amp; ( ̅ &amp; ) &amp; ) Post- processor (</formula><formula xml:id="formula_6">E (𝑙) = ℎ (𝑙) 𝑤 (E (𝑙−1) , A) = 𝜎 D − 1 2 A D − 1 2 E (𝑙−1) Ω (𝑙) ,<label>(5)</label></formula><p>where A = A + I is the adjacency matrix with self-loop, D is the degree matrix of A, and the other notations are similar to Eq. ( <ref type="formula" target="#formula_4">4</ref>). GNN Learner assumes that the connection between two nodes is related to not only features but also the original structure.</p><p>In SUBLIME, we choose the most suitable learner to model S according to the characteristics of different datasets. In Appendix B, we analyze the properties of different graph learners and discuss how we allocate learners for each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Post-processor</head><p>The post-processor 𝑞(•) aims to refine the sketched adjacency matrix S into a sparse, non-negative, symmetric and normalized adjacency matrix S. To this end, four post-processing steps are applied sequentially, i.e., sparsification 𝑞 𝑠𝑝 (•), activation 𝑞 𝑎𝑐𝑡 (•), symmetrization 𝑞 𝑠𝑦𝑚 (•), and normalization 𝑞 𝑛𝑜𝑟𝑚 (•). Sparsification. The sketched adjacency matrix S is often dense, representing a fully connected graph structure. However, such adjacency matrix usually makes little sense for most applications and results in expensive computation cost <ref type="bibr" target="#b44">[45]</ref>. Hence, we conduct a k-nearest neighbors (kNN)-based sparsification on S. Concretely, for each node, we keep the edges with top-k connection values and set the rest to 0. The sparsification 𝑞 𝑠𝑝 (•) is expressed as:</p><formula xml:id="formula_7">S(𝑠𝑝) 𝑖 𝑗 = 𝑞 𝑠𝑝 S𝑖 𝑗 = S𝑖 𝑗 , S𝑖 𝑗 ∈ top-k( S𝑖 ), 0, S𝑖 𝑗 ∉ top-k( S𝑖 ),<label>(6)</label></formula><p>where top-k( S𝑖 ) is the set of top-k values of row vector S𝑖 . To keep the gradient flow, we do not apply sparsification for the FGP learner. For large-scale graphs, we perform the kNN sparsification with its locality-sensitive approximation <ref type="bibr" target="#b10">[11]</ref> where the nearest neighbors are selected from a batch of nodes instead of all nodes, which reduces the requirement of memory. Symmetrization and Activation. In real-world graphs, the connections are often bi-directional, which requires a symmetric adjacency matrix. In addition, the edge weights should be non-negative according to the definition of adjacency matrix. To meet these conditions, the symmetrization and activation are performed as:</p><formula xml:id="formula_8">S(𝑠𝑦𝑚) = 𝑞 𝑠𝑦𝑚 𝑞 𝑎𝑐𝑡 S(𝑠𝑝) = 𝜎 𝑞 S(𝑠𝑝) + 𝜎 𝑞 S(𝑠𝑝) ⊺ 2 ,<label>(7)</label></formula><p>where 𝜎 𝑞 (•) is a non-linear activation. For metric learning-based learners, we define 𝜎 𝑞 (•) as ReLU function. For FGP learner, we apply the ELU function to prevent gradient from disappearing. Normalization. To guarantee the edge weights are within the range [0, 1], we finally conduct a normalization on S. In particular, we apply a symmetrical normalization:</p><formula xml:id="formula_9">S = 𝑞 𝑛𝑜𝑟𝑚 S(𝑠𝑦𝑚) = D(𝑠𝑦𝑚) − 1 2 S(𝑠𝑦𝑚) D(𝑠𝑦𝑚) − 1 2 ,<label>(8)</label></formula><p>where D(𝑠𝑦𝑚) is the degree matrix of S(𝑠𝑦𝑚) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multi-view Graph Contrastive Learning</head><p>Since we have obtained a well-parameterized adjacency matrix S, a natural question that arises here is: how to provide an effective supervision signal guiding the graph structure learning without label information? Our answer is to acquire the supervision signal from data itself via multi-view graph contrastive learning. To be concrete, we construct two graph views based on the learned structure and the original data respectively. Then, data augmentation is applied to both views. Finally, we maximize the MI between two augmented views with node-level contrastive learning. Learner view is directly built by integrating the learned adjacency matrix S and the feature matrix X together, which is denoted as G 𝑙 = (S, X). In each training iteration, S and the parameters used to model it are directly updated by gradient descent to discover optimal graph structures. In SUBLIME, we initialize learner views as the kNN graph built on features, since it is an effective way to provide a starting point for GSL, as suggested in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Specifically, for FGP learner, we initialize the parameters corresponding to kNN edges as 1 while the rest as 0. For attentive learner, we let each element in 𝜔 (𝑙) ∈ 𝜔 to be 1. Then, feature-level similarities are computed according to the metric function, and the kNN graph is obtained by the sparsification post-processing. For MLP and GNN learners, similarly, we set the embedding dimension to be 𝑑 and initialize Ω (𝑙) ∈ 𝜔 as identity matrices.</p><p>Anchor view plays a "teacher" role that provides correct and stable guidance for GSL. For the structure refinement task where the original structure A is available, we define anchor view as G 𝑎 = (A 𝑎 , X) = (A, X); for the structure inference task where A is inaccessible, we take an identity matrix I as the anchor structure: G 𝑎 = (A 𝑎 , X) = (I, X). To provide a stable learning target, anchor view is not updated by gradient descent but a novel bootstrapping mechanism which will be introduced in Section 4.4. 4.3.2 Data Augmentation. In contrastive learning, data augmentation is a key to benefiting the model through exploring richer underlying semantic information by making the learning tasks more challenging to solve <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b58">59]</ref>. In SUBLIME, we exploit two simple but effective augmentation schemes, i.e., feature masking and edge dropping, to corrupt the graphs views at both structure and feature levels. Feature masking. To disturb the node features, we randomly select a fraction of feature dimensions and mask them with zeros. Formally, for a given feature matrix X, a masking vector m (𝑥) ∈ {0, 1} 𝑑 is first sampled, where each element is drawn from a Bernoulli distribution with probability 𝑝 (𝑥) independently. Then, we mask the feature vector of each node with m (𝑥) :</p><formula xml:id="formula_10">X = T 𝑓 𝑚 (X) = [x 1 ⊙ m (𝑥) , • • • , x 𝑛 ⊙ m (𝑥) ] ⊺ ,<label>(9)</label></formula><p>where X is the augmented feature matrix, T 𝑓 𝑚 (•) is the feature masking transformation, and x 𝑖 is the transpose of the i-th row vector of X. Edge dropping. Apart from masking features, we corrupt the graph structure by randomly dropping a portion of edges. Specifically, for a given adjacency matrix A, we first sample a masking matrix M (𝑎) ∈ {0, 1} 𝑛×𝑛 , where each element M (𝑎) 𝑖 𝑗 is drawn from a Bernoulli distribution with probability 𝑝 (𝑎) independently. After that, the adjacency matrix is masked with M (𝑎) :</p><formula xml:id="formula_11">A = T 𝑒𝑑 (A) = A ⊙ M (𝑎) , (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where A is the augmented adjacency matrix, and T 𝑒𝑑 (•) is the edge dropping transformation.</p><p>In SUBLIME, we jointly leverage these two augmentation schemes to generate augmented graphs on both learner and anchor views:</p><formula xml:id="formula_13">G 𝑙 = (T 𝑒𝑑 (S), T 𝑓 𝑚 (X)), G 𝑎 = (T 𝑒𝑑 (A 𝑎 ), T 𝑓 𝑚 (X)),<label>(11)</label></formula><p>where G 𝑙 and G 𝑎 are the augmented learner view and anchor view, respectively. To obtain different contexts in the two views, the feature masking for two views employs different probabilities</p><formula xml:id="formula_14">𝑝 (𝑥) 𝑙 ≠ 𝑝 (𝑥)</formula><p>𝑎 . For edge dropping, since the adjacency matrices of two views are already significantly different, we use the same dropping probability 𝑝 (𝑎) . Note that other advanced augmentation schemes can also be applied to SUBLIME, which is left for our future research. 4.3.3 Node-level Contrastive Learning. After obtaining two augmented graph views, we perform a node-level contrastive learning to maximize the MI between them. In SUBLIME, we adopt a simple contrastive learning framework originated from SimCLR <ref type="bibr" target="#b5">[6]</ref> which consists of the following components: GNN-based encoder. A GNN-based encoder 𝑓 𝜃 (•) extracts nodelevel representations for augmented graphs G 𝑙 and G 𝑎 :</p><formula xml:id="formula_15">(𝑎) 𝑙 = 𝑝 (𝑎) 𝑎 = 𝑝</formula><formula xml:id="formula_16">H 𝑙 = 𝑓 𝜃 (G 𝑙 ), H 𝑎 = 𝑓 𝜃 (G 𝑎 ), (<label>12</label></formula><formula xml:id="formula_17">)</formula><p>where 𝜃 is the parameter of encoder 𝑓 𝜃 (•), and H 𝑙 , H 𝑎 ∈ R 𝑛×𝑑 1 (𝑑 1 is the representation dimension) are the node representation matrices for learner/anchor views, respectively. In SUBLIME, we utilize GCN <ref type="bibr" target="#b21">[22]</ref> as our encoder and set its layer number 𝐿 1 to 2. MLP-based projector. Following the encoder, a projector 𝑔 𝜑 (•) with 𝐿 2 MLP layers maps the representations to another latent space where the contrastive loss is calculated:</p><formula xml:id="formula_18">Z 𝑙 = 𝑔 𝜑 (H 𝑙 ), Z 𝑎 = 𝑔 𝜑 (H 𝑎 ), (<label>13</label></formula><formula xml:id="formula_19">)</formula><p>where 𝜑 is the parameter of projector 𝑔 𝜑 (•), and Z 𝑙 , Z 𝑎 ∈ R 𝑛×𝑑 2 (𝑑 2 is the projection dimension) are the projected node representation matrices for learner/anchor views, respectively. Node-level contrastive loss function. A contrastive loss L is leveraged to enforce maximizing the agreement between the projections 𝑧 𝑙,𝑖 and 𝑧 𝑎,𝑖 of the same node 𝑣 𝑖 on two views. In our framework, a symmetric normalized temperature-scaled cross-entropy loss (NT-Xent) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b34">35]</ref> is applied:</p><formula xml:id="formula_20">L = 1 2𝑛 𝑛 ∑︁ 𝑖=1 ℓ (𝑧 𝑙,𝑖 , 𝑧 𝑎,𝑖 ) + ℓ (𝑧 𝑎,𝑖 , 𝑧 𝑙,𝑖 ) , ℓ (𝑧 𝑙,𝑖 , 𝑧 𝑎,𝑖 ) = log 𝑒 sim(z 𝑙,𝑖 ,z 𝑎,𝑖 )/𝑡 𝑛 𝑘=1 𝑒 sim(z 𝑙,𝑖 ,z 𝑎,𝑘 )/𝑡 ,<label>(14)</label></formula><p>where sim(•, •) is the cosine similarity function, and 𝑡 is the temperature parameter. ℓ (𝑧 𝑎,𝑖 , 𝑧 𝑙,𝑖 ) is computed following ℓ (𝑧 𝑙,𝑖 , 𝑧 𝑎,𝑖 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Structure Bootstrapping Mechanism</head><p>With a fixed anchor adjacency matrix A 𝑎 defined by A or I, SUBLIME can learn graph structure S by maximizing the MI between two views. However, using a constant anchor graph may lead to several issues: (1) Inheritance of error information. Since A 𝑎 is directly borrowed from the input data, it would carry some natural noise (e.g., missing or redundant edges) of the original graph. If the noise is not eliminated in the learning process, the learned structures will finally inherit it. (2) Lack of persistent guidance. A fixed anchor graph contains limited information to guide GSL. Once the graph learner captures this information, it will be hard for the model to gain effective supervision in the following training steps. (3) Overfitting the anchor structure. Driven by the learning objective that maximizes the agreement between two views, the learned structure tends to over-fit the fixed anchor structure, resulting in a similar testing performance to the original data.</p><p>Inspired by previous bootstrapping-based algorithms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b36">37]</ref>, we design a structure bootstrapping mechanism to provide a selfenhanced anchor view as the learning target. The core idea of our solution is to update the anchor structure A 𝑎 with a slow-moving augmentation of the learned structure S instead of keeping A 𝑎 unchanged. In particular, given a decay rate 𝜏 ∈ [0, 1], the anchor structure A 𝑎 is updated every 𝑐 iterations as following: Benefiting from the structure bootstrapping mechanism, SUBLIME has nice properties that can address the aforementioned problems. With the process of updating, the weights of some noise edges gradually decrease in A 𝑎 , which relieves their negative impact on structure learning. Meanwhile, since the learning target A 𝑎 is changing during the training phase, it can always incorporate more effective information to guide the learning of topology, and the over-fitting problem is naturally resolved. More importantly, our structure bootstrapping mechanism leverages the learned knowledge to improve the learning target in turn, pushing the model to discover increasingly optimal graph structure constantly. Besides, the slow-moving average (with 𝜏 &gt; 0.99) updating ensures the stability of training.</p><formula xml:id="formula_21">A 𝑎 ← 𝜏A 𝑎 + (1 − 𝜏)S.<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Overall Framework</head><p>In this subsection, we first illustrate the training process of SUBLIME, and then introduce the tricks to help apply it to large-scale graphs. Model training. In our training process, we first initialize the parameters and anchor adjacency matrix A 𝑎 . Then, in each iteration, we perform forward propagation to compute the contrastive loss L, and update all the parameters jointly via back propagation. After back propagation, we update A 𝑎 by bootstrapping structure mechanism every 𝑐 iterations. Finally, we acquire the learned topology represented by S. As analyzed in Appendix C, the time complexity</p><formula xml:id="formula_22">of SUBLIME is O (𝑛 2 𝑑 +𝑚𝑑 1 𝐿 1 +𝑛𝑑 2 1 𝐿 1 +𝑛𝑑 2 2 𝐿 2 +𝑛𝑘).</formula><p>The algorithmic description is provided in Appendix D. Scalability extension. To extend the scalability of SUBLIME, the key is to avoid O (𝑛 2 ) space complexity and time complexity. To this end, we adopt the following measures: (1) To avoid explosive number of parameters, we use metric learning-based learners instead of FGP learner. (2) For sparsification post-processing, we consider a locality-sensitive approximation for kNN graph <ref type="bibr" target="#b10">[11]</ref>. (3) For graph contrastive learning, we compute the contrastive loss L for a mini-batch of samples instead of all nodes. (4) To reduce the space complexity of the bootstrapped structure, we perform the update in Eq. ( <ref type="formula" target="#formula_21">15</ref>) with a larger iteration interval 𝑐 (𝑐 ≥ 10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we conduct empirical experiments to demonstrate the effectiveness of the proposed framework SUBLIME. We aim to answer five research questions as follows: RQ1: How effective is SUBLIME for learning graph structure under unsupervised settings? RQ2: How does the structure bootstrapping mechanism influence the performance of SUBLIME? RQ3: How do key hyper-parameters impact the performance of SUBLIME? RQ4: How robust is SUBLIME to adversarial graph structures? and RQ5: What kind of graph structure is learned by SUBLIME?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setups</head><p>Downstream tasks for evaluation. We use node classification and node clustering tasks to evaluate the quality of learned topology. For node classification, We conduct experiments on both structure inference/refinement scenarios, and use classification accuracy as our metric. For node clustering, the experiments are conducted on structure refinement scenario, and four metrics are employed, including clustering accuracy (C-ACC), Normalized Mutual Information (NMI), F1-score (F1) and Adjusted Rand Index (ARI). Datasets. We evaluate SUBLIME on eight real-world benchmark datasets, including four graph-structured datasets (i.e., Cora, Citeseer <ref type="bibr" target="#b33">[34]</ref>, Pubmed <ref type="bibr" target="#b26">[27]</ref> and ogbn-arxiv <ref type="bibr" target="#b16">[17]</ref>) and four non-graph datasets (i.e., Wine, Cancer, Digits and 20news <ref type="bibr" target="#b0">[1]</ref>). Details of datasets are summarized in Appendix E. Baselines. For node classification, we mainly compare SUBLIME with two categories of methods, including three structure-fixed GNN methods (i.e., GCN <ref type="bibr" target="#b21">[22]</ref>, GAT <ref type="bibr" target="#b39">[40]</ref> and GraphSAGE (SAGE for short) <ref type="bibr" target="#b14">[15]</ref>), and six supervised GSL methods (i.e., LDS <ref type="bibr" target="#b11">[12]</ref>, GRCN <ref type="bibr" target="#b52">[53]</ref>, Pro-GNN <ref type="bibr" target="#b19">[20]</ref>, GEN <ref type="bibr" target="#b44">[45]</ref>, IDGL <ref type="bibr" target="#b6">[7]</ref> and SLAPS <ref type="bibr" target="#b10">[11]</ref>). We also consider GDC <ref type="bibr" target="#b22">[23]</ref>, a diffusion-based graph structure improvement method, and SLAPS-2s, a variant of SLAPS <ref type="bibr" target="#b10">[11]</ref> which only uses denoising autoencoder to learn topology, as two baselines of unsupervised GSL. In structure inference scenario, we further add three conventional feature-based classifiers (Logistic Regression, Linear SVM and MLP) for comparison. For node clustering task, we consider baseline methods belonging to the following three  categories: 1) feature-based clustering methods (i.e., K-means <ref type="bibr" target="#b15">[16]</ref> and Spectral Clustering (SC for short) <ref type="bibr" target="#b27">[28]</ref>); 2) structure-based clustering methods (i.e., GraphEncoder (GE for short) <ref type="bibr" target="#b37">[38]</ref>, DeepWalk (DW for short) <ref type="bibr" target="#b32">[33]</ref>, DNGR <ref type="bibr" target="#b3">[4]</ref> and M-NMF <ref type="bibr" target="#b45">[46]</ref>); and 3) attributed graph clustering methods (i.e., RMSC <ref type="bibr" target="#b48">[49]</ref>, TADW <ref type="bibr" target="#b50">[51]</ref>, VGAE <ref type="bibr" target="#b20">[21]</ref>, ARGA <ref type="bibr" target="#b29">[30]</ref>, MGAE <ref type="bibr" target="#b42">[43]</ref>, AGC <ref type="bibr" target="#b54">[55]</ref> and DAEGC <ref type="bibr" target="#b41">[42]</ref>). For other experimental details, including infrastructures and hyper-parameter, interested readers can refer to Appendix F. Our code is available at https://github.com/GRAND-Lab/SUBLIME.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Comparison (RQ1)</head><p>Node classification in structure inference scenario. Table <ref type="table" target="#tab_1">1</ref> reports the classification accuracy of our method and other baselines in structure inference scenario. For structure-fixed GNNs (i.e., GCN, GAT and GraphSAGE) and GSL methods designed for structure refinement scenarios (i.e., GRCN, Pro-GNN, GEN and GDC), we use kNN graphs as their input graphs, where 𝑘 is tuned in the same search space to our method.</p><p>As can be observed, without the guidance of labels, our proposed SUBLIME outperforms all baselines on 3 out of 8 benchmarks and achieves the runner-up results on the rest datasets. This competitive performance benefits from the novel idea of guiding GSL with a selfenhanced learning target by graph contrastive learning. Besides, the result on ogbn-arxiv exhibits the scailbility of SUBLIME.</p><p>We make other observations as follows. Firstly, the performance of structure-fixed GNNs (taking kNN graphs as input) is superior   to conventional feature-based classifiers on most datasets, which shows the benefit of considering the underlying relationship among samples. Secondly, GSL methods achieve better performance than structure-fixed methods, indicating the significance of structure optimization. Thirdly, compared to supervised GSL methods, the unsupervised methods also achieve competitive results without the supervision of labels, which shows their effectiveness.</p><p>Node classification in structure refinement scenario. Table <ref type="table" target="#tab_2">2</ref> summarizes the classification performance of each method in structure refinement scenario. We find that SUBLIME still shows very promising results against not only the self-supervised but also supervised methods, indicating that SUBLIME can leverage selfsupervision signal to improve the original graphs effectively.</p><p>Node clustering in structure refinement scenario. In Table <ref type="table" target="#tab_3">3</ref>, we report the results of node clustering. Compared to baselines, our performance improvement illustrates that optimizing graph structures is indeed helpful to the clustering task. Meanwhile, the implementation of SUBLIME for node clustering task suggests that our learned topology can be applied to not only node classification task but also a wide range of downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study (RQ2)</head><p>In our structure bootstrapping mechanism, the bootstrapping decay rate 𝜏 control the trade-off between updating anchor graph too sharply (with smaller 𝜏) and too slowly (with larger 𝜏). When 𝜏 = 1, anchor graph is never updated and remains as a constant structure.</p><p>To verify the effectiveness of the proposed mechanism, we adjust the value of 𝜏 and the results are shown in Table <ref type="table" target="#tab_4">4</ref>. We also plot the curves of accuracy and loss value w.r.t. training epoch with different 𝜏, which are shown in Fig. <ref type="figure" target="#fig_4">3</ref>. As shown in Table <ref type="table" target="#tab_4">4</ref>, without structure bootstrapping mechanism (𝜏 = 1), the classification accuracy decreases by 1.5% on average, indicating the mechanism helps improve the quality of learned graphs. From Fig. <ref type="figure" target="#fig_4">3</ref>(a), we further find an obvious drop after around 1500 iterations when 𝜏 = 1, demonstrating the lack of effective guidance hurts the performance. When 𝜏 is within [0.999, 0.99999], the accuracy can converge to a high value (as shown in Fig. <ref type="figure" target="#fig_4">3</ref>  meaning that SUBLIME can learn a stable and informative structure with the bootstrapping mechanism. However, the performance declines with 𝜏 becoming smaller, especially on Cora dataset. We conjecture that with sharp updating, the anchor graph tends to be polluted by the learned graph obtained in the early training stage, which fails to capture accurate connections. Another problem caused by a too small 𝜏 is the unstable training, which can be seen in Fig. <ref type="figure" target="#fig_4">3</ref>(a) and 3(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Sensitivity Analysis (RQ3)</head><p>Using the structure inference case, we investigate the sensitivity of critical hyper-parameters in SUBLIME, including the probabilities 𝑝 (𝑥) , 𝑝 (𝑎) for data augmentation and the number of neighbors 𝑘 in kNN for sparsification and learner initialization. The discussion for 𝑝 (𝑥) and 𝑘 are provided below while the analysis for 𝑝 (𝑎) is given in Appendix G. Feature masking probability 𝑝 (𝑥) . Fig. <ref type="figure" target="#fig_6">4</ref>(a) shows the performance under different combinations of masking probabilities of two views on Cora dataset. We observe that the value of 𝑝 (𝑥) 𝑎 between 0.6 and 0.8 produces higher accuracy. Compared to 𝑝 (𝑥) 𝑎 , SUBLIME is less sensitive to the choice of 𝑝 (𝑥) 𝑙 , suggesting a good performance when 𝑝 (𝑥) 𝑙 ∈ [0, 0.7]. When 𝑝 (𝑥) is larger than 0.8, the features will be heavily undermined, resulting worse results. Number of neighbors 𝑘. To investigate its sensitivity, we search the number of neighbors 𝑘 in the range of {5, 10, • • • , 40} for three datasets. As is demonstrated in Fig. <ref type="figure" target="#fig_6">4</ref>(b), the best selection for each dataset is different, i.e., 𝑘 = 30 for Cora, 𝑘 = 20 for Citeseer, and 𝑘 = 15 for Pubmed. A common phenomenon is that a too large or too small 𝑘 results in poor performance. We conjecture that an extremely small 𝑘 may limit the number of beneficial neighbors, while an overlarge 𝑘 causes some noisy connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Robustness Analysis (RQ4)</head><p>To evaluate the robustness of SUBLIME against adversarial graphs, we randomly remove edges from or add edges to the original graph structure of Cora dataset and validate the performance on the corrupted graphs. We change the ratios of modified edges from 0 to 0.9 to simulate different attack intensities. We compared our method to GCN <ref type="bibr" target="#b21">[22]</ref> and Pro-GNN <ref type="bibr" target="#b19">[20]</ref>, a supervised graph structure method for graph adversarial defense. As we can see in Fig. <ref type="figure" target="#fig_8">5</ref>, SUBLIME consistently achieves better or comparable results in both settings. When the edge deletion rates become larger, our method shows more significant performance gains, indicating that SUBLIME has stronger robustness against serious structural attacks.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Visualization (RQ5)</head><p>To investigate what kind of graph structure is learned by SUBLIME, we select a subgraph from Cora dataset with nodes in two categories and visualize the edge weights in original graph, graphs learned by Pro-GNN and SUBLIME, respectively. The selected categories are Case base (C) and Rule learning (R), each of which has 10 labeled nodes (L) and 10 unlabeled nodes (U). Note that the labels of the labeled nodes are used to refine the graph structures in Pro-GNN, but are not used to optimize topology in SUBLIME. As we can see in Fig. <ref type="figure" target="#fig_9">6</ref>, numerous intra-class edges are learned by SUBLIME, while the learned inter-class edges are far fewer than intra-class edges. In contrast, the original graph only provides scarce intra-class edges. We conclude that SUBLIME can learn connections between two nodes sharing similar semantic information, which improves the quality of graph topology. Moreover, in Pro-GNN, there are more connections built across labeled nodes than unlabeled nodes, indicating an edge distribution bias in the graph learned by such a supervised method. Conversely, SUBLIME equally constructs edges across all nodes belonging to the same class as each node can receive the essential supervision from the contrastive objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we make the first investigation on the problem of unsupervised graph structure learning. To tackle this problem, we design a novel method, SUBLIME, which is capable of leveraging data itself to generate optimal graph structures. To learn graph structures, our method uses contrastive learning to maximize the agreement between the learned topology and a self-enhanced learning target. Extensive experiments demonstrate the superiority of SUBLIME and rationality of the learned structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A NOTATIONS</head><p>In this paper, we denote scalars with letters (e.g., 𝑘), column vectors with boldface lowercase letters (e.g., x), matrices with boldface uppercase letters (e.g., X), and sets with calligraphic fonts (e.g., V).</p><p>The frequently used notations are listed in Table <ref type="table">5</ref>.</p><p>Table <ref type="table">5</ref>: Frequently used notations.</p><p>Notation Description G = (A, X)</p><p>The (original) graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑛, 𝑚, 𝑑</head><p>The number of nodes/edges/features. A ∈ [0, 1] 𝑛×𝑛 The (original) adjacency matrix. X ∈ R 𝑛×𝑑</p><p>The feature matrix.</p><formula xml:id="formula_23">G 𝑙 = (S, X)</formula><p>The learned graph / Learner graph view. S ∈ [0, 1] 𝑛×𝑛</p><p>The learned adjacency matrix. S ∈ R 𝑛×𝑛</p><p>The sketched adjacency matrix. E ∈ R 𝑛×𝑑</p><p>The embedding matrix.</p><formula xml:id="formula_24">G 𝑎 = (A 𝑎 , X)</formula><p>Anchor graph view.</p><formula xml:id="formula_25">A 𝑎 ∈ [0, 1] 𝑛×𝑛</formula><p>The anchor adjacency matrix. G 𝑙 , G 𝑎</p><p>The augmented learner/anchor view.</p><formula xml:id="formula_26">𝑑 1 , 𝑑 2</formula><p>The dimension of node representation/projection. H 𝑙 , H 𝑎 ∈ R 𝑛×𝑑1 The representation matrix of learner/anchor view. Z 𝑙 , Z 𝑎 ∈ R 𝑛×𝑑2 The projected representation matrix of learner/anchor view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L</head><p>The contrastive loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑝 𝜔 (•)</head><p>The graph learner with parameter 𝜔.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑞(•)</head><p>The post-processor.</p><formula xml:id="formula_27">T 𝑓 𝑚 (•), T 𝑒𝑑 (•)</formula><p>The feature masking/edge dropping augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑓 𝜃 (•)</head><p>The GNN-based encoder with parameter 𝜃 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑔 𝜑 (•)</head><p>The MLP-based projector with parameter 𝜑.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑘</head><p>The number of neighbors in kNN.</p><p>𝑝 (𝑥) , 𝑝 (𝑎)  The masking/dropping probability for T 𝑓 𝑚 (•)/T 𝑒𝑑 (•).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝜏, 𝑐</head><p>The decay rate/interval for bootstrapping updating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⊙</head><p>The Hadamard operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• ⊺</head><p>The transposition operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ANALYSIS OF GRAPH LEARNERS</head><p>In Table <ref type="table" target="#tab_5">6</ref>, We summarize the properties of the proposed graph learners, including their memory, parameter and time complexity. For metric learning-based graph learners, we consider the complexities with locality-sensitive kNN sparsification post-processing <ref type="bibr" target="#b10">[11]</ref> where the neighbors are selected from a batch of nodes (batch size = 𝑏 1 ). We provide our analysis as follows:</p><p>• Since FGP learner can model each edge independently and directly, it enjoys several advantages such as the flexibility to model connections and low time complexity. However, its O (𝑛 2 ) space complexity makes it hard to be applied to the modeling of large-scale graphs. • Among all metric learning-based learners, attentive learner has the lowest parameter and time complexity w.r.t. dimension 𝑑. It is suitable for the situation with high feature dimension and low correlation between features. • Compared to attentive learner, MLP and GNN learner require larger space and time complexity to consider the correlation between features and original topology. • With the effective kNN sparsification, the memory and time complexity are reduced from O (𝑛 2 ) to O (𝑛), which improves the scalability of the metric learning-based learners. Considering these properties, we allocate the suitable learner for each dataset. Specifically, for small datasets whose node numbers are less than 3000 (e.g., Cora), we use FGP learners to model them due to the flexibility and acceptable complexity. For larger datasets with high-dimensional raw features (e.g., Citeseer), we  Initialize the anchor adjacency matrix by: A a ← A;  </p><formula xml:id="formula_28">Ω Non-linear " # S # S ⊙ ω X E Sim. ! A # S !"#$% X E Sim. ! Ω Ω × X E Sim. ! # S O (𝑛 2 ) O (𝑛 2 ) O (1) Attentive Ω Non-linear " # S # S ⊙ ω X E Sim. ! A # S !"#$% X E Sim. ! Ω Ω × X E Sim. ! # S O (𝑛𝑑𝐿 + 𝑛𝑘) O (𝑑𝐿) O (𝑛𝑑𝐿 + 𝑛𝑑𝑏 1 ) MLP Ω Non-linear " # S # S ⊙ ω X E Sim. ! A # S !"#$% X E Sim. ! Ω Ω × X E Sim. ! # S O (𝑛𝑑𝐿 + 𝑛𝑘) O (𝑑 2 𝐿) O (𝑛𝑑 2 𝐿 + 𝑛𝑑𝑏 1 ) GNN Ω Non-linear " # S # S ⊙ ω X E Sim. ! A # S !"#$% X E Sim. ! Ω Ω × X E Sim. ! # S O (𝑛𝑑𝐿 + 𝑛𝑘) O (𝑑 2 𝐿) O (𝑚𝑑𝐿+ 𝑛𝑑 2 𝐿 + 𝑛𝑑𝑏 1 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C COMPLEXITY ANALYSIS</head><p>We analyze the time complexity of each component of SUBLIME. For graph learner, the complexity has been described in Table <ref type="table" target="#tab_5">6</ref>. The time complexity of post-processor is mainly contributed by sparsification, which is O (𝑛𝑑𝑏 1 ) for effective kNN and O (𝑛 2 𝑑) for </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ALGORITHM</head><p>The training algorithm of SUBLIME is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E DATASETS</head><p>In Table <ref type="table" target="#tab_8">7</ref>, we summarize the statistics of benchmark datasets. The dataset splitting follows the previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref>. Details of these datasets are introduced as follows.</p><p>• Cora <ref type="bibr" target="#b33">[34]</ref> is a citation network where each node is a machine learning paper belonging to 7 research topics and each edge is a citation between papers. • Citeseer <ref type="bibr" target="#b33">[34]</ref> is a citation network containing 6 types of machine learning papers: Agents, AI, DB, IR, ML, and HCI. Nodes denote papers and edges denote citation relationships. • Pubmed <ref type="bibr" target="#b26">[27]</ref> is a citation network from the PubMed database, where nodes are papers about three diabete types about diabetes and edges are citations among them. • ogbn-arxiv <ref type="bibr" target="#b16">[17]</ref> is a citation network with Computer Science arXiv papers. The features are the embeddings of words in its title and abstract. The labels are 40 subject areas. • Wine <ref type="bibr" target="#b0">[1]</ref> is a non-graph dataset containing the results of a chemical analysis of 178 wines derived from three different cultivars. Features are the quantities of 13 constituents found. • Cancer <ref type="bibr" target="#b0">[1]</ref> is a binary classification dataset of diagnosis of breast tissues (malignant/benign). The features are computed from a digitized image of a breast mass. • Digits <ref type="bibr" target="#b0">[1]</ref> is a non-graph dataset containing handwritten digits in 10 classes. Each sample is a 8 × 8 image of a digit. • 20news <ref type="bibr" target="#b0">[1]</ref> is a non-graph dataset comprising newsgroups posts on 20 topics. Following <ref type="bibr" target="#b11">[12]</ref>, we select 10 topics with 9, 607 samples in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F IMPLEMENTATION DETAILS F.1 Computing Infrastructures</head><p>We implement SUBLIME using PyTorch 1.7.1 <ref type="bibr" target="#b30">[31]</ref> and DGL 0.7.1 <ref type="bibr" target="#b43">[44]</ref>. All experiments are conducted on a Linux server with an Intel Xeon 4214R CPU and four Quadro RTX 6000 GPUs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Evaluation Details</head><p>Through node classification tasks, we evaluate the quality of the learned structures by re-training a classifier with the learned structure as its constant input. Specifically, we use the learned adjacency matrices to train GCN-based classification models, and record the testing result with the highest validation accuracy. The averaged accuracy over five rounds of running is used to assess the classification performance. For ogbn-arxiv dataset, we utilize a three-layer GCN with 256 hidden units as the evaluation model. For the rest datasets, a two-layer GCN with 32 hidden units is employed.</p><p>For node clustering tasks, we evaluate the performance of our method by measuring the quality of the learned representations. Concretely, following the baseline methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b54">55]</ref>, we train our framework for a fixed number of epochs and apply K-means algorithm for 10 runs to group the learned representations. The representations are generated by the contrastive learning encoder 𝑓 𝜃 taking learned graph G 𝑙 = (S, X) as its input without augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Hyper-parameter Specifications</head><p>We perform grid search to select hyper-parameters on the following searching space: the dimension of representation and projection is searched in {16, 32, 64, 128, 256, 512}; 𝑘 on kNN is tuned amongst {5, 10, 15, 20, 25, 30, 35, 40}; feature masking probability 𝑝 (𝑥) is tuned from 0.1 to 0.9; edge dropping probability 𝑝 (𝑎) is searched in {0, 0.25, 0.5, 0.75}; the bootstrapping decay rate is chosen from {0.99, 0.999, 0.9999, 0.99999, 1}; and the learning rate of Adam optimizer is selected from {0.01, 0.001, 0.0001}. The temperature for contrastive loss is fixed to 0.2. The layer numbers of encoder (𝐿 1 ), projector (𝐿 2 ), and embedding network (𝐿) are set to 2.</p><p>For our baselines, we reproduce the experiments using their official open-source codes or borrow the reported results in their papers. We carefully tune their hyper-parameters to achieve optimal performance. To compare fairly, we use the random seeds {0, 1, 2, 3, 4} for all classification methods, and fix the seed to 0 for clustering methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G PARAMETER SENSITIVITY OF 𝑝 (𝑎)</head><p>We vary the dropping rate 𝑝 (𝑎) from 0 to 0.95 on Cora, Citeseer and Pubmed datasets, and the results are shown in Fig. <ref type="figure" target="#fig_12">7</ref>. As we can see, when 𝑝 (𝑎) is between 0.2 and 0.65, SUBLIME achieves better performance. When the edge dropping rate is overlarge, the structures on both views will be deteriorated, causing a sharp drop of performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Our proposed unsupervised GSL paradigm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Concept maps of (a) the existing supervised GSL paradigm and (b) our proposed unsupervised GSL paradigm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The overall pipeline of SUBLIME. In the graph structure learning module, the graph learner 𝑝 𝜔 generates the sketched adjacency matrix S, and then the post processor 𝑞 converts S into the learned structure S. After that, the structure bootstrapping contrastive learning module optimizes S by maximizing the agreement between the learner view and anchor view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Test accuracy w.r.t. epoch. Contrastive loss value w.r.t. epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Curves of training process on Cora dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Accuracy w.r.t. feature masking rates. Accuracy w.r.t. number of neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Sensitivity of hyper-parameters 𝑝 (𝑥) and 𝑘.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Accuracy w.r.t. edge deletion rate. Accuracy w.r.t. edge addition rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Test accuracy in the scenarios where graph structures are perturbed by edge deletion or addition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Heatmaps of the subgraph adjacency matrices of (a) the original graph with self-loop, the graph learned by (b) Pro-GNN and (c) SUBLIME on Cora dataset. A block in darker color indicates a larger edge weight between two nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>4 else 5 9 Calculate</head><label>459</label><figDesc>Initialize the anchor adjacency matrix by: A a ← I;6 end 7 for 𝑒 = 1, 2, • • • , 𝐸 do 8Calculate S with graph learner 𝑝 𝜔 by Eq. (1) or (2); S with post-processor 𝑞( S) by Eq. (6) -Eq. (8); 10 Establish two graph views by G 𝑙 = (S, X), G 𝑎 = (A 𝑎 , X); 11 Obtain augmented graph views G 𝑙 , G 𝑎 by Eq. (9) -Eq. (11) with probability 𝑝</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Sensitivity analysis for 𝑝 (𝑎) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>4.3.1 Graph View Establishment. Different from general graph contrastive learning methods<ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b58">59]</ref> that obtain both views from the original data, SUBLIME defines the learned graph as one view, and constructs the other view with input data. The former, named learner view, explores potential structures in every step. The latter, named anchor view, provides a stable learning target for GSL.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Node classification accuracy (percentage with standard deviation) in structure inference scenario. Available data for graph structure learning during the training phase is shown in the first column, where X, Y, A 𝑘𝑛𝑛 correspond to node features, labels and the adjacency matrix of kNN graph, respectively. The highest and second highest results are highlighted with boldface and underline, respectively. The symbol "OOM" means out of memory.</figDesc><table><row><cell>Available Data for GSL</cell><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell cols="2">Dataset ogbn-arxiv</cell><cell>Wine</cell><cell>Cancer</cell><cell>Digits</cell><cell>20news</cell></row><row><cell>-</cell><cell>LR</cell><cell>60.8±0.0</cell><cell>62.2±0.0</cell><cell>72.4±0.0</cell><cell>52.5±0.0</cell><cell cols="2">92.1±1.3</cell><cell>93.3±0.5</cell><cell>85.5±1.5</cell><cell>42.7±1.7</cell></row><row><cell>-</cell><cell>Linear SVM</cell><cell>58.9±0.0</cell><cell>58.3±0.0</cell><cell>72.7±0.1</cell><cell>51.8±0.0</cell><cell cols="2">93.9±1.6</cell><cell>90.6±4.5</cell><cell>87.1±1.8</cell><cell>40.3±1.4</cell></row><row><cell>-</cell><cell>MLP</cell><cell>56.1±1.6</cell><cell>56.7±1.7</cell><cell>71.4±0.0</cell><cell>54.7±0.1</cell><cell cols="2">89.7±1.9</cell><cell>92.9±1.2</cell><cell>36.3±0.3</cell><cell>38.6±1.4</cell></row><row><cell>-</cell><cell>GCN 𝑘𝑛𝑛 [22]</cell><cell>66.5±0.4</cell><cell>68.3±1.3</cell><cell>70.4±0.4</cell><cell>54.1±0.3</cell><cell cols="2">93.2±3.1</cell><cell>83.8±1.4</cell><cell>91.3±0.5</cell><cell>41.3±0.6</cell></row><row><cell>-</cell><cell>GAT 𝑘𝑛𝑛 [40]</cell><cell>66.2±0.5</cell><cell>70.0±0.6</cell><cell>69.6±0.5</cell><cell>OOM</cell><cell cols="2">91.5±2.4</cell><cell>95.1±0.8</cell><cell>91.4±0.1</cell><cell>45.0±1.2</cell></row><row><cell>-</cell><cell>SAGE 𝑘𝑛𝑛 [15]</cell><cell>66.1±0.7</cell><cell>68.0±1.6</cell><cell>68.7±0.2</cell><cell>55.2±0.4</cell><cell cols="2">87.4±0.8</cell><cell>93.7±0.3</cell><cell>91.6±0.7</cell><cell>45.4±0.4</cell></row><row><cell>X, Y</cell><cell>LDS [12]</cell><cell>71.5±0.8</cell><cell>71.5±1.1</cell><cell>OOM</cell><cell>OOM</cell><cell cols="2">97.3±0.4</cell><cell>94.4±1.9</cell><cell>92.5±0.7</cell><cell>46.4±1.6</cell></row><row><cell>X, Y, A 𝑘𝑛𝑛</cell><cell>GRCN [53]</cell><cell>69.6±0.2</cell><cell>70.4±0.3</cell><cell>70.6±0.1</cell><cell>OOM</cell><cell cols="2">96.6±0.4</cell><cell>95.4±0.6</cell><cell>92.8±0.2</cell><cell>41.8±0.2</cell></row><row><cell>X, Y, A 𝑘𝑛𝑛</cell><cell>Pro-GNN [20]</cell><cell>69.2±1.4</cell><cell>69.8±1.7</cell><cell>OOM</cell><cell>OOM</cell><cell cols="2">95.1±1.5</cell><cell>96.5±0.1</cell><cell>93.9±1.9</cell><cell>45.7±1.4</cell></row><row><cell>X, Y, A 𝑘𝑛𝑛</cell><cell>GEN [45]</cell><cell>69.1±0.7</cell><cell>70.7±1.1</cell><cell>70.7±0.9</cell><cell>OOM</cell><cell cols="2">96.9±1.0</cell><cell>96.8±0.4</cell><cell>94.1±0.4</cell><cell>47.1±0.3</cell></row><row><cell>X, Y</cell><cell>IDGL [7]</cell><cell>70.9±0.6</cell><cell>68.2±0.6</cell><cell>70.1±1.3</cell><cell>55.0±0.2</cell><cell cols="2">98.1±1.1</cell><cell>95.1±1.0</cell><cell>93.2±0.9</cell><cell>48.5±0.6</cell></row><row><cell>X, Y</cell><cell>SLAPS [11]</cell><cell>73.4±0.3</cell><cell>72.6±0.6</cell><cell>74.4±0.6</cell><cell>56.6±0.1</cell><cell cols="2">96.6±0.4</cell><cell>96.6±0.2</cell><cell>94.4±0.7</cell><cell>50.4±0.7</cell></row><row><cell>A 𝑘𝑛𝑛</cell><cell>GDC [23]</cell><cell>68.1±1.2</cell><cell>68.8±0.8</cell><cell>68.4±0.4</cell><cell>OOM</cell><cell cols="2">96.1±1.0</cell><cell>95.9±0.4</cell><cell>92.6±0.5</cell><cell>46.4±0.9</cell></row><row><cell>X</cell><cell>SLAPS-2s [11]</cell><cell>72.1±0.4</cell><cell>69.4±1.4</cell><cell>71.1±0.5</cell><cell>54.2±0.2</cell><cell cols="2">96.2±2.1</cell><cell>95.9±1.2</cell><cell>93.6±0.8</cell><cell>47.7±0.7</cell></row><row><cell>X</cell><cell>SUBLIME</cell><cell>73.0±0.6</cell><cell>73.1±0.3</cell><cell>73.8±0.6</cell><cell>55.5±0.1</cell><cell cols="2">98.2±1.6</cell><cell>97.2±0.2</cell><cell>94.3±0.4</cell><cell>49.2±0.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Node classification accuracy (percentage with standard deviation) in structure refinement scenario.</figDesc><table><row><cell>Available Data for GSL</cell><cell>Method</cell><cell>Cora</cell><cell cols="3">Dataset Citeseer Pubmed ogbn-arxiv</cell></row><row><cell>-</cell><cell>GCN</cell><cell>81.5</cell><cell>70.3</cell><cell>79.0</cell><cell>71.7±0.3</cell></row><row><cell>-</cell><cell>GAT</cell><cell cols="3">83.0±0.7 72.5±0.7 79.0±0.3</cell><cell>OOM</cell></row><row><cell>-</cell><cell>SAGE</cell><cell cols="3">77.4±1.0 67.0±1.0 76.6±0.8</cell><cell>71.5±0.3</cell></row><row><cell>X, Y, A</cell><cell>LDS</cell><cell cols="2">83.9±0.6 74.8±0.3</cell><cell>OOM</cell><cell>OOM</cell></row><row><cell>X, Y, A</cell><cell>GRCN</cell><cell cols="3">84.0±0.2 73.0±0.3 78.9±0.2</cell><cell>OOM</cell></row><row><cell>X, Y, A</cell><cell cols="3">Pro-GNN 82.1±0.4 71.3±0.4</cell><cell>OOM</cell><cell>OOM</cell></row><row><cell>X, Y, A</cell><cell>GEN</cell><cell cols="3">82.3±0.4 73.5±1.5 80.9±0.8</cell><cell>OOM</cell></row><row><cell>X, Y, A</cell><cell>IDGL</cell><cell cols="3">84.0±0.5 73.1±0.7 83.0±0.2</cell><cell>72.0±0.3</cell></row><row><cell>A</cell><cell>GDC</cell><cell cols="3">83.6±0.2 73.4±0.3 78.7±0.4</cell><cell>OOM</cell></row><row><cell>X, A</cell><cell cols="4">SUBLIME 84.2±0.5 73.5±0.6 81.0±0.6</cell><cell>71.8±0.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Node clustering performance (4 metrics in percentage) in structure refinement scenario.</figDesc><table><row><cell>Method</cell><cell cols="2">Cora C-ACC NMI</cell><cell>F1</cell><cell cols="4">Citeseer ARI C-ACC NMI F1</cell><cell>ARI</cell></row><row><cell>K-means</cell><cell>50.0</cell><cell cols="3">31.7 37.6 23.9</cell><cell>54.4</cell><cell cols="3">31.2 41.3 28.5</cell></row><row><cell>SC</cell><cell>39.8</cell><cell cols="3">29.7 33.2 17.4</cell><cell>30.8</cell><cell>9.0</cell><cell>25.7</cell><cell>8.2</cell></row><row><cell>GE</cell><cell>30.1</cell><cell>5.9</cell><cell>23.0</cell><cell>4.6</cell><cell>29.3</cell><cell>5.7</cell><cell>21.3</cell><cell>4.3</cell></row><row><cell>DW</cell><cell>52.9</cell><cell cols="3">38.4 43.5 29.1</cell><cell>39.0</cell><cell cols="3">13.1 30.5 13.7</cell></row><row><cell>DNGR</cell><cell>41.9</cell><cell cols="3">31.8 34.0 14.2</cell><cell>32.6</cell><cell cols="2">18.0 30.0</cell><cell>4.3</cell></row><row><cell>M-NMF</cell><cell>42.3</cell><cell cols="3">25.6 32.0 16.1</cell><cell>33.6</cell><cell>9.9</cell><cell>25.5</cell><cell>7.0</cell></row><row><cell>RMSC</cell><cell>46.6</cell><cell cols="3">32.0 34.7 20.3</cell><cell>51.6</cell><cell cols="3">30.8 40.4 26.6</cell></row><row><cell>TADW</cell><cell>53.6</cell><cell cols="3">36.6 40.1 24.0</cell><cell>52.9</cell><cell cols="3">32.0 43.6 28.6</cell></row><row><cell>VGAE</cell><cell>59.2</cell><cell cols="3">40.8 45.6 34.7</cell><cell>39.2</cell><cell cols="3">16.3 27.8 10.1</cell></row><row><cell>ARGA</cell><cell>64.0</cell><cell cols="3">44.9 61.9 35.2</cell><cell>57.3</cell><cell cols="3">35.0 54.6 34.1</cell></row><row><cell>MGAE</cell><cell>68.1</cell><cell cols="3">48.9 53.1 56.5</cell><cell>66.9</cell><cell cols="3">41.6 52.6 42.5</cell></row><row><cell>AGC</cell><cell>68.9</cell><cell cols="3">53.7 65.6 44.8</cell><cell>67.0</cell><cell cols="3">41.1 62.5 41.5</cell></row><row><cell>DAEGC</cell><cell>70.4</cell><cell cols="3">52.8 68.2 49.6</cell><cell>67.2</cell><cell cols="3">39.7 63.6 41.0</cell></row><row><cell>SUBLIME</cell><cell>71.3</cell><cell cols="3">54.2 63.5 50.3</cell><cell>68.5</cell><cell cols="3">44.1 63.2 43.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Test accuracy corresponding to different bootstrapping decay rate 𝜏 in structure refinement scenario.</figDesc><table><row><cell>Dataset</cell><cell>1</cell><cell cols="3">Bootstrapping decay rate 𝜏 0.99999 0.9999 0.999 0.99</cell></row><row><cell>Cora</cell><cell>82.1</cell><cell>83.2</cell><cell>84.2</cell><cell>82.4 70.9</cell></row><row><cell cols="2">Citeseer 71.9</cell><cell>72.6</cell><cell>73.5</cell><cell>73.4 72.6</cell></row><row><cell cols="2">Pubmed 80.1</cell><cell>80.3</cell><cell>81.0</cell><cell>80.8 80.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Properties of graph learners.</figDesc><table><row><cell>Learner</cell><cell>Sketch</cell><cell>Memory</cell><cell>Params</cell><cell>Time</cell></row><row><cell>FGP</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Number of epochs 𝐸. Output: Learned Adjacency Matrix S 1 Initialize parameters 𝜔, 𝜃 , 𝜑; 2 if A is provided then 3</figDesc><table><row><cell cols="3">Algorithm 1: The training algorithm of SUBLIME</cell></row><row><cell cols="3">Input: Feature matrix X; Adjacency matrix A (optional);</cell></row><row><cell cols="3">Number of nearest neighbors 𝑘; Bootstrapping decay</cell></row><row><cell cols="3">rate and interval 𝜏,𝑐; Feature masking probability</cell></row><row><cell>𝑝</cell><cell>(𝑥) 𝑙 , 𝑝</cell><cell>(𝑥)</cell></row></table><note>𝑎 ; Edge dropping probability 𝑝 (𝑎) ; Temperature 𝑡;</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>𝑙 , H 𝑎 with encoder 𝑓 𝜃 by Eq. (12); 𝑙 , Z 𝑎 with encoder 𝑔 𝜙 by Eq. (13);Bootstrapping update A a with decay 𝜏 by Eq. (15) ;</figDesc><table><row><cell>14</cell><cell>Calculate the contrastive loss L by Eq. (14) ;</cell></row><row><cell>16</cell><cell>if e mod c = 0 then</cell></row><row><cell>17</cell><cell></cell></row><row><cell>18</cell><cell>end</cell></row><row><cell cols="2">19 end</cell></row><row><cell cols="2">choose attentive learner considering its low parameter/time com-</cell></row><row><cell cols="2">plexity w.r.t. dimension 𝑑. For large-scale datasets with relevantly</cell></row><row><cell cols="2">low feature dimensions (e.g., 20news), we adopt MLP learners to</cell></row><row><cell cols="2">capture the correlation between features. In graph refinement sce-</cell></row><row><cell cols="2">narios where original graphs are available, GNN learners can be</cell></row><row><cell cols="2">further considered to leverage the extra topology information.</cell></row></table><note>(𝑎) ; 12 Calculate node representations H 13 Calculate projections Z 15 Update parameters 𝜔, 𝜃 , 𝜑 by applying gradient descent;</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Statistics of datasets. In the contrastive learning module, the complexities of feature masking and edge dropping are O (𝑑) and O (𝑚), respectively. For the encoder and projector, the total complexity isO (𝑚𝑑 1 𝐿 1 + 𝑛𝑑 2 1 𝐿 1 + 𝑛𝑑 2 2 𝐿 2 ).For contrastive loss computation, the complexity is O (𝑛 2 ) for its full-graph version, O (𝑛𝑏 2 ) for the minibatch version, where 𝑏 2 is the batch size of contrastive learning.</figDesc><table><row><cell>Dataset</cell><cell>Nodes</cell><cell>Edges</cell><cell cols="3">Classes Features Label Rate</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>7</cell><cell>1,433</cell><cell>0.052</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4,732</cell><cell>6</cell><cell>3,703</cell><cell>0.036</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>44,338</cell><cell>3</cell><cell>500</cell><cell>0.003</cell></row><row><cell cols="3">ogbn-arxiv 169,343 1,166,243</cell><cell>40</cell><cell>128</cell><cell>0.537</cell></row><row><cell>Wine</cell><cell>178</cell><cell>N/A</cell><cell>3</cell><cell>13</cell><cell>0.056</cell></row><row><cell>Cancer</cell><cell>569</cell><cell>N/A</cell><cell>2</cell><cell>30</cell><cell>0.018</cell></row><row><cell>Digits</cell><cell>1,797</cell><cell>N/A</cell><cell>10</cell><cell>64</cell><cell>0.028</cell></row><row><cell>20news</cell><cell>9,607</cell><cell>N/A</cell><cell>10</cell><cell>236</cell><cell>0.010</cell></row><row><cell cols="2">conventional kNN.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The corresponding author is Shirui Pan. This work was supported by an ARC Future Fellowship (No. FT210100097).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust spectral clustering for noisy data: Modeling sparse corruptions improves latent embeddings</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yves</forename><surname>Matkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="737" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep neural networks for learning graph representations</title>
		<author>
			<persName><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiongkai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
				<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Iterative deep graph learning for graph neural networks: Better and robust node embeddings</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Zaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training</title>
		<author>
			<persName><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
				<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph learning from data under Laplacian and structural constraints</title>
		<author>
			<persName><forename type="first">Eduardo</forename><surname>Hilmi E Egilmez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Pavez</surname></persName>
		</author>
		<author>
			<persName><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="825" to="841" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SLAPS: Self-Supervision Improves Structure Learning for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Bahare</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Layla</forename><forename type="middle">El</forename><surname>Asri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazemi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning discrete structures for graph neural networks</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1972" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations</title>
		<author>
			<persName><forename type="first">John</forename><surname>Giorgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osvald</forename><surname>Nitski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="879" to="895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bootstrap Your Own Latent -A New Approach to Self-Supervised Learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21271" to="21284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Algorithm AS 136: A k-means clustering algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manchek</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the royal statistical society. series c (applied statistics)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="1979">1979. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Open Graph Benchmark: Datasets for Machine Learning on Graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ANEMONE: Graph Anomaly Detection with Multi-Scale Contrastive Learning</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianhua</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
				<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3122" to="3126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph structure learning for robust graph neural networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="66" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Variational Graph Auto-Encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems Workshop</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Weißenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="13354" to="13366" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Anomaly Detection on Attributed Networks via Contrastive Self-Supervised Learning</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Graph self-supervised learning: A survey</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00111</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structural inference for uncertain networks</title>
		<author>
			<persName><forename type="first">Travis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">Ej</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page">12306</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Query-driven active surveying for collective classification</title>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umd</forename><surname>Edu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Workshop on Mining and Learning with Graphs</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On spectral clustering: Analysis and an algorithm</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Andrew Y Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarially regularized graph autoencoder for graph embedding</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Shirui Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2609" to="2615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph representation learning via graphical mutual information maximization</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minnan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
				<meeting>The Web Conference</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fedproto: Federated prototype learning over heterogeneous devices</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning deep representations for graph clustering</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep Graph Infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Contrastive Graph Poisson Networks: Semi-Supervised Learning with Extremely Limited Labels</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibing</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attributed Graph Clustering: A Deep Attentional Embedding Approach</title>
		<author>
			<persName><forename type="first">Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3670" to="3676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mgae: Marginalized graph autoencoder for graph clustering</title>
		<author>
			<persName><forename type="first">Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
				<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph Structure Estimation Neural Networks</title>
		<author>
			<persName><forename type="first">Ruijia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanpeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="342" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Community preserving network embedding</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-view Graph Contrastive Representation Learning for Drug-Drug Interaction Prediction</title>
		<author>
			<persName><forename type="first">Yingheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaosen</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2921" to="2933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A Comprehensive Survey on Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robust multi-view spectral clustering via low-rank and sparse decomposition</title>
		<author>
			<persName><forename type="first">Rongkai</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Network representation learning with rich text information</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Graph-revised convolutional network</title>
		<author>
			<persName><forename type="first">Donghan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruohong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="378" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Self-Supervised Multi-Channel Hypergraph Convolutional Network for Social Recommendation</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="413" to="424" />
		</imprint>
	</monogr>
	<note>Nguyen Quoc Viet Hung, and Xiangliang Zhang</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Attributed Graph Clustering via Adaptive Graph Convolution</title>
		<author>
			<persName><forename type="first">Xiaotong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4327" to="4333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A Vertical Federation Framework Based on Representation Learning</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanglin</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on Big Data Analytics for Cyber-Physical System in Smart City</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="627" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Yizhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10698</idno>
		<title level="m">Towards Graph Self-Supervised Learning with Contrastive Adjusted Zooming</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03036</idno>
		<title level="m">Deep Graph Structure Learning for Robust Representations: A Survey</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
				<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2069" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
