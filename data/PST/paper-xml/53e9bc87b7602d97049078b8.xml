<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Silhouette-Based Method for Object Classification and Human Action Recognition in Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yiğithan</forename><surname>Dedeoğlu</surname></persName>
							<email>yigithan@cs.bilkent.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Bilkent University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">B</forename><forename type="middle">Uğur</forename><surname>Töreyin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Electronics Engineering</orgName>
								<address>
									<postCode>06800</postCode>
									<settlement>Bilkent, Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Uğur</forename><surname>Güdükbay</surname></persName>
							<email>gudukbay@cs.bilkent.edu.tr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Bilkent University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">Enis</forename><surname>Çetin</surname></persName>
							<email>cetin@bilkent.edu.tr</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Electronics Engineering</orgName>
								<address>
									<postCode>06800</postCode>
									<settlement>Bilkent, Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Silhouette-Based Method for Object Classification and Human Action Recognition in Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">53BBA60D048378FE4F8FBE5B07B91BEA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we present an instance based machine learning algorithm and system for real-time object classification and human action recognition which can help to build intelligent surveillance systems. The proposed method makes use of object silhouettes to classify objects and actions of humans present in a scene monitored by a stationary camera. An adaptive background subtracttion model is used for object segmentation. Template matching based supervised learning method is adopted to classify objects into classes like human, human group and vehicle; and human actions into predefined classes like walking, boxing and kicking by making use of object silhouettes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Classifying types and understanding activities of moving objects in video is both a challenging problem and an interesting research area with many promising applications. Our motivation in studying this problem is to design a human action recognition system that can be integrated into an ordinary visual surveillance system with real-time moving object detection, classification and activity analysis capabilities. The system is therefore supposed to work in real time. Considering the complexity of temporal video data, efficient methods must be adopted to create a fast, reliable and robust system. In this paper, we present such a system which operates on gray scale video imagery from a stationary camera.</p><p>In the proposed system moving object detection is handled by the use of an adaptive background subtraction scheme which reliably works both in indoor and outdoor environments <ref type="bibr" target="#b6">[7]</ref>.</p><p>After segmenting moving pixels from the static background of the scene, connected regions are classified into predetermined object categories: human, human group and vehicle. The classification algorithm depends on the comparison of the silhouettes of the detected objects with pre-labeled (classified) templates in an object silhouette database. The template database is created by collecting sample object silhouettes from sample videos and labeling them manually with appropriate categories. The silhouettes of the objects are extracted from the connected foreground regions by using a contour tracing algorithm <ref type="bibr" target="#b10">[11]</ref>. The action recognition system also exploits objects' silhouettes obtained from video sequences to classify actions. It mainly consists of two major steps: manual creation of silhouette and action templates offline and automatic recognition of actions in real-time. In classifying actions of humans into predetermined classes like walking, boxing and kicking; temporal signatures of different actions in terms of silhouette poses are used.</p><p>The remainder of this paper is organized as follows. Section 2 gives an overview of the related work. In the next two sections we give the details of moving object segmentation and object classification. In the next section, visual action recognition system is explained. Experimental results are discussed in Section 6 and finally we conclude the paper with Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There have been a number of surveys about object detection, classification and human activity analysis in the literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Detecting regions corresponding to moving objects such as people and vehicles in video is the first basic step of almost every vision system because it provides a focus of attention and simplifies the processing on subsequent analysis steps. Due to dynamic changes in natural scenes such as sudden illumination and weather changes, repetitive motions that cause clutter (tree leaves moving in blowing wind), motion detection is a difficult problem to process reliably. Frequently used techniques for moving object detection are background subtraction, statistical methods, temporal differencing and optical flow <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Moving regions detected in video may correspond to different objects in real-world such as pedestrians, vehicles, clutter, etc. It is very important to recognize the type of a detected object in order to track it reliably and analyze its activities correctly. Currently, there are two major approaches towards moving object classification which are shape-based and motion-based methods <ref type="bibr" target="#b25">[26]</ref>. Shape-based methods make use of the objects' 2D spatial information like bounding rectangle, area, silhouette and gradient of detected object regions; whereas motion-based methods use temporally tracked features of objects for the classification solution.</p><p>The approach presented in <ref type="bibr" target="#b14">[15]</ref> makes use of the objects' silhouette contour length and area information to classify detected objects into three groups: human, vehicle and other. The method depends on the assumption that humans are, in general, smaller than vehicles and have complex shapes. Dispersedness is used as the classification metric and it is defined as the square of contour length (perimeter) over object's are. Classification is performed at each frame and tracking results are used to improve temporal classification consistency.</p><p>The classification method developed by Collins et al. <ref type="bibr" target="#b6">[7]</ref> uses view dependent visual features of detected objects to train a neural network classifier to recognize four classes: human, human group, vehicle and clutter. The inputs to the neural network are the dispersedness, area and aspect ratio of the object region and the camera zoom magnification. Like the previous method, classification is performed at each frame and results are kept in a histogram to improve temporal consistency of classification. Some of the methods in the literature use only temporal motion features of objects in order to recognize their classes <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>. In general, they are used to distinguish non-rigid objects (e.g. human) from rigid objects (e.g. vehicles). The method proposed in <ref type="bibr" target="#b5">[6]</ref> is based on the temporal self-similarity of a moving object. As an object that exhibits periodic motion evolves, its self-similarity measure also shows a periodic motion. The method exploits this clue to categorize moving objects using periodicity.</p><p>The systems for action recognition using video can be divided into three groups according to the methods they use: general signal processing techniques to match action signals, template matching and state-space approaches.</p><p>The first group treats the action recognition problem as a classification problem of the temporal activity signals of the objects according to pre-labeled reference signals representing typical human actions <ref type="bibr" target="#b25">[26]</ref>. For instance Kanade et al. makes use of the signals generated by the change of the angle between the torso and the vertical line that passes through a human's body to distinguish walking and running patterns <ref type="bibr" target="#b6">[7]</ref>. In another work Schuldt et al. make use of a local SVM approach to define local properties of complex motion patterns and classify the patterns using well known popular classifier Support Vector Machine <ref type="bibr" target="#b20">[21]</ref>. General methods such as Dynamic time warping, Hidden Markov models and Neural Networks are used to process the action signals.</p><p>Second group of approaches converts image sequences into static shape patterns and in the recognition phase compares the patterns with pre-stored ones. For instance by using PCA, Chomat et al. created motion templates and a Bayes classifier was used to perform action recognition <ref type="bibr" target="#b3">[4]</ref>.</p><p>The last group considers each pose of the human body as a state and calculates a probability density function for each different action sequences <ref type="bibr" target="#b23">[24]</ref>. A sequence can be thought of as a tour between different states. Hence the probability density function can be calculated from different tours of the same action. The probability functions than can be used to recognize test sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Scene Background for Segmentation</head><p>We use a combination of a background model and low-level image post-processing methods to create a foreground pixel map and extract object features at every video frame. Our implementation of background subtraction algorithm is partially inspired by the study presented in <ref type="bibr" target="#b6">[7]</ref> and works on grayscale video imagery from a static camera. Background subtraction method initializes a reference background with the first few frames of video input. Then it subtracts the intensity value of each pixel in the current image from the corresponding value in the reference background image. The difference is filtered with an adaptive threshold per pixel to account for frequently changing noisy pixels. The reference background image and the threshold values are updated with an IIR filter to adapt to dynamic scene changes.</p><p>Let I n (x) represent the gray-level intensity value at pixel position (x) and at time instance n of video image sequence I which is in the range [0, 255]. Let B n (x) be the corresponding background intensity value for pixel position (x) estimated over time from video images I 0 through I n -1. As the generic background subtraction scheme suggests, a pixel at position (x) in the current video image belongs to foreground if it satisfies:</p><p>where T n (x) is an adaptive threshold value estimated using the image sequence I 0 through I n -1. The above equation is used to generate the foreground pixel map which represents the foreground regions as a binary array where a 1 corresponds to a foreground pixel and a 0 stands for a background pixel. The reference background B n (x) is initialized with the first video image I 0 , B 0 = I 0 , and the threshold image is initialized with some pre-determined value (e.g. 15).</p><p>Since this system will be used in outdoor environments as well as indoor environments, the background model needs to adapt itself to the dynamic changes such as global illumination change (day night transition) and long term background update (parking a car in front of a building). Therefore the reference background and threshold images are dynamically updated with incoming images. The update scheme is different for pixel positions which are detected as belonging to foreground (x Є FG) and which are detected as part of the background (x Є BG):</p><p>where α, β and γ (Є [0.0, 1.0]) are learning constants which specify how much information from the incoming image is put to the background and threshold images.</p><p>The output of foreground region detection algorithm generally contains noise and therefore is not appropriate for further processing without special post-processing. Morphological operations, erosion and dilation <ref type="bibr" target="#b10">[11]</ref>, are applied to the foreground pixel map in order to remove noise that is caused by the first three of the items listed above. Our aim in applying these operations is to remove noisy foreground pixels that do not correspond to actual foreground regions and to remove the noisy background pixels near and inside object regions that are actually foreground pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. Sample objects and their silhouettes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Calculating Object Features</head><p>After detecting foreground regions and applying post-processing operations to remove noise and shadow regions, the filtered foreground pixels are grouped into connected regions (blobs) and labeled by using a two-level connected component labeling algorithm presented in <ref type="bibr" target="#b10">[11]</ref>. After finding individual blobs that correspond to objects, spatial features like bounding box, size, center of mass and silhouettes of these regions are calculated.</p><p>In order to calculate the center of mass point, C m = (x Cm , y Cm ), of an object O, we use the following equation <ref type="bibr" target="#b17">[18]</ref>:</p><p>where n is the number of pixels in O.</p><p>Both in offline and online steps of the classification algorithm, the silhouettes of the detected object regions are extracted from the foreground pixel map by using a contour tracing algorithm presented in <ref type="bibr" target="#b10">[11]</ref>. Figure <ref type="figure">1</ref>  where the Dist function is the Euclidian distance.</p><p>Different objects have different shapes in video and therefore have silhouettes of varying sizes. Even the same object has altering contour size from frame to frame. In order to compare signals corresponding to different sized objects accurately and to make the comparison metric scale-invariant we fix the size of the distance signal. Let N be the size of a distance signal DS and let C be the constant for fixed signal length. The fix-sized distance signal is then calculated by sub-sampling or supersampling the original signal DS as follows:</p><p>In the next step, the scaled distance signal is normalized to have integral unit area. The normalized distance signal is calculated using the following equation:</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows a sample silhouette and its original and scaled distance signals. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Classifying Objects</head><p>The ultimate aim of different smart visual surveillance applications is to extract semantics from video to be used in higher level activity analysis tasks. Categorizing the type of a detected video object is a crucial step in achieving this goal. With the help of object type information, more specific and accurate methods can be developed to recognize higher level actions of video objects. Hence, we present a video object classification method based on object shape similarity to be used as a part of a "smart" visual surveillance system. Typical video scenes may contain a variety of objects such as people, vehicles, animals, natural phenomenon (e.g. rain, snow), plants and clutter. However, main target of interest in surveillance applications are generally humans and vehicles.</p><p>The classification metric used in our method measures object similarity based on the comparison of silhouettes of the detected object regions extracted from the foreground pixel map with pre-labeled (manually classified) template object silhouettes stored in a database. The whole process of object classification method consists of two steps:</p><p>• Offline step: A template database of sample object silhouettes is created by manually labeling object types. • Online step: The silhouette of each detected object in each frame is extracted and its type is recognized by comparing its silhouette based feature with the ones in the template database in real time during surveillance. After the comparison of the object with the ones in the database, a template shape with minimum distance is found. The type of this object is assigned to the type of the object which we wanted to classify.</p><p>The template silhouette database is created offline by extracting several object contours from different scenes. Since the classification scheme makes use of object similarity, the shapes of the objects in the database should be representative poses of different object types. Figure <ref type="figure">3</ref> shows the template database we use for object classification. It consists of 24 different poses: 14 for human, 5 for human group and 5 for vehicles.</p><p>In classification step, our method does not use silhouettes in raw format, but rather compares converted silhouette distance signals. Hence, in the template database we store only the distance signal of the silhouette and the corresponding type information for both computational and storage efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3. Sample object silhouette template database</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Classification Metric</head><p>Our object classification metric is based on the similarity of object shapes. There are numerous methods in the literature for comparing shapes <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b12">13]</ref>. The reader is especially referred to the surveys presented in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b15">16]</ref> for good discussions on different techniques.</p><p>Our classification metric compares the similarity between the shapes of two objects, A and B, by finding the distance between their corresponding distance signals, Distance between two objects can be computed using more sophisticated methods such as dynamic programming providing a nonlinear warping of the horizontal axis <ref type="bibr" target="#b2">[3]</ref> instead of the linear warping used in the calculation Dist AB . However, a straightforward implementation of dynamic programming increases computational complexity and may not be suitable for the purposes of a real-time system.</p><p>In order to reduce noise in object classification a maximum likelihood scheme is adopted. The assigned object types are counted for a window of k (= 5) frames and the maximum one is assigned as the type. This reduces false classifications due to errors in segmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Recognizing Human Actions</head><p>After detecting the type of an object, if it is a human, its actions can be recognized. The action recognition system can recognize six different human actions which are: walking, boxing and kicking. Figure <ref type="figure" target="#fig_4">6</ref> shows video frames from sample sequences for these action types. The whole process of human action recognition method consists of two steps:</p><p>• Offline step: A pose template database by using human silhouettes for different poses is created. The silhouettes in this database are used to create a pose histogram which is used as an action template. An action template database is created by using these histograms calculated from sample action sequences.</p><p>• Online step: The silhouette of each detected human in each frame is extracted and its pose is matched with one in the pose template database. Then a histogram of the matched poses is created at each frame by using a history window of the matched human poses. Then the calculated histogram is matched against the ones in the template action database, and the label of the action with minimum distance is assigned as the current action label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Creating Silhouette-Based Pose Template Database</head><p>A typical human action such as walking involves repetitive motion. Although throughout a video sequence several hundreds of silhouettes can be extracted for a subject, the shapes of the silhouettes will exhibit an almost periodic similarity. Hence, the basic set of shapes for a full period can represent an action. Furthermore, the key poses in the basic motion set show differences from action to action. For instance, the silhouettes of a walking person from side view can be represented with three key poses corresponding to the cases of full stretched legs, closed legs and partially stretched legs. Similarly, the boxing action again can be represented with two key poses: (i) one arm is stretched and (ii) both arms are near the chest. Some of the possible poses that can be seen during walking action are shown in Figure <ref type="figure">5</ref> with an ID number beneath.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 5. Sample silhouettes from a walking sequence</head><p>The template pose database is manually created with extracted object silhouettes as shown in Figure <ref type="figure">5</ref> and contains key poses for all of the actions that can be recognized by the system. The pose silhouettes are labeled with integer IDs in the range [1, ID MAX ]. The template database which we used in our tests contains 82 poses for different actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Creating Action Template Database</head><p>After creating the pose database the next step is to create action templates. Actions can be represented with a histogram of key poses (pose IDs) it matches. In other words, if we create a histogram of the size of the total number of key silhouettes in the silhouette template database, and match generated silhouettes at each frame of the Then for each S i a corresponding pose match P i is found in the silhouette pose template database by using the distance metric explained in Section 3.1. Let L = {P 1 , P 2 , …, P N } represent the list of matched poses, where P i Є [1, ID MAX ]. Then the list L can be used to create a histogram H (with ID MAX bins) of IDs. After the histogram is created, it is normalized to have unit area and made ready to represent an action template like a signature. A histogram H j is created in this manner for each action j, j Є {Walking, Boxing, Kicking}, and these histograms form the action template database. Figure <ref type="figure" target="#fig_5">7</ref> shows sample action histograms for each action. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Recognizing Actions in Real-Time</head><p>After creating action template database with histograms for distinct actions, test actions are recognized in real-time.</p><p>In order to recognize an action, we keep a circular list of the IDs of the matching silhouettes in the template pose database for the subject's silhouette. In next step, the distance between H T and each action template histogram H j in the action database is calculated. The distance metric in this calculation is Euclidian distance and defined similar to the Dist AB as explained in Section 4.1. The action type label of the action histogram Hj, which has the minimum distance with H T is assigned as the label of the current test action A T . Figure <ref type="figure" target="#fig_7">8</ref> shows a sample history of poses for a window size of w = 4, in the actual implementation we use w = 25. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Results</head><p>All of the tests are performed by using a video player and analyzer application that we implemented for developing our computer vision algorithms, on Microsoft Windows XP Professional operating system on a computer with an Intel PIV-2600 MHz CPU and 512 MB of RAM.</p><p>In order to test the object classification algorithm we first created a sample object template database by using an application to extract and label object silhouettes. We used four sample video clips that contain human, human group and vehicle samples. We used the template object database to classify objects in several movie clips containing human, human group and vehicle. We prepared a confusion matrix to measure the performance of our object classification algorithm. The confusion matrix is shown in Table <ref type="table" target="#tab_0">1</ref>. The confusion matrix is for the following object types: Human, Human Group and Vehicle.</p><p>We performed our action recognition experiments with three human subjects. One subject is used to create the template pose and template action databases and the other subjects are used in recognition tests. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>In this paper, we proposed a novel system for real-time object classification and human action recognition using object silhouettes. The test results show that the presented method is promising and can be improved with some further work to reduce false alarms. The proposed methods can also be utilized as part of a multimedia database to extract useful facts from video clips <ref type="bibr" target="#b18">[19]</ref>.</p><p>A weakness of the proposed methods is that they are view dependent. If the camera setup is different in training and testing, the success rate will be too low. Automating the template database creation steps will help to obtain a self calibrating object classification and human action recognition system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>shows sample detected foreground object regions and the extracted silhouettes. Another feature extracted from the object is the silhouette distance signal. Let S = {p 1 , p 2 ,… , p n } be the silhouette of an object O consisting of n points ordered from top center point of the detected region in clockwise direction and C m be the center of mass point of O. The distance signal DS = {d 1 , d 2 ,… , d n } is generated by calculating the distance between C m and each p i starting from 1 through n as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Sample distance signal calculation and normal and scaled distance signals</figDesc><graphic coords="6,188.94,183.12,197.16,120.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4</head><label>4</label><figDesc>Figure 4 shows the silhouettes, silhouette signals and signal distances of a sample query object and template database objects for type classification.Distance between two objects can be computed using more sophisticated methods such as dynamic programming providing a nonlinear warping of the horizontal axis<ref type="bibr" target="#b2">[3]</ref> instead of the linear warping used in the calculation Dist AB . However, a straightforward implementation of dynamic programming increases computational complexity and may not be suitable for the purposes of a real-time system.In order to reduce noise in object classification a maximum likelihood scheme is adopted. The assigned object types are counted for a window of k (= 5) frames and the maximum one is assigned as the type. This reduces false classifications due to errors in segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Sample query object and its distances (D) to several objects in the silhouette template database. Object types are Human (H), Human Group (HG) and Vehicle (V). The matching object is shown with the bounding rectangle.</figDesc><graphic coords="8,42.30,254.88,345.60,130.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Sample video frames for different action types</figDesc><graphic coords="10,90.78,62.40,248.52,165.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Un-normalized histograms for actions in the template database</figDesc><graphic coords="10,42.18,431.64,345.72,70.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Let A T = {S i-(w-1) , S i-(w-2) , …, S i } be the fixed length list of the silhouettes of a test subject in the last w frames of video. For each S i , a corresponding pose template match P i is found in the silhouette pose template database by using the same distance metric used in training. Let L T = {P 1 , P 2 , …, P N } represent the list of matched pose IDs, where P i Є [1, ID MAX ]. After this step, like in the training phase, a normalized histogram H T of IDs is created by using the IDs in L T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Sample history window for a test sequence</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Confusion matrix for object classificationWe created action templates for the following actions: Walking, Boxing and Kicking. Below, the confusion matrix for the cumulative action recognition results is shown:</figDesc><table><row><cell></cell><cell></cell><cell>Human</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Human</cell><cell>Group</cell><cell>Vehicle</cell><cell>Success</cell></row><row><cell>Human</cell><cell>175</cell><cell>13</cell><cell>20</cell><cell>84.13%</cell></row><row><cell>Human Group</cell><cell>12</cell><cell>52</cell><cell>14</cell><cell>66.67%</cell></row><row><cell>Vehicle</cell><cell>38</cell><cell>22</cell><cell>238</cell><cell>79.86%</cell></row><row><cell>Average Success Rate</cell><cell></cell><cell></cell><cell></cell><cell>76.88%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Confusion matrix for action recognition</figDesc><table><row><cell></cell><cell cols="3">Walking Boxing Kicking</cell><cell>Success</cell></row><row><cell>Walking</cell><cell>12</cell><cell>1</cell><cell>1</cell><cell>85.71%</cell></row><row><cell>Boxing</cell><cell>0</cell><cell>4</cell><cell>0</cell><cell>100.00%</cell></row><row><cell>Kicking</cell><cell>0</cell><cell>1</cell><cell>3</cell><cell>75.00%</cell></row><row><cell cols="2">Average Success Rate</cell><cell></cell><cell></cell><cell>86.94%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported in part by European Commission Sixth Framework Program with Grant No: 507752 (MUSCLE Network of Excellence Project).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human motion analysis: a review</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="428" to="440" />
			<date type="published" when="1999-03">March 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An efficiently computable metric for comparing polygonal shapes</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Arkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Chew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kedem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S B</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Recognition and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="209" to="216" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Report on progress with respect to partial solutions on human detection algorithms, human activity analysis methods, and multimedia databases</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Enis</forename><surname>Cetin</surname></persName>
		</author>
		<ptr target="www.muscle-noe.org" />
	</analytic>
	<monogr>
		<title level="m">FP6-NoE: MUSCLE (Multimedia Understanding Through Computation and Semantics)</title>
		<imprint>
			<date type="published" when="2005-05">May 2005</date>
		</imprint>
		<respStmt>
			<orgName>EU</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">WP-11 Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognizing motion using local appearance</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chomat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Intelligent Robotic Systems, University of Edinburgh</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Silhouette-based human identification from body shape and gait</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Fifth IEEE Conf. on Automatic Face and Gesture Recognition</title>
		<meeting>of Fifth IEEE Conf. on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="366" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust real-time periodic motion detection, analysis and applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="781" to="796" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A system for video surveillance and monitoring: VSAM final report</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
		<idno>CMU-RI-TR-00-12</idno>
		<imprint>
			<date type="published" when="2000-05">May 2000</date>
		</imprint>
		<respStmt>
			<orgName>Robotics Institute, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Moving object detection, tracking and classification for smart video surveillance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dedeoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Ankara</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. of Computer Eng. Bilkent University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s Thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The analysis of human motion and its application for visual surveillance</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd IEEE International Workshop on Visual Surveillance</title>
		<meeting>of the 2nd IEEE International Workshop on Visual Surveillance<address><addrLine>Fort Collins, U.S.A.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="3" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">W4: A real time system for detecting and tracking people</title>
		<author>
			<persName><forename type="first">I</forename><surname>Haritaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="962" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Image based measurement systems: object recognition and parameter estimation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Heijden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996-01">January 1996</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A real-time system for monitoring of cyclists and pedestrians</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heikkila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Silven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Second IEEE Workshop on Visual Surveillance</title>
		<meeting>of Second IEEE Workshop on Visual Surveillance<address><addrLine>Fort Collins, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-06">June 1999</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Shape-based detection of humans for video surveillance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ramoser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schlgl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Int. Conf. on Image Processing</title>
		<meeting>of IEEE Int. Conf. on Image essing<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1013" to="1016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Local application of optic flow to analyse rigid versus non-rigid motion</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Lipton</surname></persName>
		</author>
		<idno>CMU-RI-TR-99-13</idno>
		<imprint>
			<date type="published" when="1999-12">December 1999</date>
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Robotics Institute, Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Moving target classification and tracking from real-time video</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fujiyoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Workshop Applications of Computer Vision</title>
		<meeting>of Workshop Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A survey of shape analysis techniques</title>
		<author>
			<persName><forename type="first">S</forename><surname>Loncaric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="983" to="1001" />
			<date type="published" when="1998-08">August 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Background subtraction techniques</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mcivor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Image and Vision Computing</title>
		<meeting>of Image and Vision Computing<address><addrLine>Auckland, New Zealand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A histogram-based approach for object-based query-by-shape-and-color in multimedia databases</title>
		<author>
			<persName><forename type="first">E</forename><surname>Saykol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Gudukbay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ulusoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1170" to="1180" />
			<date type="published" when="2005-11">November 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Database Model for Querying Visual Surveillance by Integrating Semantic and Low-Level Features</title>
		<author>
			<persName><forename type="first">E</forename><surname>Saykol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Gudukbay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ulusoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (LNCS), (Proc. of 11th International Workshop on Multimedia Information Systems-MIS&apos;05)</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Candan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Celentano</surname></persName>
		</editor>
		<meeting><address><addrLine>Sorrento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">September 2005</date>
			<biblScope unit="volume">3665</biblScope>
			<biblScope unit="page" from="163" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">KiMPA: A kinematics-based method for polygon approximation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Saykol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gulesir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Gudukbay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ulusoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (LNCS)</title>
		<editor>
			<persName><forename type="first">Tatyana</forename><surname>Yakhno</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="volume">2457</biblScope>
			<biblScope unit="page" from="186" to="194" />
		</imprint>
	</monogr>
	<note>Advances in Information Sciences</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recognizing human actions: a local SVM approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICPR&apos;04</title>
		<meeting>of ICPR&apos;04<address><addrLine>Cambridge, UK.</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive background mixture models for realtime tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Grimson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="246" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Moving object detection in wavelet compressed video</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">U</forename><surname>Toreyin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Cetin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aksay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Akhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="255" to="265" />
			<date type="published" when="2005">2005</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
	<note>EURASIP</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">HMM based falling person detection using both audio and video</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">U</forename><surname>Toreyin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dedeoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Cetin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Workshop on Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="volume">3766</biblScope>
			<biblScope unit="page" from="211" to="220" />
			<date type="published" when="2005-10-21">Oct. 21, 2005. 2005</date>
			<publisher>Springer-Verlag GmbH</publisher>
			<pubPlace>Beijing, China</pubPlace>
		</imprint>
	</monogr>
	<note>in conjunction with ICCV 2005</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">State-of-the-art in shape matching</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagedoorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Principles of Visual Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="87" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recent developments in human motion analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="585" to="601" />
			<date type="published" when="2003-03">March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Classifying moving objects as rigid or non-rigid</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wixson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Selinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of DARPA Image Understanding Workshop</title>
		<meeting>of DARPA Image Understanding Workshop</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="341" to="358" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
