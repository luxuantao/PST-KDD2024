<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-20">20 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yudong</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Dilusense Technology Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Keyu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Dilusense Technology Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sen</forename><surname>Liang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yongjin</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hujun</forename><surname>Bao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
							<email>juyong@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-20">20 Mar 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2103.11078v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generating high-fidelity talking head video by fitting with the input audio sequence is a challenging problem that receives considerable attentions recently. In this paper, we address this problem with the aid of neural scene representation networks. Our method is completely different from existing methods that rely on intermediate representations like 2D landmarks or 3D face models to bridge the gap between audio input and video output. Specifically, the feature of input audio signal is directly fed into a conditional implicit function to generate a dynamic neural radiance field, from which a high-fidelity talking-head video corresponding to the audio signal is synthesized using volume rendering. Another advantage of our framework is that not only the head (with hair) region is synthesized as previous methods did, but also the upper body is generated via two individual neural radiance fields. Experimental results demonstrate that our novel framework can (1) produce high-fidelity and natural results, and (2) support free adjustment of audio signals, viewing directions, and background images. 1 * This work was done when Yudong Guo and Keyu Chen were intern at Dilusense.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Synthesizing high-fidelity audio-driven facial video sequences is an important and challenging problem in many applications like digital humans, chatting robots, and virtual video conferences. Regarding the talking-head generation process as a cross-modal mapping from audio to visual faces, the synthesized facial images are expected to perform natural speaking styles while synchronizing photo-realistic streaming results as same as the original videos.</p><p>Currently, a wide range of approaches have been proposed for this task. Earlier methods built upon professional artist modelling <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b45">46]</ref> or complicated motion capture system <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b42">43]</ref> are limited in high-end areas of the movie and game industry. Recently, many deep-learning-based tech-niques <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b35">36]</ref> are proposed to learn the audio-toface translation by generative adversarial networks (GANs). However, resolving such a problem is highly challenging because it is non-trivial to faithfully relate the audio signals and face deformations, including expressions and lip motions. Therefore, most of these methods utilize some intermediate face representations including reconstructing explicit 3D face shapes <ref type="bibr" target="#b43">[44]</ref> and regressing expression coefficients <ref type="bibr" target="#b35">[36]</ref> or 2D landmarks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b37">38]</ref>. Due to the information loss caused by the intermediate representation, it might lead to semantic mismatches between original audio signals and the learned face deformations. Moreover, existing methods suffer from several limitations, such as only rendering the mouth part <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36]</ref> or fixed by static head pose <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b6">7]</ref>, thus are not suitable for advanced talking head editing tasks, like pose-manipulation and background-replacement.</p><p>To address these issues of existing talking head methods, we turn attention to recent developed neural radiance fields (NeRF). We present AD-NeRF, an audio-driven neural radiance fields model that can handle the cross-modal mapping problem without introducing extra intermediate representation. Different from existing methods which rely on 3D face shape, expression coefficient or 2D landmarks to encode the facial image, we adopt the neural radiance field (NeRF) <ref type="bibr" target="#b22">[23]</ref> to represent the scenes of talking heads. Inspired by dynamic NeRF <ref type="bibr" target="#b14">[15]</ref> for modeling appearance and dynamics of a human face, we directly map the corresponding audio features to dynamic neural radiance fields to represent the target dynamic subject. Thanks to the neural rendering techniques which enable a powerful ray dispatching strategy, our model can well represent some finescale facial components like teeth and hair, and achieves better image qualities than existing GAN-based methods. Moreover, the volumetric representation provides a natural way to freely adjust the global deformation of the animated speakers, which can not be achieved by traditional 2D image generation methods. Furthermore, our method takes the head pose and upper body movement into consideration and is capable of producing vivid talking-head results for realworld applications. Specifically, our method takes a short video sequence, including the video and audio track of a target speaking person as input. Given the audio features extracted via Deep-Speech <ref type="bibr" target="#b0">[1]</ref> model and the face parsing maps, we aim to construct an audio-conditional implicit function that stores the neural radiance fields for talking head scene representations. As the movement of the head part is not consistent with that of the upper body part, we further split the neural radiance field representation into two components, one for the foreground face and the other for the foreground torso. In this way, we can generate natural talking-head sequences from collected training data. Please refer to the supplementary video for better visualization of our results.</p><p>In summary, the contributions of our proposed talkinghead synthesis method contain three main aspects:</p><p>• We present a brand-new audio-driven talking head method that directly maps the audio features to dynamic neural radiance fields for human portraits rendering, without any other intermediate modalities that may cause information loss. Ablation studies show that such direct mapping has better capability in producing accurate lip motion results.</p><p>• We decompose the neural radiance fields of human portrait scenes into two branches to model the head and torso deformation respectively, which helps to generate more natural talking head results.</p><p>• With the help of audio-driven NeRF representation, our method enables talking head video editing applications like pose-manipulation and backgroundreplacement, which would be valuable for potential virtual reality applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly review the talking-head generation approaches from different categories and the neural rendering techniques applied in related fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Audio-driven Facial Animation</head><p>The goal of audio-driven facial animation is to reenact a specific person in sync with arbitrary input speech sequences. Based on the applied targets and techniques, it can be categorized into two classes: model-based and datadriven methods. The model-based approaches <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b45">46]</ref> require expertise works to establish the relationships between audio semantics and lip motions, such as phonemeviseme mapping <ref type="bibr" target="#b13">[14]</ref>. Therefore, they are inconvenient for general applications except for advanced digital creations like movie and game avatars. With the rise of deep learning techniques, many data-driven methods are proposed to generate photo-realistic talking-head results. Earlier methods try to synthesize the lip motions that fulfill the train-ing data of a still facial image <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">37]</ref>. Later it is improved to generate full image frames for President Obama by using quantities of his addressing videos <ref type="bibr" target="#b33">[34]</ref>. Based on the developed 3D face reconstruction <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b10">11]</ref> and generative adversarial networks, more and more approaches are proposed by intermediately estimating 3D face shapes <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44]</ref> or facial landmarks <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b37">38]</ref>. In contrast to our method, they are restrained by the representation capability of latent modalities, i.e., prior parametric models or low-dimensional landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Implicit Neural Scene Networks</head><p>Neural scene representation is the use of neural networks for representing the shape and appearance of scenes. The neural scene representation networks (SRNs) was first introduced in <ref type="bibr" target="#b32">[33]</ref>, in which the geometry and appearance of an object is represented as a neural network that can be sampled at points in space. Since from last year, Neural Radiance Fields (NeRF) <ref type="bibr" target="#b22">[23]</ref> has gained a lot of attention for neural rendering and neural reconstruction tasks. The underlying implicit representation of the shape and appearance of 3D objects can be transformed into volumetric ray sampling results. Follow-up works extend this idea by using in-the-wild training data including appearance interpolation <ref type="bibr" target="#b21">[22]</ref>, introducing deformable neural radiance fields to represent non-rigidly moving objects <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29]</ref>, and optimizing NeRF without pre-computed camera parameters <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Neural Rendering for Human</head><p>Neural radiance fields for rendering human heads and bodies have also attracted many attentions. <ref type="bibr" target="#b39">[40]</ref> presents a compositional 3D scene representation for learning highquality dynamic neural radiance fields for upper body. <ref type="bibr" target="#b29">[30]</ref> adopts pixel-aligned features <ref type="bibr" target="#b30">[31]</ref> in NeRF representation to generalize to unseen identities at test time. <ref type="bibr" target="#b15">[16]</ref> presents a meta-learning framework for estimating neural radiance fields form a single headshot portrait. <ref type="bibr" target="#b14">[15]</ref> proposes dynamic neural radiance fields for modeling the appearance and dynamics of a human face. <ref type="bibr" target="#b25">[26]</ref> integrates observations across video frames to enable novel view synthesis for human body from a sparse multi-view video.</p><p>Despite the advances in human-centered neural rendering techniques, they all focus on the view synthesis of dynamic face and body. Our method is to consider a crossmodel translation problem by integrating the audio features and implicit function. To the best of our knowledge, our method is the first approach by solving the talking-head generation problem via Neural Radiance Fields. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we will first present the overall pipeline of our talking-head synthesis framework. Then we will introduce each component of our method. Finally, we will summarize the algorithm design with more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Our talking-head synthesis framework (Fig. <ref type="figure" target="#fig_0">1</ref>) is trained on a short video sequence along with the audio track of a target person. Based on the neural rendering idea, we implicitly model the deformed human heads and upper bodies by neural scene representation, i.e., neural radiance fields. In order to bridge the domain gap between audio signals and visual faces, we extract the semantic audio features and learn a conditional implicit function to map the audio features to neural radiance fields (Sec. 3.2). Finally, visual faces are rendered from the neural radiance fields using volumetric rendering (Sec. 3.3). In the inference stage, we can generate faithful visual features simply from the audio input. Besides, our method can also generate realistic speaking styles of the target person. It is achieved by estimating the neural radiance fields of dynamic heads and upper bodies in a separate manner (Sec. 3.4). While we transform the volumetric features into a novel canonical space, the heads and other body parts will be rendered differently with their individual implicit models and thus produce very natural results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Neural Radiance Fields for Talking Heads</head><p>Based on the standard neural radiance field scene representation <ref type="bibr" target="#b22">[23]</ref>, we present a conditional radiance field of a talking head using a conditional implicit function with an additional audio code as input. Apart from viewing direction d and 3D location x, the semantic feature of audio a will be added as another input of the implicit function F θ . In practice, F θ is realized by a multi-layer percep-tron (MLP). With all concatenated input vectors (a, d, x), the network will estimate color values c accompanied with densities σ along the dispatched rays. The entire implicit function can be formulated as follows:</p><formula xml:id="formula_0">F θ : (a, d, x) −→ (c, σ).<label>(1)</label></formula><p>Semantic Audio Feature. In order to extract the semantically meaningful information from acoustic signals, we employ the popular DeepSpeech <ref type="bibr" target="#b0">[1]</ref> model to predict a 29dimensional feature code for each 20ms audio clip. In our implementation, several continuous frames of audio features are jointly sent into a temporal convolutional network to eliminate noisy signals from raw input. Specifically, we use the features a ∈ R 16×29 from the sixteen neighboring frames to represent the current state of audio modality. The usage of audio features instead of regressed expression coefficients <ref type="bibr" target="#b35">[36]</ref> or facial landmarks <ref type="bibr" target="#b38">[39]</ref> is beneficial for alleviating the training cost of intermediate translation network and preventing potential semantic mismatching issue between audio and visual signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Volume Rendering with Radiance Fields</head><p>With the color c and density σ predicted by implicit model F θ mentioned above, we can employ the volume rendering process by accumulating the sampled density and RGB values along the rays casted through each pixel to compute the output color for image rendering results. Like <ref type="bibr" target="#b22">[23]</ref>, the expected color C of a camera ray r(t) = o + td with camera center o, viewing direction d and near bound t n and far bound t f is evaluated as:</p><formula xml:id="formula_1">C(r; θ, Π, a) = t f tn σ θ (r(t)) • c θ (r(t), d) • T (t)dt, (2)</formula><p>where c θ (•) and σ θ (•) are the outputs of the implicit function F θ described above. T (t) is the accumulated transmit- tance along the ray from t n to t:</p><formula xml:id="formula_2">T (t) = exp − t tn σ(r(s))ds .<label>(3)</label></formula><p>In Eq. ( <ref type="formula">2</ref>), Π is the estimated rigid pose parameters of the face, represented by a rotation matrix R ∈ R 3×3 and a translation vector t ∈ R 3×1 , i.e., Π = {R, t}. Note that during the training stage, we only use the head pose information instead of any 3D face shapes for our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Individual NeRFs Representation</head><p>The reason of taking head pose into account for rendering process is that, compared to static background, the human body parts (including head and torso) are dynamically moving from frame to frame. Therefore, it is essential to transform the deformed points from camera space to canonical space for radiance fields training. The previous method <ref type="bibr" target="#b14">[15]</ref> tries to handle the dynamic movements by decoupling the foreground and background based on the automatic predicted density, i.e., for dispatched rays passing through the foreground pixels, the human parts will be predicted with high densities while the background images will be ignored with low densities. However, there exist some ambiguities to transform the torso region into canonical space. Since the movement of the head part is not consistent with the movement of the torso part and the pose parameters Π are estimated for the face shape only, applying the same rigid transformation to both the head and torso region together would result unsatisfactory rendering results in upper body. To tackle this issue, we model these two parts with two individual neural radiance fields: one for the head part and the other for the torso part.</p><p>As illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>, we initially leverage an automatic face parsing method <ref type="bibr" target="#b20">[21]</ref> to divide the training image into three parts: static background, head and torso. We first train the implicit function for head part F head θ . During this step, we regard the head region determined by parsing map as the foreground and the rest to be background. The head pose Π is applied to the sampled points along the ray casted through each pixel. The last sample on the ray is assumed to lie on the background with a fixed color, namely, the color of the pixel corresponding to the ray, from the background image. Then we convert the rendering image of F head θ to be the new background and make the torso part to be the foreground. Next we continue to train the second implicit model F torso θ . In this stage, there are no available pose parameters for the torso region. So we assume all points live in canonical space (i.e., without transforming them with head pose Π) and add the face pose Π to be another input condition (combined with point location x, viewing direction d and audio feature a) for radiance fields prediction. In other words, we implicitly treat the head pose Π as an additional input, instead of using Π for explicit transformation within F torso θ . In the inference stage, both the head part model F head θ and the torso part model F torso θ accept the same input parameters, including the audio conditional code a and the pose coefficients Π. The volume rendering process will first go through the head model accumulating the sampled density and RGB values for all pixels. The rendered image is expected to cover the foreground head area on a static background. Then the torso model will fill the missing body part by predicting foreground pixels in the torso region. In general, such individual neural radiance field representation design is helpful to model the inconsistent head and upper body movements and to produce natural talking head results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Editing of Talking Head Video</head><p>Since both neural radiance fields take semantic audio feature and pose coefficients as input to control the speaking content and the movement of talking head, our method could enable audio-driven and pose-manipulated talking head video generation by replacing the audio input and adjusting pose coefficients, respectively. Moreover, since we use the corresponding pixel on the background image as the last sample for each ray, the implicit networks learn to predict low density values for the foreground samples if the ray is passing through a background pixel, and high density values for foreground pixels. In this way, our method decouples foreground-background regions and enables background editing simply by replacing the background image. We further demonstrate all these editing applications in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Training Details</head><p>Dataset. For each target person, we collect a short video sequence with audio track for training. The average video length is 3-5 minutes and all in 25 fps. The recording camera and background are both assumed to be static. In testing, our method allows arbitrary audio input such as speech from different identities, gender and language. Training Data Preprocessing. There are three main steps to preprocess the training dataset: (1) We adopt an automatic parsing method <ref type="bibr" target="#b20">[21]</ref> to label the different semantic regions for each frame; (2) We apply the multi-frame optical flow estimation method <ref type="bibr" target="#b16">[17]</ref> to get dense correspondences across video frames in near-rigid regions like forehead, ear and hair, and then estimate pose parameters using bundle adjustment <ref type="bibr" target="#b1">[2]</ref>. It is worth noting that the estimated poses are only effective for the face part but not the other body regions like neck and shoulders, i.e., the face poses could not represent the entire movements of upper body;</p><p>(3) We construct a clean background image without person (as shown in Fig. <ref type="figure" target="#fig_1">2</ref>) according to all sequential frames. This is achieved by removing the human region from each frame based on the parsing results and then computing the aggregation results of all the background images. For the missing area, we use Poisson Blending <ref type="bibr" target="#b26">[27]</ref> to fix the pixels with neighbor information. Network &amp; Loss Function. In general, our proposed neural radiance field representation network has two main constraints. The first one is the temporal smooth filter. In Sec. 3.2, we mentioned to process the DeepSpeech feature with a window size of 16. The 16 continuous audio features will be sent into a 2D convolutional network to regress the per-frame latent code. In order to assure the stability within audio signals, we adopt the self-attention idea <ref type="bibr" target="#b35">[36]</ref> to train a temporal filter on the continuous audio code. The filter is implemented by 1D convolution layers with softmax activation. Hence the final audio condition a is given by the temporally filtered latent code.</p><p>Second, we constrain the rendering image of our method to be the same as the training groundtruth. Let I r ∈ R W ×H×3 be the rendered image and I g ∈ R W ×H×3 to be the groundtruth, the optmization target is to reduce the photo-metric reconstruction error between I r and I g . Specifically, the loss function is formulated as:</p><formula xml:id="formula_3">L photo (θ) = W w=0 H h=0 I r (w, h) − I g (w, h) 2 , I r (w, h) = C(r w,h ; θ, Π, a) (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>In this section, we will give our detailed experiment settings in Sec. 4.1. In order to validate the efficiency of our algorithm design, we show the ablation study results in Sec. 4.2. In Sec. 4.3, we conduct comparisons with state-ofthe-art talking head generation methods, including qualitative and quantitative evaluations. Finally, we provide more results of our approach. For better visual experience, we strongly suggest to watch the supplementary videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Detail</head><p>We implement our framework in PyTorch <ref type="bibr" target="#b24">[25]</ref>. Both networks are trained with Adam <ref type="bibr" target="#b19">[20]</ref> solver with initial learning rate 0.0005. We train each model for 400k iterations. In each iteration, we randomly sample a batch of 2048 rays through the image pixels. We train the networks with RTX 3090 and train each model for 400k iterations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>We validate two main components adopted in our framework. First, we compare the neural rendering results based on direct audio condition and additional intermediate condition. Second, we explore the beneficial of training separated neural radiance fields for head and torso region. Audio condition. As aforementioned in Sec. 3.2, our NeRF based talking head model is directly conditioned on audio features to avoid the training cost and information loss within additional intermediate modalities. In Fig. <ref type="figure" target="#fig_2">3</ref>, we compare the rendering images generated from audio code and audio-estimated expression code. We use the same network structure as <ref type="bibr" target="#b35">[36]</ref> to estimate expression code from audio. From the illustration results, it can be clearly observed that the audio conditioning is helpful for precise lip synchronization.</p><p>Individual training for head and torso region. Another factor we would evaluate is the individual training strategy for head and torso part. To demonstrate the advantages of training two separate neural radiance fields network for these two regions, we conduct an ablative experiment by training a single one NeRF network for the human body movements. In such case, the torso area including neck and shoulders are transformed by the estimated head pose matrices. Therefore there are obviously inaccurate mismatching pixels around the boundary of upper body. We visualize the photo-metric error map of this region for the rendering image and groundtruth. From Fig. <ref type="figure" target="#fig_3">4</ref>, the illustrated results prove that our individual training strategy is beneficial for better image reconstruction quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation</head><p>In this section, we compare our method with two categories of talking head synthesis approaches: pure imagebased <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">37]</ref> and intermediate model-based <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36]</ref> ones. We conduct both quantitative and qualitative experiments to evaluate the visualized results generated by each method. In the following, we first summarize the compared methods from two categories and then introduce our designed evaluation metrics. Comparison with Image-based Method. There are a branch of talking head generation methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">37]</ref> entirely lying in the image domain. Recent deeplearning-based approaches are trained for multiple identities and thus can be applied for new target person. However, the limitation of these methods is obvious as they are only capable of producing still face crop images, and differs from our method that produces full-size images with backgrounds and natural taking styles of target person. In Fig. <ref type="figure" target="#fig_5">6</ref>, we present the audio-driven facial animation results generated by our method and three competitive methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">37]</ref>. It can be clearly observed that the image-based talking head methods are restricted by the input image size and thus could not producing high-resolution imagery as we do.</p><p>Comparison with Model-based Method. The modelbased method refers to the approach that takes prior information in generating photo-realistic face images. The key component of this categorical methods is the statistical model, e.g., PCA model for mouth textures <ref type="bibr" target="#b33">[34]</ref> or 3D morphable model for face shapes <ref type="bibr" target="#b35">[36]</ref>.</p><p>In comparison, we extract the audio from the released demos of the two methods as the input of our framework (we assume the released demos as their best results since both of them did not provide pre-trained model), named as testset A (from Neural Voice Puppertry <ref type="bibr" target="#b35">[36]</ref>) and testset B (from SynthesizingObama <ref type="bibr" target="#b33">[34]</ref>). In Fig. <ref type="figure" target="#fig_4">5</ref>, we show some selected audio-driven talking head frames from each method. Note that the prior model generally requires large quantities of training data, for example, <ref type="bibr" target="#b33">[34]</ref> reported to use 14 hours high-quality Obama Addressing videos for training and <ref type="bibr" target="#b35">[36]</ref> took more than 3 hours data for training and 2-3 minutes long video for fine-tuning, while our method only requires a short video clip (3-5 minutes) for training. Despite the huge gap of the training dataset size, our approach is still capable of producing comparable natural results to the other two methods.</p><p>Moreover, our method owns the advantage of freely manipulating the viewing directions on the target person, which means that we can naturally rotate the "virtual camera" to observe the speaking actors from arbitrary novel angles. We further demonstrate the free-viewing-direction results in Fig. <ref type="figure">8</ref> and our supplementary video.</p><p>Metrics. We employ multiple evaluation metrics to demonstrate the superiority of our method to the others. As an audio-driven talking head generation work, the synchronized visual faces are expected to be consistent with audio input while remaining high image fidelity and realistics. To this end, we propose a combined evaluation design, including SyncNet <ref type="bibr" target="#b8">[9]</ref> scores for audio-visual synchronization quality, Action Units (AU) detection <ref type="bibr" target="#b2">[3]</ref> (by OpenFace <ref type="bibr" target="#b3">[4]</ref>) for facial action coding consistency between source and generated results, and a diversified user study on image realistics, fidelity and synchronization consistency.</p><p>SyncNet <ref type="bibr" target="#b8">[9]</ref> is commonly used to validate the audiovisual consistency for lip synchronization and facial anima- We compare with <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b33">[34]</ref> on the demo sequences. Our method not only remains the semantics of lip motion, but also supports free adjustment on viewing angles. Please watch our supplementary video for visual results.  <ref type="table">1</ref>. We conduct comparisons on two testsets (A and B) collected from the demos of Neural Voice Puppertry <ref type="bibr" target="#b35">[36]</ref> and Synthesizin-gObama <ref type="bibr" target="#b33">[34]</ref>, respectively. indicates that the confidence value in SyncNet score is better with higher results. means that AU error is better with smaller numbers. Moreover, our method can synthesize full-frame imagery while enables pose manipulation and background replacement thanks to the audio-driven neural radiance fields. tion tasks. In this experiment, we use a pretrained SyncNet model to compute the audio-sync offset and confidence of speech-driven facial sequences generated by each comparing method (Tab. 1). Higher confidence values are better.</p><p>Besides, we employ an action units (AU) detection module by OpenFace <ref type="bibr" target="#b3">[4]</ref> to compute the facial action units intensities for the source video that providing audio signals and the corresponding generated results. This metric is aimed at evaluating the muscle activation consistency between the source faces and driven ones. The ideal talking-heads are expected to perform similar facial movements as the source faces do. We select the lower face and mouth-related AUs as active subjects and compute the mean errors between source and driven faces. The quantitative results are given in Tab. 1.</p><p>Finally, we conduct a user study comparisons with help of multiple attendees. Each participant is asked to rate the talking-head generation results based on three major aspects: audio-visual synchronization quality, image fidelity and image realistics. We collect the rating results within 1 to 10 (the higher the better) and compute the average score that each method gained. The processed statistics are visualized in Fig. <ref type="figure" target="#fig_6">7</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Applications on Talking Head Editing</head><p>As described in Sec. 3.5, our method could enable talking head video editing on audio signal, head movement, and background image. First we show the audio-driven results of the same video with inputs from diverse audio input from different persons in Fig. <ref type="figure">9</ref>. As we can see, our method produces reasonable results with arbitrary audio input from different identities, gender, and language. Then we show the pose-manipulation and background-replacement results of our method in Fig. <ref type="figure">8</ref>. We can see that our method allows freely adjusted viewing directions and various background images replacement for high-fidelity talking portraits synthesis with the trained neural radiance fields. We believe these features would be very exciting for the virtual reality applications like virtual meetings and digital humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a novel method for high-fidelity talking head synthesis based on neural radiance fields. Using volume rendering on two elaborately designed NeRFs, our method is able to directly synthesize human head and upper body from audio signal without relying on intermediate representations. Our trained model allows arbitrary audio input from different identity, gender and language and supports free head pose manipulation, which are highly demanded features in virtual meetings and digital humans.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Framework of our proposed talking-head synthesis method. Given a portrait video sequence of a person, we train two neural radiance fields to synthesize high-fidelity talking head with volume rendering.</figDesc><graphic url="image-11.png" coords="3,334.67,118.72,50.70,74.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Training process of the two neural radiance fields. We reconstruct the head part and upper-body with Head-NeRF (Step 1) and Torso-NeRF (Step 2) respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Ablation study on using direct audio or intermediate facial expression representation to condition the NeRF model. Based on the zoom-in results, it can be observed that direct audio condition has better capability in producing accurate lip motion results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Ablation study on training individual neural radiance field representation for head and torso. Rendering head and torso region with individual NeRFs is helpful to reduce the misalignment error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. Comparison with model-based methods. We compare with<ref type="bibr" target="#b35">[36]</ref> and<ref type="bibr" target="#b33">[34]</ref> on the demo sequences. Our method not only remains the semantics of lip motion, but also supports free adjustment on viewing angles. Please watch our supplementary video for visual results.</figDesc><graphic url="image-69.png" coords="7,235.23,146.54,52.19,52.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Comparison with image-based methods. The most difference between our method and the image-based ones is the limitation of input image size, which decides the image quality of generation results. Please watch our video demo for more results.</figDesc><graphic url="image-75.png" coords="7,78.77,508.26,56.83,56.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Rating scores from participants. Based on the statics on three different terms, our method achieves comparable results with the other two model-based methods. However, our method only requires a very short video sequence for training, while the other two are trained on multiple and large datasets.</figDesc><graphic url="image-80.png" coords="8,50.36,267.41,75.43,75.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .Figure 9 .</head><label>89</label><figDesc>Figure 8. Our method can generate talking head frames with freely adjusted viewing directions and various background images. Each row from left to right: original frames from a video, reconstructed results with audio and pose from the original video, two samples of background-replacement results, two samples of pose-manipulation results.</figDesc><graphic url="image-85.png" coords="8,50.36,350.21,75.43,75.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table</head><label></label><figDesc></figDesc><table><row><cell>Methods</cell><cell cols="4">SyncNet score [9] testset A testset B testset A testset B AU error [4]</cell><cell>Pose</cell><cell cols="2">Full-frame Background</cell></row><row><cell>[Chen et al.] [7]</cell><cell>6.129</cell><cell>4.388</cell><cell>2.588</cell><cell>3.475</cell><cell></cell><cell></cell><cell></cell></row><row><cell>[Wiles et al.] [42]</cell><cell>4.257</cell><cell>3.976</cell><cell>3.134</cell><cell>3.127</cell><cell>static</cell><cell></cell><cell></cell></row><row><cell>[Vougioukas et al.] [37]</cell><cell>5.865</cell><cell>6.712</cell><cell>2.156</cell><cell>2.658</cell><cell></cell><cell></cell><cell></cell></row><row><cell>[Thies et al.] [36]</cell><cell>4.932</cell><cell>-</cell><cell>1.976</cell><cell>-</cell><cell>copied from</cell><cell></cell><cell></cell></row><row><cell>[Suwajanakorn et al.] [34]</cell><cell>-</cell><cell>5.836</cell><cell>-</cell><cell>2.176</cell><cell>source</cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>5.239</cell><cell>5.411</cell><cell>2.133</cell><cell>2.287</cell><cell>freely adjusted</cell><cell></cell><cell></cell></row><row><cell>Original</cell><cell>5.895</cell><cell>6.178</cell><cell>0</cell><cell>0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishita</forename><surname>Sundaram Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingliang</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Andrew</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Kybernetes</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross-dataset learning and person-specific normalisation for automatic action unit detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Openface 2.0: Facial behavior analysis toolkit</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018)</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video rewrite: driving visual speech with audio</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Covell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Slaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIG-GRAPH</title>
				<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Expressive speech-driven facial animation</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><forename type="middle">C</forename><surname>Tien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petros</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical cross-modal talking face generation with dynamic pixel-wise loss</title>
		<author>
			<persName><forename type="first">Lele</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">K</forename><surname>Maddox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyao</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Out of time: automated lip sync in the wild</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Multi-view Lip-reading, ACCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Capture, learning, and synthesis of 3d speaking styles</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cudeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cassidy</forename><surname>Laidlaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunde</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Jali: an animator-centric viseme model for expressive lip synchronization</title>
		<author>
			<persName><forename type="first">Pif</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Landreth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Fiume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Trainable videorealistic speech animation</title>
		<author>
			<persName><forename type="first">Tony</forename><surname>Ezzat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gadi</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Confusions among visually perceived consonants</title>
		<author>
			<persName><forename type="first">Cletus</forename><forename type="middle">G</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of speech and hearing research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic neural radiance fields for monocular 4d facial avatar reconstruction</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Gafni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichang</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chia-Kai</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05903</idno>
		<title level="m">Portrait neural radiance fields from a single image</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A variational approach to video registration with subspace constraints</title>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasios</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="286" to="314" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cnn-based real-time dense face reconstruction with inverserendered photo-realistic face images</title>
		<author>
			<persName><forename type="first">Yudong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boyi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Audio-driven facial animation by joint endto-end learning of pose and emotion</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Herva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Maskgan: Towards diverse and interactive facial image manipulation</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Cheng-Han Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingyun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Alexey Dosovitskiy, and Daniel Duckworth. NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noha</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nerf: Representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Keunhong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utkarsh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sofien</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12948</idno>
		<title level="m">Deformable neural radiance fields</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans</title>
		<author>
			<persName><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Poisson image editing</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2003 Papers</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="313" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Speech-driven 3d facial animation with implicit emotional awareness: A deep learning approach</title>
		<author>
			<persName><forename type="first">Hai</forename><forename type="middle">X</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Pavlovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Albert</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enric</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName><surname>D-Nerf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13961</idno>
		<title level="m">Neural radiance fields for dynamic scenes</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pva: Pixel-aligned volumetric avatars</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lombardi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02697</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization</title>
		<author>
			<persName><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2304" to="2314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Real-time vision and speech driven avatars for multimedia applications</title>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Schreer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Englert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Eisert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Tanger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Scene representation networks: Continuous 3d-structure-aware neural scene representations</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Vincent Sitzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName><surname>Wetzstein</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Synthesizing obama: learning lip sync from audio</title>
		<author>
			<persName><forename type="first">Supasorn</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ira</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A deep learning approach for generalized speech animation</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taehwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Krahe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasio</forename><surname>Garcia Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural voice puppetry: Audio-driven facial reenactment</title>
		<author>
			<persName><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Realistic speech-driven facial animation with gans. International Journal of Computer Vision</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mead: A large-scale audio-visual dataset for emotional talking-face generation</title>
		<author>
			<persName><forename type="first">Kaisiyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianyi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linsen</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoqian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">One-shot free-view neural talking-head synthesis for video conferencing</title>
		<author>
			<persName><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.15126</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning compositional radiance fields of dynamic human heads</title>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timur</forename><surname>Bagautdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lombardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zollhöfer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangzhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Prisacariu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07064</idno>
		<title level="m">NeRF−−: Neural radiance fields without known camera parameters</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">X2face: A network for controlling face generation using images, audio, and pose codes</title>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Wiles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Sophia Koepke</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Performance-driven facial animation</title>
		<author>
			<persName><forename type="first">Lance</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
				<imprint>
			<date type="published" when="2006">2006 Courses. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Audio-driven talking face video generation with natural head pose</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zipeng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Jin</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10137</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Few-shot adversarial learning of realistic neural talking head models</title>
		<author>
			<persName><forename type="first">Egor</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksandra</forename><surname>Shysheya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Egor</forename><surname>Burkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visemenet: Audiodriven animator-centric speech animation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Landreth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
