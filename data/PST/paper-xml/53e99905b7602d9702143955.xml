<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning without Local Minima in Radial Basis Function Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Monica</forename><surname>Bianchini</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, ZEEE</roleName><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, ZEEE</roleName><forename type="first">Marco</forename><surname>Gori</surname></persName>
						</author>
						<title level="a" type="main">Learning without Local Minima in Radial Basis Function Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">170CC30FD4DAA75BCAE4EB2C909A09F2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning from examples plays a central role in artificial neural networks. The success of many learning schemes is not guaranteed, however, since algorithms like backpropagation may get stuck in local minima, thus providing suboptimal solutions. For feedforward networks, the theoretical results reported in [SI, 161, [15], and [20] show that optimal learning can be achieved provided that certain conditions on the network and the learning environment are met. A similar investigation is put forward in this paper for the case of networks using radial basis functions (RBF) [lo], [14]. The analysis proposed in [6]</p><p>is extended naturally under the assumption that the patterns of the learning environment are separable by hyperspheres. In that case, we prove that the attached cost function is local minima free with respect to all the weights. This provides us with some theoretical foundations for a massive application of RBF in pattern recognition.</p><p>'As pointed out in [9], however, the generalization to new examples is better for networks with a hidden layer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>N the last few years there has been renewed interest in I artificial neural networks (ANN'S) which have been used for several different applications. For example, in pattern recognition the remarkable experience gained by numerous experiments suggests that ANN'S are very promising tools. To exploit the potential learning capabilities of these techniques, however, a more thorough knowledge of the learning algorithms seems really necessary. Many of these algorithms essentially deal with the optimization of functions that, in pattern recognition, may be based on several thousand variables. The rich literature on optimization algorithms still has to be exploited in a suitable way to deal effectively with such functions. There is no doubt, however, that the shape of those cost functions deeply affects the success of any candidate algorithm. The presence of stationary points, and particularly of local minima, may severely limit the effectiveness of any neural network learning scheme. Although many suggestions have been given for avoiding local minima (see, e.g., [18],</p><p>[21]), their presence represents a serious problem from the computational point of view. For local minima free cost functions just simple gradient descent algorithms allow us to discover optimal solutions with a limited computational burden. This motivates the research of conditions for guaranteeing the absence of local minima. To some extent, such conditions give us an idea of the difficulties of the problem we are dealing with.</p><p>Manuscript received <ref type="bibr">January 29, 1993;</ref><ref type="bibr">revised August 2, 1993 and April</ref> The authors are with Dipattimento di Sistemi e Informatica, Universith di <ref type="bibr">IEEE Log Number 9409149. 7, 1994</ref>. This research was supported in part by MURST 40%. Firenze, Italy.</p><p>In the case of feedforward networks, analyses on optimal learning have been carried out by numerous researchers in the attempt to find examples of local minima and conditions which imply local minima free error surfaces. A general analysis on that problem can be found in [6] which also ensures that the error surfaces are local minima free in the case of pyramidal networks.</p><p>There have also been attempts to establish straightforward sufficient conditions for guaranteeing the absence of local minima. Roughly speaking, backpropagation (BP) convergence is guaranteed for "many input" and "many hidden" networks. In [6], the local minima analysis is specialized for the case of linearly separable patterns. Linear separability is likely to hold for patterns represented by "many coordinates." This hypothesis does not require constraints on the network architecture and particularly on the number of hidden units. As a consequence, a good generalization can be attained on the test set. The limitation, of course, is with the condition itself, which can be met in some problems of pattern recognition, but certainly not in general. Moreover, the use of hidden units for dealing with linearly separable patterns may seem unnecessary, since a one-layer network suffices for separating the learning set. '  Poston ef al. [15] have shown that the absence of local minima is guaranteed for networks having one hidden layer and as many hidden units as patterns. <ref type="bibr">Yu [20]</ref> independently obtained an interesting result that contains Poston's result as a particular case. In practice, the conditions they assume obviously lead to nets with "many hiddens." In the case of "many hidden networks," the absence of local minima can be ensured without making assumptions about geometrical or statistical properties of data. Unfortunately, the resulting architectures are not likely to generalize very well to new examples because of the large number of trainable parameters. It is worth mentioning that the condition assumed in <ref type="bibr">[20]</ref> is slightly less restrictive than the one proposed in <ref type="bibr">[15]</ref>, since it establishes that the number of hidden units required for ensuring local minima free error surface is equal to the number of different inputs. Although it seems unlikely to have coincident inputs in practical applications, Yu's analyses may also be useful when considering "clustering" of inputs, at least when those clusters are nearly reduced to single points.</p><p>Recently, many researchers have used radial basis functions (RBF) for a wide range of applications and have also proposed comparisons to feedforward nets (see, e.g., [lo]). To the best of our knowledge, however, no analyses have been carried out on the problem of local minima far these networks. In this paper we investigate the process of leambg with RBF, in terms of the associated cost function's shape. When l e d n g from examples without prior knowledge, that shape gives some indications on the difficulty of the problem to be solved, no matter what kind of learning algorithm is used. There are many different approaches for adjusting the weights of an RBF. Basically, we can distinguish between hybrid and global learning. With hybrid schemes, first, self-organization takes place at the hidden layer and, second, LMS optimization is performed at the output layer Global learning simply deals with the optimization of the cost fundon associated with network and learning environment. We mainly focus on global learning because it is better suited for a theoretical analysis, but we also establish some intriguing connections to hybrid learning schemes.</p><p>To find the stationary points of the cost function, we compute its gradient in terms of vectorial equations. Afterwards, we introduce some hypotheses regarding the network and the learning environment which, to some extent, are dual with respect to the PR2 (for pattern recognition) hypotheseb that were assumed in [6]. The network archimre has one hidden layer of locally-tuned processing units fully-connected with the inputs. Each output is instead connected to its own group of hidden units. As in [6] we assume the existence of a zero cost solution for the learning problem. The piatttms of the learning environment are assumed to be separable by hyperspheres.</p><p>This means that all positive examples must belong to regions bounded by hyperspheres whereas all eventual negative txamples, which do not belong to the assumed classes, must be in the complementary domain. The main result of the paper is that the cost function is local minima free under these assumptions.</p><p>Moreover, we relate the computation of 'RBF to Kohonen's learning vector quantization (LVQ) [8]. "'his comparison suggests that the selected updating of hidden neurons based on the distance to the current pattem is likely to improve the chances of finding optimal solutions and also theoretically supports the use of hybrid learning. As stressed in f191, a good weight initialization plays a crucial role in avoiding local minima, and our analysis suggests that the self-organization step d a y also be useful from this point of view.</p><p>n.</p><formula xml:id="formula_0">A UNIFIED VECTORIAL FORMULATION OF LEARNING IN RADIAL BASIS FkJNCTIONS</formula><p>In this section, we define the formalism adopted throughout the paper and report some general results which will turn out to be useful in the following. We consider particularly, multilayered architectures with a hidden layer of locally-tuned units [lo] and an output layer of ordinary processing units</p><p>[163.3 21n many cases, after self-organization has taken place at the hidden layer, the optimization with respect to all the parameters may turn out to be better than simple LMS for the weights of the output layer.</p><p>3The RBF networks proposed in [lo] have a linear output. The assumption made in this paper, however, does not change the essence of the analysis, which can also be carried out under the hypothesis of linear outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. De&amp;itions</head><p>The following three entities need to be defined: a network NI a learning environment C, (set of data used for learning), and a cost index ET [16].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Network nl</head><p>It has a multilayered architecture with one hidden layer! With reference to index 1, we distinguish between the input layer (1 = 0), the hidden layer (I = l), and the output layer (1 = 2). The number of neurons per layer is denoted by n(Z). Each neuron of layer I is referred to by its index i(l): i(l) = 1,. e . , n(l). When the pattern t is presented at the input, for each neuron we consider u q ) ( t ) : neuron i(l)'s activation;</p><p>The activation is computed by propagating forward the outputs of the previous level neurons.' As already mentioned, two different kinds of neurons are considered Locally-tuned processing units in the hidden layer, and ordinary processing units in the output layer. Depending on the kind of neuron, the following processing is performed</p><formula xml:id="formula_1">4 0 ) j(O)=l q ) (t): neuron i(l)'s output.</formula><p>(1)</p><formula xml:id="formula_2">U i ( l ) ( t ) = W i ( l ) + a2 ( Z j ( O ) ( t ) -W i ( l ) , j ( 0 ) l 2 4 1 ) j ( l ) = l @ ( 2 ) ( t ) =Wi(2) + Wi(2),j(l)q(l)(t) = ' w i ( 2 ) + w ; ( 2 ) x l ( t ) . (2b)</formula><p>The output of neuron i(l) is related to the activation as follows</p><formula xml:id="formula_3">(3) In the above equations ~~( ~) , ~( l -~)</formula><p>denotes the weight of the link between the neurons j ( Z -1) and i(Z), and</p><formula xml:id="formula_4">%(1) = &amp; ( l ) ) = exP(-ai(l)), Z i ( 2 ) = f ( % ( 2 ) ) = 1/(1 + exp (-%(2)))*</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>xl(t) [ ~l ( l ) ( t ) , . . . , ~n ( l ) ( t ) l ' , K ( l )</head><p>A h ( l ) , I ( l -l ) , . * * I W(l),n(l-l)I'.</p><p>wi(l) denotes the threshold of ordinary and locally-tuned neurons and oql) defines the "width" of the Gaussians used as locally-tuned processing units.</p><p>Remark: Unlike the RBF proposed in [lo], (2a) includes a bias term. Since</p><formula xml:id="formula_5">%(l) = exp (--"i(l))</formula><p>4This restriction on the number of hidden layers is motivated by recent 51n the sequel, both layer and pattern indexes may be omitted for the sake research on approximation techniques (see, e.g., <ref type="bibr">[12]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>of simplicity.</head><p>the contribution of hidden neuron z(1) to output neuron j ( 2 ) turns out to be Analogous relationships hold for 8 2 , which, instead, belongs to R"(')+'? "(' 1, as there is no row for differentiation with respect to the Gaussian width term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Gradient Commtation</head><formula xml:id="formula_6">It X o ( t ) -Wi(1) 112 e( 1) [Wj(2),i(l) exp (-Wi(l))l exp</formula><p>By denoting wj(2),%(1) [wj(2).i(1) exp (-wi(l))l, the RBF On the basis of these definitions, the gradient of ( <ref type="formula">5</ref>) can be written in a very compact vectorial formula. Thanks to the under consideration becomes exactly the one assumed in <ref type="bibr">[IO]</ref>.</p><p>hypothesis concerning the network architecture, the computation of the gradient can be performed by using the elegant BP style.6</p><p>For the output layer, the use of BP computing scheme makes it possible to determine the stationary points as follows</p><formula xml:id="formula_7">(7) being 2 1 = [X1(l) + $ . X n ( l )</formula><p>II] E RT?"(l)+l and II = [I, . . . , 11' E RT. Notice that in this case we can even obtain the matrix 8 2 = A?[&amp; directly.</p><p>For locally-tuned hidden neurons, the use of backpropagation rule leads to</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">) Leaming Environment Le</head><p>that set as a collection of T input-output pairs</p><p>We use a set of supervised data. It is convenient to think of</p><formula xml:id="formula_8">c, = { ( X o ( t ) , D ( t ) ) . X o ( t ) E R"(O), D ( t ) E R"(2), t = 1, * . . . T } . 6*(2) = 0 =+ A?&amp;) = 0, i ( 2 ) = 1,. . . ,n(2)</formula><p>X o ( t ) is the input pattern and D ( t ) is its corresponding target.</p><p>Each component of D ( t ) is either d or 2 (asymptotical targets).</p><p>3) Cost Index means of the cost function Given the inPut-ouQut data fitting is m e a ~~e d by</p><formula xml:id="formula_9">T ET(Wz,j;Mj. Le) = Et Gz(1) 0 * j ; , % ( l ) K ( i ) = 0, i ( 1 ) = 1.. . . , n ( l ) (8) where t=l T "(2) = 4 1 [d,(2)(t) -"3(2)(t)I2. (5) t=l3(2)=1</formula><p>To understand the basic result proposed in this paper, some more symbols have to be introduced which allow us to deal a compact vectorial formulation of the problem.</p><p>X,(l) I . [ q l ) ( 1 ) , . . . . X%(~)(T')]' E RT is called output trace of neuron ~( 1 ) . This vector stores the output of neuron i(1) for all the T patterns of the learning environment.</p><p>The output trace for all the neurons of a given layer 1 is called layer output trace. It is kept in the matrix Xi = [X1(q . . . X,(l)] E RT7"('). 0 5 1 5 2. ( <ref type="formula">6</ref>)</p><p>wt(l),J(l-l) is the weight that connects neuron j ( l -1) to neuron ~( 1 ) .</p><p>The resulting matrix Wl-1 E R"(')? "(l-l) is referred to as layer weight matrix. The symbol 0 denotes the weight space.</p><p>Let y%(l)(t) = dEt/da,(l)(t). We call delta trace the vector Y,(l) = [yt(l)(l). . . . . y,(l)(T)]' E RT and layer delta trace the matrix Yl = [Yl(l) . . . Y,(l)] E RT!"('). We denote by S)) c RT~"(') the set of all the Y, generated by varying the weights in 0. We assume that there is no connection which skips a layer. Therefore, for the weights connecting layers zero to one, the gradient can be represented by a matrix </p><formula xml:id="formula_10">81 E R"(0)+2)n(1). whose generic element is if j(0) = n(0) + 2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OFTIMAL LEARNING FOR PA' MERNS SEPARATED BY HYPERSPHERES</head><p>In this section we analyze the above stated problem of learning from examples as the optimization of the cost function (5). Basically, we deal with global learning and batch mode weight updating. The weight configurations determined by such a scheme are defined by ( <ref type="formula">7</ref>) and (8), stating that the gradient of cost function ( <ref type="formula">5</ref>) is null. As discussed further on, the above learning scheme is somewhat related to pattern mode weight updating and, also, to hybrid learning schemes. Since we assume a multilayered RBF architecture, the results established in [6] can be extended conveniently. We focus 61t is worth mentioning that such a computation is not necessarily the most useful scheme to be used for learning. This is particularly clear for RBF where "centers and widths" are determined with very efficient self-organization schemes [lo]. Notice, however, that we are interested in performing an analysis on the shape of the cost surface and, mainly, in determining the stationary points of the cost, no matter what learning algorithm is used.</p><p>in particular on all the weight configurations associated with JJl -t 0, E = 1, 2, and provide a natural extension of Theorems 2) the weight layer matrix W1 has full row rank;</p><p>3) the f?llowing relationships between the kernel of matri-Vi(1) = l , . . . , n(1). ( <ref type="formula">10</ref>)</p><p>In the hypotheses of this theorem, batch mode BP cannot get stuck in local minima. Once the gradient method converges, it reaches the absolute minimum with "null cost." At this point, it is worth making a few remarks concerning the hypotheses of the theorem. Like Theorem 1 in [6], the first two assumptions are easy to understand and are quite reasonable. The first one typically holds for pattern recognition problems, in which hidden and output layers contain input representations that are progressively more and more compressed when going towards the outputs. As also <ref type="bibr">Baldi and Homik [l]</ref> pointed out, the second hypothesis is not a very restrictive one and holds with probability one for random matrices. The problem with point 2 of PRI-bis.2 assumptions, however, is that it must be checked throughout the learning process. The third hypothesis is the hardest to check, since it requires the knowledge of Szl): i.e., the set of all the Y,(l) E RT generated by varying the weights in R.</p><p>To discover meaningful conditions, with a straightforward practical interpretation, we need to put forward assumptions about data. In the following theorem we require that the input patterns be separated by hyperspheres. Pro08 See the Appendix. 0 Some remarks are worth mentioning concerning this result. 1 ) Network Architecture: As already noticed for feedforward nets, network assumption point 2 of PRl-bis assumptions is quite reasonable in practice [6]. <ref type="bibr">Plaut and Hinton [13]</ref> showed that this kind of architecture learns faster than one with fully-connected layers. Jacobs et al.</p><p>[7] considered network assumption point 2 of PR2-bis as a first step towards the conception of modular architectures that are usually well suited for good generalization.</p><p>2) Ourput Coding: It is quite easy to realize that Theorem 2 also holds for networks with only one output and positive examples belonging to a region delimited by a given hypersphere. In this case, the class c is simply coded by one, whereas E is coded by zero. This extension also holds in the case of C exclusively coded classes and a set of negative examples which do not belong to the assumed classes.</p><p>3) Beyond Hyperspheres: When choosing more general processing units for the hidden layer, one may wonder if the results established in Theorem 2 also hold for different separation surfaces. It can be shown that this is indeed the case for more general RBF in which the locally-tuned processing units follow the equation being Q,(l) E Rn(')tn(O) a symmetric matrix associated with neuron i ( 1). In that case, following exactly the proving scheme drawn in Lemma 5, (see the Appendix) it can be shown that the cost function (5) is local minima free if the patterns are separated by a general quadratic surface. This extension is related to input preprocessing which allows us to obtain complex separation surfaces with just one layer network (linear machines) (see, e.g., [l 1, pp. 29-30]). Notice that the quadratic preprocessing for linear machines is not equivalent to the RBF studied in this paper for at least two reasons. First, with linear machines and quadratic preprocessing, the quadratic separation among patterns of different classes is a necessary condition for determining their parameters. Consequently, when the separation condition does not hold, no solution can be found, whereas with RBF the condition established by Theorem 2 is only sufficient and, therefore, solutions might be found even when the separation condition is not met. Second, the generalization to new examples is generally different. With multilayered architectures like RBF, the prior knowledge on a given problem can be exploited much better than for linear machines, and this may lead to significant improvements in generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">) LMS-Threshold Functions:</head><p>The analysis put forward in this paper assumes quadratic cost functions having "asymp totical targets" which force the learning algorithms towards asymptotical weights. In practice, however, this may limit the efficiency of a learning algorithm. The use of LMS-Threshold cost functions <ref type="bibr">[17]</ref> removes this kind of limitation and allows us to guarantee the absence of local minima while ensuring finite weight solutions (see, e.g., <ref type="bibr">[4] and [17]</ref> for the case of feedforward nets).</p><p>5) Forcing Limited Hyperspheres: In many cases, the optimization algorithms for the cost (5) may discover hyperspheres with "large" a. In the limit, very large values for a produce discrimination surfaces very similar to those obtained with ordinary processing units. If besides classes discrimination one is interested in rejecting "negative" input patterns (i.e., to get nearly zero outputs for patterns that do not belong to any valid class), then separation with smaller hyperspheres allows us to better exploit the advantages of locally tuned processing units. A possible way of restricting the input regions mapped to valid classes is to provide negative examples during learning. Unfortunately, in most relevant applications defining the set of negative examples is nearly impossible. We can, however, force a prior constraint on the radii of the hyperspheres and learn under such a constraint. We can easily demonstrate that Theorem 2 also holds in this case. It suffices to assume a = B/(1 + exp(-a,)) and consider the cost as a function of ap. In so doing, g is constrained into the interval (0, B) that can be properly chosen with some prior knowledge about the problem to be solved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6) Relationships with Hybrid Leaming:</head><p>It is well known that pattern mode weight updating departs to some extent from exact gradient descent of cost function ET. The adoption of "small" learning rates,7 however, arbitrady reduces the difference with respect to batch mode <ref type="bibr">[16]</ref>. Following the gradient descent scheme in pattern mode, the weights ol' the locally-tuned processing units are updated according to</p><formula xml:id="formula_11">V i ( 1 ) = 1,. . . , n(l), t = 1, . . . , T (14)</formula><p>where E is a learning rate, Wt!(l, = [W~(I),:(O), . . . , w ; ( ~) , ~( o ) ] ' , and T is the iteration index. This equation is essentially the same as the one used for performing the updating of codebook vectors in LVQ [8]. In particular, 2 q ; ( 1 ) ( t ) / c ~f <ref type="bibr">( ~)</ref> plays the role of coefficient a ( t ) and is consistent with the requirement of being asymptotically null. The basic difference, however, is that in BP optimization schemes each pattern of the learning environment affects each vector W;(l) (codebook vector), whereas with LVQ just the patterns closest to X o ( t ) in the Euclidean metric are taken into account (one vector in LVQl and two vectors in LVQ2 and LVQ3). In BP optimization schemes, however, the updating of the codebook vectors depends strongly on the distance (IXo(t) -Wi(l) 11. For a given pattern X o ( t ) , all codebook vectors W;(l) such that q l ) ( t ) M 7The term "small" is strictly related to the dimension of the learning environment. 0, in practice, are not updated since Y;(~) M 0 in that case. One more significant difference is that BP optimization typically leads to distributed representations, whereas the LVQ solution produces codevectors that locally model clusters of points.</p><p>These remarks also suggest that hybrid learning as described in <ref type="bibr">[lo]</ref> may turn out to be less affected by suboptimal solutions. In that case the self-organization step provides a first tuning of the codebook vectors such that they go away from each other. In so doing, in practice, the number of codebook vectors reacting to a given pattern decreases. The resulting effect is that of linking the reaction of processing units to specific patterns. As a result, for a given processing unit the optimization takes place in a subset of the learning environment. Hence, hybrid learning performs a sort of divide and conquer and is likely to be faster than ordinary BP optimization beginning from random weights. The conclusion is that in many practical cases hybrid learning can be very successful, particularly if BP optimization is used as second step instead of simple LMS used for adjusting W1. 7 ) Beyond Y I 's Sign Rule: Lemma 4 (see the Appendix) plays a very crucial role in the proof of Theorem 2. Basically, it describes J', in terms of the signs of its components. The sign structure proposed by Lemma 4 holds for any point of the weight space. Theorem 2 is based on that description of Y1 and assumes no other knowledge on nonlinear map Yl(0). Theorem 2 could be made tighter by a more accurate description of that map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSIONS</head><p>In this paper we have given some analyses on the problem of learning in RBF independently of the learning algorithm. Using the proposed vectorial formalism, it is shown that when patterns are separated by hyperspheres, or more generally by quadratics, the cost function associated with radial basis function is local minima free. Notice that our conditions are only sufficient and are based on an approximate knowledge of the map &amp;(n) that derives from the assumption of asymptotical targets, typical of pattern recognition problems. The theoretical conclusions reported in [3], however, indicate that the use of nonasymptotical targets, commonly used in function approximation, introduces additional spurious local minima. It is worth mentioning that even with local minima free cost functions, most learning algorithms may be trapped into a plateau due to the saturation of either locally-tuned processing units or ordinary squashing units. The analysis proposed in the paper allows us also to establish some intriguing comparisons to hybrid learning. Basically, hybrid learning performs a sort of divide and conquer, with respect to the learning environment, that is likely to improve optimal convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. APPENDIX</head><p>In the sequel, the track of the proofs given in [6] will be followed, as almost all results obtained there can be extended to RBF using the stated vector notation. The basic ideas used in proving Theorem 2 are quite simple. The assumed data structure allows us to conclude that 7 1 --f 0 (Lemma 5). This conclusion relies on &amp;'s sign rule that holds under the assumption of asymptotical targets (Lemma 4).</p><p>As already mentioned, this assumption limits our analysis to pattem recognition. By using the PR2-bis network hypotheses, &amp; + 0 follows directly from Y1 --t 0, unless the weights of the last layer are null. In this special case, however, using Lemma 6, we prove that these ccmflgurations are not local minima. Finally, the condition Y2 -+ 0 implies, according to Lemma 3, that only the configurations for which ET 0 are minima. Some preliminary lemmas from [6] and slightly restated. Lemmas 5 and 6, instead, are proven under the new assumption on RBF and pattems separated by hyperspheres. 0 Remark: The proof, given in [6], assumes that there is outputltarget matching for all the pattems exfept &amp;, for which there is a mismatch between the target di(z)(t) and the output f ( a z ( 2 ) ( 0 ) .</p><p>The scheme of this proof, however, can easily be extended to all the configurations such that Y2 --t 0 and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem I :</head><p>If the PRI-bis hypotheses hold, then Lemma 2 indicates that the points for which the gradient is "zero" are the same for which &amp; is "zero." From Lemma 3, it follows that only the points whose cost is "zero" represent attractive points for the gradient descent (minima of the Consequently Y,(l) = 0 is the unique solution of ( <ref type="formula">22</ref>). 0</p><p>Lemma 6: Let PR2-bis hypotheses hold, and let us assume the existence of a stationary point such that the weights connecting the neurons of the hidden sublayer H , to the corresponding output c are all null, i.e., WC,Zc(l) = 0, %(1) E H,.</p><p>(28)</p><p>Then, this stationary point is not a local minimum.</p><p>Pro08 Condition (28) implies that for i ( 2 ) = c we get</p><formula xml:id="formula_12">Y i ( 2 ) W = f/(wc)(f(wc) -U t ) ) # 0 (29)</formula><p>where w, is the bias for output c, d,(t) is the target for pattern t and f is the squash-function of output neurons. From the network hypotheses it follows that the Hessian matrix 'FI has the following block-diagonal structure where Y, E RT has the sign structure explained by the right side of (25). From Lemma 5 and condition (28) we get that 'M;(O, 1) is not identically null. Therefore, the matrix 8, has the following structure where P is assumed positive definite and D is not the null matrix. By applying Sylvester's theorem (e.g., see [2 p. 1041)</p><p>it can be obtained that 'FIc has both positive and negative eigenvalues. Hence the proof of the lemma directly follows from (30). 0 Proof ofZ7zeorem 2: From Lemma 6 it follows that, if the PR2-bis hypotheses hold, the stationary points defined by the condition of zero weight for the connections between any hidden sublayer to the corresponding output are not local minima. Therefore, we only have to examine the case in which the weights connecting each hidden sublayer to the corresponding output are not all null. In this case, Theorem 2 easily follows from a direct application of Theorem 1 because PR2bis network conditions satisfy point 1 of PRI-bis assumptions and point 2 of PRl-bis assumptions hypotheses, and PR2bis learning environment and output coding conditions satisfy 0 point 3 of PRI -bis assumptions hypothesis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>L J and MT (e) is the T-matrix-replica operator which creates a matrix with T rows all equal to the argument Wz!(l,. In the above equation Ai(1) = [&amp;(1)(1) . . . Ai(l)(T)I E RT where Ai(l)(t) = I 1 X o ( t ) -W(1) It2 /r:(l), t = 1 , . . . , T , derives from differentiation with respect to Gaussian widths, and II = [I, . . . , 11' E RT derives from biases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1Theorem 1 :</head><label>1</label><figDesc>and 2 proven in [6]. All the results in this section hold for a network N having enough capacity for the given learning environment Le, i.e., we assume the existence of one null cost solution. Cost function E T ( w , , ~; N , Le) has no local minima if the network N and its associated learning environment Le satisfy the following PRl -bishypotheses: 1) n(2) 5 n(1) (pyramidal hypothesis);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 2 :</head><label>2</label><figDesc>Cost function E T ( w ; , ~; N , Le) has no local minima if the network N and the learning environment Le satisfy the following PR2-bis hypotheses: ces and Szl) hold Ker[,?A,i(l)] "S:,) = {a}, Pm08 See the Appendix. 0 Network. a&gt; the network has C outputs, where C is the number of classes; b) full connections are assumed from the input to the hidden layer. The hidden layer is divided into C sublayers, H I , ' . -, H,, . . . , Hc, and connections are only permitted from any sublayer to the associated output. The sublayer H, contains n,(l) neurons referred to by the index ic( 1). Output coding. Exclusive coding is used for the output, i.e., if pattern t belongs to class c, c = 1,. . . , C, then d i ( t ) = Learning environment. All the patterns of Le are separated by hyperspheres, i.e., for each class c, with c = 1, . . . , C, there exists a pair 11 Xo(t) -C, 11 -r, 5 0, 11 Xo(t) -C, (1 -r,&gt;O, Vtnotinclassc (12) Vtinclassc, where C, and T-, are center and radius of the hypersphere, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Lemma I :</head><label>:</label><figDesc>Let p s consider the input layer ( I = 0) and assume that Ker[XA,i(l)] n .S$) = {a}. Then, for an arbitrary real constant E &gt; 0 there exists 6 = S ( E ) &gt; 0 such that Proof: See [6]. 0 Lemma 2: Let us assume that the PRI -bis hypotheses hold and let E &gt; 0 be an arbitrary positive real number. Then, for a compact region of the weight space, there exists 6 = 6(e) &gt; 0, such that Proof: See [6]. Note that, in this case, the compactness hypothesis impliesAs a result, we need to choose dmin = min{d, lJl} to extend U Lemma 3: Let us consider a configuration for which Y2 --+ 0 and the cost ET # 0. For this configuration there exist i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Lemma 4 :Lemma5:</head><label>4</label><figDesc>Let us consider a network whose architecture matches PR2-bis hypotheses and assume also exclusive coding for the C classes. Partitioning the vector Kc(l) as ET # 0. cost). 0 we get the following "sign" structure IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 6, NO. 3, MAY 1995 where e; A [l, 1, . per class, and 1 1 E RTc , T, is the number of pattems Qi, = fl if WC,ic(l) # 0, See [6]. Notice that, in this case, the derivative of 0 Let us consider a network N and a learning locally-tuned output function is always negative. environment Le that match the PR2-bis hypotheses. Then x;,i(l)K(l) = 0, i(1) = 1 , . * . , If the pattems are separable by hyperspheres, then (12) holds. As a result, for each Wqr) E R"(O), there exist C vectors @;(W+)) = (&amp;(W+)), 1, D(Wl(l))) E R"each class c and for each neuron ic( 1) of the hidden sublayer H,, the following equality must hold Because of (25) and (20) in Lemma 4, it easily follows that Y,c(l) = 0, Vi,(l) = l , . . . , n,p) and c = l , . . . , C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>where E, E R("(0) f 2 ) Q r (1) f n c ( 1 )+ 1 I (n(0)+2)nr ( l ) + n c (1 ) + I is the submatrix associated to the subnetwork defined by the input layer, hidden sublayer H,, and output c. the weights connecting the hidden units to the outputs and the inputs to the hiddens, respectively, while 'FIJO, 1) = 'FI,(l, 0)' E R ( " ( 0 ) + 2 ) " &lt; ( 1 ) n c ( l ) + l represents the cross-contribution of these weights.We observe that condition (28) implies that the delta traceY,c(l) E R T , b'G(1) E H,,is identically null. Hence, from BP equations we obtain that X,(O, 0) is the null matrix. The generic element of H,(O, 1) has the following expression T d2 ET '2C ( % ( O ) ( t ) -WZc(1where i ( 2 ) = c, ~( 0 )denotes the generic input and f"is the locally-tuned output function. Hence, any subcolumn 'FI,"(O, 1) E I?"(')+' of ;Ft,(O, 1) can be written as</figDesc></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural networks and principal component analysis: Learning from examples without local minima</title>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks, R. Bellman, Introduction to Matrix Analysis</title>
		<imprint>
			<date type="published" when="1974">1974</date>
			<pubPlace>New York McGraw-Hill</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Backpropagation fails to separate where perceptrons succeed</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Slawny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="665" to="674" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Backpropagation for linearly separable patterns: A detailed analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Con$ Neural Networks</title>
		<meeting>IEEE Int. Con$ Neural Networks<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="1818" to="1822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Successes and failures of hackpropagation: A theoretical investigation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Progress in Neural Networks, 0. Omidvar</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the problem of local minima in backpropagation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
	<note>COINS</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The self-organizing map</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<biblScope unit="volume">78</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generalization and network design strategies</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Connecrionism in Perspective</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast leaning in networks of locally-tuned processing units</title>
		<author>
			<persName><forename type="first">J</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Darken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuraI Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="294" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning Machines</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nilsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965">1965</date>
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Universal approximation using radial-basisfunction networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Sandbeg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="246" to="257" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning set of filters using backpropagation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Phut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Language</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3541</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Networks for approximation and leaning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1481" to="1497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Local minima and backpropagation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Poston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Con&amp; Neural Networks</title>
		<meeting>Int. Joint Con&amp; Neural Networks<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="173" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Williams</surname></persName>
		</author>
		<editor>Parallel Distributed Processing D. Rumelhart and J. McClelland</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ch</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="318" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Backpropagation separates when perceptrons do</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Sussman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Join1 Con$ Neural Networks</title>
		<meeting>Int. Join1 Con$ Neural Networks<address><addrLine>Washington DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Terminal attractor learning algorithms for backpropagation neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Con$ Neurul Networks</title>
		<meeting>Int. Joint Con$ Neurul Networks<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-11">Nov. 1991</date>
			<biblScope unit="page" from="183" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Avoiding false local minima by proper initialization of connections</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wessels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barnad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="899" to="905" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Can backpropagation error surface not have local minima?</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1019" to="1020" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Terminal attractors in neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurul Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="53" to="58" />
			<date type="published" when="1989">1989. 1989</date>
			<publisher>Ahlex Publishing</publisher>
			<pubPlace>Norwood, NJ</pubPlace>
		</imprint>
	</monogr>
	<note>in press</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
