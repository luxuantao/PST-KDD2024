<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2020 ROBERTA: A ROBUSTLY OPTIMIZED BERT PRE-TRAINING APPROACH</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2020 ROBERTA: A ROBUSTLY OPTIMIZED BERT PRE-TRAINING APPROACH</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019)  that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE, SQuAD, SuperGLUE and XNLI. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Self-training methods such as ELMo <ref type="bibr" target="#b35">(Peters et al., 2018)</ref>, GPT <ref type="bibr" target="#b39">(Radford et al., 2018)</ref>, BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>, XLM <ref type="bibr" target="#b25">(Lample &amp; Conneau, 2019)</ref>, and XLNet <ref type="bibr" target="#b56">(Yang et al., 2019)</ref> have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most. Training is computationally expensive, limiting the amount of tuning that can be done, and modeling advances are often conflated with changes in data size or composition.</p><p>We present a replication study of BERT pretraining <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>, which includes a careful evaluation of the effects of hyperparameter tuning and training set size. We find that BERT was significantly undertrained and propose an improved training recipe, which we call RoBERTa, that can match or exceed the performance of all of the post-BERT methods. Our modifications are simple, they include: (1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data. We also collect a large new dataset (CC-NEWS) of comparable size to other privately used datasets, to better control for training set size effects.</p><p>When controlling for training data, our improved training procedure improves upon the published BERT results on the GLUE <ref type="bibr" target="#b52">(Wang et al., 2019b)</ref> and SQuAD <ref type="bibr" target="#b41">(Rajpurkar et al., 2016)</ref> benchmarks. When trained for longer over additional data, our model achieves a score of 88.5 on the public GLUE leaderboard, matching the 88.4 reported by <ref type="bibr" target="#b56">Yang et al. (2019)</ref>. Our model establishes a new stateof-the-art on 4/9 of the GLUE tasks, as well as RACE <ref type="bibr" target="#b24">(Lai et al., 2017)</ref>, SuperGLUE <ref type="bibr" target="#b51">(Wang et al., 2019a)</ref>, and XNLI <ref type="bibr" target="#b7">(Conneau et al., 2018)</ref>, and matches the state-of-the-art on SQuAD. Overall, we re-establish that BERT's masked language model training objective is competitive with recently proposed alternatives such as perturbed autoregressive language modeling (Yang et al., 2019). 2   In summary, the contributions of this paper are: (1) We present a set of important BERT design choices and training strategies and introduce alternatives that lead to better downstream task performance; (2) We use a novel dataset, CC-NEWS, and confirm that using more data for pretraining further improves performance on downstream tasks; (3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods. We release our model, pretraining and fine-tuning code.</p><p>Setup: BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> takes as input a concatenation of two segments (sequences of tokens), x 1 , . . . , x N and y 1 , . . . , y M . Segments usually consist of more than one natural sentence. The two segments are presented as a single input sequence to BERT with special tokens delimiting them: [CLS ], x 1 , . . . , x N , [SEP ], y 1 , . . . , y M , <ref type="bibr">[EOS ]</ref>. M and N are constrained such that M +N &lt; T , where T is a parameter that controls the maximum sequence length during training.</p><p>Architecture: BERT uses the now ubiquitous transformer architecture <ref type="bibr" target="#b50">(Vaswani et al., 2017)</ref>, which we will not review in detail. We use a transformer architecture with L layers. Each block has A self-attention heads and hidden dimension H.</p><p>Training Objectives: BERT uses two pretraining objectives: masked language modeling and next sentence prediction. For the Masked Language Model (MLM) objective, BERT is trained via a crossentropy loss to predict 15% of the input tokens, selected at random. To prevent the model from cheating, 80% of these selected tokens are replaced by a special [MASK ] symbol in the input, 10% are replaced by a random token from the vocabulary, and 10% are left unchanged. Optimization: BERT is optimized with AdamW (Kingma &amp; Ba, 2015) using the following parameters: β 1 = 0.9, β 2 = 0.999, = 1e-6 and decoupled weight decay of 0.01 <ref type="bibr" target="#b29">(Loshchilov &amp; Hutter, 2019)</ref>. The learning rate is warmed up over the first 10,000 steps to a peak value of 1e-4, and then linearly decayed. BERT trains with a dropout of 0.1 on all layers and attention weights, and a GELU activation function <ref type="bibr" target="#b17">(Hendrycks &amp; Gimpel, 2016)</ref>. Models are pretrained for S = 1,000,000 updates, with mini-batches containing B = 256 sequences of maximum length T = 512 tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Next Sentence</head><p>Data: BERT is trained on a combination of BOOKCORPUS <ref type="bibr" target="#b59">(Zhu et al., 2015)</ref> plus English WIKIPEDIA, which totals 16GB of uncompressed text.<ref type="foot" target="#foot_2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">IMPLEMENTATION</head><p>We reimplement BERT in FAIRSEQ <ref type="bibr" target="#b34">(Ott et al., 2019)</ref>. We primarily follow the original BERT optimization hyperparameters, given in Section 2, except for the peak learning rate and number of warmup steps, which are tuned separately for each setting. We found training to be very sensitive to the Adam epsilon term, and in some cases we obtained better performance or improved stability after tuning it. We also set β 2 = 0.98 to improve stability when training with large batch sizes.</p><p>We pretrain with sequences of at most T = 512 tokens. Unlike <ref type="bibr" target="#b11">Devlin et al. (2019)</ref>, we do not randomly inject short sequences, and we do not train with a reduced sequence length for the first 90% of updates. We train only with full-length sequences.</p><p>We train with mixed precision floating point arithmetic on DGX-1 machines, each with 8 × 32GB Nvidia V100 GPUs interconnected by Infiniband <ref type="bibr" target="#b31">(Micikevicius et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DATA</head><p>BERT-style pretraining crucially relies on large quantities of text. <ref type="bibr" target="#b1">Baevski et al. (2019)</ref> demonstrate that increasing data size can result in improved end-task performance. Several efforts have trained on datasets larger and more diverse than the original BERT <ref type="bibr" target="#b40">(Radford et al., 2019;</ref><ref type="bibr" target="#b56">Yang et al., 2019;</ref><ref type="bibr" target="#b57">Zellers et al., 2019)</ref>. Unfortunately, not all of the additional datasets can be publicly released. For our study, we focus on gathering as much data as possible for experimentation, allowing us to match the overall quality and quantity of data as appropriate for each comparison.</p><p>We consider five English-language corpora of varying sizes and domains, totaling over 160GB of uncompressed text: (1&amp;2) BOOKCORPUS <ref type="bibr" target="#b59">(Zhu et al., 2015)</ref> plus English WIKIPEDIA, which is the original data used to train BERT (16GB); (3) CC-NEWS, which we collect from the English portion of the CommonCrawl News dataset <ref type="bibr" target="#b32">(Nagel, 2016)</ref>, containing 63 million English news articles crawled between September 2016 and February 2019 (76GB after filtering);<ref type="foot" target="#foot_3">4</ref> (4) OPEN-WEBTEXT <ref type="bibr" target="#b15">(Gokaslan &amp; Cohen, 2019)</ref>, an open-source recreation of the WebText corpus described in <ref type="bibr" target="#b40">Radford et al. (2019)</ref>, containing web content extracted from URLs shared on Reddit with at least three upvotes (38GB);<ref type="foot" target="#foot_4">5</ref> (5) STORIES, a dataset introduced in Trinh &amp; Le (2018) containing a subset of CommonCrawl data filtered to match the story-like style of Winograd schemas (31GB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">EVALUATION</head><p>Following previous work, we evaluate our pretrained models by finetuning on downstream tasks:</p><p>• GLUE: The General Language Understanding Evaluation (GLUE) benchmark <ref type="bibr" target="#b52">(Wang et al., 2019b)</ref> is a collection of 9 datasets for evaluating natural language understanding systems. Tasks are framed as either single-sentence classification or sentence-pair classification tasks. The GLUE organizers provide training and development data splits as well as a submission server and leaderboard that allows participants to evaluate and compare their systems on private held-out test data. • SQuAD: The Stanford Question Answering Dataset (SQuAD) provides a paragraph of context and a question. The task is to answer the question with a span extracted from the context. We evaluate on SQuAD V1.1 and V2.0 <ref type="bibr" target="#b41">(Rajpurkar et al., 2016;</ref><ref type="bibr">2018)</ref>. In V1.1 the context always contains an answer, while in V2.0 some questions are not answered in the provided context. • RACE: ReAding Comprehension from Examinations (RACE) <ref type="bibr" target="#b24">(Lai et al., 2017</ref>) is a large-scale reading comprehension dataset collected from English examinations in China. The task is to choose among four possible answers to a given question, using a given passage of text as context. • Additional Benchmarks: In the Appendix we present additional results for SuperGLUE <ref type="bibr" target="#b51">(Wang et al., 2019a)</ref> and XNLI <ref type="bibr" target="#b7">(Conneau et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TRAINING PROCEDURE ANALYSIS</head><p>This section explores and quantifies which choices are important for successfully pretraining BERT models. We keep the model architecture fixed.<ref type="foot" target="#foot_5">6</ref> Specifically, we begin by training BERT models with the same configuration as BERT BASE (L = 12, H = 768, A = 12, 110M params).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">STATIC VS. DYNAMIC MASKING</head><p>As discussed in Section 2, BERT relies on predicting randomly masked tokens. The original BERT implementation performed masking once during data preprocessing, resulting in a single static mask.</p><p>To avoid repeating the same masks at every epoch, training data was duplicated 10 times prior to preprocessing, so that each training sequence was seen with the same mask only four times over the course of 40 training epochs. We instead train with dynamic masking, where we generate the masking pattern on-the-fly each time we input a sequence to the model. This becomes crucial when pretraining for more steps or with larger datasets, and additionally performs marginally better than static masking on some downstream tasks (see Appendix A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MODEL INPUT FORMAT AND NEXT SENTENCE PREDICTION</head><p>In the original BERT pretraining procedure, the model observes two concatenated document segments and is trained via an auxiliary Next Sentence Prediction (NSP) loss to predict whether these segments were sampled contiguously from the same document or from distinct documents. The NSP objective was designed to improve performance on downstream tasks, such as Natural Language Inference <ref type="bibr" target="#b4">(Bowman et al., 2015)</ref>, which require predicting relationships between pairs of sentences. <ref type="bibr" target="#b11">Devlin et al. (2019)</ref> observe that removing NSP hurts performance, with significant performance degradation on QNLI, MNLI, and SQuAD 1.1. However, recent work has questioned the necessity of the NSP loss <ref type="bibr" target="#b25">(Lample &amp; Conneau, 2019;</ref><ref type="bibr" target="#b56">Yang et al., 2019;</ref><ref type="bibr" target="#b21">Joshi et al., 2019)</ref>.</p><p>To better understand this discrepancy, we compare several alternative training formats:</p><p>• SEGMENT-PAIR+NSP: This follows the original input format used in BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>, with the NSP loss. Each input has a pair of segments, which can each contain multiple natural sentences, but the total combined length must be less than 512 tokens. • SENTENCE-PAIR+NSP: Each input contains a pair of natural sentences, either sampled from a contiguous portion of one document or from separate documents. Since these inputs are significantly shorter than 512 tokens, we increase the batch size so that the total number of tokens remains similar to SEGMENT-PAIR+NSP. We retain the NSP loss. • FULL-SENTENCES: Each input is packed with full sentences sampled contiguously from one or more documents, such that the total length is at most 512 tokens. Inputs may cross document boundaries. When we reach the end of one document, we begin sampling sentences from the next document and add an extra separator token between documents. We remove the NSP loss. • DOC-SENTENCES: Inputs are constructed similarly to FULL-SENTENCES, except that they may not cross document boundaries. Inputs sampled near the end of a document may be shorter than 512 tokens, so we dynamically increase the batch size in these cases to achieve a similar number of total tokens as FULL-SENTENCES. We remove the NSP loss.</p><p>Results Table <ref type="table" target="#tab_0">1</ref> shows results for the four different settings. We first compare the original SEGMENT-PAIR input format from <ref type="bibr" target="#b11">Devlin et al. (2019)</ref> to the SENTENCE-PAIR format; both formats retain the NSP loss, but the latter uses single sentences. We find that using individual sentences hurts performance on downstream tasks, which we hypothesize is because the model is not able to learn long-range dependencies.</p><p>We next compare training without the NSP loss and training with blocks of text from a single document (DOC-SENTENCES). We find that this setting outperforms the originally published BERT BASE results and that removing the NSP loss matches or slightly improves downstream task performance, in contrast to <ref type="bibr" target="#b11">Devlin et al. (2019)</ref>. It is possible that the original BERT implementation may only have removed the loss term while still retaining the SEGMENT-PAIR input format.</p><p>Finally we find that restricting sequences to come from a single document (DOC-SENTENCES) performs slightly better than packing sequences from multiple documents (FULL-SENTENCES </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">TRAINING WITH LARGE BATCHES</head><p>Past work in neural machine translation has shown that training with large mini-batches can improve optimization speed and end-task performance when the learning rate is tuned appropriately <ref type="bibr" target="#b33">(Ott et al., 2018)</ref>. Large batches are also easily parallelized via data parallel training.<ref type="foot" target="#foot_6">7</ref> </p><p>Table <ref type="table" target="#tab_1">2</ref> shows the masked LM perplexity and end-task accuracy for BERT BASE as we increase the batch size, while tuning the learning rate. <ref type="bibr" target="#b11">Devlin et al. (2019)</ref> originally trained BERT BASE for 1M steps with a batch size of 256 sequences; however a batch size of 2K sequences performs better, even controlling for the number of epochs, suggesting that the original BERT batch size was too small. We also observe that training with extremely large batches (8K) becomes more efficient as we train for more epochs. <ref type="foot" target="#foot_7">8</ref> In the remainder of our experiments we train with batches of 8K sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">TEXT ENCODING</head><p>Byte-Pair Encoding (BPE) <ref type="bibr" target="#b45">(Sennrich et al., 2016</ref>) is a hybrid between character-and word-level modeling based on subwords units. BPE vocabulary sizes typically range from 10K-100K subword units; however, unicode characters can account for a sizeable portion of this vocabulary when modeling large and diverse corpora, such as the ones considered in this work.</p><p>The original BERT implementation <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>  Early experiments revealed only minor differences between these encodings, with the byte-level BPE achieving slightly worse end-task performance on some tasks. Nevertheless, we believe the advantages of a universal encoding scheme outweighs the minor degredation in performance and use this encoding in the remainder of our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ROBERTA</head><p>In the previous section we propose modifications to the BERT pretraining procedure that improve end-task performance. We now aggregate these improvements and evaluate their combined impact. We call this configuration RoBERTa for Robustly optimized BERT approach. Specifically, RoBERTa is trained with dynamic masking (Section 4.1), FULL-SENTENCES without NSP loss (Section 4.2), large mini-batches (Section 4.3) and a larger byte-level BPE (Section 4.4). For example, XLNet <ref type="bibr" target="#b56">(Yang et al., 2019)</ref> was pretrained using 10 times more data than BERT, with a batch size eight times larger for half as many optimization steps, thus seeing four times as many sequences in pretraining compared to <ref type="bibr" target="#b11">Devlin et al. (2019)</ref>.</p><p>To help disentangle the importance of these factors from other modeling choices (e.g., the pretraining objective), we begin by training RoBERTa following the BERT LARGE architecture (L = 24, H = 1024, A = 16, 355M parameters). We pretrain for 100K steps over a comparable BOOKCORPUS plus WIKIPEDIA dataset as was used in <ref type="bibr" target="#b11">Devlin et al. (2019)</ref>. We pretrain our model using 1024 V100 GPUs, which takes approximately one day per 100K steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We present our results in Table <ref type="table" target="#tab_3">3</ref>. When controlling for training data, we observe that RoBERTa provides a large improvement over the originally reported BERT LARGE results, reaffirming the importance of the design choices we explored in Section 4.</p><p>Next, we combine this data with the three additional datasets described in Section 3.2. We train RoBERTa over the combined data with the same number of training steps as before (100K). In total, we pretrain over 160GB of text. We observe further improvements in performance across all downstream tasks, validating the importance of data size and diversity in pretraining.<ref type="foot" target="#foot_8">9</ref> </p><p>Finally, we pretrain RoBERTa for significantly longer, increasing the number of pretraining steps from 100K to 300K, and then further to 500K. We again observe significant gains in downstream task performance, and the 300K and 500K step models outperform XLNet LARGE across most tasks. We note that even our longest-trained model does not appear to overfit our data and would likely benefit from additional training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">GLUE RESULTS</head><p>For GLUE, we consider two finetuning settings. In the first setting (single-task, dev), we finetune RoBERTa separately for each of the GLUE tasks, using only the training data for the corresponding task. We consider a limited hyperparameter sweep with batch sizes ∈ {16, 32} and learning rates ∈ {1e-5, 2e-5, 3e-5}, with a linear warmup for the first 6% of steps followed by a linear decay to 0. We finetune for 10 epochs with early stopping based on each task's dev set. The rest of the hyperparameters remain the same as during pretraining. In this setting, we report the median development set results for each task over five random initializations, without model ensembling.</p><p>In the second setting (ensembles, test), we compare RoBERTa to other approaches on the test set via the GLUE leaderboard. While many submissions to the GLUE leaderboard depend on multi-task  <ref type="formula">2019</ref>) and <ref type="bibr" target="#b56">Yang et al. (2019)</ref>, respectively. finetuning, our submission depends only on single-task finetuning. For RTE, STS and MRPC we finetune starting from the MNLI single-task model, following <ref type="bibr" target="#b36">Phang et al. (2018)</ref>. We explore a slightly wider hyperparameter space, described in Appendix C, and ensemble between 5 and 7 models per task. Two of the GLUE tasks require task-specific finetuning approaches to achieve competitive leaderboard results; these approaches are described in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We present our results in Table <ref type="table" target="#tab_4">4</ref>. In the first setting (single-task, dev), RoBERTa achieves state-of-the-art results on all 9 of the GLUE task development sets. Crucially, RoBERTa uses the same masked language modeling pretraining objective and architecture as BERT LARGE , yet consistently outperforms both BERT LARGE and XLNet LARGE . This raises questions about the relative importance of model architecture and pretraining objective, compared to more mundane details like dataset size and training time that we explore in this work. A more comprehensive comparison of the BERT and XLNet pretraining objectives is needed, but is left to future work.</p><p>In the second setting (ensembles, test), we submit RoBERTa to the GLUE leaderboard and achieve state-of-the-art results on 4 out of 9 tasks and the highest average score to date. Notably, RoBERTa does not depend on multi-task finetuning, and we expect future work may further improve these results by incorporating more sophisticated multi-task finetuning procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SQUAD RESULTS</head><p>We adopt a much simpler approach for SQuAD compared to past work. While BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> and XLNet <ref type="bibr" target="#b56">(Yang et al., 2019)</ref> augment their training data with additional QA datasets, we only finetune RoBERTa using the provided SQuAD training data. We also use a single learning rate for all layers, in contrast to the custom layer-wise learning rate scheduled used by <ref type="bibr" target="#b56">Yang et al. (2019)</ref>.</p><p>For SQuAD v1.1 we follow the same finetuning procedure as <ref type="bibr" target="#b11">Devlin et al. (2019)</ref>. For SQuAD v2.0, we additionally classify whether a given question is answerable; we train this classifier jointly with the span predictor by summing the classification and span loss terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Accuracy </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We present our results in Table <ref type="table" target="#tab_5">5</ref>. On the SQuAD v1.1 development set, RoBERTa matches the state-of-the-art set by XLNet. On the SQuAD v2.0 development set, RoBERTa sets a new state-of-the-art, improving over XLNet by 0.4 points (EM) and 0.6 points (F1).</p><p>We also submit RoBERTa to the public SQuAD 2.0 leaderboard. Most of the top systems build upon either BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> or XLNet <ref type="bibr" target="#b56">(Yang et al., 2019)</ref> and therefore rely on additional external training data. Our single RoBERTa model outperforms all but one of the single model submissions, and is the top scoring system among those that do not rely on additional external data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">RACE RESULTS</head><p>In RACE, systems are provided with a passage of text, an associated question, and must classify which of four candidate answers is correct. We modify RoBERTa for this task by concatenating each candidate answer with the corresponding question and passage. We encode each of these four sequences and pass the resulting [CLS] representations through a fully-connected layer, which is used to predict the correct answer. We truncate question-answer pairs that are longer than 128 tokens and, if needed, the passage so that the total length is at most 512 tokens.</p><p>Results are presented in Table <ref type="table" target="#tab_6">6</ref>. RoBERTa achieves state-of-the-art accuracy across all settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Pretraining methods have been designed with different training objectives, including language modeling <ref type="bibr" target="#b9">(Dai &amp; Le, 2015;</ref><ref type="bibr" target="#b35">Peters et al., 2018;</ref><ref type="bibr" target="#b19">Howard &amp; Ruder, 2018)</ref>, machine translation <ref type="bibr" target="#b30">(McCann et al., 2017)</ref>, and masked language modeling <ref type="bibr" target="#b11">(Devlin et al., 2019;</ref><ref type="bibr" target="#b25">Lample &amp; Conneau, 2019)</ref>. Many recent papers have used a basic recipe of finetuning models for each end task <ref type="bibr" target="#b19">(Howard &amp; Ruder, 2018;</ref><ref type="bibr" target="#b39">Radford et al., 2018)</ref>, and pretraining with some variant of a masked language model objective. However, newer methods have improved performance by multi-task fine tuning <ref type="bibr" target="#b13">(Dong et al., 2019)</ref>, incorporating entity embeddings <ref type="bibr" target="#b48">(Sun et al., 2019)</ref>, span prediction <ref type="bibr" target="#b21">(Joshi et al., 2019)</ref>, and multiple variants of autoregressive pretraining <ref type="bibr" target="#b47">(Song et al., 2019;</ref><ref type="bibr" target="#b5">Chan et al., 2019;</ref><ref type="bibr" target="#b56">Yang et al., 2019)</ref>. Performance is also typically improved by training bigger models on more data <ref type="bibr" target="#b11">(Devlin et al., 2019;</ref><ref type="bibr" target="#b1">Baevski et al., 2019;</ref><ref type="bibr" target="#b56">Yang et al., 2019;</ref><ref type="bibr" target="#b40">Radford et al., 2019)</ref>. Our goal was to replicate, simplify, and better tune the training of BERT, as a reference point for better understanding the relative performance of all of these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>We evaluate a number of design decisions when pretraining BERT models, demonstrating that performance can be substantially improved by training the model longer, with bigger batches over more data; removing the next sentence prediction objective; training on longer sequences; and dynamically changing the masking pattern applied to the training data. We additionally use a novel dataset, CC-NEWS, and release our models and code for pretraining and finetuning at: anonymous URL.</p><p>Our improved pretraining procedure, which we call RoBERTa, achieves state-of-the-art results on GLUE, RACE, SQuAD, SuperGLUE and XNLI. These results illustrate the importance of these previously overlooked design decisions and suggest that BERT's pretraining objective remains competitive with recently proposed alternatives.</p><p>A STATIC VS. DYNAMIC MASKING  <ref type="bibr" target="#b56">Yang et al. (2019)</ref>. We find that our reimplementation with static masking performs similar to the original BERT model, and dynamic masking is comparable or slightly better than static masking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B TASK-SPECIFIC MODIFICATIONS FOR GLUE</head><p>Two of the GLUE tasks require task-specific finetuning approaches to achieve competitive leaderboard results:</p><p>QNLI Recent submissions on the GLUE leaderboard adopt a pairwise ranking formulation for the QNLI task, in which candidate answers are mined from the training set and compared to one another, and a single (question, candidate) pair is classified as positive <ref type="bibr" target="#b28">(Liu et al., 2019b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b56">Yang et al., 2019)</ref>. This formulation significantly simplifies the task, but is not directly comparable to BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>. Following recent work, we adopt the ranking approach for our test submission, but for direct comparison with BERT all reported development set results are based on a pure classification approach.</p><p>WNLI We found the provided NLI-format data to be challenging to work with. Instead we use the reformatted WNLI data from SuperGLUE <ref type="bibr" target="#b51">(Wang et al., 2019a)</ref>, which indicates the span of the query pronoun and referent. We then finetune RoBERTa using a variation of the approach from <ref type="bibr" target="#b23">Kocijan et al. (2019)</ref>. In particular, for a given input sentence, we first use spaCy <ref type="bibr" target="#b18">(Honnibal &amp; Montani, 2017)</ref> to extract additional candidate noun phrases from the sentence, and then finetune our model so that it assigns higher scores to positive referent phrases than for any of the generated negative candidate phrases.</p><p>In contrast to <ref type="bibr" target="#b23">Kocijan et al. (2019)</ref>, who finetune BERT using a margin ranking loss between (query, candidate) pairs, we instead use a single cross entropy loss term over the log-probabilities for the query and all mined candidates. This reduces the number of hyperparameters that need to be tuned and in practice produces more stable results on the development set. Our best model achieved 92.3% development set accuracy, compared to 90.2% accuracy for the margin loss approach.</p><p>One unfortunate consequence of our overall approach is that we can only make use of the  <ref type="bibr" target="#b8">(Dagan et al., 2006;</ref><ref type="bibr" target="#b2">Bar Haim et al., 2006;</ref><ref type="bibr" target="#b14">Giampiccolo et al., 2007;</ref><ref type="bibr" target="#b3">Bentivogli et al., 2009)</ref> and Winograd NLI (WNLI) <ref type="bibr" target="#b26">(Levesque et al., 2011)</ref>. We also evaluate RoBERTa on the SuperGLUE benchmark <ref type="bibr" target="#b51">(Wang et al., 2019a)</ref>, which consists of 8 natural language understanding tasks. <ref type="foot" target="#foot_11">12</ref> We largely follow the same setup for SuperGLUE as we did for GLUE, with several task-specific modifications:</p><p>• BoolQ and MultiRC: we follow the same input format as the <ref type="bibr" target="#b51">Wang et al. (2019a)</ref> baseline.</p><p>• CB: we finetune starting from the MNLI model, following <ref type="bibr" target="#b36">Phang et al. (2018)</ref>.</p><p>• COPA: we concatenate the premise and each alternative with because and so markers for cause and effect questions, respectively. This input format more closely matches the pretraining data format and provides better results in practice. • ReCoRD: during training we adopt a pairwise ranking formulation with one negative and positive entity for each (passage, query) pair. At evaluation time, we pick the entity with the highest score for each question. • WiC: we input the pair of sentences as normal. We then feed the concatenation of the representations of the two marked words and the [CLS] token to the classification layer. • RTE and WSC: we reused our submission to the GLUE leaderboard.</p><p>In Table <ref type="table" target="#tab_11">11</ref>  Table <ref type="table" target="#tab_1">12</ref>: Results on XNLI <ref type="bibr" target="#b7">(Conneau et al., 2018)</ref> for RoBERTa LARGE in the TRANSLATE-TEST setting. We report macro-averaged accuracy (∆) using the provided English translations of the XNLI test sets. RoBERTa achieves state of the art results on all 15 languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Prediction (NSP) is a binary classification loss for predicting whether two segments follow each other in the original text. Positive examples are created by taking consecutive sentences from the text corpus. Negative examples are created by pairing segments from different documents. Positive and negative examples are sampled with equal probability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Development set results for base models pretrained over BOOKCORPUS and WIKIPEDIA. All models are trained for 1M steps with a batch size of 256 sequences. We report F1 for SQuAD and accuracy for MNLI-m, SST-2 and RACE. Reported results are medians over five random initializations (seeds). Results for BERT BASE and XLNet BASE are from<ref type="bibr" target="#b56">Yang et al. (2019)</ref>.</figDesc><table><row><cell>Model</cell><cell cols="4">SQuAD 1.1/2.0 MNLI-m SST-2 RACE</cell></row><row><cell cols="2">Our reimplementation (with NSP loss):</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SEGMENT-PAIR</cell><cell>90.4/78.7</cell><cell>84.0</cell><cell>92.9</cell><cell>64.2</cell></row><row><cell>SENTENCE-PAIR</cell><cell>88.7/76.2</cell><cell>82.9</cell><cell>92.1</cell><cell>63.0</cell></row><row><cell cols="2">Our reimplementation (without NSP loss):</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FULL-SENTENCES</cell><cell>90.4/79.1</cell><cell>84.7</cell><cell>92.5</cell><cell>64.8</cell></row><row><cell>DOC-SENTENCES</cell><cell>90.6/79.7</cell><cell>84.7</cell><cell>92.7</cell><cell>65.6</cell></row><row><cell>BERT BASE</cell><cell>88.5/76.3</cell><cell>84.3</cell><cell>92.8</cell><cell>64.3</cell></row><row><cell>XLNet BASE (K = 7)</cell><cell>-/81.3</cell><cell>85.8</cell><cell>92.7</cell><cell>66.1</cell></row><row><cell>XLNet BASE (K = 6)</cell><cell>-/81.0</cell><cell>85.6</cell><cell>93.4</cell><cell>66.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Perplexity on held-out validation data and dev set accuracy on MNLI-m and SST-2 for various batch sizes (# sequences) as we vary the number of passes (epochs) through the BOOKS + WIKI data. Reported results are medians over five random initializations (seeds). The learning rate is tuned for each batch size. All results are for BERT BASE with FULL-SENTENCE inputs.</figDesc><table><row><cell cols="7">batch size learning rate epochs steps perplexity MNLI-m SST-2</cell></row><row><cell>256</cell><cell>1e-4</cell><cell>32</cell><cell>1M</cell><cell>3.99</cell><cell>84.7</cell><cell>92.5</cell></row><row><cell></cell><cell></cell><cell>32</cell><cell>125K</cell><cell>3.68</cell><cell>85.2</cell><cell>93.1</cell></row><row><cell>2K</cell><cell>7e-4</cell><cell>64</cell><cell>250K</cell><cell>3.59</cell><cell>85.3</cell><cell>94.1</cell></row><row><cell></cell><cell></cell><cell>128</cell><cell>500K</cell><cell>3.51</cell><cell>85.4</cell><cell>93.5</cell></row><row><cell></cell><cell></cell><cell>32</cell><cell>31K</cell><cell>3.77</cell><cell>84.4</cell><cell>93.2</cell></row><row><cell>8K</cell><cell>1e-3</cell><cell>64</cell><cell>63K</cell><cell>3.60</cell><cell>85.3</cell><cell>93.5</cell></row><row><cell></cell><cell></cell><cell>128</cell><cell>125K</cell><cell>3.50</cell><cell>85.8</cell><cell>94.1</cell></row></table><note>). However, because the DOC-SENTENCES format results in variable batch sizes, we use FULL-SENTENCES in the remainder of our experiments for easier comparison with related work.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>used a character-level BPE vocabulary of size 30K. We instead adopt the larger byte-level BPE vocabulary of size 50K introduced in Radford et al. (2019), which uses bytes rather than unicode characters as the base subword units and can therefore encode any input text without introducing "unknown" tokens. This adds approximately 15M and 20M extra parameters for BERT BASE and BERT LARGE , respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Development set results for RoBERTa as we pretrain over more data (16GB → 160GB of text) and pretrain for longer (100K → 300K → 500K steps). Each row accumulates improvements from the rows above. RoBERTa matches the architecture and training objective of BERT LARGE . Results for BERT LARGE and XLNet LARGE are from Devlin et al. (2019) and Yang et al. (2019), respectively. Complete results on all GLUE tasks can be found in Appendix C.</figDesc><table><row><cell>Model</cell><cell>data</cell><cell cols="5">batch size steps SQuAD (v1.1/2.0) MNLI-m SST-2</cell></row><row><cell>RoBERTa</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>with BOOKS + WIKI</cell><cell>16GB</cell><cell>8K</cell><cell>100K</cell><cell>93.6/87.3</cell><cell>89.0</cell><cell>95.3</cell></row><row><cell cols="2">+ additional data ( §3.2) 160GB</cell><cell>8K</cell><cell>100K</cell><cell>94.0/87.7</cell><cell>89.3</cell><cell>95.6</cell></row><row><cell>+ pretrain longer</cell><cell>160GB</cell><cell>8K</cell><cell>300K</cell><cell>94.4/88.7</cell><cell>90.0</cell><cell>96.1</cell></row><row><cell>+ pretrain even longer</cell><cell>160GB</cell><cell>8K</cell><cell>500K</cell><cell>94.6/89.4</cell><cell>90.2</cell><cell>96.4</cell></row><row><cell>BERTLARGE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>with BOOKS + WIKI</cell><cell>13GB</cell><cell>256</cell><cell>1M</cell><cell>90.9/81.8</cell><cell>86.6</cell><cell>93.7</cell></row><row><cell>XLNetLARGE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>with BOOKS + WIKI</cell><cell>13GB</cell><cell>256</cell><cell>1M</cell><cell>94.0/87.8</cell><cell>88.4</cell><cell>94.4</cell></row><row><cell>+ additional data</cell><cell>126GB</cell><cell>2K</cell><cell>500K</cell><cell>94.5/88.8</cell><cell>89.8</cell><cell>95.6</cell></row></table><note>Additionally, we investigate two other important factors that have been under-emphasized in previous work: (1) the data used for pretraining, and (2) the number of training passes through the data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on GLUE. All results are based on a 24-layer architecture. BERT LARGE and XLNet LARGE results are from Devlin et al. (2019) and Yang et al. (2019), respectively. RoBERTa results on the dev set are a median over five runs. RoBERTa results on the test set are ensembles of single-task models. For RTE, STS and MRPC we finetune starting from the MNLI model.</figDesc><table><row><cell></cell><cell>MNLI</cell><cell cols="9">QNLI QQP RTE SST MRPC CoLA STS WNLI Avg</cell></row><row><cell cols="3">Single-task single models on dev</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERTLARGE</cell><cell>86.6/-</cell><cell>92.3</cell><cell>91.3</cell><cell cols="2">70.4 93.2</cell><cell>88.0</cell><cell>60.6</cell><cell>90.0</cell><cell>-</cell><cell>-</cell></row><row><cell>XLNetLARGE</cell><cell>89.8/-</cell><cell>93.9</cell><cell>91.8</cell><cell cols="2">83.8 95.6</cell><cell>89.2</cell><cell>63.6</cell><cell>91.8</cell><cell>-</cell><cell>-</cell></row><row><cell>RoBERTa</cell><cell>90.2/90.2</cell><cell>94.7</cell><cell>92.2</cell><cell cols="2">86.6 96.4</cell><cell>90.9</cell><cell>68.0</cell><cell>92.4</cell><cell>91.3</cell><cell>-</cell></row><row><cell cols="5">Ensembles on test (from leaderboard as of July 25, 2019)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ALICE</cell><cell>88.2/87.9</cell><cell>95.7</cell><cell>90.7</cell><cell cols="2">83.5 95.2</cell><cell>92.6</cell><cell>68.6</cell><cell>91.1</cell><cell>80.8</cell><cell>86.3</cell></row><row><cell>MT-DNN</cell><cell>87.9/87.4</cell><cell>96.0</cell><cell>89.9</cell><cell cols="2">86.3 96.5</cell><cell>92.7</cell><cell>68.4</cell><cell>91.1</cell><cell>89.0</cell><cell>87.6</cell></row><row><cell>XLNet</cell><cell>90.2/89.8</cell><cell>98.6</cell><cell>90.3</cell><cell cols="2">86.3 96.8</cell><cell>93.0</cell><cell>67.8</cell><cell>91.6</cell><cell>90.4</cell><cell>88.4</cell></row><row><cell>RoBERTa</cell><cell>90.8/90.2</cell><cell>98.9</cell><cell>90.2</cell><cell cols="2">88.2 96.7</cell><cell>92.3</cell><cell>67.8</cell><cell>92.2</cell><cell>89.0</cell><cell>88.5</cell></row><row><cell>Model</cell><cell cols="2">SQuAD 1.1 EM F1</cell><cell cols="2">SQuAD 2.0 EM F1</cell><cell cols="2">Model</cell><cell></cell><cell></cell><cell cols="2">SQuAD 2.0 EM F1</cell></row><row><cell cols="5">Single models on dev, w/o data augmentation</cell><cell cols="6">Single models on test (as of July 25, 2019)</cell></row><row><cell>BERT LARGE</cell><cell cols="3">84.1 90.9 79.0</cell><cell>81.8</cell><cell cols="2">XLNet LARGE</cell><cell></cell><cell></cell><cell cols="2">86.3  † 89.1  †</cell></row><row><cell cols="4">XLNet LARGE 89.0 94.5 86.1</cell><cell>88.8</cell><cell cols="2">RoBERTa</cell><cell></cell><cell></cell><cell>86.8</cell><cell>89.8</cell></row><row><cell>RoBERTa</cell><cell cols="3">88.9 94.6 86.5</cell><cell>89.4</cell><cell cols="6">XLNet + SG-Net Verifier 87.0  † 89.9  †</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results on SQuAD. † indicates results that depend on additional external training data. RoBERTa uses only the provided SQuAD data in both dev and test settings. BERT LARGE and XLNet LARGE results are from Devlin et al. (</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Middle High Results on the RACE test set. BERT LARGE and XLNet LARGE results from Yang et al. (2019).</figDesc><table><row><cell cols="4">Single models on test (as of July 25, 2019)</cell></row><row><cell>BERT LARGE</cell><cell>72.0</cell><cell>76.6</cell><cell>70.1</cell></row><row><cell>XLNet LARGE</cell><cell>81.7</cell><cell>85.4</cell><cell>80.2</cell></row><row><cell>RoBERTa</cell><cell>83.2</cell><cell>86.5</cell><cell>81.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparison between the published BERT BASE results from<ref type="bibr" target="#b11">Devlin et al. (2019)</ref> to our reimplementation with either static or dynamic masking. We report F1 for SQuAD and accuracy for MNLI-m and SST-2. Reported results are medians over 5 random initializations (seeds). Reference results are from</figDesc><table><row><cell cols="4">Masking SQuAD 2.0 MNLI-m SST-2</cell></row><row><cell>reference</cell><cell>76.3</cell><cell>84.3</cell><cell>92.8</cell></row><row><cell cols="2">Our reimplementation:</cell><cell></cell><cell></cell></row><row><cell>static</cell><cell>78.3</cell><cell>84.3</cell><cell>92.5</cell></row><row><cell>dynamic</cell><cell>78.7</cell><cell>84.0</cell><cell>92.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>positive training examples, which excludes over half of the provided training data. 10C FULL RESULTS ON GLUEIn Table8we present the full set of development set results for RoBERTa on all 9 GLUE datasets.11  We present results for a LARGE configuration with 355M parameters that follows BERT LARGE , as well as a BASE configuration with 125M parameters that follows BERT BASE . Development set results on GLUE tasks for various configurations of RoBERTa. All results are a median over five runs.</figDesc><table><row><cell cols="8">MNLI QNLI QQP RTE SST MRPC CoLA STS</cell></row><row><cell>RoBERTa BASE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+ all data + 500k steps</cell><cell>87.6</cell><cell>92.8</cell><cell>91.9</cell><cell>78.7 94.8</cell><cell>90.2</cell><cell>63.6</cell><cell>91.2</cell></row><row><cell>RoBERTa LARGE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>with BOOKS + WIKI</cell><cell>89.0</cell><cell>93.9</cell><cell>91.9</cell><cell>84.5 95.3</cell><cell>90.2</cell><cell>66.3</cell><cell>91.6</cell></row><row><cell>+ additional data ( §3.2)</cell><cell>89.3</cell><cell>94.0</cell><cell>92.0</cell><cell>82.7 95.6</cell><cell>91.4</cell><cell>66.1</cell><cell>92.2</cell></row><row><cell>+ pretrain longer 300k</cell><cell>90.0</cell><cell>94.5</cell><cell>92.2</cell><cell>83.3 96.1</cell><cell>91.1</cell><cell>67.4</cell><cell>92.3</cell></row><row><cell>+ pretrain longer 500k</cell><cell>90.2</cell><cell>94.7</cell><cell>92.2</cell><cell>86.6 96.4</cell><cell>90.9</cell><cell>68.0</cell><cell>92.4</cell></row><row><cell cols="4">D PRETRAINING HYPERPARAMETERS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Hyperparam</cell><cell cols="4">RoBERTa LARGE RoBERTa BASE</cell><cell></cell><cell></cell></row><row><cell cols="2">Number of Layers</cell><cell></cell><cell>24</cell><cell>12</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Hidden size</cell><cell></cell><cell>1024</cell><cell>768</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">FFN inner hidden size</cell><cell>4096</cell><cell>3072</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Attention heads</cell><cell></cell><cell>16</cell><cell>12</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Attention head size</cell><cell></cell><cell>64</cell><cell>64</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dropout</cell><cell></cell><cell></cell><cell>0.1</cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Attention Dropout</cell><cell></cell><cell>0.1</cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Warmup Steps</cell><cell></cell><cell>24k</cell><cell>24k</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Peak Learning Rate</cell><cell></cell><cell>4e-4</cell><cell>6e-4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Batch Size</cell><cell></cell><cell></cell><cell>8k</cell><cell>8k</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Weight Decay</cell><cell></cell><cell>0.01</cell><cell>0.01</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Max Steps</cell><cell></cell><cell></cell><cell>500k</cell><cell>500k</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Learning Rate Decay</cell><cell>Linear</cell><cell cols="2">Linear</cell><cell></cell><cell></cell></row><row><cell>Adam</cell><cell></cell><cell></cell><cell>1e-6</cell><cell>1e-6</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Adam β 1</cell><cell></cell><cell></cell><cell>0.9</cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Adam β 2</cell><cell></cell><cell></cell><cell>0.98</cell><cell>0.98</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Gradient Clipping</cell><cell></cell><cell>0.0</cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Hyperparameters for pretraining RoBERTa LARGE and RoBERTa BASE .</figDesc><table><row><cell cols="3">E FINETUNING HYPERPARAMETERS</cell><cell></cell><cell></cell></row><row><cell>Hyperparam</cell><cell cols="2">RACE SQuAD</cell><cell>GLUE</cell><cell>SuperGLUE</cell></row><row><cell>Learning Rate</cell><cell>1e-5</cell><cell>1.5e-5</cell><cell cols="2">{1e-5, 2e-5, 3e-5} {1e-5, 2e-5, 3e-5}</cell></row><row><cell>Batch Size</cell><cell>16</cell><cell>48</cell><cell>{16, 32}</cell><cell>32</cell></row><row><cell>Weight Decay</cell><cell>0.1</cell><cell>0.01</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>Max Epochs</cell><cell>4</cell><cell>2</cell><cell>10</cell><cell>{10, 50}</cell></row><row><cell cols="2">Learning Rate Decay Linear</cell><cell>Linear</cell><cell>Linear</cell><cell>Linear</cell></row><row><cell>Warmup ratio</cell><cell>0.06</cell><cell>0.06</cell><cell>0.06</cell><cell>0.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Hyperparameters for finetuning RoBERTa LARGE on RACE, SQuAD and GLUE. We select the best hyperparameter values based on the median of 5 random seeds for each task.</figDesc><table /><note>ment (RTE)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>F RESULTS ON SUPERGLUE Results on SuperGLUE. All results are based on a 24-layer architecture. RoBERTa results on the development set are a median over five runs. RoBERTa results on the test set are ensembles of single-task models. Averages are obtained from the SuperGLUE leaderboard.</figDesc><table><row><cell></cell><cell>BoolQ</cell><cell>CB</cell><cell cols="6">COPA MultiRC ReCoRD RTE WiC WSC Avg</cell></row><row><cell cols="3">Single-task single models on dev</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT++</cell><cell>80.1</cell><cell>96.4/95.0</cell><cell>78.0</cell><cell cols="2">70.7/24.7 70.6/69.8 82.3</cell><cell>74.9</cell><cell>68.3</cell><cell>74.6</cell></row><row><cell>RoBERTa</cell><cell>86.9</cell><cell>98.2/-</cell><cell>94.0</cell><cell>85.7/-</cell><cell>89.5/89.0 86.6</cell><cell>75.6</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">Ensembles on test (from leaderboard as of August 12, 2019)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT</cell><cell>77.4</cell><cell>75.7/83.6</cell><cell>70.6</cell><cell cols="2">70.0/24.1 72.0/71.3 71.7</cell><cell>69.6</cell><cell>64.4</cell><cell>69.0</cell></row><row><cell>BERT++</cell><cell>79.0</cell><cell>84.8/90.4</cell><cell>73.8</cell><cell cols="2">70.0/24.1 72.0/71.3 79.0</cell><cell>69.6</cell><cell>64.4</cell><cell>71.5</cell></row><row><cell>Outside Best</cell><cell>80.4</cell><cell>-</cell><cell>84.4</cell><cell cols="2">70.4/24.5 74.8/73.0 82.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RoBERTa</cell><cell>87.1</cell><cell>90.5/95.2</cell><cell>90.6</cell><cell cols="2">84.4/52.5 90.6/90.0 88.2</cell><cell>69.9</cell><cell>89.0</cell><cell>84.6</cell></row><row><cell>Human (est.)</cell><cell>89.0</cell><cell>95.8/98.9</cell><cell>100.0</cell><cell cols="2">81.8/51.9 91.7/91.3 93.6</cell><cell cols="3">80.0 100.0 89.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>we present RoBERTa results on the 8 SuperGLUE datasets. RoBERTa achieves stateof-the-art results on the development and test sets for BoolQ, CB, COPA, MultiRC and ReCoRD and the highest average score to date on the SuperGLUE leaderboard. Machine translation baselines (TRANSLATE-TEST) XLM (MLM+TLM) 85.0 79.0 79.5 78.1 77.8 77.6 75.5 73.7 73.7 70.8 70.4 73.6 69.0 64.7 65.1 74.2 XLM-en 88.8 81.4 82.3 80.1 80.3 80.9 76.2 76.0 75.4 72.0 71.9 75.6 70.0 65.8 65.8 76.2 RoBERTa 91.3 82.9 84.3 81.2 81.7 83.1 78.3 76.8 76.6 74.2 74.0 77.5 70.9 66.6 66.8 77.8</figDesc><table><row><cell cols="3">G RESULTS ON XNLI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>en</cell><cell>fr</cell><cell>es</cell><cell>de</cell><cell>el</cell><cell>bg</cell><cell>ru</cell><cell>tr</cell><cell>ar</cell><cell>vi</cell><cell>th</cell><cell>zh</cell><cell>hi</cell><cell>sw</cell><cell>ur</cell><cell>∆</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Our models and code are available at: anonymous URL.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">These other methods could possibly improve with more tuning as well; we leave this to future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><ref type="bibr" target="#b56">Yang et al. (2019)</ref> use the same dataset but report having only 13GB of text after data cleaning. This is most likely due to subtle differences in the underlying data collection or preprocessing.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">We use news-please<ref type="bibr" target="#b16">(Hamborg et al., 2017)</ref> to collect and extract CC-NEWS. CC-NEWS is similar to the REALNEWS dataset described in<ref type="bibr" target="#b57">Zellers et al. (2019)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">The authors and their affiliated institutions are not affiliated with the creation of the OpenWebText dataset.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">Studying architectural changes, including larger architectures, is an important area for future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">Even without large scale parallel hardware, large batch training can improve training efficiency through gradient accumulation -i.e., accumulating gradients from multiple mini-batches before each optimization step.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><ref type="bibr" target="#b56">You et al. (2019)</ref> train BERT with even larger batch sizes, up to 32K sequences. We leave further exploration of the limits of large batch training to future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8">A more careful analysis disentangling data size and diversity is needed, but is left to future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9">While we only use the provided WNLI training data, our results could potentially be improved by augmenting this with additional pronoun disambiguation datasets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10">  11  The GLUE datasets are: CoLA<ref type="bibr" target="#b53">(Warstadt et al., 2018)</ref>, Stanford Sentiment Treebank (SST)<ref type="bibr" target="#b46">(Socher et al., 2013)</ref>, Microsoft Research Paragraph Corpus (MRPC)<ref type="bibr" target="#b12">(Dolan &amp; Brockett, 2005)</ref>, Semantic Textual Similarity Benchmark (STS)(Agirre et al., 2007), Quora Question Pairs (QQP)<ref type="bibr" target="#b20">(Iyer et al., 2016)</ref>, Multi-Genre NLI (MNLI)<ref type="bibr" target="#b54">(Williams et al., 2018)</ref>, Question NLI (QNLI)<ref type="bibr" target="#b41">(Rajpurkar et al., 2016)</ref>, Recognizing Textual Entail-</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11">The SuperGLUE datasets are: BoolQ<ref type="bibr" target="#b6">(Clark et al., 2019)</ref>, CommitmentBank (CB)<ref type="bibr" target="#b10">(De Marneffe et al., 2019)</ref>, Choice of Plausible Alternatives (COPA)<ref type="bibr" target="#b43">(Roemmele et al., 2011)</ref>, Multi-Sentence Reading Comprehension (MultiRC)<ref type="bibr" target="#b22">(Khashabi et al., 2018)</ref>, Reading Comprehension with Commonsense Reasoning (ReCoRD)<ref type="bibr" target="#b58">(Zhang et al., 2018)</ref>, Recognizing Textual Entailment (RTE)<ref type="bibr" target="#b8">(Dagan et al., 2006;</ref><ref type="bibr" target="#b2">Bar Haim et al., 2006;</ref><ref type="bibr" target="#b14">Giampiccolo et al., 2007;</ref><ref type="bibr" target="#b3">Bentivogli et al., 2009)</ref>, Words in Context (WiC)<ref type="bibr" target="#b37">(Pilehvar &amp; Camacho- Collados, 2019)</ref>, the Winograd Schema Challenge (WSC)<ref type="bibr" target="#b26">(Levesque et al., 2011)</ref>, and Winogender Schema Diagnostics<ref type="bibr" target="#b38">(Poliak et al., 2018;</ref><ref type="bibr" target="#b44">Rudinger et al., 2018)</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007)</title>
				<editor>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lluís</forename><surname>Màrquez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Richard</forename><surname>Wicentowski</surname></persName>
		</editor>
		<meeting>the Fourth International Workshop on Semantic Evaluations (SemEval-2007)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Cloze-driven pretraining of self-attention networks</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07785</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The second PASCAL recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Roy</forename><surname>Bar Haim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The fifth PASCAL recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoa</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Trang</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">KERMIT: Generative insertion-based modeling for sequences</title>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01604</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BoolQ: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019</title>
				<meeting>NAACL-HLT 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Xnli: Evaluating cross-lingual sentence representations</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The PASCAL recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The CommitmentBank: Investigating projection in naturally occurring discourse</title>
		<author>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandy</forename><surname>Simons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><surname>Tonhauser</surname></persName>
		</author>
		<ptr target="https://github.com/mcdm/CommitmentBank/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>To appear in proceedings of Sinn und Bedeutung 23. Data can be</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Paraphrasing</title>
				<meeting>the International Workshop on Paraphrasing</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03197</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The third PASCAL recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing</title>
				<meeting>the ACL-PASCAL workshop on textual entailment and paraphrasing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanya</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="http://web.archive.org/save/http://Skylion007.github.io/OpenWebTextCorpus" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Openwebtext corpus</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">news-please: A generic news crawler and extractor</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hamborg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><surname>Meuschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Breitinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bela</forename><surname>Gipp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Symposium of Information Science</title>
				<meeting>the 15th International Symposium of Information Science</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ines</forename><surname>Montani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">First quora dataset release: Question pairs</title>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Dandekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kornl</forename><surname>Csernai</surname></persName>
		</author>
		<ptr target="https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">SpanBERT: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10529</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Looking beyond the surface: A challenge set for reading comprehension over multiple sentences</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015">2018. 2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="252" to="262" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations (ICLR)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A surprisingly robust trick for winograd schema challenge</title>
		<author>
			<persName><forename type="first">Ana-Maria</forename><surname>Vid Kocijan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oana-Maria</forename><surname>Cretu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yordan</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Yordanov</surname></persName>
		</author>
		<author>
			<persName><surname>Lukasiewicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06290</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Race: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04683</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07291</idno>
		<title level="m">Cross-lingual language model pretraining</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Winograd schema challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improving multi-task deep neural networks via knowledge distillation for natural language understanding</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09482</idno>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11504</idno>
		<title level="m">Multi-task deep neural networks for natural language understanding</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Bkg6RiCqY7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6297" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mixed precision training</title>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Nagel</surname></persName>
		</author>
		<ptr target="http://web.archive.org/save/http://commoncrawl.org/2016/10/news-dataset-available" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scaling neural machine translation</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation (WMT)</title>
				<meeting>the Third Conference on Machine Translation (WMT)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">FAIRSEQ: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>System Demonstrations</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Fvry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01088</idno>
		<title level="m">Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">WiC: The word-in-context dataset for evaluating context-sensitive meaning representations</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pilehvar</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Collecting diverse natural language inference problems for sentence representation evaluation</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aparajita</forename><surname>Haldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Edward</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Steven White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Improving language understanding with unsupervised learning</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Time Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>OpenAI</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cosmin</forename><surname>Adrian Bejan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 AAAI Spring Symposium Series</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">MASS: Masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danxiang</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<title level="m">ERNIE: Enhanced representation through knowledge integration</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A simple method for commonsense reasoning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02847</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">SuperGLUE: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>arXiv preprint 1905.00537</idno>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>1805.12471</idno>
		<title level="m">Neural network acceptability judgments</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">North American Association for Computational Linguistics (NAACL)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Xlnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Generalized autoregressive pretraining for language understanding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00962</idno>
		<title level="m">Reducing bert pre-training time from 3 days to 76 minutes</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12616</idno>
		<title level="m">Defending against neural fake news</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno>arXiv preprint 1810.12885</idno>
		<title level="m">ReCoRD: Bridging the gap between human and machine commonsense reading comprehension</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1506.06724</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
