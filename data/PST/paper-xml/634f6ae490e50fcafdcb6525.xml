<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hidet: Task Mapping Programming Paradigm for Deep Learning Tensor Programs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yaoyao</forename><surname>Ding</surname></persName>
							<email>yaoyao@cs.toronto.edu</email>
						</author>
						<author>
							<persName><forename type="first">Cody</forename><surname>Hao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bojian</forename><surname>Zheng</surname></persName>
							<email>bojian@cs.toronto.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yizhi</forename><surname>Liu</surname></persName>
							<email>yizhiliu@amazon.com</email>
						</author>
						<author>
							<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
							<email>wangyida@amazon.com</email>
						</author>
						<author>
							<persName><forename type="first">Gennady</forename><surname>Pekhimenko</surname></persName>
							<email>pekhimenko@cs.toronto.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Amazon Web Services</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Amazon Web Services</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Amazon Web Services</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hidet: Task Mapping Programming Paradigm for Deep Learning Tensor Programs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As deep learning models nowadays are widely adopted by both cloud services and edge devices, the latency of deep learning model inferences becomes crucial to provide efficient model serving. However, it is challenging to develop efficient tensor programs for deep learning operators due to the high complexity of modern accelerators (e.g., NVIDIA GPUs and Google TPUs) and the rapidly growing number of operators.</p><p>Deep learning compilers, such as Apache TVM, adopt declarative scheduling primitives to lower the bar of developing tensor programs. However, we show that this approach is insufficient to cover state-of-the-art tensor program optimizations (e.g., double buffering). In this paper, we propose to embed the scheduling process into tensor programs and use dedicated mappings, called task mappings, to define the computation assignment and ordering directly in the tensor programs. This new approach greatly enriches the expressible optimizations by allowing developers to manipulate tensor programs at a much finer granularity (e.g., allowing program statement-level optimizations). We call the proposed method the task-mapping-oriented programming paradigm. In addition, we propose a new post-scheduling fusion optimization that allows developers to focus on scheduling each single operator and automates the fusion after scheduling. It greatly reduces the engineering efforts for operator fusion. Our proposed paradigm also constructs an efficient hardware-centric schedule space, which is agnostic to the program input size and can tune the performance of tensor programs in minutes.</p><p>With the proposed paradigm, we implement a deep learning compiler -Hidet. Extensive experiments on modern convolution and transformer models show that Hidet outperforms state-of-theart DNN inference framework, ONNX Runtime, and compiler, TVM equipped with scheduler AutoTVM and Ansor, by up to 1.48? (1.22? on average) with enriched optimizations. It also reduces the tuning time by 20? and 11? compared with AutoTVM and Ansor, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep neural networks (DNNs) <ref type="bibr" target="#b26">[27]</ref> have achieved state-of-the-art (SOTA) results in various tasks such as image recognition <ref type="bibr">[19, 26,</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tuning Time</head><p>Eng. Effort cuDNN <ref type="bibr" target="#b10">[11]</ref>   <ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>, natural language translation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref>, and autonomous driving <ref type="bibr" target="#b12">[13]</ref>. In deployment environments, these models are repeatedly executed to serve continuous user requests, named model serving. Thus, it is crucial to reduce the latency and maximize the throughput of model execution to ensure safety, save energy, and improve user experience.</p><formula xml:id="formula_0">? ? ? ? N/A ? ? ?? CUTLASS [25] ?? ?? ? ? ? ? AutoTVM [10] ? ? ? ?? ? ? ? ?? Ansor [47] ? ? ? ? ? ? ? ? Hidet (ours) ? ? ? ? ? ? ? ??</formula><p>There are two major ways to execute a DNN model. ( <ref type="formula">1</ref>) DL frameworks such as TensorFlow <ref type="bibr" target="#b0">[1]</ref>, PyTorch <ref type="bibr" target="#b31">[32]</ref> and ONNX Runtime <ref type="bibr" target="#b13">[14]</ref> dispatch operators to kernel libraries such as cuDNN <ref type="bibr" target="#b10">[11]</ref>, cuBLAS <ref type="bibr" target="#b19">[20]</ref>, and CUTLASS <ref type="bibr" target="#b24">[25]</ref> during execution. <ref type="bibr" target="#b1">(2)</ref> On the other hand, deep learning compilers such as Tensorflow-XLA <ref type="bibr" target="#b34">[35]</ref> and TVM <ref type="bibr" target="#b8">[9]</ref> automatically generate kernels through a compilation process for the given operators. Various schedulers such as Ansor <ref type="bibr" target="#b46">[47]</ref> and AutoTVM <ref type="bibr" target="#b9">[10]</ref> are used to schedule the kernels during compilation to achieve high performance. Table <ref type="table" target="#tab_0">1</ref> compares different kernel libraries and schedulers in terms of the graph-and kernellevel optimizations, tuning time, and engineering efforts required to support new operators.</p><p>Kernel libraries (e.g., cuDNN <ref type="bibr" target="#b10">[11]</ref> and cuBLAS <ref type="bibr" target="#b19">[20]</ref>) provide a collection of highly optimized hand-crafted kernels (e.g., convolutions and matrix multiplications). These libraries achieve near-peak performance on widely used input sizes, as they are able to implement a large spectrum of optimizations in low-level languages (e.g., CUDA C/C++ and assembly code). However, manually tweaking a kernel to optimize for performance is laborious, error-prone, and requires expertise in writing low-level language codes. Thus, it is difficult to generalize to other input shapes, new operators, and kernel fusion patterns. In addition, template-based libraries such as arXiv:2210.09603v1 [cs.LG] 18 Oct 2022 CUTLASS <ref type="bibr" target="#b24">[25]</ref> employ C++ templates to generate tensor programs for different input shapes on the fly. Although template-based libraries can achieve competitive performance on many input shapes by dynamically tuning the optimization hyper-parameters, they do not reduce the complexity of writing tensor programs for new operators and only provide limited fusion capability (e.g., only a small number of predefined operators can be fused with matrix multiplication).</p><p>Alternatively, deep learning compilers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b48">49]</ref> are proposed to compile deep learning networks into tensor programs automatically. Existing SOTA deep learning compilers adopt the idea of decoupling computation definition and scheduling, originally proposed by Halide <ref type="bibr" target="#b33">[34]</ref>. The computation definition of an operator only defines how each element of the output tensor is computed mathematically, and the schedule defines the way the execution is performed, such as the loop order and thread binding <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34]</ref>. Compilers leverage schedulers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b46">47]</ref> to tune the hyper-parameters of the schedule to optimize operator performance for each input shape. Unlike kernel libraries and templates that target at a fixed set of operators and limited fusion patterns, compilers are capable of supporting more operators and more flexible fusion patterns automatically.</p><p>On the other hand, existing SOTA compilers are mostly based on the loop-oriented scheduling primitives, which manipulate the loop structure of a tensor program in a declarative manner (e.g., loop split, reorder, and binding). Although loop-oriented scheduling primitives have achieved great success in simplifying tensor program writing <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b46">47]</ref>, certain key optimizations (e.g., double buffering <ref type="bibr" target="#b24">[25]</ref>) are hard to implement. Specifically, loop-oriented scheduling primitives cannot cover certain fine-grained tensor program transformations that cannot be expressed as loop transformations (Section 3.1). Besides, loop-oriented scheduling also suffers from the long kernel tuning time due to the rarity of efficient schedules in the tremendous tuning spaces. For instance, AutoTVM <ref type="bibr" target="#b9">[10]</ref> takes 15 hours to tune a single CNN model Inception V3 <ref type="bibr" target="#b38">[39]</ref> on a modern GPU.</p><p>In this work, we propose a new paradigm for writing efficient tensor programs: task-mapping-oriented programming paradigm. In this paradigm, we define the parallelizable computations in an operator as tasks, and the process of assigning and ordering the tasks to parallel processing units (e.g., threads) as scheduling. The developers can directly define the scheduling in the tensor program through task mappings <ref type="foot" target="#foot_0">1</ref> . This paradigm simplifies the development of tensor programs without sacrificing the ability to express optimizations requiring fine-grained program manipulation. With the in-program scheduling, this paradigm also allows us to search the tensor program in an efficient hardware-centric schedule space that is agnostic to input size to dramatically reduce the tuning time. We also propose post-scheduling fusion to fuse the scheduled operator with surrounding operators automatically, so developers don't need to worry about fusion when writing schedule templates.</p><p>We implement a new DNN compiler called Hidet based on the proposed ideas. In this work, we mainly focus on optimizing DNN inference on GPUs, as it is the most commonly used DNN accelerator. The proposed ideas also apply to other accelerators such as CPUs and TPUs <ref type="bibr" target="#b23">[24]</ref>. Extensive experiments on modern convolutional and transformer models show that Hidet outperforms SOTA DNN inference frameworks and schedulers, AutoTVM <ref type="bibr" target="#b9">[10]</ref> and Ansor <ref type="bibr" target="#b46">[47]</ref>, by up to 1.48? (1.22? on average) while reducing the tuning time of the two schedulers by 20? and 11?, respectively.</p><p>We summarize our contributions as follows:</p><p>? We identify and present the limited expressiveness of looporiented scheduling adopted by SOTA DNN compilers, which fundamentally limits the compilers' ability to efficiently compile complex tensor programs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND 2.1 CUDA Programming Model</head><p>The CUDA programming platform <ref type="bibr" target="#b30">[31]</ref> is widely used by deep learning systems on NVIDIA GPUs. In this section, we briefly introduce the CUDA programming model on modern GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B B</head><p>Gr aphics Pr ocessing Unit (GPU)   Kernel, Thread Block, and Thread. When running a task on the GPU, thousands of threads will be executed. Each thread executes the same piece of code, called kernel code. When launching a kernel, a grid of threads will be dispatched onto the GPU as shown in Figure <ref type="figure" target="#fig_1">1</ref>. Each kernel usually comprises tens to thousands of thread blocks, while each thread block comprises tens to hundreds of threads. In the kernel code, pre-defined variables threadIdx and blockIdx, and suffix x, y, and z are used to access the 3-dimensional index of thread in a thread block and the thread block in the grid of blocks.</p><p>Hardware Implementation. Each modern GPU has tens to hundreds of streaming multiprocessors (SMs). Each SM supports scheduling up to thousands of concurrent threads <ref type="bibr" target="#b30">[31]</ref>. Threads in a thread block are partitioned into warps, and each warp contains 32 consecutive threads executing the same instructions. There are two kinds of programmable on-chip memory: shared memory and registers. Registers are privately allocated to each thread, while threads can access the same piece of shared memory allocated to their thread block. When launching a kernel, the thread blocks are dispatched to the SMs wave by wave <ref type="bibr" target="#b17">[18]</ref>. Each thread block will only be dispatched to a single SM while each SM may contain multiple thread blocks. The number of maximum resident thread blocks per SM is limited by the size of shared memory, register file, and warp scheduling units.</p><p>Operators in the deep neural network are implemented as GPU kernels. When running a neural network, we launch these kernels following an order satisfying the operator dependency. Among these operators, matrix multiplication (also known as linear or dense layer) is one of the most important operators. We next present an efficient implementation of matrix multiplication using CUDA and take it as an example throughout the paper.  This section illustrates an efficient implementation of matrix multiplication ? = ?? (all matrices are [1024, 1024]) on modern NVIDIA GPUs via Tensor Cores <ref type="bibr" target="#b11">[12]</ref>. Figure <ref type="figure" target="#fig_2">2</ref> shows the desired workflow. In step 1 , we decompose the matrix multiplication into independent subtasks by tiling the M and N dimensions. After tiling, there will be ? M tile size ? ? N tile size independent subtasks while each sub-task is a matrix multiplication with size: M tile size ? N tile size ? ?. Each subtask will be assigned to a thread block. Inside each thread block, the K dimension will be further tiled into ? K tile size tiles, and the thread block will apply step 2 -3 to each K tile. In step 2 , threads in the thread block load fragments of matrix A and B from global memory to shared memory collectively (i.e., different threads load different parts of the fragments). All threads in a thread block will be synchronized to make sure the data loading is finished before proceeding to the next step. In step 3 , 4 warps in the thread block work on 4 ? 4 = 16 matrix multiply accumulates (MMAs), each of which is an operation ? 16?16 = ? 16?8 ? 8?16 + ? 16?16 . Each warp conducts 4 MMAs using NVIDIA Tensor Core <ref type="bibr" target="#b11">[12]</ref> with 4 sequential iterations. Once we accumulate the results of matrix multiplication for each K tile, we can 4 store the results from the accumulating register to global memory. Figure <ref type="figure" target="#fig_3">3</ref>   There are two ways to implement the kernel: (1) directly write the CUDA C code as in kernel libraries <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref>, or (2) use declarative loop-oriented scheduling primitives. In the next subsection, we would give a brief introduction to the second method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Efficient Matrix Multiplication</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Declarative Loop-Oriented Scheduling</head><p>To simplify tensor program optimization, Halide <ref type="bibr" target="#b33">[34]</ref> proposes a programming paradigm of tensor programs, in which the computation definition and scheduling of the computation are decoupled. This programming paradigm is adopted by state-of-the-art DNN compiler <ref type="bibr" target="#b8">[9]</ref> and schedulers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b46">47]</ref>. Since this paradigm offers a set of declarative scheduling primitives to manipulate the loop structure of tensor programs, we name it declarative loop-oriented scheduling.</p><p>Figure <ref type="figure">4</ref> shows the workflow of loop-oriented scheduling. Developers first provide a mathematical computation of the operator that defines how each element in the tensor is computed. The example gives the definition of matrix multiplication, where the (i, j)-th element of the output is a sum reduction. Given the computation definition, the schedulers first 1 generate a default tensor program from the computation definition automatically by translating the compute and reduce primitives to nested loops. Then, a series of declarative scheduling primitives are applied to transform the loop structure of the default tensor program for better performance on the specific hardware. Table <ref type="table" target="#tab_5">2</ref>: Loop-oriented scheduling primitives in TVM <ref type="bibr" target="#b8">[9]</ref>. The primitive fuse, split, reorder, and bind transforms the program by fusing loop, splitting loop into sub-loops, reordering loops, and binding a loop to hardware-specific axis.</p><p>in TVM <ref type="bibr" target="#b8">[9]</ref>. 2 In the example of step 2 , we only list the first few scheduling primitives to implement the matrix multiplication, as TVM has used over 80 primitives to schedule matrix multiplication.</p><p>Starting from the default program, we first split the i and j loops with factor 64 into (oi, ii) and (oj, ij), respectively, then reorder loops into (oi, oj, ii, ij), and finally bind oi and oj to blockIdx.x and block-Idx.y, respectively. With these primitives, we can get the scheduled program in Figure <ref type="figure">4</ref>.</p><p>There are several ways to make use of a programming paradigm in a deep learning compiler. Intuitively, we can manually write a schedule for each workload (i.e., an operator with certain attributes on particular hardware) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34]</ref>. However, this approach requires significant engineering efforts to achieve optimal performance for all widely used operators and their typical input sizes. Consequently, tunable parameters (e.g., tile size and loop orders) are introduced for developers to specify in the schedules. In this way, a manual schedule becomes a schedule template and can be optimized by auto-tuning frameworks <ref type="bibr" target="#b9">[10]</ref> for various input shapes and hardware. To further save the time of writing a schedule template, auto-scheduling approaches that generate a schedule by applying pre-defined rules to the computation definition have been proposed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b46">47]</ref>. 2 Schedule primitives that relocate loops are omitted.</p><p>However, as we illustrate in the next section, the schedule space from the loop-oriented scheduling paradigm is still inefficient. As a result, 1) it is challenging to achieve competitive performance on operators that are highly optimized by kernel libraries since loop-oriented scheduling can not express some key optimizations, 2) schedulers need hours to days to find the best schedule configuration in the schedule space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MOTIVATION</head><p>In this section, we summarize the challenges faced by state-of-theart loop-oriented scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Limited Optimization Support</head><p>The declarative loop-oriented scheduling primitives suffer from limited support for key optimizations. We use an important optimization, double buffering <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref>, that has been adopted in several vendor libraries (e.g., cuBLAS <ref type="bibr" target="#b19">[20]</ref> and CUTLASS <ref type="bibr" target="#b24">[25]</ref>) but not supported by TVM <ref type="bibr" target="#b8">[9]</ref>, to illustrate this fundamental limitation.</p><p>The implementation of matrix multiplication in Figure <ref type="figure" target="#fig_3">3</ref> is suboptimal, since all threads in the same thread blocks are likely to be blocked by one type of hardware resource (i.e., memory bandwidth in Step 2 or computation units in Step 3) while leaving the other idle. This is because the data loading (L7) and computation (L10) use the same buffer and synchronization (L8) needs to be used to satisfy data dependency. p, q = k0 % 2, (k0 + 1) % 2 RegsA, RegsB = cooperative_load(A, B, k0 + 1)  The double buffering optimization shown in Figure <ref type="figure" target="#fig_6">5</ref> alleviates the aforementioned problem by using two buffers: one is used for pre-loading the fragments for the next iteration (L8 and L10), while the other is used for computation in the current iteration (L9). We first preload the next tile of matrix A and B into registers (L8), and store them to shared memory after the computation of current tile (L10). This is more efficient because computation in L9 can be executed while the global memory loading in L8 is on the fly with thread-level parallelism. With double buffering, the threads in a thread block can utilize both memory accessing units and computation units at the same time.</p><formula xml:id="formula_1">RegsC = block_mma(SmemA[p], SmemB[p], RegsC) SmemA[q], SmemB[q] = RegsA, RegsB sync_threads() RegsC = block_mma(SmemA[0], SmemB[0], RegsC) ...</formula><p>However, this optimization cannot be implemented using existing declarative loop-oriented scheduling primitives in Table <ref type="table" target="#tab_5">2</ref>. This is because none of the schedule primitives can manipulate the loop body at a fine granularity 3 . As a result, although loop-oriented scheduling simplifies tensor program writing, its declarative style of scheduling prevents developers from implementing optimizations requiring fine-grained manipulation of tensor programs. Besides double buffering, thread block swizzle <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b40">41]</ref> and efficient usage 4  of Tensor Core MMA PTX instruction <ref type="bibr" target="#b21">[22]</ref>, and multi-stage asynchronous prefetching <ref type="bibr" target="#b24">[25]</ref> are widely used optimizations in kernel libraries <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref>, but are difficult to implement with declarative loop-oriented scheduling.  One important advantage of compilers with code generation over kernel libraries is the ability to optimize arbitrary workloads, especially workloads with multiple fused operators (e.g., Conv2d-BatchNorm-ReLU in convolutional neural networks <ref type="bibr" target="#b18">[19]</ref>, and Reshape-Matmul-Transpose in transformer models <ref type="bibr" target="#b14">[15]</ref>). For example, Figure <ref type="figure" target="#fig_8">6</ref> illustrates how TVM <ref type="bibr" target="#b8">[9]</ref> fuses Conv2d-BatchNorm-ReLU into a single kernel. Specifically, TVM groups operators to form subgraphs. Each sub-graph can contain only one anchor operator, which is usually the most compute-intensive one (e.g., convolution or matrix multiplication) with a carefully designed schedule template. Then, the schedule template of the anchor operator will be used to schedule the entire sub-graph, meaning that the schedule template has to support all possible fusion scenarios, which greatly increases the complexity of writing schedule templates. Although auto-schedulers (e.g., Ansor <ref type="bibr" target="#b46">[47]</ref>) are proposed to generate schedule templates automatically from the computation definition with pre-defined auto-scheduling rules, it is challenging to extend the auto-schedulers with new rules. This is because the new rule has to be compatible with all existing rules and needs to be general enough to support all operators. Thus, it is still challenging to support fusion, while not increasing the complexity of writing specialized schedule templates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dedicated Schedule Template for Fusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Long Tuning Time</head><p>In addition to expressiveness and extensibility, the tuning time of existing state-or-the-art schedulers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b46">47]</ref> typically ranges from hours to days due to inefficient schedule spaces. The majority of their schedule spaces are composed of loop tiling factors. To constrain the schedule space size and avoid conditional if-else branches, existing frameworks only cover perfect tile sizes (i.e., only tile ?extent loop with proper factors of ?). For example, potential tile 3 Even though TVM tried to use a new primitive called double_buffer to implement double buffering optimization, it does not separate the global memory loading and shared memory storing, thus can only achieve sub-optimal performance. 4 Directly use MMA PTX instruction instead of WMMA instruction <ref type="bibr" target="#b21">[22]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of schedules</head><p>Figure <ref type="figure">7</ref>: Sizes of schedule spaces adopted by AutoTVM <ref type="bibr" target="#b9">[10]</ref>.</p><p>factors of a loop with extent 10 only include 1, 2, 5, and 10. As a result, the space constructed by these frameworks with loop-oriented scheduling depends on the input shapes of the target workload. We name this category of schedule space as input-centric schedule space. We observe two challenges with input-centric schedule space. <ref type="bibr" target="#b0">(1)</ref> The schedule space size grows exponentially along with the number of input size factors. Figure <ref type="figure">7</ref> shows the number of schedules for each convolution in ResNet-50 <ref type="bibr" target="#b18">[19]</ref>. There are up to 10 8 schedules to search for a single convolutional layer. (2) The schedule space might not include the schedule with optimal performance as nonperfect tile sizes are not considered. An extreme example is that both Ansor and AutoTVM fail to find a valid schedule for matrix multiplication with M=N=K=2039 because 2039 is a prime number.</p><p>To address the first challenge, the state-of-the-art schedulers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b46">47]</ref> employ a cost model to predict the performance of schedules and use genetic evolution search to increase the search efficiency. However, the search process still requires about half an hour to tune a single operator, resulting in 8 to 15 hours to tune an Inception V3 model <ref type="bibr" target="#b38">[39]</ref>. Long tuning time prevents existing schedulers from optimizing DNNs with graph-level optimizations <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29]</ref> and upperlevel applications such as neural architecture search <ref type="bibr" target="#b49">[50]</ref>. Both of them need the latency of a kernel to guide their optimization and network searching within a short amount of tuning time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">KEY IDEAS</head><p>To address the challenges mentioned above, we propose a new programming paradigm for tensor programs -task-mapping-oriented programming paradigm. This paradigm embeds the scheduling into the tensor program and allows more optimizations compared with the existing declarative style of scheduling. We also propose postscheduling fusion to simplify sub-graph scheduling by automatically fusing surrounding operators to the operator with scheduled tensor program. The proposed paradigm also enables efficient partial tiling (tile size is not required to divide loop extent) to tune the tensor program in a small hardware-centric schedule space and significantly reduces the tuning time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task-Mapping-Oriented Programming Paradigm</head><p>Loop-oriented scheduling manipulates a tensor program through declarative loop-oriented scheduling primitives to simplify the tensor programming, but at the same time prevents fine-grained manipulations and optimizations. Task Mapping Details</p><formula xml:id="formula_2">? (3) Implement task (i, k)</formula><p>Figure <ref type="figure">8</ref>: Scheduling the cooperative loading with declarative loop-oriented scheduling and task-mapping-oriented programming paradigm. In declarative loop-oriented scheduling, developers apply a series declarative scheduling primitives to an automatically generated program to transform the tensor program into a more efficient one. Instead of employing declarative primitives, task-mapping-oriented programming paradigm allows developers to directly embed the scheduling in the tensor program, and enables a larger spectrum of optimizations compared with loop-oriented scheduling.</p><p>We observe that the goal of loop-oriented scheduling primitives is either to (1) assign the computations to parallel processing units (e.g., threads), or (2) specify the execution order of the computations assigned to each processing unit. Figure <ref type="figure">8</ref> shows the cooperative loading of the matrix A in the matrix multiplication as an example (we omitted the block offset and only show the loading of the matrix A for simplicity). In this example, loop-oriented scheduling applies three primitives (i.e., loop split, fuse, and bind) to assign the loading tasks of 512 (64x8) elements to 128 threads and each thread loads 4 elements in order.</p><p>Instead of scheduling through applying declarative primitives, we propose to embed the scheduling into tensor programs and use dedicated mappings, called task mappings, to define the computations assignment and ordering directly in the program. We use the example in Figure <ref type="figure">8</ref> to demonstrate how to use task mapping to fulfill the desired scheduling. In step (1), a task mapping is first defined, which assigns 64x8 tasks to 128 threads. Then, in step (2), each task (i, k) assigned to a thread is iterated by calling the task mapping with thread index threadIdx.x. Finally, in step (3), the task is implemented using its index (i, k). The three steps decouple the task assignment and the implementation of every single task, greatly simplifying tensor program developments. Compared with declarative loop-oriented scheduling, it schedules directly in the tensor program and allows more fine-grained optimizations. Besides this, it also allows developers to fall back on some dimensions to traditional loops to implement optimizations such as double buffering <ref type="bibr" target="#b24">[25]</ref>. Since the task mapping is the key component used in the three steps, we name our new approach -a task-mapping-oriented programming paradigm.</p><p>The task mapping defined in step ( <ref type="formula">1</ref>) is derived from task mapping composition of two basic task mappings (i.e., repeat(4, 1) and spatial <ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b7">8)</ref>). The table in Figure <ref type="figure">8</ref> gives the details of all appeared task mappings. The formal definition of task mapping and its composition are given in Section 5.1.</p><p>The proposed paradigm simplifies tensor program development without sacrificing the optimization expressiveness. Beyond the scheduling of a single operator, it is also important to schedule a fused sub-graph as operator fusion could greatly reduce the memory traffic to accelerate the end-to-end DNN execution <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref>.  We propose to decompose the scheduling of fused sub-graph into two steps, as shown in Figure <ref type="figure" target="#fig_10">9</ref>. In step 1 , we select the anchor operator as TVM <ref type="bibr" target="#b8">[9]</ref> does, but only schedule the anchor operator alone. In step 2 , we fuse the surrounding operators to the scheduled tensor program of the anchor operator automatically. With this decoupling, the scheduling of anchor operator does not need to consider the whole sub-graph, which greatly reduces the engineering efforts required to design schedule template for sub-graph compared with AutoTVM <ref type="bibr" target="#b9">[10]</ref>. We call this approach post-scheduling fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Post-Scheduling Fusion</head><p>In post-scheduling fusion, the anchor operator can be fused with operators before (as prologue) and after (as epilogue) it. We decide if an operator is fusible based on its characteristics. If an operator has no reduction computation, it is defined as injective and qualified as prologue operator. If an operator is injective and each element in the input tensor contributes to a single element in the output tensor, it is defined as bijective and qualified as epilogue operator. For example, all elementwise operators (e.g., addition, ReLU <ref type="bibr" target="#b2">[3]</ref>) and transform operators (e.g., reshape, transpose) are bijective operators and are qualified as both prologue and epilogue operators.</p><p>With post-scheduling fusion, we can concentrate on the scheduling of a single operator while supporting flexible and effective fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hardware-Centric Scheduling Space</head><p>Existing state-of-the-art schedulers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b46">47]</ref> adopt the input-centric schedule space discussed in Section 3.3, in which the schedule chooses the proper factors of loop extent as the split or tile factors, which makes the schedule space unscalable and fails to cover the optimal performance derived from tile sizes that are not proper factors of loop extents. In addition to constructing a schedule space based on input sizes, another approach is to design the schedule space based on hardware, named hardware-centric schedule space. Hardware-centric schedule space decouples the schedule space from the input size by employing predicated loading (i.e., protecting the data loading by checking if the accessing indices are in bounds), and is widely used by kernel libraries <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>With the proposed paradigm, we can provide a small but efficient hardware-centric schedule space. Since the tile factors are based on hardware resources (e.g., 64x64, 128x64, 16x32, etc), hardwarecentric schedule spaces are orders of magnitude smaller than inputcentric schedule spaces. For example, the schedule space we adopted for matrix multiplication contains less than 200 schedules, which is on average 10 5 ? smaller than a typical schedule space in Au-toTVM <ref type="bibr" target="#b9">[10]</ref>. Simply enumerating all schedules would be enough and can be done within one minute in Hidet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">HIDET: SYSTEM DESIGN</head><p>With the above key ideas, we design and implement a DNN compiler, named Hidet. Figure <ref type="figure" target="#fig_1">10</ref> shows the overall design. Hidet firstly 1 imports a deep neural network from a widely used framework (e.g., PyTorch <ref type="bibr" target="#b31">[32]</ref>) or a model file in ONNX <ref type="bibr" target="#b4">[5]</ref> format, and then 2 performs graph-level optimizations, such as constant folding and partition of fusible sub-graphs. After graph-level optimizations, each anchor operator in the fusible sub-graphs is lowered for scheduling. In Hidet, we 3 schedule the operator with task-mappingoriented programming paradigm (Section 5.1) into a tensor program and tune the schedule in hardware-centric schedule space. Then, in step 4 , the post-scheduling fusion (Section 5. surrounding operators automatically. 5 Finally, the tensor program will be lowered to a CUDA kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Task-Mapping-Oriented Programming Paradigm</head><p>One key challenge when optimizing tensor programs for a certain device with parallel processing units (e.g., modern CPUs, GPUs, and TPUs) is how to assign the independent (sub) tasks to the parallel processing units. Using cooperative loading in Figure <ref type="figure">8</ref> as an example, when loading the fragment of matrix A with shape 64x8 from global memory to shared memory, the 512 tasks are assigned to the 128 threads in a thread block, and each thread is assigned with 4 loading tasks. In this example, tasks are assigned to parallel processing units, called workers, and the tasks assigned to each worker will be executed in a specific order. In this section, we will first formalize the task assignment and ordering as task mapping, then introduce a binary operator on task mappings to compose task mappings, and finally discuss the scheduling based on task mappings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Task</head><p>Mapping. Formally, we define a worker set ? ? to be a set containing worker IDs from 0 to ? -1 (? is the number of workers): ? ? = {0, 1, . . . , ? -1}. We also define a task domain ? as</p><formula xml:id="formula_3">? = {(? 0 , ? 1 , . . . , ? ?-1 ) | 0 ? ? ? &lt; ? ? , ? ? ? Z},</formula><p>to represent all tasks we are interested in, where ? is the dimension of the task domain. Then, a task mapping ? is defined as a function that maps each worker in the worker set to a list of tasks in the task domain, that is (0) , ? (1) , . . . , ? (?-1) ].</p><formula xml:id="formula_4">? (?) = [?</formula><p>where ? ? W and ? (?) ? T. We call d = (? 0 , ? 1 , . . . , ? ?-1 ) the task shape of the task mapping.</p><p>We find two basic task mappings that are very useful. The repeat(d1, ..., dm) task mapping maps a grid of tasks (d1, ..., dm) to a single worker while the spatial(d1, ..., dm) task mapping maps a grid of tasks (d1, ..., dm) to the same number of workers and each worker only works on a single task. Besides these basic mappings, Hidet also allows developers to define custom task mappings by specifying the task shape, number of workers, and the mapping function. Though all examples are in 2-dimension, the task mapping can have an arbitrary number of task shape dimensions.</p><p>5.1.2 Task Mapping Composition. In the example of cooperative loading, we can observe a hierarchical structure. The 64x8=512 tasks can be partitioned into 4 groups of tasks and each group contains 16x8=128 tasks. The 128 tasks in each group are executed by 128 threads. If we take each task group as a macro-task and the 128 threads as a macro-worker, then task-mapping of the macro-tasks to macro-workers is a task mapping that maps 4 tasks to a single worker, denoted by repeat(4, 1). This example demonstrates that all the tasks in a task mapping can be treated entirely as a single task and all the workers can be treated entirely as a single worker in another task mapping to create a composed task mapping.</p><p>We formalize this idea as follows. Let ? 1 , ? 2 be two task mappings with the same task domain dimension. Let ? 1 , ? 2 be the number of workers and d 1 , d 2 be the task shapes of the two task mappings. We define ? 3 be the composed task mapping of ? 1 and ? 2 that has ? 1 ? 2 workers and task shape d 3 = d 1 ? d 2 . 5 The mapping function is defined as</p><formula xml:id="formula_5">? 3 (?) = [t 1 ? d 2 + t 2 | t 1 ? ? 1 (??/? 2 ?), t 2 ? ? 2 (? % ? 2 )].</formula><p>The task mapping composition is denoted as</p><formula xml:id="formula_6">? 3 = ? 1 ? ? 2 . Task composition is associative, that is (? 1 ? ? 2 ) ? ? 3 = ? 1 ? (? 2 ? ? 3 ), holds for arbitrary task mappings ? 1 , ? 2 , ? 3 .</formula><p>Task mapping composition is a powerful tool to construct new task mappings. For example, if we want to execute a grid of task (m, n) with a single worker in column-major order, we can construct this task mapping with repeat(1, n) * repeat(m, 1). We can also compose more than two task mappings together. Task mapping spatial(4, 2) * repeat(2, 2) * spatial(4, 8) * repeat <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b3">4)</ref> is used in matrix multiplication with CUDA Core <ref type="bibr" target="#b30">[31]</ref>. They correspond to the warps in a block (2x4), the number of repeats for each warp (2x2), the layout of threads in a warp (4x8), and the number of C elements each thread works on (4x4), respectively.</p><p>Task mappings and their composition could greatly simplify the tensor program writing as it employs dedicated mappings to define the task assignment and ordering, and free developers from writing complex loops and index calculation to achieve the same goal. We call the tensor program writing paradigm based on task mappings as task-mapping-oriented programming paradigm for tensor programs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Scheduling Mechanisms.</head><p>Based on the paradigm, we further implement two scheduling mechanisms in Hidet: template-based scheduling and rule-based scheduling. Inspired by Ansor <ref type="bibr" target="#b46">[47]</ref> and Halide-AutoScheduler <ref type="bibr" target="#b1">[2]</ref>, rule-based scheduling directly generates the tensor program from one operator's computation definition, without any extra engineering efforts and is used for the majority of operators in Hidet. On the other hand, the rule-based scheduling might not be able to generate an efficient-enough tensor program for key operators such as matrix multiplication. Inspired by AutoTVM <ref type="bibr" target="#b9">[10]</ref>, we also allow developers to provide a tensor 5 We use ? to denote the element-wise multiplication.</p><p>program template to support efficient scheduling of these operators. Figure <ref type="figure" target="#fig_12">11</ref> illustrates the two scheduling mechanisms.  Rule-based Scheduling generates the tensor program given the computation definition automatically. It traverses the computation definition in the form of a directed acyclic graph (DAG) and applies pre-defined rules to translate each node in the DAG into a part of the final tensor program. Because this mechanism does not require developers to write a dedicated schedule template, it is widely used in Hidet for the operators that do not include reduction, such as reshape, transpose, slice, and all element-wise arithmetic operators. On the other hand, for operators demanding extreme optimizations like matrix multiplication, we use another scheduling mechanism, named template-based scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Operator Computation Definition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rule-based</head><p>Template-based Scheduling schedules the operator with the given template. A schedule template is a tensor program written with parameterized task mappings. Each schedule template is equipped with a schedule space containing a collection of available parameters for the parameterized task mappings, and the template can be instantiated with an arbitrary choice from the schedule space. During scheduling, Hidet first enumerates the schedule choice from the schedule space. Then the schedule choice is used to create the task mappings for the given program template. Finally, Hidet instantiates the template into a tensor program and measures its performance. The schedule with the best performance is used. We refer to the process as tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Post-Scheduling Fusion</head><p>To alleviate the complexity of scheduling sub-graph as in AutoTVM, we propose to decouple the sub-graph scheduling into two stages:</p><p>(1) scheduling the anchor operator and (2) fusing the scheduled tensor program with surrounding operators. The decoupling allows developers to focus on the scheduling of the anchor operator instead of the whole sub-graph, and automates the fusion of the scheduled tensor program with other operators in the sub-graph. During tuning, the performance of fused tensor programs will be used as the target to maximize, thus the decoupling does not hurt the final performance. Figure <ref type="figure" target="#fig_13">12</ref> shows an example of post-scheduling fusion. In step 1 , during the graph-level optimization stage, a pass partitions the computation graph into sub-graphs. Given the sub-graph, in step 2 , a selected anchor operator will be scheduled into a tensor program with one of the scheduling mechanisms in Section 5.1.3. Finally, in step 3 , the remaining operators will be fused into the scheduled program. These operators are classified into two categories: prologue operators for each input tensor and epilogue operators for each output tensor. Each prologue operator defines how each element access for the input tensor is computed, and the epilogue operator defines how the output tensor elements are furthermore computed and stored in the output of the fused operator. In this example, the access of A[99 -i] will be replaced by C[99 -i] * 2.0, and the ?-th element of output tensor is furthermore computed (i.e., multiply by 3.0) and stored to the fused output tensor D with indices (i / 50, i % 50).</p><p>The post-scheduling fusion simplifies the operator fusion. Another advantage is the reuse of existing highly optimized operators (e.g., matrix multiplication) to support new operators (e.g., convolution). In Hidet, we can implement the convolution operators as four operators with img2col algorithm <ref type="bibr" target="#b7">[8]</ref>, one of which is matrix multiplication and the other three are simple transform operators. With post-scheduling fusion, we fuse the other three operators into a matrix multiplication and reuse all optimizations (e.g., parallel reduction on k dimension <ref type="bibr" target="#b24">[25]</ref>) for matrix multiplications to convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head><p>Implementation. We implement Hidet from scratch with ?20K lines of code in Python and C++. Two levels of IR are used in Hidet: graph-level IR to represent the computation graph of DNN models and tensor-level IR to represent tensor programs with schedules. Hidet lowers the tensor program written with task mappings to CUDA C code and compiles it with the CUDA compiler. Notably, we only implement two efficient schedule templates for matrix multiplication and reduction operators (e.g, sum reduction) to cover all operators in evaluated models. Most operators are either scheduled by the rule-based scheduling mechanism, or converted to matrix multiplication to reuse existing templates (e.g., convolutions). Platform. We conduct experiments on a server equipped with a 16-core Intel i9-12900K CPU (with hyper-threading enabled), 64 GiB DRAM, and one NVIDIA RTX 3090 GPU with NVIDIA driver 510.73.08 and CUDA 11.6.</p><p>Workloads. We benchmark on a wide range of representative networks to demonstrate the optimization generality of Hidet. ResNet-50 <ref type="bibr" target="#b18">[19]</ref> is one of the most-commonly used CNNs for image classification. Inception-V3 <ref type="bibr" target="#b38">[39]</ref> is a CNN that employs multiple paths of convolutions with different kernel sizes. MobileNet-V2 <ref type="bibr" target="#b35">[36]</ref> is a light-weight CNN based on separable convolutions. Bert <ref type="bibr" target="#b14">[15]</ref> is a widely-used transformer-based natural language processing (NLP) model. GPT-2 <ref type="bibr" target="#b32">[33]</ref> is an NLP model targeting sequence-to-sequence tasks such as natural language translation and question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">End-to-end Evaluation</head><p>We evaluate all workloads on Hidet against PyTorch <ref type="bibr" target="#b31">[32]</ref> 1.11, Onnx Runtime <ref type="bibr" target="#b13">[14]</ref> 1.11.1, AutoTVM <ref type="bibr" target="#b9">[10]</ref>, and Ansor <ref type="bibr" target="#b46">[47]</ref> in TVM <ref type="bibr" target="#b8">[9]</ref> 0.9.dev with commit c07a46327. PyTorch is a widely used DNN framework. Onnx Runtime is a high-performance inference engine. Both of them leverage high performance kernel libraries cuDNN <ref type="bibr" target="#b10">[11]</ref> and cuBLAS <ref type="bibr" target="#b19">[20]</ref>. AutoTVM and Ansor are two stateof-the-art schedulers based on loop-oriented scheduling and inputcentric tuning spaces. We set the number of tuning trials in Au-toTVM and Ansor to 1000 and 800, respectively, as suggested in official documents. Performance. Figure <ref type="figure" target="#fig_15">13</ref> shows the results of end to end inference latency with single batch. Hidet outperforms all baselines on most models by up to 1.48?, and on average by 1.22?. This is because Hidet is able to automatically fuse sub-graph, tune the schedule for given input size (vs. PyTorch and Onnx Runtime), and express more optimizations such as double buffering <ref type="bibr" target="#b24">[25]</ref> (vs. AutoTVM and Ansor). One exception is Ansor on MobileNetV2, as Ansor could find a better schedule for depthwise convolutions. We can implement similar schedules in Hidet, and we leave such implementations to future work. In addition, we note that AutoTVM performs worse on Bert and GPT-2 (27ms and 41ms). This is because AutoTVM's schedule templates for workloads in these two models lack optimizations. Tuning Cost. We compare the tuning cost (i.e., elapsed time for tuning) of AutoTVM, Ansor, and Hidet in Figure <ref type="figure" target="#fig_1">14</ref>. Hidet reduces the tuning cost by 11? and 20? compared with Ansor and AutoTVM, respectively. This is because Hidet adopts a small (e.g., 180 schedules in matrix multiplication) but efficient schedule space with the proposed paradigm. As a result, Hidet only needs minutes to exhaustively enumerate all candidates. On the other hand, AutoTVM <ref type="bibr" target="#b9">[10]</ref> and Ansor <ref type="bibr" target="#b46">[47]</ref> adopt schedule spaces with 10 5 to 10 8 candidates, which prevents them from finding the optimal schedule in their space in a short time. Note that although AutoTVM only spends 2 minutes for Bert and GPT-2 due to their small schedule spaces with less than 20 schedules, the schedule spaces are ineffective and can not achieve competitive performance (Figure <ref type="figure" target="#fig_15">13</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Case Studies</head><p>In this subsection, we conduct several case studies to further demystify the effectiveness of Hidet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Schedule Space Comparison.</head><p>To compare the efficiency of three schedule spaces adopted by AutoTVM, Ansor, and Hidet, we depict the latency distribution of schedules in the three schedule spaces in Figure <ref type="figure" target="#fig_6">15</ref>. The benchmark workload is a convolution in    highly optimized operators to save engineering efforts. For example, in Hidet, we implement convolution through matrix multiplication, namely implicit general matrix multiplication (GEMM) convolution, which is also known as img2col <ref type="bibr" target="#b7">[8]</ref> algorithm. With post-scheduling fusion, we are able to fuse the additional required operators in img2col into the matrix multiplication automatically and reuse the optimizations we implemented for it (e.g., parallel reduction on k dimension <ref type="bibr" target="#b24">[25]</ref>). The implicit GEMM convolution with parallel k reduction allows Hidet's generated kernels to saturate the GPU computation resources and outperforms the existing kernel libraries and DNN compilers. Figure <ref type="figure" target="#fig_16">18</ref> shows the performance of the Conv-Bn-ReLU sub-graphs in ResNet50 among Onnx Runtime, Ansor, and Hidet. Hidet outperforms Onnx Runtime and Ansor on most convolutions as the convolution can also parallelize on the reduction dimensions (e.g., input channels, and kernel sizes).</p><p>ResNet50 IncpV3 MbNetV2 Bert GPT-2 Geo-Mean 0  TensorRT applied both graph-level and operator-level optimizations. Figure <ref type="figure" target="#fig_18">19</ref> shows the comparison of TensorRT and Hidet. Hidet outperforms TensorRT on the three CNNs because Hidet is able to tune for the given input sizes and fuse operators automatically according to their mathematical definition. On the other hand, TensorRT outperforms Hidet on the transformer <ref type="bibr" target="#b42">[43]</ref> networks such as Bert and GPT-2. Since TensorRT is close-sourced, we speculate, by interpreting its optimization log, that TensorRT recognizes self-attention layers in transformer models and applies dedicated optimizations due to the popularity of these models. On the other hand, Hidet only has two schedule templates to cover all operators in benchmarked networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORKS</head><p>Many existing deep learning compilers adopt loop-oriented scheduling primitives <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34]</ref> and establish auto-tuning frameworks on top of them <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref><ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref> with input-centric schedule spaces. In contrast, Hidet leverages task-mapping-oriented programming paradigm with hardware-centric schedule spaces, so that it is able to achieve better performance with much shorter tuning time.</p><p>In addition to loop-oriented scheduling, there are more approaches to optimize a tensor program. Deep learning frameworks such as PyTorch <ref type="bibr" target="#b31">[32]</ref> and TensorFlow <ref type="bibr" target="#b0">[1]</ref> leverage off-the-shelf kernel libraries (e.g., cuDNN <ref type="bibr" target="#b10">[11]</ref> and cuBLAS <ref type="bibr" target="#b19">[20]</ref>) as well as hand-crafted kernels to cover widely used operators. CUTLASS <ref type="bibr" target="#b24">[25]</ref> is an open C++ template library with efficient matrix multiplication kernel on CUDA. Unlike Hidet that has seamless support for efficient fusion and auto-tuning, those libraries require large amount of engineering efforts and nontrivial expertise to integrate important optimizations to be performant on each hardware platform. TVM community also noticed the limited expressiveness problem of existing declarative loop-oriented scheduling mechanism. TensorIR <ref type="bibr" target="#b16">[17]</ref>, a concurrent work with Hidet, is recently proposed to allow developers to directly write tensor programs instead of applying a series of declarative primitives to the auto-generated tensor program. The difference is that TensorIR adopts a hybrid style of scheduling that allows developers to manipulate the tensor program with loop transform primitives on hand-crafted tensor programs, while Hidet uses task mapping for scheduling.</p><p>Moreover, Tensorflow-XLA <ref type="bibr" target="#b34">[35]</ref> is a domain-specific compiler for linear algebra. FreeTensor <ref type="bibr" target="#b39">[40]</ref> is a domain-specific language that supports irregular tensor programs. Besides optimizing each single operator for DNN inference, Rammer <ref type="bibr" target="#b29">[30]</ref> and IOS <ref type="bibr" target="#b15">[16]</ref> propose to parallelize independent operators in a network, and TASO <ref type="bibr" target="#b22">[23]</ref> applies auto-generated rewriting rules to optimize DNN in graph level. These graph-level optimizations are orthogonal to Hidet, and can be used to enhance the graph-level optimizations of Hidet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>We observe that the state-of-the-art DNN compilers based on looporiented scheduling cannot express important optimizations that require fine-grained manipulation of the tensor program. To address this limitation, we propose task-mapping-oriented programming paradigm, a new paradigm to write and schedule tensor programs that simplifies tensor program writing and scheduling without sacrificing the ability to express optimizations as in kernel libraries. Based on this paradigm, we implemented a new DNN inference framework called Hidet. Experiments show that Hidet achieves up to 1.48? speedup (1.22? on average), compared with state-of-theart DNN inference frameworks (e.g., Onnx Runtime) and compilers (e.g., TVM equipped with AutoTVM and Ansor). Hidet also reduces 11? tuning cost compared with Ansor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>We would like to thank the members of EcoSystem research laboratory in University of Toronto for their feedback on the early manuscript, and special thanks to Xingyang Song, Christina Giannoula, Anand Jayarajan, and Jiacheng Yang. This paper was supported by the Canada Foundation for Innovation JELF grant, NSERC</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of CUDA programming model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Efficient Matrix Multiplication on CUDA Platform.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Pseudo-code of Matrix Multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>RegsA, RegsB = register fp32[...], fp32[...] SmemA, SmemB = shared fp32[2, 64, 8], fp32[2, 8, 64] ... SmemA[0], SmemB[0] = cooperative_load(A, B, 0) sync_threads() for k0 in range(127):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Preloading</head><label></label><figDesc>Next Tile of A/ B into Regs Computation of Cur r ent Tile Two Buffers for A &amp; B Store Next Tile of A/ B into Shar ed Memory</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Double Buffering Optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Schedule Sub-Graph with Anchor Op's Schedule Template</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Workflow of TVM sub-graph fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Two steps in post-scheduling fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>2 Figure 10 :</head><label>210</label><figDesc>Figure 10: Overall Design of Hidet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Two Scheduling Mechanisms in Hidet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Example of Post-Scheduling Fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: End-to-end comparison between state-of-the-art DNN inference frameworks and compilers with Hidet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Comparison of Onnx Runtime, Ansor, and Hidet on the Conv2d-Bn-ReLU sub-graphs in ResNet50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Comparison of TensorRT and Hidet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different DNN kernel libraries and compilers on graph-and kernel-level optimizations, tuning time, and engineering efforts to support new operators.</figDesc><table><row><cell>Higher</cell><cell>Higher</cell><cell>Lower</cell><cell>Lower</cell></row><row><cell>Better</cell><cell>Better</cell><cell>Better</cell><cell>Better</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>?</head><label></label><figDesc>We introduce the task-mapping-oriented programming paradigm to simplify tensor program development without sacrificing the expressiveness of optimizations compared with hand-crafted implementations. Based on this paradigm, we propose post-scheduling fusion to fuse the scheduled program with surrounding operators. The paradigm also allows us to search in the hardware-centric schedule space to reduce the tuning time significantly.</figDesc><table /><note><p>? We implemented a new DNN compiler, named Hidet, based on the proposed ideas. Extensive experiments show that Hidet outperforms state-of-the-art DNN frameworks and compilers by up to 1.48? and reduces tuning time by 11?.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>shows the scheduling primitives</cell></row></table><note><p>Schedule Pr imitives Or iginal Pr ogr am Scheduled Pr ogr am fuse(i, j) for i in range(128): for j in range(4): body(i, j) for ij in range(512): body(ij / 4, ij % 4) split(i, 128) for i in range(512): body(i) for oi in range(4): for ii in range(128): body(oi * 128 + ii) reorder(i, j) for i in range(128): for j in range(4): body(i, j) for j in range(4): for i in range(128): body(i, j) bind(i, threadIdx.x) for i in range(128): body(i) body(threadIdx.x)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Because the schedule spaces of AutoTVM and Ansor are too large, we take the 1000 and 800 schedules from the tuning process of AutoTVM and Ansor, respectively, and compare them with the entire space with only 180 schedules in Hidet. The figure shows that most schedules covered by Hidet schedule space have superior performance (latency &lt; 73?s) than those in spaces adopted by AutoTVM and Ansor thanks to the better expressiveness of the proposed paradigm.6.2.2 Performance Sensitivity over InputSizes. The quality of the final schedule derived from AutoTVM and Ansor is sensitive to the input size due to their input-centric schedule spaces. Even a small change in the input size would result in a large performance difference. To compare the performance sensitivity over input sizes, we benchmark matrix multiplications with consecutive input sizes.</figDesc><table><row><cell></cell><cell>0.2</cell><cell>OnnxRuntime</cell><cell>Ansor</cell><cell>Hidet</cell></row><row><cell>Latency (ms)</cell><cell>0.1</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell cols="2">Conv2d-Bn-ReLU in ResNet50</cell></row><row><cell cols="5">ResNet50 InceptionV3 MbNetV2 8h 15h 9h 4h 9h 4h Hidet speedup tuning by Bert GPT-2 Average 6h 4h 20x (AutoTVM) and 11x (Ansor) on average. AutoTVM Ansor Hidet 2m 2m 51m 52m 20m 45m 22m 5m 5m 19m Figure 14: Tuning cost of AutoTVM, Ansor, and Hidet. 1 h 6 h 12 h 18 h 39 73 93 800 Latency ( s) 0 5 10 Schedule Latency Tuning Cost (Hours) Distribution Density 180 sch. 800 sch. 1000 sch. AutoTVM Ansor Hidet Figure 15: Schedule latency distribution of schedule spaces from AutoTVM, Ansor, and Hidet. X-axis is in log scale. ResNet50 with input image size 28x28, input channels 256, ker-nel size 3, padding 1, and stride 2. 2048 2047 2046 2045 2044 2043 2042 2039 2 4 6 Latency (ms) 13 38 Failed Failed 7 27 AutoTVM Ansor Hidet Matrix Multiplication (M=N=K) 0 1 4 8 0 5 10 Latency (ms) PyTorch OnnxRuntime AutoTVM Ansor Hidet Figure 17: Comparison on batch size 1, 4, and 8 of ResNet50.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>6.2.3 Evaluation on Different Batch Sizes. Figure 17 depicts the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>latency of ResNet50 with different batch sizes. When batch size is</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>small (1 and 4), AutoTVM and Ansor outperform Onnx Runtime as</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>they can find schedules that utilize the GPU computation resources</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>well (e.g., enough thread blocks to saturate all SMs), while kernel</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>libraries do not. At larger batch sizes (e.g., 8), we observe that</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>although AutoTVM and Ansor can still find schedules that saturate</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>all SMs, they cannot outperform Onnx Runtime, because the latency</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>of each thread block is longer than Onnx Runtime's, due to the lack</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>of important optimizations such as double buffering [25]. On the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>other hand, Hidet outperforms all of them as Hidet could perform</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>well on both aspects.</cell></row></table><note><p><p><p><p><p>Figure 16  </p>shows that the performance of AutoTVM and Ansor fluctuates significantly. Even worse, for a prime number input size (e.g., 2039), both schedulers failed to find a valid schedule. On the other Figure 16: Comparison of AutoTVM, Ansor, and Hidet on matrix multiplication with consecutive input sizes.</p>hand, with the hardware-centric schedule space, Hidet achieves consistent performance on these input sizes.</p>6.2.4 Post-Scheduling Fusion Evaluation.</p>With post-scheduling fusion, we can implement an operator with a highly optimized schedule template, and composite new operators with pre-implemented,</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The name task mapping comes from the abstraction where a scheduling process can be considered as the one that maps tasks to processing units in both spatial and temporal dimensions.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandelion</forename><surname>Man?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to optimize halide with tree search and random programs</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karima</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riyadh</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tzu-Mao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micha?l</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kayvon</forename><surname>Fatahalian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019-07">jul 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep learning using rectified linear units (relu)</title>
		<author>
			<persName><forename type="first">Abien</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agarap</forename></persName>
		</author>
		<idno>ArXiv, abs/1803.08375</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Tiramisu: A polyhedral compiler for expressing fast and portable code</title>
		<author>
			<persName><forename type="first">Riyadh</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malek</forename><surname>Ben Romdhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Del Sozzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdurrahman</forename><surname>Akkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Suriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoaib</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE/ACM International Symposium on Code Generation and Optimization</title>
		<meeting>the 2019 IEEE/ACM International Symposium on Code Generation and Optimization</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="193" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://github.com/onnx/onnx" />
		<title level="m">Open neural network exchange</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cudadma: Optimizing gpu memory bandwidth via warp specialization</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brucek</forename><surname>Khailany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC &apos;11: Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Optimizing compute shaders for l2 locality using thread-group id swizzling</title>
		<author>
			<persName><forename type="first">Louis</forename><surname>Bavoil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">High performance convolutional neural networks for document processing</title>
		<author>
			<persName><forename type="first">Sidd</forename><surname>Kumar Chellapilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrice</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth international workshop on frontiers in handwriting recognition</title>
		<imprint>
			<publisher>Suvisoft</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tvm: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><forename type="middle">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to optimize tensor programs</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3389" to="3400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">cudnn: Efficient primitives for deep learning</title>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno>ArXiv, abs/1410.0759</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nvidia a100 tensor core gpu: Performance and innovation</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Choquette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wishwesh</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Giroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Stam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Krashinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="29" to="35" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<ptr target="https://onnxruntime.ai/,2021.Ver-sion:1.11.1" />
		<title level="m">ONNX Runtime developers. Onnx runtime</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">BERT: pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR, abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ios: Inter-operator scheduler for cnn acceleration</title>
		<author>
			<persName><forename type="first">Yaoyao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gennady</forename><surname>Pekhimenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="167" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Tensorir: An abstraction for automatic tensorized program optimization</title>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wuwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junru</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><forename type="middle">Hao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Demystifying the placement policies of the nvidia gpu thread block scheduler for concurrent kernels. SIGMETRICS Perform</title>
		<author>
			<persName><forename type="first">Guin</forename><surname>Gilman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Ogden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Walls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eval. Rev</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="81" to="88" />
			<date type="published" when="2021-03">mar 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Basic linear algebra on nvidia gpus</title>
		<author>
			<persName><surname>Nvidia Inc</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">NVIDIA Inc. Nvidia tensorrt</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Parallel thread execution isa</title>
		<author>
			<persName><surname>Nvidia Inc</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Taso: optimizing deep learning computation with automatic generation of graph substitutions</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oded</forename><surname>Padon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Warszawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles</title>
		<meeting>the 27th ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="47" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raminder</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Luc</forename><surname>Cantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifford</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Vazir Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajendra</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Richard</forename><surname>Robert Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alek</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Killebrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diemthu</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyuan</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maire</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Norrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayana</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Penukonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emad</forename><surname>Salek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Samadiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Severn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Sizikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jed</forename><surname>Snelham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Souter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mercedes</forename><surname>Swing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Thorson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horia</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erick</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Tuttle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doe</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><surname>Hyun Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2017-06">jun 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cutlass: Cuda template library for dense linear algebra at all levels and scales</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Kerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duane</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Demouth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naila</forename><surname>Farooqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tavenrath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vince</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Gornish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bageshri</forename><surname>Sathe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<meeting><address><addrLine>Bart</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimizing {CNN} model inference on {CPUs}</title>
		<author>
			<persName><forename type="first">Yizhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vin</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 USENIX Annual Technical Conference (USENIX ATC 19)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1025" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">RAMMER: Enabling Holistic Deep Learning Compiler Optimizations with Rtasks</title>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youshan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>USENIX Association, USA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The gpu computing era</title>
		<author>
			<persName><forename type="first">John</forename><surname>Nickolls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="69" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alch?-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acm Sigplan Notices</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="519" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Xla : Compiling machine learning for peak performance</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Sabne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Freetensor: A free-form dsl with holistic optimizations for irregular tensor programs</title>
		<author>
			<persName><forename type="first">Shizhi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jidong</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation (PLDI &apos;22), PLDI &apos;22</title>
		<meeting>the 43rd ACM SIGPLAN International Conference on Programming Language Design and Implementation (PLDI &apos;22), PLDI &apos;22<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Locality-aware cta scheduling for gaming applications</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ukarande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suryakant</forename><surname>Patidar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ram</forename><surname>Rangan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021-12">dec 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Tensor comprehensions: Framework-agnostic high-performance machine learning abstractions</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Zinenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zach</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Verdoolaege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<idno>ArXiv, abs/1802.04730</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">UNIT: Unifying Tensorized Instruction Compilation</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Nowatzki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>IEEE Press</publisher>
			<biblScope unit="page" from="77" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bolt: Bridging the gap between auto-tuners and hardware-native performance</title>
		<author>
			<persName><forename type="first">Jiarong</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibo</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">DietCode: Automatic optimization for dynamic tensor programs</title>
		<author>
			<persName><forename type="first">Bojian</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><surname>Hao Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Fromm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gennady</forename><surname>Pekhimenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Marculescu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</editor>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="848" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ansor: Generating high-performance tensor programs for deep learning</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengfan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><surname>Hao Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameer</forename><surname>Haj-Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koushik</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="863" to="879" />
		</imprint>
	</monogr>
	<note>OSDI 20</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Enabling automatic mapping for tensor computations on spatial accelerators with hardware abstraction</title>
		<author>
			<persName><forename type="first">Size</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Amos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual International Symposium on Computer Architecture, ISCA &apos;22</title>
		<meeting>the 49th Annual International Symposium on Computer Architecture, ISCA &apos;22<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="874" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Flextensor: An automatic schedule exploration and optimization framework for tensor computation on heterogeneous system</title>
		<author>
			<persName><forename type="first">Size</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwen</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
