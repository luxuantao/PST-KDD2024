<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A statistical model for tensor PCA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andrea</forename><surname>Montanari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Statistics &amp; Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emile</forename><surname>Richard</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A statistical model for tensor PCA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">393BEC27651A076A0BC7872A5FBD172E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the Principal Component Analysis problem for large tensors of arbitrary order k under a single-spike (or rank-one plus noise) model. On the one hand, we use information theory, and recent results in probability theory, to establish necessary and sufficient conditions under which the principal component can be estimated using unbounded computational resources. It turns out that this is possible as soon as the signal-to-noise ratio β becomes larger than C √ k log k (and in particular β can remain bounded as the problem dimensions increase). On the other hand, we analyze several polynomial-time estimation algorithms, based on tensor unfolding, power iteration and message passing ideas from graphical models. We show that, unless the signal-to-noise ratio diverges in the system dimensions, none of these approaches succeeds. This is possibly related to a fundamental limitation of computationally tractable estimators for this problem. We discuss various initializations for tensor power iteration, and show that a tractable initialization based on the spectrum of the unfolded tensor outperforms significantly baseline methods, statistically and computationally. Finally, we consider the case in which additional side information is available about the unknown signal. We characterize the amount of side information that allows the iterative algorithms to converge to a good estimate.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Given a data matrix X, Principal Component Analysis (PCA) can be regarded as a 'denoising' technique that replaces X by its closest rank-one approximation. This optimization problem can be solved efficiently, and its statistical properties are well-understood. The generalization of PCA to tensors is motivated by problems in which it is important to exploit higher order moments, or data elements are naturally given more than two indices. Examples include topic modeling, video processing, collaborative filtering in presence of temporal/context information, community detection <ref type="bibr" target="#b0">[1]</ref>, spectral hypergraph theory. Further, finding a rank-one approximation to a tensor is a bottleneck for tensor-valued optimization algorithms using conditional gradient type of schemes. While tensor factorization is NP-hard <ref type="bibr" target="#b10">[11]</ref>, this does not necessarily imply intractability for natural statistical models. Over the last ten years, it was repeatedly observed that either convex optimization or greedy methods yield optimal solutions to statistical problems that are intractable from a worst case perspective (well-known examples include sparse regression and low-rank matrix completion).</p><p>In order to investigate the fundamental tradeoffs between computational resources and statistical power in tensor PCA, we consider the simplest possible model where this arises, whereby an unknown unit vector v 0 is to be inferred from noisy multilinear measurements. Namely, for each unordered k-uple {i 1 , i 2 , . . . , i k } ⊆ [n], we measure X i1,i2,...,</p><formula xml:id="formula_0">i k = β(v 0 ) i1 (v 0 ) i2 • • • (v 0 ) i k + Z i1,i2,...,i k ,<label>(1)</label></formula><p>with Z Gaussian noise (see below for a precise definition) and wish to reconstruct v 0 . In tensor notation, the observation model reads (see the end of this section for notations) X = β v 0 ⊗k + Z .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spiked Tensor Model</head><p>This is analogous to the so called 'spiked covariance model' used to study matrix PCA in high dimensions <ref type="bibr" target="#b11">[12]</ref>.</p><p>It is immediate to see that maximum-likelihood estimator v ML is given by a solution of the following problem maximize X, v ⊗k , Tensor PCA subject to v 2 = 1 .</p><p>Solving it exactly is -in general-NP hard <ref type="bibr" target="#b10">[11]</ref>.</p><p>We next summarize our results. Note that, given a completely observed rank-one symmetric tensor v 0 ⊗k (i.e. for β = ∞), it is easy to recover the vector v 0 ∈ R n . It is therefore natural to ask the question for which signal-to-noise ratios can one still reliably estimate v 0 ? The answer appears to depend dramatically on the computational resources<ref type="foot" target="#foot_0">1</ref> . Ideal estimation. Assuming unbounded computational resources, we can solve the Tensor PCA optimization problem and hence implement the maximum likelihood estimator v ML . We use recent results in probability theory to show that this approach is successful for</p><formula xml:id="formula_1">β ≥ µ k = √ k log k(1 + o k (1)</formula><p>). In particular, above this threshold<ref type="foot" target="#foot_1">2</ref> we have, with high probability,</p><formula xml:id="formula_2">v ML -v 0 2 2 2.01 µ k β .<label>(2)</label></formula><p>We use an information-theoretic argument to show that no approach can do significantly better, namely no procedure can estimate v 0 accurately for β ≤ c √ k (for c a universal constant).</p><p>Tractable estimators: Unfolding. We consider two approaches to estimate v 0 that can be implemented in polynomial time. The first approach is based on tensor unfolding: starting from the tensor X ∈ k R n , we produce a matrix Mat(X) of dimensions n q × n k-q . We then perform matrix PCA on Mat(X). We show that this method is successful for β n ( k/2 -1)/2 . A heuristics argument suggests that the necessary and sufficient condition for tensor unfolding to succeed is indeed β n (k-2)/4 (which is below the rigorous bound by a factor n 1/4 for k odd). We can indeed confirm this conjecture for k even and under an asymmetric noise model.</p><p>Tractable estimators: Warm-start power iteration and Approximate Message Passing. We prove that, initializing power iteration uniformly at random, it converges very rapidly to an accurate estimate provided β n (k-1)/2 . A heuristic argument suggests that the correct necessary and sufficient threshold is given by β n (k-2)/2 . Motivated by the last observation, we consider a 'warm-start' power iteration algorithm, in which we initialize power iteration with the output of tensor unfolding. This approach appears to have the same threshold signal-to-noise ratio as simple unfolding, but significantly better accuracy above that threshold. We extend power iteration to an approximate message passing (AMP) algorithm <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b3">4]</ref>. We show that the behavior of AMP is qualitatively similar to the one of naive power iteration. In particular, AMP fails for any β bounded as n → ∞.</p><p>Side information. Given the above computational complexity barrier, it is natural to study weaker version of the original problem. Here we assume that extra information about v 0 is available. This can be provided by additional measurements or by approximately solving a related problem, for instance a matrix PCA problem as in <ref type="bibr" target="#b0">[1]</ref>. We model this additional information as y = γv 0 + g (with g an independent Gaussian noise vector), and incorporate it in the initial condition of AMP algorithm. We characterize exactly the threshold value γ * = γ * (β) above which AMP converges to an accurate estimator. The thresholds for various classes of algorithms are summarized below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Required β (rigorous) Required β (heuristic) Tensor Unfolding O(n ( k/2 -1)/2 ) n (k-2)/4 Tensor Power Iteration (with random init.)</p><formula xml:id="formula_3">O(n (k-1)/2 ) n (k-2)/2 Maximum Likelihood 1 - Information-theory lower bound 1 -</formula><p>We will conclude the paper with some insights that we believe provide useful guidance for tensor factorization heuristics. We illustrate these insights through simulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Notations</head><p>Given X ∈ k R n a real k-th order tensor, we let {X i1,...,i k } i1,...,i k denote its coordinates and define a map X : R n → R n , by letting, for v ∈ R n ,</p><formula xml:id="formula_4">X{v} i = j1,••• ,j k-1 ∈[n] X i,j1,••• ,j k-1 v j1 • • • v j k-1 .<label>(3)</label></formula><p>The outer product of two tensors is X⊗Y, and, for v ∈ R n , we define</p><formula xml:id="formula_5">v ⊗k = v ⊗• • •⊗v ∈ k R n as the k-th outer power of v. We define the inner product of two tensors X, Y ∈ k R n as X, Y = i1,••• ,i k ∈[n] X i1,••• ,i k Y i1,••• ,i k . (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>We define the Frobenius (Euclidean) norm of a tensor X, by X F = X, X , and its operator norm by</p><formula xml:id="formula_7">X op ≡ max{ X, u 1 ⊗ • • • ⊗ u k : ∀i ∈ [k] , u i 2 ≤ 1}.<label>(5)</label></formula><p>For the special case k = 2, it reduces to the ordinary 2 matrix operator norm. For π ∈ S k , we will denote by X π the tensor with permuted indices</p><formula xml:id="formula_8">X π i1,••• ,i k = X π(i1),••• ,π(i k )</formula><p>. We call the tensor X symmetric if, for any permutation π ∈ S k , X π = X. It is proved <ref type="bibr" target="#b22">[23]</ref> that, for symmetric tensors, the value of problem Tensor PCA coincides with X op up to a sign. More precisely, for symmetric tensors we have the equivalent representation max{| X, u ⊗k | : u 2 ≤ 1}. We denote by G ∈ k R n a tensor with independent and identically distributed entries G i1,••• ,i k ∼ N(0, 1) (note that this tensor is not symmetric). We define the symmetric standard normal noise tensor</p><formula xml:id="formula_9">Z ∈ k R n by Z = 1 k! k n π∈S k G π .<label>(6)</label></formula><p>We use the loss function</p><formula xml:id="formula_10">Loss( v, v 0 ) ≡ min v -v 0 2 2 , v + v 0 2 2 = 2 -2| v, v 0 | .<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Ideal estimation</head><p>In this section we consider the problem of estimating v 0 under the observation model Spiked Tensor Model, when no constraint is imposed on the complexity of the estimator. Our first result is a lower bound on the loss of any estimator. Theorem 1. For any estimator</p><formula xml:id="formula_11">v = v(X) of v 0 from data X, such that v(X) 2 = 1 (i.e. v : ⊗ k R n → S n-1</formula><p>), we have, for all n ≥ 4,</p><formula xml:id="formula_12">β ≤ k 10 ⇒ E Loss( v, v 0 ) ≥ 1 32 .<label>(8)</label></formula><p>In order to establish a matching upper bound on the loss, we consider the maximum likelihood estimator v ML , obtained by solving the Tensor PCA problem. As in the case of matrix denoising, we expect the properties of this estimator to depend on signal to noise ratio β, and on the 'norm' of the noise Z op (i.e. on the value of the optimization problem Tensor PCA in the case β = 0). For the </p><formula xml:id="formula_13">Z op ≤ µ k (k odd),<label>(9)</label></formula><formula xml:id="formula_14">lim n→∞ Z op = µ k (k even). (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>Further Z op concentrates tightly around its expectation. Namely, for any n, k</p><formula xml:id="formula_16">P Z op -E Z op ≥ s ≤ 2 e -ns 2 /(2k) . (<label>11</label></formula><formula xml:id="formula_17">)</formula><formula xml:id="formula_18">Finally µ k = √ k log k(1 + o k (1)) for large k.</formula><p>For instance, a large order-3 Gaussian tensor should have Z op ≈ 2.87, while a large order 10 tensor has Z op ≈ 6.75. As a simple consequence of Lemma 2.1, we establish an upper bound on the error incurred by the maximum likelihood estimator. Theorem 2. Let µ k be the sequence of real numbers introduced above. Letting v ML denote the maximum likelihood estimator (i.e. the solution of Tensor PCA), we have for n large enough, and all s &gt; 0</p><formula xml:id="formula_19">β ≥ µ k ⇒ Loss( v ML , v 0 ) ≤ 2 β (µ k + s) ,<label>(12)</label></formula><p>with probability at least 1 -2e -ns 2 /(16k) .</p><p>The following upper bound on the value of the Tensor PCA problem is proved using Sudakov-Fernique inequality. While it is looser than Lemma 2.1 (corresponding to the case β = 0), we expect it to become sharp for β ≥ β k a suitably large constant. Lemma 2.2. Under Spiked Tensor Model model, we have</p><formula xml:id="formula_20">lim sup n→∞ E Z op ≤ max τ ≥0 β τ √ 1 + τ 2 k + k √ 1 + τ 2 . (<label>13</label></formula><formula xml:id="formula_21">)</formula><p>Further, for any s ≥ 0,</p><formula xml:id="formula_22">P Z op -E Z op ≥ s ≤ 2 e -ns 2 /(2k) . (<label>14</label></formula><formula xml:id="formula_23">)</formula><p>3 Tensor Unfolding</p><p>A simple and popular heuristics to obtain tractable estimators of v 0 consists in constructing a suitable matrix with the entries of X, and performing PCA on this matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Symmetric noise</head><p>For an integer k ≥ q ≥ k/2, we introduce the unfolding (also referred to as matricization or reshape) operator Mat q : ⊗ k R n → R n q ×n k-q as follows. For any indices i</p><formula xml:id="formula_24">1 , i 2 , • • • , i k ∈ [n]</formula><p>, we let a = 1 + q j=1 (i j -1)n j-1 and b = 1 + k j=q+1 (i j -1)n j-q-1 , and define</p><formula xml:id="formula_25">[Mat q (X)] a,b = X i1,i2,••• ,i k . (<label>15</label></formula><formula xml:id="formula_26">)</formula><p>Standard convex relaxations of low-rank tensor estimation problem compute factorizations of Mat q (X) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref>. Not all unfoldings (choices of q) are equivalent. It is natural to expect that this approach will be successful only if the signal-to-noise ratio exceeds the operator norm of the unfolded noise Mat q (Z) op . The next lemma suggests that the latter is minimal when Mat q (Z) is 'as square as possible' . A similar phenomenon was observed in a different context in <ref type="bibr" target="#b16">[17]</ref>.</p><p>Lemma 3.1. For any integer k/2 ≤ q ≤ k we have, for some universal constant C k ,</p><formula xml:id="formula_27">1 (k -1)! n (q-1)/2 1 - C k n max(q,k-q)) ≤ E Mat q (Z) op ≤ √ k n (q-1)/2 + n (k-q-1)/2 . (<label>16</label></formula><formula xml:id="formula_28">)</formula><p>For all n large enough, both bounds are minimized for q = k/2 . Further</p><formula xml:id="formula_29">P Mat q (Z) op -E Mat q (Z) op ≥ t ≤ 2 e -nt 2 /(2k) .<label>(17)</label></formula><p>The last lemma suggests the choice q = k/2 , which we shall adopt in the following, unless stated otherwise. We will drop the subscript from Mat.</p><p>Let us recall the following standard result derived directly from Wedin perturbation Theorem <ref type="bibr" target="#b23">[24]</ref>, and stated in the context of the spiked model.</p><p>Theorem 3 (Wedin perturbation). Let M = βu 0 w 0 T + Ξ ∈ R m×p be a matrix with u 0 2 = w 0 2 = 1. Let w denote the right singular vector of M. If β &gt; 2 Ξ op , then</p><formula xml:id="formula_30">Loss( w, w 0 ) ≤ 8 Ξ 2 op β 2 . (<label>18</label></formula><formula xml:id="formula_31">)</formula><p>Theorem 4. Letting w = w(X) denote the top right singular vector of Mat(X), we have the following, for some universal constant C = C k &gt; 0, and b ≡ (1/2)( k/2 -1).</p><p>If β ≥ 5 k 1/2 n b then, with probability at least 1 -n -2 , we have</p><formula xml:id="formula_32">Loss w, vec v 0 ⊗ k/2 ≤ C kn 2b β 2 . (<label>19</label></formula><formula xml:id="formula_33">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Asymmetric noise and recursive unfolding</head><p>A technical complication in analyzing the random matrix Mat q (X) lies in the fact that its entries are not independent, because the noise tensor Z is assumed to be symmetric. In the next theorem we consider the case of non-symmetric noise and even k. This allows us to leverage upon known results in random matrix theory <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b4">5]</ref> to obtain: (i) Asymptotically sharp estimates on the critical signal-to-noise ratio; (ii) A lower bound on the loss below the critical signal-to-noise ratio. Namely, we consider observations</p><formula xml:id="formula_34">X = βv 0 ⊗k + 1 √ n G .<label>(20)</label></formula><p>where G ∈ ⊗ k R n is a standard Gaussian tensor (i.e. a tensor with i.i.d. standard normal entries).</p><p>Let w = w( X) ∈ R n k/2 denote the top right singular vector of Mat(X). For k ≥ 4 even, and define b ≡ (k -2)/4, as above. By <ref type="bibr" target="#b17">[18,</ref><ref type="bibr">Theorem 4]</ref>, or [5, Theorem 2.3], we have the following almost sure limits</p><formula xml:id="formula_35">β ≤ (1 -ε)n b ⇒ lim n→∞ w( X), vec(v 0 ⊗(k/2) ) = 0 ,<label>(21)</label></formula><formula xml:id="formula_36">β ≥ (1 + ε)n b ⇒ lim inf n→∞ w( X), vec(v 0 ⊗(k/2) ) ≥ ε 1 + ε . (<label>22</label></formula><formula xml:id="formula_37">)</formula><p>In other words w( X) is a good estimate of v 0 ⊗(k/2) if and only if β is larger than n b .</p><p>We can use w( X) ∈ R 2b+1 to estimate v 0 as follows. Construct the unfolding Mat 1 (w) ∈ R n×n 2b (slight abuse of notation) of w by letting, for i ∈ [n], and j ∈ [n 2b ],</p><formula xml:id="formula_38">Mat 1 (w) i,j = w i+(j-1)n ,<label>(23)</label></formula><p>we then let v to be the left principal vector of Mat 1 (X). We refer to this algorithm as to recursive unfolding.</p><p>Theorem 5. Let X be distributed according to the non-symmetric model <ref type="bibr" target="#b19">(20)</ref> with k ≥ 4 even, define b ≡ (k -2)/4. and let v be the estimate obtained by two-steps recursive matricization.</p><formula xml:id="formula_39">If β ≥ (1 + ε)n b then, almost surely lim n→∞ Loss( v, v 0 ) = 0 . (<label>24</label></formula><formula xml:id="formula_40">)</formula><p>We conjecture that the weaker condition β n (k-2)/4 is indeed sufficient also for our original symmetric noise model, both for k even and for k odd.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Power Iteration</head><p>Iterating over (multi-) linear maps induced by a (tensor) matrix is a standard method for finding leading eigenpairs, see <ref type="bibr" target="#b13">[14]</ref> and references therein for tensor-related results. In this section we will consider a simple power iteration, and then its possible uses in conjunction with tensor unfolding. Finally, we will compare our analysis with results available in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Naive power iteration</head><p>The simplest iterative approach is defined by the following recursion</p><formula xml:id="formula_41">v 0 = y y 2 , and v t+1 = X{v t } X{v t } 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Power Iteration</head><p>The following result establishes convergence criteria for this iteration, first for generic noise Z and then for standard normal noise (using Lemma 2.1). Theorem 6. Assume</p><formula xml:id="formula_42">β ≥ 2 e(k -1) Z op ,<label>(25) y</label></formula><formula xml:id="formula_43">, v 0 y 2 ≥ (k -1) Z op β 1/(k-1) . (<label>26</label></formula><formula xml:id="formula_44">)</formula><p>Then for all t ≥ t 0 (k), the power iteration estimator satisfies Loss(v t , v 0 ) ≤ 2e Z op /β. If Z is a standard normal noise tensor, then conditions (25), (26) are satisfied with high probability provided</p><formula xml:id="formula_45">β ≥ 2ek µ k = 6 k 3 log k 1 + o k (1) ,<label>(27)</label></formula><formula xml:id="formula_46">y, v 0 y 2 ≥ kµ k β 1/(k-1) = β -1/(k-1) 1 + o k (1) .<label>(28)</label></formula><p>In Section 6 we discuss two aspects of this result: (i) The requirement of a positive correlation between initialization and ground truth ; (ii) Possible scenarios under which the assumptions of Theorem 6 are satisfied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Asymptotics via Approximate Message Passing</head><p>Approximate message passing (AMP) algorithms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b3">4]</ref> proved successful in several highdimensional estimation problems including compressed sensing, low rank matrix reconstruction, and phase retrieval <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21</ref>]. An appealing feature of this class of algorithms is that their highdimensional limit can be characterized exactly through a technique known as 'state evolution.' Here we develop an AMP algorithm for tensor data, and its state evolution analysis focusing on the fixed β, n → ∞ limit. Proofs follows the approach of <ref type="bibr" target="#b3">[4]</ref> and will be presented in a journal publication.</p><p>In a nutshell, our AMP for Tensor PCA can be viewed as a sophisticated version of the power iteration method of the last section. With the notation f (x) = x/ x 2 , we define the AMP iteration over vectors v t ∈ R n by v 0 = y, f (v -1 ) = 0, and</p><formula xml:id="formula_47">v t+1 = X{f (v t )} -b t f (v t-1 ) , b t = (k -1) f (v t ), f (v t-1 ) k-2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AMP</head><p>Our main conclusion is that the behavior of AMP is qualitatively similar to the one of power iteration. However, we can establish stronger results in two respects:</p><p>1. We can prove that, unless side information is provided about the signal v 0 , the AMP estimates remain essentially orthogonal to v 0 , for any fixed number of iterations. This corresponds to a converse to Theorem 6. 2. Since state evolution is asymptotically exact, we can prove sharp phase transition results with explicit characterization of their locations.</p><p>We assume that the additional information takes the form of a noisy observation y = γ v 0 + z, where z ∼ N(0, I n /n). Our next results summarize the state evolution analysis. Proposition 5.1. Let k ≥ 2 be a fixed integer. Let {v 0 (n)} n≥1 be a sequence of unit norm vectors v 0 (n) ∈ S n-1 . Let also {X(n)} n≥1 denote a sequence of tensors X(n) ∈ ⊗ k R n generated following Spiked Tensor Model. Finally, let v t denote the t-th iterate produced by AMP, and consider its orthogonal decomposition</p><formula xml:id="formula_48">v t = v t + v t ⊥ ,<label>(29)</label></formula><p>where v t is proportional to v 0 , and v t ⊥ is perpendicular. Then v t ⊥ is uniformly random, conditional on its norm. Further, almost surely</p><formula xml:id="formula_49">lim n→∞ v t , v 0 = lim n→∞ v t , v 0 = τ t ,<label>(30)</label></formula><formula xml:id="formula_50">lim n→∞ v t ⊥ 2 = 1 ,<label>(31)</label></formula><p>where τ t is given recursively by letting τ 0 = γ and, for t ≥ 0 (we refer to this as to state evolution):</p><formula xml:id="formula_51">τ 2 t+1 = β 2 τ 2 t 1 + τ 2 t k-1 .<label>(32)</label></formula><p>The following result characterizes the minimum required additional information γ to allow AMP to escape from those undesired local optima. We will say that {v </p><formula xml:id="formula_52">β &gt; ω k ≡ (k -1) k-1 /(k -2) k-2 ∼ √ ek .</formula><p>Then AMP converges almost surely to a desired local optimum if and only if γ &gt; 1/ k (β) -1</p><p>where k (β) is the largest solution of (1 -</p><formula xml:id="formula_53">) (k-2) = β -2 ,</formula><p>In the special case k = 3, and β &gt; 2, assuming γ &gt; β(1/2 -1/4 -1/β 2 ), AMP tends to a desired local optimum. Numerically β &gt; 2.69 is enough for AMP to achieve v 0 , v ≥ 0.9 if γ &gt; 0.45.</p><p>As a final remark, we note that the methods of <ref type="bibr" target="#b15">[16]</ref> can be used to show that, under the assumptions of Theorem 7, for β &gt; β k a sufficiently large constant, AMP asymptotically solves the optimization problem Tensor PCA. Formally, we have, almost surely,</p><formula xml:id="formula_54">lim t→∞ lim n→∞ X, v t ⊗k -X op = 0. (<label>33</label></formula><formula xml:id="formula_55">)</formula><p>6 Numerical experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Comparison of different algorithms</head><p>Our empirical results are reported in the appendix. The main findings are consistent with the theory developed above:</p><p>• Tensor power iteration (with random initialization) performs poorly with respect to other approaches that use some form of tensor unfolding. The gap widens as the dimension n increases. • All algorithms based on initial unfolding (comprising PSD-constrained PCA and recursive unfolding) have essentially the same threshold. Above that threshold, those that process the singular vector (either by recursive unfolding or by tensor power iteration) have superior performances over simpler one-step algorithms.</p><p>Our heuristic arguments suggest that tensor power iteration with random initialization will work for β n 1/2 , while unfolding only requires β n 1/4 (our theorems guarantee this for, respectively, β n and β n 1/2 ). We plot the average correlation | v, v 0 | versus (respectively) β/n 1/2 and β/n 1/4 . The curve superposition confirms that our prediction captures the correct behavior already for n of the order of 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">The value of side information</head><p>Our next experiment concerns a simultaneous matrix and tensor PCA task: we are given a tensor X ∈ ⊗ 3 R n of Spiked Tensor Model with k = 3 and the signal to noise ratio β = 3 is fixed. In addition, we observe M = λv 0 v 0 T + N where N ∈ R n×n is a symmetric noise matrix with upper diagonal elements i &lt; j iid N i,j ∼ N(0, 1/n) and the value of λ ∈ [0, 2] varies. This experiment mimics a rank-1 version of topic modeling method presented in <ref type="bibr" target="#b0">[1]</ref> where M is a matrix representing pairwise co-occurences and X triples.</p><p>The analysis in previous sections suggests to use the leading eigenvector of M as the initial point of AMP algorithm for tensor PCA on X. We performed the experiments on 100 randomly generated instances with n = 50, 200, 500 and report in Figure <ref type="figure" target="#fig_0">1</ref>  and lim n→∞ v(X), v 0 = 0 otherwise Simultaneous PCA appears vastly superior to simple PCA. Our theory captures this difference quantitatively already for n = 500.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Simultaneous PCA at β = 3. Absolute correlation of the estimated principal component with the truth | v, v 0 |, simultaneous PCA (black) compared with matrix (green) and tensor PCA (blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>the mean values of | v 0 , v(X) | with confidence intervals.Random matrix theory predicts limn→∞ v 1 (M ), v 0 = √ 1 -λ -2 [8]. Thus we can set γ = √ 1 -λ -2and apply the theory of the previous section. In particular, Proposition 5.1 implieslim n→∞ v(X), v 0 = β 1/2 + 1/4 -1/β 2 if γ &gt; β 1/2 -1/4 -1/β 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>matrix case k = 2, this coincides with the largest eigenvalue of Z. Classical random matrix theory shows that -in this case-Z op concentrates tightly around 2<ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b2">3]</ref>.It turns out that tight results for k ≥ 3 follow immediately from a technically sophisticated analysis of the stationary points of random Morse functions by Auffinger, Ben Arous and Cerny<ref type="bibr" target="#b1">[2]</ref>. Lemma 2.1. There exists a sequence of real numbers {µ k } k≥2 , such that</figDesc><table><row><cell>lim sup</cell></row><row><cell>n→∞</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>t } t converges almost surely to a desired local optimum if, Consider the Tensor PCA problem with k ≥ 3 and</figDesc><table><row><cell>lim t→∞</cell><cell>lim n→∞</cell><cell>Loss(v t / v t</cell><cell>2 , v 0 ) ≤</cell><cell>4 β 2 .</cell></row><row><cell>Theorem 7.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Here we write F (n) G(n) if there exists a constant c independent of n (but possibly dependent on n, such that F (n) ≤ c G(n)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Note that, for k even, v0 can only be recovered modulo sign. For the sake of simplicity, we assume here that this ambiguity is correctly resolved.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partially supported by the NSF grant CCF-1319979 and the grants AFOSR/DARPA FA9550-12-1-0411 and FA9550-13-1-0036.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Telgarsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1210.7559</idno>
		<title level="m">Tensor decompositions for learning latent variable models</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random matrices and complexity of spin glasses</title>
		<author>
			<persName><forename type="first">A</forename><surname>Auffinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ben Arous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cerny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="201" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Silverstein</surname></persName>
		</author>
		<title level="m">Spectral Analysis of Large Dimensional Random Matrices</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>nd edition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The dynamics of message passing on dense graphs, with applications to compressed sensing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bayati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="764" to="785" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The singular values and vectors of low rank perturbations of large rectangular random matrices</title>
		<author>
			<persName><forename type="first">Florent</forename><surname>Benaych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Georges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><forename type="middle">Rao</forename><surname>Nadakuditi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="120" to="135" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Local operator theory, random matrices and Banach spaces</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Szarek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook on the Geometry of Banach spaces</title>
		<imprint>
			<publisher>Elsevier Science</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="317" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Message Passing Algorithms for Compressed Sensing</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maleki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="18914" to="18919" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The largest eigenvalues of sample covariance matrices for a spiked population: diagonal case</title>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Péché</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page">73302</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural reconstruction with approximate message passing (neuramp)</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhargava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2555" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A limit theorem for the norm of random matrices</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Probability</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="252" to="261" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Most tensor problems are np-hard</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hillar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On consistency and sparsity for principal components analysis in high dimensions</title>
		<author>
			<persName><forename type="first">I</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">486</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Approximate message passing with consistent parameter estimation and applications to sparse learning</title>
		<author>
			<persName><forename type="first">U</forename><surname>Kamilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2447" to="2455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Shifted power method for computing tensor eigenpairs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mayo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1095" to="1124" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tensor completion for estimating missing values in visual data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Musialski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="208" to="220" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Richard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.4775</idno>
		<title level="m">Non-negative principal component analysis: Message passing algorithms and sharp asymptotics</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Square deal: Lower bounds and improved relaxations for tensor recovery</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference in Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Asymptotics of sample eigenstructure for a large dimensional spiked covariance model</title>
		<author>
			<persName><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1617</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A new convex relaxation for tensor completion</title>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Approximate message passing for bilinear models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schniter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cevher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Workshop Signal Process</title>
		<meeting>Workshop Signal ess</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">68</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Compressive phase retrieval via generalized approximate message passing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Schniter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rangan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communication, Control, and Computing (Allerton), 2012 50th Annual Allerton Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="815" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Statistical performance of convex tensor decomposition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The absolute-value estimate for symmetric multilinear forms. Linear Algebra and its Applications</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Waterhouse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Perturbation bounds in connection with singular value decomposition</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Wedin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BIT Numerical Mathematics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
