<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Statistical Language Models for Information Retrieval A Critical Review</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
							<email>czhai@cs.uiuc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<addrLine>201 N. Goodwin</addrLine>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Statistical Language Models for Information Retrieval A Critical Review</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CBDB8AC4A36BBE1EB3D755FA4BAB6066</idno>
					<idno type="DOI">10.1561/1500000008</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Statistical language models have recently been successfully applied to many information retrieval problems. A great deal of recent work has shown that statistical language models not only lead to superior empirical performance, but also facilitate parameter tuning and open up possibilities for modeling nontraditional retrieval problems. In general, statistical language models provide a principled way of modeling various kinds of retrieval problems. The purpose of this survey is to systematically and critically review the existing work in applying statistical language models to information retrieval, summarize their contributions, and point out outstanding challenges.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of an information retrieval (IR) system is to rank documents optimally given a query so that relevant documents would be ranked above nonrelevant ones. In order to achieve this goal, the system must be able to score documents so that a relevant document would ideally have a higher score than a nonrelevant one.</p><p>Clearly the retrieval accuracy of an IR system is directly determined by the quality of the scoring function adopted. Thus, not surprisingly, seeking an optimal scoring function (retrieval function) has always been a major research challenge in information retrieval. A retrieval function is based on a retrieval model, which formalizes the notion of relevance and enables us to derive a retrieval function that can be computed to score and rank documents.</p><p>Over the decades, many different types of retrieval models have been proposed and tested. A great diversity of approaches and methodology has developed, but no single unified retrieval model has proven to be most effective. Indeed, finding the single optimal retrieval model has been and remains a long-standing challenge in information retrieval research.</p><p>The field has progressed in two different ways. On the one hand, theoretical models have been proposed often to model relevance through inferences; representative models include the logic models <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b114">115]</ref> and the inference network model <ref type="bibr" target="#b108">[109]</ref>. However, these models, while theoretically interesting, have not been able to directly lead to empirically effective models, even though heuristic instantiations of them can be effective. On the other hand, there have been many empirical studies of models, including many variants of the vector space model <ref type="bibr" target="#b88">[89,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b95">96]</ref> and probabilistic models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b109">110,</ref><ref type="bibr" target="#b108">109]</ref>. The vector-space model with heuristic TF-IDF weighting and document length normalization has traditionally been one of the most effective retrieval models, and it remains quite competitive as a state of the art retrieval model. The popular BM25 (Okapi) retrieval function is very similar to a TF-IDF vector space retrieval function, but it is motivated and derived from the 2-Poisson probabilistic retrieval model <ref type="bibr" target="#b83">[84,</ref><ref type="bibr" target="#b85">86]</ref> with heuristic approximations. BM25 is one of the most robust and effective retrieval functions. Another effective retrieval model is divergence from randomness which is based on probabilistic justifications for several term weighting components <ref type="bibr" target="#b0">[1]</ref>.</p><p>While both vector space models and BM25 rely on heuristic design of retrieval functions, an interesting class of probabilistic models called language modeling approaches to retrieval have led to effective retrieval functions without much heuristic design. In particular, the query likelihood retrieval function <ref type="bibr" target="#b79">[80]</ref> with Dirichlet prior smoothing <ref type="bibr" target="#b123">[124]</ref> has comparable performance to the most effective TF-IDF weighting retrieval functions including BM25 <ref type="bibr" target="#b23">[24]</ref>. Due to their good empirical performance and great potential of leveraging statistical estimation methods, the language modeling approaches have been attracting much attention since Ponte and Croft's pioneering paper published in ACM SIGIR 1998 <ref type="bibr" target="#b79">[80]</ref>. Many variations of the basic language modeling approach have since been proposed and studied, and language models have now been applied to multiple retrieval tasks such as crosslingual retrieval <ref type="bibr" target="#b53">[54]</ref>, distributed IR <ref type="bibr" target="#b94">[95]</ref>, expert finding <ref type="bibr" target="#b24">[25]</ref>, passage retrieval <ref type="bibr" target="#b58">[59]</ref>, web search <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b75">76]</ref>, genomics retrieval <ref type="bibr" target="#b128">[129]</ref>, topic tracking <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b98">99]</ref>, and subtopic retrieval <ref type="bibr" target="#b121">[122]</ref>.</p><p>This survey is to systematically review this development of the language modeling approaches. We will survey a wide range of retrieval models based on language modeling and attempt to make connections between this new family of models and traditional retrieval models. We will summarize the progress we have made so far in these models and point out remaining challenges to be solved in order to further increase their impact.</p><p>The survey is written for readers who have already had some basic knowledge about information retrieval. Readers with no prior knowledge about information retrieval will find it more comfortable to read an IR textbook (e.g., <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b62">63]</ref>) first before reading this survey. The readers are also assumed to have already had some basic knowledge about probability and statistics such as maximum likelihood estimator, but a reader should still be able to follow the high-level discussion in the survey even without such background.</p><p>The rest of the survey is organized as follows. In Section 2, we review the very first generation of language models which are computationally as efficient as any other existing retrieval model. The success of these early models has stimulated many follow-up studies and extensions of language models for retrieval. In Section 3, we review work that aims at understanding why these language models are effective and why they can be justified based on relevance. In Section 4, we review work on extending and improving the basic language modeling approach. Feedback is an important component in an IR system, but it turns out that there is some difficulty in supporting feedback with the first generation basic language modeling approach. In Section 5, we review several lines of work on developing and extending language models to support feedback (particularly pseudo feedback). They are among the most effective language models for retrieval. In Section 6, we further review a wide range of applications of language models to different special retrieval tasks where a standard language model is often extended or adapted to better fit a specific application. Finally, in Section 7, we briefly review some work on developing general theoretical frameworks to facilitate systematic applications of language models to IR. We summary the survey and discuss future research directions in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Basic Language Modeling Approach</head><p>In this section, we review the basic language modeling approach (often called the query likelihood scoring method) which represents the very first generation of language models applied to information retrieval. Extensions of these models are reviewed in the next few sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Ponte and Croft's Pioneering Work</head><p>The language modeling approach was first introduced by Ponte and Croft in their SIGIR 98 paper <ref type="bibr" target="#b79">[80]</ref>. In this work, they proposed a new way to score a document, later often called the query likelihood scoring method. The basic idea behind the new approach is simple: first estimate a language model for each document, and then rank documents by the likelihood of the query according to the estimated language model of each document. This new method was shown to be quite effective. They called this approach "language modeling approach" due to the use of language models in scoring.</p><p>The term language model refers to a probabilistic model of text (i.e., it defines a probability distribution over sequences of words). Before it was applied to retrieval, it had already been used successfully in related areas such as speech recognition <ref type="bibr" target="#b38">[39]</ref> and machine translation <ref type="bibr" target="#b10">[11]</ref>. In these applications, language models are used to assess what kind of word sequences are more typical according to language usages, and inject the right bias accordingly into a speech recognition system or machine translation system to prefer an output sequence of words with high probability according to the language model.</p><p>In the basic language modeling approach proposed by Ponte and Croft, the query is assumed to be a sample of words drawn according to a language model estimated based on a document (i.e., a document language model). We will then ask the question: which document language model gives our query the highest probability? Documents can thus be ranked based on the likelihood of generating the query using the corresponding document model. Intuitively, if a document language model gives the query a high probability, the query words must have high probabilities according to the document language model, which further means that the query words occur frequently in the document.</p><p>Formally, the general idea of the query likelihood retrieval function can be described as follows. Let Q be a query and D a document. Let θ D be a language model estimated based on document D. We define the score of document D with respect to query Q as the conditional probability p(Q|θ D ). That is, score(Q, D) = p(Q|θ D ).</p><p>(2.1)</p><p>Clearly in order to use such a model to score documents, we must solve two problems: (1) how to define θ D ? (2) how to estimate θ D based on document D? Note that the definition of θ D is quite critical as it would completely determine how we model the query, thus essentially determining the query representation to be adopted.</p><p>In Ponte and Croft's paper, the model θ D has not been explicitly defined, but the final retrieval function derived suggests that the model is a multiple Bernoulli model. Formally, let V = {w 1 , . . . , w |V | } be the vocabulary of the language of our documents. We can define a binary random variable X i ∈ {0, 1} for each word w i to indicate whether word w i is present (X i = 1) or absent (X i = 0) in the query. Thus model θ D would have precisely |V | parameters, i.e., θ D = {p(X i = 1|D)} i∈ <ref type="bibr">[1,|V |]</ref> , which can model presence and absence of all the words in the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Ponte and Croft's Pioneering Work 143</head><p>According to this model, the query likelihood can be written as:</p><formula xml:id="formula_0">p(Q|θ D ) = w i ∈Q p(X i = 1|D) w i / ∈Q (1 -p(X i = 1|D)),</formula><p>where the first product is for words in the query and the second words not occurring in the query. Computationally, the retrieval problem now boils down to estimating all the |V | parameters (i.e., p(X i = 1|D)) based on D; different ways to estimate the parameters would lead to different retrieval functions.</p><p>In order to estimate the multiple Bernoulli model θ D , we would assume that document D is a sample of θ D . If we are to interpret D as a single bit vector representing the presence and absence of each word, we would not be able to capture term frequency (TF) since a Bernoulli model only models the presence and absence of a word rather than how many times a word occurs. To capture TF, we can treat each word w i in D as a sample from our model where only w i has shown up and all other words are absent. Thus according to the maximum likelihood (ML) estimator, p(X i = 1|D) is equal to the relative frequency of word w i in D, i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>p(X</head><formula xml:id="formula_1">i = 1|D) = c(w i , D) |D| ,</formula><p>where c(w i , D) is the count of word w i in D and |D| is the length of D (i.e., the total word counts). This derivation is discussed in detail in <ref type="bibr" target="#b68">[69]</ref>. One problem with this ML estimator is that an unseen word in document D would get a zero probability, making all queries containing an unseen word have zero probability p(Q|θ D ). This is clearly undesirable. More importantly, since a document is a very small sample for our model, the ML estimate is generally not accurate. So an important problem we have to solve is to smooth the ML estimator so that we do not assign zero probability to unseen words and can improve the accuracy of the estimated language model in general.</p><p>In Ponte and Croft's model, they set the probability of an unseen word to that of the word in the whole collection of documents. Intuitively, this is to say that if we do not observe a word in the document, we would assume that the probability of such a word is the same as the probability of seeing the word in any document in the whole collection. This ensures that none of the words in the collection would get a zero probability. To further improve the robustness of smoothing, they also heuristically take the geometric mean of the ML estimate and the average term frequency in all other documents in the collection <ref type="bibr" target="#b79">[80]</ref>.</p><p>Ponte and Croft's work makes two important contributions in studying retrieval models: First, it introduces a new effective probabilistic ranking function based on query likelihood with smoothed estimate of model parameters. While the previous probabilistic models (e.g., <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b82">83]</ref>) have failed to directly lead to an empirically effective retrieval function due to the difficulty in estimating parameters,<ref type="foot" target="#foot_0">1</ref> this new query likelihood retrieval model makes the parameter estimation problem easier to solve (see Section 3 for more discussion about this). Second, it connects the difficult problem of text representation and term weighting in IR with the language modeling techniques that have been wellstudied in other application areas such as statistical machine translation and speech recognition, making it possible to exploit various kinds of language modeling techniques to address the representation problem. Such a connection was actually recognized in some early work, but no previous work has looked into the problem of how to estimate such a model accurately. For example, Wong and Yao <ref type="bibr" target="#b113">[114]</ref> proposed to use a multinomial model to represent a document, but they just used the ML estimator and did not further study the estimation problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">BBN and Twenty-One in TREC-7</head><p>At about the same time and apparently independent of Ponte and Croft's work, two TREC-7 participating groups, BBN <ref type="bibr" target="#b69">[70]</ref> and Twenty-One <ref type="bibr" target="#b33">[34]</ref>, have used the same idea of scoring documents with querylikelihood but with a slightly different definition of the actual model. Instead of using multiple Bernoulli, these two groups used a multinomial distribution model, which is more commonly called a unigram language model. Such a model directly models the counts of terms and is more common in other applications such as speech recognition than the multiple Bernoulli; the latter was more popular for retrieval and was already used in an earlier probabilistic retrieval model <ref type="bibr" target="#b82">[83]</ref>. Both groups have achieved very good empirical performance in the TREC-7 evaluation using their new models.</p><p>Specifically, these two groups define θ D as a unigram language model or multinomial word distribution, i.e., θ</p><formula xml:id="formula_2">D = {p(w i |D)} i∈[1,|V |] ,</formula><p>where p(w i |D) is the probability of word w i . Note that as in the previous section, we use θ D to denote a probability distribution and p(w|D) to denote the probability of a word according to the distribution θ D . However, unlike in multiple Bernoulli, where our constraint is p(</p><formula xml:id="formula_3">X i = 1|D) + p(X i = 0|D) = 1, here our constraint is |V | i=1 p(w i |D) = 1.</formula><p>According to such a model, the likelihood of a query Q = q 1 ...q m , where q i is a query word, would be</p><formula xml:id="formula_4">p(Q|θ D ) = m i=1 p(q i |D).</formula><p>For example, a document language model θ D might assign a probability of 0.1 to the word "computer" and 0.05 to the word "virus" (i.e., p(computer|D) = 0.1, p(virus|D) = 0.05). If our query Q is "computer virus," we would have p(Q|θ D ) = 0.1 * 0.05 = 0.005. Thus intuitively, the more frequently a query word occurs in document D, the higher the query likelihood would be for D, capturing the basic TF retrieval heuristic <ref type="bibr" target="#b23">[24]</ref>.</p><p>As in the case of multiple Bernoulli, the retrieval problem is now reduced to the problem of estimating the language model θ D (i.e., p(w|D) for each word w). Once again, the issue of smoothing the ML estimate is critical. In both groups' work, θ D is smoothed by interpolating the ML estimate with a background language model estimated using the entire collection:</p><formula xml:id="formula_5">p(w|D) = (1 -λ) c(w, D) |D| + λp(w|C),</formula><p>where p(w|C) is a collection (background) language model estimated based on word counts in the entire collection and λ ∈ [0, 1] is a smoothing parameter.</p><p>The two groups used a slightly different formula for estimating p(w|C); BBN used the normalized total counts of a word in the collection while Twenty-One used the normalized total number of documents containing a word <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b69">70]</ref>. The general idea is similar, though, and it is also similar to what Ponte and Croft used in their estimate of θ D .</p><p>These two groups also went beyond the basic query likelihood scoring formula to introduce a document prior p(D) using Bayes formula, thus suggesting that we essentially score a document based on the conditional probability p(D|Q):</p><formula xml:id="formula_6">p(D|Q) = p(Q|D)p(D) p(Q) ∝ p(Q|D)p(D). (2.2)</formula><p>Note that in this derivation, we have not specified how to interpret p(Q|D). One way is to interpret it as p(Q|θ D ), which would give us precisely the basic query likelihood scoring formula originally introduced in <ref type="bibr" target="#b79">[80]</ref>. However, other interpretations may also be possible (e.g., the translation model <ref type="bibr" target="#b3">[4]</ref>).</p><p>The document prior p(D) (which should be distinguished from p(θ D )) can be useful for introducing additional retrieval criteria to favor documents with certain features, and indeed has been explored in <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b57">58]</ref>. This prior presumably can also be added to the query likelihood formula proposed by Ponte and Croft. Thus this formulation is a more general formulation of the basic language modeling approach than the query likelihood retrieval function proposed by Ponte and Croft. A similar formulation was also given in <ref type="bibr" target="#b74">[75]</ref>, where Ng also discussed other issues including how to estimate the smoothing parameter with pseudo feedback.</p><p>There are two additional points worth mentioning: First, the BBN group presented their model as a Hidden Markov Model (HMM) <ref type="bibr" target="#b81">[82]</ref>; the HMM makes smoothing more explicit and also has the potential to accommodate more sophisticated models, particularly combining different representations <ref type="bibr" target="#b69">[70]</ref>. Second, the Twenty-One group revealed the connection of their language model with traditional retrieval heuristics such as TF-IDF weighting <ref type="bibr" target="#b95">[96]</ref> and document length normalization <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b29">30]</ref>, which offers an intuitive justification for this new retrieval model. A more general derivation of this connection can be found in <ref type="bibr" target="#b123">[124]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Variants of the Basic Language Modeling Approach</head><p>The basic language modeling approach (i.e., the query likelihood scoring method) can be instantiated in different ways by varying (1) θ D (e.g., multiple Bernoulli or multinomial), (2) estimation methods of θ D (e.g., different smoothing methods), or (3) the document prior p(D). Indeed, this has led to many variants of this basic model, which we now briefly review.</p><p>Although the original work by Ponte and Croft used the multiple Bernoulli model, it has not been as popular as the multinomial model. One reason may be because the latter can capture the term frequency in documents (as well as the query) more naturally than the former; indeed, the multiple Bernoulli model clearly ignores query term frequencies and is also somewhat unnatural to incorporate TF in the documents. Note that both models make some independence assumption about term occurrences, but their assumptions are different. In multiple Bernoulli model, the presence/absence of a term is assumed to be independent of that of other terms, whereas in multinomial model, every word occurrence is assumed to be independent, including the multiple occurrences of the same term. Since once an author starts to use a term in a document, the author tends to use the term again, treating multiple occurrences of the same term as independent can cause "over counting" of the occurrences of a term. Recently some new models (e.g., Dirichlet compound multinomial <ref type="bibr" target="#b61">[62]</ref>) have been proposed to address this problem and model word burtiness; they may potentially lead to better retrieval models. The multiple Bernoulli model and multinomial model were compared empirically earlier in the context of text categorization <ref type="bibr" target="#b63">[64]</ref> and later in the context of query likelihood for retrieval <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b96">97]</ref>. The current conclusions seem to be that multinomial model is better, but more evaluation is needed to make more definitive conclusions.</p><p>Recently, a Poisson model was proposed as an alternative for the query likelihood retrieval function <ref type="bibr" target="#b64">[65]</ref> and some promising results have been achieved. One potential advantage of multiple Bernoulli over multinomial is the possibility of naturally smoothing the model for each term independently (because each term is treated as an independent event),<ref type="foot" target="#foot_1">2</ref> which provides flexibility for optimizing smoothing at a perterm basis, while multinomial can naturally capture term frequencies in the query, which are ignored in multiple Bernoulli. Poisson model can accommodate both flexible smoothing and modeling term frequencies, making it a very interesting model to further study.</p><p>Another variation is to relax the independence assumptions made in the basic model to capture some limited dependency such as through bigram language models. We will review this line of work and other extensions in Section 4.</p><p>Estimation of θ D is quite critical for this family of models, and a particularly important issue is how to smooth the maximum likelihood estimate which assigns zero probability to unseen words. Many different smoothing methods have been used. In addition to those mentioned earlier in our discussion, there are many other smoothing methods that can be applied (see, e.g., <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b44">45]</ref>). Zhai and Lafferty <ref type="bibr" target="#b123">[124]</ref> empirically compared three different smoothing methods, including Jelinek-Mercer (fixed coefficient interpolation) <ref type="bibr" target="#b39">[40]</ref>, Dirichlet prior <ref type="bibr" target="#b60">[61]</ref>, and absolute discounting <ref type="bibr" target="#b73">[74]</ref>, on several standard test collections. Most of these smoothing methods end up interpolating the original maximum likelihood estimate with the collection background language model in some way. Despite this similarity, different smoothing methods can perform differently. It was found that in general, Dirichlet prior smoothing works the best, especially for keyword queries (nonverbose queries). The reason may be because it adjusts the amount of smoothing according to the length of a document in a reasonable way (longer documents get less smoothing). Furthermore, interpolation-based smoothing methods all work better than backoff smoothing methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b43">44]</ref>, even though the latter works well for speech recognition, which is likely due to the lack of an IDF effect in backoff smoothing. This point will be further elaborated in Section 3.2.</p><p>The Dirichlet prior smoothing method can be derived by using Bayesian estimation (instead of ML estimation) with a Dirichlet conjugate prior <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b124">125]</ref>, and the formula is as follows:</p><formula xml:id="formula_7">p(w|D) = c(w, D) + µp(w|C) |D| + µ ,</formula><p>where p(w|C) is a background (collection) language model and µ is a smoothing parameter, which can be interpreted as the total number of pseudo counts of words introduced through the prior. The Dirichlet prior smoothing method has now become a very popular smoothing method for smoothing language models in an IR task.</p><p>The study by Zhai and Lafferty has also shown that retrieval performance can be quite sensitive to the setting of smoothing parameters and suggested that smoothing plays two different roles in the query likelihood retrieval formula, an issue we will further discuss later.</p><p>All these smoothing methods discussed so far are simple in the sense that different documents are smoothed using the same collection language model. Intuitively, each document can have its own "reference language model" for smoothing, and there has been work done in this direction. We will review this line of work in Section 4.</p><p>The document prior p(D) can naturally incorporate any static ranking preferences of documents (i.e., ranking preferences independent of a query) such as PageRank scores or other document features. In this line of the work, Kraaij and coauthors <ref type="bibr" target="#b46">[47]</ref> successfully leveraged this prior to implement an interesting Web search heuristic for named page finding. Their idea is to prefer pages with shorter URLs since an entry page tends to have a shorter URL. They used some training data to estimate the prior p(D) based on URL lengths, and showed that this prior can improve search performance significantly <ref type="bibr" target="#b46">[47]</ref>. Li and Croft <ref type="bibr" target="#b57">[58]</ref> studied how to leverage the document prior p(D) to implement time-related preferences in retrieval so that a document with a more recent time would be preferred. This strategy has been shown to be effective for a particular set of "recency queries." In a study by Kurland and Lee <ref type="bibr" target="#b48">[49]</ref>, a PageRank score computed using induced links between documents based on document similarity has been used as a prior to improve retrieval accuracy. In <ref type="bibr" target="#b131">[132]</ref> priors to capture document quality are shown to be effective for improving the accuracy of the top-ranked documents in ad hoc web search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Summary</head><p>In this section, we reviewed the basic language modeling approach, which is roughly characterized by the use of query likelihood for scoring and simple smoothing methods based on a background collection language model. Such a basic approach (especially with Dirichlet prior smoothing) has been shown to be as effective as well-tuned existing retrieval models such as pivoted length normalization and BM25 <ref type="bibr" target="#b23">[24]</ref>. Retrieval functions in this basic language modeling approach can generally be computed as efficiently as any standard TF-IDF retrieval model with the aid of an inverted index. <ref type="foot" target="#foot_2">3</ref>Although the query likelihood retrieval method has performed well empirically, there were questions raised regarding its foundation as a retrieval model, particularly its connection with the key notion in retrieval -relevance <ref type="bibr" target="#b97">[98]</ref>. Indeed, none of the early work has provided a rigorous treatment of the language model θ D , nor has it provided a solid connection between query likelihood and relevance. Thus it is unclear how we should interpret θ D : is it a model for documents or for queries? One may also question whether such models have just happened to perform well, but without any solid relevance-based foundation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Relevance-based Justification for Query Likelihood</head><p>The superficial justification based on Equation (2.2) suggests the following relevance-based interpretation: Suppose that there is precisely one relevant document, and the observed query has been "generated" using that relevant document. We can then use the Bayes' Rule to infer which document is "that relevant document" based on the observed query. This leads to Equation (2.2), which boils down to scoring with P (Q|D) under the assumption of a uniform prior p(D). Unfortunately, such a "single relevant document" formulation raises many questions as discussed in <ref type="bibr" target="#b97">[98]</ref>.</p><p>To better understand the retrieval foundation of the query likelihood method, Lafferty and Zhai <ref type="bibr" target="#b50">[51]</ref> offered a more general relevance-based derivation of the query likelihood method. Specifically, they show that the query likelihood retrieval function can be justified in a similar way as the classical probabilistic retrieval model based on the probability ranking principle <ref type="bibr" target="#b84">[85]</ref>.</p><p>The starting point of the derivation is the conditional probability p(R = 1|Q, D) (R ∈ {0, 1} is a binary relevance random variable) which is the probability that document D is relevant to query Q. The probability ranking principle provides a justification for ranking documents for a query based on this conditional probability. This is equivalent to ranking documents based on the odds ratio, which can be further transformed using Bayes' Rule:</p><formula xml:id="formula_8">O(R = 1|Q, D) = p(R = 1|Q, D) p(R = 0|Q, D) ∝ p(Q, D|R = 1) p(Q, D|R = 0) . (<label>3.1)</label></formula><p>At this point, we may decompose the joint probability P (Q, D|R) in two different ways:</p><p>(1) document generation: p(Q, D|R) = p(D|Q, R)p(Q|R), and (2) query generation: p(Q, D|R) = p(Q|D, R)p(D|R).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>With document generation, we have</head><formula xml:id="formula_9">O(R = 1|Q, D) ∝ p(D|Q, R = 1) p(D|Q, R = 0) .</formula><p>Thus the ranking is equivalent to the ranking given by the classical Robertson-Sparck-Jones probabilistic model <ref type="bibr" target="#b82">[83]</ref> if we define P (D|Q, R) as multiple Bernoulli models <ref type="bibr" target="#b50">[51]</ref>: See <ref type="bibr" target="#b87">[88]</ref> for an in-depth discussion of other variants such as multinomial and Poisson as well as the relationship between different variants. With query generation, we have</p><formula xml:id="formula_10">O(R = 1|Q, D) ∝ p(Q|D, R = 1) p(Q|D, R = 0) p(R = 1|D) p(R = 0|D</formula><p>) .</p><p>If we make the assumption that p(Q|D, R = 0) = p(Q|R = 0) (i.e., the distribution of "nonrelevant queries" does not depend on the particular document, which is not a very strong assumption), we obtain</p><formula xml:id="formula_11">O(R = 1|Q, D) ∝ p(Q|D, R = 1) p(R = 1|D) p(R = 0|D) .</formula><p>The term p(R=1|D) p(R=0|D) can be interpreted as a prior of relevance on a document, which can be estimated based on additional information such as the links pointing to D in a hypertext collection. Without such extra knowledge, we may assume that this term is the same across all the documents, which gives the following relation <ref type="bibr" target="#b50">[51]</ref> <ref type="foot" target="#foot_3">1</ref> </p><formula xml:id="formula_12">O(R = 1|Q, D) ∝ p(Q|D, R = 1),</formula><p>which provides a relevance-based justification for the query likelihood scoring method.</p><p>Note that the query likelihood derived above, i.e., p(Q|D, R), has an additional relevance variable R in the conditional probability, which is essential to obtain a relevance-based justification for query likelihood scoring and gives a clear interpretation of the θ D mentioned earlier. Specifically, it suggests that the probability p(Q|D) used in all the query likelihood scoring methods should be interpreted as p(Q|D, R = 1), which intuitively means the probability that a user who likes document D would pose query Q.</p><p>Clearly this does not mean that the user can only like one document, thus there is no concern about the "single relevant document." Furthermore, this also suggests that θ D is really a model for what kind of queries would be posed if a user wants to retrieve document D. When we estimate θ D using a document D as observation, we are essentially using words in D to approximate the queries a user might pose to retrieve D, which is reasonable but not ideal. For example, anchor text describing a link to document D can be a better approximation of the queries a user might pose to retrieve D, and can thus be leveraged to improve the estimate of θ D as explored in <ref type="bibr" target="#b72">[73]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Query Likelihood, Smoothing, and TF-IDF Weighting</head><p>In another line of work attempting to understand why query likelihood scoring is effective as a retrieval method, Zhai and Lafferty studied the robustness of query likelihood scoring and examined how retrieval performance is affected by different strategies for smoothing <ref type="bibr" target="#b120">[121,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b125">126]</ref>. Through comparing several different smoothing methods, they have observed: (1) retrieval performance is sensitive to the setting of smoothing parameters and the choice of smoothing methods;</p><p>(2) the sensitive patterns are different for keyword queries (all words are content-carrying keywords) and verbose queries (queries are sentences describing the information need, thus contain many common nondiscriminative words).</p><p>The first observation suggests that while heuristic term weighting in traditional retrieval models has been replaced with language model estimation (particularly smoothing) in the query likelihood approach, we have not been able to escape from the need for heuristic tuning of parameters since nonoptimal smoothing can degrade retrieval performance significantly. However, compared with TF-IDF weighting parameters, a smoothing parameter is more meaningful from the view point of statistical estimation. Indeed, completely automatic tuning of the smoothing parameters is shown to be possible in <ref type="bibr" target="#b124">[125]</ref> and the performance with automatic parameter setting is comparable to the optimal performance achieved through manual tuning.</p><p>The second observation suggests that smoothing plays two distinct roles in the query likelihood scoring methods: one obvious role is to address the data sparseness issue (since a document is a small sample) and improve the accuracy of the estimated language model; the other nonobvious role is to model the noisy (nondiscriminative) words in the query. It is conjectured that it is this second role that has caused the different sensitivity patterns for keyword and verbose queries; indeed since the modeling of noise in queries is much more critical for verbose queries than keyword queries, it is not surprising that additional smoothing is often needed (for the second role) to achieve optimal performance for verbose queries than keyword queries as observed in <ref type="bibr" target="#b125">[126]</ref>.</p><p>This second role of smoothing is also closely related to a general connection between smoothing with a background language model and the IDF effect in the query likelihood scoring formula. In <ref type="bibr" target="#b123">[124]</ref>, it is shown that if we smooth a document language model with a general smoothing scheme where an unseen word w in document D would have a probability proportional to the probability of the word given by a collection language model (i.e. p(w|D) = α D p(w|C) with a parameter α D to control the amount of smoothing), the query likelihood scoring function can be rewritten as follows:</p><formula xml:id="formula_13">log p(Q|D) =   i:c(q i ,D)&gt;0 log p s (q i |D) α D p(q i |C)   + m log α D + m i=1 log p(q i |C),</formula><p>where p s (q i |D) is the smoothed probability of a seen query word q i and m is the query length.</p><p>Since the last term does not affect ranking, it can be ignored for ranking. As a result, we see that the formula essentially involves a sum of term weights over all the matched query terms in the document, just as in any other traditional retrieval function. Moreover, each matched term contributes a TF-IDF like weight. In this sense, the query likelihood retrieval function simply offers an alternative way of implementing TF-IDF weighting and document length normalization heuristics. In particular, the IDF effect is achieved through having p(q i |C) in the denominator of the weighting function. This means that through smoothing, we implicitly penalize words that are common in the collection (with high p(q i |C)). This also explains why we can model the noise in the query through more aggressive smoothing. See <ref type="bibr" target="#b124">[125]</ref> for more discussion about this.</p><p>The equation above also shows that computing the query likelihood scoring function using any smoothing method based on a collection language model is as efficient as computing a traditional retrieval function such as the pivoted length normalization function of the vector space model <ref type="bibr" target="#b95">[96]</ref> with an inverted index. Indeed, the query likelihood retrieval function with several different smoothing methods has been implemented in this way in the Lemur toolkit (http://www.lemurproject.org/), which is the main retrieval toolkit currently available for experimenting with language modeling retrieval approaches.</p><p>These understandings provide an empirical explanation for why the query likelihood retrieval function is reasonable from retrieval perspective in the sense that it can be regarded as just another way of implementing TF-IDF weighting and document length normalization. They also suggest that the implementation of IDF heuristic in this approach is not as direct as in a traditional model, leading some researchers to have explored alternative ways to incorporate IDF <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improving the Basic Language Modeling Approach</head><p>In Section 2, we restricted the discussion to the family of models that use simple smoothing methods based on a background language model; their efficiency is comparable to any traditional TF-IDF model. In this section, we review some improvements to the basic language modeling approach, which often outperform the basic approach, but also tend to demand significantly more computation than the basic approach. All these improvements remain in the family of query-likelihood scoring, which distinguishes them from the other models to be reviewed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Beyond Unigram Models</head><p>A natural extension of the basic query likelihood method is to go beyond unigram language models. Unlike unigram language models where the occurrences of words are assumed to be completely independent (an assumption obviously not holding), these models can capture some dependency between words. For example, Song and Croft <ref type="bibr" target="#b96">[97]</ref> have studied using bigram and trigram language models. In a bigram language model, the generation of a current word would be dependent on the previous word generated, thus it can potentially capture the dependency of adjacent words (e.g., phrases). Specifically, the query likelihood would be</p><formula xml:id="formula_14">p(Q|D) = p(q 1 |D) m i=2 p(q i |q i-1 , D),</formula><p>where p(q i |q i-1 , D) is the conditional probability of generating q i after we have just generated q i-1 .</p><p>Such n-gram models capture dependency based on word positions. Other work has attempted to capture dependency based on grammar structures <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b99">100,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b100">101]</ref>. In all these approaches, the retrieval formula eventually boils down to some combination of scores from matching units larger than single words (e.g., bigrams, headmodifier pairs, or collocation pairs). While these approaches have mostly shown benefit of capturing dependencies, the improvement tends to be insignificant or at least not so significant as some other extensions that can achieve some kind of pseudo feedback effect. (These other extensions will be reviewed in the next section.) One reason for these nonexciting results may be because as we move to more complex models to capture dependency, our data becomes even more sparse, making it difficult to obtain accurate estimation of the model. The general observation on these models is consistent with what researchers have observed on some early effort on applying natural language processing techniques to improve indexing, notably phrase-based indexing <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b119">120]</ref>.</p><p>A more successful retrieval model that can capture limited dependencies is the Markov Random Field model proposed in <ref type="bibr" target="#b67">[68]</ref>. This model is a general discriminative model where arbitrary features can be combined in a retrieval function. In most of the applications of such a model, the features are typically the scores of a document with respect to a query using an existing retrieval function such as the query likelihood, thus the Markov Random Field model essentially serves as a way to combine multiple scoring strategies and scoring with multiple representations. In particular, it has been shown that one can combine unigram language modeling scoring with bigram scoring as well as scoring based on word collocations within a small window of text. Such a combination achieves better retrieval accuracy than using only unigram scoring <ref type="bibr" target="#b67">[68]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cluster-based Smoothing and Document Expansion</head><p>Smoothing every document with the same collection language model is intuitively not optimal since we essentially assume that all the unseen words in different documents would have similar probabilities. Ideally, we should use some document-dependent "augmented text data" that can more accurately reflect the content of the document under consideration. With such reasoning, several researchers have attempted to exploit the corpus structure to achieve such document-specific smoothing.</p><p>The work in this line can be grouped into two categories: (1) Cluster documents and smooth a document with the cluster containing the document. (2) For each document, obtain the most similar documents in the collection and then smooth the document with the obtained "neighbor documents."</p><p>In Liu and Croft <ref type="bibr" target="#b59">[60]</ref>, documents are clustered using a cosine similarity measure, and each document is smoothed with the cluster containing the document by interpolating the original maximum likelihood estimate p(w|D) with a cluster language model p(w|Cluster), which is further smoothed by interpolating itself with a collection language model p(w|C). Such a model is shown to outperform the baseline smoothing method that only uses the collection language model for smoothing. However, the improvement is mostly insignificant. One possible reason may be because the two roles of smoothing have been mixed, thus if the parameters are not set appropriately, then smoothing using cluster-based language model may actually end up penalizing terms common in the cluster due to the IDF effect of smoothing, thus lowering the scores of documents matching terms in the cluster. In Kurland and Lee <ref type="bibr" target="#b47">[48]</ref>, the authors presented a general strategy for exploiting the cluster structure to achieve an effect similar to smoothing document language models with cluster language models; document language models are not explicitly smoothed with a cluster language model, but a document is scored based on a weighted combination of its regular query likelihood score with the likelihood of the query given the clusters containing the document.</p><p>A soft clustering strategy has been adopted to smooth document language models through using the Latent Dirichlet Allocation (LDA) model to do clustering <ref type="bibr" target="#b112">[113]</ref>. With this model, we allow a document to be in multiple topics (roughly like document clusters, but characterized by unigram language models) with some uncertainties. Thus smoothing of a document can involve an interpolation of potentially many clusters; this is different from <ref type="bibr" target="#b59">[60]</ref>, where just one cluster is used for smoothing. Results reported in <ref type="bibr" target="#b112">[113]</ref> are quite encouraging.</p><p>A problem with smoothing a document using a cluster is that the cluster is not necessarily a good representation of similar documents to the document to be smoothed. This is clearly the case when the document is at the boundary of the cluster. To address this problem, Tao and others <ref type="bibr" target="#b105">[106]</ref> proposed to construct a document-specific "neighborhood" in the document space, essentially to form a cluster for each document with the document at the center of the cluster. Intuitively, such a neighborhood contains the documents that are most similar to the document, thus serves well for smoothing. To further improve the robustness of the smoothing method, the authors assign weights to the neighbors based on a cosine similarity measure so that a document farther away would contribute less to smoothing. They then use the probabilistic neighborhood to smooth the count of a word by interpolating the original count in the document with a weighted sum of counts of the word in the neighbor documents to obtain a smoothed count for each word. Such smoothed counts thus represent an "expanded document," and are then used as if they were the true counts of the words in the document for further smoothing with a collection language model. Experiment results show that such a document expansion method not only outperforms the baseline simple smoothing method (i.e., with only a collection language model), but also outperforms the cluster-based smoothing method proposed in <ref type="bibr" target="#b59">[60]</ref>. Moreover, it can be combined with pseudo feedback to further improve performance <ref type="bibr" target="#b105">[106]</ref>.</p><p>In <ref type="bibr" target="#b92">[93]</ref>, this neighborhood-based document expansion method is further extended to allow for smoothing with remotely related documents through probabilistic propagation of term counts. This new smoothing method is shown to outperform the simple smoothing methods using a collection language model. It also achieves consistently better precision in the top-ranked documents than both cluster-based and document expansion smoothing methods. But interestingly, it has a worse mean average precision than the latter, indicating room for further research to improve this smoothing method.</p><p>A general optimization framework, which covers mostly all the work mentioned above as special cases, is proposed in <ref type="bibr" target="#b66">[67]</ref>. In this work, a general objective function is explicitly defined for smoothing language models over graph structures, thus offering a general principled framework for smoothing. Several new smoothing methods have been derived using the framework with some outperforming the state of the methods <ref type="bibr" target="#b66">[67]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parsimonious Language Models</head><p>All the methods for smoothing discussed so far end up interpolating counts of words in various documents, thus the estimated document language model generally assigns high probabilities to frequent words including stop words. From retrieval perspective, we would like our model to be more discriminative (i.e., IDF heuristic). While smoothing with a collection language model can achieve the needed discrimination indirectly, one may also attempt to do it more directly. Motivated by this reasoning, a "distillation" strategy with a two-component mixture model was proposed in <ref type="bibr" target="#b120">[121]</ref>, where a query or a document is assumed to be generated from a mixture model involving two components: one is a fixed background (collection) language model and the other a topic language model to be estimated. If we estimate the topic language model by fitting such a two-component mixture model to some text sample (e.g., query or document), the common words would be easily "explained" by the background model; as a result, the estimated topic model would be more discriminative and tend to assign high probabilities to content-carrying words which do not have high probabilities according to the background model. The query distillation experiments in <ref type="bibr" target="#b120">[121]</ref> have shown positive results from using the distillation strategy. Such a distillation strategy was further generalized in <ref type="bibr" target="#b34">[35]</ref> to be used in all stages of retrieval, including indexing stage, query stage, and feedback stage. In all cases, the basic idea is to use a background language model to "factor out" the nondiscriminative "background words" from a language model. The authors call such language models parsimonious language models. Unfortunately, such parsimonious models have not shown significant improvement in retrieval accuracy, though they can be useful for reducing the index size <ref type="bibr" target="#b34">[35]</ref>. This result appears to be counter-intuitive. There could be two possible explanations: (1) The current language modeling retrieval approach already has some "builtin" IDF heuristic (i.e., through interpolation with a collection language model), so making the estimated model more discriminative would not add more benefit. (2) There may be complicated interactions between smoothing and term weighting, so with a more discriminative language model, we may need to adjust smoothing accordingly as well. Further study of such models is needed to understand their potential better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Full Bayesian Query Likelihood</head><p>In all the work we have discussed so far, we estimate θ D using a point estimator, which means we obtain our best guess of θ D . Intuitively, there are uncertainties associated with our estimate, and our estimate may not be accurate. A potentially better method is thus to consider this uncertainty and use the posterior distribution of θ D (i.e., p(θ D |D)) to compute the query likelihood. Such a full Bayesian treatment was proposed and studied in Zaragoza and others <ref type="bibr" target="#b118">[119]</ref>.</p><p>Their new scoring function is</p><formula xml:id="formula_15">p(Q|D) = p(Q|θ D )p(θ D |D)dθ D .</formula><p>The regular query likelihood scoring formula can be seen as a special case of this more general query likelihood when we assume that p(θ D |D) is entirely concentrated at one single point.</p><p>Although the integral looks intimidating, it actually has a closed form solution when we use a conjugate prior for computing the posterior distribution p(θ D |D), making it relatively efficient to compute this likelihood. Indeed, the scoring formula is not much more expensive than a scoring formula using simple smoothing <ref type="bibr" target="#b118">[119]</ref>.</p><p>Unfortunately, empirical evaluation shows that this new model, while theoretically very interesting, does not outperform the simple query likelihood function significantly. However, when this new model is combined with linear interpolation smoothing, the performance is better than any other combinations of existing smoothing methods. This may suggest that the new model cannot model the query noise very well, thus it can be substantially improved when it is combined with the linear interpolation smoothing to obtain the extra smoothing needed for modeling query noise. As the authors pointed out, it would be interesting to further study how to model the query noise using a full Bayesian model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Translation Model</head><p>The work mentioned so far is all essentially based on the same query likelihood scoring function which performs the retrieval task through exact keyword matching in a way similar to a traditional retrieval model. In order to allow inexact matching of semantically related words and address the issues of synonym and polysemy, Berger and Lafferty proposed a very important extension to the basic exact matching query likelihood function by allowing the query likelihood to be computed based on a translation model of the form p(u|v), which gives the probability that word v can be "semantically translated" to word u <ref type="bibr" target="#b3">[4]</ref>.</p><p>Formally, in this new model, the query likelihood is computed in the following way:</p><formula xml:id="formula_16">p(Q|D) = m i=1 w∈V p(q i |w)p(w|D),</formula><p>where p(q i |w) is the probability of "translating" word w into q i . This translation model can be understood by imagining a user who likes document D would formulate a query in two steps. In the first, the user would sample a word from document D; in the second, the user would "translate" the word into possibly another different but semantically related word.</p><p>It is easy to see that if p(q i |w) only allows a word to be translated into itself, we would recover the simple exact matching query likelihood. In general, of course, p(q i |w) would allow us to translate w to other semantically related words by giving those other words a nonzero probability. This enables us to score a document by counting the matches between a query word and a different but semantically related word in the document.</p><p>A major challenge here is how to obtain the translation model p(q i |w). The best training data for estimating this translation model would be many relevance judgments for all the documents. Unfortunately we generally do not have such training data available. As an approximation, Berger and Lafferty used a heuristic method to generate some synthetic query-document pairs for training the translation model. Using this method, they have shown that the translation model can improve retrieval performance significantly over the baseline exact matching query likelihood <ref type="bibr" target="#b3">[4]</ref>.</p><p>An alternative way of estimating the translation model based on document titles was proposed in <ref type="bibr" target="#b41">[42]</ref>, which has also been shown to be effective. Furthermore, WordNet and co-occurrences of words have been exploited to define the translation model p(q i |w) in <ref type="bibr" target="#b13">[14]</ref>, and improvement of performance is observed.</p><p>Another challenge in using such a model in a practical system is how to improve the scoring efficiency as we now have to consider many other words for possible matchings with each query word. Indeed, evaluation of this method in TREC-8 has revealed that there are significant challenges in handling efficiently the large number of parameters and scoring all the documents <ref type="bibr" target="#b4">[5]</ref>.</p><p>Despite these challenges, the translation model provides a principled way of achieving "semantic smoothing" and enables semantic matching of related words. It thus makes an important contribution in extending the basic query likelihood retrieval model. Such a model has later been used successfully in applying language models to cross-lingual information retrieval <ref type="bibr" target="#b117">[118]</ref>.</p><p>The cluster-based query likelihood method proposed in <ref type="bibr" target="#b47">[48]</ref> can also be regarded as a form of a translation model where the whole document is "translated" into a query as a single unit through a set of clusters, giving the following query likelihood formula:</p><formula xml:id="formula_17">p(Q|D) = G i ∈G p(Q|G i )p(G i |D),</formula><p>where G i is a cluster of documents and G is a pre-constructed set of clusters of documents in the collection. This method has shown some improvement over the simple query likelihood method when combined with the simple query likelihood method, but does not perform well alone. Since the translation of a document into a cluster G i causes loss of information, matching based on the clusters alone may not be discriminative enough to distinguish relevant documents from nonrelevant ones, even though such a matching can potentially increase recall due to the allowed inexact matching of terms. This may explain why such methods alone often do not perform well, but they would perform much better when they are combined with a basic model that can supply the needed word-level discrimination. Similar observations have also been made in <ref type="bibr" target="#b37">[38]</ref> where Probabilistic Latent Semantic Indexing (PLSI) was used to learn a lower dimension representation of text in terms of probabilistic topics. PLSI will be discussed further in Section 6.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Summary</head><p>In this section, we reviewed a number of models that all attempted to extend the basic query likelihood retrieval method in various ways. They are often substantially more expensive to compute than the basic model. Many of the extensions have not really led to significant improvement over the basic model. Given their complexity and the relative insignificant improvement (compared with models to be reviewed in the next section), most of these models have not found widespread applications. However, some document-specific smoothing methods have been shown to improve performance significantly, and the computation can often be done offline at the indexing stage. So it is still feasible to use these methods in a large-scale application system. Also, the translation model has later been applied to cross-lingual IR tasks successfully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query Models and Feedback in Language Models</head><p>In the query likelihood retrieval model, we are mostly concerned with estimating a document language model θ D . One limitation of this scoring method is that it is unnatural and hard to support feedback mainly because the query is assumed to be a sample of a language model, thus it is unclear how we should interpret any expanded/modified query. To address this problem, a new scoring strategy based on computing the Kullback-Leibler (KL) divergence of a document language model and a query language model has been introduced. The KL-divergence retrieval model can naturally support feedback by casting it as a problem of estimating a query language model (or relevance model) based on feedback information. In this section, we review this development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Difficulty in Supporting Feedback with Query Likelihood</head><p>Feedback is an important technique to improve retrieval accuracy. Both relevance feedback and pseudo feedback have been well-supported in traditional models (e.g., Rocchio <ref type="bibr" target="#b86">[87]</ref> for the vector space model and term re-weighting for the classical probabilistic model <ref type="bibr" target="#b82">[83]</ref>). Naturally, in the early days when the query likelihood scoring method was introduced, people also explored feedback <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b78">79]</ref>.</p><p>However, unlike in the traditional models where feedback can be naturally accommodated, in the query likelihood retrieval method, it is rather awkward to support feedback. The problem is caused by the fact that in all the query likelihood retrieval methods, the query is regarded as a sample of some kind of language model. Thus it would not make much sense to talk about improving the query by adding additional terms and/or adjusting weights of those terms as done in the vector space model <ref type="bibr" target="#b86">[87]</ref>, nor is it natural to use the feedback documents from a user or from pseudo feedback to improve our estimate of models (i.e., improving θ D ) as done in the classical probabilistic model <ref type="bibr" target="#b82">[83]</ref>.</p><p>Due to this difficulty, early work on achieving feedback using the query likelihood scoring method tends to be quite heuristic, and the techniques used are often not as elegant as the query likelihood method itself. For example, in <ref type="bibr" target="#b78">[79]</ref>, terms with high probabilities in the feedback documents but low probabilities in the collection are selected using a ratio approach as additional query terms. While this method generally performs well (similarly to Rocchio <ref type="bibr" target="#b86">[87]</ref>), the ratio approach is conceptually restricted to the view of query as a set of terms, so it cannot be applied to the more general case when the query is considered as a sequence of terms in order to incorporate the frequency information of a query term. Also, the influence of feedback cannot be controlled through term weighting; a term is either added to the query or not. Similar strategies for heuristic expansion of queries were also studied in Miller and others <ref type="bibr" target="#b69">[70]</ref> and <ref type="bibr" target="#b74">[75]</ref>. But in all these approaches, it is no longer conceptually clear how to interpret the expanded query in a probabilistic way.</p><p>Several studies <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b124">125]</ref> have used feedback documents to optimize the smoothing parameter or query term re-weighting. While these methods do not cause conceptual inconsistency, they also do not achieve full benefit of feedback due to the limited use of feedback information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Kullback-Leibler Divergence Retrieval Model</head><p>The difficulty in supporting feedback with query likelihood scoring has motivated the development of a more general family of probabilistic similarity models called Kullback-Leibler (KL) divergence retrieval model <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b122">123]</ref>. In this model, we define two different language models, one for a query (θ Q ) and one for a document (θ D ). That is, we will assume that the query is a sample observed from a query language model θ Q (which presumably represents a user's information need), while the document is a sample from a document language model θ D (which represents the topic/content of a document). We can then use the KL-divergence of these two models to measure how close they are to each other and use their distance (indeed, negative distance) as a score to rank documents. This way, the closer the document model is to the query model, the higher the document would be ranked.</p><p>Intuitively, such a scoring strategy is very similar to the vector-space model except that we now have probabilistic representation of text and work with a probabilistic distance/similarity function. Formally, the score of a document D with respect to a query Q is given by:</p><formula xml:id="formula_18">s(D, Q) = -D(θ Q ||θ D ) (5.1) = - w∈V p(w|θ Q ) log p(w|θ Q ) p(w|θ D ) (5.2) = w∈V p(w|θ Q ) log p(w|θ D ) - w∈V p(w|θ Q ) log p(w|θ Q ) (5.3)</formula><p>Since the last term is query entropy and does not affect ranking of documents, ranking based on negative KL-divergence is the same as ranking based on negative cross entropy w∈V p(w|θ Q ) log p(w|θ D ). <ref type="foot" target="#foot_4">1</ref>With this model, the retrieval task is reduced to two subtasksestimating θ Q and θ D , respectively. The estimation of document model θ D is similar to that in the query likelihood retrieval model, but the estimation of query model θ Q offers interesting opportunities of leveraging feedback information to improve retrieval accuracy. Specifically, feedback information can be exploited to improve our estimate of θ Q . Such a feedback method is called model-based feedback in <ref type="bibr" target="#b122">[123]</ref>.</p><p>On the surface, the KL-divergence model appears to be quite different from the query likelihood method. However, it turns out that it is easy to show that the KL-divergence model covers the query likelihood method as a special case when we use the empirical query word distribution to estimate θ Q <ref type="bibr" target="#b49">[50]</ref>, i.e.,</p><formula xml:id="formula_19">p(w|θ Q ) = c(w, Q) |Q| .</formula><p>In this sense, the KL-divergence model is a generalization of the query likelihood scoring method with the additional advantage of supporting feedback more naturally. This KL-divergence retrieval model was first proposed in <ref type="bibr" target="#b49">[50]</ref> within a risk minimization retrieval framework, which introduces the concept of query language model (in additional to the document language model) and models the retrieval problem as a statistical decision problem <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b120">121,</ref><ref type="bibr" target="#b126">127]</ref>. However, KL-divergence had previously been used for distributed information retrieval <ref type="bibr" target="#b116">[117]</ref>.</p><p>By truncating the query model θ Q to keep only high probability words and renormalizing it, we can score a KL-divergence model efficiently. Indeed, we may rewrite the scoring function in the same way as in the case of query likelihood to obtain a scoring formula essentially involving a sum over the terms with nonzero probabilities for both θ Q and θ D <ref type="bibr" target="#b75">[76]</ref>:</p><formula xml:id="formula_20">score(D, Q) rank = w∈D p(w|θ Q ) log p s (w|θ D ) α D p(w|C) + log α D . (5.4)</formula><p>Thus the generalization of query likelihood as KL-divergence does not really incur much extra computational overhead; it is generally regarded as a state-of-the-art retrieval model based on language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Estimation of Query Models</head><p>With the KL-divergence retrieval model, feedback can be achieved through re-estimating the query model θ Q based on feedback information.</p><p>Several methods have been proposed to perform such model-based feedback in the pseudo feedback setting. Interestingly, the relevance feedback setting appears to have not attracted that much attention, though these methods can presumably also be applied to relevance feedback. However, sometimes these methods may need to be adapted appropriately to handle relevance feedback; in particular, feedback based on only negative information (i.e., nonrelevant information) remains challenging with the KL-divergence retrieval model and additional heuristics may need to be used <ref type="bibr" target="#b111">[112]</ref>. This is in contrast to the documentgeneration probabilistic models such as the Robertson-Sparck Jones model <ref type="bibr" target="#b82">[83]</ref> which can naturally use negative examples to improve the estimate of the nonrelevant document model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Model-based Feedback</head><p>Zhai and Lafferty <ref type="bibr" target="#b122">[123]</ref> proposed two methods for estimating an improved query model θ Q using feedback documents. Both methods follow the basic idea of interpolating an existing query model (e.g., one estimated based on the empirical query word distribution) with an estimated feedback topic model.</p><p>Specifically, let θ Q be the current query model and θ F be a feedback topic model estimated based on (positive) feedback documents F = {d 1 , . . . , d n }. The updated new query model θ Q is given by</p><formula xml:id="formula_21">p(w|θ Q ) = (1 -α)p(w|θ Q ) + αp(w|θ F ),</formula><p>where α ∈ [0, 1] is a parameter to control the amount of feedback. The two methods differ in the way of estimating θ F with F .</p><p>One approach uses a two-component mixture model to fit the feedback documents where one component is a fixed background language model p(w|C) estimated using the collection and the other is an unknown, to-be-discovered topic model p(w|θ F ). Essentially, the words in F are assumed to fall into two kinds: (1) background words (to be explained by p(w|C)) and (2) topical words (to be explained by p(w|θ F )). By fitting such a mixture model to the data, we can "factor out" the background words and obtain a discriminative topic model which would assign high probabilities to words that are common in the feedback documents but not common in the collection (thus not well explained by p(w|C)). The log-likelihood function is</p><formula xml:id="formula_22">log p(F |θ F ) = w∈V c(w, F ) log((1 -λ)p(w|θ F ) + λp(w|C)),</formula><p>where λ ∈ [0, 1] is a parameter indicating how much weight is put on the background model, which can be interpreted as the amount of background words we would like to factor out. The topic model θ F can be obtained by using the ML estimator, which can be computed using the Expectation-Maximization (EM) algorithm <ref type="bibr" target="#b122">[123]</ref>.</p><p>The other approach proposed in <ref type="bibr" target="#b122">[123]</ref> uses an idea similar to Rocchio in the vector space model <ref type="bibr" target="#b86">[87]</ref> and assumes that θ F is a language model that is very close to the language model of every document in the feedback document set F , but far away from the collection language model which can be regarded as an approximation of nonrelevant language model. The distance between language models is measured using KL-divergence. The problem of computing θ F boils down to solving the following optimization problem:</p><formula xml:id="formula_23">θF = arg min θ 1 n n i=1 D(θ||θ i ) -λD(θ||θ C ),</formula><p>where θ i is the language model estimated using document d i ∈ F , θ C is the background collection language model p(w|C), and λ ∈ [0, 1) is a parameter to control the distance between the estimated θ F and the background model θ C . This optimization problem has an analytical solution:</p><formula xml:id="formula_24">p(w| θF ) ∝ exp 1 1 -λ 1 n n i=1 log p(w|θ i ) - 1 1 -λ log p(w|C) .</formula><p>Both the mixture model method and the divergence minimization method are shown to be quite effective for pseudo feedback with performance comparable to or better than Rocchio <ref type="bibr" target="#b122">[123]</ref>. However, both methods (especially divergence minimization) are also shown to be sensitive to parameter settings.</p><p>There has been some follow-up work on improving the robustness of the mixture model feedback method <ref type="bibr" target="#b106">[107,</ref><ref type="bibr" target="#b107">108]</ref>. In Tao and Zhai <ref type="bibr" target="#b106">[107]</ref>, the mixture model was extended to better integrate the original query model with the feedback documents and to allow each feedback document to potentially contribute differently to the estimated feedback topic language model. The extended model is shown to be relatively more robust than the original model, but the model is still quite sensitive to the number of documents used for feedback <ref type="bibr" target="#b106">[107]</ref>. Moreover, due to the use of several priors, this new model has more prior parameters that need to be set manually with little guidance.</p><p>In Tao and Zhai <ref type="bibr" target="#b107">[108]</ref>, these prior parameters were eliminated through a regularized EM algorithm, and a more robust pseudo feedback model is established. Indeed, it has been shown that with no parameter tuning, the model delivers comparable performance to a well-tuned baseline pseudo feedback model.</p><p>The main ideas introduced in this new model and estimation method are the following: (1) Each feedback document is allowed to have a potentially different amount of noisy words, and the amount of noise is estimated with no need of manual tuning. This makes it more robust with respect to the number of documents used for pseudo feedback. <ref type="bibr" target="#b1">(2)</ref> The interpolation of the original query model with the feedback model is implemented by treating the original query model as a prior in a Bayesian estimation framework. This makes the interpolation more meaningful and offers the opportunity to dynamically change the interpolation weights during the estimation process. (3) The parameter estimation process (EM algorithm) is carefully regularized so that we would start with the original query model and gradually enrich it with additional words picked up from the feedback documents. Such regularization ensures that the estimated model stays close to the original query. (4) This gradual enrichment process stops when "sufficient" new words have been picked up by the EM algorithm, where "sufficient" roughly corresponds to reaching a balance between the original query model and the new topic model picked up from the feedback documents (i.e., interpolation with a 0.5 weight).</p><p>A different approach to improving robustness of pseudo feedback is presented in Collins-Thompson and Callan <ref type="bibr" target="#b18">[19]</ref>, where the idea is to perform sampling over both the feedback documents and the query to generate alternative sets of feedback documents and alternative query variants. Feedback models obtained from each alternative set can then be combined to improve the robustness of the estimated feedback model. Experiments using a variant of the relevance model <ref type="bibr" target="#b54">[55]</ref> as the baseline feedback method show that the proposed sampling method can improve the robustness of feedback even though not necessarily the accuracy of feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Markov Chain Query Model Estimation</head><p>Another approach to estimating a query model is to iteratively mine the entire corpus by following a Markov chain formed by documents and terms <ref type="bibr" target="#b49">[50]</ref>. The basic idea of this approach is to exploit term cooccurrences to learn a translation model t(u|v) which can be expected to capture the semantic relations between words in the sense that t(u|v) would give a high probability to word u if it is semantically related to word v. Specifically, we can imagine a surfer iteratively following a Markov chain of the form w 0 → d 0 → w 1 → d 1 . . ., where w i is a word and d i a document, and the transition probability from a document to a word is given by the document language model p(w|d), while the transition probability from a word to a document is assumed to be the posterior probability p(d|w) ∝ p(w|d)p(d). When visiting a word, the surfer is further assumed to stop at the word with probability 1α which is a parameter to be empirically set. The translation probability t(u|v) can then be defined as the probability of stopping at u if the surfer starts with v. Clearly, the same Markov chain can also be exploited to compute other translation probabilities such as t(d 1 |d 2 ) or t(d|u) without much modification.</p><p>Once we have such a translation model, we can assume that a user has an information need characterized by a query model θ Q , and the user has formulated the current query Q through sampling a word from θ Q and then "translating" it to a query word in Q according to the translation model. Given the observed Q, we can then compute the posterior probability of a word being selected from θ Q and use this probability to estimate θ Q :</p><formula xml:id="formula_25">p(w|θ Q ) ∝ m i=1 t(q i |w)p(w|U ),</formula><p>where p(w|U ) is our prior probability that a word w would have been chosen by user U ; it can be set to the collection language model p(w|C) with no additional knowledge.</p><p>Intuitively, this model exploits global co-occurrences of words to expand a query and obtain an enriched query language model. However, while such a global expansion has been shown to be effective, the expansion is much more effective if the Markov chain is restricted to going through the top-ranked documents for a query <ref type="bibr" target="#b49">[50]</ref>. Thus the method can also be regarded as a way to perform pseudo feedback with language models. The observation that local co-occurrence analysis is more effective than global co-occurrence analysis is also consistent with a study of a traditional retrieval model <ref type="bibr" target="#b115">[116]</ref>. Intuitively, this is because the local documents (i.e., documents close to the query) can prevent noisy words from being picked from feedback due to distracting co-occurrences.</p><p>In Collins-Thompson and Callan <ref type="bibr" target="#b17">[18]</ref>, such a Markov chain expansion method has been extended to include multiple types of term associations, such as co-occurrences in an external corpus, co-occurrences in top-ranked search results, term associations obtained from an external resource (e.g., WordNet). While the expansion accuracy is not better than a strong baseline expansion method, such a massive expansion strategy is shown to be more robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">Relevance Model</head><p>The work reviewed so far on using language models for IR is rooted at the query likelihood retrieval method. In 2001, another very interesting language model, called relevance model was developed by Lavrenko and Croft <ref type="bibr" target="#b54">[55]</ref>. The motivation for this model comes from the difficulty in estimating model parameters in the classical probabilistic model when we do not have relevance judgments.</p><p>The classical probabilistic model can be obtained by using the same derivation as discussed in Section 3.1 and taking the documentgeneration decomposition of the joint probability p(Q, D|R):</p><formula xml:id="formula_26">O(R|Q, D) ∝ p(D|Q, R = 1) p(D|Q, R = 0) . (<label>5.5)</label></formula><p>We see that our main tasks are to estimate two document models, one for relevant documents (i.e., p(D|Q, R = 1)) and one for nonrelevant documents (i.e., p(D|Q, R = 0)). If we assume a multiple Bernoulli model for p(D|Q, R), we will obtain precisely the Binary Independence Model proposed by Robertson and Sparck Jones <ref type="bibr" target="#b82">[83]</ref> and further studied by others (e.g., <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b109">110]</ref>). The model parameters can be estimated by using some examples of relevant and nonrelevant documents, making this an attractive model for relevance feedback. However, when we do not have relevance judgments, it would be difficult to estimate the parameters. Croft and Harper <ref type="bibr" target="#b19">[20]</ref> studied this problem and introduced two approximations: (1) the nonrelevant document model p(D|Q, R = 0) can be estimated by assuming all the documents in the collection to be nonrelevant. ( <ref type="formula">2</ref>) the relevant document model p(D|Q, R = 1) is assumed to give a constant probability to all the query words. Using these assumptions, they showed that this classical probabilistic model would lead to a scoring formula with IDF weighting for matched terms. This is indeed a very interesting derivation and provides some probabilistic justification of IDF. However, while the first assumption is reasonable, the second is clearly an over-simplification. A more reasonable approximation may be to use some top-ranked documents as an approximation of relevant documents, i.e., follow the idea of pseudo relevance feedback. This is essentially the idea behind the relevance model work <ref type="bibr" target="#b54">[55]</ref>.</p><p>In the relevance model, a multinomial model is used to model a document, thus we can capture the term frequency naturally. (Previously, 2-Poisson mixture models had been proposed as a member of the classical probabilistic models to model term frequency, and an approximation of that model has led to the effective BM25 retrieval function <ref type="bibr" target="#b83">[84]</ref>.) Using multinomial distribution, we have</p><formula xml:id="formula_27">O(R|Q, D) ∝ n i=1 p(d i |Q, R = 1) n i=1 p(d i |Q, R = 0) , (<label>5.6)</label></formula><p>where document</p><formula xml:id="formula_28">D = d 1 • • • d n .</formula><p>Since p(d i |Q, R = 0) can be reasonably approximated by p(d i |C) (i.e., collection language model), the main challenge is to estimate p(d i |Q, R = 1), which captures word occurrences in relevant documents and is called a relevance model. In <ref type="bibr" target="#b54">[55]</ref>, the authors proposed two methods for estimating such a relevance model, both based on the idea of using the top-ranked documents to approximate relevant documents to estimate the relevance model p(w|Q, R = 1).</p><p>In Model 1, they essentially use the query likelihood p(Q|D) as a weight for document D and take a weighted average of the probability of word w given by each document language model. Clearly only the top ranked documents matter because other documents would have a very small or zero weight. Formally, the formula is as follows:</p><formula xml:id="formula_29">p(w|Q, R = 1) ∝ p(w, Q|R = 1)</formula><p>(5.7)</p><formula xml:id="formula_30">≈ θ D ∈Θ p(θ D )p(w|θ D )p(Q|θ D ) (5.8) = θ D ∈Θ p(θ D )p(w|θ D ) m i=1 p(q i |θ D ),<label>(5.9)</label></formula><p>where Θ represents the set of smoothed document models in the collection. p(θ D ) can be set to uniform. In Model 2, they compute the association between each word and the query using documents containing both query terms and the word as "bridges." The strongly associated words are then assigned high probabilities in the relevance model. Formally, p(w|Q, R = 1) ∝ p(Q|w, R = 1)p(w)</p><p>(5.10) <ref type="bibr">(5.11)</ref> where p(θ D |w) can be computed as</p><formula xml:id="formula_31">= p(w) m i=1 θ D ∈Θ p(q i |θ D )p(θ D |w),</formula><formula xml:id="formula_32">2 p(θ D |w) ∝ p(w|θ D )p(θ D ) θ D ∈Θ p(w|θ D )p(θ D )</formula><p>.</p><p>Once again, we see that the document models that give query words high probabilities dominate the computation. Thus this model intuitively also assigns high probabilities to words that occur frequently in documents that match the query well.</p><p>While both models can be potentially computed over the entire space of empirical document models, in the experiments reported in <ref type="bibr" target="#b54">[55]</ref>, the authors restricted the computation to the top 50 documents returned for each query. This not only improves the efficiency, but also improves the robustness of the estimated model as we are at a lower risk of including some distracting document models. Indeed, as shown in <ref type="bibr" target="#b54">[55]</ref>, including more documents can be potentially harmful. This is the same observation as in <ref type="bibr" target="#b49">[50]</ref>, all suggesting that these models are essentially alternative ways of implementing pseudo feedback with language models.</p><p>Both versions of the relevance model are shown to be quite effective <ref type="bibr" target="#b54">[55]</ref>. The relevance model has also later been applied to other tasks such as cross-lingual IR <ref type="bibr" target="#b53">[54]</ref>.</p><p>Although relevance model was motivated by the classical probabilistic model, it can clearly be regarded as a way to estimate the query language model. Conceptually, there appears to be little difference between relevance model and query model, both are to model what a user is interested in. That is, we may view p(w|Q, R = 1) as the same as p(w|θ Q ) discussed in Section 5.2. Indeed, in some later studies <ref type="bibr" target="#b51">[52]</ref>, it was shown that scoring with the KL-divergence function works better than scoring with the classical probabilistic model for the relevance model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.4">Structured Query Models</head><p>Sometimes a query may be represented in multiple ways or semistructured so that it has multiple fields. For example, in the case of multiple representations, one representation may be based on unigrams and another may be based on word associations extracted from some domain resources <ref type="bibr" target="#b1">[2]</ref>. In the TREC Genomics Track, gene queries are examples of queries with multiple fields: a gene query often consists of two fields, one containing the name of a gene (usually a phrase) and one with a set of symbols <ref type="bibr" target="#b127">[128]</ref>. In all these cases, using one single query language model to represent the query appears to an over-simplification as it does not allow us to flexibly put different weights on different representations of fields.</p><p>A common solution to these problems is to define the query model as a mixture model, which was done in <ref type="bibr" target="#b1">[2]</ref> for combining multiple sources of knowledge about query expansion and in <ref type="bibr" target="#b127">[128]</ref> for assigning different weights to different fields of a gene query. Specifically, let Q = {Q 1 , . . . , Q k } be a query with k fields or representations. The mixture query model is defined as</p><formula xml:id="formula_33">p(w|θ Q ) = k i=1 λ i p(w|θ Q i ),</formula><p>where p(w|θ Q i ) is a query model corresponding to field or representation Q i , and λ i is the corresponding weight.</p><p>In <ref type="bibr" target="#b127">[128]</ref>, a pseudo feedback algorithm is proposed to expand each p(w|θ Q i ) and estimate λ i simultaneously. The basic idea is to use each field (Q i ) to define a prior on θ Q i and fit the mixture model to a set of feedback documents in the same way as fitting the two-component mixture model for model-based feedback discussed in Section 5.3.1. Such semi-structured query model is shown to be effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Summary</head><p>In this section, we discussed how feedback (particularly pseudo feedback) can be performed with language models. As a generalization of query likelihood scoring, the KL-divergence retrieval model has now been established as the state-of-the-art approach for using language models to rank documents. It supports all kinds of feedback through estimating a query language model based on feedback information. We reviewed several different approaches to improving the estimation of a query language model by using word co-occurrences in the corpus. Although some approaches are meant to work on the entire corpus, they tend to work much better when restricting the estimation to using only the top-ranked documents. Thus it is fair to say that all these methods are essentially different ways to implement the traditional pseudo feedback heuristic with language models. Among all the methods, the two-component mixture model <ref type="bibr" target="#b107">[108,</ref><ref type="bibr" target="#b122">123]</ref> and the relevance model <ref type="bibr" target="#b54">[55]</ref> appear to be most effective and robust and also are computationally feasible.</p><p>Although most work on language models deals with the standard monolingual ad hoc search task, there has also been a lot of research on applying language models to many other special retrieval tasks, including cross-lingual retrieval, distributed IR, expert finding, personalized search, modeling redundancy, subtopic retrieval, and topic mining, among others. In this section, we will review this line of work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Cross-lingual Information Retrieval</head><p>A major challenge in cross-lingual IR is to cross the language barrier in some way, typically involving translating either the query or the document from one language to the other. The translation model discussed in Section 4.5 can be naturally applied to solve this problem by defining the translation probability p(u|v) on terms in the two different languages involved in the retrieval task. For example, u may be a word in Chinese and v a word in English. In such a case, p(u|v) would give us the probability that u is a good translation of English word v in Chinese; intuitively it captures the semantic association between words in different languages.</p><p>Xu and co-authors <ref type="bibr" target="#b117">[118]</ref> applied this idea to cross-lingual IR, and proposed the following cross-lingual query likelihood retrieval function:</p><formula xml:id="formula_34">p(Q|D) = m i=1 αp(q i |C S ) + (1 -α) w∈V T p(q i |w)p(w|D) ,</formula><p>where C S is the collection in the source language (i.e., the language of the query Q), V T is the vocabulary set of the target language (i.e., the language of the document D), and α is a smoothing parameter.</p><p>As in the translation model for monolingual ad hoc retrieval, a major challenge here is to estimate the translation probability p(q i |w). In <ref type="bibr" target="#b117">[118]</ref>, the authors experimented with several options, including using a bilingual word list, parallel corpora, and a combination of them. The cross-lingual query likelihood retrieval function has been shown to be quite effective, achieving over 85% performance of monolingual retrieval baseline.</p><p>In another line of work on applying language models to CLIR, Lavrenko and co-authors <ref type="bibr" target="#b53">[54]</ref> adapted the relevance model (Model 1) in two ways to perform CLIR, both based on the KL-divergence scoring function. The document language model θ D is estimated in a normal way, thus it assigns probabilities to words in the target language. Their main idea is to adapt relevance model so that we can start with a query Q in the source language to estimate a query model θ Q that can assign probabilities to words in the target language. This way, the query model and the document model can be compared using the KL-divergence scoring function since they are now in the same (target) language.</p><p>Their first method is to leverage a parallel corpus where documents in the source language are paired with translations of them in the target language. In this case, the document model θ D in their relevance model can be generalized to include two separate models, one for each language. That is, θ D = (θ S D , θ T D ), where θ S D is the model for the source document and θ T D the model for the corresponding target document. With this setup, the relevance model can be generalized in a straightforward way to give the following probability of word w T in the target language according to the query model θ Q :</p><formula xml:id="formula_35">p(w T |θ Q ) = θ D ∈Θ p(θ D )p(w T |θ T D ) m i=1</formula><p>p(q i |θ S D ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Distributed Information Retrieval 181</head><p>The pairing of θ S D and θ T D has enabled the crossing of the language barrier.</p><p>Their second method is to leverage a bilingual dictionary to induce a translation model p(w S |w T ) and use this translation model to convert the document language model p(w T |D), which is in the target language, to a document language model for the source language p(w S |D). That is,</p><formula xml:id="formula_36">p(w T |θ Q ) = θ D ∈Θ p(θ D )p(w T |θ D ) m i=1 p(q i |θ D ) (6.1) = θ D ∈Θ p(θ D )p(w T |θ D ) m i=1 w∈V T p(q i |w)p(w|θ D ). (6.2)</formula><p>This time, the translation model p(q i |w) has enabled the crossing of the language barrier. These models have been shown to achieve very good retrieval performance (90%-95% of a strong monolingual baseline).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Distributed Information Retrieval</head><p>Language models have also been applied to perform distributed IR. The task of distributed IR can often be decomposed into two subtasks: (1) resource selection; and (2) result fusion. Language models have been applied to both tasks.</p><p>For resource selection, which is to select the most promising collections to search based on a query, the general idea is to treat each collection as a special "document" and apply standard language models to rank collections. In an early work by Xu and Croft <ref type="bibr" target="#b116">[117]</ref>, the authors cluster the documents to form topical clusters. Each cluster is then treated as one coherent subcollection, which is then used to estimate a topic language model. The KL-divergence between the empirical query word distribution and the estimated topic language model is then used to select the most promising topical collections for further querying. Such a clustering method is shown to be effective for collection selection <ref type="bibr" target="#b116">[117]</ref>.</p><p>In <ref type="bibr" target="#b94">[95]</ref>, the authors proposed a language modeling framework for resource selection and result fusion. In this framework, documents in each collection are scored using regular query likelihood retrieval function but smoothed with the background language model corresponding to the collection. As a result, the scores of documents in different collections are strictly speaking not comparable because of the use of different background language model for smoothing. A major contribution of the work <ref type="bibr" target="#b94">[95]</ref> is to derive an adjustment strategy that can ensure that the scores of all the documents would be comparable after adjustment.</p><p>Specifically, let D be a document in collection C i . In general, we score the document for query Q with query likelihood and rank documents based on p(Q|D, C i ). The likelihood is conditioned on C i because of smoothing, thus directly merging results based on their query likelihood scores p(Q|D, C i ) would be problematic since the scores may not be comparable. Thus they use probabilistic rules to derive a normalized form of the likelihood denoted as p(Q|D), which can then be used as scores of documents for the purpose of result fusion. They show that ranking based on p(Q|D) is equivalent to ranking based on +1 , where β is a parameter to be empirically set. Thus when we merge the results, we just need to divide the original score p(Q|D, C i ) by the normalizer βp(C i |Q) + 1, which can be computed using Bayes' rule and the likelihood of the query given collection C i (i.e., p(Q|C i )). Their experiment results show that this language modeling approach is effective for distributed IR and outperforms a state-of-the-art method (i.e., CORI) <ref type="bibr" target="#b94">[95]</ref>.</p><formula xml:id="formula_37">p(Q|D,C i ) βp(C i |Q)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Structured Document Retrieval and Combining Representations</head><p>Most retrieval models are designed to work on a bag of words representation of a document, but ignore any structure of a document. In reality, a document often has both intra-document structures (e.g., title vs. body) and inter-document structures (e.g., hyperlinks and topical relations), which can be potentially leveraged to improve search accuracy. This is especially true in XML retrieval and Web search. It is also common that one may obtain multiple text representations of the same document, which should be combined to improve retrieval accuracy. In all these problems, we can assume that a document D has k parts or text representations D = {D 1 , . . . , D k }, and our goal is to rank such documents with consideration of the known structure of the document. In Ogilvie and Callan <ref type="bibr" target="#b77">[78]</ref>, the authors have extended the basic query likelihood to address this problem. Their approach allows different parts of a document or different representations of a document to be combined with different weights. Specifically, the "generation" process of a query given a document is assumed to consist of two steps: In the first step, a part D i is selected from the structured document D according to a selection probability p(D i |D). In the second, a query is generated using the selected part D i . Thus, the query likelihood is given by</p><formula xml:id="formula_38">p(Q|D) = m i=1 p(q i |D) (6.3) = m i=1 k j=1 p(q i |D j )p(D j |D).<label>(6.4)</label></formula><p>In <ref type="bibr" target="#b77">[78]</ref>, such a two-step generation process was not explicitly given, but their model implies such a generation process. The "part selection probability" p(D i |D) is denoted by λ i in <ref type="bibr" target="#b77">[78]</ref>; it can be interpreted as the weight assigned to D i and can be set based on prior knowledge or estimated using training data. How to implement such a model efficiently was discussed in length in <ref type="bibr" target="#b76">[77]</ref>. Experiment results show that this language modeling approach to combining multiple representations is effective. Language models have also been applied to XML retrieval by other researchers <ref type="bibr" target="#b32">[33]</ref>.</p><p>A general probabilistic propagation framework was proposed in <ref type="bibr" target="#b91">[92]</ref> to combine probabilistic content-based retrieval models including language models with link structures (e.g., hyperlinks). Results show that the propagation framework can improve ranking accuracy over pure content-based scoring. While the framework is not specific to language models, it was shown in <ref type="bibr" target="#b91">[92]</ref> that the performance is much better if the content-based scores can be interpreted as probabilities of relevance. Another general (nonprobabilistic) propagation framework was proposed in <ref type="bibr" target="#b80">[81]</ref> which has been shown to be effective for improving Web search through both score propagation and term count propagation. How to integrate such propagation frameworks with language models more tightly remains an interesting future research question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Personalized and Context-sensitive Search</head><p>In personalized search, we would like to use more user information to better infer a user's information need. With language models, this means we would like to estimate a better query language model with more user information. In <ref type="bibr" target="#b93">[94,</ref><ref type="bibr" target="#b104">105]</ref>, the authors proposed several estimation methods for estimating a query language model based on implicit feedback information, including the previous queries and clickthroughs of a user. These methods are shown to be effective for improving search accuracy for a new related query.</p><p>In <ref type="bibr" target="#b93">[94]</ref>, implicit feedback within a single search session is considered. This is to simulate a scenario when the initial search results were not satisfactory to the user, so the user would reformulate the query potentially multiple times. The feedback information available consists of the previous queries and the snippets of viewed documents (i.e., clickthrough information). Given the user's current query, the question is how to use such feedback information to improve the estimate of the query language model θ Q . In <ref type="bibr" target="#b93">[94]</ref>, four different methods were proposed to solve this problem, all essentially leading to some interpolation of many unigram language models estimated using different feedback information, respectively. Different methods mainly differ in the way to assign weights to different types of information (e.g., queries vs. snippets). Experiment results show that using the history information, especially the snippets of viewed documents, can improve search accuracy for the current query. It is also shown to be beneficial to use a dynamic interpolation coefficient similar to Dirichlet prior smoothing.</p><p>In <ref type="bibr" target="#b104">[105]</ref>, implicit feedback using the entire search history of a user is considered. Since in this setup, there is potentially noisy information in the search history, it is important to filter out such noise when estimating the query language model. The idea presented in <ref type="bibr" target="#b104">[105]</ref> for solving this problem is the following: First, each past query is treated as a unit and represented by the snippets of the top-ranked search results. Second, the search results (snippets) of the current query are used to assign a weight to each past query based on the similarity between the search results of the past query and those of the current one. The weighting helps filter out any noisy information in the history. Finally, the query language model is estimated as a weighted combination of unigram language models for each past query. The second step is implemented by using a mixture model with each past query contributing a component language model to fit the current search results. The EM algorithm is used to compute the ML estimate so that we can obtain optimal weights for all the past queries. Intuitively, the weight on each past query indicates how well the search results of that query can explain the current search results, i.e., similarity between that past query and the current query. Evaluation shows that such a language modeling approach to query estimation based on search history can improve performance substantially <ref type="bibr" target="#b104">[105]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Expert Finding</head><p>The task of expert finding as set up in the TREC Enterprise Track is the following: Given a list of names and emails of candidate experts and text collections where their expertise may be mentioned, retrieve experts with expertise on a given topic (described by keywords). Language models have been applied to this task with reasonable success <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>In <ref type="bibr" target="#b24">[25]</ref>, a general probabilistic model is presented for solving this problem with an analogous derivation to the one given in <ref type="bibr" target="#b50">[51]</ref>. Specifically, three random variables are introduced: (1) T for topic; (2) C for a candidate expert; (3) R ∈ {0, 1} for relevance. The goal is to rank the candidates according to the conditional probability p(R = 1|T, C). Following the derivation in <ref type="bibr" target="#b50">[51]</ref>, the authors derived two families of models corresponding to two different ways of factoring the joint probability p(T, C|R), either as p(T |R, C)p(C|R), which is called topic generation model or p(C|T, R)p(T |R), which is called candidate generation model. They also proposed three techniques to improve the estimation of models: (1) a mixture model for modeling the candidate mentions, which can effectively assign different weights to different representations of an expert; (2) topic expansion to enrich topic representation; (3) emailbased candidate prior to prefer candidates with many email mentions. These techniques are shown to be empirically effective.</p><p>In <ref type="bibr" target="#b2">[3]</ref>, the authors proposed two different topic generation models for expert finding. In both models (indeed, also in most other studies of expert finding with the TREC Enterprise Track setup), the documents where mentions of a candidate and terms describing a topic co-occur serve as "bridges" to assess the associations between a candidate and a topic. However, the two models differ in the way a topic is "generated" from a candidate. In Model 1, a topic is generated by generating each word in the topic independently, thus the generation of two words of the topic can potentially be going through a different document, and a bridge document only needs to match the candidate and one topic word well. In Model 2, the whole topic is generated together using the same document as a bridge, thus requiring the bridge document to match both the candidate and the entire topic well. Thus intuitively Model 2 appears to more reasonable than Model 1. Indeed, empirical experiments also show that Model 2 outperforms Model 1 <ref type="bibr" target="#b2">[3]</ref>. This is a case where some analysis of the assumptions made in designing language models can help assess the effectiveness of a model for retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Modeling Redundancy and Novelty</head><p>A basic task in many applications is to measure the redundancy between two documents; the purpose is often to remove or reduce the redundancy in the documents. Language models can be used to naturally solve this problem.</p><p>For example, in <ref type="bibr" target="#b120">[121,</ref><ref type="bibr" target="#b121">122]</ref>, a simple two-component mixture model is used to measure the redundancy (or equivalently novelty) of a document D 2 with respect to another document D 1 . The idea is to assume that the redundancy of D 1 with respect to D 2 corresponds to how well we can predict the content of D 1 using a language model estimated based on D 2 . Intuitively, if D 1 is very similar to D 2 , then we should expect the model based on D 2 to predict D 1 well (i.e., give high probability to D 1 ), whereas if they are quite different, the model would not predict D 1 well. where p(w|C) is a background collection language model. Essentially, this is to let the background model and θ D 2 to compete for explaining D 1 , and λ * ∈ [0, 1] indicates the relative "competitiveness" of θ D 2 to the background model, thus intuitively captures the redundancy. λ * can be computed using the EM algorithm. The novelty can be defined as 1λ * .</p><p>A similar but slightly more sophisticated three-component mixture model was proposed in <ref type="bibr" target="#b130">[131]</ref> in order to capture novelty in information filtering.</p><p>Note that the redundancy/novelty captured in this way is asymmetric in the sense that if we switch the roles of D 1 and D 2 , the redundancy value would be different. This is reasonable as in general, redundancy is asymmetric (considering a case where one document is part of another document). Another way of measuring redundancy/novelty with language models is to compute the cross-entropy between two document language models to obtain asymmetric similarities <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Predicting Query Difficulty</head><p>Yet another use of language models is to predict query difficulty <ref type="bibr" target="#b20">[21]</ref>. The idea is to compare the query model and the collection language model and a query would be assumed to be difficult if its query model is close to the collection language model. The assumption made here is that a discriminative query tends to be easier and the discriminativeness of a query can be measured by the KL-divergence of the query model and the collection model. Specifically, a measure called "query clarity" is defined in <ref type="bibr" target="#b20">[21]</ref> as follows:</p><formula xml:id="formula_39">clarity(Q) = w∈V p(w|θ Q ) log p(w|θ Q ) p(w|C) ,</formula><p>where θ Q is usually an expanded query model using any feedback-based query model estimation method (e.g., mixture model <ref type="bibr" target="#b122">[123]</ref> or relevance model <ref type="bibr" target="#b54">[55]</ref>). Positive correlation between the clarity scores and retrieval accuracy has been observed <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8">Subtopic Retrieval</head><p>The subtopic retrieval task represents an interesting retrieval task because it requires modeling the dependency of relevance of individual documents <ref type="bibr" target="#b121">[122]</ref>. Given a topic query, the task of subtopic retrieval is to retrieve documents that can cover as many subtopics of the topic as possible. If we are to solve the problem with a traditional retrieval model, we likely would have a great deal of redundancy in the top ranked documents. As a result, although most top-ranked documents may be relevant to the query, they may all cover just one subtopic, thus we do not end up having a high coverage of subtopics. Intuitively, we may solve this problem by attempting to remove the redundancy in the search results, hoping that by avoiding covering already covered subtopics, we will have a higher chance of covering new subtopics quickly. This is precisely the idea explored in <ref type="bibr" target="#b121">[122]</ref>, where the authors used the novelty measure discussed in Section 6.6 in combination with the query likelihood relevance scoring to iteratively select the best document that is both relevant and different from the already picked documents, a strategy often called maximal marginal relevance (MMR) ranking <ref type="bibr" target="#b15">[16]</ref>.</p><p>In <ref type="bibr" target="#b120">[121]</ref>, topic models (PLSA <ref type="bibr" target="#b37">[38]</ref> and LDA <ref type="bibr" target="#b8">[9]</ref>) (to be discussed in more detail in Section 6.9) are applied to model the underlying subtopics and a KL-divergence retrieval function is then applied to rank documents based on subtopic representation. This method has not worked as well as the MMR method reported in <ref type="bibr" target="#b121">[122]</ref>, but it may be possible to combine such a subtopic representation with word-level representation to improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.9">Topic Mining</head><p>The probabilistic latent semantic analysis (PLSA) model was introduced by Hofmann in <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> as a model for analyzing and extracting the latent topics in text documents. In <ref type="bibr" target="#b37">[38]</ref>, Hofmann has shown that using the latent topics discovered by PLSA to represent documents can improve retrieval performance (called probabilistic latent semantic indexing, or PLSI). Later, many different extensions of this model have been proposed, mostly for the purpose of mining (extracting) latent topics in text and revealing interesting topical patterns (e.g., temporal topical trends).</p><p>The basic idea of PLSA is to assume that each word in a document is generated from a finite mixture model with k multinomial component models (i.e., k unigram language models). Formally, let D be a document and θ 1 , . . . , θ k be k multinomial distributions over words, representing k latent topics. Associated with D we have a document-specific topic selection probability distribution p D (i) (i = 1, . . . , k), which indicates the probability of selecting θ i to generate a word in the document. The log-likelihood of document D is then</p><formula xml:id="formula_40">log p(D) = w∈V c(w, D) log k i=1 p D (i)p(w|θ i ) ,</formula><p>where V is the vocabulary set, c(w, D) is the count of word w in D. PLSA can be estimated using the standard maximum likelihood estimator (with the Expectation-Maximization (EM) algorithm <ref type="bibr" target="#b21">[22]</ref>) to obtain parameter values for θ i and p D (i). Clearly, if k = 1, PLSA degenerates to the simple unigram language model. PLSA is generally used to fit a set of documents. Since the topic models θ i are tied across the documents, they can capture clusters of words that co-occur with each other, and help discover interesting latent topics in a collection of text.</p><p>There are two problems with PLSA: (1) it is not really a generative model because the topic selection probability is defined in a documentspecific way; (2) it has many parameters, making it hard to find a global maximum in parameter estimation. To address these limitations, Blei and co-authors <ref type="bibr" target="#b8">[9]</ref> proposed Latent Dirichlet Allocation (LDA) as an extension of PLSA. The main idea is to define p D (i) in a "generative" way by drawing the distribution p D (i) from a Dirichlet distribution. This not only gives us a generative model that can be used to sample "future documents," but also reduces the number of parameters significantly. However, the estimation of PLSA is no longer as simple as using the standard EM algorithm, and tends to be computationally much more expensive <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b70">71]</ref>. Many extensions of LDA have since been proposed to model coordinated data, hierarchical topics, and temporal topic patterns (see e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b102">103]</ref>).</p><p>Although LDA is advantageous over PLSA as a generative model, for the purpose of mining topics, it is unclear whether regularizing the topic choices with a parametric Dirichlet distribution is advantageous; intuitively, this makes the estimated p D (i) less discriminative. PLSA has also been extended in several studies mostly to accommodate a topic hierarchy <ref type="bibr" target="#b35">[36]</ref>, incorporate context variables such as time and location <ref type="bibr" target="#b65">[66]</ref>, and analyze sentiments <ref type="bibr" target="#b64">[65]</ref>. In <ref type="bibr" target="#b129">[130]</ref>, a background topic is introduced to PLSA to make the extracted topic models more focusing on the content words rather than the common words in the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.10">Summary</head><p>In this section, we reviewed a wide spectrum of work on using language models for different kinds of special retrieval tasks. Since the central topic of this review paper is ad hoc retrieval, we have intentionally focused on applications of language models in unsupervised settings and left out work on using language models in supervised learning settings (where labeled training data is needed) because the latter, which includes tasks such as text categorization, information filtering, and topic tracking and detection, is better reviewed through comparing the generative language models with many other competing supervised learning methods, notably discriminative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unifying Different Language Models</head><p>In addition to the study of individual retrieval models using language modeling, there has also been some work attempting to establish a general formal framework to unify different language models and facilitate systematic explorations of language models in information retrieval. We have seen from previous sections that with language models we may rank documents using three different strategies: (1) query likelihood (i.e., computing p(Q|D) or p(Q|θ D )); (2) document likelihood ratio (i.e., computing p(D|Q, R = 1)/p(D|Q, R = 0)); and (3) KL-divergence (i.e., computing D(θ Q ||θ D )). The first two can be derived from the same ranking criterion p(R = 1|Q, D) as shown in <ref type="bibr" target="#b50">[51]</ref>, thus they naturally follow the probability ranking principle <ref type="bibr" target="#b84">[85]</ref>. An interesting question is whether we can further unify the third one (i.e., KL-divergence scoring) with the first two. One major advantage of unifying these different scoring methods is that we will obtain a general retrieval framework that can serve as a road map to explore variations of language models systematically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Risk Minimization</head><p>A major work in this line is the risk minimization framework <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b120">121,</ref><ref type="bibr" target="#b126">127]</ref>. The basic idea of this framework is to formalize the retrieval problem generally as a decision problem with Bayesian decision theory, which provides a solid theoretical foundation for thinking about problems of action and inference under uncertainty <ref type="bibr" target="#b5">[6]</ref>. Language models are introduced into the framework as models for the observed data, particularly the documents and queries.</p><p>Specifically, we assume that we observe the user U, the query q, the document source S, and the collection of documents C. We view a query as being the output of some probabilistic process associated with the user U, and similarly, we view a document as being the output of some probabilistic process associated with an author or document source S. A query (document) is the result of choosing a model, and then generating the query (document) using that model.</p><p>The system is supposed to choose an optimal action to take in response to these observations. An action corresponds to a possible response of the system to a query. We can represent all actions by A = {(D i , π i )}, where D i ⊆ C is a subset of C (results) and π i ∈ Π is some presentation strategy.</p><p>In Bayesian decision theory, to each such action a i = (D i , π i ) ∈ A there is associated a loss L(a i , θ, F (U), F (S)), which in general depends upon all of the parameters of our model, θ ≡ (θ Q , {θ i } N i=1 ) as well as any relevant user factors F (U) and document source factors F (S).</p><p>In this framework, the expected risk of action a i is given by</p><formula xml:id="formula_41">R(D i , π i | U, q, S, C) = Θ L(D i , π i , θ, F (U), F (S)) p (θ | U, q, S, C) dθ,</formula><p>where the posterior distribution is given by</p><formula xml:id="formula_42">p (θ | U, q, S, C) ∝ p (θ Q | q, U) N i=1 p (θ i | d i , S).</formula><p>The Bayes decision rule is then to choose the action a * with the least expected risk:</p><formula xml:id="formula_43">a * = (D * , π * ) = arg min D,π R(D, π | U, q, S, C).</formula><p>That is, to select D * and present D * with strategy π * . The risk minimization framework provides a general way to frame a retrieval problem using language models. With different instantiations of the query model θ Q and document models θ i , it can accommodate potentially many different language models, while the loss function can be instantiated in different ways to reflect different retrieval/ranking criteria. It has been shown in <ref type="bibr" target="#b120">[121,</ref><ref type="bibr" target="#b126">127]</ref> that the risk minimization framework covers many existing retrieval models as special cases and can serve as a roadmap for exploring new models. For example, the probability ranking principle (thus both the query likelihood scoring method and the document likelihood ratio scoring method) can be derived by making the independent loss assumption 1 and defining the loss function based on the relevance status of a document. The KLdivergence scoring function can be justified by defining the loss of returning a document based on the KL-divergence of its language model and the query language model.</p><p>In general, we do not have to make the independent loss assumption and can define the loss function over the entire ranked list of documents. Thus in the risk minimization framework, it is possible to go beyond independent relevance to capture redundancy between documents (e.g., as done in <ref type="bibr" target="#b121">[122]</ref>), which is hard to capture when the retrieval problem is formulated as computing a score based on matching one query with one document. See Section 6.8 for more discussion of using language models to solve the subtopic retrieval problem.</p><p>The optimization setup of the risk minimization is quite general and offers potential for combining language models with the line of work on learning to rank (e.g., <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">43]</ref> and their recent extensions <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Generative Relevance</head><p>Another important work is the generative relevance framework developed in Lavrenko's thesis <ref type="bibr" target="#b51">[52]</ref>. In <ref type="bibr" target="#b51">[52]</ref>, the following generative relevance 1 The independent loss assumption says that the loss of returning a document in response to a query is independent of returning other documents. This assumption clearly does not hold in reality because redundant documents do not have independent loss. Thus the probability ranking principle has the limitation of not being able to model redundancy among the search results.</p><p>hypothesis was proposed:</p><p>Generative Relevance Hypothesis: For a given information need, queries expressing that need and documents relevant to that need can be viewed as independent random samples from the same underlying generative model. Lavrenko developed three different retrieval functions under this hypothesis (i.e., query likelihood, document likelihood, and KLdivergence) and proposed a general technique called kernel-based allocation for estimating various kinds of language models <ref type="bibr" target="#b51">[52]</ref>. The generative relevance hypothesis has two important implications from the perspective of deriving retrieval models: (1) It naturally accommodates matching of queries and documents even if they are in different languages (as in the case of cross-lingual retrieval) or in different media (e.g., text queries on images). ( <ref type="formula">2</ref>) It makes it possible to estimate and improve a relevant document language model based on examples of queries and vice versa. Conceptually, the generative relevance framework can be regarded as a special case of risk minimization when document models and query models are assumed to be in the same space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Language Models vs. Traditional Retrieval Models</head><p>It has been a long-standing challenge in IR research to develop robust and effective retrieval models. As a new generation of probabilistic retrieval models, language modeling approaches have several advantages over traditional retrieval models such as the vector-space model and the classical probabilistic retrieval model:</p><p>First, these language models generally have a good statistical foundation. This makes it possible to leverage many established statistical estimation methods to set parameters in a retrieval function as demonstrated in <ref type="bibr" target="#b107">[108,</ref><ref type="bibr" target="#b124">125]</ref>. Following rigorous statistical modeling also forces any assumptions to be made explicit. A good understanding of such assumptions often helps diagnose the weakness and strength of a model and thus better interpret experiment results.</p><p>Second, they provide a principled way to address the critical issue of text representation and term weighting. The issue of term weighting has long been recognized as critical, but before language modeling approaches were proposed, this issue had been traditionally addressed mostly in a heuristic way. Language models, multinomial unigram language models particular, can incorporate term frequencies and document length normalization naturally into a probabilistic model. While such connection has also been made in the classic probabilistic retrieval model (e.g., <ref type="bibr" target="#b82">[83]</ref>), the estimation of parameters was not addressed as seriously as in the language models. Third, language models can often be more easily adapted to model various kinds of complex and special retrieval problems than traditional models as discussed in Section 6. The benefit has largely come from the availability of many well-understood statistical models such as finite mixture models, which can often be estimated easily by using the EM algorithm.</p><p>However, the language modeling approaches also have some deficiencies as compared with traditional models:</p><p>First, there is a lack of explicit discrimination in most of the language models developed so far. For example, in the query likelihood retrieval function, the IDF effect is achieved through smoothing the document language model with a background model. While this can be explained by modeling the noise in the query, it seems to be a rather unnatural way to penalize matching common words, at least as compared with the traditional TF-IDF weighting. Such a lack of discrimination is indeed a general problem with all generative models as they are designed to describe what the data looks like rather than how the data differs.</p><p>Second, the language models have been found to be less robust than the traditional TF-IDF model in some cases and can perform poorly or be very sensitive to parameter setting. For example, the feedback methods proposed in <ref type="bibr" target="#b122">[123]</ref> are shown to be sensitive to parameter setting, whereas a traditional method such as Rocchio appears to be more robust. This may be the reason why language models have not yet been able to outperform well-tuned full-fledged traditional methods consistently and convincingly in TREC evaluation. In particular, BM25 term weighting coupled with Rocchio feedback remains a strong baseline which is at least as competitive as any language modeling approach for many tasks.</p><p>Third, some sophisticated language models can be computationally expensive (e.g., the translation model), which may limit their uses in large-scale retrieval applications.</p><p>It should be noted that these relative advantages and disadvantages are based on a quite vague distinction between language models and classical probabilistic retrieval models. Indeed, the boundary is not very clear. Conceptually, any probabilistic model of text can be called a language model. In this sense, the classical probabilistic retrieval model such as the Robertson-Sparck Jones model is certainly also a language model (i.e., multiple Bernoulli). However, for historical reasons, the term language model tends to be used to refer to either the use of a multinomial model (or a higher order n-gram model such as bigram or trigram language model) or the query likelihood retrieval function and its generalization KL-divergence retrieval function. Thus readers should be careful about the vague distinction of the so-called language models from other (traditional) probabilistic models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Summary of Research Progress</head><p>Since the pioneering work by Ponte and Croft <ref type="bibr" target="#b79">[80]</ref>, a lot of progress has been made in studying the language modeling approaches to IR, which we briefly reviewed in this survey. The following is an incomplete list of some of the most important developments:</p><p>• Framework and justification for using LMs for IR have been established: The query likelihood retrieval method has been shown to be a well-justified model according to the probability ranking principle <ref type="bibr" target="#b50">[51]</ref>. General frameworks such as the risk minimization framework <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b120">121,</ref><ref type="bibr" target="#b126">127]</ref> and the generative relevance framework <ref type="bibr" target="#b51">[52]</ref> offer road maps for systematically applying language models to retrieval problems. • Many effective models have been developed and they often work well for multiple tasks:</p><p>-The KL-divergence retrieval model <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b122">123]</ref>, which covers the query likelihood retrieval model, has been found to be a solid and empirically effective retrieval model that can easily incorporate many advanced language models; many methods have been developed to improve estimation of query language models.</p><p>-Dirichlet prior smoothing has been recognized as an effective smoothing method for retrieval <ref type="bibr" target="#b123">[124]</ref>. The KL-divergence retrieval model combined with Dirichlet prior smoothing represents the state-ofthe-art baseline method for the language modeling approaches to IR.</p><p>-The translation model proposed in <ref type="bibr" target="#b3">[4]</ref> enables handling polysemy and synonyms in a principled way with a great potential for supporting semantic information retrieval.</p><p>-The relevance model <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b54">55]</ref> offers an elegant solution to the estimation problem in the classical probabilistic retrieval model as well as serves as an effective feedback method for the KL-divergence retrieval model.</p><p>-Mixture unigram language models have been shown to be very powerful and can be useful for many purposes such as pseudo feedback <ref type="bibr" target="#b122">[123]</ref>, improving model discriminativeness <ref type="bibr" target="#b34">[35]</ref>, and modeling redundancy <ref type="bibr" target="#b121">[122,</ref><ref type="bibr" target="#b130">131]</ref>. • It has been shown that completely automatic tuning of parameters is possible for both nonfeedback retrieval <ref type="bibr" target="#b124">[125]</ref> and pseudo feedback <ref type="bibr" target="#b107">[108]</ref>. • LMs can be applied to virtually any retrieval task with great potential for modeling complex IR problems (as surveyed in Section 6).</p><p>For practitioners who want to apply language models in specific applications, the KL-divergence retrieval function combined with Dirichlet prior smoothing for estimating document language models and either relevance model or mixture model for estimating query language models can be highly recommended. These methods have all been implemented in the Lemur toolkit (http://www.lemurproject.org/).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Future Directions</head><p>Despite much progress has been made in applying language models to IR, there are still many challenges to be solved to fully develop the potential of such models. The following is a list of some interesting opportunities for future research:</p><p>Challenge 1 : Develop an efficient, robust and effective language model for ad hoc retrieval that can (1) optimize retrieval parameters automatically, (2) perform as well as or better than well-tuned traditional retrieval methods with pseudo feedback (e.g., BM25 with Rocchio), and (3) be computed as efficiently as traditional retrieval methods. Would some kind of language model eventually replace the currently popular BM25 and Rocchio? How to implement IDF more explicitly in a language modeling approach may be an important issue to further study. Relaxing the assumption that the same words occur independently in a document (e.g., by using the Dirichlet Compound Model) may also be necessary to capture TF normalization more accurately.</p><p>Challenge 2 : Demonstrate consistent and substantial improvement by going beyond unigram language models. While there has been some effort in this direction, the empirical performance improvement of the more sophisticated models over the simple models tends to be insignificant. This is consistent with what has been observed in traditional retrieval models. Would we ever be able to achieve significant improvement over the unigram language models by using higher-order n-gram models or capturing limited syntactic/semantic dependencies among words? As we go beyond unigram language models, reliable estimation of the model becomes more challenging due to the problem of data sparseness. Thus developing better estimation techniques (e.g., those that can lead to optimal weighting of phrases conditioned on weighting of single words) may be critical for making more progress in this direction.</p><p>Challenge 3 : Develop language models to support personalized search. Using more user information and a user's search context to better infer a user's information need is essential for optimizing search accuracy. This is especially important when the search results are not satisfactory and the user would reformulate the query many times. How can we use language models to accurately represent a user's interest and further incorporate such knowledge into a retrieval model? Detailed analysis of user actions (e.g., skipping some results and viewing others, deleting query terms but adding them back later, recurring interests vs. adhoc information needs) may be necessary to obtain an accurate representation of a user's information need.</p><p>Challenge 4 : Develop language models that can support "life-time learning." One important advantage of language models is the potential benefit from improved estimation of the models based on additional training data. As a search engine is being used, we will be able to collect a lot of implicit feedback information such as clickthroughs. How can we develop language models that can learn from all such feedback information from all the users to optimize retrieval results for future queries? From the viewpoint of personalized search, how can we leverage many users of a system to improve performance for a particular user (i.e., supporting collaborative search)? Translation models appear to be especially promising in this direction, and they are complementary with the recently developed discriminative models for learning to rank documents such as RankNet <ref type="bibr" target="#b11">[12]</ref> and Ranking SVM <ref type="bibr" target="#b42">[43]</ref>. It should be extremely interesting to study how to combine these two complementary approaches.</p><p>Challenge 5 : Develop language models that can model document structures and subtopics. Most existing work on studying retrieval models, including work on language models, has assumed a simple bag-ofwords representation of text. While such a representation ensures that the model would work for any text, in a specific application, documents often have certain structures that can be potentially exploited to improve search accuracy. For example, often it is some part of a long document that is relevant. How can we model potentially different subtopics in a single document and match only the relevant part of a document with a query? Mixture models and hidden Markov models may be promising in this direction.</p><p>Challenge 6 : Generalize language models to support ranking of both unstructured and structured data. Traditionally, structured data and unstructured data (text) have been managed in different ways with structured data mainly handled through a relational database while unstructured data through an information retrieval system, leading to two different research communities (i.e., the database community and the information retrieval community). Recently, however, the boundary between the two communities seems to become vague. First, as exploratory search on databases becomes more and more popular on the Web, DB researchers are now paying much attention to the problem of ranking structured data in a database. The information needs to be satisfied are very similar to those in a retrieval system. Second, some database fields may contain long text (e.g., abstracts of research surveys), while most text documents also have some structured meta-data (e.g., authors, dates). Thus a very interesting question is whether we can generalize language models to develop unified probabilistic models for searching/ranking both structured data and unstructured data. The INEX initiative (http://inex.is.informatik.uniduisburg.de/) has stimulated a lot of research in developing XML retrieval models (i.e., semi-structured data retrieval models), but we are still far from a unified model for unstructured, semi-structured, and structured data.</p><p>Challenge 7 : Develop language models for hypertext retrieval. As an abstract representation, the Web can be regarded a hypertext collection. Language models developed so far have not explicitly incorporated hyperlinks and the associated anchor text into the model. How can we use language modeling to develop a hypertext retrieval model for Web search? How should we define a generative model for hypertext?</p><p>Challenge 8 : Develop/Extend language models for retrieval with complex information needs. Language models are natural for modeling topical relevance. But in many retrieval applications, a user's information need consists of multiple dimensions of preferences with topical relevance being only one of them. Other factors such as readability, genre, and sentiment may also be important. How can we use language models to capture such nontopical aspects? How can we develop or extend language models to optimize ranking of documents based on multiple factors? In this direction, recent work has shown that the learning-to-rank approaches are quite promising, thus again it would be very interesting to study how to combine the language modeling approaches (generative approaches) with the learning-to-rank approaches (discriminative approaches).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>6. 7</head><label>7</label><figDesc>Predicting Query Difficulty 187 Formally, let θ D 2 be a language model estimated using D 2 , we define the redundancy of D 1 with respect to D 2 as λ * = arg max λ log p(D 1 |λ, θ D 2 ) , D 1 ) log(λp(w|θ D 2 ) + (1λ)p(w|C)), (6.6)</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The relevance model<ref type="bibr" target="#b54">[55]</ref> to be discussed later can be regarded as a technique to solve the parameter estimation problem in these classic probabilistic models.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>One can also let the Dirichlet prior smoothing parameter µ take a term-specific value µ i for term w i to achieve term-specific smoothing, for multinomial, but this is not as natural as in the case of multiple Bernoulli.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>This point will be further elaborated in Section 3.2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_3"><p>A stronger assumption p(R, D) = p(R)p(D) has been used in<ref type="bibr" target="#b50">[51]</ref> to derive this relation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_4"><p>Note that although the two ways to rank documents are equivalent in the ad hoc retrieval setting, where we always compare documents for the same query and can thus ignore any document-independent constant, the KL-divergence and cross entropy would generate quite different results when we compare scores across different queries as in the case of filtering or topic detection and tracking<ref type="bibr" target="#b45">[46]</ref>. Specifically, one measure may generate scores more comparable across queries than the other, depending on whether including the query entropy makes sense. For a detailed analysis of this difference and alternative ways of normalizing scores, see<ref type="bibr" target="#b45">[46]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_5"><p>The formula given in<ref type="bibr" target="#b54">[55]</ref> is p(M i |w) = p(w|M i )p(w)/p(M i ), which is probably meant to be p(M i |w) = p(w|M i )p(M i )/p(w); M i is the same as θ D .</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>I want to thank Jamie Callan, editor of Foundations and Trends in IR, for the opportunity to write this review, for his technical feedback about the review, and for his periodical reminders which ensured progress in my writing. I also want to thank him for encouraging me to give a tutorial on this topic at several conferences. This survey is largely a written elaboration of the tutorial. I am very grateful to John Lafferty for supervising my dissertation on this topic. The conceptual framework and a substantial part of this survey are based on my dissertation work. I want to thank Donald Metzler and two anonymous reviewers for their many useful comments and suggestions on how to improve this survey. (Naturally, any errors that remain are solely my own responsibility.) I also wants to thank many peer researchers, especially W. Bruce  Croft, Stephen Robertson, Victor Lavrenko, Rong Jin, Tao Tao, David A. Evans, and Wessel Kraaij, for their useful discussions on various issues about the topic covered in this survey. I give special thanks to James Finlay and Mike Casey of Now Publishers for their help and support in preparing the final version of this review. My own research work covered in this survey is supported in part by the Advanced Research and Development Activity in Information 204 Acknowledgments Technology (ARDA) under its Statistical Language Modeling for Information Retrieval Research Program, by the National Science Foundation under a CAREER grant (IIS-0347933), by Google, Microsoft, and IBM through their research programs, and by the Alfred P. Sloan Foundation through a research fellowship. Any opinions, findings, and conclusions, or recommendations expressed in this paper are those of the author and do not necessarily reflect the views of the sponsors.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Probabilistic models of information retrieval based on measuring the divergence from randomness</title>
		<author>
			<persName><forename type="first">G</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J V</forename><surname>Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information System</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="357" to="389" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using query contexts in information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR 2007</title>
		<meeting>ACM SIGIR 2007</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="15" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Formal models for expert finding in enterprise corpora</title>
		<author>
			<persName><forename type="first">K</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Azzopardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR-06</title>
		<meeting>SIGIR-06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Information retrieval as statistical translation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="222" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Weaver system for document retrieval</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC 1999</title>
		<meeting>TREC 1999</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Statistical Decision Theory and Bayesian Analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Berger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Springer-Verlap</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical topic models and the nested Chinese restaurant process</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Correlated topic models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS &apos;05: Advances in Neural Information Processing Systems</title>
		<meeting>NIPS &apos;05: Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic topic models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning</title>
		<meeting>the 23rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A statistical approach to machine translation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cocke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Roossin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="85" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to rank using gradient descent</title>
		<author>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML &apos;05: Proceedings of the 22nd International Conference on Machine Learning</title>
		<meeting><address><addrLine>USA, New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to rank with nonsmooth cost functions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ragno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS 2006</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hoffman</surname></persName>
		</editor>
		<meeting>NIPS 2006</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="193" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Integrating word relationships into language models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 2005 ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="298" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to rank: From pairwise approach to listwise approach</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-F</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML 2007</title>
		<title level="s">ACM International Conference Proceeding Sereies</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<meeting>ICML 2007</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">227</biblScope>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The use of MMR, diversity-based reranking for reordering documents and producing summaries</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR&apos;98</title>
		<meeting>SIGIR&apos;98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="335" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An empirical study of smoothing techniques for language modeling</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
		<idno>TR-10-98</idno>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Harvard University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Query expansion using random walk models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM CIKM 2005</title>
		<meeting>ACM CIKM 2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="704" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimation and use of uncertainty in pseudo-relevance feedback</title>
		<author>
			<persName><forename type="first">K</forename><surname>Collins-Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR 2007</title>
		<meeting>ACM SIGIR 2007</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="303" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using probabilistic models of document retrieval without relevance information</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="285" to="295" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predicting query performance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cronen-Townsend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual International ACM SIGIR conference on Research and Development in Information Retrieval (SIGIR 2002)</title>
		<meeting>the 25th Annual International ACM SIGIR conference on Research and Development in Information Retrieval (SIGIR 2002)</meeting>
		<imprint>
			<date type="published" when="2002-08">August 2002</date>
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Royal Statistical Society B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Noun-phrase analysis in unrestricted text for information retrieval</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 1996</title>
		<meeting>ACL 1996</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A formal study of information retrieval heuristics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 2004 ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Probabilistic models for expert finding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECIR 2007</title>
		<meeting>ECIR 2007</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="418" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Probabilistic models in information retrieval</title>
		<author>
			<persName><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Computer Journal</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="243" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Language models and uncertain inference in information retrieval</title>
		<author>
			<persName><forename type="first">N</forename><surname>Fuhr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Language Modeling and IR Workshop</title>
		<meeting>the Language Modeling and IR Workshop</meeting>
		<imprint>
			<date type="published" when="2001-06-01">May 31-June 1 2001</date>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dependence language model for information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;04: Proceedings of the 27th Annual International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>USA, New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="170" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Information retrieval: Algorithms and heuristics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Frieder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A probabilistic justification for using tf x idf term weighting in information retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Digital Libraries</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="131" to="139" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Using language models for information retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>University of Twente</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Term-specific smoothing for the language modeling approach to information retrieval: The importance of a query term</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR</title>
		<meeting>ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="35" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Statistical language models for intelligent XML retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Search on XML Data</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="107" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Twenty-One at TREC-7: Ad-hoc and crosslanguage track</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Seventh Text REtrieval Conference (TREC-7)</title>
		<meeting>Seventh Text REtrieval Conference (TREC-7)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="227" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Parsimonious language models for information retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The cluster-abstraction model: Unsupervised learning of topic hierarchies from text data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI&apos; 99</title>
		<meeting>IJCAI&apos; 99</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="682" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of UAI 1999</title>
		<meeting>UAI 1999</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Probabilistic latent semantic indexing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR&apos;99</title>
		<meeting>ACM SIGIR&apos;99</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<title level="m">Statistical Methods for Speech Recognition</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Interpolated estimation of Markov source parameters from sparse data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition in Practice</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Gelsema</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">N</forename></persName>
		</editor>
		<imprint>
			<date type="published" when="1980">1980</date>
			<biblScope unit="page" from="381" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Topic tracking for radio, tv broadcast, and newswire</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Walls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the DARPA Broadcast News Workshop</title>
		<meeting>the DARPA Broadcast News Workshop</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="199" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Title language model for information retrieval</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR</title>
		<meeting>ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="42" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM KDD</title>
		<meeting>the ACM KDD</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Estimation of probabilities from sparse data for the language model component of a speech recognizer</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="400" to="401" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improved backing-off for m-gram language modeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kneser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Variations on language modeling for information retrieval</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>University of Twente</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The importance of prior probabilities for entry page search</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Westerveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR 2002</title>
		<meeting>ACM SIGIR 2002</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Corpus structure, language models, and ad hoc information retrieval</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kurland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;04: Proceedings of the 27th Annual International Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="194" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">PageRank without hyperlinks: Structural re-ranking using links induced by language models</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kurland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;05: Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>USA, New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="306" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Document language models, query models, and risk minimization for information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR&apos;01</title>
		<meeting>SIGIR&apos;01</meeting>
		<imprint>
			<date type="published" when="2001-09">September 2001</date>
			<biblScope unit="page" from="111" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Probabilistic relevance models based on document and query generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Modeling and Information Retrieval</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</editor>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A generative theory of relevance</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Relevance models for topic detection and tracking</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deguzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Laflamme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Human Language Technology Research</title>
		<meeting>the Second International Conference on Human Language Technology Research<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="115" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Cross-lingual relevance models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Choquette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR</title>
		<meeting>ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Relevance-based Language Models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR&apos;01</title>
		<meeting>SIGIR&apos;01</meeting>
		<imprint>
			<date type="published" when="2001-09">September 2001</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Representation and learning in information retrieval</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lewis</surname></persName>
		</author>
		<idno>91-93</idno>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pachinko allocation: DAG-structured mixture models of topic correlations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML &apos;06: Proceedings of the 23rd International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Time-based language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;03: Proceedings of the Twelfth International Conference on Information and Knowledge Management</title>
		<meeting><address><addrLine>USA, New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="469" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Passage retrieval based on language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;02: Proceedings of the Eleventh International Conference on Information and Knowledge Management</title>
		<meeting><address><addrLine>USA, New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="375" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Cluster-based retrieval using language models</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;04: Proceedings of the 27th Annual International Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="186" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A hierarchical Dirichlet language model</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Peto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="289" to="307" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Modeling word burstiness using the Dirichlet distribution</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Madsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kauchak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML &apos;05: Proceedings of the 22nd International Conference on Machine Learning</title>
		<meeting><address><addrLine>USA, New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schutze</surname></persName>
		</author>
		<title level="m">Introduction to Information Retrieval</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A comparison of event models for Naive Bayes text classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI-1998 Learning for Text Categorization Workshop</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A study of Poisson query generation model for information retrieval</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="319" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A mixture model for contextual text mining</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD &apos;06</title>
		<meeting>KDD &apos;06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="649" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A general optimization framework for smoothing language models on graph structures</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;08: Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>USA, New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="611" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A Markov random field model for term dependencies</title>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 2005 ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="472" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Formal multiple-Bernoulli models for language modeling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lavrenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;04: Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>USA, New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="540" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A hidden Markov model information retrieval system</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Expectation-propagation for the generative aspect model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the UAI</title>
		<meeting>the UAI</meeting>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="352" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Capturing term dependencies using a language model based on sentence trees</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;02: Proceedings of the Eleventh International Conference on Information and Knowledge Management</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="383" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Relevant query feedback in statistical language modeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Croft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;03: Proceedings of the Twelfth International Conference on Information and Knowledge Management</title>
		<meeting><address><addrLine>USA, New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="560" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">On structuring probabilistic dependencies in stochastic language modeling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Essen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kneser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A maximum likelihood ratio information retrieval model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Text REtrieval Conference (TREC-8)</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Voorhees</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Harman</surname></persName>
		</editor>
		<meeting>the Eighth Text REtrieval Conference (TREC-8)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="483" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Experiments using the Lemur toolkit</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>in Proceedings of the 2001 TREC conference</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Using language models for flat text queries in XML retrieval</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Initiative for the Evaluation of XML Retrieval Workshop (INEX 2003)</title>
		<meeting>the Initiative for the Evaluation of XML Retrieval Workshop (INEX 2003)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Combining document representations for knownitem search</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR 2003</title>
		<meeting>ACM SIGIR 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="143" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">A language modeling approach to information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ponte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts at Amherst</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A language modeling approach to information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGIR&apos;98</title>
		<meeting>the ACM SIGIR&apos;98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="275" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A study of relevance propagation for web search</title>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2005</title>
		<meeting>SIGIR 2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="408" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="285" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Relevance weighting of search terms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="129" to="146" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR&apos;94</title>
		<meeting>SIGIR&apos;94</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="232" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">The probability ranking principle in IR</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="294" to="304" />
			<date type="published" when="1977-12">December 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Okapi at TREC-3</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sparck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName><surname>Gatford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Third Text Retrieval Conference (TREC-3)</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Harman</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="109" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Relevance feedback in information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rocchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The SMART Retrieval System: Experiments in Automatic Document Processing</title>
		<imprint>
			<publisher>Prentice-Hall Inc</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">A parallel derivation of probabilistic information retrieval models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Roelleke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;06: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Automatic Text Processing: The Transformation, Analysis and Retrieval of Information by Computer</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Introduction to Modern Information Retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcgill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A theory of term importance in automatic text analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="33" to="44" />
			<date type="published" when="1975-02">Jan-Feb 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A probabilistic relevance propagation model for hypertext retrieval</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shakery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM 2006</title>
		<meeting>CIKM 2006</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="550" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Smoothing document language models with probabilistic term count propagation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shakery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="139" to="164" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Context-sensitive information retrieval using implicit feedback</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2005</title>
		<meeting>SIGIR 2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A language modeling framework for resource selection and results merging</title>
		<author>
			<persName><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CIKM 2002</title>
		<meeting>CIKM 2002</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="391" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Pivoted document length normalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1996 ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 1996 ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A general language model for information retrieval</title>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 1999 ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="279" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Language modeling and relevance</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Sparck</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Modeling for Information Retrieval</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="57" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Language models for topic tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Spitters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kraaij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Modeling for Information Retrieval</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="95" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Biterm language models for document retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Srikanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;02: Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="425" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Exploiting syntactic structure of queries in a language modeling approach to IR</title>
		<author>
			<persName><forename type="first">M</forename><surname>Srikanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;03: Proceedings of the Twelfth International Conference on Information and Knowledge Management</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="476" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Incorporating query term dependencies in language models for document retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Srikanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;03: Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="405" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Probabilistic authortopic models for information discovery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rosen-Zvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD&apos;04</title>
		<meeting>KDD&apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="306" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Information retrieval using robust natural language processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Strzalkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vauthey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th annual meeting on Association for Computational Linguistics</title>
		<meeting>the 30th annual meeting on Association for Computational Linguistics<address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Mining long-term search history to improve search accuracy</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="718" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Language model information retrieval with document expansion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics<address><addrLine>Morristown, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="407" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Mixture clustering model for pseudo feedback in information retrieval</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Meeting of the International Federation of Classification Societies</title>
		<meeting>the 2004 Meeting of the International Federation of Classification Societies</meeting>
		<imprint>
			<publisher>Spriner</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Regularized estimation of mixture models for robust pseudo-relevance feedback</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR</title>
		<meeting>ACM SIGIR</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page" from="162" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Evaluation of an inference network-based retrieval model</title>
		<author>
			<persName><forename type="first">H</forename><surname>Turtle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="187" to="222" />
			<date type="published" when="1991-07">July 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">A theoretical basis for the use of co-occurrence data in information retrieval</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Documentation</title>
		<imprint>
			<biblScope unit="page" from="106" to="119" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">A non-classical logic for information retrieval</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Van Rijsbergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="481" to="485" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Improve retrieval accuracy for difficult queries using negative feedback</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;07: Proceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge Management</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="991" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">LDA-based document models for ad-hoc retrieval</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;06: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="178" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">A probability distribution model for information retrieval</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K M</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing and Management</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="39" to="53" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">On modeling information retrieval with probabilistic inference</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K M</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="69" to="99" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Query expansion using local and global document analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGIR&apos;96</title>
		<meeting>the SIGIR&apos;96</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="4" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Cluster-based language models for distributed retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGIR&apos;99</title>
		<meeting>the SIGIR&apos;99</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="254" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Evaluating a probabilistic model for cross-lingual information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;01: Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>USA, New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="105" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Bayesian extension to the language model for ad hoc information retrieval</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR 2003</title>
		<meeting>ACM SIGIR 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="4" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Fast statistical parsing of noun phrases for document indexing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Conference on Applied Natural Language Processing (ANLP-97)</title>
		<imprint>
			<date type="published" when="1997-04-03">March 31-April 3 1997</date>
			<biblScope unit="page" from="312" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">Risk minimization and language modeling in text retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Beyond independent relevance: Methods and evaluation metrics for subtopic retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR&apos;03</title>
		<meeting>ACM SIGIR&apos;03</meeting>
		<imprint>
			<date type="published" when="2003-08">August 2003</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Model-based feedback in the language modeling approach to information retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth International Conference on Information and Knowledge Management</title>
		<imprint>
			<publisher>CIKM</publisher>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">A study of smoothing methods for language models applied to ad hoc information retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR&apos;01</title>
		<meeting>ACM SIGIR&apos;01</meeting>
		<imprint>
			<date type="published" when="2001-09">September 2001</date>
			<biblScope unit="page" from="334" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Two-stage language models for information retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR&apos;02</title>
		<meeting>ACM SIGIR&apos;02</meeting>
		<imprint>
			<date type="published" when="2002-08">August 2002</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="179" to="214" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">A risk minimization framework for information retrieval</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing Management</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="31" to="55" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">UIUC/MUSC at TREC 2005 Genomics Track</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Velivelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shakery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC 2005</title>
		<meeting>TREC 2005</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Improving the robustness of language models -UIUC TREC 2003 robust and genomics experiments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC 2003</title>
		<meeting>TREC 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="667" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">A cross-collection mixture model for comparative text minning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Velivelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 10th ACM SIGKDD International Conference on Knowledge Discovery in Data Mining</title>
		<meeting>eeding of the 10th ACM SIGKDD International Conference on Knowledge Discovery in Data Mining</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="743" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Redundancy detection in adaptive filtering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR&apos;02</title>
		<meeting>SIGIR&apos;02</meeting>
		<imprint>
			<date type="published" when="2002-08">August 2002</date>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Document quality models for web ad hoc retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;05: Proceedings of the 14th ACM International Conference on Information and Knowledge Management</title>
		<meeting><address><addrLine>USA, New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="331" to="332" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
