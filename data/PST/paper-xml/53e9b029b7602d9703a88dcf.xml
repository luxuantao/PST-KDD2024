<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards a Computational Model of Sketching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kenneth</forename><forename type="middle">D</forename><surname>Forbus</surname></persName>
							<email>forbus@nwu.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Qualitative Reasoning Group</orgName>
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<addrLine>1890 Maple Avenue</addrLine>
									<postCode>60201</postCode>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ronald</forename><forename type="middle">W</forename><surname>Ferguson</surname></persName>
							<email>ferguson@nwu.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Qualitative Reasoning Group</orgName>
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<addrLine>1890 Maple Avenue</addrLine>
									<postCode>60201</postCode>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeffery</forename><forename type="middle">M</forename><surname>Usher</surname></persName>
							<email>usher@ils.nwu.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Qualitative Reasoning Group</orgName>
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<addrLine>1890 Maple Avenue</addrLine>
									<postCode>60201</postCode>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards a Computational Model of Sketching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">45DACD1739AD003E677F9566C75564C1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I</term>
					<term>2</term>
					<term>1[Artificial Intelligence] Applications and Expert Systems</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sketching is a powerful means of interpersonal communication. While many useful multimodal systems have been created, current systems are far from achieving human-like participation in sketching. A computational model of sketching would help characterize these differences and help us better understand how to overcome them. This paper is a first step towards such a model. We start with an example of a sketching system (nuSketch COA Creator) designed to aid military planners, to provide context and a source of examples. We then describe four dimensions of sketching, visual understanding, conceptual understanding, language understanding, and drawing, that can be used to characterize the competence of existing systems and identify open problems. The issues involved will be illustrated by examples from our experience with nuSketch. Three research challenges are posed, to serve as milestones towards a computational model of sketching that can explain and replicate human abilities in this area.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Person-to-person communication often involves diagrams, charts, and drawings on white boards and other shared surfaces. People point, mark, highlight, underscore, and use other gestures to help disambiguate what they are saying. Being able to use multiple modalities, i.e., speech and gesture, to communicate ideas is especially crucial for spatial information <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b27">27]</ref>. The ability to understand spatial representations, and to use them appropriately in dialogue, is a critical skill that we need to embed in software, in order to create systems that better understand the users they are interacting with.</p><p>We focus here on sketching, by which we mean a communication activity involving a combination of interactive drawing plus linguistic interaction. The drawing carries the spatial aspects of what is to be communicated. The linguistic interaction provides a complementary conceptual channel that guides the interpretation of what is drawn and provides information that is not easily depicted spatially. Most people are not artists, and even artists cannot produce, in real time, drawings of complex objects and relationships that are recognizable solely visually without breaking the flow of conversation. The verbal description that occurs during drawing, punctuated by written labels, compensates for inaccuracies in drawing. Follow-up questions may be needed to disambiguate what aspects of a drawing are intended versus accidental.</p><p>There is now a substantial body of research on multimodal interfaces <ref type="bibr" target="#b24">[24]</ref>. Sketching is clearly a form of multimodal interaction, but not all multimodal interactions are sketching. Many multimodal interfaces focus on placement of predefined entities, e.g., selecting a location, often via pointing (cf. <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b7">7]</ref>). Such selection operations require a minimal shared understanding on the part of the participants, and hence has provided a natural starting point for multimodal interface research. Work that comes closer to sketching (cf. <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b29">29]</ref>) incorporates more domain semantics, to increase the level of shared understanding. This progression suggests that to achieve the kind of flexible interaction that sketching provides in human-to-human communication, multimodal research will rely heavily upon, and even drive, AI research. This paper examines sketching in that light, to provide a framework for understanding the phenomena and suggesting new research directions.</p><p>The rest of this paper describes our progress towards a computational model of sketching. We start with an example, our nuSketch multimodal interface architecture, showing how it has been used to create a system for creating and reasoning about military courses of action sketches (nuSketch COA Creator). We then step back and describe a framework for sketching, motivated by a combination of constraints from computation and from cognitive science research. We end by identifying three challenges for research on sketching, as a potential way to benchmark progress in the area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">nuSketch: A MULTIMODAL ARCHITECTURE FOR SKETCHING</head><p>nuSketch is designed as a general-purpose multimodal architecture to support sketching. The best way to illustrate nuSketch's abilities is through an example application. Military planners use a Course of Action sketch (COA sketch) when planning an operation. COA sketches express the gist of a plan, before many details, such as timing, have been worked out. Traditionally such sketches are created using acetate overlays on maps, or on paper starting with hand-drawn abstractions of critical Figure <ref type="figure" target="#fig_0">1</ref> illustrates a course of action drawn using the nuSketch COA Creator. A layer metaphor organizes the interface. Like acetate layers, each nuSketch layer corresponds to some category of domain information, such as terrain analysis, enemy disposition, disposition of your units, and so forth. Switching between layers is accomplished by clicking on the tabs to the left. Multiple layers can be displayed at once, or hidden, or grayed out as convenient. Maps as backgrounds for sketching are provided by bitmap layers, which are registered against geospatial coordinate systems.</p><p>While multiple layers can be visible at any time, only one layer at a time is active. The choice of active layer determines how user inputs are interpreted. For example, the Terrain Features layer enables planners to draw key properties of an area, such as rivers, mountains, cities, roads, and so on. <ref type="foot" target="#foot_0">1</ref> The Terrain Class layer enables users to mark regions that restrict vehicular movement (trafficability), due to factors such as slope, soil type, or vegetation. Items are drawn on layers via spoken commands (e.g., "Add severely restricted terrain") accompanied by gestures, whose interpretations depend on the command. For adding regions, the curve drawn is taken to be the boundary of the region, so it is closed and filled with the appropriate texture to indicate that the command was understood. For adding standard symbols, e.g. towns or military units, the user's gesture indicates a bounding box, and the appropriate glyph is retrieved from the KB and displayed there, scaled appropriately. Military tasks are more complex, requiring several gestures to specify which unit is doing the task, what the task is being done to, and often properties such as where it is being done and what path should be taken.</p><p>When a new glyph is created, a set of assertions constituting the system's conceptual understanding of that visual element and what it represents in domain terms is also created. This conceptual understanding facilitates reasoning to support the user. For instance, geographic queries are made by dragging and dropping sketch elements onto a simple parameterized dialog (Figure <ref type="figure" target="#fig_1">2</ref>). These queries are answered by using qualitative and visual reasoning to interpret the spatial entities and relationships in the sketch <ref type="bibr" target="#b12">[12]</ref>. Similarly, users can request critiques based on analogies with prior plans, with the application of the advice to their plan illustrated by the system highlighting the appropriate visual elements of the sketch (Figure <ref type="figure">3</ref>). The analogies are based on the visual and conceptual descriptions constructed during sketching, which are fed to a general-purpose analogy matcher, the Structure-Mapping Engine (SME) <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b14">14]</ref>. The cases are also created via sketching, with additional information about the relative wisdom of particular decisions stored in the case as a basis for providing advice.</p><p>The nuSketch COA Creator has been developed using a joint applications development model, using a combination of Army interns on the development team and frequent formative feedback from other military personnel. It has been distributed to a small group of alpha testers 2 , with very encouraging results. As a consequence, experiments with the system are planned for 2 Our alpha testers include active US military personnel from several commands, retired military personnel associated with DARPA's Command Post of the Future program ("graybeards"), and researchers from several institutions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The nuSketch Architecture</head><p>Figure <ref type="figure">4</ref> shows the nuSketch architecture. The Ink Processor accepts pen input, does simple signal processing, and passes timestamped data to the Multimodal Parser. The other input to the multimodal parser is from a commercial speech recognizer, which produces time-stamped text strings. The Multimodal Parser uses grammars that include both linguistic and gesture information, to produce propositions that are interpreted by the Dialogue Manager. The Dialogue Manager and the KB contents are the only application-specific components of nuSketch. The Dialogue Manager is responsible for interpreting propositions and supplying grammars to the speech recognizer and Multimodal Parser based on context (as determined by its own state and the active layer). Central to nuSketch is the use of a knowledge-based reasoner (DTE 3 ), which provides integrated access to a number of reasoning services, including analogical reasoning and geographic reasoning. The Dialogue Manager uses DTE for its reasoning, and all domainspecific knowledge is stored in the KB. For example, the glyphs corresponding to the visual symbols in a domain are stored as part of the knowledge base, so that how something is depicted can be reasoned about (e.g., if a glyph is not available for a specific unit type, a glyph corresponding to a more general type of unit is used).</p><p>Several aspects of nuSketch are inspired by Quickset <ref type="bibr" target="#b7">[7]</ref>, a multimodal interface system for setting up military simulations. Like Quickset, we use off-theshelf speech recognition and time-stamp ink and speech signals to facilitate integrating information across modalities. Quickset incorporates inkrecognition schemes that nuSketch does not (as a matter of principle; see below). Because Quickset was designed as an interface for legacy computer systems, it lacks an integrated reasoning system. As the discussion below will make clear, this significantly limits QuickSet's potential as a model of sketching. For example, it does not reason about depiction as nuSketch can.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dimensions of sketching</head><p>The power of sketching in human communication arises from the high bandwidth it provides <ref type="bibr" target="#b27">[27]</ref>. There is high perceptual bandwidth because the shared drawing is interpreted by the participants' powerful visual apparatus. There is high conceptual bandwidth because the combination of visual and linguistic channels facilitates the interaction needed to create a shared conceptual model. Sketching covers a wide span of activities that occur under a variety of settings. A computational model of sketching must identify what knowledge and skills the participants need for such activities. We characterize these competencies along four dimensions: visual understanding, language understanding, conceptual understanding, and drawing skills. Variations along these dimensions determine how many different types of interactions something having those skills can participate in. We describe each in turn.</p><p>Visual understanding. This dimension characterizes how deeply the spatial properties of the ink are understood. The simplest level of understanding is recognizing gestures. Gestures indicate locations or sizes, often including an action to be taken with regard to something at that location (e.g., selecting or 3 DTE (Domain Theory Environment) is a reasoning system that combines a prolog-style query-driven inference system with a logic-based TMS to enable heterogeneous inference systems to interoperate.  <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b30">30]</ref>. We do not consider a system with only this level of visual understanding to be capable of sketching, since it does not understand the spatial relationships between visual elements.</p><p>The next level of visual understanding is the use of a visual symbology, i.e. a collection of glyphs, representing conceptual elements of the domain whose spatial properties can also convey conceptual meaning. Schematic diagrams in various technical fields and formal visual languages such as the military task language illustrated above are two examples. Is a CAD system a participant in sketching? We argue no, for two reasons. First, it is not taking an active role as a participant. In multimodal interactions, even during data entry the system is engaged in recognizing the kinds of entities and actions the user intends. Second, the time and conceptual overhead needed to deal with menus prevents the maintenance of a conversation-like flow <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b27">27]</ref>. By contrast, multimodal systems that use recognition procedures to "parse" ink automatically (cf. Quickset), or use speech plus gesture to create entities (cf. nuSketch) keep the interaction more like dealing with another person, someone capable of looking at what you are drawing and hearing what you are saying, and responding appropriately. The amount of visual reasoning needed in a sketching interaction depends in part on the desires of the participants. For example, to our surprise, the military personnel we have interacted with typically did not want automatic glyph recognition for hand-drawn unit symbols. To them, it was easier to just say what they wanted and use gesture for placement and sizing. However, we also discovered that, even in what seems to be a tightly restricted domain, human use of visual symbols requires richer visual processing than we had anticipated.</p><p>Most multimodal systems rely on a combination of speech and black-box recognition algorithms (e.g., hidden Markov models or neural nets) operating on digital ink to identify a user's intent (cf. <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b16">16]</ref>). While certainly useful in some applications, we claim that they are detours from paths that will lead to human-like sketching capabilities. The reason is that people are very flexible in their use of visual symbols, and they expect the same flexibility from their partners. Three examples from the military planning domain illustrate this point. First, Figure <ref type="figure">5</ref> illustrates how complex visual symbols can be. In terrain analysis, broad red arrows indicate avenues of approach. This multi-headed "arrow", drawn by a military officer, accurately conveys where units might move. However, it is hard to see how any statistical recognizer could be trained up in advance to recognize such a complex figure. The second example is Figure <ref type="figure">6</ref>, which shows two arrows that are very similar, except for the style in which they are drawn. Most visual symbologies assign different meanings to dashed versus solid arrows, so it would not be enough to simply recognize both of these as arrows. The richness of visual properties that can arise even with very stylized visual symbologies is illustrated by the third example, Figure <ref type="figure">7</ref>, which shows a pair of attacks that has been automatically identified as symmetric by high-level visual reasoning <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref>. Understanding such emergent configurations requires computing spatial relationships across multiple visual elements as they are drawn, and understanding the conceptual import of these relationships.</p><p>These examples suggest that powerful visual skills are one of the keys to human-like sketching. We suspect that work like Saund's <ref type="bibr" target="#b28">[28]</ref> on perceptual organization will play a major role in bringing sketching systems closer to human capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conceptual understanding:</head><p>As a communicative act, sketching requires common ground <ref type="bibr" target="#b3">[3]</ref>; the depth of representation of what is sketched is probably the single strongest factor determining how flexible communication can be. To be sure, there must be enough visual and language understanding, and these can be traded off against each other somewhat, but it is the degree of shared conceptual model that ultimately limits what can be communicated, no matter what modalities are available. As might be expected, this is the weakest area for current systems.</p><p>The simplest level of conceptual understanding for sketching is the ability to handle a fixed collection of types of entities and relationships (cf. <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b30">30]</ref>). It is also the level most commonly used, since it suffices to issue commands to other software systems, the primary purpose of most existing multimodal interfaces. Type information is often used to reduce ambiguity, e.g., if a gesture indicating the argument to a MOVE command might be referring to a tank or a fence, the latter is ruled out.</p><p>Moving beyond identifying an intended command and its arguments requires broader and deeper common ground. Domainspecific systems (e.g., Quickset, particular nuSketch applications) obviously need knowledge about their domain. But there are areas of knowledge that cut across multiple domains of discourse that seem to be necessary to achieve flexible communication via sketching:</p><p>• Qualitative representations of space. Being able to reason about regions, paths, and relative locations is important in every spatial domain <ref type="bibr" target="#b13">[13,</ref><ref type="bibr">8]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Qualitative representations of shape. The ability to abstract away minor differences in order to describe important properties facilitates recognition <ref type="bibr" target="#b9">[9]</ref>.</p><p>We claim that qualitative representations are crucial for several reasons. First, they are well suited for handling the sorts of approximate spatial descriptions provided by hand-drawn figures, layouts, and maps. Second, the level of description they provide is close to the descriptions of continuous properties common in human discourse <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b29">29]</ref>. The nuSketch COA Creator, for instance, relies on qualitative representations to understand geographic questions and as part of the encoding of a situation that facilitates retrieval for generating critiques via analogy.</p><p>Other types of general knowledge are needed for flexible sketching as well:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Graphical conventions. Many conventions used in drawings are deliberately unrealistic, e.g., cutaways to show the internal structure of a complex object. Using sequences of snapshots to depict dynamics requires interpreting spatial repetition as temporal progression. Understanding these conventions is necessary for many types of sketches <ref type="bibr" target="#b21">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Standard visual symbols. Part of our shared visual language consists of simplified drawings that convey complex concepts easily. Stick-figure drawings and many other types of cartoons (cf. <ref type="bibr" target="#b25">[25]</ref>) are examples.</p><p>Graphical conventions and visual symbols require combining visual/spatial knowledge with conceptual knowledge, and thus we suspect are a crucial area for improvement to create better sketching systems.</p><p>Language Understanding: Language provides several services during sketching. It can ease the load on vision by labeling entities and specifying what type of thing is being drawn. It is used for stating what spatial relationships are essential versus accidental, and describing entities and relationships not depicted in the drawing. Speech is the most common linguistic modality used during sketching because it enables visual attention to remain on the diagram, although short handwritten labels are often used as well. Existing multimodal systems tend to use off-the-shelf speech recognition systems, limiting them to finite-state or definite clause grammars (cf. <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b24">24]</ref>). Given the differences in complexity between spoken and written text, such grammars, albeit with multimodal extensions, are likely to remain sufficient <ref type="bibr" target="#b1">[1]</ref>. Consequently, the most important dimension for characterizing language understanding in sketching systems concerns dialogue management <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b21">21]</ref>. Most systems have been command-oriented, with some support for system-initiated clarification questions. We know of no sketching systems that use full mixed-initiative dialogs. We suspect two reasons for this. First, when multimodal interfaces are grafted onto legacy software, the existing output presentation systems are often used. Second, the relatively shallow conceptual understanding used in most systems does not enable them to do much on their own, so they are less likely to be able to interject anything.</p><p>Mixed-initiative dialogues will become crucial as the complexity of sketching increases. For example, we have discovered that the standard technique of using timeouts as the means for identifying when a user has stopped drawing a symbol (cf. <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b16">16]</ref>) fails completely when the spatial shape or configuration of a symbol must carry complex information. The avenues of approach arrows in Figure <ref type="figure">5</ref> illustrate this phenomenon, as do most complex paths drawn on a map. Consider what happens when the user is thinking hard about a path, or tracing a path through complex terrain. Such drawings must be done carefully, since incorrect paths can have serious consequences. Interpreting sketched paths requires care, since it requires detecting and removing unintentional overlaps with terrain features that make the path as literally drawn impossible. The same system timeouts that keep a dialogue moving along when simple stereotyped glyphs are being drawn are far too short in these interactions, leading to intense frustration. It is essential to understand when the user is engaged in a complex drawing, requiring sophisticated interpretation, versus when the user is drawing something that can safely be interpreted as a simple glyph. As the scope of applications tackled becomes broader, e.g., using sketching in knowledge acquisition, the need for richer dialogue models becomes even stronger. For example, as the topics of sketches become more open-ended, the need to engage in clarification dialogues will be more frequent (e.g., "Is the specific position of the nucleus inside the cell significant?").</p><p>Drawing capabilities: Sketching is a two-way street; ideally visual and linguistic expression should be modalities available to all participants (cf. <ref type="bibr" target="#b25">[25]</ref>). The state of the art in natural language generation and text to speech is constantly improving, and such improvements will of course benefit sketching systems. Visual expression by sketching programs provides some new challenges. The simplest forms of visual expression are highlighting and performing operations on human-drawn elements (e.g., moving, rotating, or resizing). Some systems complement their human partners by neatening their diagrams (cf. <ref type="bibr" target="#b20">[20]</ref>), while in other applications maintaining the informal, original ink is vital <ref type="bibr" target="#b21">[21]</ref>. The ability to modify a user's sketch as part of asking a clarification question, and generating new sketches to start a dialog, are beyond the present state of the art. Significant progress has been made on expressing the visual skills needed for graphical production tasks such as layout (cf. <ref type="bibr" target="#b24">[24]</ref>), the key barriers for sketching are the lack of understanding of both the domain and visual representations, as outlined above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CHALLENGES FOR COMPUTATIONAL MODELS OF SKETCHING</head><p>The discussion of the dimensions of sketching should make it clear that, while currently software can be built that participates in sketching in a limited way, the state of the art is far from creating systems that have the depth and flexibility of a human partner. In the spirit of encouraging progress, we suggest three challenges as useful benchmarks to measure progress in the area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Integrated compositional semantics:</head><p>The preponderance of systems that use "black-box" recognizers is more a function of them being easily available and of the limited range of tasks tackled to date than their suitability for use in sketching. An example provides the best illustration. The symbols used on military maps are highly standardized, with thick books providing visual symbols for almost every conceivable occasion. Nevertheless, during military exercises unique visual symbols are sometimes generated to cover special needs. Figure <ref type="figure" target="#fig_3">8</ref> shows an icon, drawn on a post-it, that appeared in several places on an intelligence map in a recent US Army exercise.</p><p>This symbol represents a downed US pilot at its location. Although this symbol cannot be found in any military manual, it is quite easy to interpret. Even non-military people tend to get it after one or two leading questions (what is the thing on the right? Okay, it's a crashed airplane. Who might that be?). There are degrees of ambiguity in the interpretation: Some people interpret the dashed lines coming out of the pilot's head as sweat rather than tears, and some think the person is a passenger. However, no one who is told what the symbol means has trouble identifying the airplane and the pilot and the pilot's unhappy/stressed state as a consequence of a hypothesized crash.</p><p>This seemingly simple interpretation problem requires an enormous amount of knowledge: Of airplanes, pilots, and their relationships, of the visual appearance of airplanes and how modifications of that appearance might be interpreted (i.e., a crashed airplane), and of conventions for depicting people and their states. The ability to combine visual and conceptual understanding in a compositional way to decode everyday sketches is, we believe, a major challenge for computational models of sketching.</p><p>User-extensible visual symbologies: As less restrictive domains are attempted, confining users to a predefined vocabulary of visual symbols will be infeasible. Asking a user to provide dozens to hundreds of samples to train a statistical recognition system in order to add a new glyph, for instance, is quite unnatural. Some systems add a mode that enables users to describe the specific pattern of strokes they will use to draw a new symbol (cf. <ref type="bibr" target="#b16">[16]</ref>), but this method does not scale to complex glyphs (because there are too many ways to draw them), and does not provide a method for tying the new glyph to representations in a knowledge base. When a new glyph is introduced in human-tohuman sketching, the introducer may have to linguistically mark its occurrence for a while when first used, but over time the other participants learn to recognize it. The ability to engage in a mixedinitiative dialogue seems essential for specifying the conceptual meaning of new symbols. Being able to interactively specify the domain semantics of a new glyph, and have the software start picking up how to recognize it through normal interactions, will be an important benchmark since it will enable the bootstrapping of sketching systems.</p><p>Visual analogies: Being able to compare sketches is an important aspect of comparing what the sketches are about (e.g., comparing engineering designs or comparing COAs). Shared history provides an important form of common ground, so the ability to recognize when aspects of the current sketch have been seen before will enable software participants to take on more of a community memory role. Currently there are domain-specific systems that do sketch-based retrieval <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b17">17]</ref>, but these only operate in narrow domains. Some progress has been made on using similarity in visual encoding, particularly to detect symmetry and regularity in line drawings <ref type="bibr" target="#b11">[11]</ref>, but using these and other analogical encoding techniques in visual understanding is currently an area of active research.</p><p>Sketching systems that can carry out such visual analogies and retrievals in a broad range of domains will be another important benchmark in modeling sketching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DISCUSSION</head><p>Sketching is a powerful human-to-human means of communication, and a powerful metaphor for human-computer interaction. We have argued that this power remains mostly untapped, i.e. that the state of the art is still far from creating software that participates in sketching with the same fluency as humans. We argued that two key areas of improvement are depth of conceptual understanding and visual processing. The three challenges we outlined provide, we believe, benchmarks that would mark significant advances towards more human-like sketching systems. Even leaving aside its importance as an interface modality, research on sketching provides an arena for investigating the intersection of conceptual knowledge, visual understanding, and language, making it a valuable area for investigation in order to understand human cognition. We hope that this paper encourages more research in this area.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A Course of Action Sketch terrain features. A well-worked out vocabulary of visual symbols is used to represent terrain features, military units, and tasks assigned to units.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: nuSketch provides an interface to intelligent suppo tools, such as geographic reasoning. Results can be spoken o shown via highlighting on the sketch, as in this query about t shortest trafficable path that enemy tank battalion 1 can tak to reach River Town.</figDesc><graphic coords="2,319.08,369.24,261.12,168.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 Figure 3 :Figure 5 :</head><label>435</label><figDesc>Figure 4: nuSketch architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: A novel, but easily understandable, visual symbol</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Many planners prefer to work from this description, which abstracts out the significant features of an area, instead of the more cluttered view that a map background provides.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>This research was supported by DARPA under the High Performance Knowledge Bases, Command Post of the Future, and Rapid Knowledge Formation programs. Major Robert Rasch (US Army) and Captain Bill Turmel (US Army) played essential roles in the design and development of the nuSketch COA Creator. We thank Brigadier General Keith Holcomb (USMC, retired) and Colonel Michael Heredia (US Army) for many excellent suggestions. We also thank Herb Clark, Brian Dennis, Pat Hayes, Jim Mahoney, Stanley Peters, Eric Saund, and Bill Scherlis for insightful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The TRAINS Project: A Case Study in Defining a Conversational Planning Agent</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental and Theoretical AI</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Put-That-There: Voice and gesture at the graphics interface</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Bolt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="262" to="270" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Using language</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<title level="m">Speaking in Time. Proceedings of the ESCA workshop on dialogue and prosody</title>
		<meeting><address><addrLine>Veldhoven, the Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999-09-01">1999. September 1-3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The role of natural language in a multimodal interface</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UIST</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="143" to="149" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Synergistic use of direct manipulation and natural language</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dalrymple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gargan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schlossberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tyler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CHI-89</title>
		<meeting>CHI-89</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="227" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">QuickSet: Multimodal interaction for distributed applications</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcgee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oviatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pittman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Annual International Multimodal Conference (Multimedia &apos;97)</title>
		<meeting>the Fifth Annual International Multimodal Conference (Multimedia &apos;97)<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1997-11">1997. November 1997</date>
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Calculi for Qualitative Spatial Reasoning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Symbolic Mathematical Computation</title>
		<editor>
			<persName><forename type="first">J A</forename><surname>Calmet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Campbell</surname></persName>
		</editor>
		<editor>
			<persName><surname>Pfalzgraf</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="1996">1996. 1996</date>
			<biblScope unit="volume">1138</biblScope>
			<biblScope unit="page" from="124" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Query Processing in Spatial-Query-by-Sketch</title>
		<author>
			<persName><forename type="first">M</forename><surname>Egenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Languages and Computing</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="403" to="424" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Structure-Mapping Engine: Algorithm and examples</title>
		<author>
			<persName><forename type="first">B</forename><surname>Falkenhainer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gentner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1" to="63" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Forbus</surname></persName>
		</author>
		<title level="m">GeoRep: A Flexible Tool for Spatial Representation of Line Drawings. Proceedings of AAAI-2000</title>
		<meeting><address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Qualitative Spatial Interpretation of Course-of-Action Diagrams</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Turmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Forbus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Qualitative Reasoning</title>
		<meeting>the 14th International Workshop on Qualitative Reasoning<address><addrLine>Morelia, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-06">2000. June, 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Diagrammatic Reasoning: Cognitive and Computational Perspectives</title>
		<author>
			<persName><forename type="first">K</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glasgow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="183" to="202" />
		</imprint>
	</monogr>
	<note>Qualitative Spatial Reasoning: Framework and Frontiers</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Incremental structure-mapping</title>
		<author>
			<persName><forename type="first">K</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gentner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Cognitive Science Society</title>
		<meeting>the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="1994-08">1994. August</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Qualitative Spatial Reasoning: The CLOCK Project</title>
		<author>
			<persName><forename type="first">K</forename><surname>Forbus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Faltings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<date type="published" when="1991-10">October, 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Electronic Cocktail Napkin -computer support for working with diagrams</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Design Studies</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="70" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Drawing Analogies -Supporting Creative Architectural Design with Visual References</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3d International Conference on Computational Models of Creative Design</title>
		<editor>
			<persName><forename type="first">M-L</forename><surname>Maher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gero</surname></persName>
		</editor>
		<meeting><address><addrLine>Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="37" to="58" />
		</imprint>
		<respStmt>
			<orgName>University of Sydney</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attention, Intentions, and the Structure of Discourse</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Sidner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On Line and On Paper: Visual representations, visual culture, and computer graphics in design engineering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Henderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pattern Recogntion and Beautification for a pen-based interface</title>
		<author>
			<persName><forename type="first">Julia</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Faure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICDAR&apos;95: Montreal (Canada)</title>
		<meeting>ICDAR&apos;95: Montreal (Canada)</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="58" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sketching storyboards to illustrate interface behaviors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Landay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI&apos;96 Conference Companion: Human Factors in Computing Systems</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatial parsing for visual languages</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Languages</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Ichikawa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Ligomendes</surname></persName>
		</editor>
		<meeting><address><addrLine>NY</addrLine></address></meeting>
		<imprint>
			<publisher>Plenum Press</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Automated Spoken Dialogue Systems</title>
		<editor>Luperfoy, S.</editor>
		<imprint>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>in preparation</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Maybury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Whalster</surname></persName>
		</author>
		<title level="m">Readings in Intelligent User Interfaces</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Understanding Comics: The Invisible Art</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Mccloud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Kitchen Sink Press</publisher>
			<pubPlace>NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multimodal user interfaces in the Open Agent Architecture</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Cheyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Julia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IUI97</title>
		<meeting>IUI97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ten myths of multimodal interaction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Oviatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="74" to="81" />
			<date type="published" when="1999-11">1999. November, 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Perceptual Organization in an Interactive Sketch Editing Application</title>
		<author>
			<persName><forename type="first">E</forename><surname>Saund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moran</surname></persName>
		</author>
		<idno>ICCV &apos;95</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generating Multiple New Designs from a Sketch</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Stahovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shrobe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Thirteenth National Conference on Artificial Intelligence, AAAI-96</title>
		<meeting>Thirteenth National Conference on Artificial Intelligence, AAAI-96</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="1022" to="1029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multimodal interfaces for multimedia information agents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Suhm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP 97</title>
		<meeting>of ICASSP 97</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
