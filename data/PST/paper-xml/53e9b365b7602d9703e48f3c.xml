<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discerning the Dominant Out-of-Order Performance Advantage: Is it Speculation or Dynamism?</title>
				<funder>
					<orgName type="full">National Defense Science and Engineering Graduate Fellowship</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Mcfarlin</surname></persName>
							<email>dmcfarlin@cmu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Charles</forename><surname>Tucker</surname></persName>
							<email>cetucker@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Craig</forename><surname>Zilles</surname></persName>
							<email>zilles@illinois.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Houston</settlement>
									<region>Texas</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discerning the Dominant Out-of-Order Performance Advantage: Is it Speculation or Dynamism?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D.3.4 [Software]: Programming Languages-Processors:Compilers, Optimization, C.0 [Computer Systems Organization]: General-Hardware/software interfaces General Terms Performance Keywords Optimization</term>
					<term>Speculation</term>
					<term>Dynamic Scheduling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we set out to study the performance advantages of an Out-of-Order (OOO) processor relative to in-order processors with similar execution resources. In particular, we try to tease apart the performance contributions from two sources: the improved schedules enabled by OOO hardware speculation support and its ability to generate different schedules on different occurrences of the same instructions based on operand and functional unit availability. We find that the ability to express good static schedules achieves the bulk of the speedup resulting from OOO. Specifically, of the 53% speedup achieved by OOO relative to a similarly provisioned inorder machine, we find that 88% of that speedup can be achieved by using a single "best" static schedule as suggested by observing an OOO schedule of the code. We discuss the ISA mechanisms that would be required to express these static schedules.</p><p>Furthermore, we find that the benefits of dynamism largely come from two kinds of events that influence the application's critical path: load instructions that miss in the cache only part of the time and branch mispredictions. We find that much of the benefit of OOO dynamism can be achieved by the potentially simpler task of addressing these two behaviors directly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The current state of parallel programming has unleashed a tension of design criteria on modern general-purpose CPUs. There are neither so few nor so many parallel programs that either parallel or single-thread performance can be marginalized. Instead CPU designers must design chips with high single-thread performance, but vigilantly maximize performance/area and performance/watt.</p><p>One important design decision that plays centrally in this tradeoff is the choice between in-order and out-of-order instruction scheduling by the hardware. GPUs and lower-end processors have 1. the OOO has few constraints other than true dependencies and functional unit availability to constrain schedule generation, whereas existing ISAs prevent the expression of some speculative schedules by compilers.</p><p>2. OOO can execute the same instructions in different schedules at different points of the execution, reacting to the specific events observed during each execution. We'll call this advantage dynamism.</p><p>Little effort has been directed at quantifying the contributions of each of these effects, but we believe that this is an important question. Specifically, if the difference between OOO and in-order is largely not the result of dynamism, but rather due to the inability of the compiler to express the desired schedule given the available ISA interface, this is something that can be addressed by further ISA development.</p><p>To this end, this paper reports a study we undertook to characterize the relative importance of dynamism in OOO and better understand the schedule differences between in-order and OOO schedulers.</p><p>This paper makes the following contributions:</p><p>? We demonstrate that most (on average 88%) of the OOO's performance advantage is due to the speculation support that enables the formation of high quality schedules. ? We show that the OOO's remaining performance advantage from dynamism is predominantly from its ability to deploy different schedules in the immediate locus of a cache miss or branch misprediction. ? We quantify the ability of a complexity-effective, "off-theshelf" solution for cache miss recovery coupled with a simple mechanism for misprediction recovery to cover the remaining performance gap to the OOO. ? We provide recommendations for realizing performance competitive in-order designs given current technology and workload trends.</p><p>The remainder of the paper is organized as follows: Section 2 elaborates on the need for the present work. Section 2.1 examines the motivation for the strong tendency towards OOO designs in industry and the factors limiting the performance competitiveness of inorder designs. In Section 2.2 we employ a criticality framework to explore the theoretical and practical implications of dynamism on performance and provide intuition as to its principle and we argue, limited advantages over the OOO's baseline speculation support. Section 3 looks at our methodology for quantifying the relative contributions of dynamism and speculation to OOO performance; the presentation and discussion of our results appear in Sections 4 and 5. The rich history of the in-order vs. OOO debate along with the challenges academia and industry encountered in trying to match the OOO's performance are explored in Section 6. We conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background/Motivation</head><p>Previous work has observed that OOO scheduling hardware tends to spend most of its time scheduling the same small subset of the static program, and moreover tends to create the same cycle-bycycle schedule repeatedly. Caching these schedules is the motivation for Execution Caching <ref type="bibr" target="#b46">[47]</ref>, to allow the scheduler to be shut down (which they argue produces power savings) some of the time.</p><p>Our work extends this intuition in several ways. First, previous work has been in terms of actual architectural implementations, which tends to mask the theoretical limitations of static scheduling with implementation details. We seek conclusions applicable more generally to statically scheduled architectures. In addition, we want to tease apart the OOO performance advantage to see whether an aggressive in-order design has any hope of matching its performance, and what sorts of features it would require to do so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Perceived Out-of-Order Performance Advantage</head><p>Despite some of the well-known advantages offered by in-order designs (usually lower power, higher frequency, and reduced HW complexity), Out-of-Order designs dominate general purpose computing and are proliferating into domains where OOO designs were conventionally seen as poorly suited e.g. transaction processing <ref type="bibr" target="#b40">[41]</ref>, data/control-plane processing <ref type="bibr" target="#b19">[20]</ref> and the embedded space <ref type="bibr" target="#b15">[16]</ref>. While direct comparisons between in-order and OOO designs are difficult, OOO designs seem to consistently provide higher single-thread performance relative to comparably provisioned inorder designs <ref type="bibr" target="#b21">[22]</ref>.</p><p>The common explanation for the IO/OOO performance disparity is that the OOO is inherently better at exploiting memory-level parallelism as even the most sophisticated static schedule is hobbled by the stall-on-use/head-of-line blocking problem inherent to cache misses in IO designs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22]</ref>. In contrast, the enduring trend over several generations of OOO designs is their ability to toler-</p><formula xml:id="formula_0">!" #" $!" $#" %!" %#" &amp;!" &amp;#" '!" !" #" $" %" &amp;" '" (" )" *" !+" !!" !#" !$" !%"</formula><p>!&amp;" ,-./0,12340"56/3,"57-,8/" ,9:;&lt;=&gt;?@&lt;"5A;B="5C9=D"-@&lt;ECAF"-:"6"%GHAIJ""'%G0:KEI"3LJ"!#)G0:KEI" M-5"-&gt;KG9NG-EO&lt;EP"6@&lt;EAQ&lt;O"N9E"/R0,B:K"#+++"SAT&lt;E"4ACC&gt;EB"&lt;K"ACPU" VR&lt;E=&lt;:KAQ&lt;"9N"0W&lt;=&gt;?9:",I=C&lt;;X""" (!)"*+",,," -./012*3"4506/7" 4*389:3";771/"&lt;=*1&gt;7" ?:8@";378=102*37" A/6/08/B"C=*D"E&gt;"F*" '"4*37/012G/"H97:0" H6*0I7" ate an ever increasing number of longer latency cache misses; Intel's 22nm Ivy Bridge boasts a window size nearly twice that of the 45nm Penryn <ref type="bibr" target="#b23">[24]</ref>. Indeed, given the ever increasing complexity and non-determinism of the memory hierarchy, the MLP advantage seems to be what is motivating the trend towards OOO designs. Recent in-order processors have explicitly incorporated HW features to "buy back" <ref type="bibr" target="#b27">[28]</ref> this loss of MLP relative to the OOO including run-ahead execution (Power6) and Subordinate scout threading (Rock) <ref type="bibr" target="#b21">[22]</ref>. Yet, a recent in-order design that combined a sophisticated run-ahead scheme, advanced prefetchers, register checkpointing, and gated store-buffer with a good compiler was still some 42% slower than a comparably provisioned OOO design <ref type="bibr" target="#b21">[22]</ref>. Why? Figures 1 and 2 highlight a major challenge for in-order designs in matching the OOO: the OOO's ability to simultaneously execute instructions from a consistently larger number of consecutive basic blocks. Figure <ref type="figure" target="#fig_0">1</ref> compares the average region size (number of basic blocks) found by some of the best performing static scheduling techniques documented in the literature when applied to SPECint 2000. Trace Scheduling <ref type="bibr" target="#b41">[42]</ref>, Superblock <ref type="bibr" target="#b20">[21]</ref> and Hyperblock <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37]</ref> are all well known, often profile-driven global scheduling techniques employed by static compilers. Incremental Commit/Non-Atomic Traces <ref type="bibr" target="#b50">[51]</ref> and Atomic Frames <ref type="bibr" target="#b35">[36]</ref> are techniques employed by runtimes to dynamically optimize binaries; the first three techniques require modest amounts of HW support; the last two require fairly aggressive HW implementations with the adaptive version of Atomic Regions continually reoptimizing certain regions based on performance/event counters.</p><p>Excluding Atomic Frames, the first four scheduling techniques produce regions of, on average, between 3.3 and 5 basic blocks in size. How does this compare to the OOO? Figure <ref type="figure" target="#fig_1">2</ref> shows that overlapping the execution of instructions from 4 basic blocks is only sufficient to account for about 70% of the OOO's execution cycles; the OOO spends the remaining 30% of its execution cycles issuing instructions derived from 5 or more consecutive basic blocks. Note Figure <ref type="figure" target="#fig_1">2</ref>'s "long tail"; even overlapping the execution of instructions from 6 consecutive basic blocks, comparable to the overlap achieved by the best performing Atomic Frames variant, only accounts for about 82% of the OOO's execution cycles. <ref type="foot" target="#foot_0">1</ref>Adding yet another consecutive basic block (a region size increase of 16%) contributes only another 3% improvement to coverage. Furthermore, the emerging performance trend of static scheduling techniques for SPECint 2006 is not particularly encouraging; <ref type="bibr" target="#b42">[43]</ref> reports that the average region size for global trace scheduling fell about 15%, from 3.7 basic blocks to 3.2 basic blocks.</p><p>While this data re-affirms the well-known need for hardware support for software speculation, state-of-the-art static scheduling techniques that rely on such support still fail to match the dynamic schedules produced by the OOO. We argue that the primary enabler for the higher quality, dynamic schedules is the OOO's more general and better provisioned speculation support facilities (very large ROB, renamer, larger Load/Store Queue, dynamic/adaptive memory disambiguation) <ref type="bibr" target="#b48">[49]</ref> and the generally lower cost of misspeculation; the OOO can more readily adapt to data misspeculation <ref type="bibr" target="#b9">[10]</ref> while its finer-grained commit and cheaper recovery mechanisms give it an edge over Non-Atomic Traces and Atomic Frames <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b44">45]</ref>. Consequently, a combination of HW support for software speculation, some form of non-blocking instruction issue and, as will be demonstrated, the ability to recover from mispredicts is necessary to close the performance gap. To the best of our knowledge, no industrial or academic design has explored the theoretical desirability and practical necessity of this combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Scheduling vs. Dynamism</head><p>It is easy to attribute the benefits of OOO almost entirely to dynamism, that is the ability of OOO to generate schedules at runtime based on the actual path taken by the code and execution latencies of the operations. In this section, we attempt to tease apart what OOO provides to understand which component could potentially be provided by simpler mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Path Specificity:</head><p>Because out-of-order performs scheduling after branch prediction it gets the benefit of scheduling based on the predicted path. This creates a important practical distinction between out-oforder and in-order when the predictability of a branch exceeds the branch's bias. Consider, for example, the following code:</p><p>Block A is terminated by a branch that can flow to either block B or block C. Each basic block contains a collection of instructions with dependencies, which result in the schedule for each basic block to be narrower than the machine's width. There is, however, potential for overlap between basic blocks (i.e., the critical path through the whole application is not the sequence of critical paths through each basic block).</p><p>On an out-of-order machine, if block B is predicted to follow block A, then the instructions from block B will be intermingled with those from block A. Similarly, if the path from block A to block C is predicted, then C's instructions will be intermingled with block A.</p><p>Of course, many compilers are capable of hoisting instructions across basic block boundaries, often using ILP scheduling techniques like superblock scheduling. What ILP scheduling techniques accomplish is to statically pick a path for optimization (e.g., the path from block A to B) and hoist instructions from block B into block A. Using this technique, when the code executes the path from A to B, it can achieve schedule quality similar to that of the out-of-order machine. When the path from A to C is taken, however, the collection of instructions from block B that were hoisted to block A are wasted work and we have achieved no overlap between C and A<ref type="foot" target="#foot_1">2</ref> . This is not a serious problem if the the path from A to B is much more frequent than the path from A to C. Furthermore, the behavior of the A to C path with an ILP scheduling compiler is much like the behavior of the out-of-order machine when it predicts incorrectly to follow the path from A to B when the path A to C is needed; in this case, the out-of-order will hoist much of B before the branch in A is resolved at which point C will need to be fetched, likely resulting in no overlap between A and C.</p><p>In short, an ILP compiler typically achieves good overlap in proportion with the bias of the branch, while the out-of-order does so in proportion to the predictability of the branch. This results in a significant advantage for the out-of-order because branch predictability is almost always higher than branch bias, sometimes significantly so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variable Latency:</head><p>Tolerance of long latency operations is an often touted strength of out-of-order processors, but it is important to recognize that static scheduling -through techniques like modulo schedulingcan effectively tolerate long latencies as well. The key advantage of out-of-order is two-fold: first, in its ability to tolerating variable latency, and, second, in how this interacts with path specificity.</p><p>While most instructions on modern machines have a fixed latency, some instructions, notably load instructions, have a variable latency. This variable latency is important when it potentially affects the critical path. Consider the following three scenarios:</p><p>Scenario A: (off the critical path)</p><formula xml:id="formula_1">(int * )A[N]; int C[N]; for (int i = 0 ; i &lt; N ; i ++) { C[i] = * A[i]; }</formula><p>This code exhibits a "spine and ribs" structure where the loop carried dependence (i++) forms the critical path and the work of the loop (a serial dependence chain of two loads and store) are a rib hanging off that spine. Because the variable latency -the second load -is on the rib (and hence off the critical path) a compiler for an in-order machine could theoretically schedule the consumers of that load for its worst case latency. This would achieve the same performance as the out-of-order, because there is little benefit to completing the spines early.</p><p>Scenario B: (on the critical path)</p><formula xml:id="formula_2">int A[N], index = 0; for (int i = 0 ; i &lt; N ; i ++) { index = A[index]; }</formula><p>In this code example, we can pack the code as tight as possible. The critical path will always go through the loads, no matter what their latency.</p><p>Scenario C: (the critical path is a function of latency) What is particularly challenging for static scheduling to deal with is when the critical path through a region is a function of whether an access hits or misses in the cache. Below is a simple, illustrative example of this consisting of 4 loads. The data dependence graph for this code is shown in Figure <ref type="figure" target="#fig_2">3(a)</ref>.</p><formula xml:id="formula_3">z = p1-&gt;x-&gt;z + p2-&gt;x-&gt;z;</formula><p>In this code, how the second pair or loads should be scheduled depends on which loads miss, so that we can overlap misses. Figure 3(b) shows a scenario where the p1-&gt;x-&gt;z load should be scheduled last so that both misses overlap, and Figure <ref type="figure" target="#fig_2">3(c)</ref> shows the complementary scenario that benefits from the other ordering. This is a situation where there is no substitute for dynamism. Early Branch Resolution: While it doesn't happen frequently, an OOO machine has the potential to overlap branch misprediction latency with other execution. In the code that follows, the branch on b does not depend on the computation before it, so it can be resolved in parallel with the computation.</p><formula xml:id="formula_4">p-&gt;y ++; if (b) { ...</formula><p>Achieving this overlap can be challenging for in-order machines because branches generally must be placed at the end of their basic block (to demarcate which instructions belong in the block), which also constrains their place in the schedule. Two solutions to this problem are available. On the software side, the pre-branch computation can be pushed down onto both downstream paths at the cost of code replications. On the hardware side, "prepare-tobranch" mechanisms <ref type="bibr" target="#b28">[29]</ref> have been proposed that separate the computation of a branch outcome from the branch's role as a basic block boundary; IBM's Cell processor is a recent example employing such a mechanism <ref type="bibr" target="#b18">[19]</ref> Branch Misprediction:</p><p>One of the fundamental advantages of dynamic scheduling over static schedules is that ability of the dynamic scheduler to "ramp up" after and during a branch mispredict. Specifically, the dynamic scheduler has superior recovery from branch mispredicts as it is able to respond to actual instruction readiness that occurs during and after the mispredict recovery period rather than the readiness assumptions encoded in a static schedule. The main insight here is that the ILP instructions hoisted by the static scheduler between a critical path producer and consumer to cover non-unit latencies can in fact extend the critical path when these non-unit latencies have already been covered (in part or in full ) by the misprediction recovery latency. Once the correct path instructions hit the execute stage, a statically scheduled machine must still churn through the non-critical hoisted instructions before reaching the critical path consumer despite the consumer's immediate readiness upon misprediction recovery; a dynamically scheduled machine has no such limitation and can immediately execute the critical path consumer.</p><p>We illustrate this phenomenon through two examples excerpted from SPEC2000 Integer benchmarks and visualized in Figures <ref type="figure">4</ref> and<ref type="figure">5</ref>. Each figure follows the same convention: All examples have consumers of loads located in the fall-through path of a conditional branch. Both the dynamic scheduler and static scheduler are able to cover the load-to-use latency of the loads by hoisting instructions from the fall-through-path after the branch but before the consumer. In the common case of a correct branch prediction (not-taken), the dynamic schedule and the static schedule execute in the same number of cycles. The real disparity in performance comes in the event of a branch misprediction. The load prior to the branch executes in the same cycle in both schedules. Assume that both redirect the front-end at the same time and both incur the same mispredict penalty. In both cases, the L1 hit latency is covered by the time to redirect the front-end. In other words, for both scheduling techniques, the consumer of the load is ready to execute as soon as it enters the window. In the parlance of Fields criticality <ref type="bibr" target="#b13">[14]</ref>, the consumer of the load issued prior to the mispredicted branch is fetch critical. The delinquent execution of r0 in E relative to D in Figure <ref type="figure">4</ref> (derived from gap) shows the performance difference between the two scheduling techniques; the dynamic scheduler is able to immediately execute the consumer of LD(A) (it is the oldest, ready instruction in the window). In contrast, the static schedule that enters the window after the branch misprediction recovery consists of instructions that were hoisted to cover the L1-hit latency but this latency has already been covered by the mispredict latency. As a result, these hoisted instructions now prevent the immediate execution of the load's consumer.</p><p>Though the branch mispredict latency extends the critical path length for both the dynamically and statically scheduled versions, the hoisted instructions in the static schedule now further extend the critical path length by delaying the use of a critical, ready instruction. In other words, a static schedule to cover the L1hit latency along the critical path is now penalized by the L1-hit latency. An extension of this scenario occurs when, with sufficient profiling, the static scheduler schedules for an L2 hit and is able to find sufficient ILP to cover the L2-hit latency. In this case, the execution of the use of the L2-hit is delayed by 10 cycles (the mispredict recovery latency) when the redirected instruction stream finally reaches the execute stage.</p><p>Figure <ref type="figure">5</ref> derived from eon demonstrates that this problem is compounded by the presence of cascading loads i.e.loads that feed other loads. In this example, r0 = LD(A) feeds r1 = LD(r0) before a conditional branch. The consumer of r1 is after the conditional branch. As in example 1, in the case of a correctly predicted fall-through, the performance of the static schedule is identical to that of the dynamic schedule; the static schedule initiates r0 = LD(A) before the conditional branch and pushes the use of r0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Method</head><p>In an attempt to separate the speculation and dynamism aspects of OOO execution, we have developed a simulator framework that allows us to run a benchmark on the simulator configured as an OOO processor and capture the dynamic schedules used, identify a dominant schedule for each region, and replay these schedules in a second in-order execution. In this section, we describe our choice of benchmarks (Section 3.1), how we collect the dynamic schedules (3.2), how we replay these schedules It should be noted that this work is a limit study of sorts. Specifically, we make no attempt to describe the mechanisms that would be necessary to produce these schedules nor the architectural features that would be necessary to encode these schedules. We do, however, think that a useful mental model for the system under study is a machine like the Transmeta Crusoe <ref type="bibr" target="#b8">[9]</ref>, which consisted of a VLIW processor coupled with the CMS dynamic binary translator. Our study can be viewed as investigating the potential for a futuristic dynamic optimizer provided it was not constrained in the static schedules it could generate and it could effectively profile the execution so as to generate relevant schedules.</p><p>In addition to scheduling code, such a dynamic optimizer is capable of applying other optimizations (e.g., converting a storeto-load forwarding into a register communication) that can reduce the program's critical path and/or number of executed operations. While these transformations are valuable and could potentially improve the performance of an optimized in-order execution past a traditional out-of-order, we will not consider those here because they would only make our goal of understanding the role dynamism plays in OOO execution by obfuscating where performance is coming from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Benchmarks</head><p>Because the largest benefits of OOO have historically come from non-numeric programs <ref type="foot" target="#foot_2">3</ref> , we focus on integer programs, from the SPECint 2000 benchmark suite, in this work. For our Alpha-based simulation framework, our executables are compiled using the DEC C Alpha compiler (V5.9-005), with peak optimizations enabled for the 4-wide, in-order DEC Alpha 21164, but no profile feedback.</p><p>For each experiment, we run several checkpoints from each benchmark in SPECint 2000. Each run executes for 2 million instructions to warm up caches and branch predictors then execution time is measured for the next 10 million instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Harvesting Schedules</head><p>Generating our static schedules is a two step process. In the first step, we run our OOO simulator and collect a trace of the schedule as it was scheduled by the OOO machine. To make the selection of a dominant schedule tractable, we find dominant schedules for various regions of code and then stitch them back together. Thus, as we collect the trace, we break it into snippets of code, terminating snippets at backwards branches and function returns. Each time a snippet is captured, we record the cycle that an instruction executed relative to the first instruction of each snippet (which executed in cycle 0 by definition). Due to the nature of the OOO, it is common for instructions to have negative offsets. We also record the PC and the offset of the first instruction of the next snippet, which we use as a stitching point when we attempt to put the snippets back together.</p><p>We aggregate all of the snippets from all of the checkpoints of a given benchmark and post-process them together. The first step is to discard any schedules that embed a mispredicted branch; we found that these schedules are too pessimistic to assume as our static schedule. Second, any time the stitching point has a negative offset (e.g., when the first instruction of a subsequent snippet happens to execute before the first instruction of this schedule), we set the stitching offset to zero to avoid a deadlock in our simulations. After these two transformations, we simply select the most common schedule (i.e., the mode) to be our static schedule. This schedule along with its key -the sequence of PCs in program order that make up the schedule -are entered into a "database" which is used in the subsequent simulation. Together these snippets provide more than 99.999% coverage of our simulated dynamic instruction stream.</p><p>As our "region formation" heuristic is very simple, it has some known shortcomings. First, it often results in short snippets. This is largely mitigated by overlapping the schedule of snippets through the stitching discussed below. Tight loops with high trip counts have the potential to be more problematic. Our methodology can underestimate the performance of such loops because it is constrained to produce code with integer initiation intervals. For example, a loop with 5 instructions will result in an initiation interval of 2, where a compiler, by unrolling this loop could pack 4 iterations into 5 cycles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Replaying Dominant Schedules</head><p>We have modified the front end of our simulator to allow us to replay these dominant schedules as if they were static schedules provided by a compiler. At the fetch stage of our timing simulator, we exploit the functional-first nature of our timing simulator to examine the upcoming dynamic trace of instructions to generate a key and select the appropriate snippet. By construction, none of our snippet keys are a prefix of any other, so this lookup process is a simple one. In this way, we construct a short queue of snippets and their schedules.</p><p>We combine the snippet schedules into a composite schedule by placing each successive schedule's cycle 0 at the insertion point indicated by the previous schedule and populating the composite schedule. Because our snippet schedules are selected independently, there is the potential that we selected a snippet schedule that never executed with the schedule for the snippet before it. As a result, this naive stitching can result in unrealizable schedules due to oversubscribing the width of the machine or by violating data dependencies. If one of these violations is detected, the insertion point is "bumped" forward a cycle and the process is repeated. In principle, most of this work could be done offline, but as our postprocessor doesn't actually decode the instructions and hence cannot detect dependence violations, it was simplest to implement this as part of the front end. Most notably, stitching is performed in the front end with no awareness of dynamic events like branch mispredictions or cache misses, so is performed the same way each time for a given sequence of basic blocks.</p><p>Our simulator framework lacks support for feeding wrong-path instructions into our timing model. When our predictor model indicates a mispredicted branch, we stall fetch of subsequent instructions until the branch hits an execute unit (and resolves), after which fetch begins again. We use the same approach for all simulations. As previously noted our snippet coverage is almost but not quite 100%. So as to be not too pessimistic with the less than 0.00001% of the total instructions that aren't covered by snippets, dynamic scheduling is used for these instructions in the simulator. We don't, however, allow the execution of these instructions to overlap with snippets before or after them, which means our handling of these instructions is likely conservative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Processor Model</head><p>Our processor model is configured to approximate the resources and latencies in modern OOO processors <ref type="bibr" target="#b33">[34]</ref>, as shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>In order to support pre-scheduled code in the simulator, we need to make a handful of simplifications and modifications to the processor model's front end. First, as previously noted, we use a functional-first methodology which lacks support for injecting wrong-path instructions into the execution. The impact of this limitation is mitigated by the fact that we are studying the relative performance of the pre-scheduled runs to a baseline (OOO) and all runs incur this limitation.</p><p>Second, it would be difficult to accurately model the instruction cache performance of the pre-scheduled runs, so we assume a perfect instruction cache. This has little impact on the baseline runs because our benchmarks have relatively small instruction working sets, but may result in some over-prediction of performance for the the pre-scheduled runs which will necessarily require some amount of code replication in order to achieve the desired static schedules in the presence of control flow.</p><p>Third, we perform branch prediction using the PC of branches as they appear in the original execution stream. In a real system, any place code has been replicated, each of those static instances would likely index into different places in the branch predictor. This separation can have negative (e.g., longer training times, more predictor aliasing) and positive (e.g., the streams may be more predictable when separated) affects. We cannot, however, consider this in our simulation infrastructure because our snippets selection is derived from future path information such that many snippet branches are completely biased. By using prediction based on the original program stream, we can ensure that our performance results are not benefiting from any prediction anomalies.</p><p>When branch mispredictions occur, their penalties are enforced at the point where the branch was scheduled; that is, post-branch cycles of the schedule are not fetched until the branch itself has executed. This does not however prevent right-path instructions that were scheduled above the branch to execute in parallel with the branch resolution as if they had been hoisted above the branch by the compiler. We are not, however, able to model the contention from wrong path instructions that the compiler might also have desired to hoist above the branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Avoiding Cache Miss-Induced Stalls</head><p>Continual Flow Pipelining <ref type="bibr" target="#b45">[46]</ref> was originally proposed to give an existing OOO processor a larger effective window without growing complex structures. It does this by "slicing out" instructions transitively dependent on (and stalled by) a load miss (or equivalently any other high-latency event) into a separate queue and re-inserting them in the back-end when the load miss returns. In-order retire still requires that these sliced instructions hold ROB slots, but other resources (physical registers, reservation station slots) can be used for independent work. Notably, this does not allow for load/store queue entries to be reclaimed; dependencies through both registers and memory must be identified to slice out the correct instructions. Once the load miss comes back, the design "rallies", leveraging existing superscalar / SMT front-end hardware to dovetail the slice contents with the fresh instructions still being fetched.</p><p>In-order architectures rarely actually stall the processor when they encounter a cache miss ("stall on load"). Instead, they they allow independent instructions to proceed in the pipeline in parallel with the cache fill, but only up to the first instruction which uses the missing memory value, at which point they must stall ("stall on use").</p><p>In iCFP (in-order Continual Flow Pipelining) <ref type="bibr" target="#b21">[22]</ref>, this sliceout mechanism requires the much the same speculative-execution support an OOO does (ROB or equivalent result-buffering queue, load-store alias detection / avoidance), but allows the processor to make independent progress by not stalling on the first use of the result. Instead, iCFP poisons the result of the load and adds it to the slice buffer, and any later instruction that uses a poisoned value is sliced / poisoned in turn. When the miss returns, instructions are reissued from the slice buffer in parallel to fresh instructions from the front-end (in the usual super-scalar fashion), potentially exposing new misses and MLP.</p><p>Since we assume the same speculation support in our experimental machine, iCFP makes an excellent compliment, and allows us to effectively extract another schedule in the shadow of a cache miss. Our implementation slices out the poisoned portions of each issue group into a FIFO, and always rallies to the head of that queue when any miss returns. Rallied issue groups are allowed to merge with fresh groups if they fit together, though no dependence checking is required for this merge due to the use of poison bits. See section 4.1 for its performance impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>In an effort to understand the roles that speculation and dynamism play in contributing to OOO performance, we first attempt to measure their relative contributions by measuring what fraction of OOO performance can be achieved through the OOO's speculation support without its ability to schedule dynamically (Section 4.1). We then move on to understand in what ways does dynamism contribute the remaining performance and to what degree do mechanisms that provide dynamism in the specific circumstances where it most benefits us can substitute for the general dynamism mechanism that is OOO execution (Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Speculation vs. Dynamism</head><p>To understand what fraction of baseline performance is contributed by OOO execution, we begin by comparing the performance of the baseline OOO to a comparably provisioned in-order superscalar, with both machines executing the traditionally compiled code. The   in-order performance (normalized to the OOO) is shown in the leftmost bar of Figure <ref type="figure" target="#fig_6">6</ref>. We find the in-order machine to achieve, on average, 47% of the performance of the OOO, which is consistent with previous results <ref type="bibr" target="#b21">[22]</ref>. This same work suggests that coupling a comparably provisioned in-order with an advanced run-ahead mechanism, iCFP is only sufficient to reach 57% of the OOO's performance, for the integer programs currently under investigation. The pre-scheduled code exploits the speculation support of an out-of-order without benefiting from dynamism. Running the prescheduled code on the same in-order processor model results in substantially higher performance. As shown in the right bars of Figure <ref type="figure" target="#fig_6">6</ref>, the pre-scheduled code achieves 89% of the OOO performance compared to the baseline in-order's 47%. From this data, it appears that it is OOO's ability to generate aggressive schedules, and not its ability to vary these schedules based on dynamic events, which is responsible for most of its performance advantage. Specifically, speculation provides roughly 79% of the gap between the in-order and OOO baselines.</p><formula xml:id="formula_5">!" !#$" !#%" !#&amp;" !#'" !#(" !#)" !#*" !#+" !#," $" ! " # $ % &amp; ! ! ' ( ) * + ! ! , -. ! ! / ) % ! ! / ' ' ! ! / # $ % ! ! 0 ' 1 ! ! % ) (<label>2</label></formula><formula xml:id="formula_6">!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! G%,(<label>1</label></formula><formula xml:id="formula_7">?!$GHI!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! J%,(<label>1</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Understanding Dynamism</head><p>By comparing the execution schedules produced by the OOO simulator with that of the pre-scheduled in-order, we observed two situations that seem to contribute the bulk of the performance discrepancy: re-scheduling in the locus of a cache miss and in the locus of a branch misprediction. In an attempt to quantify their relative contributions as well as to understand to what degree these types of dynamism could be captured by mechanisms that are potentially simpler than a general out-of-order mechanism, we undertook two further experiments.</p><p>First, we enhanced our simulator to implement a simple implementation of in-order Continual Flow Pipelining (CFP) <ref type="bibr" target="#b21">[22]</ref>, as described in Section 3.5. The dynamism that iCFP provides is very specialized and at a relatively course granularity. If there are no cache misses, iCFP has no impact on the execution. Upon a cache miss, the program is split into two streams, a miss-dependent stream and a miss-independent stream. Each of these streams retains its existing schedule, but these schedules can slip with respect to each other.</p><p>In spite of the somewhat simplistic nature of this dynamism, it seems to provide exactly what the pre-scheduled in-order execution requires. With iCFP, the pre-scheduled code can, on average, achieve 99% of the performance of the OOO, as is shown in Figure <ref type="figure" target="#fig_7">7</ref>. We find that this makes a lot of sense. The pre-scheduled code already encodes the code re-ordering that is required for common case good performance, so the fact that iCFP does nothing in the absence of a cache miss is not a problem.</p><p>However, even in this configuration, the performance of eon, gap and vortex still lags behind the OOO by 2%, 4.9% and 5.2% respectively. We attribute this performance deficit to the in-order's inferior "recovery after branch mispredict" behavior described in detail in Section 2.2 with visualizations of such behavior excerpted from eon, gap and vortex. As an expedient, we found that simply executing the next 20 instructions after a branch mispredict on the dynamic scheduler enabled these benchmarks to match the OOO's performance; no more than 4% of the total instruction count is directed towards the dynamic scheduler in this manner. We further observed that the schedules discovered by the dynamic scheduler were remarkably stable suggesting a simple software only solution to this performance issue: branch-after-mispredict in which the front-end is redirected to an alternate version of the correct path schedule which is optimized for the instruction readiness that occurs during the mispredict recovery interval.</p><p>We also note (but do not show) that the combination of iCFP and speculative scheduling support actually enables the enhanced in-order to surpass the performance of the OOO in some cases. This is due to the fact that while waiting for misses to return, the Execute stage in the enhanced in-order only stalls when the ROB is full. In contrast, the OOO is compelled to stall when the Instruction Queue is full; practical designs generally feature Instruction Queues that have a fraction (typically less than half) of the number of entries contained in the ROB <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>The performance of our simulated machine with these harvested schedules suggests that the dynamic flexibility of a full-blown OOO processor is (expensive) over-kill and shows us some of the architectural features a more statically-scheduled processor needs to achieve similar performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Benefit of Dynamism</head><p>The primary advantage of a dynamic scheduler is that it can schedule for the dynamically-exposed critical path, without having to store and retrieve those schedules. The OOO gets many schedules from a single set of instructions, while our in-order requires explicit schedules to be provided for every cache-miss and branchmispredict that it schedules after. While it seems likely that not all of these schedules contribute significantly to performance, the front-end which can support this combinatorial explosion of schedules may be infeasible. We expect that profile-guided JIT compilation could identify the critical misses and mispredicts, and even provide reasonable code-growth, but these does not eliminate our need for low-latency access to a second (or third, or fourth) schedule to execute in the shadow of these unlikely events. The trade off of dynamic scheduling is thus between bringing several schedules to the execution core, or generating them on-site.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">What mechanisms do inorders need to generate</head><p>OOO-like schedules?</p><p>Given the above, there are two pieces of the in-order architecture that are missing. The first is a scheme for decorating the instruction stream with secondary schedules, to be used in the event of unexpected longlatency events. In the case of a cache miss, this is akin to a "branchon-cache-miss" where control is revectored based on the hit signal from the data cache similar to the mechanisms proposed in <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b0">[1]</ref>. This obviously has little advantage if the schedule cannot see the use before the miss returns in addition to the more general problem of generating good miss-handling routines. The other mechanism is a "branch-on-branch-mispredict", where the schedule injected in the front end of the machine might be different, based on whether that path was predicted correctly, or it was the result of a re-steer.</p><p>The second piece of the puzzle is a mechanism for doing software alias speculation. The OOO uses a load-store queue to great effect, dynamically teasing out dependencies through memory, which allows for dynamic schedules that are aggressive where a compiler must be conservative. A non-faulting memory instruction, combined with a "branch-on-alias" would allow for the commoncase execution to reap the benefits of not-always-safe load hoisting without sacrificing correctness. An OOO may also employ alias-prediction hardware, but the overhead of software scheduling adaptation in the presence of often-aliasing instructions may be prohibitively expensive, so this prediction mechanism may still be desirable (akin to branch prediction) in the in-order design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>There is an extensive and venerable body of literature focusing on the fundamental advantages and disadvantages of dynamically scheduled hardware particularly in the context of attempting to match the OOO's performance with less complex hardware <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b49">50]</ref>. While insightful and prescient in many respects especially regarding the scalability of dynamically scheduled hardware, the small memory footprints and low branch complexity of the programs used for comparison purposes in these studies make it challenging for the reader to discern what contribution dynamism, independent of speculation support mechanisms, makes to overall performance. Follow-on studies examined the static vs. dynamic scheduling issue in the context of the emerging memory wall and power-constrained but performance-intensive platforms <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b38">39]</ref>. These works make strong arguments for a middle-ground between statically and dynamically scheduled hardware particularly due to perceived impending circuit limitations on OOO cycle times as well as concerns over object compatibility with the proposed simpler hardware.</p><p>Industry R&amp;D <ref type="bibr" target="#b11">[12]</ref> and academia <ref type="bibr" target="#b12">[13]</ref> were exploring this middle-ground between statically and dynamically scheduled hardware, ("quasi-dynamic scheduling" in the parlance of rePLay <ref type="bibr" target="#b34">[35]</ref>) while contemporaneous commercial processors at two extremes were the Itanium <ref type="bibr" target="#b39">[40]</ref> and Transmeta processors <ref type="bibr" target="#b26">[27]</ref>. Both the Itanium and Transmeta designs provide extensive hardware support for software speculation; data and control speculation are facilitated by associative structures and non-trapping instructions that the software can manipulate and query. Recovery from misspeculation differs in that the Transmeta design features automatic HW rollback <ref type="bibr" target="#b8">[9]</ref> while Itanium exposes explicit rollback and recovery to software. Subsequently, Itanium designs have refined/expanded their HW structures to permit more control and data speculation <ref type="bibr" target="#b30">[31]</ref> and the most recently announced version has greater HW support for dynamism <ref type="bibr" target="#b47">[48]</ref> in a manner explored by <ref type="bibr" target="#b5">[6]</ref>. We also note the encouraging trend of expanded HW support for software speculation, specifically the HW transactional memory systems in IBM's BlueGene/Q and Intel's forthcoming Haswell architecture <ref type="bibr" target="#b24">[25]</ref>.</p><p>Some form of transactional memory or HW atomicity is typically required <ref type="bibr" target="#b32">[33]</ref> for quasi-dynamic scheduling but even a limited form combined with specialized functional units can facilitate performance comparable to a narrow-window OOO <ref type="bibr" target="#b7">[8]</ref>. Though we do not address it in this work, quasi-dynamic and static scheduling all suffer from the region boundary issue that does not afflict the OOO; the OOO's ability to rotely eliminate control-flow and object code barriers to hoisting instructions is difficult to match both in theory <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b48">49]</ref> and practice <ref type="bibr" target="#b16">[17]</ref>. Matching the OOO's performance requires, at a minimum, the immensely challenging task of devising good predictors that function on large regions <ref type="bibr" target="#b16">[17]</ref> while the HW support required to "region slip" is non-trivial.</p><p>Coping with memory operations remains problematic for both static and quasi-dynamically scheduled machines as they are typically compelled to stall on the use of an unready register. Simply finding sufficient instructions to cover load-to-use latency on an inorder can be a challenge and can introduce complications as seen in the mispredict recovery examples above. Some proposed solutions include dispatching instructions to latency-specific queues based on latency information encoded into the instructions at compile time <ref type="bibr" target="#b17">[18]</ref> to more dataflow oriented ISAs and associated HW <ref type="bibr" target="#b31">[32]</ref> though all come at the cost of significantly higher HW complexity and instruction encoding bloat.</p><p>Solutions for handling variable latency loads in general purpose in-orders range from the extremely aggressive hardware prefetchers, software prefetching support and runahead execution of the Power6 <ref type="bibr" target="#b4">[5]</ref> to more modest compiler-inserted prefetch threads which use an idle SMT context <ref type="bibr" target="#b25">[26]</ref>. An excellent overview of runahead techniques that occupy the middle ground between these extremes is provided by <ref type="bibr" target="#b2">[3]</ref>. We chose iCFP <ref type="bibr" target="#b21">[22]</ref> due to its recentness, its focus on in-order processors and the merits of its "rally" and "advance" modes which collectively enable it to outperform most rival techniques in the literature. We found iCFPs treatment of implementation issues equally compelling, though probably the greatest challenge facing runahead techniques is validation/verification. Finally, the microarchitectural substrate required to implement iCFP (multiple, checkpointed register files, advanced store queue, etc) could also provide HW support for software data and control speculation if these mechanisms were exposed at the ISA level.</p><p>Lastly, the observation that the OOO continually generates high quality schedules has motivated several research efforts to capture/re-use these schedules or to allow a compiler/runtime to indicate when dynamic schedules should be generated. The major motivation in this regard has been energy savings, <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b48">49]</ref>, generally at the cost of performance degradation though off-thecritical-path HW rescheduling of ROB traces in <ref type="bibr" target="#b33">[34]</ref> yields a performance speedup over the baseline OOO. It must be noted that these approaches invariably execute a significant percentage of their instructions via the dynamic scheduler (upwards of 70%, 45% and 20% for <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49]</ref> respectively) rather than executing the captured OOO traces on the in-order HW . Our own experiments suggest that even executing only 5% of bzip2's instructions via the dynamic scheduler enhanced performance by 18% relative to executing all instructions on the in-order HW. Finally, a novel and inspirational use of captured OOO schedules was demonstrated in <ref type="bibr" target="#b10">[11]</ref> in which programs destined to run on an in-order architecture were first passed through a simulated OOO-version of the architecture, their traces captured and then appropriately patched up (in the form of compensation code for control/data misspeculation) to run directly on the in-order architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>Out-of-Order processors generally attain higher performance on control intensive integer code than in-order designs, including those with hardware support for software speculation. Conventional wisdom for this disparity in performance is the OOO's ability to avoid head-of-line blocking and exploit far-flung memorylevel parallelism; collectively we call this ability "dynamism"; the major enabler for dynamism is the OOO's general-purpose, wellprovisioned speculation support mechanisms e.g. large ROB, renamer, Load/Store Queue etc which can rollback speculative state at lower cost than even the most advanced in-order designs. We demonstrated that nearly 88% of the speedup attained by the OOO over an in-order design can be attributed to these speculation support mechanisms alone. Of the remaining performance differential, a complexity-effective run-ahead mechanism, iCFP, was sufficient to close the gap to the OOO in most cases. The final 1-2% performance gap was found to be in the in-order's inability to match the OOO's recovery after a branch mispredict; we proposed and qualified a simple mechanism for addressing this deficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Average region size achieved by top-performing scheduling techniques for in-order machines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The basic block overlap ability of a representative outof-order design.</figDesc><graphic url="image-1.png" coords="2,329.39,100.02,53.10,105.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Scenario C variable latency code example: a) data dependence graph, b) p1-&gt;x and p2-&gt;x-&gt;z miss, c) p2-&gt;x and p1-&gt;x-&gt;z miss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>?</head><label></label><figDesc>Time flows from left to right ? Instructions that issue in the same cycle are shown in the same column ? Non critical path instructions are denoted by a label of the form ILP[0-9]+ ? A shows the in-order, linear instruction sequence as emitted by the compiler; Critical path instructions are identified by decorated borders ? B shows an execution trace of the dynamically scheduled code (OOO) when the fall-through is correctly predicted ? C shows an execution trace of the statically scheduled code (VLIW with packets shown as rounded rectangles) when the fall-through is correctly predicted ? D shows an execution trace of the dynamically scheduled code (OOO) when the fall-through is mispredicted ? E shows an execution trace of the statically scheduled code (VLIW) when the fall-through is mispredicted</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. Misprediction recovery latency covers L1-hit latency</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Performance Comparison of the Baseline In-Order and the In-Order Enhanced With Support for Software Speculation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Performance Comparison of the Baseline OOO and the In-Order Enhanced With Support for Software Speculation and iCFP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Simulated machine parameters</figDesc><table><row><cell>Front end</cell><cell>4-wide fetch (no I$ modeled)</cell></row><row><cell></cell><cell>10-cycle branch mispredict</cell></row><row><cell></cell><cell>GShare (16b index, 16b history)</cell></row><row><cell></cell><cell>Cascading Indirect Predictor</cell></row><row><cell></cell><cell>2K-entry target buffer</cell></row><row><cell></cell><cell>16-entry RAS</cell></row><row><cell cols="2">Execution Maximum 4 of:</cell></row><row><cell></cell><cell>4 Int (1 cycle simple, 5 mul/div)</cell></row><row><cell></cell><cell>2 FP (4 simple, 20 complex)</cell></row><row><cell></cell><cell>2 Load/Store (1 AGEN/TLB) + cache latency)</cell></row><row><cell></cell><cell>64 entry window</cell></row><row><cell>Back end</cell><cell>128 entry ROB</cell></row><row><cell></cell><cell>4-wide commit</cell></row><row><cell>L1$</cell><cell>32KiB, 64B line, 4-way LRU, 3 cycles</cell></row><row><cell>L2$</cell><cell>256KiB, 64B line, 8-way LRU, 25 cycles</cell></row><row><cell>DRAM</cell><cell>150 cycle latency, 16B/cycle</cell></row><row><cell>iCFP</cell><cell>64 entry slice buffer</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Architectures such as TRIPS<ref type="bibr" target="#b16">[17]</ref> and the Region Slip variant of rePlay<ref type="bibr" target="#b44">[45]</ref> provide for the overlapping of Hyperblocks and Atomic Frames, respectively, at the cost of substantially increased hardware, runtime and compiler complexity as well as costlier misspeculation penalties</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Alternatively, we could hoist some instructions from both B and C into block A, but typically machine width and functional unit limitations prevent us from hoist as many instructions from either path if we choose to hoist from both.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Numeric programs often have extensive instruction-level parallelism, more regular control flow, and simpler memory-access patterns (e.g., fewer pointer-based data structures) which make traditional compiler optimization more successful resulting in very competitive performance on in-order machines.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="8.">Acknowledgments</head><p>We are deeply indebted to <rs type="person">Naveen Neelakantam</rs> and <rs type="person">Sara Baghsorkhi</rs>, both of <rs type="affiliation">Intel Corp</rs>., for reviewing an early draft of this work. We also thank the anonymous reviewers for their insightful comments and suggestions. <rs type="person">Daniel S. McFarlin</rs> was supported by an <rs type="funder">National Defense Science and Engineering Graduate Fellowship</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Critical path optimization-unload hard extended scalar block</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Babaian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Okunev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Volkonsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USPTO</title>
		<imprint>
			<biblScope unit="volume">6584611</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beating in-order stalls with &quot;flea-flicker&quot; two-pass pipelining</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Sias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Nystrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><forename type="middle">W</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="33" />
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enhancing mlp: Runahead execution and related techniques</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Brian Kreskamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Montesinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IACOMA Technical Report</title>
		<imprint>
			<biblScope unit="volume">512</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An investigation of the performance of various dynamic scheduling techniques</title>
		<author>
			<persName><forename type="first">M</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th annual international symposium on Microarchitecture</title>
		<meeting>the 25th annual international symposium on Microarchitecture<address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Runahead execution vs. conventional data prefetching in the ibm power6 microprocessor</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Cain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nagpurkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPASS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="203" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An epic processor with pending functional units</title>
		<author>
			<persName><forename type="first">L</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computing</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Zima</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Joe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Seo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Shimasaki</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer Berlin / Heidelberg</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2327</biblScope>
			<biblScope unit="page" from="445" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comparing static and dynamic code scheduling for multiple-instruction-issue processors</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mahlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><forename type="middle">W</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual international symposium on Microarchitecture</title>
		<meeting>the 24th annual international symposium on Microarchitecture<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="25" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Softhv: a hw/sw co-designed processor with horizontal and vertical fusion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Codina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gonz?lez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM International Conference on Computing Frontiers, CF &apos;11</title>
		<meeting>the 8th ACM International Conference on Computing Frontiers, CF &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Transmeta Code Morphing Software: Using Speculation, Recovery, and Adaptive Retranslation to Address Reallife Challenges</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Dehnert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Code Generation and Optimization</title>
		<meeting>the International Symposium on Code Generation and Optimization</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Inside Intel Core Microarchitecture and Smart Memory Access</title>
		<author>
			<persName><forename type="first">J</forename><surname>Doweck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Intel Whitepaper</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">VHC: Quickly Building an Optimizer for Complex Embedded Architectures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dupr?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Darch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Code Generation and Optimization</title>
		<meeting>the International Symposium on Code Generation and Optimization</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="53" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DAISY: Dynamic compilation for 100% architectural compatibility</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ebcioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Altman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International Symposium on Computer Architecture</title>
		<meeting>the 24th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
			<biblScope unit="page" from="26" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Performance Characterization of a Hardware Framework for Dynamic Optimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fahs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 34th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2001-12">Dec. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Focusing processor policies via Critical-Path prediction</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Fields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bodik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International Symposium on Computer Architecture</title>
		<meeting>the 28th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001-07">July 2001</date>
			<biblScope unit="page" from="74" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluation of static and dynamic scheduling for media processors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fritts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Media Processors and DSPs, Micro &apos;00</title>
		<meeting>the 2nd Workshop on Media Processors and DSPs, Micro &apos;00</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Mips aptiv cores hit the mark</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Gardner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
	<note type="report_type">Microprocessor Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An evaluation of the trips computer system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gebhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Maher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Coons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Robatmili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th international conference on Architectural support for programming languages and operating systems, ASPLOS &apos;09</title>
		<meeting>the 14th international conference on Architectural support for programming languages and operating systems, ASPLOS &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cheap out-of-order execution using delayed issue</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference of Computer Design, CD 2000</title>
		<meeting>the International Conference of Computer Design, CD 2000</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="549" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Synergistic processing in cell&apos;s multicore architecture</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gschwind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Hofstee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Flachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yamazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="10" to="24" />
			<date type="published" when="2006-03">Mar. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Netlogic doubles up xlp</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Halfhill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-04">April 2011</date>
		</imprint>
	</monogr>
	<note type="report_type">Microprocessor Report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Data-Dependency Graph Transformations for Instruction Scheduling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heffernan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">icfp: Tolerating all-level cache misses in in-order processors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nagarakatte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="19" />
			<date type="published" when="2010-01">Jan. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Informing memory operations: Providing memory performance feedback in modern processors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Annual International Symposium on Computer Architecture</title>
		<meeting>the 23rd Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Intel 64 and ia-32 architectures optimization reference manual. Intel Technical Manual</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Intel architecture instruction set extensions programming reference. Intel Technical Manual</title>
		<author>
			<persName><surname>Intel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Design and evaluation of compiler algorithms for pre-execution</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on Architectural support for programming languages and operating systems, ASPLOS-X</title>
		<meeting>the 10th international conference on Architectural support for programming languages and operating systems, ASPLOS-X<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="159" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Klaiber</surname></persName>
		</author>
		<title level="m">The Technology Behind Crusoe Processors. Transmeta Whitepaper</title>
		<imprint>
			<date type="published" when="2000-01">Jan. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">IBM POWER6 microarchitecture</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Starke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Fields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>O'connell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Ronchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Vaden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM J. Res. Dev</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="639" to="662" />
			<date type="published" when="2007-11">November 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reducing the branch penalty in pipelined processors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lilja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="47" to="55" />
			<date type="published" when="1988-07">July 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An investigation of static versus dynamic scheduling</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Love</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th annual international symposium on Computer Architecture, ISCA &apos;90</title>
		<meeting>the 17th annual international symposium on Computer Architecture, ISCA &apos;90<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="192" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Itanium 2 processor microarchitecture</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mcnairy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soltis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="44" to="55" />
			<date type="published" when="2003-03">Mar. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Static placement, dynamic issue (spdi) scheduling for edge architectures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Kushwaha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Parallel Architectures and Compilation Techniques, PACT &apos;04</title>
		<meeting>the 13th International Conference on Parallel Architectures and Compilation Techniques, PACT &apos;04<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hardware atomicity for reliable software speculation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Neelakantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zilles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Symposium on Computer Architecture</title>
		<meeting>the 34th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="174" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reusing cached schedules in an out-of-order processor with in-order issue logic</title>
		<author>
			<persName><forename type="first">O</forename><surname>Palomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Navarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE international conference on Computer design, ICCD&apos;09</title>
		<meeting>the 2009 IEEE international conference on Computer design, ICCD&apos;09<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="246" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">rePLay: A Hardware Framework for Dynamic Optimization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lumetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="590" to="608" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Increasing the size of atomic instruction blocks using control flow assertions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Crum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd annual ACM/IEEE international symposium on Microarchitecture</title>
		<meeting>the 33rd annual ACM/IEEE international symposium on Microarchitecture<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="303" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Combining hyperblocks and exit prediction to increase front-end bandwidth and performance</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jimnez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dynamically scheduled vliw processors</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Rau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international symposium on Microarchitecture, MI-CRO 26</title>
		<meeting>the 26th annual international symposium on Microarchitecture, MI-CRO 26<address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="80" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Instruction-level parallel processorsdynamic and static scheduling tradeoffs</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Rudd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd AIZU International Symposium on Parallel Algorithms / Architecture Synthesis, PAS &apos;97</title>
		<meeting>the 2nd AIZU International Symposium on Parallel Algorithms / Architecture Synthesis, PAS &apos;97<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">74</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Itanium processor microarchitecture</title>
		<author>
			<persName><forename type="first">H</forename><surname>Sharangpani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="24" to="43" />
			<date type="published" when="2000-09">Sept. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The next-generation 64b sparc core in a t4 soc processor</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sathianathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Turullols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Masleid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Konstadinidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Golla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Grohoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcallister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISSCC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="60" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Optimal Global Instruction Scheduling Using Enumeration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shobaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>University of California Davis</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Optimal trace scheduling using enumeration</title>
		<author>
			<persName><forename type="first">G</forename><surname>Shobaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wilken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heffernan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2009-03">Mar. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient superscalar performance through boosting</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 5th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="248" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improving quasidynamic schedules through region slip</title>
		<author>
			<persName><forename type="first">F</forename><surname>Spadini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fahs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lumetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international symposium on Code generation and optimization: feedbackdirected and runtime optimization, CGO &apos;03</title>
		<meeting>the international symposium on Code generation and optimization: feedbackdirected and runtime optimization, CGO &apos;03<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="149" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Continual flow pipelines</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Akkary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Upton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th international conference on Architectural support for programming languages and operating systems, ASPLOS-XI</title>
		<meeting>the 11th international conference on Architectural support for programming languages and operating systems, ASPLOS-XI<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="107" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Execution cache-based microarchitecture power-efficient superscalar processors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Talpes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marculescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Very Large Scale Integr. Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="26" />
			<date type="published" when="2005-01">Jan. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Poulson: An 8 core 32nm next generation intel itanium processor</title>
		<author>
			<persName><forename type="first">S</forename><surname>Undy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Low-power, lowcomplexity instruction issue using compiler assistance</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Valluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th annual international conference on Supercomputing, ICS &apos;05</title>
		<meeting>the 19th annual international conference on Supercomputing, ICS &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Limits of instruction-level parallelism</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Wall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="176" to="188" />
			<date type="published" when="1991-04">Apr. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Incremental commit groups for nonatomic trace processing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Yourst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ghose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 38th annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="67" to="80" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
