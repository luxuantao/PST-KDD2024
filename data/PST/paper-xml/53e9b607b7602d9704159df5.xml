<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimizing Time Series Discretization for Knowledge Discovery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Örchen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Data Bionics Research Group</orgName>
								<orgName type="institution">Philipps-University</orgName>
								<address>
									<postCode>35032</postCode>
									<settlement>Marburg, Marburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alfred</forename><surname>Ultsch</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Data Bionics Research Group</orgName>
								<orgName type="institution">Philipps-University</orgName>
								<address>
									<postCode>35032</postCode>
									<settlement>Marburg, Marburg</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Optimizing Time Series Discretization for Knowledge Discovery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4D82F111F844721FDDD896AD92BA0C2A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.5 [Computing Methodologies]: Pattern Recognition time series</term>
					<term>discretization</term>
					<term>persistence</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge Discovery in time series usually requires symbolic time series. Many discretization methods that convert numeric time series to symbolic time series ignore the temporal order of values. This often leads to symbols that do not correspond to states of the process generating the time series and cannot be interpreted meaningfully. We propose a new method for meaningful unsupervised discretization of numeric time series called Persist. The algorithm is based on the Kullback-Leibler divergence between the marginal and the self-transition probability distributions of the discretization symbols. Its performance is evaluated on both artificial and real life data in comparison to the most common discretization methods. Persist achieves significantly higher accuracy than existing static methods and is robust against noise. It also outperforms Hidden Markov Models for all but very simple cases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Many time series data mining algorithms work on symbolic time series. For numeric time series they usually perform unsupervised discretization as a preprocessing step, e.g. <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b5">5]</ref>. The choice of the method and the accompanying parameters are rarely justified, however. For the discovery of knowledge that is interpretable and useful to the expert, it is of great importance that the resulting interval boundaries are meaningful within the domain. If the time series is produced by an underlying process, that consists of recurring persisting states, it is desirable to obtain a discretization where the intervals in the value dimension describe these states.</p><p>The most commonly used discretization methods are equal width and equal frequency histograms. The former method is vulnerable to outliers in the data. A single extreme value will affect all bin boundaries and their widths severely. The symbols obtained from two datasets cannot be meaningfully compared when outliers are present. This way any attempt of knowledge discovery, usually involving interpretation of the symbols as states like high or low will give spurious and misleading results. Both histogram methods potentially place cuts in high density regions of the observed marginal probability distribution of values. This is another disadvantage, if discretization is performed not merely for quantization and speedup of processing, but rather for gaining insight into the process generating the data. In general, it is hard to justify why a value slightly left of the cut should be different from a value slightly to the right. The higher the density at a cut point, the more of such problematic symbols we have. The assignment of values to a certain state is somewhat arbitrary near the decision boundaries. The same disadvantages also apply to other methods, e.g. setting cuts based on location and dispersion measures.</p><p>While static data offers no information other than the actual values themselves, time series contain valuable temporal structure that is not used by the methods described above. We propose a new method for meaningful unsupervised discretization of univariate time series by taking the temporal order of values into account. The discretization is performed optimizing the persistence of the resulting states.</p><p>In Section 2 we discuss related methods. The definition of the quality score in Section 3 is followed by the description of the new discretization algorithm in Section 4. The effectiveness of our approach is demonstrated with artificial and real life data in Section 5. Results, limitations, and extensions are discussed in Section 6. Section 7 summarizes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK AND MOTIVATION</head><p>A review of supervised discretization methods is given in <ref type="bibr" target="#b14">[14]</ref>, both supervised and unsupervised methods are covered in <ref type="bibr" target="#b3">[3]</ref> and more recently in <ref type="bibr" target="#b17">[17]</ref>. The only unsupervised methods for discretization listed in the above reviews are equal width and equal frequency histograms. With unsupervised discretization no class labels are available, thus there can be no optimization w.r.t. classification accuracy. But for time series data in particular there is rarely some sort of labeling for the time points available. Far more common is the classification of whole time series (e.g. <ref type="bibr">[8,</ref><ref type="bibr" target="#b22">22]</ref>) but a single label is of little help for the discretization of the numerical values.</p><p>Recently, the Symbolic Approximation (SAX) has been proposed in <ref type="bibr" target="#b16">[16]</ref>. Based on the Piecewise Aggregate Aggregation (PAA) <ref type="bibr" target="#b13">[13]</ref> and the assumption of normality of the resulting aggregated values, a method similar to equal frequency histograms is obtained. SAX is the first symbolic representation of time series with an approximate distance function that lower bounds the Euclidean distance. While SAX is one of the first discretization methods designed especially for time series data, the temporal aspect of the data is only taken into account by the preprocessing step of performing the PAA. The window size and the alphabet size create a tradeoff between efficiency and approximation accuracy. The choice of these parameters has been analyzed in the context of temporal rule mining in <ref type="bibr" target="#b7">[7]</ref>. Different methods for model selection are tried and judged by the support and confidence of the resulting rules. Rules are, however, typically created to gain a deeper understanding of the data and the patterns therein. Arguably, rules with high support and confidence are less likely to be spurious results. But they will not be useful if the interval boundaries of the discretization are not meaningful to the domain expert.</p><p>Several discretization methods for time series are discussed by Daw et al. <ref type="bibr" target="#b2">[2]</ref>. The discretization is said to be often motivated by the desire to speed up processing and to remove noise. Many real life time series are smooth <ref type="bibr" target="#b11">[11]</ref>. Using each time point or a small window for discretization will usually produce consecutive stretches of the same symbol. Daw et al. state that "from the standpoint of observing meaningful patterns, high frequencies of symbol repetition are not very useful and usually indicate over-sampling of the original data". But interesting temporal phenomena do not necessarily occur at the same time scale. Trying to avoid this so called over-sampling would mean to enlarge the window size, possibly destroying short temporal phenomena in some places. We think that with smooth time series it will be better to keep this high level of temporal resolution and search for persisting states. This results in labeled interval sequences representing the concept of duration. More complex temporal patterns covering the concepts of coincidence and order can be searched with the Time Series Knowledge Mining (TSKM) framework <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20]</ref>.</p><p>The related task of time series segmentation (e.g. <ref type="bibr" target="#b10">[10]</ref>) is beyond the scope of this paper. Segmentation does not lead to recurring state labels per se. Instead of dividing the value dimension in intervals, the time dimension is segmented to produce line or curve segments homogeneous w.r.t. some quality measure. Feature extraction on the segments can lead to recurring labels like increasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PERSISTENCE SCORE</head><p>We propose a new quality score for meaningful unsupervised discretization of time series by taking the temporal information into account and searching for persistence. If discretization is not done purely for the sake of compression and noise filtering <ref type="bibr" target="#b2">[2]</ref> the analyst is commonly interested in states of the underlying process generating the time series. We assume that the time series contain enduring states and that these states are of interest. Any long term trend that could cause such states not to be aligned on the value axis should be removed, e.g. by substracting moving averages. Time series with very fast changing behaviour are not well suited for the discovery of persisting states, but they can be after an appropriate feature extraction.</p><p>We argue, that one discretization is better than another if the resulting states show more persisting behavior. We expect many knowledge discovery approaches to profit from more meaningful symbols incorporating the temporal structure of the time series, e.g. rule discovery in univariate <ref type="bibr" target="#b6">[6]</ref> and multivariate <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b20">20]</ref> time series, or anomaly detection <ref type="bibr" target="#b12">[12]</ref>. The quality measure used for persistence is based on the Kullback-Leibler divergence of the marginal and selftransition probability distributions of the symbols. Let S = {S1, ..., S k } be the set of possible symbols and s = {si|si ∈ S i = 1..n} be a symbolic time series of length n. Let P (Sj) be the marginal probability of the symbol Sj. The k × k matrix of transition probabilities is given by A(j, m) = P (si = Sj|si-1 = Sm). The self-transition probabilities are the values on the main diagonal of A.</p><p>If there is no temporal structure in the time series, the symbols can be interpreted as independent observations of a random variable according to the marginal distribution of symbols. The probability of observing a each symbol is independent from the previous symbol, i.e. P (si = Sj |si-1) = P (Sj). The transition probabilities are A(j, m) = P (Sj). The most simple temporal structure is a first order Markov model (e.g. <ref type="bibr" target="#b21">[21]</ref>). Each state depends only on the previous state, i.e. P (si = Sj|si-1, ...si-m) = P (Sj|si-1). Persistence can be measured by comparing these two models. If there is no temporal structure, the transition probabilities of the Markov model should be close to the marginal probabilities. If the states show persisting behavior, however, the self-transition probabilities will be higher than the marginal probabilities. If a process is less likely to stay in a certain state, the particular transition probability will be lower than the corresponding marginal value. Since it is the self-transitions in particular we are interested in, we will not compare the complete distributions of k symbols but only the the distributions of self vs. non-self per state. Note, that we are not assuming the symbolic time series to be produced by a first order Markov model. We merely use the self-transition probabilities as an indicator for persistence. The higher this probability, the longer the intervals of this particular state tend to be.</p><p>A well known measure for comparing two probability distributions is the Kullback-Leibler divergence <ref type="bibr" target="#b15">[15]</ref>. For two discrete probability distributions P = {p1, ..., p k } and Q = {q1, ..., q k } of k symbols it is defined in Equation <ref type="formula" target="#formula_0">1</ref>.</p><formula xml:id="formula_0">KL(P, Q) = k X i=1 pilog( pi qi )<label>(1)</label></formula><p>A symmetric version is obtained by taking the mean of both directions (Equation <ref type="formula" target="#formula_1">2</ref>).</p><formula xml:id="formula_1">SKL(P, Q) = 1 2 (KL(P, Q) + KL(Q, P ))<label>(2)</label></formula><p>Figure <ref type="figure" target="#fig_0">1</ref> shows a plot of the symmetric Kullback-Leibler divergence between two binary random variables for probability distributions from {0.01, 0.99} to {0.99, 0.01} for each. The value is 0 on the diagonal of the pq plane, where both distributions are equal. The values rise continuously toward the extreme differences in the left and right corners. For binary random variables we define the shortcut notation in </p><formula xml:id="formula_2">SKL(p, q) := SKL({p, 1 -p}, {q, 1 -q}) ∀p, q ∈]0, 1]<label>(3)</label></formula><p>The persistence score of state j is defined in Equation <ref type="formula">4</ref>as the product of the symmetric Kullback-Leibler divergence of the transition and marginal probability distribution for self vs. non-self with an indicator variable. The indicator determines the sign of the score. States with self-transition probabilities higher than the marginal will obtain positive values and states with low self-transition probabilities inhibit negative values. The score is zero if and only if the probability distributions are equal. P ersistence(Sj) = sgn(A(j, j)-P (Sj))SKL(A(j, j), P (Sj)) (4) A summary score for all states is defined in Equation 5 as the mean of the values per state. This captures the notion of mean persistence, i.e. all or most states need to have high persistence for achieving high persistence scores. The higher the persistence score, the more likely it is to observe at any point in time the same state as for the previous time point and this implies longer persisting state intervals.</p><formula xml:id="formula_3">P ersistence(S) = 1 k k X m=1 P ersistence(Sm)<label>(5)</label></formula><p>The calculation of the persistence scores is straight forward. Maximum likelihood estimates of all involved probabilities can easily be obtained by counting the number of symbols for each state for the P (Sj) and the numbers of each possible state pair for A. A problem arises if there are self-transitions that are never observed. The Kullback-Leibler divergence is not defined if any involved probability is zero. This is a very rare case, however. The marginal probabilities are never zero, otherwise a state wouldn't exist. The non-self transition probability can only be zero if a state only occurs in a contiguous segment at the end of the time series. This can happen for at most one state. The self-transition can only be zero if a state never follows itself, unlikely for smooth time series. To avoid an invalid score with zero probabilities we set the probability to a small value (e.g. n -1 ) that is subtracted from the complementary event.</p><p>As long as the value is smaller than the smallest observed probability, the scores can still be used for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">THE PERSIST ALGORITHM</head><p>The persistence score is used to guide the selection of bins in the Persist algorithm described in this chapter. The first step is to obtain a set of candidate bin boundaries from the data. We propose to use equal frequency binning with a large number of bins to obtain a coarse sampling of candidate cuts in sparse regions and a fine sampling in dense regions. As a default value for the number of bins we use 100 to get the percentiles of the data. If a higher accuracy is required more bins can be used. In each iteration of the algorithm all available candidate cuts are individually added to the current set of cuts and the persistence score is calculated. The cut achieving the highest persistence is chosen. This is repeated until the desired number of bins is obtained.</p><p>Let  The symbols are not required to be equally frequent, thus even rare states can achieve high scores. In many cases it will be desirable to avoid the creation of very rare states, i.e. bin boundaries very close to cuts previously chosen. We excluded states that cover less than 5% of the time points. This is not shown in Figure <ref type="figure" target="#fig_2">2</ref> for brevity of presentation.</p><formula xml:id="formula_4">X = {xi|xi ∈ R i = 1..n}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>The evaluation of unsupervised algorithms is difficult due to the lack of the ground truth. We evaluated the performance of the Persist algorithm by extensive experiments using artificial data with known states and real life data where the true states were rather obvious. We compared the novel algorithm with the following eight methods. EQF: For equal frequency histograms k bins containing approximately the same number of data points were created. SAX: Inspired by the SAX method k bins were chosen such that values from a normal distribution with the same mean and standard deviation as the data would be equally likely in each bin. Note, that we are actually using a special case of SAX. A window size of one for the PAA is used, effectively skipping this step. We are also not using numerosity reduction or sliding windows to be comparable with the other methods. The results should therefore not be held against the full SAX method designed to preserve time series distances. EQW: For equal width histograms the range of the data was divided into k segments of equal length. M±S: The mean (µ) and standard deviation (σ) were used for creating bins. For an even number of bins the boundaries were {µ + iσ|i = -k-2 2 , ..., k-2 2 }, for odd values of k we used {µ + `isgn(i) 1  2 ´σ|i = -k-1 2 , ..., k-1 2 } (k &gt; 1). M±A: Same as M±S using median and adjusted median absolute deviation (AMAD) instead of mean and standard deviation, respectively. KM: The k-Means algorithm 1 with uniform initialization and Manhattan distance was used to obtain k clusters. The bin boundaries were calculated as the midpoints of neighboring cluster centers. GMM: A Gaussian mixture model <ref type="bibr" target="#b1">[1]</ref> was initialized with the cluster centers from k-Means and the Expectation Maximization (EM) algorithm was used to estimate the means, variances, and mixing weights. The bin boundaries were obtained by maximum likelihood decision. HMM: A Hidden Markov Model <ref type="bibr" target="#b21">[21]</ref> with k states and a single Gaussian output distribution per state was initialized with the result of the GMM estimation. The training of the model and the calculation of the Viterbi state sequence were performed using the HMM Toolbox 2 . HMM is the only competing method using the temporal information of the time series. It is not quite comparable to the other methods, however, because it does not return a set of bins. The state sequence is directly created.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Artificial data</head><p>We generated artificial data using a specified number of states and Gaussian distributions per state. The standard deviation for each state was chosen with uniform random numbers between 0.1 and 1. The mean value of the first state was set to zero. The distance of the means from one state to the next was specified in terms of the number of standard deviations of both distributions. This distance is chosen with uniform random numbers between 1 and 2. E.g. if the standard deviation of the first and second states are 0.5 and 0.7, respectively, and the distance is 1.5, the second mean will be 0 + 1.5(0.5 + 0.7) = 1.8. Each new state was chosen randomly with all states being equally likely. The duration of each state was chosen with uniform random numbers from the interval [0.005n, 0.05n]. We generated 1000 time series of length 1000 for k = 2, ..., 7 states. For each time series 10 additional noisy versions were created by adding 1% to 10% outliers uniformly drawn from the interval determined by the mean ± the range of the original time series.</p><p>An example for 4 states and 5% outliers is shown in  <ref type="bibr" target="#b23">[23]</ref> of the marginal empirical probability distribution. Three states are visible as peaks with significant overlap. The existence of the 4th state on the right is not at all obvious from the marginal probability distribution, because it is less frequent than the other states.</p><p>We applied all discretization methods using the known number of states. We measured the accuracy of the discretization by comparing the obtained state sequence with the true state sequence used for generating the data. The median accuracies and the deviations (AMAD) for k = 5 states and three levels of outlier contamination are listed in Table <ref type="table" target="#tab_0">1</ref>. We used the robust median and AMAD measures,  The Persist algorithm always has a higher median accuracy than any static method with large distances to the second best. The deviation is also much smaller than for the other methods, indicating high consistency. Even with 10% outliers, the performance of the new algorithm is still better than for any static method applied to the same data without outliers! Compared to the only other temporal method, HMM, the performance of Persist is slightly worse for 0% outliers. But with larger levels of outlier contamination, the HMM results degrade rapidly, even below the results from several static methods. The best competing methods for noisy data are EQF and SAX. The performance of both degrades rather slowly with increasing noise. Without outliers the GMM method shows the best performance among the static methods, but the accuracies are very bad with added noise. The methods using measures of location and dispersion (M±S,M±A) do not seem to be well suited for the generated datasets in general. The results for other values of k were similar. The absolute differences in accuracy were smaller for k = 2, 3, 4 and even larger for k = 6, 7. The performance of HMM degraded later w.r.t. outlier contamination for fewer states and earlier for more states. Figure <ref type="figure" target="#fig_6">4</ref> plots the median accuracies for 3 states, all methods, and all outlier levels. Again, the Persist method is always the best except for HMM at low outlier levels. EQW and GMM perform bad in general. All other methods degrade gently in the presence of noise, but are clearly inferior to Persist.</p><p>In order to check the results for statistical significance, we tested the hypothesis that the accuracy of the Persist is better than the accuracy of the competing algorithms. Since the accuracies do not follow a normal distribution, we used the robust (but weaker) rank sum test instead of the t-test. The test was performed for all k and all noise levels. For the competing static methods all p-values were smaller than 0.001, clearly indicating superior performance that can be attributed to the incorporation of temporal information. Compared to HMM, the results are significantly better for the larger amounts of outliers and worse for no of few outliers. The more states are present, the less robust HMM tends to be. Table <ref type="table" target="#tab_1">2</ref> shows the result of the statistical tests between Persist and HMM. A plus indicates Persist to be better than HMM, for a minus the accuracy is significantly lower, circles are placed where the p-values were larger than 0.01. In summary, the Persist algorithm was able to recover the original state sequence with significantly higher accuracy and more consistency than all competing static methods. The temporal HMM method is slightly better than Persist for no or few outliers, but much worse for more complicated and realistic settings with more states and outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Real data</head><p>For real life data the states of the underlying process are typically unknown. Otherwise a discretization into recurring states wouldn't be necessary. We explored the behavior of the Persist algorithm in comparison with the other methods on datasets that clearly show several states. Due to lack of  space the results of only one dataset are presented. Other datasets well suited for our method include e.g. the Power <ref type="bibr" target="#b9">[9]</ref>, the Auslan <ref type="bibr">[8]</ref>, and the Context Recognition data <ref type="bibr" target="#b18">[18]</ref>.</p><p>The Muscle dataset (donated to <ref type="bibr" target="#b9">[9]</ref>) describes the muscle activation of a professional inline speed skater <ref type="bibr" target="#b20">[20]</ref>. The muscle is expected to switch mainly between being active and relaxed states. A 5s excerpt of the data is shown in Figure <ref type="figure" target="#fig_8">5</ref>(a). The muscle activation was calculated from the original EMG (Electromyography) measurements by taking the logarithm of the energy derived from a wavelet analysis and is displayed in arbitrary units. Figure <ref type="figure" target="#fig_8">5(b)</ref> shows the persistence score for k = 2, ..., 7. The scores for k = 3, ..5 are almost the same. Consulting an expert we chose k = 3 states even though the score for 4 is slightly higher. The resulting bin boundaries of four selected methods are shown in Figure <ref type="figure" target="#fig_10">6</ref> as vertical lines on top of a probability density estimation plot. All methods except Persist place cuts in high density regions. EQF sets the first cut very close to the peak corresponding to the state of low muscle activation. This will result in a large amount of transitions between the first two states. EQW and KM do the same for the second peak in the density, corresponding to high muscle activation. Persist is the only method that places a cut to the right of the second peak. This results in a state for very high activation. The validity of this state can also be seen from Figure <ref type="figure" target="#fig_8">5</ref>(a), where the horizontal lines correspond to the bins selected by Persist. The very high values are not randomly scattered during the interval of high activation but rather concentrate toward the end of each activation phase. This interesting temporal structure is not visible from the density plot and is not discovered by the other methods. The long interval of high activation is the gliding phase, where the muscle is needed to stabilize the body's center of gravity while gliding forward. The culmination of energy at the end is explained by the push off with the foot to generate forward propulsion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DISCUSSION</head><p>The proposed quality score and algorithm for detecting persisting states has been shown to outperform existing methods on artificial data in the task of detecting the true underlying states. The results on the real dataset lead to more meaningful interval boundaries with a state of very high muscle activity, that was neglected by the other methods. Nevertheless the method is simple, exact, and easy to implement.</p><p>The only competing method using temporal structure was HMM. The EM estimation of HMM is more complex, needs  a good initialization, is sensitive to noise, and only converges to a local maximum of the likelihood. The quality of HMM was better than that of Persist without noise, but degraded rapidly when outliers were added. HMM models are also harder to interpret than the result of binning methods like Persist.</p><p>In <ref type="bibr" target="#b4">[4]</ref> EM like algorithms for the discovery of recurring states in time series are presented. An initial guess of the number of segments is required, however. This will be very difficult for long time series. We only require a rough guess for the number of states.</p><p>For the application of Persist, the time series need to be de-trended and should contain states defined by intervals on the value axis. This can be extended to handle states describing local trends e.g. with differentiation filters. Persisting states in multivariate time series can be discovered by discretizing each component and searching for coinciding states <ref type="bibr" target="#b19">[19]</ref> or by incorporating the quality score in clustering methods. More research is needed on the suitability of the persistence score for selecting the number of states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">SUMMARY</head><p>Many time series discretization methods ignore the temporal order of values. The incorporation of temporal information leads to the unsupervised Persist algorithm. It achieves much higher accuracy than existing static methods and is robust against noise. It also outperforms HMM for all but very simple cases. The symbols created by the Persist method are likely to produce meaningful results in knowledge discovery tasks like rule generation or anomaly detection because they correspond to persisting states.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Symmetric Kullback-Leibler divergence of P = {p, 1 -p} and Q = {q, 1 -q}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>be the numerical values of a time series of length n and C = {cj |cj ∈ R j = 1..m} be the set of candidate cuts. Let D be a function performing the actual discretization given X and a set of bin boundaries B. The Persist algorithm for discovering k states is given in Figure 2. The time complexity is O(n). B := ∅ // bin boundaries for i = 1...k -1 P := ∅ // stores persistence scores foreach c ∈ C // all candidates P := P ∪ {P ersistence(D(X, B ∪ {c}))} end b := maxi(P ) B := B ∪ {c b } // add best cut end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Persist algorithm for unsupervised time series discretization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 3(a). The horizontal lines indicate the true means of the 4 states. The large spikes are caused by the outliers. Figure 3(b) shows the Pareto Density Estimation (PDE)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1</head><label></label><figDesc>the implementation in the Matlab Statistics Toolbox 2 http://www.ai.mit.edu/˜murphyk/Software/hmm.html</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Artificial data with 4 states and 5% outliers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Median accuracy for 3 states</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Muscle data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Probability density estimate with bins for muscle data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Median accuracy for 5 states</figDesc><table><row><cell cols="2">Outliers 0%</cell><cell>5%</cell><cell>10%</cell></row><row><cell>EQF</cell><cell>0.74 ± 0.08</cell><cell>0.71 ± 0.08</cell><cell>0.69 ± 0.07</cell></row><row><cell>SAX</cell><cell>0.74 ± 0.09</cell><cell>0.74 ± 0.08</cell><cell>0.72 ± 0.08</cell></row><row><cell>EQW</cell><cell>0.67 ± 0.16</cell><cell>0.33 ± 0.09</cell><cell>0.32 ± 0.08</cell></row><row><cell>M±S</cell><cell>0.56 ± 0.11</cell><cell>0.48 ± 0.10</cell><cell>0.43 ± 0.09</cell></row><row><cell>M±A</cell><cell>0.51 ± 0.16</cell><cell>0.48 ± 0.15</cell><cell>0.45 ± 0.13</cell></row><row><cell>KM</cell><cell>0.71 ± 0.21</cell><cell>0.66 ± 0.22</cell><cell>0.61 ± 0.24</cell></row><row><cell>GMM</cell><cell>0.79 ± 0.18</cell><cell>0.27 ± 0.12</cell><cell>0.24 ± 0.11</cell></row><row><cell>HMM</cell><cell>0.94 ± 0.08</cell><cell>0.52 ± 0.34</cell><cell>0.44 ± 0.29</cell></row><row><cell>Persist</cell><cell cols="3">0.90 ± 0.03 0.86 ± 0.03 0.83 ± 0.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Test decisions of Persist vs. HMM</figDesc><table><row><cell>Outliers</cell><cell>States</cell></row><row><cell></cell><cell>2 3 4 5 6 7</cell></row><row><cell cols="2">0% ---• + +</cell></row><row><cell cols="2">1% ---+ + +</cell></row><row><cell cols="2">2% --• + + +</cell></row><row><cell cols="2">3% --+ + + +</cell></row><row><cell cols="2">4% -• + + + +</cell></row><row><cell cols="2">5% -+ + + + +</cell></row><row><cell cols="2">6% -+ + + + +</cell></row><row><cell cols="2">7% -+ + + + +</cell></row><row><cell cols="2">8% -+ + + + +</cell></row><row><cell cols="2">9% -+ + + + +</cell></row><row><cell cols="2">10% • + + + + +</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: We thank Dr. Olaf Hoos, Department of Sports Medicine, Philipps-University Marburg, Germany, for the muscle data and interpretation of the results.</p><p>Reproducible Results Statement: All datasets and code used in this work are available by emailing the first author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A Gentle Tutorial on the EM Algorithm and its Application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<idno>ICSI-TR-97-021</idno>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>University of Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A review of symbolic analysis of experimental data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Daw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tracy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of Scientific Instruments</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="916" to="930" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised and unsupervised discretization of continuous features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dougherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Machine Learning</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="194" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Finding recurrent sources in sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int. Conf. on Computational Molecular Biology</title>
		<meeting>7th Int. Conf. on Computational Molecular Biology</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="123" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sequential association rule mining with time lags</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Harms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deogun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems (JIIS)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Temporal rule discovery using genetic programming and specialized hardware</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Hetland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saetrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Int. Conf. on Recent Advances in Soft Computing</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Lotfi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Garibaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>John</surname></persName>
		</editor>
		<meeting>4th Int. Conf. on Recent Advances in Soft Computing</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="182" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The role of discretization parameters in sequence rule evolution</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Hetland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Saetrom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int. Conf. on Knowledge-Based Intelligent Information &amp; Engineering Systems</title>
		<meeting>7th Int. Conf. on Knowledge-Based Intelligent Information &amp; Engineering Systems</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning comprehensible descriptions of multivariate time series</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Kadous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Int. Conf. on Machine Learning</title>
		<meeting>16th Int. Conf. on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="454" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<ptr target="http://www.cs.ucr.edu/˜eamonn/tsdma/index.html" />
		<title level="m">UCR Time Series Data Mining Archive</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Segmenting time series: A survey and novel approach</title>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining in Time Series Databases</title>
		<imprint>
			<publisher>World Scientific Publishing Company</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the need for time series data mining benchmarks: A survey and empirical demonstration</title>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kasetty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">th ACM SIGKDD</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="102" to="111" />
			<date type="published" when="2002">2002</date>
			<pubPlace>Edmonton, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding surprising patterns in a time series database in linear time and space</title>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lonardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th ACM SIGKDD</title>
		<meeting>8th ACM SIGKDD<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-07">July 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dimensionality reduction for fast similarity search in large time series databases</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="263" to="286" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Error-based and entropy-based discretization of continuous features</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Int. Conf. on Knowledge Discovery and Data Mining</title>
		<meeting>2nd Int. Conf. on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="114" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On information and sufficiency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A symbolic representation of time series, with implications for streaming algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lonardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th ACM SIGMOD, DMKD workshop</title>
		<meeting>8th ACM SIGMOD, DMKD workshop</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discretization: An enabling technique</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="393" to="423" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sensor signal data set for exploring context recognition of mobile devices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mäntyjärvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Himberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kangas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Tuomela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huuskonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Int. Conf. on Pervasive Computing</title>
		<meeting><address><addrLine>Linz/Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discovering temporal knowlegde in multivariate time series</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mörchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ultsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. GfKl</title>
		<meeting>GfKl<address><addrLine>Dortmund, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Extracting interpretable muscle activation patterns with time series knowledge mining</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mörchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ultsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hoos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Knowledge-Based &amp; Intelligent Engineering Systems</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A tutorial on hidden markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE</title>
		<meeting>of IEEE</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning first order logic time series classifiers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Boström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Int. Conf. on Inductive Logic Programming</title>
		<meeting>10th Int. Conf. on Inductive Logic Programming</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="260" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pareto Density Estimation: Probability Density Estimation for Knowledge Discovery</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ultsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. GfKl</title>
		<meeting>GfKl<address><addrLine>Cottbus, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
