<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
							<email>yuancao@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">83B42BCD30091E8F6CDBF37CD38FFDE3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the training and generalization of deep neural networks (DNNs) in the over-parameterized regime, where the network width (i.e., number of hidden nodes per layer) is much larger than the number of training data points. We show that, the expected 0-1 loss of a wide enough ReLU network trained with stochastic gradient descent (SGD) and random initialization can be bounded by the training loss of a random feature model induced by the network gradient at initialization, which we call a neural tangent random feature (NTRF) model. For data distributions that can be classified by NTRF model with sufficiently small error, our result yields a generalization error bound in the order of r Opn ´1{2 q that is independent of the network width. Our result is more general and sharper than many existing generalization error bounds for over-parameterized neural networks. In addition, we establish a strong connection between our generalization error bound and the neural tangent kernel (NTK) proposed in recent work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has achieved great success in a wide range of applications including image processing <ref type="bibr" target="#b19">[20]</ref>, natural language processing <ref type="bibr" target="#b16">[17]</ref> and reinforcement learning <ref type="bibr" target="#b33">[34]</ref>. Most of the deep neural networks used in practice are highly over-parameterized, such that the number of parameters is much larger than the number of training data. One of the mysteries in deep learning is that, even in an over-parameterized regime, neural networks trained with stochastic gradient descent can still give small test error and do not overfit. In fact, a famous empirical study by Zhang et al. <ref type="bibr" target="#b37">[38]</ref> shows the following phenomena:</p><p>• Even if one replaces the real labels of a training data set with purely random labels, an overparameterized neural network can still fit the training data perfectly. However since the labels are independent of the input, the resulting neural network does not generalize to the test dataset. • If the same over-parameterized network is trained with real labels, it not only achieves small training loss, but also generalizes well to the test dataset.</p><p>While a series of recent work has theoretically shown that a sufficiently over-parameterized (i.e., sufficiently wide) neural network can fit random labels <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b38">39]</ref>, the reason why it can generalize well when trained with real labels is less understood. Existing generalization bounds for deep neural networks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b27">28]</ref> based on uniform convergence usually cannot provide non-vacuous bounds <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13]</ref> in the over-parameterized regime. In fact, the empirical observation by Zhang et al. <ref type="bibr" target="#b37">[38]</ref> indicates that in order to understand deep learning, it is important to distinguish the true data labels from random labels when studying generalization. In other words, it is essential to quantify the "classifiability" of the underlying data distribution, i.e., how difficult it can be classified.</p><p>Certain effort has been made to take the "classifiability" of the data distribution into account for generalization analysis of neural networks. Brutzkus et al. <ref type="bibr" target="#b6">[7]</ref> showed that stochastic gradient descent (SGD) can learn an over-parameterized two-layer neural network with good generalization for linearly separable data. Li and Liang <ref type="bibr" target="#b24">[25]</ref> proved that, if the data satisfy certain structural assumption, SGD can learn an over-parameterized two-layer network with fixed second layer weights and achieve a small generalization error. Allen-Zhu et al. <ref type="bibr" target="#b0">[1]</ref> studied the generalization performance of SGD and its variants for learning two-layer and three-layer networks, and used the risk of smaller two-layer or three-layer networks with smooth activation functions to characterize the classifiability of the data distribution. There is another line of studies on the algorithm-dependent generalization bounds of neural networks in the over-parameterized regime <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b13">14]</ref>, which quantifies the classifiability of the data with a reference function class defined by random features <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> or kernels <ref type="foot" target="#foot_1">1</ref> . Specifically, Daniely <ref type="bibr" target="#b9">[10]</ref> showed that a neural network of large enough size is competitive with the best function in the conjugate kernel class of the network. Arora et al. <ref type="bibr" target="#b3">[4]</ref> gave a generalization error bound for twolayer ReLU networks with fixed second layer weights based on a ReLU kernel function. Cao and Gu <ref type="bibr" target="#b7">[8]</ref> showed that deep ReLU networks trained with gradient descent can achieve small generalization error if the data can be separated by certain random feature model <ref type="bibr" target="#b31">[32]</ref> with a margin. Yehudai and Shamir <ref type="bibr" target="#b36">[37]</ref> used the expected loss of a similar random feature model to quantify the generalization error of two-layer neural networks with smooth activation functions. A similar generalization error bound was also given by E et al. <ref type="bibr" target="#b13">[14]</ref>, where the authors studied the optimization and generalization of two-layer networks trained with gradient descent. However, all the aforementioned results are still far from satisfactory: they are either limited to two-layer networks, or restricted to very simple and special reference function classes.</p><p>In this paper, we aim at providing a sharper and generic analysis on the generalization of deep ReLU networks trained by SGD. In detail, we base our analysis upon the key observations that near random initialization, the neural network function is almost a linear function of its parameters and the loss function is locally almost convex. This enables us to prove a cumulative loss bound of SGD, which further leads to a generalization bound by online-to-batch conversion <ref type="bibr" target="#b8">[9]</ref>. The main contributions of our work are summarized as follows:</p><p>• We give a bound on the expected 0-1 error of deep ReLU networks trained by SGD with random initialization. Our result relates the generalization bound of an over-parameterized ReLU network with a random feature model defined by the network gradients, which we call neural tangent random feature (NTRF) model. It also suggests an algorithm-dependent generalization error bound of order r Opn ´1{2 q, which is independent of network width, if the data can be classified by the NTRF model with small enough error.</p><p>• Our analysis is general enough to cover recent generalization error bounds for neural networks with random feature based reference function classes, and provides better bounds. Our expected 0-1 error bound directly covers the result by Cao and Gu <ref type="bibr" target="#b7">[8]</ref>, and gives a tighter sample complexity when reduced to their setting, i.e., r Op1{ 2 q versus r Op1{ 4 q where is the target generalization error. Compared with recent results by Yehudai and Shamir <ref type="bibr" target="#b36">[37]</ref>, E et al. <ref type="bibr" target="#b13">[14]</ref> who only studied two-layer networks, our bound not only works for deep networks, but also uses a larger reference function class when reduced to the two-layer setting, and therefore is sharper.</p><p>• Our result has a direct connection to the neural tangent kernel studied in Jacot et al. <ref type="bibr" target="#b17">[18]</ref>. When interpreted in the language of kernel method, our result gives a generalization bound in the form of r OpL ¨ay J pΘ pLq q ´1y{nq, where y is the training label vector, and Θ pLq is the neural tangent kernel matrix defined on the training input data. This form of generalization bound is similar to, but more general and tighter than the bound given by Arora et al. <ref type="bibr" target="#b3">[4]</ref>.</p><p>Notation We use lower case, lower case bold face, and upper case bold face letters to denote scalars, vectors and matrices respectively. For a vector v " pv 1 , . . . , v d q T P R d and a number 1 ď p ă 8, let }v} p " p ř d i"1 |v i | p q 1{p . We also define }v} 8 " max i |v i |. For a matrix A " pA i,j q mˆn , we use }A} 0 to denote the number of non-zero entries of A, and denote }A} F " p ř d i,j"1 A 2 i,j q 1{2 and }A} p " max }v}p"1 }Av} p for p ě 1. For two matrices A, B P R mˆn , we define xA, By " TrpA J Bq. We denote by A ľ B if A ´B is positive semidefinite. In addition, we define the asymptotic notations Op¨q, r Op¨q, Ωp¨q and r Ωp¨q as follows. Suppose that a n and b n be two sequences. We write a n " Opb n q if lim sup nÑ8 |a n {b n | ă 8, and a n " Ωpb n q if lim inf nÑ8 |a n {b n | ą 0. We use r Op¨q and r Ωp¨q to hide the logarithmic factors in Op¨q and Ωp¨q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Setup</head><p>In this section we introduce the basic problem setup. Following the same standard setup implemented in the line of recent work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b7">8]</ref>, we consider fully connected neural networks with width m, depth L and input dimension d. Such a network is defined by its weight matrices at each layer: for L ě 2, let W 1 P R mˆd , W l P R mˆm , l " 2, . . . , L ´1 and W L P R 1ˆm be the weight matrices of the network. Then the neural network with input x P R d is defined as</p><formula xml:id="formula_0">f W pxq " ? m ¨WL σpW L´1 σpW L´2 ¨¨¨σpW 1 xq ¨¨¨qq,<label>(2.1)</label></formula><p>where σp¨q is the entry-wise activation function. In this paper, we only consider the ReLU activation function σpzq " maxt0, zu, which is the most commonly used activation function in applications. It is also arguably one of the most difficult activation functions to analyze, due to its non-smoothess. We remark that our result can be generalized to many other Lipschitz continuous and smooth activation functions. For simplicity, we follow Allen-Zhu et al. <ref type="bibr" target="#b1">[2]</ref>, Du et al. <ref type="bibr" target="#b10">[11]</ref> and assume that the widths of each hidden layer are the same. Our result can be easily extended to the setting that the widths of each layer are not equal but in the same order, as discussed in Zou et al. <ref type="bibr" target="#b38">[39]</ref>, Cao and Gu <ref type="bibr" target="#b7">[8]</ref>.</p><p>When L " 1, the neural network reduces to a linear function, which has been well-studied. Therefore, for notational simplicity we focus on the case L ě 2, where the parameter space is defined as</p><formula xml:id="formula_1">W :" R mˆd ˆpR mˆm q L´2 ˆR1ˆm .</formula><p>We also use W " pW 1 , . . . , W L q P W to denote the collection of weight matrices for all layers. For W, W 1 P W, we define their inner product as xW, W 1 y :" ř L l"1 TrpW J l W 1 l q. The goal of neural network learning is to minimize the expected risk, i.e.,</p><formula xml:id="formula_2">min W L D pWq :" E px,yq"D L px,yq pWq,<label>(2.2)</label></formula><p>where L px,yq pWq " ry ¨fW pxqs is the loss defined on any example px, yq, and pzq is the loss function. Without loss of generality, we consider the cross-entropy loss in this paper, which is defined as pzq " logr1 `expp´zqs. We would like to emphasize that our results also hold for most convex and Lipschitz continuous loss functions such as hinge loss. We now introduce stochastic gradient descent based training algorithm for minimizing the expected risk in (2.2). The detailed algorithm is given in Algorithm 1.</p><p>Algorithm 1 SGD for DNNs starting at Gaussian initialization Input: Number of iterations n, step size η.</p><p>Generate each entry of W p1q l independently from N p0, 2{mq, l P rL ´1s. Generate each entry of W p1q L independently from N p0, 1{mq. for i " 1, 2, . . . , n do Draw px i , y i q from D. Update W pi`1q " W piq ´η ¨∇W L pxi,yiq pW piq q. end for Output: Randomly choose x W uniformly from tW p1q , . . . , W pnq u.</p><p>The initialization scheme for W p1q given in Algorithm 1 generates each entry of the weight matrices from a zero-mean independent Gaussian distribution, whose variance is determined by the rule that the expected length of the output vector in each hidden layer is equal to the length of the input. This initialization method is also known as He initialization <ref type="bibr" target="#b15">[16]</ref>. Here the last layer parameter is initialized with variance 1{m instead of 2{m since the last layer is not associated with the ReLU activation function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Main Results</head><p>In this section we present the main results of this paper. In Section 3.1 we give an expected 0-1 error bound against a neural tangent random feature reference function class. In Section 3.2, we discuss the connection between our result and the neural tangent kernel proposed in Jacot et al. <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">An Expected 0-1 Error Bound</head><p>In this section we give a bound on the expected 0-1 error L 0´1 D pWq :" E px,yq"D r1ty ¨fW pxq ă 0us obtained by Algorithm 1. Our result is based on the following assumption. Assumption 3.1. The data inputs are normalized: }x} 2 " 1 for all px, yq P supppDq. Assumption 3.1 is a standard assumption made in almost all previous work on optimization and generalization of over-parameterized neural networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b13">14]</ref>. As is mentioned in Cao and Gu <ref type="bibr" target="#b7">[8]</ref>, this assumption can be relaxed to c 1 ď }x} 2 ď c 2 for all px, yq P supppDq, where c 2 ą c 1 ą 0 are absolute constants.</p><p>For any W P W, we define its ω-neighborhood as BpW, ωq :" tW 1 P W : }W 1 l ´Wl } F ď ω, l P rLsu. Below we introduce the neural tangent random feature function class, which serves as a reference function class to measure the "classifiability" of the data, i.e., how easy it can be classified. Definition 3.2 (Neural Tangent Random Feature). Let W p1q be generated via the initialization scheme in Algorithm 1. The neural tangent random feature (NTRF) function class is defined as</p><formula xml:id="formula_3">FpW p1q , Rq " f p¨q " f W p1q p¨q `x∇ W f W p1q p¨q, Wy : W P Bp0, R ¨m´1{2 q ( ,</formula><p>where R ą 0 measures the size of the function class, and m is the width of the neural network.</p><p>The name "neural tangent random feature" is inspired by the neural tangent kernel proposed by Jacot et al. <ref type="bibr" target="#b17">[18]</ref>, because the random features are the gradients of the neural network with random weights. Connections between the neural tangent random features and the neural tangent kernel will be discussed in Section 3.2.</p><p>We are ready to present our main result on the expected 0-1 error bound of Algorithm 1. Theorem 3.3. For any δ P p0, e ´1s and R ą 0, there exists</p><formula xml:id="formula_4">m ˚pδ, R, L, nq " r O `polypR, Lq ˘¨n 7 ¨logp1{δq</formula><p>such that if m ě m ˚pδ, R, L, nq, then with probability at least 1 ´δ over the randomness of W p1q , the output of Algorithm 1 with step size η " κ ¨R{pm ? nq for some small enough absolute constant κ satisfies</p><formula xml:id="formula_5">E " L 0´1 D p x Wq ‰ ď inf f PF pW p1q ,Rq # 4 n n ÿ i"1 ry i ¨f px i qs + `O« LR ? n `c logp1{δq n ff ,<label>(3.1)</label></formula><p>where the expectation is taken over the uniform draw of x W from tW p1q , . . . , W pnq u.</p><p>The expected 0-1 error bound given by Theorem 3.3 consists of two terms: The first term in (3.1) relates the expected 0-1 error achieved by Algorithm 1 with a reference function class-the NTRF function class in Definition 3.2. The second term in (3.1) is a standard large-deviation error term. As long as R " r Op1q, this term matches the standard r Opn ´1{2 q rate in PAC learning bounds <ref type="bibr" target="#b32">[33]</ref>. Remark 3.4. The parameter R in Theorem 3.3 is from the NTRF class and introduces a trade-off in the bound: when R is small, the corresponding NTRF class FpW p1q , Rq is small, making the first term in (3.1) large, and the second term in (3.1) is small. When R is large, the corresponding function class FpW p1q , Rq is large, so the first term in (3.1) is small, and the second term will be large. In particular, if we set R " r Op1q, the second term in (3.1) will be r Opn ´1{2 q. In this case, the "classifiability" of the underlying data distribution D is determined by how well its i.i.d. samples can be classified by FpW p1q , r</p><p>Op1qq. In other words, Theorem 3.3 suggests that if the data can be classified by a function in the NTRF function class FpW p1q , r</p><p>Op1qq with a small training error, the over-parameterized ReLU network learnt by Algorithm 1 will have a small generalization error.</p><p>Remark 3.5. The expected 0-1 error bound given by Theorem 3.3 is in a very general form. It directly covers the result given by Cao and Gu <ref type="bibr" target="#b7">[8]</ref>. In Appendix A.1, we show that under the same assumptions made in Cao and Gu <ref type="bibr" target="#b7">[8]</ref>, to achieve expected 0-1 error, our result requires a sample complexity of order r Op ´2q, which outperforms the result in Cao and Gu <ref type="bibr" target="#b7">[8]</ref> by a factor of ´2.</p><p>Remark 3.6. Our generalization bound can also be compared with two recent results <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b13">14]</ref> for two-layer neural networks. When L " 2, the NTRF function class FpW p1q , r Op1qq can be written as</p><formula xml:id="formula_6">f W p1q p¨q `x∇ W1 f W p1q p¨q, W 1 y `x∇ W2 f W p1q p¨q, W 2 y : }W 1 } F , }W 2 } F ď r Opm ´1{2 q ( .</formula><p>In contrast, the reference function classes studied by Yehudai and Shamir <ref type="bibr" target="#b36">[37]</ref> and E et al. <ref type="bibr" target="#b13">[14]</ref> are contained in the following random feature class:</p><formula xml:id="formula_7">F " f W p1q p¨q `x∇ W2 f W p1q p¨q, W 2 y : }W 2 } F ď r Opm ´1{2 q ( ,</formula><p>where W p1q " pW p1q 1 , W p1q 2 q P R mˆd ˆR1ˆm are the random weights generated by the initialization schemes in Yehudai and Shamir <ref type="bibr" target="#b36">[37]</ref>, E et al. <ref type="bibr" target="#b13">[14]</ref> <ref type="foot" target="#foot_2">2</ref> . Evidently, our NTRF function class FpW p1q , r</p><p>Op1qq is richer than F-it also contains the features corresponding to the first layer gradient of the network at random initialization, i.e., ∇ W1 f W p1q p¨q. As a result, our generalization bound is sharper than those in Yehudai and Shamir <ref type="bibr" target="#b36">[37]</ref>, E et al. <ref type="bibr" target="#b13">[14]</ref> in the sense that we can show that neural networks trained with SGD can compete with the best function in a larger reference function class.</p><p>As previously mentioned, the result of Theorem 3.3 can be easily extended to the setting where the widths of different layers are different. We should expect that the result remains almost the same, except that we assume the widths of hidden layers are all larger than or equal to m ˚pδ, R, L, nq. We would also like to point out that although this paper considers the cross-entropy loss, the proof of Theorem 3.3 offers a general framework based on the fact that near initialization, the neural network function is almost linear in terms of its weights. We believe that this proof framework can potentially be applied to most practically useful loss functions: whenever p¨q is convex/Lipschitz continuous/smooth, near initialization, L i pWq is also almost convex/Lipschitz continuous/smooth in W for all i P rns, and therefore standard online optimization analysis can be invoked with online-to-batch conversion to provide a generalization bound. We refer to Section 4 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Connection to Neural Tangent Kernel</head><p>Besides quantifying the classifiability of the data with the NTRF function class FpW p1q , r</p><p>Op1qq, an alternative way to apply Theorem 3.3 is to check how large the parameter R needs to be in order to make the first term in (3.1) small enough (e.g., smaller than n ´1{2 ). In this subsection, we show that this type of analysis connects Theorem 3.3 to the neural tangent kernel proposed in Jacot et al. <ref type="bibr" target="#b17">[18]</ref> and later studied by Yang <ref type="bibr" target="#b35">[36]</ref>, Lee et al. <ref type="bibr" target="#b22">[23]</ref>, Arora et al. <ref type="bibr" target="#b2">[3]</ref>. Specifically, we provide an expected 0-1 error bound in terms of the neural tangent kernel matrix defined over the training data. We first define the neural tangent kernel matrix for the neural network function in (2.1). Definition 3.7 (Neural Tangent Kernel Matrix). For any i, j P rns, define</p><formula xml:id="formula_8">r Θ p1q i,j " Σ p1q i,j " xx i , x j y, A plq ij " ˜Σplq i,i Σ plq i,j Σ plq i,j Σ plq j,j ¸, Σ pl`1q i,j " 2 ¨Epu,vq"N `0,A plq ij ˘rσpuqσpvqs, r Θ pl`1q i,j " r Θ plq i,j ¨2 ¨Epu,vq"N `0,A plq ij ˘rσ 1 puqσ 1 pvqs `Σpl`1q i,j</formula><p>.</p><p>Then we call Θ pLq " rp r Θ pLq i,j</p><p>`ΣpLq i,j q{2s nˆn the neural tangent kernel matrix of an L-layer ReLU network on training inputs x 1 , . . . , x n . Definition 3.7 is the same as the original definition in Jacot et al. <ref type="bibr" target="#b17">[18]</ref> when restricting the kernel function on tx 1 , . . . , x n u, except that there is an extra coefficient 2 in the second and third lines. This extra factor is due to the difference in initialization schemes-in our paper the entries of hidden layer matrices are randomly generated with variance 2{m, while in Jacot et al. <ref type="bibr" target="#b17">[18]</ref> the variance of the random initialization is 1{m. We remark that this extra factor 2 in Definition 3.7 will remove the exponential dependence on the network depth L in the kernel matrix, which is appealing. In fact, it is easy to check that under our scaling, the diagonal entries of Σ pLq are all 1's, and the diagonal entries of r Θ pLq are all L's.</p><p>The following lemma is a summary of Theorem 1 and Proposition 2 in Jacot et al. <ref type="bibr" target="#b17">[18]</ref>, which ensures that Θ pLq is the infinite-width limit of the Gram matrix pm ´1x∇ W f W p1q px i q, ∇ W f W p1q px j qyq nˆn , and is positive-definite as long as no two training inputs are parallel.</p><p>Lemma 3.8 (Jacot et al. <ref type="bibr" target="#b17">[18]</ref>). For an L layer ReLU network with parameter set W p1q initialized in Algorithm 1, as the network width m Ñ 8 <ref type="foot" target="#foot_3">3</ref> , it holds that</p><formula xml:id="formula_9">m ´1x∇ W f W p1q px i q, ∇ W f W p1q px j qy P Ý Ñ Θ pLq i,j</formula><p>, where the expectation is taken over the randomness of W p1q . Moreover, as long as each pair of inputs among x 1 , . . . , x n P S d´1 are not parallel, Θ pLq is positive-definite. Remark 3.9. Lemmas 3.8 clearly shows the difference between our neural tangent kernel matrix Θ pLq in Definition 3.7 and the Gram matrix K pLq defined in Definition 5.1 in Du et al. <ref type="bibr" target="#b10">[11]</ref>. For any i, j P rns, by Lemma 3.8 we have</p><formula xml:id="formula_10">Θ pLq i,j " lim mÑ8 m ´1ř L l"1 x∇ W l f W p1q px i q, ∇ W l f W p1q px j qy.</formula><p>In contrast, the corresponding entry in K pLq is</p><formula xml:id="formula_11">K pLq i,j " lim mÑ8 m ´1x∇ W L´1 f W p1q px i q, ∇ W L´1 f W p1q px j qy.</formula><p>It can be seen that our definition of kernel matrix takes all layers into consideration, while Du et al. <ref type="bibr" target="#b10">[11]</ref> only considered the last hidden layer (i.e., second last layer). Moreover, it is clear that Θ pLq ľ K pLq . Since the smallest eigenvalue of the kernel matrix plays a key role in the analysis of optimization and generalization of over-parameterized neural networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4]</ref>, our neural tangent kernel matrix can potentially lead to better bounds than the Gram matrix studied in Du et al. <ref type="bibr" target="#b10">[11]</ref>.</p><p>Corollary 3.10. Let y " py 1 , . . . , y n q J and λ 0 " λ min pΘ pLq q. For any δ P p0, e ´1s, there exists r m ˚pδ, L, n, λ 0 q that only depends on δ, L, n and λ 0 such that if m ě r m ˚pδ, L, n, λ 0 q, then with probability at least 1 ´δ over the randomness of W p1q , the output of Algorithm 1 with step size η " κ ¨inf r yiyiě1 a r y J pΘ pLq q ´1 r y{pm ? nq for some small enough absolute constant κ satisfies</p><formula xml:id="formula_12">E " L 0´1 D p x Wq ‰ ď r O « L ¨inf r yiyiě1 c r y J pΘ pLq q ´1 r y n ff `O«c logp1{δq n ff ,</formula><p>where the expectation is taken over the uniform draw of x W from tW p1q , . . . , W pnq u. Remark 3.11. Corollary 3.10 gives an algorithm-dependent generalization error bound of overparameterized L-layer neural networks trained with SGD. It is worth noting that recently Arora et al. <ref type="bibr" target="#b3">[4]</ref> gives a generalization bound r O `ay J pH 8 q ´1y{n ˘for two-layer networks with fixed second layer weights, where H 8 is defined as</p><formula xml:id="formula_13">H 8</formula><p>i,j " xx i , x j y ¨Ew"Np0,Iq rσ 1 pw J x i qσ 1 pw J x j qs. Our result in Corollary 3.10 can be specialized to two-layer neural networks by choosing L " 2, and yields a bound r O `ay J pΘ p2q q ´1y{n ˘, where Θ p2q i,j " H 8 i,j `2 ¨Ew"Np0,Iq rσpw J x i qσpw J x j qs. Here the extra term 2 ¨Ew"Np0,Iq rσpw J x i qσpw J x j qs corresponds to the training of the second layer-it is the limit of 1 m x∇ W2 f W p1q px i q, ∇ W2 f W p1q px j qy. Since we have Θ p2q ľ H 8 , our bound is sharper than theirs. This comparison also shows that, our result generalizes the result in Arora et al. <ref type="bibr" target="#b3">[4]</ref> from two-layer, fixed second layer networks to deep networks with all parameters being trained. Remark 3.12. Corollary 3.10 is based on the asymptotic convergence result in Lemma 3.8, which does not show how wide the network need to be in order to make the Gram matrix close enough to the NTK matrix. Very recently, Arora et al. <ref type="bibr" target="#b2">[3]</ref> provided a non-asymptotic convergence result for the Gram matrix, and showed the equivalence between an infinitely wide network trained by gradient flow and a kernel regression predictor using neural tangent kernel, which suggests that the generalization of deep neural networks trained by gradient flow can potentially be measured by the corresponding NTK. Utilizing this non-asymptotic convergence result, one can potentially specify the detailed dependency of r m ˚pδ, L, n, λ 0 q on δ, L, n and λ 0 in Corollary 3.10.</p><p>Remark 3.13. Corollary 3.10 demonstrates that the generalization bound given by Theorem 3.3 does not increase with network width m, as long as m is large enough. Moreover, it provides a clear characterization of the classifiability of data. In fact, the a r y J pΘ pLq q ´1 r y factor in the generalization bound given in Corollary 3.10 is exactly the NTK-induced RKHS norm of the kernel regression classifier on data tpx i , r y i qu n i"1 . Therefore, if y " f ˚pxq for some f ˚p¨q with bounded norm in the NTK-induced reproducing kernel Hilbert space (RKHS), then over-parameterized neural networks trained with SGD generalize well. In Appendix E, we provide some numerical evaluation of the leading terms in the generalization bounds in Theorem 3.3 and Corollary 3.10 to demonstrate that they are very informative on real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Proof of Main Theory</head><p>In this section we provide the proof of Theorem 3.3 and Corollary 3.10, and explain the intuition behind the proof. For notational simplicity, for i P rns we denote L i pWq " L pxi,yiq pWq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Proof of Theorem 3.3</head><p>Before giving the proof of Theorem 3.3, we first introduce several lemmas. The following lemma states that near initialization, the neural network function is almost linear in terms of its weights. Lemma 4.1. There exists an absolute constant κ such that, with probability at least 1 ´OpnL 2 q ëxpr´Ωpmω 2{3 Lqs over the randomness of W p1q , for all i P rns and W, W 1 P BpW p1q , ωq with ω ď κL ´6rlogpmqs ´3{2 , it holds uniformly that</p><formula xml:id="formula_14">|f W 1 px i q ´fW px i q ´x∇f W px i q, W 1 ´Wy| ď O ´ω1{3 L 2 a m logpmq ¯¨ř L´1 l"1 }W 1 l ´Wl } 2 .</formula><p>Since the cross-entropy loss p¨q is convex, given Lemma 4.1, we can show in the following lemma that near initialization, L i pWq is also almost a convex function of W for any i P rns. Lemma 4.2. There exists an absolute constant κ such that, with probability at least 1 ´OpnL 2 q ëxpr´Ωpmω 2{3 Lqs over the randomness of W p1q , for any ą 0, i P rns and W, W 1 P BpW p1q , ωq with ω ď κL ´6m ´3{8 rlogpmqs ´3{2 3{4 , it holds uniformly that</p><formula xml:id="formula_15">L i pW 1 q ě L i pWq `x∇ W L i pWq, W 1 ´Wy ´ .</formula><p>The locally almost convex property of the loss function given by Lemma 4.2 implies that the dynamics of Algorithm 1 is similar to the dynamics of convex optimization. We can therefore derive a bound of the cumulative loss. The result is given in the following lemma. Lemma 4.3. For any , δ, R ą 0, there exists</p><formula xml:id="formula_16">m ˚p , δ, R, Lq " r O `polypR, Lq ˘¨ ´14 ¨logp1{δq</formula><p>such that if m ě m ˚p , δ, R, Lq, then with probability at least 1 ´δ over the randomness of W p1q , for any W ˚P BpW p1q , Rm ´1{2 q, Algorithm 1 with η " ν {pLmq, n " L 2 R 2 {p2ν 2 q for some small enough absolute constant ν has the following cumulative loss bound:</p><formula xml:id="formula_17">ř n i"1 L i pW piq q ď ř n i"1 L i pW ˚q `3n .</formula><p>We now finalize the proof by applying an online-to-batch conversion argument <ref type="bibr" target="#b8">[9]</ref>, and use Lemma 4.1 to relate the neural network function with a function in the NTRF function class. and therefore ty i ¨rp y i `fW p1q px i qsu ď n ´1{2 , i P rns. (4.4)</p><p>Denote F " m ´1{2 ¨pvecr∇f W p1q px 1 qs, . . . , vecr∇f W p1q px n qsq P R rmd`m`m 2 pL´2qsˆn . Note that entries of Θ pLq are all bounded by L. Therefore, the largest eigenvalue of Θ pLq is at most nL, and we have r y J pΘ pLq q ´1 r y ě n ´1L ´1}r y} 2 2 " L ´1. By Lemma 3.8 and standard matrix perturbation bound, there exists m ˚pδ, L, n, λ 0 q such that, if m ě m ˚pδ, L, n, λ 0 q, then with probability at least 1 ´δ, F J F is strictly positive-definite and }pF J Fq ´1 ´pΘ pLq q ´1} 2 ď inf r yiyiě1 r y J pΘ pLq q ´1 r y{n.</p><p>(4.5)</p><p>Let F " PΛQ J be the singular value decomposition of F, where P P R mˆn , Q P R nˆn have orthogonal columns, and Λ P R nˆn is a diagonal matrix. Let w vec " PΛ ´1Q J p y, then we have Therefore by (4.5) and the fact that }p y} 2 2 " B 2 n, we have }w vec } 2 2 " p y J rpF J Fq ´1 ´pΘ pLq q ´1sp y `p y J pΘ pLq q ´1 p y ď B 2 ¨n ¨}pF J Fq ´1 ´pΘ pLq q ´1} 2 `B2 ¨r y J pΘ pLq q ´1 r y ď 2B</p><formula xml:id="formula_18">F</formula><p>2 ¨r y J pΘ pLq q ´1 r y.</p><p>Let W P W be the parameter collection reshaped from m ´1{2 w vec . Then clearly }W l } F ď m ´1{2 }w vec } 2 ď r O ´br y J pΘ pLq q ´1 r y ¨m´1{2 ¯, and therefore W P B `0, O `ar y J pΘ pLq q ´1 r y ¨m´1{2 ˘˘. Moreover, by (4.6), we have p y i " x∇ W f W p1q px i q, Wy. Plugging this into (4.4) then gives y i ¨"f W p1q px i q `x∇ W f W p1q px i q, Wy ‰( ď n ´1{2 .</p><p>Since p f p¨q " f W p1q p¨q `x∇ W f W p1q p¨q, Wy P F `Wp1q , r O `ar y J pΘ pLq q ´1 r y ˘˘, applying Theorem 3.3 and taking infimum over r y completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this paper we provide an expected 0-1 error bound for wide and deep ReLU networks trained with SGD. This generalization error bound is measured by the NTRF function class. The connection to the neural tangent kernel function studied in Jacot et al. <ref type="bibr" target="#b17">[18]</ref> is also discussed. Our result covers a series of recent generalization bounds for wide enough neural networks, and provides better bounds.</p><p>An important future work is to improve the over-parameterization condition in Theorem 3.3 and Corollary 3.10. Other future directions include proving sample complexity lower bounds in the over-parameterized regime, implementing the results in Jain et al. <ref type="bibr" target="#b18">[19]</ref> to obtain last iterate bound of SGD, and establishing uniform convergence based generalization bounds for over-parameterized neural networks with methods developped in Bartlett et al. <ref type="bibr" target="#b5">[6]</ref>, Neyshabur et al. <ref type="bibr" target="#b26">[27]</ref>, Long and Sedghi <ref type="bibr" target="#b25">[26]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>J w vec " pQΛP J qpPΛ ´1Q J</figDesc><table><row><cell></cell><cell></cell><cell cols="2">p yq " p y.</cell><cell>(4.6)</cell></row><row><cell cols="2">Moreover, by direct calculation we have</cell><cell></cell><cell></cell></row><row><cell>}w vec } 2 2 " }PΛ ´1Q J</cell><cell>p y} 2 2 " }Λ ´1Q J</cell><cell>p y} 2 2 " p y J QΛ ´2Q J</cell><cell>p y " p y J pF J Fq ´1 p y.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Since random feature models and kernel methods are highly related<ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>, we group them into the same category. More details are discussed in Section 3.2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>Normalizing weights to the same scale is necessary for a proper comparison. See Appendix A.2 for details.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>The original result by Jacot et al.<ref type="bibr" target="#b17">[18]</ref> requires that the widths of different layers go to infinity sequentially. Their result was later improved by Yang<ref type="bibr" target="#b35">[36]</ref> such that the widths of different layers can go to infinity simultaneously.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank Peter Bartlett for a valuable discussion, and Simon S. Du for pointing out a related work <ref type="bibr" target="#b2">[3]</ref>. We also thank the anonymous reviewers and area chair for their helpful comments. This research was sponsored in part by the National Science Foundation CAREER Award IIS-1906169, IIS-1903202, and Salesforce Deep Learning Research Award. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Proof of Theorem 3.3. For i P rns, let L 0´1 i pW piq q " 1 y i ¨fW piq px i q ă 0 ( . Since cross-entropy loss satisfies 1tz ď 0u ď 4 pzq, we have L 0´1 i pW piq q ď 4L i pW piq q. Therefore, setting " LR{ ? 2νn in Lemma 4.3 gives that, if η is set as a ν{2R{pm ? nq, then with probability at least 1 ´δ,</p><p>Note that for any i P rns, W piq only depends on px 1 , y 1 q, . . . , px i´1 , y i´1 q and is independent of px i , y i q. Therefore by Proposition 1 in Cesa-Bianchi et al. <ref type="bibr" target="#b8">[9]</ref>, with probability at least 1 ´δ we have</p><p>By definition, we have</p><p>. Therefore combining (4.1) and (4.2) and applying union bound, we obtain that with probability at least 1 ´2δ,</p><p>for all W ˚P BpW p1q , Rm ´1{2 q. We now compare the neural network function f W ˚px i q with the function F W p1q ,W ˚px i q :" f W p1q px i q `x∇f W p1q px i q, W ˚´W p1q y P FpW p1q , Rq. We have</p><p>where the first inequality is by the 1-Lipschitz continuity of p¨q and Lemma 4.1, the second inequality is by W ˚P BpW p1q , Rm ´1{2 q, and last inequality holds as long as m ě C 1 R 2 L 12 rlogpmqs 3 n 3 for some large enough absolute constant C 1 . Plugging the inequality above into (4.3) gives</p><p>Taking infimum over W ˚P BpW p1q , Rm ´1{2 q and rescaling δ finishes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Proof of Corollary 3.10</head><p>In this subsection we prove Corollary 3.10. The following lemma shows that at initialization, with high probability, the neural network function value at all the training inputs are of order r Op1q.</p><p>Lemma 4.4. For any δ ą 0, if m ě KL logpnL{δq for a large enough absolute constant K, then with probability at least 1 ´δ, |f W p1q px i q| ď Op a logpn{δqq for all i P rns.</p><p>We now present the proof of Corollary 3.10. The idea is to construct suitable target values p y 1 , . . . , p y n , and then bound the norm of the solution of the linear equations p y i " x∇f W p1q px i q, Wy, i P rns. In specific, for any r y with r y i y i ě 1, we examine the minimum distance solution to W p1q that fit the data tpx i , r y i qu n i"1 well and use it to construct a specific function in F `Wp1q , r O `ar y J pΘ pLq q ´1 r y ˘˘.</p><p>Proof of Corollary 3.10. Set B " logt1{rexppn ´1{2 q ´1su " Oplogpnqq, then for cross-entropy loss we have pzq ď n ´1{2 for z ě B. Moreover, let B 1 " max iPrns |f W p1q px i q|. Then by Lemma 4.4, with probability at least 1 ´δ, B 1 ď Op a logpn{δqq for all i P rns. For any r y with r y i y i ě 1, let B " B `B1 and p y " B ¨r y, then it holds that for any i P rns, y i ¨rp y i `fW p1q px i qs " y i ¨p y i `yi ¨fW p1q px i q ě B `B1 ´B1 ě B,</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning and generalization in overparameterized neural networks, going beyond two layers</title>
		<author>
			<persName><forename type="first">Allen-Zhu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1811.04918</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A convergence theory for deep learning via over-parameterization</title>
		<author>
			<persName><forename type="first">Allen-Zhu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03962</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11955</idno>
		<title level="m">On exact computation with an infinitely wide neural net</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08584</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Stronger generalization bounds for deep nets via a compression approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05296</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spectrally-normalized margin bounds for neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Telgarsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Brutzkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10174</idno>
		<title level="m">Sgd learns over-parameterized networks that provably generalize on linearly separable data</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A generalization theory of gradient descent for learning overparameterized deep relu networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01384</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the generalization ability of on-line learning algorithms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Conconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gentile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="2050" to="2057" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sgd learns the conjugate kernel class of the network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daniely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03804</idno>
		<title level="m">Gradient descent finds global minima of deep neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02054</idno>
		<title level="m">Gradient descent provably optimizes over-parameterized neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.11008</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A comparative analysis of the optimization and generalization property of two-layer neural network and random feature models under gradient descent dynamics</title>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Al</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04326</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Golowich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.06541</idno>
		<title level="m">Size-independent sample complexity of neural networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Jacot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hongler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07572</idno>
		<title level="m">Neural tangent kernel: Convergence and generalization in neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Making the last iterate of sgd information theoretically optimal</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nagaraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Netrapalli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12443</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">(not) bounding the true error</title>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<author>
			<persName><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Wide neural networks of any depth evolve as linear models under gradient descent</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06720</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haupt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05159</idno>
		<title level="m">On tighter generalization bound for deep neural networks: Cnns, resnets, and beyond</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01204</idno>
		<title level="m">Learning overparameterized neural networks via stochastic gradient descent on structured data</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Size-free generalization bounds for convolutional neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sedghi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12600</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09564</idno>
		<title level="m">A pacbayesian approach to spectrally-normalized margin bounds for neural networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The role of over-parametrization in generalization of neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Norm-based capacity control in neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Towards moderate overparameterization: global convergence guarantees for training shallow neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Oymak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04674</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Understanding machine learning: From theory to algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben-David</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05369</idno>
		<title level="m">On the margin theory of feedforward neural networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04760</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">On the power and limitations of random features for understanding neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yehudai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00687</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08888</idno>
		<title level="m">Stochastic gradient descent optimizes over-parameterized deep relu networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
