<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SIMR: Single Instruction Multiple Request Processing for Energy-Efficient Data Center Microservices</title>
				<funder ref="#_2jNFMhs">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mahmoud</forename><surname>Khairy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of ECE Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ahmad</forename><surname>Alawneh</surname></persName>
							<email>aalawneh@purdue.edu</email>
							<affiliation key="aff1">
								<orgName type="department">School of ECE Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aaron</forename><surname>Barnes</surname></persName>
							<email>barnes88@purdue.edu</email>
							<affiliation key="aff2">
								<orgName type="department">School of ECE Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Timothy</forename><forename type="middle">G</forename><surname>Rogers</surname></persName>
							<email>timrogers@purdue.edu</email>
							<affiliation key="aff3">
								<orgName type="department">School of ECE Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shardn</forename><surname>Shard1</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">URLShorten TextSearch</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shard1</forename><surname>Imagesearch</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">URLShorten TextSearch</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Shardn</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">URLShorten TextSearch</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SIMR: Single Instruction Multiple Request Processing for Energy-Efficient Data Center Microservices</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>SIMT</term>
					<term>Data Center</term>
					<term>Microservices</term>
					<term>GPU Microservice { } SPMD SIMR-Aware HTTP Server RPU HW (Latency-Optimized SIMT Engine) Batch Similar Requests Client Requests</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contemporary data center servers process thousands of similar, independent requests per minute. In the interest of programmer productivity and ease of scaling, workloads in data centers have shifted from single monolithic processes toward a micro and nanoservice software architecture. As a result, single servers are now packed with many threads executing the same, relatively small task on different data.</p><p>State-of-the-art data centers run these microservices on multi-core CPUs. However, the flexibility offered by traditional CPUs comes at an energy-efficiency cost. The Multiple Instruction Multiple Data execution model misses opportunities to aggregate the similarity in contemporary microservices. We observe that the Single Instruction Multiple Thread execution model, employed by GPUs, provides better thread scaling and has the potential to reduce frontend and memory system energy consumption. However, contemporary GPUs are ill-suited for the latency-sensitive microservice space.</p><p>To exploit the similarity in contemporary microservices, while maintaining acceptable latency, we propose the Request Processing Unit (RPU). The RPU combines elements of outof-order CPUs with lockstep thread aggregation mechanisms found in GPUs to execute microservices in a Single Instruction Multiple Request (SIMR) fashion. To complement the RPU, we also propose a SIMR-aware software stack that uses novel mechanisms to batch requests based on their predicted controlflow, split batches based on predicted latency divergence and map per-request memory allocations to maximize coalescing opportunities. Our resulting RPU system processes 5.7? more requests/joule than multi-core CPUs, while increasing single thread latency by only 1.44?.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The growth of hyperscale data centers has steadily increased in the last decade, and is expected to continue in the coming era of Artificial Intelligence and the Internet of Things <ref type="bibr" target="#b0">[1]</ref>. However, the slowing of Moore's Law <ref type="bibr" target="#b1">[2]</ref> has resulted in energy <ref type="bibr" target="#b2">[3]</ref>, environmental <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> and supply chain <ref type="bibr" target="#b5">[6]</ref> issues that has lead data centers to embrace custom hardware/software solutions <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>While improving Deep Learning (DL) inference has received significant attention <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, general purpose compute units are still the main driver of a data center's total cost of ownership (TCO). CPUs consume 60% of the data center power budget <ref type="bibr" target="#b9">[10]</ref>, half of which comes from the pipeline's frontend (i.e. fetch, decode, branch prediction (BP), and Outof-Order (OoO) structures) <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b14">[15]</ref>. Therefore; 30% of the data-center's total energy is spent on CPU instruction supply.</p><p>Coupled with the hardware efficiency crisis is an increased desire for programmer productivity, flexible scalability and nimble software updates that has lead to the rise of software microservices. Monolithic server software has been largely replaced with a collection of micro and nanoservices that interact via the network <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>. Compared to monolithic services, microservices spend much more time in network processing <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, have a smaller instruction and data footprint <ref type="bibr" target="#b16">[17]</ref>, and can suffer from excessive context switching due to frequent network blocking <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>.</p><p>To meet both latency and throughput demands, contemporary data centers typically run microservices on multicore, OoO CPUs with and without Simultaneous Multithreading (SMT). Previous academic and industrial work <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b27">[28]</ref> has shown that current CPUs are inefficient in the data center as many on-chip resources are underutilized or ineffective. To make better use of these resources, on-chip throughput is increased <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b28">[29]</ref> by adding more cores and raising the SMT degree <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b34">[35]</ref>. Figure <ref type="figure" target="#fig_0">1</ref> visualizes the energy-efficiency and single thread latency of different processor design points, logically separated by their execution model. On the low-latency end are OoO Multiple Instruction Multiple Data (MIMD) CPUs with a low SMT-degree. Different CPU designs trade-off single thread latency for energy-efficiency by increasing the SMT-degree and moving from OoO to in-order execution.</p><p>On the high-efficiency end are in-order Single Instruction Multiple Thread (SIMT) GPUs that support thousands of scalar threads per core. Fundamentally, GPU cores are designed to support workloads where single-threaded performance can be sacrificed for multi-threaded throughput. However, we argue that the energy-efficient nature of the GPU's execution model and scalable memory system can be leveraged by low-latency OoO cores, provided the workload performs efficiently under SIMT execution. SIMT machines aggregate scalar threads into vector-like instructions for execution (i.e. a warp). To achieve high energy-efficiency, the threads aggregated into each warp must traverse similar control-flow paths, otherwise lanes in the vector units must be masked off (decreasing SIMT-efficiency) and the benefits of aggregation disappear. We make the observation that contemporary microservices exhibit a SIMT-friendly execution pattern. Data center nodes running the same microservice across multiple requests create a natural batching opportunity for SIMT hardware, if service latencies can be met. Contemporary GPUs are ill-suited for this task, as they forego single threaded optimizations (OoO, speculative execution, etc.) in favor of excessive multithreading. Prior work on directly using GPU hardware to execute data center applications <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> reports up to 6000? <ref type="bibr" target="#b36">[37]</ref> higher latency than the CPU. Furthermore, accessing I/O resources on GPUs requires CPU co-ordination <ref type="bibr" target="#b36">[37]</ref>- <ref type="bibr" target="#b40">[41]</ref> and GPUs do not support the rich set of programming languages represented in contemporary microservices <ref type="bibr" target="#b16">[17]</ref>, hindering programmer productivity.</p><p>SIMT-on-SIMD compilers, like Intel ISPC <ref type="bibr" target="#b41">[42]</ref>, provide a potential path to run SIMT-friendly microservices on CPU SIMD units. This method has the potential to achieve high energy efficiency while leveraging some of the CPU pipeline's latency optimizations by assigning each thread to a SIMD lane. However, this approach has several drawbacks. First, each microservice thread requires more register file and cache capacity than work typically assigned to a single fine-grained SIMD lane, negatively impacting service latency. Second, this approach transforms conditional scalar branches into predicates, limiting the benefit of the CPU's branch predictor. Finally, this method requires a complete recompilation of the microservice code and new ISA extensions for the scalar instructions with no 1:1 mapping in the vector ISA (see Section VI-A for further details).</p><p>To this end, we propose replacing the CPUs in contemporary data centers with a general-purpose architecture customized for microservices: the Request Processing Unit (RPU). The RPU improves the energy-efficiency of contemporary CPUs by leveraging the frontend and memory system design of SIMT processors, while meeting the single thread latency and programmability requirements of microservices by maintaining OoO execution and support for the CPU's ISA and software stack. Under ideal SIMT-efficiency conditions, the RPU improves energy-efficiency in three ways. First, the 30% of total data center energy spent on CPU instruction supply can be reduced by the width of the SIMT unit (up to 32 in our proposal). Second, SIMT pipelines make use of vector register files and SIMD execution units, saving area and energy versus a MIMD pipeline of equivalent throughput. Finally, SIMT memory coalescing aggregates access among threads in the same warp, producing up to 32? fewer memory system accesses. Although the cache hit rate for SMT CPUs may be high when concurrent threads access similar code/data, bandwidth and energy demands on both cache and OoO structures will be higher than an OoO SIMT core where threads are aggregated.</p><p>Moving from a scalar MIMD pipeline to a vector-like SIMT pipeline has a latency cost. To meet timing constraints, the clock and/or pipeline depth of the SIMT execution units must be longer than that of a MIMD core with fewer threads. However, the SIMT core's memory coalescing capabilities help offset this increase in latency by reducing the bandwidth demand on the memory system, decreasing the queueing delay experienced by individual threads. In our evaluation, we faithfully model the RPU's increased pipeline latency (Section IV) and demonstrate that despite a pessimistic assumption that the ALU pipeline is 4? deeper in the RPU <ref type="bibr" target="#b42">[43]</ref> and that L1 hit latency is &gt; 2? higher, the average service latency is only 44% higher than a MIMD CPU chip.</p><p>A well designed software system that is aware of the hardware's aggregating nature and can balance SIMT efficiency with end-to-end request latency is critical to the RPU's success. To meet these demands, we co-design the RPU with a SIMR-aware software system pictured in Figure <ref type="figure" target="#fig_1">2</ref>. The RPU executes a general-purpose CPU ISA, supporting all the same functionality as a typical CPU core, but aggregates the use of all its frontend structures over multiple threads.  between concurrent threads: In the microservice design paradigm, a monolithic logic tier is broken down into smaller, software-friendly microservices where each is responsible for a small piece of the system. Figure <ref type="figure">3</ref> depicts a simple microservice graph for a social network service similar to <ref type="bibr" target="#b16">[17]</ref>. Each node in the data center is tasked with many threads all running the same microservice. When monolithic services are disaggregated, divergent controlflow paths are often split into different microservices. That is, if/else conditionals in the monolithic service are split into one service for if and one service for else. Such an organization makes it much more common that concurrent microservices on the same machine traverse exactly the same control-flow path before sending their request to the next microservice. In addition, the per-thread data cache requirement is significantly reduced, as each thread fundamentally does less work. Figure <ref type="figure" target="#fig_2">4</ref> shows the SIMT control flow efficiency of modern microservices, assuming they are batched on arrival into groups of 32 threads. On average, we are able to achieve 68% SIMT efficiency when applying naive batching. In Section III-B1, we propose optimized batching techniques, which bring efficiency to 92%. Key Observation #3: Modern data centers already rely on request batching: In order to enable SIMT execution, requests have to be batched and executed together. Batching can introduce additional latency, however, batching is already heavily used in data centers and employed in at least one microservice on each network path. For example:</p><p>(1) deep learning inference batches requests to increase accelerator compute throughput <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b45">[46]</ref>, (2) key-value store applications, like memcached <ref type="bibr" target="#b46">[47]</ref>, batch to amortize the network overhead, (3) streaming graph analytics <ref type="bibr" target="#b47">[48]</ref> batch to alleviate lock contention, and (4) dynamic power management <ref type="bibr" target="#b48">[49]</ref>- <ref type="bibr" target="#b50">[51]</ref> applies batching to save power. Therefore, if we apply batching to exploit request similarity, the batching overhead is amortized, as there are already microservices on the same path that employ batching.</p><p>Key Observation #4: In the data center, all throughput gains must be made under a tight latency constraint: The trade-off between brawny and wimpy cores in the data center is a well-studied problem <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>. However, the use of wimpy cores has not been widely adopted by data center providers <ref type="bibr" target="#b51">[52]</ref>. Under the same power budget, wimpy cores can increase throughput <ref type="bibr" target="#b53">[54]</ref>, but have a higher task execution latency than brawny cores. This increase in total request latency makes them ill-suited for the data center's QoS-sensitive workloads. Prior work has argued that energy-optimized systems in the data center must ensure that their single-thread latency is no worse than 2? that of brawny cores <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b51">[52]</ref>. The same argument applies for GPUs, that have high energy-efficiency, but have unacceptably high service latency, 6000? worse than CPUs for SPEC-Web <ref type="bibr" target="#b36">[37]</ref>, and 10? worse for memcached <ref type="bibr" target="#b35">[36]</ref>.</p><p>Key Observation #5: Future data center nodes need to increase their on-chip thread count: Previous academic and industrial work <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b27">[28]</ref> has shown that current CPUs are inefficient when executing data center workloads as there are many underutilized resources. They suggest that an increase in the number of threads on-chip is necessary to better use these resources <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Figure <ref type="figure" target="#fig_3">5</ref> depicts the off-chip bandwidth and thread count per socket scaling in the future. CPU vendors typically ensure 2 GB/sec of DRAM BW per thread. If this is the case, we need to provide up to 256 threads per socket with DDR5 <ref type="bibr" target="#b54">[55]</ref>- <ref type="bibr" target="#b57">[58]</ref> and 512 threads with DDR6 <ref type="bibr" target="#b58">[59]</ref> and HBM <ref type="bibr" target="#b59">[60]</ref> to utilize the available off-chip BW. The industry standard to increase on-chip throughput is by adding more chiplets <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, cores <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> and increasing the SMT degree <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>; however, we argue that introducing SIMT to OoO CPU cores will provide a more energy-efficient mechanism to scale onchip throughput.</p><p>Given these five observations, we design our RPU hardware and software system to exploit the similarity among requests in microservices through intelligent batching. The RPU's OoO SIMT frontend is able to meet the latency constraints of contemporary services, while improving upon the energy-efficiency and thread-density of modern CPUs. In the next section, we discuss our system's design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SIMR SYSTEM</head><p>Figure <ref type="figure" target="#fig_1">2</ref> presents a high level overview of our SIMR system. Groups of independent Remote Procedure Call (RPC) or HTTP requests are received by our SIMR-Aware server. The server ( I in Figure <ref type="figure" target="#fig_1">2</ref>) groups requests into a batch based on each request's Application Program Interface (API) similarity and argument size. The batches in the RPU are analogous to warps in a GPU. Our batch size is tunable based on resource contention, desired QoS, arrival rate and system configuration (Section III-B explores these parameters). Then, the server launches a service request to the RPU driver and hardware. The RPU hardware ( II ) executes the batch in lock-step fashion over the OoO SIMT pipeline (Section III-A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. RPU Hardware</head><p>Figure <ref type="figure">6</ref> presents a detailed overview of our RPU hardware. Our RPU chip contains multiple RPU cores, and a few CPU cores. The role of the CPU cores is to run the OS process, HTTP server, and RPU driver while the RPU cores run the microservices requests' workload. Each RPU core is similar to a brawny OoO CPU core, except hardware is added (highlighted in green) to perform multithreading in a SIMT fashion. The design philosophy of the RPU is that the area/power savings gained by SIMT execution and amortizing frontend (e.g., OoO control logic, branch predictor, fetch&amp;decode), are used to increase the thread context and throughput at the backend ( 1 in Figure <ref type="figure">6</ref>, e.g., scalar/SIMD physical register file (PRF), execution units, and cache resources); thus we still maintain the same area/power budget and improve overall throughput/watt. It is worth noting that the RPU thread has the same coarse granularity as the CPU thread, such that the RPU thread has a similar thread context of integer and SIMD register file space. In addition, all execution units, including the SIMD engines, are increased by the number of SIMT lanes.</p><p>OoO SIMT Pipeline: When merging the RPU's SIMT pipeline with speculative, OoO execution, we assume the following design principles. First, the active mask is propagated with the instruction throughout the entire pipeline <ref type="bibr" target="#b1">( 2 )</ref>. Therefore, register alias table (RAT), instruction buffer and reorder buffer entries are extended to include the active mask (AM). Second, to handle register renaming of the same variable used in different branches, a micro-op is inserted to merge registers from the different paths <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>. Third, the branch predictor operates at the batch (or warp) granularity, i.e., only one prediction is generated for all the threads in a batch. When updating the branch history, we apply a majority voting policy of branch results ( 3 ) to optimize for the most common control flow. The instructions from mispredicted threads are flushed at the commit stage and the corresponding PCs and active mask are updated accordingly. Adding majority voting circuitry before branch prediction increases branch execution latency and energy. We account for these overheads in our evaluation, detailed in Section IV.</p><p>Control Flow Divergence Handling: To address control flow divergence, a hardware SIMT convergence optimizer ( 4 ) is employed to serialize divergent paths <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>. The optimizer relies on stack-less reconvergence with a MinPC heuristic policy <ref type="bibr" target="#b63">[64]</ref>- <ref type="bibr" target="#b65">[66]</ref>. In this scheme, each thread has its The selected PC is given to the basic block whose entry point has the lowest address. The MinPC heuristic relies on the assumption that reconvergence points are found at the lowest point of the code they dominate <ref type="bibr" target="#b65">[66]</ref>. For function calls we assume a MinSP policy <ref type="bibr" target="#b63">[64]</ref> which gives priority to the deepest function call or we set a convergence barrier at the instruction following the procedure call. Figure <ref type="figure">7</ref> contains a simple example of MinPC policy analysis and shows how PC selection interacts with divergent control flow. When threads execute divergent control flows, the paths are serialized, and each path is associated with the current PC and corresponding active mask. This control flow divergence serialization overhead is minimized by intelligent batching techniques which we describe in Section III-B1. The MinPC strategy has been found to achieve 100% accuracy <ref type="bibr" target="#b63">[64]</ref> to determine correct reconvergence points for GPGPU workloads and up to 94% for CPU SPECint workloads <ref type="bibr" target="#b64">[65]</ref>. Even in the rare cases where the policy misses the correct reconvergence points, it still reconverges not too far behind and achieves overall good SIMT control efficiency (Section III-B1). The stack-less reconvergence approach is transparent to the compiler and ISA, and can handle indirect branches without profiling or virtual ISA support. This differs from stack-based approaches that are widely used in modern GPUs <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b66">[67]</ref> which require compiler-assisted static analysis to determine correct reconvergence points and ISA support to update the hardware stack <ref type="bibr" target="#b67">[68]</ref> and list all the targets of indirect branches <ref type="bibr" target="#b68">[69]</ref>.</p><p>Running threads in lock-step execution and serializing di-vergent paths can induce deadlocks when programs employ inter-thread synchronization <ref type="bibr" target="#b69">[70]</ref>- <ref type="bibr" target="#b71">[72]</ref>. There have been several proposals to alleviate the SIMT-induced deadlock issue on GPUs. All of the proposed solutions rely on multipath execution to allow control flow paths that are not at the top of the SIMT stack to make forward progress. In the RPU, when an active thread's PC has not been updated for k cycles and there have been at least b atomic instructions decoded within the k-cycle window (an indication for spin locking by other selected threads), then the waiting thread is prioritized and we switch to the other path for t cycles. Otherwise, the default MinSP-PC is applied. Multi-path interleaving requires partitioning the return address stack (RAS) in the branch prediction unit to support multiple control flows <ref type="bibr" target="#b72">[73]</ref>.</p><p>Another issue is that the MinSP-PC selection policy can increase the branch prediction latency, hindering pipeline utilization. To mitigate this issue, we can leverage techniques proposed for complex, multi-cycle branch history structures, such as hierarchical or ahead pipelining prediction <ref type="bibr" target="#b73">[74]</ref>.</p><p>Sub-batch Interleaving: Previous work <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref> show that data center workloads tend to exhibit low IPC per thread (a range of 0.5-1 out of 5, and up to 1.75 IPC when SMT enabled <ref type="bibr" target="#b19">[20]</ref>), due to long memory latency at the back-end and instruction fetch misses at the frontend <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b25">[26]</ref>. To increase our execution unit utilization and ensure a reasonable backend execution area, we implement sub-batch interleaving <ref type="bibr" target="#b74">[75]</ref> as depicted in Figure <ref type="figure" target="#fig_4">8a</ref>. By decreasing the number of SIMT lanes (m) per execution unit to be a fraction of batch size (n), we issue threads over multiple cycles. Sub-batch interleaving along with OoO scheduling can hide nanosecond-scale latencies efficiently, increasing IPC utilization. Another advantage of sub-batch interleaving is that we can skip issue slots of non-active threads to mitigate control divergence penalties and support smaller batches of execution <ref type="bibr" target="#b74">[75]</ref>. To hide longer microsecond-scale latencies <ref type="bibr" target="#b75">[76]</ref>- <ref type="bibr" target="#b77">[78]</ref>, multiple batches can be interleaved via hardware batch scheduling in a coarse-grain, round-robin manner with zero-overhead context switching. However, studying multi-batch scheduling to hide microsecond-scale latency is beyond the scope of this work. Memory Coalescing: To improve memory efficiency, a low-latency memory coalescing unit (MCU) is placed before the load and store queues <ref type="bibr" target="#b4">( 5 )</ref>. As described in Figure <ref type="figure" target="#fig_4">8b</ref>, the MCU is designed to coalesce memory accesses to the same cache line from threads in a single batch, making better use of cache throughput and avoiding cache access serialization. The MCU filters out accesses to shared interrequest data structures that might exist in the heap or data segments <ref type="bibr" target="#b24">[25]</ref>. To balance the need for a low cache hit latency with avoiding divergent access serialization, the MCU only detects the two most common memory coalescing scenarios: when all threads access the same word, or when threads access consecutive words from the same cache line. This is unlike the complex sub-batch sharing in GPU data coalescing <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b78">[79]</ref> which increases memory access latency to detect more complex locality patterns <ref type="bibr" target="#b79">[80]</ref>.</p><p>LD/ST Unit: In our MCU, if neither simple pattern is detected, the number of accesses generated will equal the number of active SIMT lanes. All accesses from the same instruction will allocate one row in the load or store queue <ref type="bibr" target="#b5">( 6 )</ref>, sharing the same PC and age fields/logic, and thus amortizing the memory scheduling and dependence prediction <ref type="bibr" target="#b80">[81]</ref> overhead. Figure <ref type="figure" target="#fig_5">9</ref> depicts the LD/ST unit (LSU) structure in more detail. The entries of the RPU's LD/ST queues are expanded such that each row can contain as many addresses as there are SIMT lanes. Further, we assign an independent content-address memory (CAM) for each lane to account for in-parallel store-to-load forwarding. For coalesced accesses, only one slot in the entry (entry#0) is allocated and broadcast for CAM comparisons. To save area, we do not preserve the loaded value in the load queue; instead, we write the return value to the register file directly and set the corresponding valid bit. Therefore, the load instruction is completed, and the tag is broadcast when all the slots in the entry are valid and completed.</p><p>Cache and TLB: To serve the throughput needs of many threads while achieving scalable area and energy consumption, the RPU uses a banked L1 cache. The load/store queues are connected to the L1 cache banks via a crossbar <ref type="bibr" target="#b6">( 7 )</ref>. To ensure TLB throughput can match the L1 throughput,  each L1 data bank is associated with a TLB bank. Since the interleaving of data over cache banks is at a smaller granularity than the page size, TLB entries may be duplicated over multiple banks. This duplication overhead reduces the effective capacity of the DTLBs, but allows for high throughput translation on cache+TLB hits. As a result of the duplication, all TLB banks are checked on the per-entry TLB invalidation instructions <ref type="bibr" target="#b81">[82]</ref>. Sections III-B3 and III-B4 discuss how we alleviate contention to preserve intra-thread locality and achieve acceptable latency via batch size tuning and SIMR-aware memory allocation.</p><p>Weak Consistency Model+NMCA: To exploit the fact that requests rarely communicate and exhibit low coherence, read-write sharing or locking <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, as well as extensive use of eventual consistency in data center <ref type="bibr" target="#b82">[83]</ref>, we design the memory system to be similar to a GPU, i.e., weak memory consistency with non-multi-copy-atomicity (NMCA) <ref type="foot" target="#foot_0">1</ref> . RPU implements a simple, relaxed coherence protocol with notransient states or invalidation acknowledgments, similar to the ones proposed in HMG <ref type="bibr" target="#b87">[88]</ref> and QuickRelease <ref type="bibr" target="#b88">[89]</ref>. That is, cache coherence and memory ordering are only guaranteed at synchronization points (i.e., barriers, fences, acquire/release), and all atomic operations are moved to the shared L3 cache. Therefore, we no longer have coreto-core coherence communication, and thus we replace the commonly-used mesh network in CPUs with a higherbisection-bandwidth, lower-latency core-to-memory crossbar <ref type="bibr" target="#b7">( 8 )</ref>. Furthermore, NMCA permits threads on the same lane to share the store queue, reducing the complexity of having a separate store queue per thread <ref type="bibr" target="#b84">[85]</ref>. This relaxed memory model allows our design to scale the number of threads efficiently, improving thread density by an order of magnitude.</p><p>1) CPU vs GPU vs RPU: Table <ref type="table" target="#tab_6">II</ref> lists the key architectural differences between CPUs, GPUs and our RPU. The RPU takes advantage of the latency-optimizations and Request similarity <ref type="bibr" target="#b36">[37]</ref> &amp; high frontend power consumption <ref type="bibr" target="#b10">[11]</ref> SIMT execution to amortize frontend overhead Inter-request data sharing <ref type="bibr" target="#b24">[25]</ref> Memory coalescing and an increase in the number of threads sharing private caches Low coherence/locks <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> and eventual consistency <ref type="bibr" target="#b82">[83]</ref> Weak memory ordering, relaxed coherence with non-memory-copy-atomicity &amp; higher bandwidth core-to-memory interconnect Low IPC due to frequent frontend stalls and memory latency <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b25">[26]</ref> Multi-thread/sub-batch interleaving DRAM &amp; L3 BW are underutilized, data prefetchers are ineffective <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref> High thread level parallelism (TLP) to fully utilize BW Microservice/nanoservice have a smaller cache footprint <ref type="bibr" target="#b16">[17]</ref> High TLP and decrease L1&amp;L2 cache capacity/thread programmability of the CPU while exploiting the SIMT efficiency and memory model scalability of the GPU. Finally, Table <ref type="table" target="#tab_7">III</ref> summarizes a set of data center characteristics that create inefficiencies in CPU designs and how the RPU mitigates them.</p><p>2) An Examination of SMT vs SIMT Energy Efficiency: This subsection examines why the RPU's SIMT execution is able to outperform MIMD SMT hardware for data center workloads. Equation 1 presents an analytical computation of the RPU's energy efficiency (EE) gain over the CPU. In Equation <ref type="formula" target="#formula_0">1</ref>, n is the RPU batch size, eff is average RPU SIMT efficiency, and r is the ratio of memory requests that exhibit inter-thread locality within a single SIMR batch. CPU energy is divided into frontend+OoO overhead (including fetch, decode, branch prediction, OoO control logic and load/store queue), execution (including register reading/writing and instruction execution), memory system (including private and L3 caches), and static energies.</p><formula xml:id="formula_0">EE = CP U Energy RP U Energy = Exec Energy + M em Energy + F E OoO Energy Exec Energy + (1 -r)M em Energy + 1 n * ef f * +Static Energy [r * M em Energy + F E OoO Energy + Static Energy ] + SIM T Overhead<label>(1)</label></formula><p>In Equation <ref type="formula" target="#formula_0">1</ref>, the RPU's energy consumption in frontend and OoO overheads are amortized by running threads in lock step; hence the energy consumed for instruction fetch, decode, branch prediction, control logic and CAM tag accesses <ref type="bibr" target="#b89">[90]</ref>   register file control, and load/store queue are all consumed only once for all the threads in a single batch (see Figures <ref type="figure">6</ref> and<ref type="figure" target="#fig_5">9</ref>). In scalar CPU designs, the frontend and OoO overheads have to be consumed for each thread. Even with SMT, the entire CPU pipeline is partitioned among the simultaneous threads. Threads on the same core are executed independently <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b90">[91]</ref>, which fails to exploit thread similarity and increases single thread latency.</p><p>Coalesced memory accesses are also amortized in the RPU by generating and sending only one access for the batch to the memory system. While private cache hits and MSHR merges can filter out some of these coalesced accesses in a SMT design, the programmer must guarantee that simultaneous threads are launched and progress together in order to capture this inter-thread data locality <ref type="bibr" target="#b91">[92]</ref>, <ref type="bibr" target="#b92">[93]</ref> and still must pay the energy cost of multiple cache accesses. Furthermore, since SIMT can execute more threads/core given the same area constraints, the reach of its locality optimizations is wider.</p><p>The final metric SIMT execution amortizes is static energy. The RPU improves throughput/area and has a smaller SRAM budget/thread compared to an SMT core. The RPU introduces an energy overhead (SIM T Overhead in Equation <ref type="formula" target="#formula_0">1</ref>) to account for the SIMT convergence optimizer, majority voting circuit, active mask propagation, MCUs, larger caches and multi-bank L1/L2 arbitration. However, at high SIMT efficiency, the energy savings from the amortized metrics greatly outweigh the SIMT management overhead.</p><p>Figure <ref type="figure" target="#fig_6">10</ref> shows the energy consumption breakdown per pipeline stage of our studied microservices when running on CPU (Section IV details our experimental methodology). As shown in the figure, workloads consume a considerable amount of energy at the frontend and OoO stages, with an average of 73% <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The HDSearch-leaf and Recommender-leaf are the exception with 39% and 60% of energy consumed on frontend+OoO respectively. These workload contain fully SIMD vectorized functions; therefore, the backend consumes a large fraction of the energy. The memory subsystem consumes 20% of energy on average. By substituting the values in Equation 1 with the amortized components, which consume 50-90% of the total CPU energy, an anticipated 2-10x energy efficiency gain can be achieved with the RPU when SIMT efficiency is high and accesses are frequently coalesced. This anticipated energy efficiency is aligned with previous work <ref type="bibr" target="#b93">[94]</ref> which studied energy efficiency when vectorizing data-parallel workloads (PARSEC) on CPU hardware.</p><p>In the next section we experimentally show the amount of SIMT control and memory efficiency present in microservice workloads and explore the effect of different batch sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. SIMR Software Stack</head><p>Figure <ref type="figure" target="#fig_1">12</ref> compares the RPU's software (SW) stack, to that of the CPU and GPU. GPU computing ( B in Figure <ref type="figure" target="#fig_1">12</ref>) generally requires the programmer to use a specialized language, like CUDA, and (in the case of NVIDIA) uses a closed-source compiler, runtime, driver, and ISA. These all restrict programmer productivity. While GPUs have been successful accelerating deep learning inference, they are poorly suited for workloads with middling parallelism and tight deadlines.</p><p>Microservice developers typically use a variety of highlevel open-source programming languages and libraries ( A ). For the RPU, we maintain the traditional CPU software stack ( C , E ), changing only the HTTP server, driver and memory management software. The RPU is ISA-compatible with the traditional CPU.</p><p>The role of our HTTP server ( D ) is to assign a new software thread to each incoming request <ref type="bibr" target="#b94">[95]</ref>, <ref type="bibr" target="#b95">[96]</ref>. The SIMR-aware server groups requests in a batch based on each request's Application Program Interface (API) similarity and argument size (see Section III-B1), then sends a service launch command for the batch to the RPU driver with pointers to the thread contexts of these requests.</p><p>The RPU driver ( F ) is responsible for runtime batch scheduling and virtual memory management. The driver overrides some of the OS system calls related to thread scheduling, context switching, and memory management, optimizing them for batched RPU execution. For example, context switching has to be done at the batch granularity (Section III-B5), and memory management is optimized to improve memory coalescing opportunities at runtime (Section III-B2).</p><p>To ensure efficient SIMT execution, the software stack's primary goals are to: (1) minimize control flow divergence by predicting and batching requests' control flow (Section III-B1), (2) reduce memory divergence and alleviate cache/memory contention with batch tuning and SIMR-aware virtual memory mapping (Sections III-B2, III-B3, III-B4), and (3) alleviate network/storage divergence through system-wide batch splitting (Section III-B5).</p><p>1) SIMR-Aware Batching Server: A key aspect to achieve high energy efficiency is to ensure batched threads follow the same control flow, and thus minimize control divergence. To achieve this, we need to group requests that have similar characteristics. Thus, we employ two heuristic-based proofof-concept batching techniques. First, we group requests based on API or RPC calls. Some microservices may provide more than one API, for example, memcached has set and get APIs, post provides newPost and getPostByUser calls. Therefore, we batch requests that call the same procedure to ensure they will execute the same source code. Second, we group requests that have similar argument/query length. For example, when calling the Search microservice, requests that have long search query (i.e., more words) are grouped together as they will probably have more work to do than the smaller ones. Figure <ref type="figure" target="#fig_0">11</ref> shows the SIMT efficiency (i.e., = #scalar-instructions / (#batch-instructions ? batch-size)) for naive batching (based on arrival time) and an optimized per-API and per-argument batching. We demonstrate both the ideal reconvergence with stack-based IPDOM analysis <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b66">[67]</ref> and MinSP-PC heuristic policy <ref type="bibr" target="#b63">[64]</ref>. We assume a batch size of 32 requests for all microservices and we calculate the average over 75 batches  (2400 requests). As shown in Figure <ref type="figure" target="#fig_0">11</ref>, batching per-API improves SIMT efficiency for many microservices, up to 2x improvement in memcached, and 4x in Post microservices.</p><p>When taking into account per-argument length batching, the overall SIMT efficiency is further improved by 20% on average and up to 5x better on the Search-leaf and post-text microservices. In total, the stack-based analysis is able to achieve 92% SIMT efficiency. Interestingly, MinSP-PC is not far behind with an efficiency of 91% on average. In some microservices the heuristic even shows 1-2% higher efficiency due to eliminating the redundant execution of reconvergnce instructions in the stack-based approach <ref type="bibr" target="#b63">[64]</ref>.</p><p>We achieve this SIMT efficiency while making the following assumptions. First, some of these microservices are not well optimized and employ coarse-grain locking which affects our control efficiency negatively due to critical section serialization and lock spinning. In practice, optimized data center workloads rely on fine-grain locking to ensure strong performance scaling on multi-core CPUs <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b95">[96]</ref>. In our experiments we assume threads that access different memory regions within a data structure use fine-grained locks for synchronization. We also assume that a high-throughput, concurrent memory manager is used for heap segment allocation <ref type="bibr" target="#b96">[97]</ref>- <ref type="bibr" target="#b98">[99]</ref> rather than the C++ glibc allocator that uses a single shared mutex. Finally, HDSearch-midtier contains a data-dependent control flow in which one side of a branch contains more expensive code. To improve SIMT efficiency in such scenarios, we use speculative reconvergence <ref type="bibr" target="#b99">[100]</ref> to place the IPDOM synchronization point at the beginning of the expensive branch.</p><p>2) Stack Segment Coalescing: Similar to the local memory space in GPUs <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b100">[101]</ref>, Figure <ref type="figure" target="#fig_8">13</ref> depicts how the RPU driver and TLB hardware allocate and map stack memory from different threads in the same batch to minimize memory divergence. The interleaving is static and transparent to the compiler and the programmer. When the runtime system calls mmap to allocate a new stack segment for a thread <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b102">[103]</ref>, we ensure that the stack segments for all the threads in a batch are contiguous ( a in Figure <ref type="figure" target="#fig_8">13</ref>).</p><p>In hardware, we detect accesses to stack addresses and apply an interleaved data mapping ( b ), such that stack segments from different threads are interleaved every 4 bytes in the physical address space ( c ). The RPU's address generation unit overrides the stack base of all active threads with the stack base of thread 0, thus we only need one TLB translation per stack access. A hardware offset mapping uses the thread ID (TID) of the accessing thread as an index into the SS0 space to determine where the value resides in physical memory. However, this hard mapping prevents threads from accessing other threads' stack data, which is allowed in CPU programming. To resolve this issue, we calculate the target stack segment TID of each access based on the access' virtual segment address, i.e.</p><p>T argetT ID = (SSi-SS0)/StackSize, exploiting the fact that stacks are allocated consecutively in the virtual space.</p><p>If the accessing thread has permission to access the target thread's stack (discussed further in Section VI-C), then the TargetTID is used, allowing inter-thread stack accesses. Note that GPU programming languages avoid this issue by making stack values thread-local.</p><p>Coalescing Results: Figure <ref type="figure" target="#fig_9">14</ref> demonstrates the effectiveness of our stack interleaving and heap memory coalescing policies (previously described in Section III-A and Figure <ref type="figure" target="#fig_4">8b</ref>). Figure <ref type="figure" target="#fig_9">14</ref> plots the total number of L1 accesses in the RPU, normalized to a MIMD CPU, when both are executing 640 threads. The RPU's 32-thread batches generate on average 4x fewer accesses than the CPU. The causes of this traffic reduction are two-fold. First, many of our middle tier microservices contain significant stack segment accesses (up to 90% in the Post microservices) caused by frequent procedure/system calls, push/pop argument passing, and reading/writing local variables. Our stack segment interleaving technique coalesces all these accesses and generates less traffic compared to the CPU. For example, pushing an 8-byte address in each thread of a 32-thread batch onto the stack generates 8 accesses (8B x 32 threads / 32B cache lines); however, in the CPU 32 accesses are generated.</p><p>Second, microservices typically share some global data structures and constant values in the heap and data segments <ref type="bibr" target="#b24">[25]</ref> respectively. In the RPU, accesses to this shared data are coalesced within the MCU and loaded once for all the threads in a batch, improving L1 data throughput. While traffic reduction is significant in many cases, back-end data-intensive microservices, like HDSearch, still exhibit high traffic as each thread contains private data structures in the heap with little sharing, resulting in frequent divergent heap accesses.</p><p>3) Batch Size Tuning and Memory Contention : Previous work <ref type="bibr" target="#b16">[17]</ref> shows that micro and nanoservices typically exhibit a low cache footprint per thread, as services are broken down into small procedures and read-after-write interprocedure locality is often transferred to the system network via RPC calls. To exploit this fact, we increase the number of threads per RPU core compared to traditional CPUs. Figure <ref type="figure" target="#fig_10">15</ref> shows the L1 MPKI of a single threaded CPU with 64KB of L1 cache and an RPU with different batch sizes <ref type="bibr" target="#b31">(32,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4)</ref> and 256KB of L1 cache. Interestingly, many of our microservices can run at a batch size of 32 threads and require only 8KB/thread without thrashing the L1 cache. More importantly, for these microservices, the L1 MPKI is significantly improved compared to the CPU. This is because memory coalescing reduces the overall number of L1 accesses as well as the number of misses. As the batch size decreases, the coalescing efficiency is reduced.</p><p>On the other hand, some leaf node microservices, like HDsearch-leaf and Search-leaf, have high L1 MPKI compared at a batch size of 32. These are data-intensive services, exhibiting a larger intra-thread locality footprint due to divergent heap segment accesses, read-after-write temporary data and prefetch buffer to hide long memory latency. However, they show low MPKI when we throttle the batch size to 8 (see Figure <ref type="figure" target="#fig_10">15</ref>). We make similar observations for TLB and memory system contention when applying batch size tuning. Therefore, we run all our microservices at a batch size of 32, except for these data-intensive services, which are executed with a batch size of 8. Due to sub-batch interleaving, running at this smaller batch size does not affect our execution unit utilization. Regardless of batch size, our RPU hardware is configured with 8 SIMT lanes (Section IV), as such, an  8-thread batch can fully utilize the pipeline, even though amortization suffers versus a 32-thread batch. After inspecting the HDsearch-leaf source code, we found that we could reduce the L1 cache footprint of the workload by eliminating some unnecessary data copies and employing function fusion (analogous to kernel/layer fusion in GPU and DL); however, we decided not to alter the program in our experiments.</p><p>Selecting the right batch size is influenced by many other factors, e.g. the request arrival rate, desired QoS, and system configuration. As widely practiced by data center providers <ref type="bibr" target="#b19">[20]</ref>, an offline configuration can be applied to tune the batch size for a particular microservice. The time overhead to form a batch size of 8-32 requests is well tolerated by data center providers and matches those used in Google and Facebook's batching mechanisms for deep learning inference <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b45">[46]</ref>.</p><p>4) SIMR-Aware Memory Allocation : Divergent accesses to the heap have the potential to create bank conflicts in the RPU's multi-bank L1 cache. Figure <ref type="figure" target="#fig_11">16a</ref> depicts a frequent code pattern in our microservices. The program dynamically allocates a thread-private temporary array on the heap (line#4), fills the array with intermediate results in a linear fashion (line#8), and reads from this array to process the data (line#12). The top section of Figure <ref type="figure" target="#fig_11">16b</ref> shows the behavior of the default C++ SIMR-agnostic CPU allocator. We assume a virtually-indexed L1 cache as is widely employed by CPU designs. Thus, the memory allocator may assign addresses to the temp array that result in significant bank conflicts. One solution for this is to change the address mapping of the heap segment <ref type="bibr" target="#b103">[104]</ref> to interleave elements accessed by parallel threads, similar to our stack segment interleaving. However, this type of interleaving is ill-suited for heap accesses, which are less structured than stack accesses. Another solution is to rely on hardware-based xor-ing hashing <ref type="bibr" target="#b104">[105]</ref>, <ref type="bibr" target="#b105">[106]</ref>, however our experiments show that it is ineffective to alleviate bank conflicts.</p><p>To this end, we address this problem by proposing a new SIMR-aware memory allocator that the RPU driver can provide as an alternative and overrides the memory allocator   used by the run-time library through LD PRELOAD Linux utility <ref type="bibr" target="#b106">[107]</ref>, <ref type="bibr" target="#b107">[108]</ref>. Our proposed memory allocator, demonstrated in the bottom image of Figure <ref type="figure" target="#fig_11">16b</ref>, avoids data interleaving for the heap segment. Instead, the key idea is to take into account that data are already interleaved every n bytes over L1 banks (n=32B in our baseline). Therefore, if we ensure that the start address of every new memory allocation per thread follows the condition (start address%(n*tid) = 0), then accesses to the private data structure will be conflict-free for all consecutive data accesses, as shown in Figure <ref type="figure" target="#fig_11">16b</ref>. The overhead of this method is the unused few bytes at the start of each data allocation to ensure the alignment constraint (around 896 bytes for an 8-thread allocation). This memory fragmentation is amortized with large memory allocation sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) System-Level Batch Splitting :</head><p>In the RPU, context switching is done at the batch granularity, either all threads in a batch are running or all the threads in the batch are switched out. When RPU threads are blocked due to an I/O event, the RPU driver groups the I/O received interrupts and wakes all the threads in the same batch at the same time to handle their interrupts and continue lock-step execution. However, requests within the same batch can follow different control paths, and paths may be of different lengths. For memory and nanosecond-scale latencies, the paths synchronize at the IPDOM reconvergence point. However, if one path contains significantly longer millisecond-scale latency (e.g., a request to storage or the network), this can hinder the threads on the other path, extending the average latency. Figure <ref type="figure" target="#fig_13">17a</ref> illustrates a frequently-used design pattern in microservice development, in which back-end storage accesses are cached in fast in-DRAM key-value store, like memcached (line#3 in Figure <ref type="figure" target="#fig_13">17a</ref>). If the user request hits in the microsecond-scale latency memcached, the request returns immediately to the client (line#12); otherwise, it has to access the millisecond-scale storage, update the cache, and send the result back (lines#5-10). If the hit requests have to wait for the misses at the reconvergence point (line#11), then the storage latency will dominate the total average latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SIMTec (PIN-based tool)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accel-Sim &amp; McPAT uQSIM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SIMT Efficiency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chip-level cycle accurate simulator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System-level uservice-interaction simulator CPU vs RPU Requests/Joule &amp; Service Latency traces (w/ &amp; w/o batching) Dynamic Instrumentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End-to-end tail latency &amp; Max throughput</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Throughput &amp; latency</head><p>Figure <ref type="figure" target="#fig_4">18</ref>: End-to-end experimental setup.</p><p>To avoid this issue we propose a batch splitting technique, depicted in Figure <ref type="figure" target="#fig_13">17b</ref>, in which we split the batch and allow multi-path execution <ref type="bibr" target="#b108">[109]</ref> for hit and miss requests. The batch is subdivided into two batches, one for the hit requests to continue execution beyond the reconvergence point ( 4 in Figure <ref type="figure" target="#fig_13">17b</ref>), and the other for blocked requests accessing storage <ref type="bibr" target="#b2">( 3 )</ref>. The driver copies and saves the architectural state and call stack for the blocked requests. Note that in cycle-level multipath execution on GPUs <ref type="bibr" target="#b108">[109]</ref>- <ref type="bibr" target="#b110">[111]</ref>, divergent paths still ultimately converge and resources are not freed until all paths are complete. In SIMR batch splitting the fast completing path can be allowed to continue and finish execution, while the slower blocked path is context switched out, freeing up resources for other requests.</p><p>A hardware-based timeout or software-based hint can be used to determine the splitting decision. Although batch splitting reduces control efficiency, as the miss requests will continue execution alone, we can still batch these orphan requests at the storage microservice and form a new batch to be executed with a full SIMT active mask. We believe there is a wide space of future work to analyze the microservice graph for splitting and batching opportunities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP</head><p>Workloads: We study a microservice-based social graph network as depicted in Figure <ref type="figure">3</ref>, similar to the one represented in the DeathStarBench suite <ref type="bibr" target="#b16">[17]</ref>. Search, HDIm-ageSearch, Recommender, and McRouter are adopted from the ?suite benchmarks <ref type="bibr" target="#b15">[16]</ref>. We use the input data associated with the suite. The microservices use diverse libraries, including C++ stdlib, Intel MKL, OpenSSL, FLANN, Pthread, zlib, protobuf, gRPC and MLPack. The Post and User microservices are adopted from the DeathStarBench workloads <ref type="bibr" target="#b16">[17]</ref> and social graph is from SAGA-Bench <ref type="bibr" target="#b111">[112]</ref>. The microservices have been updated to interact with each other via Google's gRPC framework <ref type="bibr" target="#b112">[113]</ref>, and they are compiled with the -O3 option and SSE/AVX vectorization enabled. While the RPU can also execute other HPC/GPGPU applications that exhibit the SPMD pattern, like OpenMP and OpenCL, we limit the focus of our study to microservice workloads. Section VI-D discusses running GPGPU workloads on RPU in further detail.</p><p>Simulation Setup: We analyze our RPU system over multiple stages and simulation tools. Figure <ref type="figure" target="#fig_4">18</ref> shows our end-to-end experimental setup. First, we analyze the SIMT efficiency of our microservice workloads with an in-house x86 PIN <ref type="bibr" target="#b113">[114]</ref>-based tool, named SIMTec <ref type="bibr" target="#b114">[115]</ref>. The tool traces the dynamic control flow of CPU threads running in a batch, and calculates the associated active mask and overall SIMT efficiency. SIMTec traces the whole SW stack, including user code, libraries, and frameworks. The PIN tool operates in userspace mode so we were not able to trace system calls; however, they only represent 20% of the microservices executed instructions <ref type="bibr" target="#b16">[17]</ref>, and we expect they should exhibit high SIMT control efficiency. Second, we use the trace-driven, cycle-level Accel-Sim v1.1 <ref type="bibr" target="#b115">[116]</ref> simulator and BookSim <ref type="bibr" target="#b116">[117]</ref> for interconnect simulation to obtain chip-level throughput and service latency for the CPU and RPU. We updated Accel-Sim's frontend to execute x86 traces generated by SIMTec. CISC instructions with memory operands are broken down to multiple RISC-like instructions with separate loads and stores <ref type="bibr" target="#b117">[118]</ref>. Further, Accel-Sim's performance model has been extended to model a CPU-like pipeline with superscalar, OoO issue. Table IV lists the simulator configuration for our CPU vs. RPU analysis. We model a many-core x86-based single-threaded CPU similar to the ones found on the market today and commonly used in data centers <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b32">[33]</ref>. We also model an 8-way simultaneous multi-threading CPU (SMT8), to reflect the highest SMT degree found in the market today from IBM POWER9 <ref type="bibr" target="#b34">[35]</ref>.</p><p>We ensure both CPU and RPU have the same pipeline configuration and frequency. For SMT8, we maintain the same number of total threads and memory resources/thread as the RPU (see the last four entries in Table <ref type="table" target="#tab_12">IV</ref>). Cache latency is calculated based on CACTI v7.0 <ref type="bibr" target="#b118">[119]</ref>. The multibank caches and MCU increase the L1/L2 hit latency from 3/12 cycles in the CPU to 8/20 cycles in the RPU. For other execution units, the ALU/Branch execution latency is increased to 4 cycles in the RPU to account for the extra wiring and capacitance of adding more lanes <ref type="bibr" target="#b42">[43]</ref> and the majority voting circuit. We assume an idealistic cache coherence protocol for the CPU with zero traffic overhead, in which atomics are executed as normal memory loads in a private cache. In the RPU, atomic instructions bypass private caches and execute at a shared L3 cache. Third, to study batching effects on a large scale and capture the system implications of context switching, queuing delay, and network/storage blocking, we harness uqsim <ref type="bibr" target="#b119">[120]</ref>, an accurate and scalable simulator for interactive microservices. The simulator is configured with our social graph network along with the latency and throughput values obtained from Accel-Sim simulations to calculate system-wide end-to-end tail latency.</p><p>Energy&amp;Area Model: We use McPAT <ref type="bibr" target="#b120">[121]</ref> and some elements from GPUWattch <ref type="bibr" target="#b79">[80]</ref> to configure the CPU and RPU as described in Table <ref type="table" target="#tab_12">IV</ref> and to estimate per-component area, peak power and dynamic energy. For the RPU, we consider the additional components and augmentation required to support SIMT execution described in Figure <ref type="figure">6</ref>. The majority voting circuitry is modeled as a CAM structure (32-way comparator) to count the taken and non-taken results and a reduction tree to calculate the most selected destination address. The SIMT optimizer is modeled as 2x reduction tree to calculate the minimum PC and SP <ref type="bibr" target="#b64">[65]</ref> and a CAM structure to calculate the active mask. A 2x 32-way CAM structure is used to model the memory coalescing units <ref type="bibr" target="#b79">[80]</ref>. The RAT, ROBs, and uop buffers are extended to include the 4-byte active mask and its associated logic.</p><p>Table <ref type="table" target="#tab_13">V</ref> shows the calculated area and peak power for the RPU and single-threaded CPU at 7-nm technology <ref type="bibr" target="#b121">[122]</ref>. To support SMT-8 in the CPU, a 14% area and power increase per core is required (not shown in the table for brevity). From analyzing the table results, we can make the following observations. First, the CPU's frontend+OoO area and power overhead are roughly 40% and 50% respectively, which are aligned with modern CPU designs <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The table shows that the RPU core is 6.3x larger and consumes 4.5x more peak power than the CPU core; however, the RPU core supports 32x more threads. Second, in the RPU core, most of the area is consumed by the register file and execution units, 68% of the area vs. 35% in the CPU. The additional overhead of the RPU-only structures consume 11.8% of the RPU core. Most of this overhead comes from the 8x8 crossbar that connects the L1 banks to the LD/ST queues. Third, the dynamic energy per L1 access and L2 access in RPU is higher by a factor of 1.72x and 1.82x respectively than in CPU, due to the larger cache size, L1-Xbar and MCU. However, the generated traffic reduction and other energy savings in the frontend will outweigh this energy increase as detailed in the next section.</p><p>In Section V, we use the per-access energy numbers generated from our McPAT analysis with the simulation results generated by Accel-Sim to compute the runtime energy efficiency of each workload (Figure <ref type="figure" target="#fig_5">19</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Chip-Level Results</head><p>Figure <ref type="figure" target="#fig_5">19</ref> and Figure <ref type="figure" target="#fig_1">20</ref> show energy efficiency (requests/joule) and service latency of RPU CPU-SMT8 normalized to single threaded CPU. All the hardware executes the same number of requests (2400). On average, the RPU can achieve 5.7x higher energy efficiency compared to CPU, while still coming within 1.44x of its service latency, with the worst service latency of 1.7x at HDSearch-midtier. Overall, the RPU's service latency remains under the 2x higher latency limit defined by data center providers <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b51">[52]</ref>. The main causes of RPU's energy-efficiency are: (1) reducing the number of issued instructions by a factor of 30x, amortizing the frontend and OoO dynamic energy overhead that accounted for up to 73% in the scalar heavily-integer microservices, (2) generating 4x less traffic on average, therefore decreasing the memory energy consumption, and (3) running 6x more requests at almost the same service latency vs. the CPU, and thus amortizing the static energy. The HDSearch-leaf, HDSearch-midtier and Search-leaf microservices exhibit less energy-efficiency than the average. These workloads run at a smaller batch size and/or exhibit high memory divergence (Figure <ref type="figure" target="#fig_9">14</ref>). In HDSearch-leaf, the frontend+OoO only accounts for 39% of the CPU's energy, limiting the SIMT energy efficiency as discussed in Equation 1 and Section III-A2.</p><p>On the other hand, CPU-SMT8 only improves energy efficiency by 5% at a 5x higher service latency cost. This is because the number of issued instructions and the generated accesses are the same as in single threaded CPU. Further, SMT8 partitions the frontend resources per thread and causes cache serialization of stack segment accesses and shared heap variables, hindering service latency, whereas RPU avoids all these issues through SIMT execution.</p><p>The main causes of RPU's 1.44x higher service latency are four-fold. First, the control SIMT efficiency of some mi- croservices is below 90% (see Figure <ref type="figure" target="#fig_0">11</ref>) in which the RPU serializes the divergent paths and increases service latency. Second, when CPU threads run consecutively, they prefetch some shared data to the L1 cache for the incoming threads running on the same core. In the RPU, many threads are run in parallel and incur these compulsory misses at the same time. Third, the L1 access of the RPU is longer (3 vs 8 cycles) as a result of a larger L1 cache size, and ALU instruction latency is 4x higher due to vector pipeline, increasing data dependency latency. Fourth, there remains a slight cache contention occurring in the RPU's L1 cache for HDSearchleaf and Search-leaf even after applying batch tuning. 1) Sensitivity Analysis: We evaluate RPU's sensitivity to a number of system parameters:</p><p>? Sub-batch interleaving: In the CPU, IPC per thread is limited, with a range of 0.3-1, similar to those reported in data center studies <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b22">[23]</ref>. In the RPU, due to sub-batch interleaving, we are able to improve IPC utilization up to 4x by issuing threads over multiple cycles to the SIMT lanes. Although we reduced the number of SIMT lanes by 4x with sub-batch interleaving (i.e., from 32 to 8 lanes), we only noticed 4% performance loss on average, and up to 10% in UniqueID, compared to full width SIMT lanes. ? Moving atomics to L3: We did not experience slow down from moving atomics to the L3 cache in the RPU, likely because our microservices exhibit few atomic locks per instruction. ? SIMR-aware heap allocation: Our SIMR-aware heap segment improves the L1 cache throughput for frequently divergent heap segments in HDSearch, where a 1.8x higher throughput was achieved versus the SIMR- agnostic heap allocations. ? Majority voting: Majority voting optimizes the branch prediction for common control flow (92% of the time threads traverse the same control flow). Still, 8% control divergence causes some threads to have different predictions per-batch than they would with a per-thread prediction (i.e., as in CPUs). Since we predict next PC per entire batch, we will always have misprediction for the divergent threads of the other path (see Figure <ref type="figure">7</ref> example). Majority voting mitigates the flushes caused by these inevitable branch mispredictions by optimizing for the common control flow, and thus improving overall energy efficiency. However, the majority voting policy has little impact on performance, as in the case of divergence both paths are visited anyways, and thus the branch predictor is always correct.</p><p>2) Service Latency Analysis: Despite RPU's higher L1 access (2.3x), sub-batch interleaving, and ALU and branch execution latency (4x), some microservices are still able to achieve service latency close to the CPU, and on average only 1.44x higher latency. This is because our workloads by memory latency, with only 20% of the time with successful instruction retirement <ref type="bibr" target="#b22">[23]</ref>. In the RPU, the memory coalescing reduces on-chip memory traffic, alleviating contention and minimizing memory latency. Figure <ref type="figure" target="#fig_15">21</ref> depicts several metrics that explain the relatively little increase in service latency for the RPU. The average memory latency has been reduced by 1.33x because 4x less traffic is generated and a single-hop crossbar interconnect is employed which help offset the latency increases in instructions and cache hits.</p><p>3) GPU Performance: We also run our simulation experiments on an Ampere-like GPU model <ref type="bibr" target="#b122">[123]</ref> with the same software optimization as the RPU (e.g., stack memory coalescing and batching) and assume that the GPU supports the same CPU ISA and system calls. For the sake of brevity, we did not include the per-app results in the figures. On average, the GPU achieves 28x higher energy efficiency than the CPU but at 79x higher latency, which aligns with previous work <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. This high latency is unacceptable for QoS-sensitive data center applications <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b123">[124]</ref>. The lower clock frequency and lack of OoO / speculative  execution contribute to GPU's higher service latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. System-Level Results</head><p>Figure <ref type="figure" target="#fig_17">22</ref> shows the system-level, end-to-end 99% tail and average latency for CPU-based system and RPU-based system with and without our batch splitting technique described in Section III-B5. We scale the QPS load until reaching the highest maximum throughput at acceptable QoS and the system saturates. We configure uqsim with the end-to-end User microservice scenario passing from Web Server to User to McRouter to Memcached and Storage in Figure <ref type="figure">3</ref>. We simulate three CPU server machines with 40 cores. We assume a 90% hit rate of Memcached with 100, 20, 25, 1000 and 60 microseconds latency for User, McRouter, Memcached, Storage and network respectively. In the RPU configuration, we replace the CPU servers with RPU machines consuming the same power budget, i.e., assuming 5x higher requests/joule and 1.2x higher latency as were obtained from chip-level experiments for these services. Request batching is employed for memcached in the CPU configuration for epoll system calls to reduce network processing <ref type="bibr" target="#b119">[120]</ref>. To focus our study on processing throughput we assumed unlimited storage bandwidth for both CPU and RPU configurations.</p><p>From analyzing the end-to-end results in Figure <ref type="figure" target="#fig_17">22</ref> we can make the following observations. First, the RPU (with batch splitting) can achieve 4x higher throughput compared to the CPU (60 vs 15 kQPS) with almost the same tail and average latency. Second, the batching formation time is amortized and incurs negligible overhead at both low and high traffic load. This is due to the fact that the CPU system employs batching already for memcached. Third, without batch splitting on millisecond-scale storage accesses the RPU exhibits higher average latency than the CPU, as blocked threads are waiting on the reconvergence point for the other threads that access the storage. However, RPU without batch splitting can still attain an acceptable tail latency. Although tail latency is more important than average latency for QoS measurements, the batch splitting technique can be beneficial to ensure predictable response time when unpredictable high latency episodes occur in large online services <ref type="bibr" target="#b123">[124]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. RPU vs CPU's SIMD</head><p>A possible alternative to the RPU would be to recompile scalar CPU binaries for execution on the CPU's existing SIMD units, e.g., x86 AVX <ref type="bibr" target="#b124">[125]</ref>, <ref type="bibr">[126]</ref> or ARM SVE <ref type="bibr" target="#b125">[127]</ref>. Each request could be mapped to a SIMD lane, amortizing the frontend overhead, leveraging the latency optimizations the CPU pipeline, and executing uniform instructions on the scalar units <ref type="bibr" target="#b41">[42]</ref>. Such a transformation could be done using a SPMD-on-SIMD compiler, like Intel ISPC <ref type="bibr" target="#b41">[42]</ref>, or at the binary-level, as depicted in Figure <ref type="figure" target="#fig_1">23</ref>. However, this solution has three primary shortcomings. First, it requires a complete recompilation of the microservice code, libraries, and OS system calls. Second, SIMD units on contemporary CPUs are designed to accelerate computationally-dense inner loops. The memory system and vector ISA are not optimized for the branch-and memoryheavy microservices we focus on in the RPU. As a result, energy-efficiency and service latency will be negatively affected. For instance, binary transformation requires serializing existing SIMD instructions in the scalar binary ( D in Figure <ref type="figure" target="#fig_1">23</ref>) and predicate computation cannot take advantage of branch prediction ( E ). There are 2x more scalar units than SIMD units <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> on existing CPUs, which will go unused if the code can be fully vectorized. Finally, many existing scalar instructions lack a 1:1 mapping with any vector instruction ( F ), e.g., complex string manipulation, atomic and OS operations. Based on a manual investigation in x86 ISA <ref type="bibr" target="#b124">[125]</ref>, there are 129 AVX instructions, and 463 scalar instructions, thus only a maximum of 27% of the scalar instructions are represented in the vector ISA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-threaded vs Multi-process Services</head><p>Our proposed SIMR system focuses on multi-threaded services, which are widely used in data centers <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b126">[128]</ref>.</p><p>However, the rise of serverless computing has made multiprocess microservices more common <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b127">[129]</ref>. In multiprocess services, the separate virtual address spaces can cause both control flow and memory divergence, even if the processes use the same executable and read the same data, which also causes cache-contention issues on contemporary CPUs. We believe that with user-orchestrated inter-process data sharing and some modifications to the RPU's virtual memory these effects can be mitigated. However, since the contemporary services we study are all multi-threaded, we leave such a study as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Security Implications</head><p>The grouping of concurrent requests for SIMT execution may enable new vulnerabilities. For instance, a malicious user may generate a very long query that could affect the QoS of other short requests or leak control information. Such attacks can be mitigated in our input size-aware batching software by detecting and isolating maliciously long requests, as described in Section III-B1. Another security vulnerability is the potential for parallel threads to access each other's stack data (exploiting the fact that threads' stack data are adjacent in the physical space). However, as described in Section III-B2, the RPU's address generation unit is able to identify inter-thread stack accesses and throw an exception if such sharing is not permitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. GPGPU Workloads on RPU</head><p>The RPU can seamlessly execute other HPC, GPGPU, and DL applications that exhibit the SPMD pattern, written in OpenMP, OpenCL, or CUDA. GPUs are 2-5x more energy efficient than CPUs <ref type="bibr" target="#b128">[130]</ref>- <ref type="bibr" target="#b131">[133]</ref>, thanks to their simpler in-order pipeline, lower frequency, and software-managed caches. However, this energy efficiency comes at the cost of easy programmability. Developers need to rewrite their code in a GPGPU programming language and make a heroic effort to get the most out of the GPU's compute efficiency <ref type="bibr" target="#b132">[134]</ref>, <ref type="bibr" target="#b133">[135]</ref>. Recently Nvidia has written its back-end libraries in hand-tuned machine assembly to improve instruction scheduling <ref type="bibr" target="#b115">[116]</ref> and has proposed complex asynchronous programming APIs <ref type="bibr" target="#b134">[136]</ref> to hide memory latency via prefetching. Such optimizations are likely necessary due to the lack of OoO processing. In CPUs, the hardware supports OoO scheduling with a large instruction window, which removes these performance burdens from the programmers. Furthermore, CPU programming supports system calls naturally and does not require CPU-GPU memory copies.</p><p>We believe that the RPU takes the best of both worlds. It can execute GPGPU workloads with the same easyto-program interface as CPUs while providing energy efficiency comparable to a GPU. CPUs typically contain 1-2x 256-bit (assuming Intel AVX) SIMD engines per core <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> to amortize the frontend overhead. In the RPU, 8x lanes running in lock step, each with a dedicated 256-bit SIMD engine, can provide a wider 2048-bit SIMD unit per core, amortizing the frontend overhead even further and reducing the energy efficiency gap with the GPU. GPUs will likely remain the most energy efficient for GPGPU workloads, but we claim RPUs will not be far behind. We leave an evaluation of the achievable energy efficiency of GPGPU workloads on RPU as future work.</p><p>E. Pitfall: The RPU can be bottlenecked by I/O throughput A high throughput processor like the RPU requires high throughput storage and I/O to perform optimally. In Figure <ref type="figure" target="#fig_3">5</ref>, we demonstrate that the off-chip memory bandwidth will dramatically scale in future years with the introduction of DRR5, DDR6, and HBM in the data center. We observe similar trends for I/O standards like PCIe5 and PCIe6 <ref type="bibr" target="#b135">[137]</ref>. 128x PCIe6 lanes per single socket can provide 2 TB/sec of bidirectional I/O bandwidth. Ethernet 400/800 Gb/sec and recent NVMe interface advances will enable substantial network and storage throughput improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Pitfall: SIMR requires a lot of changes</head><p>We design the SIMR system so that the SW stack changes are as minimal as possible. As demonstrated in Section III-B and Figure <ref type="figure" target="#fig_1">12</ref>, only the HTTP server and some OS system calls are required to change in the software stack while we keep the programming interface, compiler, runtime, and ISA unaltered. In fact, data center providers adopted DL accelerators <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b45">[46]</ref> and changed the entire software stack for similar outcomes and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Pitfall: On-premise cost is of paramount importance in TCO</head><p>The capital expenses of servers can be a more important factor in the Total Cost of Ownership (TCO) of data centers than power consumption <ref type="bibr" target="#b9">[10]</ref>. While we focused our work on energy efficiency, the RPU system also improves cost efficiency. As depicted in Table <ref type="table" target="#tab_13">V</ref>, the RPU improves thread density by 5.2? compared to the CPU. A single socket RPU matches the throughput of six single-threaded CPU sockets while maintaining acceptable latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Pitfall: Batching interferes with the TCP protocol</head><p>Applying batching on the incoming requests before the TCP/IP processing can falsely indicate congestion for the end users, causing an adverse effect on the TCP congestion avoidance algorithm. To mitigate this issue, we can start applying batching at the entry of the logic layer (Figure <ref type="figure">3</ref>) and bypass the web server (or at least its TCP/IP processing) to send acknowledgments to the end users as soon as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Pitfall: Batching is not desired for interactive services</head><p>If the QoS demands of a particular service are not being met, we can tune the batch size and set a time-out to decrease latency. In general, if the service cannot tolerate batching of any size or shows low SIMT efficiency, they can be executed on the CPUs at lower throughput. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K. Open Questions</head><p>This work has spanned different levels of the system architecture and software stack to execute data center microservice efficiently. While the anticipated results look promising, there remain open questions that require further study:</p><p>? How much SIMT control efficiency exists in realworld microservices? Answering this question is critical because while we claim 92% SIMT efficiency in our workloads, this might not be the case for more diverse and complex microservices used by billion-scale users. We hope this work will encourage the data center providers to release more information about the thread similarity and SIMT efficiency of their applications. ? Can we use the existing CPU's SIMD to run the microservices rather than relying on the RPU? What changes are required to make the CPU's SIMD solution feasible? If the compilation and ISA barriers discussed in Section VI-A are resolved, how much performance and energy efficiency can SIMD achieve? ? We left some interesting architecture and software challenges unresolved in this work. For example, improving the SIMT branch prediction to predict the associated active mask along with the next PC is an interesting area to explore. Further, our L1 TLB design suffers from entry duplication, which affects the effective capacity of the DTLBs. Another interesting area to explore is building an efficient and transparent coalescing techniques for the storage and network traffic with hardware and system call support. We left other open questions throughout this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK Server Workloads on GPUs:</head><p>The most closely related work to ours are <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b44">[45]</ref>. Agrawal et al. <ref type="bibr" target="#b36">[37]</ref> proposed to run data center server workloads, namely SPEC-Web benchmarks, in lock-step execution on GPUs to exploit request similarity. This work achieved significant energy efficiency, but the authors had to rewrite the workloads from PHP to CUDA and reported 6000? worse latency. Similarly, Hetherington et al. <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b136">[138]</ref> run the memcached workload on a GPU. However the longer request latency, RISCV-based GPUs: Recent projects, Vortex <ref type="bibr" target="#b137">[139]</ref> and Simty <ref type="bibr" target="#b138">[140]</ref>, have explored building GPGPUs with a general purpose ISA like RISC-V. While these projects can improve a GPU's applicability to execute CPU binaries, they still suffer from poor latency optimization.</p><p>SIMT+OoO: Agrawal et al. <ref type="bibr" target="#b44">[45]</ref> study the SIMT efficiency of monolithic SPEC-web workloads written in PHP. Their results indicate the workloads contain promising control and memory efficiency that can be executed on SIMT hardware. They claim GPUs are ill-suited for server applications, so they propose Massively Parallel Server Processors (MSPS) to run server workloads on CPU-like SIMT hardware. However, their work lacks any architectural details and does not tackle the relevant software challenges (Section III-B). Their focus is limited to web serving applications and they require compiler support for reconvergence analysis.</p><p>Kalathingal et al. <ref type="bibr" target="#b140">[142]</ref>, <ref type="bibr" target="#b141">[143]</ref> proposed dynamic interthread vectorization architecture (DITVA) to leverage the implicit similarity that exists across in-order SMT threads when running data-parallel workloads. Tino et al. <ref type="bibr" target="#b60">[61]</ref> introduced SIMT support to an out of order pipeline (SIMT-X) to optimize OpenMP workloads. There are common design concepts between these works and our RPU micro-architecture. Nonetheless, they lack the software stack to support the microservices as they focus on dataparallel workloads. In summery, Table VII compares SIMR versus previous SIMT work at a high-level conceptual view.</p><p>GPU+OoO: Previous work <ref type="bibr" target="#b143">[145]</ref>- <ref type="bibr" target="#b147">[149]</ref> explored adding lightweight out-of-order execution in GPUs to further improve memory latency hiding. Our work is fundamentally different in that we start with an aggressive OoO CPU design, then add GPU-like SIMT elements as necessary to improve energy-efficiency. This approach frees the RPU from the constraints of the GPU programming model, introducing several new challenges we must address to efficiently execute general-purpose pre-compiled microservices. In addition, prior GPU+OoO approaches still relied on massive fine-grained multithreading and focused on throughput, whereas the RPU has significantly fewer threads and bal-ances throughput and latency, addressing unique challenges in the memory system. Furthermore, none of the prior work has made the connection between SIMT and microservices.</p><p>Microservices Acceleration: Previous work <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b148">[150]</ref>- <ref type="bibr" target="#b150">[152]</ref> have explored using hardware to accelerate microservices, with a focus on remote procedure calls <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b149">[151]</ref>, and network data transformations <ref type="bibr" target="#b148">[150]</ref>. These proposals are orthogonal to our work and could be applied on top of the RPU. These works focus on helping the CPU remove isolated bottlenecks, whereas the RPU focuses on a full system solution intended to replace the CPU.</p><p>Exploiting thread/process locality: Previous studies have proposed exploiting the locality at branch prediction <ref type="bibr" target="#b72">[73]</ref>, and caches <ref type="bibr" target="#b91">[92]</ref>, <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b151">[153]</ref> when running different instances of the same thread/process on multi-core CPUs. Still, they incur the same issues of simultaneous multi-threading discussed in Section III-A2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>Data center computing is experiencing an energyefficiency crisis. Aggressive OoO cores are necessary to meet tight deadlines but waste energy. However, modern productive software has inadvertently produced a solution hardware can exploit: the microservice. By subdividing monolithic services into small pieces and executing many instances of the same microservice concurrently on the same node, parallel threads execute similar instruction control-flow. We exploit this fact to propose our Single Instruction Multiple Request (SIMR) processing system, comprised of a novel Request Processing Unit (RPU) and an accompanying SIMR-aware software system.</p><p>The RPU adds Single Instruction Multiple Thread (SIMT) hardware to a contemporary OoO CPU core, maintaining single threaded latency close to that of the CPU. As long as SIMT efficiency remains high, all the OoO structures are accessed only once for a group of threads, and aggregation in the memory system reduces accesses. Complimenting the RPU, our SIMR-aware software system handles the unique challenges of microservice + SIMT computing by intelligently forming/splitting batches and managing memory allocation. Across 15 microservices our SIMR processing system achieves 5.7x higher requests/joule, while only increasing single thread latency by 1.44x. We believe the combination of OoO and SIMT execution opens a series of new directions in the data center design space, and presents a viable option to scale on-chip thread count in the twilight of Moore's Law.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Conceptual energy-efficiency vs. single thread latency for different compute unit design points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: High level view of our SIMR system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: SIMT control efficiency of naive batching for some microservices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Off-chip DRAM BW and thread scaling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Sub-batch interleaving and MCU to improve latency hiding and memory throughput efficiency respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: RPU's LD/ST Unit</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Dynamic energy consumption breakdown per pipeline stage as a percentage of total CPU core energy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Stack Segment (SS) coalescing (physical stack page size = virtual page size * batch size) with 4-byte interleaving. BS:batch size, TID: thread ID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: RPU L1 accesses, normalized to CPU accesses</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: L1 MPKI of a single threaded CPU vs RPU with different batch sizes<ref type="bibr" target="#b31">(32,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: SIMR-aware memory allocator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Batch split technique for control flow divergence when a path contains long network/storage blocking event.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 19 :Figure 20 :</head><label>1920</label><figDesc>Figure19: RPU and CPU-SMT8 energy efficiency (requests/joule) relative to single threaded CPU (higher is better)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Metrics that contribute to total service latency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>throughput, 1.2x latency) w/o split RPU (5x throughput, 1.2x latency) w/ split (b) End-to-end average latency</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: End-to-end tail and average latency for CPU-vs RPU-based system with and without batch split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I :</head><label>I</label><figDesc>CPU vs RPU vs GPU Key Metrics</figDesc><table><row><cell>Metric</cell><cell>CPU</cell><cell>GPU</cell><cell>RPU</cell></row><row><cell>Thread/Execution Model</cell><cell cols="2">SMT SIMT</cell><cell>SIMT</cell></row><row><cell>General Purpose Programming</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>System Calls Support</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>Service Latency</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>Energy Efficiency (Requests/Joule)</cell><cell>?</cell><cell>?</cell><cell>?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>MCU MCU SIMD PRF Xbar TLB1 . . . DBm TLBm data Tag RPU Core CPU Core CPU NoC Xbar L3$ DDR CPU Core RPU Core L3$ L3$ DDR . . . . I/Os . . . . . . 8 Normal Branch Predictor T, PC Majority Voting T, PC1 Flush/Commit (PC+AM) RoB Commit DL1 Banks Overall SoC RPU Core (SIMT+OoO) 1 2 3 5 SIMT Convergence &amp; Scheduler 4 ICache Next PC Current PC T, PC2 . . NT, PCn Select PC1 PC2 PCn . == . Active Mask (AM) . . Branch Results .. 7 SPi Atomic Decoded? Front End OoO Execution Memory Atomics L2$ L2$ .. Load Queue Age PC A1 ? Am CAM1 CAMm Age Priority Memory Dependency Predictor Age PC A1 ? Am 6 Store Queue Current PC 0 0 1 Next PC &amp; AM Figure</head><label></label><figDesc>6: RPU hardware overview. Changes to the OoO core needed to support SIMR execution are highlighted in green.</figDesc><table /><note><p>1.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>/ / BBA Basic Block "A" 2. if ( x &gt; 0) 3. { 4. / / BBB 5. } 6. else 7. { 8. / / BBC 9. } 10. / / BBD A (1111) C (0011) B (1100) D (1111) Divergent code example PC=2 PC1 PC2 PC3 PC4 Current PC (min) Active mask</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Next PC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(BP)</cell></row><row><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>1111</cell><cell>4</cell></row><row><cell>4</cell><cell>4</cell><cell cols="2">4(F) 4(F)</cell><cell>4</cell><cell>1111</cell><cell>6</cell></row><row><cell>6</cell><cell>6</cell><cell>8</cell><cell>8</cell><cell>6</cell><cell>1100</cell><cell>10</cell></row><row><cell>10</cell><cell>10</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>0011</cell><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>10 10 10 10 10 1111 12 4 8 10 Control Flow with Active Mask MinPC selection policy</head><label></label><figDesc></figDesc><table><row><cell>Figure 7: Stack-less convergence analysis example with</cell></row><row><cell>MinPC policy. Assuming instruction size = 2, and 1-cycle</cell></row><row><cell>miss penalty, F:flush.</cell></row><row><cell>own Program Counter (PC) and Stack Pointer (SP), however,</cell></row><row><cell>only one current PC (i.e., one path) is selected at a time.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table II :</head><label>II</label><figDesc>CPU vs GPU vs RPU architecture differences</figDesc><table><row><cell>Metric</cell><cell>CPU</cell><cell>GPU</cell><cell>RPU</cell></row><row><cell>Core model</cell><cell>OoO</cell><cell>In-Order</cell><cell>OoO</cell></row><row><cell>Freq</cell><cell>High</cell><cell>Moderate</cell><cell>High</cell></row><row><cell>ISA</cell><cell>ARM/x86</cell><cell>HSAIL/PTX</cell><cell>ARM/x86</cell></row><row><cell>Programming</cell><cell>General-Purpose</cell><cell>CUDA/OpenCL</cell><cell>General-Purpose</cell></row><row><cell>System Calls</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell></row><row><cell>Thread grain</cell><cell>Coarse grain</cell><cell>Fine grain</cell><cell>Coarse grain</cell></row><row><cell>TLP per core</cell><cell>Low (1-8)</cell><cell>Massive (2K)</cell><cell>Moderate (8-32)</cell></row><row><cell>Thread model</cell><cell>SMT</cell><cell>SIMT</cell><cell>SIMT</cell></row><row><cell>Consistency</cell><cell>Variant</cell><cell>Weak+NMCA</cell><cell>Weak+NMCA</cell></row><row><cell>Coherence</cell><cell>Complex</cell><cell>Relaxed Simple</cell><cell>Relaxed Simple</cell></row><row><cell>Interconnect</cell><cell>Mesh</cell><cell>Crossbar</cell><cell>Crossbar</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table III :</head><label>III</label><figDesc>CPU inefficiencies in the data center</figDesc><table><row><cell>Data center characteristics &amp; CPU inefficiency</cell><cell>RPU's mitigation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>for register renaming, reservation station,</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Frontend+OoO</cell><cell></cell><cell>Execution</cell><cell></cell><cell cols="2">Memory</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Consumption (%) 100 0 10 90 80 70 60 50 40 30 20 Energy</cell><cell>McRouter</cell><cell>backend</cell><cell>memc</cell><cell>middle-tier</cell><cell>leaf_shard</cell><cell>middle-tier</cell><cell>leaf_shard</cell><cell>middle-tier</cell><cell>leaf_shard</cell><cell>post</cell><cell>text</cell><cell>URLshort</cell><cell>uniqueID</cell><cell>userTag</cell><cell>user</cell></row><row><cell></cell><cell cols="2">Memcached</cell><cell></cell><cell cols="2">Search</cell><cell cols="4">HDSearch Recommender</cell><cell></cell><cell></cell><cell>Post</cell><cell></cell><cell></cell><cell>User</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>per-Argument-Size (Ideal stack-based) per-API + per-Argument-Size (MinSP-PC heuristic)</head><label></label><figDesc>Figure 11: SIMT control flow efficiency with different request batching policies (Batch Size = 32)</figDesc><table><row><cell>SIMT Efficiency (%) A</cell><cell cols="2">0 10 20 30 40 50 60 70 80 90 100 per-API + Webservice (C++, PHP, ?) McRouter backend memc middle-tier leaf_shard Memcached Search Naive per-API ARM/x86 compiler CUDA CUDA compiler B</cell><cell>C</cell><cell>middle-tier HDImageSearch Recommender leaf_shard middle-tier leaf_shard Webservice (C++, PHP, ?) ARM/x86 compiler</cell><cell>post</cell><cell>text</cell><cell>URLshort Post</cell><cell>uniqueID</cell><cell>userTag</cell><cell>user User Average avg</cell></row><row><cell></cell><cell>HTTP server</cell><cell>Nvidia Triton HTTP server</cell><cell>D</cell><cell>Batch-aware HTTP server</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Runtime/libs (pthread, cstdlib, ..) OS</cell><cell>CUDA runtime/libs (cudalib, tensorRT, ..) OS</cell><cell>E</cell><cell>Runtime/libs (pthread, cstdlib, ..) OS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(Process, VM, I/Os)</cell><cell>(I/Os management)</cell><cell></cell><cell>(I/Os management)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>CUDA driver (VM/thread management)</cell><cell>F</cell><cell>RPU driver (VM/thread management)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Multi Core CPU</cell><cell>GPU Hardware</cell><cell></cell><cell>RPU Hardware</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(a) CPU SW Stack</cell><cell>(b) GPU SW Stack</cell><cell></cell><cell>(c) RPU SW Stack</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Figure 12: Hardware/Software Stack of CPU vs GPU vs</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">RPU for microservices programming</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>1. Microservice () 2. // Create private temporary 3. // array in the heap segment 4. int* temp = new int[n]; 5. ???.. 6. for(int i=0; i&lt;n; i++)</head><label></label><figDesc></figDesc><table><row><cell>7.</cell><cell>// Write to the temp</cell></row><row><cell>8.</cell><cell>temp[i] = i+x;</cell></row><row><cell cols="2">9. ???..</cell></row><row><cell cols="2">10. for(int i=0; i&lt;n; i++)</cell></row><row><cell>11.</cell><cell>// Read from the temp</cell></row><row><cell>12.</cell><cell>sum += temp[i];</cell></row><row><cell cols="2">13. ???..</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>1. Procedure get_user(int userid) 2. /* first try the cache */ 3. data = memcached_fetch("userrow:" + userid) 4. if not data /* SIMT Divergence*/</head><label></label><figDesc></figDesc><table><row><cell>5.</cell><cell cols="2">/* not found : request database */</cell></row><row><cell>6.</cell><cell cols="2">data = db_select("SELECT * FROM users</cell></row><row><cell>7.</cell><cell cols="2">WHERE userid = ?", userid)</cell></row><row><cell>8.</cell><cell cols="2">/* then store in cache until next get */</cell></row><row><cell>9.</cell><cell cols="2">memcached_add("userrow:" + userid,</cell></row><row><cell>10.</cell><cell>data)</cell></row><row><cell cols="2">11. end</cell><cell>/* SIMT Reconvergence Point*/</cell></row><row><cell cols="2">12. return data</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table IV :</head><label>IV</label><figDesc>CPU vs RPU Simulated Configuration</figDesc><table><row><cell>Metric</cell><cell>CPU</cell><cell>CPU SMT</cell><cell>RPU</cell></row><row><cell>Core</cell><cell>8-wide</cell><cell>8-wide</cell><cell>8-wide</cell></row><row><cell>Pipeline</cell><cell>256-entry OoO</cell><cell>256-entry OoO</cell><cell>256-entry OoO</cell></row><row><cell>ISA</cell><cell>x86-64</cell><cell>x86-64</cell><cell>x86-64</cell></row><row><cell>Freq</cell><cell>2.5 GHZ</cell><cell>2.5 GHZ</cell><cell>2.5 GHZ</cell></row><row><cell>#Cores</cell><cell>98</cell><cell>80</cell><cell>20</cell></row><row><cell>Threads/core</cell><cell>1</cell><cell>SMT-8</cell><cell>SIMT-32 (1 batch)</cell></row><row><cell>Total Threads</cell><cell>98</cell><cell>640</cell><cell>640</cell></row><row><cell>#Lanes</cell><cell>1</cell><cell>1</cell><cell>8</cell></row><row><cell>Max IPC/core</cell><cell>8</cell><cell>8</cell><cell>64 (issue x lanes)</cell></row><row><cell>ALU/Bra Exec Lat</cell><cell>1-cycle</cell><cell>1-cycle</cell><cell>4-cycle</cell></row><row><cell>#Stages (ALU-load)</cell><cell>9-12</cell><cell>9-12</cell><cell>14-18</cell></row><row><cell>L1 Inst/core</cell><cell>64KB</cell><cell>64KB</cell><cell>64KB</cell></row><row><cell>Reg File (PRF)/core (scalar+ FP SIMD)</cell><cell>6KB</cell><cell>48KB</cell><cell>192KB</cell></row><row><cell>LSU (read/write)</cell><cell>128/64</cell><cell>128/64</cell><cell>128/64 (8x wide)</cell></row><row><cell></cell><cell>64KB, 8-way,</cell><cell>64KB, 8-way,</cell><cell>256KB, 8-way,</cell></row><row><cell>L1 Cache</cell><cell>3-cycle, 1-bank</cell><cell>3-cycle, 8-bank</cell><cell>8-cycle, 8-bank</cell></row><row><cell></cell><cell>32B/cycle</cell><cell>256B/cycle</cell><cell>256B/cycle</cell></row><row><cell>L1 TLB</cell><cell>48-entry</cell><cell>64-entry</cell><cell>256-entry, 8-bank (32-entry/bank)</cell></row><row><cell>L2 Cache</cell><cell>512KB, 8-way, 12-cycle, 1-bank</cell><cell>512KB, 8-way, 12-cycle,</cell><cell>2MB, 8-way, 20-cycle, 2-bank</cell></row><row><cell>L3 Cache</cell><cell>32MB, 16-way</cell><cell>32MB, 16-way</cell><cell>32MB, 16-way</cell></row><row><cell>DRAM</cell><cell>8x DDR5-3200, 200 GB/sec</cell><cell>10x DDR5-7200, 576 GB/sec</cell><cell>10x DDR5-7200, 576 GB/sec</cell></row><row><cell>Interconnect</cell><cell>9x9 Mesh</cell><cell>11x11 Mesh</cell><cell>20x20 Crossbar</cell></row><row><cell>OoO entries/thread</cell><cell>256, 8-wide</cell><cell>32, 1-wide</cell><cell>256, 8-wide</cell></row><row><cell>L1 capacity/thread</cell><cell>64KB</cell><cell>8KB</cell><cell>8KB</cell></row><row><cell>TLB entries/thread</cell><cell>48</cell><cell>8</cell><cell>8</cell></row><row><cell>L1B/cycle/thread</cell><cell>32B/cycle</cell><cell>32B/cycle</cell><cell>32B/cycle</cell></row><row><cell>memBW/thread</cell><cell>2 GB/sec</cell><cell>0.9 GB/sec</cell><cell>0.9 GB/sec</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table V :</head><label>V</label><figDesc>Per-component area and peak power estimates</figDesc><table><row><cell></cell><cell></cell><cell>Area</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Peak Power</cell><cell></cell></row><row><cell>Component</cell><cell>CPU</cell><cell></cell><cell>RPU</cell><cell></cell><cell>CPU</cell><cell></cell><cell>RPU</cell><cell></cell></row><row><cell></cell><cell>mm 2</cell><cell>% Core</cell><cell>mm 2</cell><cell>% Core</cell><cell>Watt</cell><cell>% Core</cell><cell>Watt</cell><cell>% Core</cell></row><row><cell>Fetch&amp;Decode</cell><cell>0.27</cell><cell>24.3</cell><cell>0.3</cell><cell>4.3</cell><cell>0.39</cell><cell>15.6</cell><cell>0.4</cell><cell>3.6</cell></row><row><cell>Branch Prediction</cell><cell>0.01</cell><cell>0.9</cell><cell>0.01</cell><cell>0.1</cell><cell>0.02</cell><cell>0.8</cell><cell>0.02</cell><cell>0.2</cell></row><row><cell>OoO</cell><cell>0.11</cell><cell>9.9</cell><cell>0.17</cell><cell>2.4</cell><cell>0.85</cell><cell>34</cell><cell>1.45</cell><cell>12.9</cell></row><row><cell>Register File</cell><cell>0.14</cell><cell>12.6</cell><cell>2.52</cell><cell>35.8</cell><cell>0.49</cell><cell>19.6</cell><cell>4.26</cell><cell>38</cell></row><row><cell>Execution Units</cell><cell>0.25</cell><cell>22.5</cell><cell>2.31</cell><cell>32.8</cell><cell>0.34</cell><cell>13.6</cell><cell>2.51</cell><cell>22.4</cell></row><row><cell>Load/Store Unit</cell><cell>0.07</cell><cell>6.3</cell><cell>0.34</cell><cell>4.8</cell><cell>0.13</cell><cell>5.2</cell><cell>0.41</cell><cell>3.7</cell></row><row><cell>L1 Cache</cell><cell>0.04</cell><cell>3.6</cell><cell>0.22</cell><cell>3.1</cell><cell>0.09</cell><cell>3.6</cell><cell>0.2</cell><cell>1.8</cell></row><row><cell>TLB</cell><cell>0.02</cell><cell>1.8</cell><cell>0.08</cell><cell>1.1</cell><cell>0.06</cell><cell>2.4</cell><cell>0.4</cell><cell>3.6</cell></row><row><cell>L2 Cache</cell><cell>0.2</cell><cell>18</cell><cell>0.71</cell><cell>10.1</cell><cell>0.13</cell><cell>5.2</cell><cell>0.24</cell><cell>2.1</cell></row><row><cell>Majority Voting</cell><cell>0</cell><cell>0</cell><cell>0.02</cell><cell>0.3</cell><cell>0</cell><cell>0</cell><cell>0.03</cell><cell>0.3</cell></row><row><cell>SIMT Optimizer</cell><cell>0</cell><cell>0</cell><cell>0.03</cell><cell>0.4</cell><cell>0</cell><cell>0</cell><cell>0.05</cell><cell>0.4</cell></row><row><cell>MCU</cell><cell>0</cell><cell>0</cell><cell>0.02</cell><cell>0.3</cell><cell>0</cell><cell>0</cell><cell>0.01</cell><cell>0.1</cell></row><row><cell>L1-Xbar</cell><cell>0</cell><cell>0</cell><cell>0.31</cell><cell>4.4</cell><cell>0</cell><cell>0</cell><cell>1.23</cell><cell>11</cell></row><row><cell>Total-1core</cell><cell>1.11</cell><cell></cell><cell>7.04</cell><cell></cell><cell>2.5</cell><cell></cell><cell>11.21</cell><cell></cell></row><row><cell></cell><cell>mm 2</cell><cell>% Chip</cell><cell>mm 2</cell><cell>% Chip</cell><cell>Watt</cell><cell>% Chip</cell><cell>Watt</cell><cell>% Chip</cell></row><row><cell>Total-Allcores</cell><cell>108.8</cell><cell>77.2</cell><cell>140.8</cell><cell>81</cell><cell>245</cell><cell>72.5</cell><cell>224.2</cell><cell>73.7</cell></row><row><cell>L3 Cache</cell><cell>7.82</cell><cell>5.5</cell><cell>7.82</cell><cell>4.5</cell><cell>0.75</cell><cell>0.2</cell><cell>0.75</cell><cell>0.2</cell></row><row><cell>NoC</cell><cell>9.78</cell><cell>6.9</cell><cell>1.72</cell><cell>1</cell><cell>36.52</cell><cell>10.8</cell><cell>7.02</cell><cell>2.3</cell></row><row><cell>Memory Ctrl</cell><cell>14.64</cell><cell>10.4</cell><cell>23.59</cell><cell>13.6</cell><cell>6.85</cell><cell>2</cell><cell>19.27</cell><cell>6.3</cell></row><row><cell>Static Power</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>49</cell><cell>14.5</cell><cell>53</cell><cell>17.4</cell></row><row><cell>Total Chip</cell><cell>141</cell><cell></cell><cell>173.9</cell><cell></cell><cell>338.1</cell><cell></cell><cell>304.2</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table VI :</head><label>VI</label><figDesc>GPU vs RPU TerminologyAlthough RPU and GPU are both SIMT-based hardware, we decided to use different hardware terminology for the RPU. Table VI compares Nvidia's GPU and our terminology.</figDesc><table><row><cell>GPU</cell><cell>RPU</cell></row><row><cell>Grid/Thread Block (1/2/3-dim)</cell><cell>SW Batch (1-dim)</cell></row><row><cell>Warp</cell><cell>HW Batch</cell></row><row><cell>Thread</cell><cell>Thread/Request</cell></row><row><cell>Kernel</cell><cell>Service</cell></row><row><cell>GPU Core / Streaming MultiProcessor</cell><cell>RPU Core / Streaming MultiRequest</cell></row><row><cell>SIMT</cell><cell>SIMR</cell></row><row><cell>CUDA Core</cell><cell>Execution Lane</cell></row><row><cell>J. RPU vs GPU Terminology</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table VII :</head><label>VII</label><figDesc>SIMR vs previous SIMT work at a high-level conceptual view of system calls support, and limited programmability have hindered wide-spread adoption of GPUs for general data center workloads.</figDesc><table><row><cell></cell><cell>OoO</cell><cell>CPU ISA</cell><cell>Thread grain</cell><cell>SW &amp; Workloads support</cell></row><row><cell>GPUs [123]</cell><cell>?</cell><cell>?</cell><cell>Fine</cell><cell>Data-parallel</cell></row><row><cell>VT [144]</cell><cell>?</cell><cell>?</cell><cell>Fine</cell><cell>Data-parallel</cell></row><row><cell>GPU+OoO [145]</cell><cell>?</cell><cell>?</cell><cell>Fine</cell><cell>Data-parallel</cell></row><row><cell>Simty [140]</cell><cell>?</cell><cell>?</cell><cell>Fine</cell><cell>Data-parallel</cell></row><row><cell>Vortex [139]</cell><cell>?</cell><cell>?</cell><cell>Fine</cell><cell>Data-parallel</cell></row><row><cell>DITVA [142]</cell><cell>?</cell><cell>?</cell><cell>Fine</cell><cell>Data-parallel</cell></row><row><cell>MSPS [45]</cell><cell>?</cell><cell>?</cell><cell>N/A</cell><cell>Web server</cell></row><row><cell>SIMT-X [61]</cell><cell>?</cell><cell>?</cell><cell>Fine</cell><cell>Data-parallel</cell></row><row><cell>SIMR</cell><cell>?</cell><cell>?</cell><cell>Coarse</cell><cell>Data-parallel &amp; Request-parallel microservices</cell></row></table><note><p>lack</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In fact, some CPU ISAs, like ARMv7<ref type="bibr" target="#b83">[84]</ref>,<ref type="bibr" target="#b84">[85]</ref> and POWER<ref type="bibr" target="#b85">[86]</ref>, already support a weak consistency model with non-multi-copy-atomicity in their specifications<ref type="bibr" target="#b86">[87]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>We thank our anonymous reviewers of ISCA 2022 and MICRO 2022 for their feedback and valuable comments. This work was supported, in part, by <rs type="funder">NSF</rs> <rs type="grantNumber">CCF #1910924</rs> and CCF #1943379 (CAREER).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2jNFMhs">
					<idno type="grant-number">CCF #1910924</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Sverdlik</surname></persName>
		</author>
		<ptr target="https://www.datacenterknowledge.com/cloud/analysts-there-are-now-more-500-hyperscale-data-centers-world" />
		<title level="m">Growth of Hyperscale Data Centers</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Moore&apos;s Law Is Dead. Now What</title>
		<author>
			<persName><forename type="first">T</forename><surname>Simonite</surname></persName>
		</author>
		<ptr target="https://www.technologyreview.com/s/601441/moores-law-is-dead-now-what/" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A new golden age for computer architecture</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Why data centres are the new frontier in the fight against climate change</title>
		<author>
			<persName><forename type="first">C</forename><surname>Trueman</surname></persName>
		</author>
		<ptr target="https://www.computerworld.com/article/3431148/why-data-centres-are-the-new-frontier-in-the-fight-against-climate-change.html" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-M</forename><surname>Munguia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rothchild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Texier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10350</idno>
		<title level="m">Carbon emissions and large neural network training</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The Chip Shortage Keeps Getting Worse. Why Can&apos;t We Just Make More</title>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pogkas</surname></persName>
		</author>
		<ptr target="https://www.bloomberg.com/graphics/2021-chip-production-why-hard-to-make-semiconductors/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A domain-specific supercomputer for training deep neural networks</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
	<note>Communications of the</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A cloud-scale acceleration architecture</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Angepat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Heil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ten lessons from three generations shaped google&apos;s tpuv4i: Industrial product</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ashcraft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gottscho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Jablin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The datacenter as a computer: An introduction to the design of warehouse-scale machines</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>H?lzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis lectures on computer architecture</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>3rd edition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fine-grain power breakdown of modern out-of-order cores and its implications on skylake-based systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Haj-Yihia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">B</forename><surname>Asher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CAMP: A technique to estimate perstructure power at run-time using a few simple parameters</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 15th International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient embedded computing</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Balfour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Black-Shaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Harting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sheffield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Power balanced pipelines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sartori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ahrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on High-Performance Comp Architecture</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">APOLLO: An Automated Power Modeling Framework for Runtime Power Introspection in High-Volume Commercial Microprocessors</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Knebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Palaniswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>in MICRO-54: 54th</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">? suite: A Benchmark Suite for Microservices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on Workload Characterization (IISWC)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An open-source benchmark suite for microservices and their hardware-software implications for cloud &amp; edge systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Katarki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ritchken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jackson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ibanez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mallery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jepsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shahbaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12114</idno>
		<title level="m">The nanoPU: Redesigning the CPU-Network Interface to Minimize RPC Tail Latency</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dagger: efficient and fast RPCs in cloud microservices with near-memory reconfigurable NICs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lazarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Adit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SoftSKU: Optimizing server architectures for microservice diversity@ scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dhanotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International Symposium on Computer Architecture</title>
		<meeting>the 46th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Accelerometer: Understanding acceleration opportunities for data center overheads at hyperscale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dhanotia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">?tune: Auto-tuned threading for OLDI microservices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Profiling a Warehouse-Scale Computer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Darago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual International Symposium on Computer Architecture</title>
		<meeting>the 42nd Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Clearing the Clouds: A Study of Emerging Scale-out Workloads on Modern Hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Seventeenth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Memory Hierarchy for Web Search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">AsmDB: Understanding and Mitigating Front-End Stalls in Warehouse-Scale Computers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Nagendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>August</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International Symposium on Computer Architecture</title>
		<meeting>the 46th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Classifying Memory Access Patterns for Prefetching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Architectural support for server-side PHP processing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Schlais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lipasti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scale-Out Processors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Picorel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Idgunji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ozer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE 39th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The next generation AMD enterprise server product architecture</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lepak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Naffziger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE hot chips</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Cascade Lake: Next generation Intel XEON scalable processor</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kottapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Looi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mandava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Valentine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vedaraman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The ARM Neoverse N1 platform: Building blocks for the next-gen cloud-to-edge infrastructure SoC</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pusdesris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Abernathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koppanalil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ringe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tummala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Ampere Computing</title>
		<ptr target="https://amperecomputing.com/altra/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Marvell Thun-derX3: Next-Generation Arm-Based Server Processor</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sugumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramirez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">IBM Power9 processor architecture</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sadasivam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Thompto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Starke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Memcachedgpu: Scaling-up scale-out key-value stores</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Hetherington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Aamodt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth ACM Symposium on Cloud Computing</title>
		<meeting>the Sixth ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rhythm: Harnessing Data Parallel Hardware for Server Workloads</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pistol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Lebeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 19th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">GPUfs: Integrating a file system with GPUs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Keidar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Witchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighteenth international conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the eighteenth international conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">GPUnet: Networking abstractions for GPU programs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wated</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Witchel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generic system calls for GPUs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vesel?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">NVIDIA GPUDirect</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/gpudirect" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ispc: A SPMD compiler for high-performance CPU programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pharr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 Innovative Parallel Computing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Instruction tables: Lists of instruction latencies, throughputs and micro-operation breakdowns for Intel, AMD and VIA CPUs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fog</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page">110</biblScope>
		</imprint>
		<respStmt>
			<orgName>Copenhagen University College of Engineering</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Netflix on AWS</title>
		<author>
			<persName><surname>Amazon</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/solutions/case-studies/netflix/,2021" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Massively Parallel Server Processors</title>
		<author>
			<persName><forename type="first">V</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Dinani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Honarmand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gschwind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalaiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.04140</idno>
		<title level="m">First-generation inference accelerator deployment at facebook</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Caching beyond RAM: Riding the Cliff</title>
		<author>
			<persName><surname>Memchached</surname></persName>
		</author>
		<ptr target="https://memcached.org/blog/nvm-multidisk/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improving streaming graph processing performance using input knowledge</title>
		<author>
			<persName><forename type="first">A</forename><surname>Basak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Alameldeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">DreamWeaver: Architectural Support for Deep Sleep</title>
		<author>
			<persName><forename type="first">D</forename><surname>Meisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">PowerNap: Eliminating Server Idle Power</title>
		<author>
			<persName><forename type="first">D</forename><surname>Meisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">?DPM: Dynamic power management for the microsecond era</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Bhuyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Brawny cores still beat wimpy cores, most of the time</title>
		<author>
			<persName><forename type="first">U</forename><surname>H?lzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MICRO</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Octopus-Man: QoSdriven task management for heterogeneous multicores in warehouse-scale computers</title>
		<author>
			<persName><forename type="first">V</forename><surname>Petrucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Laurenzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Doherty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 21st International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Amdahl&apos;s law for tail latency</title>
		<author>
			<persName><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">DDR5 specifications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hruska</surname></persName>
		</author>
		<ptr target="https://www.extremetech.com/computing/312730-ddr5-memory-specification-finalized-up-to-6400gt-s-2tb-lrdimms" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Introducing Micron DDR5 SDRAM: More Than a Generational Update</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schlachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Drake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">DDR5 vs DDR4 -All the Design Challenges &amp; Advantages</title>
		<author>
			<persName><forename type="first">R</forename><surname>Press</surname></persName>
		</author>
		<ptr target="https://www.rambus.com/blogs/get-ready-for-ddr5-dimm-chipsets/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">NVIDIA enters the ARMs race with homegrown Grace CPUS</title>
		<author>
			<persName><forename type="first">T</forename><surname>Morgan</surname></persName>
		</author>
		<ptr target="https://www.nextplatform.com/2021/04/12/nvidia-enters-the-arms-race-with-homegrown-grace-cpus/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">DDR5 vs. DDR6: Here&apos;s What to Expect in RAM Modules</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Peterson</surname></persName>
		</author>
		<ptr target="https://resources.altium.com/p/ddr5-vs-ddr6-heres-what-expect-ram-modules" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Intel to Launch Next-Gen Sapphire Rapids Xeon with High Bandwidth Memory</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cutress</surname></persName>
		</author>
		<ptr target="https://www.anandtech.com/show/16795/intel-to-launch-next-gen-sapphire-rapids-xeon-with-high-bandwidth-memory" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">SIMT-X: Extending single-instruction multi-threading to out-of-order cores</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Collange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Register renaming and scheduling for dynamic execution of predicated code</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-M</forename><surname>Kling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings HPCA Seventh International Symposium on High-Performance Computer Architecture</title>
		<meeting>HPCA Seventh International Symposium on High-Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">General-Purpose Graphics Processor Architectures</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W L</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Rogers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Computer Architecture</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Stack-less SIMT reconvergence at low cost</title>
		<author>
			<persName><forename type="first">C</forename><surname>Collange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HAL, Tech. Rep. hal-00622654</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Control flow optimization via dynamic reconvergence prediction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno>MICRO- 37&apos;04</idno>
	</analytic>
	<monogr>
		<title level="m">37th International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A mechanism for SIMD execution of SPMD programs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Takahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings High Performance Computing on the Information Superhighway. HPC Asia&apos;97</title>
		<meeting>High Performance Computing on the Information Superhighway. HPC Asia&apos;97</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Dynamic warp formation and scheduling for efficient GPU control flow</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Aamodt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
	<note>in 40th Annual</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">CUDA Binary Utilities</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">Isa</forename><surname>Ptx</surname></persName>
		</author>
		<ptr target="http://docs.nvidia.com/cuda/parallel-thread-execution/index.html" />
		<title level="m">CUDA Toolkit Documentation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">MIMD synchronization on SIMT architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eltantawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Aamodt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">CUDA and Applications to Task-based Programming</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kerbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kenzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics (Tutorials)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">NVIDIA Tesla V100 GPU architecture</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Branch prediction and simultaneous multithreading</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1996 Conference on Parallel Architectures and Compilation Technique</title>
		<meeting>the 1996 Conference on Parallel Architectures and Compilation Technique</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Effective ahead pipelining of instruction block address generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fraboulet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">30th Annual International Symposium on Computer Architecture, 2003. Proceedings.</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Improving GPU performance via large warps and two-level warp scheduling</title>
		<author>
			<persName><forename type="first">V</forename><surname>Narasiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shebanow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miftakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 44th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Attack of the killer microseconds</title>
		<author>
			<persName><forename type="first">L</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Enhancing server efficiency in the face of killer microseconds</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Taming the killer microsecond</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Palit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Honarmand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Systems and methods for coalescing memory accesses of parallel threads</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nyland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Nickolls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hirota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mandal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">Dec. 27 2011</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">806</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">GPUWattch: enabling energy optimizations in GPGPUs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hetherington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Eltantawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gilani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 40th Annual International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Dynamic speculation and synchronization of data dependences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Breach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th annual international symposium on Computer architecture</title>
		<meeting>the 24th annual international symposium on Computer architecture</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">INVLPG -Invalidate Specific TLB Entries</title>
		<author>
			<persName><surname>Felixcloutier</surname></persName>
		</author>
		<author>
			<persName><surname>Com</surname></persName>
		</author>
		<ptr target="https://www.felixcloutier.com/x86/invlpg" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Eventual consistency today: limitations, extensions, and beyond</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Overview of memory consistency</title>
		<author>
			<persName><surname>Arm</surname></persName>
		</author>
		<ptr target="https://developer.arm.com/documentation/ddi0406/c/Appendices/Barrier-Litmus-Tests/Introduction/Overview-of-memory-consistency" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Simplifying ARM concurrency: multicopyatomic axiomatic and operational models for ARMv8</title>
		<author>
			<persName><forename type="first">C</forename><surname>Pulte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Flur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sewell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Understanding power multiprocessors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alglave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maranget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd ACM SIGPLAN conference on Programming language design and implementation</title>
		<meeting>the 32nd ACM SIGPLAN conference on Programming language design and implementation</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">A tutorial introduction to the ARM and POWER relaxed memory models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Maranget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">HMG: Extending cache coherence protocols across modern hierarchical multi-gpu systems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bolotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Villa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nellans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">QuickRelease: A throughput-oriented approach to release consistency on GPUs</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE 20th International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Content-addressable memory (CAM) circuits and architectures: A tutorial and survey</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pagiamtzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sheikholeslami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of solid-state circuits</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A new x86 core architecture for the next generation of computing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Hot Chips 28 Symposium (HCS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Exploiting interthread temporal locality for chip multithreading</title>
		<author>
			<persName><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Sheaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Symposium on Parallel &amp; Distributed Processing (IPDPS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Effects of multithreading on cache performance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Hurson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">ParVec: vectorizing the PARSEC benchmark suite</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cebrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jahre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Natvig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1077" to="1100" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">The Apache HTTP server project</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Fielding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Enhancing the scalability of MemCached</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wiggins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">Intel document</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Hoard: A Scalable Memory Allocator for Multithreaded Applications</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D B K S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D B P R</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Ninth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IX)</title>
		<meeting>of the Ninth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IX)<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000-11">November 2000, 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Throughput-oriented GPU memory allocation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gelado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th symposium on principles and practice of parallel programming</title>
		<meeting>the 24th symposium on principles and practice of parallel programming</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">TCMalloc : Thread-Caching Malloc</title>
		<author>
			<persName><forename type="first">P</forename><surname>Menage</surname></persName>
		</author>
		<ptr target="http://goog-perftools.sourceforge.net/doc/tcmalloc.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Speculative reconvergence for improved SIMT efficiency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Damani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stephenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Giroux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization</title>
		<meeting>the 18th ACM/IEEE International Symposium on Code Generation and Optimization</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">CUDA C Programming Guide</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">Raw Linux Threads via System Calls</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wellons</surname></persName>
		</author>
		<ptr target="https://nullprogram.com/blog/2015/05/15/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Kerrisk</surname></persName>
		</author>
		<ptr target="https://man7.org/linux/man-pages/man2/mmap.2.html" />
		<title level="m">mmap -Linux manual page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Are dynamic memory managers on GPUs slow? a survey and benchmarks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mlakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="219" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Eliminating cache conflict misses through XOR-based placement functions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gonz?lez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Topham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Parcerisa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th international conference on Supercomputing</title>
		<meeting>the 11th international conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="76" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Pseudo-Randomly Interleaved Memory</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Rau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual International Symposium on Computer Architecture</title>
		<meeting>the 18th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="74" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Kerrisk</surname></persName>
		</author>
		<ptr target="https://man7.org/linux/man-pages/man8/ld.so.8.html" />
		<title level="m">ld.so -Linux manual page</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">What is the LDPRELOAD trick</title>
		<author>
			<persName><surname>Stackoverflow</surname></persName>
		</author>
		<ptr target="https://stackoverflow.com/questions/426230/what-is-the-ld-preload-trick" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Dynamic warp subdivision for integrated branch and memory divergence tolerance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th annual international symposium on Computer architecture</title>
		<meeting>the 37th annual international symposium on Computer architecture</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">The Dual-Path Execution Model for Efficient GPU Control Flow</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Erez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">A Scalable Multi-Path Microarchitecture for Efficient GPU Control Flow</title>
		<author>
			<persName><forename type="first">A</forename><surname>Eltantawy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Aamodt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE 20th International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">SAGA-bench: Software and hardware characterization of streaming graph analytics workloads</title>
		<author>
			<persName><forename type="first">A</forename><surname>Basak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lorica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alameldeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">gRPC: A high performance, open source universal RPC framework</title>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<ptr target="https://grpc.io/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">PIN: Building customized program analysis tools with dynamic instrumentation</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Muth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lowney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">A SIMT Analyzer for Multi-Threaded CPU Applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alawneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khairy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Rogers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Accel-Sim: An Extensible Simulation Framework for Validated GPU Modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Khairy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Rogers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="473" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">A detailed and flexible cycle-accurate network-on-chip simulator</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">U</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Michelogiannakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Balfour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Towles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international symposium on performance analysis of systems and software (ISPASS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">ZSim: Fast and accurate microarchitectural simulation of thousand-core systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual International Symposium on Computer Architecture</title>
		<meeting>the 40th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">CACTI 6.0: A tool to model large caches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HP laboratories</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">?qsim: Enabling accurate and scalable simulation for interactive microservices</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delimitrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">McPAT: An integrated power, area, and timing modeling framework for multicore and manycore architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Strong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd annual ieee/acm international symposium on microarchitecture</title>
		<meeting>the 42nd annual ieee/acm international symposium on microarchitecture</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">McPAT-Calib: A Microarchitecture Power Modeling Framework for Modern CPUs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM International Conference On Computer Aided Design</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<ptr target="https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf" />
		<title level="m">A100 Tensor Core GPU architecture</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">The Tail at Scale</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kusswurm</surname></persName>
		</author>
		<title level="m">Modern X86 Assembly Language Programming</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">The ARM scalable vector extension</title>
		<author>
			<persName><forename type="first">N</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boettcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eapen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eyole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gabrielli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horsnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Magklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Premillieu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<monogr>
		<title level="m" type="main">Which are the most used web servers</title>
		<ptr target="https://www.stackscale.com/blog/top-web-servers/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">What is the python global interpreter lock (gil)</title>
		<ptr target="https://realpython.com/python-gil/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Are GPUs Non-Green Computing Devices?</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">De</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naiouf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Debunking the 100X GPU vs. CPU myth: an evaluation of throughput computing on CPU and GPU</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chhugani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Deisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chennupaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hammarlund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th annual international symposium on Computer architecture</title>
		<meeting>the 37th annual international symposium on Computer architecture</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="451" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">Computing benchmarks: Processors</title>
		<author>
			<persName><surname>Xcelerit</surname></persName>
		</author>
		<ptr target="https://www.xcelerit.com/computing-benchmarks/processors/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Comparing energy efficiency of CPU, GPU and FPGA implementations for vision kernels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Qasaimeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Denolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vissers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zambreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE international conference on embedded software and systems (ICESS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<monogr>
		<title level="m" type="main">CUDA C Programming Guide</title>
		<ptr target="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b133">
	<monogr>
		<title level="m" type="main">Programming Massively Parallel Processors: A Hands-on Approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wen-Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Optimizing CUDA Applications for NVIDIA A100 GPU</title>
		<author>
			<persName><forename type="first">G</forename><surname>Thomas-Collignon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mehta</surname></persName>
		</author>
		<ptr target="https://developer.download.nvidia.com/video/gputechconf/gtc/2020/presentations/s21819-optimizing-applications-for-nvidia-ampere-gpu-architecture.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">PCI Express 6.0 With 256GB/s Coming in 2022 Because Screw Bandwidth Constraints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hruska</surname></persName>
		</author>
		<ptr target="https://www.extremetech.com/computing/293451-pci-express-6-0-with-256gb-s-coming-in-2022" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Characterizing and evaluating a key-value store application on heterogeneous CPU-GPU systems</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Hetherington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Aamodt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Symposium on Performance Analysis of Systems &amp; Software</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Vortex: Extending the RISC-V ISA for GPGPU and 3D-Graphics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Yalamarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Elsabbagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hyesoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>in MICRO-54: 54th</note>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Simty: generalized SIMT execution on RISC-V</title>
		<author>
			<persName><forename type="first">C</forename><surname>Collange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CARRV 2017-1st Workshop on Computer Architecture Research with RISC-V</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">"</forename><surname>Spec Organization</surname></persName>
		</author>
		<author>
			<persName><surname>Specweb</surname></persName>
		</author>
		<ptr target="https://www.spec.org/web" />
		<imprint>
			<date type="published" when="2009">2009. 2009/, 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">DITVA: Dynamic inter-thread vectorization architecture</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kalathingal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Collange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Swamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Dynamic inter-thread vectorization architecture: extracting DLP from TLP</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kalathingal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Collange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Swamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 28th International Symposium on Computer Architecture and High Performance Computing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">The Vector-Thread Architecture</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krashinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Batten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hampton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gerding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pharris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 31st Annual International Symposium on Computer Architecture</title>
		<meeting>31st Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">LOOG: Improving GPU Efficiency With Light-Weight Out-Of-Order Execution</title>
		<author>
			<persName><forename type="first">K</forename><surname>Iliakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xydis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soudris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Warped-preexecution: A GPU pre-execution approach for improving latency hiding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Ro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Annavaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">HAWS: Accelerating GPU wavefront execution through selective out-of-order execution</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Architecture and Code Optimization</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Revisiting ILP designs for throughput-oriented GPGPU architecture</title>
		<author>
			<persName><forename type="first">P</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mantor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Out-of-Order Vector Architectures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Espasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 30th Annual International Symposium on Microarchitecture</title>
		<meeting>30th Annual International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Optimus Prime: Accelerating Data Transformation in Servers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pourhabibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kassir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Drumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Cerebros: Evading the RPC tax in datacenters</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pourhabibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Daglis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">E3:{Energy-Efficient} microservices on {SmartNIC-Accelerated} servers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Phothilimthana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 USENIX Annual Technical Conference (USENIX ATC 19)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Multi-execution: multicore caching for data-similar executions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Savage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Chong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 ACM/IEEE 36th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
