<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CATE: Computation-aware Neural Architecture Encoding with Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-02-14">14 Feb 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shen</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kaiqiang</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mi</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">CATE: Computation-aware Neural Architecture Encoding with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-02-14">14 Feb 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2102.07108v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent works <ref type="bibr" target="#b51">(White et al., 2020a;</ref><ref type="bibr" target="#b58">Yan et al., 2020)</ref> demonstrate the importance of architecture encodings in Neural Architecture Search (NAS). These encodings encode either structure or computation information of the neural architectures. Compared to structure-aware encodings, computation-aware encodings map architectures with similar accuracies to the same region, which improves the downstream architecture search performance <ref type="bibr" target="#b62">(Zhang et al., 2019;</ref><ref type="bibr" target="#b51">White et al., 2020a)</ref>. In this work, we introduce a Computation-Aware Transformer-based Encoding method called CATE. Different from existing computation-aware encodings based on fixed transformation (e.g. path encoding), CATE employs a pairwise pre-training scheme to learn computation-aware encodings using Transformers with cross-attention. Such learned encodings contain dense and contextualized computation information of neural architectures. We compare CATE with eleven encodings under three major encoding-dependent NAS subroutines in both small and large search spaces. Our experiments show that CATE is beneficial to the downstream search, especially in the large search space. Moreover, the outside search space experiment shows its superior generalization ability beyond the search space on which it was trained.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Neural Architecture Search (NAS) has recently drawn considerable attention <ref type="bibr" target="#b12">(Elsken et al., 2019)</ref>. While majority of the prior work focuses on either constructing new search spaces <ref type="bibr" target="#b21">(Liu et al., 2018b;</ref><ref type="bibr" target="#b33">Radosavovic et al., 2020;</ref><ref type="bibr" target="#b38">Ru et al., 2020)</ref> or designing efficient architecture search and evaluation methods <ref type="bibr" target="#b26">(Luo et al., 2018b;</ref><ref type="bibr" target="#b39">Shi et al., 2020;</ref><ref type="bibr" target="#b53">White et al., 2021)</ref>, some of the most recent work (White et al.,   1 Michigan State University 2 University of Central Florida. Correspondence to: Shen Yan &lt;yanshen6@msu.edu&gt;, Mi Zhang &lt;mizhang@egr.msu.edu&gt;. 2020a; <ref type="bibr" target="#b58">Yan et al., 2020)</ref> sheds light on the importance of architecture encoding on the subroutines in the NAS pipeline as well as on the overall performance of NAS.</p><p>While existing NAS methods use diverse architecture encoders such as LSTM <ref type="bibr" target="#b65">(Zoph et al., 2018;</ref><ref type="bibr" target="#b26">Luo et al., 2018b)</ref>, SRM <ref type="bibr" target="#b1">(Baker et al., 2018)</ref>, MLP <ref type="bibr" target="#b20">(Liu et al., 2018a;</ref><ref type="bibr" target="#b48">Wang et al., 2020)</ref>, GCN <ref type="bibr" target="#b50">(Wen et al., 2020;</ref><ref type="bibr" target="#b39">Shi et al., 2020)</ref> or adjacency matrix itself <ref type="bibr" target="#b15">(Kandasamy et al., 2018;</ref><ref type="bibr" target="#b37">Real et al., 2019;</ref><ref type="bibr" target="#b52">White et al., 2020b)</ref>, these encoders encode either structures <ref type="bibr" target="#b26">(Luo et al., 2018b;</ref><ref type="bibr" target="#b60">Ying et al., 2019;</ref><ref type="bibr" target="#b48">Wang et al., 2020;</ref><ref type="bibr" target="#b50">Wen et al., 2020;</ref><ref type="bibr" target="#b39">Shi et al., 2020;</ref><ref type="bibr" target="#b58">Yan et al., 2020)</ref> or computations <ref type="bibr" target="#b62">(Zhang et al., 2019;</ref><ref type="bibr" target="#b28">Ning et al., 2020;</ref><ref type="bibr" target="#b53">White et al., 2021)</ref> of the neural architectures. Compared to structure-aware encodings, computation-aware encodings are able to map architectures with different structures but similar accuracies to the same region. This advantage contributes to a smooth encoding space with respect to the actual architecture performance instead of structures, which improves the efficiency of the downstream architecture search <ref type="bibr" target="#b62">(Zhang et al., 2019;</ref><ref type="bibr">2020;</ref><ref type="bibr" target="#b51">White et al., 2020a)</ref>.</p><p>We argue that current architecture encoders limit the power of computation-aware architecture encoding for NAS. The major limitations lie in their representation power and the effectiveness of their pre-training objectives. Specifically, <ref type="bibr" target="#b62">(Zhang et al., 2019)</ref> uses shallow GRUs to encode computation, which is not sufficient to capture deep contextualized computation information. Moreover, their decoder is trained with the reconstruction loss via asynchronous message passing. This is very challenging in practice because directly learning the generative model based on a single architecture is not trivial. As a result, its pre-training is less effective and the downstream NAS performance is not as competitive as state-of-the-art structure-aware encoding methods. <ref type="bibr" target="#b51">(White et al., 2020a)</ref> proposes a computation-aware encoding method based on fixed transformation called path encoding, which shows outstanding performance under the predictor-based NAS subroutine. However, path encoding scales exponentially without truncation and it inevitably causes information loss with truncation. Moreover, path encoding exhibits worse generalization performance in outside search space compared to the adjacency matrix encoding since it could not generalize to unseen paths that are not included in the training search space.</p><p>Figure <ref type="figure">1</ref>. Overview of CATE. CATE takes computationally similar architecture pairs as the input. The model is trained to predict masked operators given the pairwise computational information. Apart from the cross-attention blocks, the pretrained Transformer encoder is used to extract architecture encodings for the downstream encoding-dependent NAS subroutines.</p><p>In this work, we propose a new computation-aware neural architecture encoding method named CATE (Computation-Aware Transformer-based Encoding) that alleviates the limitations of existing computation-aware encoding methods. As shown in Figure <ref type="figure">1</ref>, CATE takes paired computationally similar architectures as its input. Similar to BERT, CATE trains the Transformer-based model <ref type="bibr" target="#b46">(Vaswani et al., 2017)</ref> using the masked language modeling (MLM) objective <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>. Each input architecture pair is corrupted by replacing a fraction of their operators with a special mask token. The model is trained to predict those masked operators from the corrupted architecture pair. CATE differs from BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> in two aspects. First, we could not use the fully-visible attention mask <ref type="bibr" target="#b34">(Raffel et al., 2020)</ref> for architecture representation learning because it does not reflect the single-directional flow of neural architectures <ref type="bibr" target="#b55">(Xie et al., 2019;</ref><ref type="bibr" target="#b61">You et al., 2020)</ref>. Therefore, instead of using a bidirectional Transformer encoder as in BERT, CATE directly uses the adjacency matrix to compute the causal mask <ref type="bibr" target="#b34">(Raffel et al., 2020)</ref>. The adjacency matrix is further augmented with the Floyd algorithm <ref type="bibr" target="#b13">(Floyd, 1962)</ref> to encode the long-range dependency of different operations. Second, each prediction in LMs has its inductive bias given the contextual information from different positions. This, however, is not the case in architecture representation learning since the prediction distribution is uniform for any valid graph. Therefore, we propose a pairwise pre-training scheme that encodes computationally similar architecture pairs through two Transformers with shared parameters. The two individual encodings are then concatenated, and the concatenated encoding is imported into another Transformer with a cross-attention encoder to encode the joint information of the architecture pair. Together with the MLM objective, CATE is able to encode the computation of architectures and learn dense and deep contextualized architecture representations that contain both local and global computation information in neural architectures. This is important for architecture encodings to be generalized to outside search space beyond the training search space.</p><p>We compare CATE with eleven status quo structure-aware and computation-aware architecture encoding methods under three major encoding-dependent subroutines as well as eight status quo NAS algorithms on NAS-Bench-101 <ref type="bibr" target="#b60">(Ying et al., 2019)</ref> (small), NAS-Bench-301 <ref type="bibr" target="#b40">(Siems et al., 2020)</ref> (large), and an outside search space <ref type="bibr" target="#b51">(White et al., 2020a)</ref> to evaluate the effectiveness, scalability, and generalization ability of CATE. Our results show that CATE is beneficial to the downstream architecture search, especially in the large search space. Specifically, we found the strongest NAS performance in all search spaces using CATE within a Bayesian optimization-based predictor subroutine together with a novel computation-aware search. Moreover, the outside search space experiment shows its superior generalization ability beyond the search space on which it was trained. Our ablation studies show that the quality of CATE encodings and downstream NAS performance are non-decreasingly improved with more training architecture pairs, more cross-attention Transformer blocks and larger dimension of the feed-forward layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Neural Architecture Search (NAS) NAS has been started with genetic algorithms <ref type="bibr" target="#b27">(Miller et al., 1989;</ref><ref type="bibr" target="#b17">Kitano, 1990;</ref><ref type="bibr" target="#b43">Stanley &amp; Miikkulainen, 2002)</ref> and recently becomes popular when <ref type="bibr" target="#b64">(Zoph &amp; Le, 2017;</ref><ref type="bibr" target="#b0">Baker et al., 2017)</ref> gain significant attention. Since then, various NAS methods including random search <ref type="bibr" target="#b19">(Li &amp; Talwalkar, 2019)</ref>, evolutionary algorithms <ref type="bibr" target="#b37">(Real et al., 2019;</ref><ref type="bibr" target="#b44">Stanley et al., 2019)</ref>, reinforcement learning <ref type="bibr" target="#b65">(Zoph et al., 2018;</ref><ref type="bibr" target="#b31">Pham et al., 2018;</ref><ref type="bibr" target="#b45">Tan et al., 2019)</ref>, Bayesian optimization <ref type="bibr" target="#b41">(Snoek et al., 2015;</ref><ref type="bibr" target="#b42">Springenberg et al., 2016;</ref><ref type="bibr" target="#b15">Kandasamy et al., 2018)</ref>, Monte Carlo Tree Search <ref type="bibr" target="#b48">(Wang et al., 2020;</ref><ref type="bibr">2019)</ref>, gradient decent <ref type="bibr" target="#b2">(Bender et al., 2018;</ref><ref type="bibr" target="#b22">Liu et al., 2019a;</ref><ref type="bibr" target="#b26">Luo et al., 2018b;</ref><ref type="bibr" target="#b5">Cai et al., 2019;</ref><ref type="bibr" target="#b57">Yan et al., 2019;</ref><ref type="bibr" target="#b7">Chu et al., 2019)</ref>, neural predictor <ref type="bibr" target="#b1">(Baker et al., 2018;</ref><ref type="bibr" target="#b50">Wen et al., 2020;</ref><ref type="bibr" target="#b58">Yan et al., 2020;</ref><ref type="bibr" target="#b39">Shi et al., 2020;</ref><ref type="bibr" target="#b53">White et al., 2021)</ref> and local search <ref type="bibr" target="#b29">(Ottelander et al., 2020;</ref><ref type="bibr" target="#b52">White et al., 2020b)</ref> have been proposed. For comprehensive surveys, please refer to <ref type="bibr" target="#b12">(Elsken et al., 2019;</ref><ref type="bibr" target="#b54">Xie et al., 2020)</ref>.</p><p>Neural Architecture Encoding Majority of the existing NAS work use adjacency matrix to encode the structures of neural architectures. However, adjacency matrix-based encoding grows quadratically as the search space scales up. <ref type="bibr" target="#b60">(Ying et al., 2019)</ref> propose categorical adjacency matrixbased encoding to ensure a fixed length encoding. They also propose continuous adjacency matrix-based encoding that is similar to DARTS <ref type="bibr" target="#b22">(Liu et al., 2019a)</ref>, where the architecture is created by taking fixed number of edges with the highest continuous values. However, this approach is not easily applicable to some NAS algorithms such as regularized evolution <ref type="bibr" target="#b36">(Real et al., 2017)</ref> without major changes. Recent NAS methods <ref type="bibr" target="#b25">(Luo et al., 2018a;</ref><ref type="bibr" target="#b48">Wang et al., 2020;</ref><ref type="bibr" target="#b50">Wen et al., 2020;</ref><ref type="bibr" target="#b39">Shi et al., 2020;</ref><ref type="bibr" target="#b58">Yan et al., 2020)</ref> use adjacency matrix as the input to LSTM/MLP/GNN to encode the structures of neural architectures in the latent space. One disadvantage of these methods is that structure encodings may not be computationally unique unless some certain graph hashing is applied <ref type="bibr" target="#b60">(Ying et al., 2019;</ref><ref type="bibr" target="#b28">Ning et al., 2020)</ref>. <ref type="bibr" target="#b53">(White et al., 2021;</ref><ref type="bibr" target="#b49">Wei et al., 2020)</ref> use path encoding and its categorical and continuous variants, which encode computation of architectures so that isomorphic cells are mapped to the same encoding. <ref type="bibr" target="#b62">(Zhang et al., 2019)</ref> uses GRU-based asynchronous message passing to encode com-putation of architectures and the model is trained with the VAE loss <ref type="bibr" target="#b16">(Kingma &amp; Welling, 2014)</ref>. CATE is inspired by the advantage of computation encoding and addresses the drawbacks of <ref type="bibr" target="#b62">(Zhang et al., 2019;</ref><ref type="bibr" target="#b53">White et al., 2021)</ref>.</p><p>Context Dependency Our work is close to self-supervised learning in language models (LMs) <ref type="bibr" target="#b9">(Dong et al., 2019)</ref>. In particular, ELMo <ref type="bibr" target="#b30">(Peters et al., 2018)</ref> uses two shallow unidirectional LSTMs <ref type="bibr" target="#b14">(Hochreiter &amp; Schmidhuber, 1997)</ref> to encode bidirectional text information, which is not sufficient for modeling deep interactions between the two directions. GPT-2 <ref type="bibr" target="#b32">(Radford et al., 2019)</ref> proposes an autoregressive language modeling with Transformer <ref type="bibr" target="#b46">(Vaswani et al., 2017)</ref> to cover the left-to-right dependency and is further generalized by XLNet <ref type="bibr" target="#b59">(Yang et al., 2019)</ref> which encodes bidirectional context. (Ro)BERT/BART/T5 <ref type="bibr" target="#b8">(Devlin et al., 2019;</ref><ref type="bibr" target="#b23">Liu et al., 2019b;</ref><ref type="bibr" target="#b18">Lewis et al., 2020;</ref><ref type="bibr" target="#b34">Raffel et al., 2020)</ref> use bidirectional Transformer encoder to encode both left and right context. In architecture representation learning, the attention mask in the encoder cannot be used to attend to all the operators because it does not reflect the single-directional flow (e.g. directed, acyclic, single-in-single-out) of the computational graphs <ref type="bibr" target="#b55">(Xie et al., 2019;</ref><ref type="bibr" target="#b61">You et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CATE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Search Space</head><p>We restrict our search space to the cell-based architectures. Following the configuration in <ref type="bibr" target="#b60">(Ying et al., 2019)</ref>, each cell is a labeled directed acyclic graph (DAG) G = (V, E), with V as a set of N nodes and E as a set of edges that connect the nodes. Each node v i ∈ V, i ∈ [1, N ] is associated with an operation selected from a predefined set of V operations, and the edges between different nodes are represented as an upper triangular binary adjacency matrix A ∈ {0, 1} N ×N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Computation-aware Neural Architecture Encoder</head><p>Our proposed computation-aware neural architecture encoder is built upon the Transformer encoder architecture which consists of a semantic embedding layer and L Transformer blocks stacked on top. Given G, each operation v i is first fed into a semantic embedding layer of size d e :</p><formula xml:id="formula_0">Emb i = Embedding(v i )<label>(1)</label></formula><p>The embedded vectors are then contextualized at different levels of abstract. We denote the hidden state after l-th layer as</p><formula xml:id="formula_1">H l = [H l 1 , ..., H l N ] of size d h</formula><p>, where H l = T (H l−1 ) and T is a transformer block containing n head heads. The l-th Transformer block is calculated as:</p><formula xml:id="formula_2">Q k = H l−1 W l qk , K k = H l−1 W l kk , V k = H l−1 W l vk (2)</formula><p>Algorithm 1 Floyd Algorithm 1: Input: the node set V, the adjacent matrix A</p><formula xml:id="formula_3">2: Ã ← A 3: for k ∈ V do 4: for i ∈ V do 5: for j ∈ V do 6: Ãi,j | = Ãi,k &amp; Ãk,j 7: Output: Ã Ĥl k = softmax( Q k K T k √ d h + M)V k (3) Ĥl = concatenate( Ĥl 1 , Ĥl 2 , . . . , Ĥl n head )<label>(4)</label></formula><formula xml:id="formula_4">H l = ReLU( Ĥl W 1 + b 1 )W 2 + b 2 (5)</formula><p>where the initial hidden state</p><formula xml:id="formula_5">H 0 i is Emb i , thus d e = d h . Q k , K k , V k stand</formula><p>for "Query", "Key" and "Value" in the attention operation of the k-th head respectively. M is the attention mask in the Transformer, where M i,j ∈ {0, −∞} indicates whether operation j is a dependent operation of operation i. W 1 ∈ R dc×d f f and W 2 ∈ R d f f ×dc denote the weights in the feed-forward layer.</p><p>Direct/Indirect Dependency Mask A pair of nodes (operations) within an architecture are dependent if there is either a directed edge that directly connects them (local dependency) or a path made of a series of such edges that indirectly connects them (long-range dependency). We create dependency masks for such pairs of nodes for both direct and indirect cases and use these dependency masks as the attention masks in the Transformer. Specifically, the direct dependency mask M Direct and the indirect dependency mask M Indirect can be created as follows:</p><formula xml:id="formula_6">M Direct i,j = 0, if A i,j = 1 −∞, if A i,j = 0 M Indirect i,j = 0, if Ãi,j = 1 −∞, if Ãi,j = 0</formula><p>where A is the adjacency matrix and Ã = Floyed(A) is derived using Floyd algorithm in Algorithm 1.</p><p>Uni/Bidirectional Encoding Finally, the final hidden vector H l N is used as the unidirectional encoding for the architecture. We also considered encoding the architecture in a bidirectional manner, where both the output node hidden vector from the original DAG and the input node hidden vector from the reversed one are extracted and then concatenated together. However, our experiments show that bidirectional encoding performs worse than unidirectional encoding. We include this result in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pre-training CATE</head><p>Architecture Pair Sampling We split the dataset into 95% training and 5% held-out test sets for our pairwise pretraining. To ensure that it does not scale with quadratic time complexity, we first sort the architectures based on their computational attributes P (e.g. number of parameters, FLOPs). We then employ a sliding window for each architecture x i and its neighborhood r(x i ) = {y : |P(x i ) − P(y)| &lt; δ}, where δ is a hyperparameter for the pairwise computation constraint. Finally, we randomly select K distinct architectures Y = {y 1 , . . . , y K }, x i / ∈ Y, Y ⊂ r(x i ) within the neighborhood to compose K architecture pairs {(x i , y 1 ), . . . , (x i , y K )} for architecture x i .</p><p>Pairwise Pre-training with Cross-Attention Once the computationally similar architecture pair is composed, we randomly select 20% operations from each architecture within the pair for masking, where 80% of them are replaced with a [M ASK] token and the remaining 20% are replaced with a random token chosen from the predefined operation set. We apply padding to architectures that have nodes less than the maximum number of nodes N in one batch to handle variable length inputs. The joint representation H L XY is derived by concatenating H L X and H L Y followed by the summation of the corresponding segment embedding. Segment embedding acts as an identifier of different architectures during pre-training. We set it to be trainable and randomly initialized. The joint representation H L XY is then contextualized with another L c -layer Transformer with the cross-attention mask M c such that segments from the two architectures can attend to each other given the pairwise information. For example, given two architectures X with three nodes and Y with four nodes in Figure <ref type="figure">1</ref>, X has access to the non-padded nodes of Y and itself, and same for Y . The cross-attention dimension of the encoder is denoted as d c . The joint representation of the last layer is used for prediction. The model is trained by minimizing the cross-entropy loss computed using the predicted operations and the original operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Encoding-dependent NAS Subroutines</head><p>(White et al., 2020a) identifies three major encodingdependent subroutines included in existing NAS algorithms: sample random architecture, perturb architecture, and train predictor model. The sample random architecture subroutine includes random search <ref type="bibr" target="#b19">(Li &amp; Talwalkar, 2019)</ref>. The perturb architecture subroutine includes regularized evolution (REA) <ref type="bibr" target="#b37">(Real et al., 2019)</ref> and local search (LS) <ref type="bibr" target="#b52">(White et al., 2020b)</ref>. The train predictor model subroutine includes neural predictor <ref type="bibr" target="#b50">(Wen et al., 2020;</ref><ref type="bibr" target="#b39">Shi et al., 2020;</ref><ref type="bibr" target="#b53">White et al., 2021)</ref>, Bayesian optimization with Gaussian process (GP) <ref type="bibr" target="#b35">(Rasmussen &amp; Williams, 2006)</ref>, and Bayesian optimization with neural networks (DNGO) <ref type="bibr" target="#b41">(Snoek et al., 2015)</ref> which is much faster to fit compared to GP and scales linearly with large datasets rather than cubically.</p><p>Inspired by <ref type="bibr" target="#b29">(Ottelander et al., 2020;</ref><ref type="bibr" target="#b52">White et al., 2020b)</ref>, we found that LS (perturb architecture) can be combined with DNGO (train predictor model). We thus propose a DNGO-based computation-aware search using CATE called CATE-DNGO-LS. Specifically, we maintain a pool of sampled architectures and take iterations to add new ones. For each iteration, we pass all architecture encodings to the predictor trained 100 epochs with samples in current pool. We select new architectures with top-5 predicted accuracy and add them to the pool. Assume there are totally M new architectures which become the new top-5 in the updated pool. We then select the nearest neighbors of the other (5-M) top-5 architectures in L2 distance in latent space and add them to the pool. Hence, there will be 5 to 10 new architectures added to the pool in each iteration. The search stops when the number of samples reaches a pre-defined budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct our experiments on two search spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NAS-Bench-101</head><p>The NAS-Bench-101 search space <ref type="bibr" target="#b60">(Ying et al., 2019)</ref> consists of approximately 420K architectures. Each architecture has its pre-computed validation and test accuracies on CIFAR-10. The cell includes up to 7 nodes and at most 9 edges with the first node as input and the last node as output. The intermediate nodes can be either 1×1 convolution, 3×3 convolution, or 3×3 max pooling. We use the number of network parameters as the computational attribute P for architecture pair sampling. We set δ to 2, 000, 000 and K to 2. The ablation studies on δ and K are summarized in Section 4.4. We split the dataset into 95% training and 5% held-out test sets for pre-training.</p><p>NAS-Bench-301 NAS-Bench-301 <ref type="bibr" target="#b40">(Siems et al., 2020)</ref> is a new surrogate benchmark on the DARTS <ref type="bibr" target="#b22">(Liu et al., 2019a)</ref> search space that is much larger than NAS-Bench-101. It uses a surrogate model to estimate the performance of roughly 10 18 architectures without exhaustively evaluating the entire search space. To convert the DARTS search space into one with the same input format as NAS-Bench-101, we add a summation node to make nodes represent operations and edges represent data flow. More details about the DARTS/NAS-Bench-301 and a cell transformation example are included in Appendix. We randomly sample 1, 000, 000 architectures in this search space, and use the same data split used in NAS-Bench-101 for pre-training. We use network FLOPs as the computational attribute P for architecture pair sampling. We set δ to 5, 000, 000 and K to 1. Because some NAS methods we compare against use the same GNN surrogate model used in NAS-Bench-301, to ensure fair comparison, we thus followed <ref type="bibr" target="#b40">(Siems et al., 2020)</ref> to use NB301-XGB-v1.0 which utilizes gradient boosted trees as the regression model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model and Training</head><p>We use a L = 12 layer Transformer encoder and a L c = 24 layer cross-attention Transformer encoder, each has 8 attention heads. The hidden state size is d h = d c = 64 for all the encoders. The hidden dimension is d f f = 64 for all the feed-forward layer. We employ AdamW <ref type="bibr" target="#b24">(Loshchilov &amp; Hutter, 2019)</ref> as our optimizer. The initial learning rate is 1e-3. The momentum parameters are set to 0.9 and 0.999. The weight decay is 0.01 for regular layer and 0 for dropout and layer normalization. We trained our model for 10 epochs with batch size of 1024 on NVIDIA Quadro RTX 8000 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with Different Encoding Schemes</head><p>In our first experiment, we compare CATE with eleven status quo architecture encoding schemes under three major encoding-dependent subroutines described in Section 3.4 on NAS-Bench-101. These encoding schemes include (1-3) one-hot/categorical/continuous adjacency matrix encoding <ref type="bibr" target="#b60">(Ying et al., 2019)</ref>, (4-6) one-hot/categorical/continuous path encoding and (7-9) their corresponding truncated counterparts <ref type="bibr" target="#b53">(White et al., 2021)</ref>, (10) D-VAE <ref type="bibr" target="#b62">(Zhang et al., 2019)</ref>, and (11) arch2vec <ref type="bibr" target="#b58">(Yan et al., 2020)</ref>. For continuous encodings, we use L2 distance as the distance metric. To examine the effectiveness of the encoding schemes themselves, we compare different encoding schemes under the same search subroutine.</p><p>Figure <ref type="figure">2</ref> illustrates our results. For each subroutine, we show the top-five best-performing encoding schemes. Overall, despite there is no overall best encoding, we found that CATE is among the top five across all the subroutines. Specifically, for sample random architecture subroutine, random search using adjacency matrix encoding performs the best. The random search using continuous encodings performs slightly worse than the adjacency encodings possibly due to the discretization loss from vector space into a fixed number of bins of same size before the random sampling.</p><p>For perturb architecture subroutine, CATE is on par with or outperforms adjacency encoding and path encoding because it is pre-trained to preserve strong computation locality information. This advantage allows the evolution or local search to find architectures with similar performance in local neighborhood more easily. Interestingly, we observe very small deviation using local search with CATE. This indicates that it always converges to some certain local minimums across different initial seeds. This is because NAS-Bench-101 already exhibits locality in edit distance, and encoding computation makes architectures even closer in terms of accuracy and thus benefits the local search.  final search performance given the sample budgets. 2) Local search (LS) is a strong baseline in both small and large search spaces. As mentioned in Section 4.1, performing LS using CATE leads to better results compared to other encodings. 3) NAS algorithms that use more than one encodingdependent subroutine in general perform better than NAS algorithms with just one subroutine. Specifically, BOGCN and BANANAS that have multiple subroutines perform better than the single-subroutine NAS algorithms such as REA, DNGO, and BOHAMIANN. Moreover, CATE-DNGO-LS leads to the best performing result in both NAS-Bench-101 and NAS-Bench-301 search spaces. Meanwhile, the improvement of CATE-DNGO-LS versus CATE-DNGO shrinks in larger search space, indicating that the larger search space is more challenging to encode.</p><p>NAS-Bench-301 uses a surrogate model trained on 60k architectures to predict the performance of all the other architectures in the DARTS search space. The performance of the other architectures, however, can be inaccurate. Given that, we further validate the effectiveness of CATE-DNGO-LS in the actual DARTS search space by training the queried architectures from scratch. We set the budget to 100 queries. Each queried architecture is trained for 50 epochs with a batch size of 96, using 32 initial channels and 8 cell layers.</p><p>The average validation error of the last 5 epochs is computed as the label. These values are chosen to be close to the proxy model used in DARTS. It takes about 3.3 GPU days to finish the search. To ensure fair comparison, we compare CATE-DNGO-LS to methods <ref type="bibr" target="#b22">(Liu et al., 2019a;</ref><ref type="bibr" target="#b19">Li &amp; Talwalkar, 2019;</ref><ref type="bibr" target="#b58">Yan et al., 2020;</ref><ref type="bibr" target="#b53">White et al., 2021)</ref> that use the common test evaluation script which is to train for 600 epochs with cutout and auxiliary tower.  <ref type="bibr" target="#b19">(Li &amp; Talwalkar, 2019)</ref> 3.29 ± 0.15 3.2 4 DARTS <ref type="bibr" target="#b22">(Liu et al., 2019a)</ref> 2.76 ± 0.09 3.3 4 BANANAS <ref type="bibr" target="#b53">(White et al., 2021)</ref> 2.67 ± 0.07 3.6 10.2 arch2vec-BO <ref type="bibr" target="#b58">(Yan et al., 2020)</ref> 2.56 ± 0.05 3.6 9.2 CATE-DNGO-LS (ours)</p><p>2.56 ± 0.08 2.9 3.3</p><p>Table <ref type="table" target="#tab_1">2</ref>. NAS results in DARTS search space using CIFAR-10.</p><p>search space with much less parameters and search cost. This is consistent with our observation in NAS-Bench-301.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Generalization to Outside Search Space</head><p>In our third experiment, inspired by <ref type="bibr" target="#b51">(White et al., 2020a)</ref>, we evaluate the generalization ability of CATE beyond the search space on which it was trained. The training search space is designed as a subset of NAS-Bench-101, where each included architecture has 2 to 6 nodes and 1 to 7 edges. The test search space is disjointed from the training search space and includes architectures with 6 nodes and 7 to 9 edges. There are 10, 026 and 60, 669 non-isomorphic graphs in the training and test space respectively. The CATE encodings are pre-trained using the training space and are used to conduct architecture search in the test space. We compare CATE with the adjacency matrix encoding because it was shown in <ref type="bibr" target="#b51">(White et al., 2020a)</ref> to have the best generalization ability compared to other encodings. A simple 2-layer MLP with hidden size 128 is used as the neural predictor for both encodings.</p><p>Figure <ref type="figure" target="#fig_1">4</ref> shows the validation error curve of the test search space given the number of 150 sample budget across 500 independent runs. As shown, CATE outperforms adjacency matrix encoding by a large margin. This indicates that CATE can better contextualize the computation information compared to fixed encodings, which generalizes better when adapting to outside search space. Moreover, the padding number of samples  <ref type="table" target="#tab_2">3</ref>. We found that strong computation locality (i.e. small δ) usually leads to better results. The choice of neighborhood size K does not have a significant effect on NAS performance. Therefore, we choose small K for faster pretraining. For NAS-Bench-301, we use the FLOPs as the computational attributes P and observe the same trend as in NAS-Bench-101 on the selection of δ and K. We report the results in Appendix. Transformer Hyperparameters We studied the effect of the number of cross-attention Transformer blocks L c and the hidden dimension of the feed-forward layer d f f on CATE. We fix δ and K for pre-training as mentioned in Section  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we presented CATE, a new computation-aware architecture encoding method based on Transformers. Unlike encodings with fixed transformations, we show that the computation information of neural architectures can be contextualized through a pairwise learning scheme trained with MLM. Our experimental results show its effectiveness and scalability along with three major encoding-dependent NAS subroutines in both small and large search spaces. We also show its superior generalization ability outside the training search space. We anticipate that the methods presented in this work can be extended to encode even larger search spaces (e.g. TuNAS <ref type="bibr" target="#b3">(Bender et al., 2020)</ref>) to study the effectiveness of different downstream NAS algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Uni/Bidirectional Encoding</head><p>As mentioned in Section 3.2, we also considered encoding the architecture in a bidirectional manner where both the output node hidden vector from the original DAG and the input node hidden vector from the reversed one are extracted and then concatenated together. Note that d c in the crossattention Transformer encoder will be doubled due to the concatenation. We compare the results of unidirectional and bidirectional encodings in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Architecture Pair Sampling Hyperparameters</head><p>As mentioned in Section 4.4, we randomly sample 1,000,000 architectures in NAS-Bench-301 for pretraining.</p><p>We use the same proxy model configuration (i.e. 100 training epochs, 32 initial channels, 8 cell layers) as used in NAS-Bench-301 to compute the model FLOPs. We plot the histogram of model FLOPs of the sampled architectures in Figure <ref type="figure" target="#fig_3">6</ref>. Given that, we experiment different δ and K and summarize the downstream NAS results in Table <ref type="table" target="#tab_9">7</ref>. Similar to our reported results on NAS-Bench-101, we find that strong locality leads to better results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Corruption Rate</head><p>By default, we randomly select 20% operations from each architecture within the pair for masking in the pairwise pre- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. NAS-Bench-301 Benchmark</head><p>NAS-Bench-301 <ref type="bibr" target="#b40">(Siems et al., 2020)</ref> is the first surrogate NAS benchmark to cover the large-scale DARTS search space <ref type="bibr" target="#b22">(Liu et al., 2019a)</ref>. The DARTS search space consists of two cells: a convolutional cell and a reduction cell, each with six nodes. For each cell, the first two nodes are the outputs from the previous two cells. The next four nodes contain two edges as input, creating a DAG. In total, there are roughly 10 18 DAGs without considering graph isomorphism, which is a much larger search space compared to NAS-Bench-101 <ref type="bibr" target="#b60">(Ying et al., 2019)</ref> and NAS-Bench-201 <ref type="bibr" target="#b10">(Dong &amp; Yang, 2020)</ref>.</p><p>NAS-Bench-301 is fully trained on around 60k architectures collected by unbiased architecture sampling using random search as well as biased and dense architecture sampling in high-performance regions using different training hyperparameters (including training time, number of parameters, and number of multiply-adds). It trains various regression models such as Random Forest (RF) <ref type="bibr" target="#b4">(Breiman, 2001)</ref>, Support Vector Regression (SVR) <ref type="bibr" target="#b11">(Drucker et al., 1997)</ref>, Graph Isomorphism Network (GIN) <ref type="bibr" target="#b56">(Xu et al., 2019)</ref> and Treebased gradient boosting model (e.g. XGBoost (XGB) <ref type="bibr" target="#b6">(Chen &amp; Guestrin, 2016)</ref>, LGBoost (LGB) <ref type="bibr" target="#b15">(Ke et al., 2017)</ref>) to predict the accuracies of unseen architectures. The three 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 best-performing models (GIN, XGB, LGB) are used to predict the search trajectories in the benchmark API.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Cell Transformation</head><p>To transform the DARTS search space into one with the same input format as NAS-Bench-101, we additionally add a summation node to make nodes to represent operations and edges to represent data flow. For example, if there is an edge from node A to node B with operation O, we create an additional node P, remove the edge A, B , and add 2 edges A, P and P, B . The operation on node P is set to be O. Given that, a 15 × 15 upper-triangular binary matrix is used to encode edges and a 15 × 11 operation matrix is used to encode operations with the order of {c k−2 , c k−1 , 3 × 3 max-pool, 3 × 3 average-pool, skip connect, 3 × 3 separable conv, 5 × 5 separable conv, 3 × 3 dilated conv, 5 × 5 dilated conv, sum, c k }. Following NAS-Bench-301, we do not include zero operator. Following <ref type="bibr" target="#b20">(Liu et al., 2018a)</ref>, we use the same cell for both normal and reduction cells. An example of cell transformation is shown in Figure <ref type="figure" target="#fig_4">7</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Comparison between CATE and SOTA NAS methods on NAS-Bench-101 (left) and NAS-Bench-301 (right). It reports the test error of 200 independent runs. The number of queried architectures is set to 150 for NAS-Bench-101 and 100 for NAS-Bench-301.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Performance on the out-of-training search space. It reports the validation error of 500 independent runs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Histogram of model parameters on NAS-Bench-101.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Histogram of model FLOPs on the sampled 1, 000, 000 architectures of NAS-Bench-301.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. A cell transformation example in DARTS search space. The top panel shows the cell. The bottom-left and bottom-right panels show its corresponding adjacency matrix and operation matrix respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison</figDesc><table><row><cell>test error [%]</cell><cell>6.5 6.6 6.7 6.8 6.9 7.0</cell><cell cols="3">Sample Random Arch:Random Search adjacency path cont_adj cont_path cate</cell><cell>test error [%]</cell><cell>6.4 6.6 6.8 7.0</cell><cell cols="3">Perturb Arch:Regularized Evolution adjacency path trunc_path cat_path cate</cell><cell>test error [%]</cell><cell>6.2 6.3 6.4 6.5 6.6 6.7</cell><cell>Perturb Arch:Local Search adjacency cont_adj path trunc_path cate</cell></row><row><cell></cell><cell>6.3 6.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6.2</cell><cell></cell><cell></cell><cell>6.0 6.1</cell></row><row><cell></cell><cell></cell><cell>20</cell><cell>40</cell><cell>60 number of samples 80 100 120 140</cell><cell></cell><cell>20</cell><cell>40</cell><cell cols="2">60 number of samples 80 100 120 140</cell><cell>20</cell><cell>40</cell><cell>60 number of samples 80 100 120 140</cell></row><row><cell></cell><cell></cell><cell cols="3">Train Predictor Model:Neural Predictor</cell><cell></cell><cell cols="4">Train Predictor Model:Bayesian Optimization (GP)</cell><cell>Train Predictor Model:Bayesian Optimization (DNGO)</cell></row><row><cell>test error [%]</cell><cell>6.4 6.6 6.8 7.0 7.2 7.4 7.6</cell><cell></cell><cell></cell><cell>dvae cat_adj cont_adj trunc_path cate</cell><cell>test error [%]</cell><cell>6.2 6.4 6.6 6.8</cell><cell></cell><cell></cell><cell>adjacency cont_adj path trunc_path cate</cell><cell>test error [%]</cell><cell>6.2 6.4 6.6 6.8</cell><cell>adjacency path trunc_path arch2vec cate</cell></row><row><cell></cell><cell>6.0 6.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>6.0</cell><cell></cell><cell></cell><cell>6.0</cell></row><row><cell></cell><cell></cell><cell>20</cell><cell>40</cell><cell>60 number of samples 80 100 120 140</cell><cell></cell><cell>20</cell><cell>40</cell><cell cols="2">60 number of samples 80 100 120 140</cell><cell>20</cell><cell>40</cell><cell>60 number of samples 80 100 120 140</cell></row><row><cell cols="9">For train predictor model subroutine, we have four observa-tions: 1) Adjacency matrix encodings perform less effective</cell><cell>Prev. SOTA (White et al., 2021) CATE-DNGO-LS (ours)</cell><cell>NAS-Bench-101 NAS-Bench-301 5.92 5.35 5.88 5.28</cell></row><row><cell cols="9">with neural predictor and DNGO. This is because edit dis-</cell></row><row><cell cols="9">tance cannot fully reflect the closeness of architectures w.r.t</cell></row><row><cell cols="9">their actual performance. 2) Path encoding performs well</cell></row><row><cell cols="9">with neural predictor but worse than other encodings with</cell></row><row><cell cols="9">Bayesian optimization. 3) D-VAE and arch2vec, two en-</cell></row><row><cell cols="9">codings learned via variational autoencoding, perform well</cell></row><row><cell cols="9">only with some certain NAS methods. It could be attributed</cell></row><row><cell cols="9">to their challenging training objective which easily leads to</cell></row><row><cell cols="9">overfitting. 4) CATE is competitive with neural predictor and outperforms all the other encodings with Bayesian opti-mization. This is because neighboring computation-aware encodings correspond with similar accuracies. Moreover, the training objective in CATE is more effective compared to the standard VAE loss used by D-VAE and arch2vec.</cell><cell>and two NAS algorithms that contain more than one encoding-dependent subroutine: BOGCN (Shi et al., 2020) (perturb arch., train predictor) and BANANAS (White et al., 2021) (sample random arch., perturb arch., train predictor). We compare these eight existing NAS algorithms with CATE-</cell></row><row><cell cols="9">4.2. Comparison with State-of-the-Art NAS Methods</cell><cell>DNGO: a NAS algorithm based on CATE encodings with the DNGO subroutine (train predictor), and CATE-DNGO-</cell></row><row><cell cols="9">In our second experiment, we compare the neural architec-ture search performance based on CATE encodings with state-of-the-art NAS algorithms on NAS-Bench-101 and</cell><cell>LS: a NAS algorithm based on CATE encodings with the combination of DNGO and LS subroutines (train predictor, perturb arch.) as described in Section 3.4.</cell></row></table><note>Figure 2. Comparison between CATE and other architecture encoding schemes under different subroutines on NAS-Bench-101: sample random architecture (top left), perturb architecture (top middle, top right), and train predictor model (bottom left, bottom middle, bottom right). It reports the test error of 200 runs given 150 queried architectures. NAS-Bench-301. Existing NAS algorithms contain one or more encoding-dependent subroutines. We consider six NAS algorithms that contain one encoding-dependent subroutine: random search (RS) (Li &amp; Talwalkar, 2019) (sample random arch.), regularized evolution (REA) (Real et al., between CATE and state-of-the-arts: Final test error [%] given 150 queried architectures on NAS-Bench-101 and 100 queried architectures on NAS-Bench-301. The result is averaged over 200 independent runs. 2019) (perturb arch.), local search (LS) (White et al., 2020b) (perturb arch.), DNGO (Snoek et al., 2015) (train predictor), BOHAMIANN (Springenberg et al., 2016) (train predictor), arch2vec-DNGO (Yan et al., 2020) (train predictor),Figure3and Table1summarize our results. We have three major findings from Figure3: 1) Architecture encoding matters especially in the large search space. The right plot shows that CATE-DNGO and CATE-DNGO-LS in DARTS search space not only converge faster but also lead to better</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">Avg. Test Error Params. Search Cost</cell></row><row><cell>(%)</cell><cell>(M)</cell><cell>(GPU days)</cell></row><row><cell>RS</cell><cell></cell><cell></cell></row></table><note>summarizes our results. As shown, CATE-DNGO-LS achieves competitive performance in the actual DARTS</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Effects of δ and K on architecture pair sampling.</figDesc><table><row><cell>δ</cell><cell>K</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell></row><row><cell cols="2">1 × 10 6</cell><cell>6.02</cell><cell>5.95</cell><cell>5.99</cell><cell>5.95</cell></row><row><cell cols="2">2 × 10 6</cell><cell>6.02</cell><cell>5.94</cell><cell>6.04</cell><cell>5.96</cell></row><row><cell cols="2">4 × 10 6</cell><cell>5.94</cell><cell>6.03</cell><cell>6.05</cell><cell>5.99</cell></row><row><cell cols="2">8 × 10 6</cell><cell>6.05</cell><cell>6.04</cell><cell>6.11</cell><cell>6.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>It shows that larger L c and d f f usually lead to better NAS performance, which indicates that deep contextualized representations are beneficial to downstream NAS.</figDesc><table><row><cell>d f f</cell><cell>L c</cell><cell>6</cell><cell>12</cell><cell>24</cell></row><row><cell>64</cell><cell></cell><cell>6.07</cell><cell>5.99</cell><cell>5.95</cell></row><row><cell>128</cell><cell></cell><cell>6.01</cell><cell>5.94</cell><cell>5.95</cell></row><row><cell>256</cell><cell></cell><cell>5.97</cell><cell>5.94</cell><cell>5.94</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Effects of Lc and d f f on pretraining CATE.</figDesc><table><row><cell cols="3">Choice of Mask Type We studied pretraining CATE with</cell></row><row><cell cols="3">direct/indirect dependency mask and summarize its down-</cell></row><row><cell cols="3">stream NAS results in Table 5. CATE trained with indirect</cell></row><row><cell cols="3">dependency mask outperforms the direct one in both bench-</cell></row><row><cell cols="3">marks, indicating that capturing long-range dependency</cell></row><row><cell cols="3">helps preserve computation information in the encodings.</cell></row><row><cell>Mask type</cell><cell>NAS-Bench-101</cell><cell>NAS-Bench-301</cell></row><row><cell>Direct</cell><cell>6.03</cell><cell>5.35</cell></row><row><cell>Indirect</cell><cell>5.94</cell><cell>5.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Direct/Indirect dependency mask selection.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>As shown, bidirectional encoding does not necessarily improve the results. Therefore, we keep unidirectional encoding in other experiments due to its simplicity and better performance.</figDesc><table><row><cell>Encoding</cell><cell>NAS-Bench-101</cell><cell>NAS-Bench-301</cell></row><row><cell>Unidirectional</cell><cell>5.88</cell><cell>5.28</cell></row><row><cell>Bidirectional</cell><cell>5.89</cell><cell>5.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Unidirectional encoding vs. bidirectional encoding. We report the final NAS test error[%]  given 150 queried architectures on NAS-Bench-101 and 100 queried architectures on NAS-Bench-301. The result is averaged over 200 independent runs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>Effects of δ and K on architecture pair sampling on NAS-Bench-301. We report the final NAS test error[%]  given 100 queried architectures on NAS-Bench-301. The result is averaged over 200 independent runs. training. We also experimented corruption rates of 15% and 30%. As shown in Table8, overall, we find that the corruption rate has a limited effect on the NAS performance. Note that the number of nodes in our search space is much smaller compared to the number of tokens in the sequence modeling tasks. Given that, using larger corruption rate may slow down the training convergence and result in degraded performance. Based on these results, we use 20% corruption rate for other experiments.</figDesc><table><row><cell>δ</cell><cell>K</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell></row><row><cell cols="2">5 × 10 6</cell><cell>5.28</cell><cell>5.29</cell><cell>5.30</cell><cell>5.30</cell></row><row><cell cols="2">1 × 10 7</cell><cell>5.30</cell><cell>5.28</cell><cell>5.29</cell><cell>5.31</cell></row><row><cell cols="2">2 × 10 7</cell><cell>5.30</cell><cell>5.30</cell><cell>5.31</cell><cell>5.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>NAS results under different corruption rates.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Accelerating neural architecture search using performance prediction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Can weight sharing outperform random architecture search? an investigation with tunas</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">XGBoost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rethinking evaluation fairness of weight sharing neural architecture search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Fairnas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01845</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">NAS-Bench-201: Extending the scope of reproducible neural architecture search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Support vector regression machines</title>
		<author>
			<persName><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Neural architecture search: A survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<editor>JMLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Algorithm 97: Shortest path</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Floyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
				<imprint>
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural computation</title>
				<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural architecture search with bayesian optimisation and optimal transport</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<editor>
			<persName><surname>Neurips</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2018. 2017</date>
		</imprint>
	</monogr>
	<note>Lightgbm: A highly efficient gradient boosting decision tree</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Designing neural networks using genetic algorithms with graph generation system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kitano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Complex systems</title>
				<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Bart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Neural architecture optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Neural architecture optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Designing neural networks using genetic algorithms</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Hegde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICGA</title>
				<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A generic graph-based neural architecture encoding scheme for predictor-based nas</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Local search is a remarkably strong baseline for neural architecture search</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Ottelander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dushatskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Virgolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Bosman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08996</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<title level="m">Deep contextualized word representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>NAACL</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OpenAI Blog</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural architecture generator optimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esperanca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Carlucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bridging the gap between sample-based and one-shot neural architecture search with bonas</title>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Nas-bench-301 and the case for surrogate benchmarks for neural architecture search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Siems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.09777</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scalable bayesian optimization using deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prabhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Bayesian optimization with robust bayesian neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Falkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Evolving neural networks through augmenting topologies</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evolutionary Computation</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Designing neural networks through neuroevolution</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nature Machine Intelligence</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Sampleefficient neural architecture search by learning action space</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06832</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Alphax: exploring neural architectures with deep neural networks and monte carlo tree search</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jinnai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fonseca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Neural predictor guided evolution for neural architecture search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><surname>Npenas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.12857</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Neural predictor for neural architecture search</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A study on encodings for neural architecture search</title>
		<author>
			<persName><forename type="first">C</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nolen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Savani</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Local search is state of the art for neural architecture search benchmarks</title>
		<author>
			<persName><forename type="first">C</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nolen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Savani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02960</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bananas: Bayesian optimization with neural architectures for neural architecture search</title>
		<author>
			<persName><forename type="first">C</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Savani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Weight-sharing neural architecture search: A battle to shrink the optimization gap</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01475</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Exploring randomly wired neural networks for image recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks? In ICLR</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Hm-nas: Efficient neural architecture search via hierarchical masking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<editor>ICCVW</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Does unsupervised architecture representation learning help neural architecture search? In NeurIPS</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">NAS-Bench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Graph structure of neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">D-vae: A variational autoencoder for directed acyclic graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Autobss: An efficient algorithm for block stacking style search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
