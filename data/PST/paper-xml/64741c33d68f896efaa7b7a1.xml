<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-26">26 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lehigh University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lehigh University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiling</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lehigh University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lehigh University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eashan</forename><surname>Adhikarla</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lehigh University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sunyang</forename><surname>Fu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Mayo Clinic</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xun</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Samsung Research America</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Central</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Cruz</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Harvard Medical School</orgName>
								<orgName type="institution">Massachusetts General Hospital</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lifang</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lehigh University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Davison</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lehigh University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quanzheng</forename><surname>Li</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Harvard Medical School</orgName>
								<orgName type="institution">Massachusetts General Hospital</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Chen</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongfang</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Mayo Clinic</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lichao</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Lehigh University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Florida</surname></persName>
						</author>
						<title level="a" type="main">BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-26">26 May 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.17100v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce a unified and generalist Biomedical Generative Pre-trained Transformer (BiomedGPT) model, which leverages self-supervision on large and diverse datasets to accept multi-modal inputs and perform a range of downstream tasks. Our experiments demonstrate that BiomedGPT delivers expansive and inclusive representations of biomedical data, outperforming the majority of preceding state-of-the-art models across five distinct tasks with 20 public datasets spanning over 15 unique biomedical modalities. Through the ablation study, we also showcase the efficacy of our multi-modal and multitask pretraining approach in transferring knowledge to previously unseen data. Overall, our work presents a significant step forward in developing unified and generalist models for biomedicine, with far-reaching implications for improving healthcare outcomes.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure <ref type="figure">1</ref>: Illustration of the diverse range of tasks supported by BiomedGPT during pretraining and subsequent fine-tuning. During the pretraining phase, we employ prevalent unimodal strategies, including masked language modeling and masked image infilling, and multimodal techniques, such as visual question answering and captioning. Object detection is also incorporated into the pretraining to infuse locational data. Following pretraining, the enhanced model is leveraged for a suite of five downstream tasks, encompassing image classification and natural language inference, demonstrating its efficient utilization of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the rapidly evolving landscape of artificial intelligence (AI), transformer-based foundation models <ref type="bibr" target="#b123">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b30">Dosovitskiy et al., 2020;</ref><ref type="bibr" target="#b11">Bommasani et al., 2021;</ref><ref type="bibr" target="#b146">Zhou et al., 2023)</ref> have emerged as a powerful tool for solving a wide range of biomedical challenges, e.g., radiograph analysis <ref type="bibr" target="#b92">(Park et al., 2022;</ref><ref type="bibr" target="#b147">Zhou et al., 2022)</ref>, biomedical text generation <ref type="bibr" target="#b147">(Luo et al., 2022)</ref>, disease prediction <ref type="bibr" target="#b104">(Rasmy et al., 2021)</ref>, and cell type annotation <ref type="bibr">(Yang et al., 2022a)</ref>. The prevailing paradigm for biomedical foundation models is the pretraining then fine-tuning. Specifically, a model is first pre-trained on a large-scale dataset and then finetuned on downstream datasets, facilitating knowledge transfer from the source domain to the target domain <ref type="bibr" target="#b88">(Niu et al., 2020)</ref>. Instead of supervised pretraining, which is constrained by the availability of labeled data, self-supervised approaches that can learn from vast amounts of data without explicit human labeling have gained widespread adoption <ref type="bibr" target="#b7">(Balestriero et al., 2023;</ref><ref type="bibr" target="#b44">Huang et al., 2023;</ref><ref type="bibr" target="#b86">Nadif &amp; Role, 2021;</ref><ref type="bibr" target="#b60">Krishnan et al., 2022)</ref>. For example, BERT-derived <ref type="bibr" target="#b27">(Devlin et al., 2018;</ref><ref type="bibr" target="#b104">Rasmy et al., 2021;</ref><ref type="bibr" target="#b65">Lee et al., 2020;</ref><ref type="bibr" target="#b39">Gu et al., 2021;</ref><ref type="bibr" target="#b13">Chakraborty et al., 2020;</ref><ref type="bibr">Alsentzer et al., 2019b)</ref> and <ref type="bibr">GPT-derived Radford et al. (2019)</ref>; <ref type="bibr" target="#b147">Luo et al. (2022)</ref>; <ref type="bibr" target="#b59">Kraljevic et al. (2021)</ref> models have been extensively studied in biomedical natural language processing and gained improved performance over prior methods. In biomedical imaging analysis, the Vision Transformer (ViT) <ref type="bibr" target="#b30">(Dosovitskiy et al., 2020)</ref> is regarded as the pretraining backbone for various tasks, including image segmentation, detection, classification, and synthesis, and also achieves promising performance <ref type="bibr" target="#b114">(Shamshad et al., 2023;</ref><ref type="bibr" target="#b121">Valanarasu et al., 2021;</ref><ref type="bibr" target="#b58">Kong et al., 2021;</ref><ref type="bibr" target="#b85">Manzari et al., 2023)</ref>.</p><p>In recent years, the increasing availability of biomedical data has set the stage for the development of multimodal AI solutions that capture the intricacies of human health and disease <ref type="bibr" target="#b1">(Acosta et al., 2022)</ref>. Given biomedical data's complexity and high dimensionality, most efforts focus on vision-language pretraining instead of omni-modal fusion <ref type="bibr" target="#b111">(Selivanov et al., 2023;</ref><ref type="bibr" target="#b14">Chambon et al., 2022)</ref>. To enable multimodal models to effectively understand both images and textual contexts, as well as to infer the associations between them accurately, researchers typically pre-train visual/textual embedders and cross-modal modules using imagetext matching and masked language modeling objectives on images and their corresponding descriptions <ref type="bibr" target="#b56">(Kim et al., 2021;</ref><ref type="bibr" target="#b141">Li et al., 2020;</ref><ref type="bibr" target="#b25">Delbrouck et al., 2022;</ref><ref type="bibr" target="#b132">Yan &amp; Pei, 2022;</ref><ref type="bibr" target="#b53">Khare et al., 2021;</ref><ref type="bibr">Chen et al., 2022c)</ref>. The CLIP architecture and its underlying contrastive pretraining <ref type="bibr" target="#b101">(Radford et al., 2021;</ref><ref type="bibr" target="#b48">Jia et al., 2021)</ref>, which aims to match paired image and caption embeddings while pushing others apart for improved representation transferability, has also been applied in biomedical AI, yielding acceptable zero-shot performance <ref type="bibr" target="#b144">(Zhang et al., 2022;</ref><ref type="bibr" target="#b43">Huang et al., 2021;</ref><ref type="bibr">Wang et al., 2022c;</ref><ref type="bibr" target="#b33">Eslami et al., 2023;</ref><ref type="bibr" target="#b143">Zhang et al., 2023)</ref>.</p><p>However, due to the limited volume and modalities in existing labeled biomedical datasets, previous works have primarily focused on either task/domain-specific or modality-specific applications, significantly restricting their practical utility<ref type="foot" target="#foot_0">1</ref> . Considering the disease classification task, the International Classification of Diseases, tenth Revision (ICD-10) currently covers approximately 69,832 diagnosis codes<ref type="foot" target="#foot_1">2</ref> , thus it is impractical and uneconomical to develop different models for each disease. Besides, such a specialized schema deviates from the demand of drawing a comprehensive picture in healthcare <ref type="bibr" target="#b74">(Lindberg et al., 1993)</ref> as many seemingly irrelevant diseases or symptoms coexist and interact, e.g., diabetes is becoming the leading cause of new cases of blindness among adults <ref type="bibr" target="#b35">(Fong et al., 2004)</ref>. Researchers typically tackle this issue from the perspective of multitasking <ref type="bibr" target="#b95">(Peng et al., 2020)</ref> and transferring <ref type="bibr" target="#b103">(Raghu et al., 2019)</ref>. However, these practices compel the available datasets to obey strong assumptions like homogeneous structures or overlapping distributions.</p><p>Recent breakthroughs have produced a new class of unified and generalist AI models capable of performing diverse tasks with a unified architecture and shared parameters <ref type="bibr" target="#b2">(Alayrac et al., 2022;</ref><ref type="bibr" target="#b70">Li et al., 2022;</ref><ref type="bibr">Wang et al., 2022b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b82">Lu et al., 2022;</ref><ref type="bibr">Chen et al., 2022b;</ref><ref type="bibr" target="#b105">Reed et al., 2022;</ref><ref type="bibr" target="#b40">Gupta et al., 2022;</ref><ref type="bibr">Chen et al., 2023b)</ref>. This remarkable advancement holds significant potential for the field of biomedicine, as it eliminates the need for specific models in specialized domains (inductive biases) by employing large-scale global-attention-based transformers <ref type="bibr" target="#b123">(Vaswani et al., 2017;</ref><ref type="bibr">Yun et al., 2019b;</ref><ref type="bibr" target="#b30">Dosovitskiy et al., 2020)</ref>. Inspired by OFA <ref type="bibr">(Wang et al., 2022b)</ref>, we propose BiomedGPT, a unified and generalist model designed for handling various types of data through straightforward serialization integrated with task-oriented prompts. Specifically, BiomedGPT embeds data from diverse input types within a common, multimodal vocabulary that can be applied across all tasks. This model utilizes a unified sequence-to-sequence abstraction for both the pretraining and finetuning stages. In addition, we infuse the task instructions directly into the input as plain text, obviating the need for extra parameters. This architectural design fosters efficient task performance, offering a seamless process regardless of the data modality or task. We pre-train and fine-tune BiomedGPT with a variety of biomedical datasets and tasks. Through comprehensive experiments, we demonstrate that BiomedGPT can effectively transfer knowledge across tasks and even compete with specialist models trained on single-domain or single-modality datasets. This is particularly evident in vision-language tasks, such as image captioning and visual question answering, where BiomedGPT achieves new state-of-the-art (SOTA) performance.</p><p>By harnessing the power of our generalist biomedical model to analyze complex data, researchers can unlock a wealth of insights and advance our understanding of the biological mechanisms underlying human health and disease, paving the way for exciting new possibilities in diagnosing, treating, and preventing diseases. Our contributions are summarized as follows:</p><p>? We present BiomedGPT, which, to our knowledge, is the first generalist AI model for biomedicine capable of accommodating various modalities, such as CT images and clinical notes, among others.</p><p>It demonstrates an impressive performance across various downstream tasks, including a vision-only task, two language-only tasks, and two vision-language tasks. ? BiomedGPT is designed to encompass a wide range of domains in biomedicine. Our experimental results set a new benchmark, illustrating the feasibility of pretraining across diverse biomedical fields such as pathology, radiology, and academic literature. This is coupled with an ability to handle various body parts across different modalities. ? We conduct extensive studies to underline the efficacy and limitations of BiomedGPT. The insights gleaned from our study, summarized in Section 4, stand to contribute significantly to the development of future iterations of generalist biomedical AI models for the wider research community. ? We commit to open-source practices by providing access to our codes<ref type="foot" target="#foot_2">3</ref> . This includes all processes, such as data preprocessing, pretraining, and fine-tuning, to ensure reproducibility and encourage further development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BiomedGPT Pipeline</head><p>Our proposed BiomedGPT is a transformer-based architecture specifically designed for the biomedical field, built upon the success of existing unified models for general data. We adhere to the fundamental principles of a generalist model: 1) Modality-Agnostic, 2) Task-Agnostic, and 3) Modality and Task Comprehensiveness. By discretizing data into patches or tokens, we achieve input/output unification using ideas from Vision Transformer (ViT) <ref type="bibr" target="#b30">(Dosovitskiy et al., 2020)</ref> and Language Models <ref type="bibr" target="#b69">(Lewis et al., 2020)</ref>. We pre-train our model on a diverse set of biomedical modalities and tasks to enhance its transferability. Our encoderdecoder architecture maps multi-modal data with task-related instructions into a common representation space, which helps to address discrepancies among biomedical modalities. Figure <ref type="figure" target="#fig_0">2</ref> showcases a snapshot of the BiomedGPT training process, highlighting its comprehensive and generalist nature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Architecture Selection</head><p>There are three mainstream architectures of the pre-trained foundation models (PFMs) <ref type="bibr" target="#b109">(Sarrouti et al., 2022;</ref><ref type="bibr" target="#b36">Fu et al., 2023)</ref>: 1) encoder-only, 2) decoder-only, and 3) encoder-decoder.</p><p>The encoder-only models use only the encoder of a transformer, which focuses on learning useful representations of the inputs. The most popular model in the encoder-only family is BERT <ref type="bibr" target="#b27">(Devlin et al., 2018)</ref>, which has several variants <ref type="bibr" target="#b62">(Lan et al., 2019;</ref><ref type="bibr" target="#b108">Sanh et al., 2019;</ref><ref type="bibr" target="#b79">Liu et al., 2019)</ref> and subsequent studies <ref type="bibr" target="#b21">(Clark et al., 2020;</ref><ref type="bibr" target="#b30">Dosovitskiy et al., 2020)</ref>. To perform diverse tasks, extra learnable modules, such as classification head and task-specific decoder, are required in fine-tuning. This means that encoder-only models may not be able to align the inputs and outputs of qualitatively different modalities and cannot conduct complicated zero-shot prediction or generation. On the other hand, the decoder-only models, such as OpenAI's GPTs <ref type="bibr" target="#b99">(Radford et al., 2018;</ref><ref type="bibr">2019;</ref><ref type="bibr" target="#b12">Brown et al., 2020)</ref> use only the decoder of a Transformer. Although a decoderonly model could do multi-tasking in a unified manner, it typically requires pre-existing representations or encodings of the input data to function. However, we have not seen an encoder that is sufficiently strong to encode all biomedical modalities. Besides, the decoder-only approach lacks the ability to learn joint representations across modalities and tasks, as it is focused solely on generating output based on a fixed input encoding. This can result in reduced model flexibility and suboptimal performance when faced with novel or complex tasks. Thus, we propose the adoption of an encoder-decoder architecture for BiomedGPT due to its exceptional ability to effectively map various modalities into a unified semantic representation space while successfully handling a wide range of diverse tasks.</p><p>We follow OFA <ref type="bibr">(Wang et al., 2022b)</ref> to design BiomedGPT, which takes BART <ref type="bibr" target="#b68">(Lewis et al., 2019)</ref> as the backbone that is implemented as a sequence-to-sequence model with a BERT-style encoder over corrupted text and a GPT-style left-to-right autoregressive decoder. We make a few architectural changes to adapt the BART architecture for BiomedGPT. First, to improve the convergence efficiency and stability in the pretraining, we add three normalization operations to each layer: a post-attention Layer Norm (LN) <ref type="bibr" target="#b5">(Ba et al., 2016)</ref>, post-first-FFN LN, and head-wise scaling within self-attention, following <ref type="bibr" target="#b115">(Shleifer et al., 2021)</ref>.</p><p>To encode positional information, we incorporate two sets of absolute position embeddings for both text and images. Rather than merely combining these embeddings with token and patch embeddings, we implement a decoupling method to separate position correlation <ref type="bibr" target="#b57">(Kitaev &amp; Klein, 2018;</ref><ref type="bibr" target="#b52">Ke et al., 2019)</ref>. Furthermore, we also incorporate 1D relative position bias for text and 2D relative position bias for image, as described in previous works <ref type="bibr" target="#b102">(Raffel et al., 2020;</ref><ref type="bibr" target="#b24">Dai et al., 2021;</ref><ref type="bibr">Wang et al., 2022d)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Input/Output Unification</head><p>To enable inputs with a wide range of modalities, including images, language, and bounding boxes, to be processed within a single model, it is necessary to embed them in a shared and unified space. For visual inputs, we directly apply CNN backbones to relax the heavy image feature extraction process, including object detection, following <ref type="bibr" target="#b56">(Kim et al., 2021)</ref>. Specifically, BiomedGPT receives the raw image x v ? R H?W ?C and maps it into a flattened 1D sequence of patches x p ? R N ?D via a ResNet module as input for the transformer, where N = HW P 2 is the number of patches given the patch size of P ? P , and D is the fixed hidden size of the transformer layers. We choose the first three blocks of ResNet layers due to their observed advantages over 1x1 convolution or naive linear projection used in ViT <ref type="bibr">(Wang et al., 2022d)</ref>. For linguistic inputs, we used byte-pair encoding (BPE) <ref type="bibr" target="#b112">(Sennrich et al., 2016)</ref> to perform the subword tokenization. The subwords are then embedded into the input features.</p><p>To handle diverse modalities without relying on task-specific output structures, we represent them with tokens drawn from a unified and finite vocabulary. To achieve this, we utilize the frozen image quantization <ref type="bibr" target="#b122">(Van Den Oord et al., 2017;</ref><ref type="bibr" target="#b34">Esser et al., 2021)</ref> and object descriptor <ref type="bibr">(Chen et al., 2022a;</ref><ref type="bibr">b)</ref> to discretize the images and objects on the target side, respectively. As to the text outputs, such as object labels and summarizations, we represent them with BPE tokens. To be more specific, the image with 256?256 resolution is sparsely encoded into a sequence of 16 ? 16, which is strongly correlated with the corresponding patch <ref type="bibr" target="#b9">(Bao et al., 2021)</ref> and can effectively reduce the sequence length of the image representation. The bounding boxes of objects in an image are expressed as sequences of location tokens in the format of integers. We hereby build a unified vocabulary for all tokens of multi-modal outputs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Natural Language as a Task Manager</head><p>Multitasking is a key attribute of a unified and generalist model. Following previous literature on language models using prompt / instruction learning <ref type="bibr" target="#b12">(Brown et al., 2020;</ref><ref type="bibr" target="#b78">Liu et al., 2023;</ref><ref type="bibr" target="#b129">Wei et al., 2022;</ref><ref type="bibr" target="#b37">Gao et al., 2021;</ref><ref type="bibr" target="#b110">Schick &amp; Sch?tze, 2021)</ref>, and the existing unified frameworks to eliminate task-specific modules, we specify each task with a handcrafted instruction excluding some tasks like Visual Question Answering (VQA), which are fully specified by their text inputs. BiomedGPT supports abstractions of several tasks, including vision-only, text-only, and vision-language, to achieve task comprehensiveness. We provide details of the pretraining tasks, fine-tuning / inference tasks, as well as their corresponding instructions in the following.</p><p>Pretraining Tasks. We consider two vision-only tasks in the pretraining: for masked image modeling (MIM) as well as image infilling, we borrow the idea of blockwise masking <ref type="bibr" target="#b10">(Bao et al., 2022)</ref> and let the model recover the masked patches in the middle part by generating the corresponding codes. The corresponding instruction is "What is the image in the middle part?". For object detection (OD), the model learns to generate the bounding box of an object with the instruction of "What are the objects in the image?". As to the text-only task, we adopt the commonly-used masked language modeling (MLM) while the instruction is "What is the complete text of 'A case of oral &lt;mask&gt; anaphylaxis' ?". Two types of multi-modal tasks are selected, including image captioning with the instruction of "What does the image describe?" and VQA with the instruction of "{Question}". The addition of OD for pretraining BiomedGPT serves to enhance visual learning inspired by <ref type="bibr" target="#b131">(Xu et al., 2021)</ref>.</p><p>Fine-tuning and Inference Tasks. Besides image captioning and VQA used in pretraining, we cover one more vision-only task and two more text-only tasks. Specifically, we use the instruction "What does the image describe?" to differentiate image classification. "What is the summary of text '{Text}'?" and "Can text1 '{Text1}' imply text2 '{Text2}'?" are exploited for text summarization and natural language inference, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Gnerative Pretraining via Seq2seq</head><p>Autoregressive or sequence-to-sequence (seq2seq) is widely used in sequential modeling <ref type="bibr" target="#b118">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b20">Cho et al., 2014;</ref><ref type="bibr" target="#b90">Oord et al., 2016)</ref>, such as large language modeling Lewis et al. ( <ref type="formula">2019</ref>  In the context of BiomedGPT, x could refer to both linguistic and visual tokens in the pretraining tasks, including subwords, image codes, and location tokens, as we mentioned in Section 2.2. Specifically, subwords are extracted by a BPE tokenizer, and we mask 30% of the tokens of the subwords in input in the MLM task as these medical words show relatively high overlapping degrees. For the OD task, location tokens are generated with Pix2Seq (Chen et al., 2022a) conditioned on the observed pixel inputs. We need data preprocessing for quantizing biomedical images using VQ-GAN <ref type="bibr" target="#b34">(Esser et al., 2021)</ref> because they are surrounded by trivial semantics, e.g., black background and the unmet input size. Therefore, we first remove the trivial background and crop the image to the bounding box of the object of interest, then resize the cropped image to be 256 ? 256, and feed the center part with 128 ? 128 resolution into the pre-trained VQ-GAN to generate the corresponding sparse image codes, which are the target output in MIM task. Vision-language tasks follow the same tokenization flow. Note that for fine-tuning, we also apply seq2seq learning but with different datasets and tasks, as shown in Table <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_2">Table 3</ref>. More implementation details are described in Section 3.1.</p><formula xml:id="formula_0">L ? (x 1,1 , ? ? ? , x i,b ) = - B b=1 log I i=1 p ? (x i,b |x 1,b , ? ? ? , x i-1,b ) = - B b=1 I i=1 log p ? (x i,b |x &lt;i,b )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Autoregressive Inference</head><p>Inference in large language models often relies on decoding strategies like beam search to improve generation quality. However, such an approach poses challenges for classification tasks, including unnecessary We did not re-run the methods but relied on the metrics reported in their original work, e.g., the SOTA results on MedMNIST v2 come from <ref type="bibr" target="#b136">(Yang et al., 2021)</ref>. Due to the absence of severe class imbalance on the selected downstream datasets, accuracy is appropriate to be used. optimization of the entire vocabulary and the possibility of generating invalid labels beyond the closed label set. To tackle these issues, we apply a beam search strategy that incorporates a prefix tree (also known as a trie), limiting the number of candidate tokens and resulting in more efficient and accurate decoding. Figure <ref type="figure" target="#fig_2">3</ref> demonstrates an example of trie-based beam search; along the path across "Lipid" and "breakdown", BiomedGPT sets logits for all invalid tokens ("mechanism" and "pathway") to -? when computing log-probabilities for the target token "in". It is worth noting that trie-based search is also applied during the validation phase of the fine-tuning stage for acceleration (approximately 16? increase in speed in our experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we showcase the experimental design and implementation details of BiomedGPT, along with its superior performance compared to previous state-of-the-art methods across various downstream tasks and datasets. We deliberately select data from different domains to show the promising generalization of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>The total vocabulary size is 59457, with 50265 language tokens, 1000 location tokens, and 8192 vision tokens. The number of vision tokens is determined by the variant of the pre-trained VQ-GAN models used in the BiomedGPT, specifically, the OpenImages <ref type="bibr" target="#b61">(Kuznetsova et al., 2020)</ref>-trained VQ-GAN with patch size of 8 and vocabulary size of 8192 using the Gumbel softmax <ref type="bibr" target="#b47">(Jang et al., 2017;</ref><ref type="bibr" target="#b84">Maddison et al., 2017)</ref> quantization.</p><p>During training, we randomly subsample 196 image patches for pretraining. The truncation to max model input length is set as 512.</p><p>To pretrain our BiomedGPT, we use the AdamW <ref type="bibr" target="#b81">(Loshchilov &amp; Hutter, 2019)</ref> optimizer with hyperparameters ? 1 = 0.9, ? 2 = 0.999, and ? = 1e-8. The peak learning rate is set to 1e-4, and we apply a linear decay scheduler with a warmup ratio of 0.01 to control the learning rate. For regularization, we set dropout to 0.1 and use a weight decay of 0.01. To enhance the training process, we use stochastic depth with a rate of 0.1 applied to the encoder and decoder, except for convolution blocks. Furthermore, we employ a diversified approach in mixing all pretraining data within each batch. This includes an assortment of multimodal, text-only, vision-only, and OD samples. The ratio applied is 8:2:1:1, which emphasizes learning and enhancing the interaction between vision and language. The models are pre-trained with 10 Nvidia A5000 GPUs and mixed precision, except for the models used for fine-tuning downstream tasks. The experimental settings, dependent on the task and data used, are described in detail in Appendix A.</p><p>In order to investigate the performance of BiomedGPT for tasks at different scales, we explicitly design three scaling models, i.e., BiomedGPT Small , BiomedGPT Medium , and BiomedGPT Base . The configurations for each model are detailed in Table <ref type="table" target="#tab_1">2</ref>. It can be observed, from the results in Table <ref type="table" target="#tab_2">3</ref>, that in most cases, larger models tend to perform better. However, in practical deployment, scaling models for better performance may not be economical or parameter-efficient in some datasets. For instance, the experimental results on BreastMNIST demonstrate that the medium-size model shows only a 0.2% accuracy improvement compared to the small model, but it requires approximately 3? more parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results on Unimodal Datasets</head><p>We select three unimodal tasks over 14 datasets, as shown in Table <ref type="table" target="#tab_2">3</ref>. For the image classification task, results show the classification accuracy on MedMNIST v2 Yang et al. ( <ref type="formula">2021</ref>) (a set of benchmark datasets) covering several biomedical domains. Our BiomedGPT Base model achieves state-of-the-art accuracy on 9 out of 10 image-only datasets. Although we are behind the state-of-the-art by a small margin on RetinaMNIST, it is important to note that the dataset was initially designed for a regression task, and thereby the performance gap could be expected.</p><p>We further investigate the model performance on text-only datasets. Our results show that for the natural language inference task with the MedNLI dataset, our model achieved an accuracy of 78.6%, which is lower than the state-of-the-art (SOTA) result of 86.5%. For text summarization, we focused on doctor-patient conversations and aimed to summarize the patient's medical questions. We used the ROUGE-L metric to measure performance. However, similar to the results in MedNLI, our model does not achieve satisfactory performance. There are several potential reasons for the difference in performance observed between our model and previous state-of-the-art models. First, our model has a constrained scale with fewer parameters than the SOTA models. Specifically, SciFive is based on T5-Large <ref type="bibr" target="#b102">(Raffel et al., 2020)</ref> with 770 million parameters, and BioBart-Large has nearly 400 million parameters, while our largest model has only 182 million parameters. Second, our corpus scale is also constrained compared to SciFive, which processed a larger-scale corpus, including Colossal Clean Crawled Corpus (C4) <ref type="bibr" target="#b102">(Raffel et al., 2020)</ref>, PubMed Abstract<ref type="foot" target="#foot_3">4</ref> , and PubMed Central (PMC)<ref type="foot" target="#foot_4">5</ref> . Third, divergent data impact may have played a role. Our model was trained on a corpus that covers both biomedical articles and clinical notes, with the goal of building a unified and comprehensive model. However, it has been reported that models pre-trained on clinical notes can perform poorly on language tasks based on biomedical articles, and vice versa <ref type="bibr" target="#b39">(Gu et al., 2021;</ref><ref type="bibr">Alsentzer et al., 2019a;</ref><ref type="bibr" target="#b66">Lehman et al., 2023)</ref>. Addressing the substantial differences between text modalities is an open question that requires further investigation to improve the transferability of biomedical language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results on Multimodal Datasets</head><p>BiomedGPT aims to tackle two multimodal tasks: image captioning and visual question answering, for which we have chosen three corresponding datasets each, as presented in Table <ref type="table" target="#tab_2">3</ref>. Specifically, we use SLAKE (English text only), PathVQA, VQA-RAD for VQA, IU X-ray, PEIR Gross, and ROCO for image captioning. Due to the limited availability of public multimodal biomedical datasets, we only pre-train our model on the training sets of some datasets and evaluate our prediction capability on the unseen testing sets. For evaluating image captioning, we employ three metrics, namely METEOR, ROUGE-L, and CIDEr, for a comprehensive comparison. During inference, we select the checkpoint with the highest CIDEr score obtained during fine-tuning. Our approach outperforms state-of-the-art methods in terms of CIDEr, particularly on the Peir Gross dataset, with a remarkable improvement of 273%. Although BiomedGPT falls short of PPKED regarding ROUGE-L on the IU X-ray dataset, it is crucial to note that this can be attributed to our model selection based on CIDEr rather than ROUGE-L during the validation phase. These two evaluation metrics accentuate different aspects of performance, as comprehensively discussed in Appendix D. Moreover, to our knowledge, no previous work is reported on the ROCO dataset; hence, we report our results without comparison.</p><p>To evaluate the performance of VQA, we report the overall accuracy, which includes closed-and open-ended question-answer pairs. Our proposed model, BiomedGPT, achieves a significant improvement over previous state-of-the-art models on SLAKE (EN) and PathVQA datasets. We also observe that the performance improvement has not reached a plateau, and we believe that we can further improve the accuracy by increasing the number of fine-tuning epochs. Besides, BiomedGPT obtain comparable predictions on the VQA-RAD dataset. In our case analysis illustrated in Fig. <ref type="figure" target="#fig_3">4</ref>, we observe that BiomedGPT often generates semantically correct outputs. An example is the middle image with the question indexed by (3), where "just one" is correctly inferred. However, the current evaluation framework counts only absolute matches, which may not fully reflect the model's capability to produce semantically accurate answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Results on Intra-&amp; Inter-distribution Transfer</head><p>We conducted intra-and inter-distribution inference experiments, performing zero-shot inference on seen and unseen datasets using our pre-trained checkpoints. These checkpoints were pre-trained on the training sets of SLAKE and PathVQA, and we evaluated their performance on the testing sets of those two datasets and VQA-RAD. Our results in Table <ref type="table" target="#tab_4">4</ref> indicate that BiomedGPT achieves excellent performance, particularly when using a larger model, on SLAKE and PathVQA. However, pre-trained on general datasets like ImageNet, the baseline OFA model experienced substantial performance degradation on these two datasets. In comparison, OFA attains performance similar to BiomedGPT on the VQA-RAD dataset, but neither model excels compared to the post-fine-tuning outcomes presented in Table <ref type="table" target="#tab_2">3</ref>. We hypothesize that the models may overfit the familiar intra-domain distribution and have difficulties handling out-of-distribution data. Upon examining the generated answers from BiomedGPT, we also observe its tendency to overfit intra-distribution instructions, leading to challenges in comprehending unseen questions and producing uncontrolled outputs. As an example, when presented with the 14th question "Where is the pathology in this image?" from the VQA-RAD dataset, BiomedGPT Base predicts</p><formula xml:id="formula_1">&lt; code_7948 &gt;&lt; code_5859 &gt;&lt; code_771 &gt; ? ? ? &lt; code_7077 &gt;&lt; code_7077 &gt;, indicating</formula><p>that the model had mistakenly interpreted the VQA task as an image generation task. Although both BiomedGPT Medisum/Small and OFA Base generate text-only answer "No", it still does not match the openended ground truth of "Head". Figure <ref type="figure" target="#fig_4">5</ref> displays the open-and closed-ended accuracies in transfer learning on VQA-RAD data. We observed that models exhibit catastrophic performance on open-ended questions.</p><p>Note that there are 251 out of 451 closed-ended QA pairs (either "Yes" or "No") in the VQA-RAD test set, with the remainder being open questions.</p><p>These observations shed light on the instruction-sensitivity challenge that arises in instruction-guided pretraining when building a unified and generalist biomedical model. To delve deeper into this issue, we present a case study in Appendix A. Additionally, our findings from the intra-and inter-distribution transfer learning experiments suggest that data diversity and scale are crucial factors in enhancing the generalist intelligence of BiomedGPT. However, due to the limited volume and diversity of existing biomedical datasets, as well as concerns regarding data privacy and imbalance, we plan to explore data augmentation and synthetic biomedical datasets in our future work.  Here, the model sizes are denoted by 'L', 'B', 'M', and 'S', which stand for large-, base-, medium-, and small-sized models, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablation Study on Pretraining Tasks</head><p>In this section, we demonstrate the effectiveness of pretraining modules. Table <ref type="table" target="#tab_5">5</ref> shows the results of the same setting for each model on the same data. It is important to note that the evaluation data used in the table was not seen during pretraining for a fair comparison. Overall, we observe that pretraining without using biomedical data (w/o PTB) will result in failure on multimodal tasks such as image captioning on ROCO and VQA on VQA-RAD. Additionally, we found that the absence of MLM and MIM will affect the performance of text-only and image-only tasks, respectively. Another interesting observation is that pre-trained OFA can achieve better performance on text-only datasets, MeQSum and NLI. This performance may be related to the fact that the text information is not "professional" enough and contains too much natural / general language. Furthermore, if we remove MIM, we observe better performance. This suggests that multi-modal pretraining may influence the unimodal tasks, especially text-only tasks, as the image is not necessarily required. In contrast, for image-only tasks, at least a dictionary of text tokens is needed for label generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Ablation Study on Pretraining Modalities</head><p>This section addresses the query: "Can the proposed model handle unseen data modalities (e.g., images from a new different imaging device like an ultrasound)?" To investigate this, we have adjusted our dataset selection for both pretraining and downstream tasks. Specifically, we've drawn 3,489 and 6,461 chest X-Ray image-text pairs from SLAKE (EN) and IU X-ray datasets, respectively. Additionally, we selected an equal quantity of images (7,452) from CheXpert while disabling MLM and OD during pretraining for simplification.</p><p>x-ray breast organc organs 0.5 0.6 0.7 0.8 0.9 1.0 Small Medium</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base ResNet50</head><p>Figure <ref type="figure">6</ref>: The classification accuracy of BiomedGPT, fine-tuned with both seen (x-ray) and unseen (ultrasound and CT) modalities, is depicted in this comparison. Here, ResNet-50, trained from scratch according to the protocol in <ref type="bibr" target="#b136">(Yang et al., 2021)</ref>, serves as a reference baseline.</p><p>The pre-trained BiomedGPT Small on X-Ray modality is subsequently fine-tuned on chest x-ray (x-ray), breast ultrasound (breast) and liver CT (organc and organs) within the radiology field. The process consists of 500 update steps, and the resulting performance is depicted in Figure <ref type="figure">6</ref>. These outcomes underscore the impressive in-domain transferability of BiomedGPT. Furthermore, our findings indicate that BiomedGPT can perform well with out-of-domain modalities when the subjects of the medical imaging are anatomically adjacent, such as chest versus breast, in our experiment. We intentionally limit the update steps to observe the efficiency of transfer between pretraining and downstream modalities. While BiomedGPT may not have achieved superior accuracies on the Liver CT datasets, we nevertheless observe performance saturation with increasing training steps. This suggests that BiomedGPT is capable of delivering solid results with more differentiated biomedical modalities across different body parts, albeit requiring additional training steps. Additional results are detailed in Appendix B.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Main Findings. In this study, we have shown that BiomedGPT can achieve competitive performance across various tasks, spanning vision, language, and multimodal domains. This is achieved by integrating a diverse range of biomedical modalities and tasks within a unified seq2seq pretraining framework. Our comprehensive experiments and ablation studies underscore the pivotal role of incorporating a wide variety of tasks and modalities in the construction of a generalist biomedical AI model. Notably, the inclusion of a broad spectrum of biomedical tasks and modalities in the pretraining phase significantly enhances the fine-tuning efficiency and ultimately bolsters the overall performance of the model. This improvement is attributed to the implicit interactions among these different factors. We noticed a fascinating observation in our studies: while OFA-a generalist model pre-trained with generic data-exhibits impressive zero-shot performance on VQA-RAD data as outlined in Table <ref type="table" target="#tab_4">4</ref>, it encounters difficulty when attempting to align image-text pairs during the fine-tuning phase. This challenge is evident in Table <ref type="table" target="#tab_5">5</ref>, where OFA's performance within a restricted number of epochs is distinctly low, achieving only 6.8 CIDEr and 2.6 accuracy for captioning and VQA tasks, respectively. This observation underscores the phenomenon that an effective zero-shot model does not necessarily translate into a superior starting point for fine-tuning tasks. BiomedGPT manages to surmount these limitations associated with multi-modal, multi-task pretraining. Furthermore, our exploration of the scaling laws proposed by <ref type="bibr" target="#b51">Kaplan et al. (Kaplan et al., 2020)</ref> reveals that enlarging the scale of the model leads to a considerable boost in performance. In conclusion, by expanding the scale of data, tasks, and the model, we foresee substantial enhancements in BiomedGPT's few-shot and zero-shot inference capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations and Suggestions.</head><p>Our extensive experiments have revealed several limitations of BiomedGPT. A primary concern is the model's sensitivity toward instructions. There are instances where the model fails to understand the instructions and makes catastrophic predictions, even producing irrelevant data types such as image codes for a VQA task. A straightforward solution could be to broaden the diversity of high-quality instruction sets during pretraining. Additionally, we must investigate methods for achieving a balance in data diversity. This encompasses aspects such as establishing suitable size ratios for data within different biomedical modalities in a batch and throughout the entire pretraining dataset and determining the optimal sequence for training with diverse inputs. Another potential avenue is to align BiomedGPT with human intent via reinforcement learning from human or AI feedback (RLF) <ref type="bibr" target="#b91">(Ouyang et al., 2022;</ref><ref type="bibr" target="#b6">Bai et al., 2022;</ref><ref type="bibr" target="#b146">Zhou et al., 2023)</ref>, a strategy employed by the latest dialogue agents such as ChatGPT<ref type="foot" target="#foot_5">6</ref> and Claude<ref type="foot" target="#foot_6">7</ref> . However, creating specific biomedical RLF datasets would be expensive, given the extensive need for domain experts.</p><p>A further significant limitation arises from two specific inputs for text-only downstream tasks: the considerable difference between clinical notes and general-domain &amp; biomedical text <ref type="bibr" target="#b66">(Lehman et al., 2023;</ref><ref type="bibr" target="#b39">Gu et al., 2021)</ref>, and the presence of vision-only tasks, which can impede the model's pattern extraction from the pure text during pretraining, as highlighted in our ablation study in Section 3.5. Generating a representative vocabulary from all domains and increasing the ratio of text inputs during pretraining may help address these issues. However, this is a balancing act as it may influence vision-related tasks.</p><p>Lastly, there is the issue of fine-tuning efficiency with respect to training speed and memory bottleneck, particularly as we aim to develop a large-scale generalist biomedical model. An emerging research direction that could address this is parameter-efficient fine-tuning (PEFT) <ref type="bibr" target="#b28">(Ding et al., 2023;</ref><ref type="bibr" target="#b77">Liu et al., 2022;</ref><ref type="bibr" target="#b72">Lialin et al., 2023)</ref>, which fine-tunes a small number of (extra) model parameters while keeping most parameters of the pre-trained models frozen. In our work, we attempt to apply prompt tuning but we do not receive the expected results; details are described in Appendix B.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented BiomedGPT, a unified and generalist framework modeling multimodal tasks in medicine together, including radiographs, digital images, text, and bounding boxes. This task-, domain-and modalityagnostic model learns the universal comprehensiveness across different tasks and supports the unification of architecture. In addition, there's no requirement for further modifications to be specified during the finetuning phase, which can save valuable time and effort. The experiments conducted on approximately 20 public biomedical datasets validate that BiomedGPT can compete with previous SOTAs in recent years, which is exciting and provides the possibility of comprehensive representations used for versatile biomedical tasks. Our combined training approach has the potential to facilitate data-driven solutions to real-world problems in the biomedical domain. We further test zero-shot learning performance in domain/task transfer, verifying its effectiveness on biomedical tasks. Additionally, we conduct ablation studies in which we exclude certain task and modality groups. The objective is to examine the effects of various pretraining tasks and modalities on downstream performance. This area of study, currently open and ripe for further exploration, could provide valuable insights into the interactions between task and modality in the pretraining process. In the future, we hope to combine more meaningful tasks in medicine (e.g., segmentation, relation extraction) and more modalities into BiomedGPT and endeavor to understand the reason why universal representations can work well. best in the initial experiments) for both fine-tuning and inference. The corresponding performance in terms of CIDEr is 0.351, 0.360, 0.311, and 0.315. We follow the same strategy for other downstream datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Task-aware Instruction Generalization</head><p>We use two other datasets -Indian Movie Face database (IMFDB) <ref type="bibr" target="#b113">(Setty et al., 2013)</ref> and FG-NET <ref type="bibr" target="#b63">(Lanitis et al., 2002)</ref> to fine-tune BiomedGPT base for age prediction. There are 19,906 images with three classes in IMFDB and 1,002 images with 62 classes in the FG-NET dataset. We perform this task using taskaware input formats for both image classification and VQA. Specifically, we use the instructions "What does the image describe?" and "What is the chronological age?", respectively. An interesting observation is that BiomedGPT can achieve 94.8% and 19.9% accuracy on these two datasets with the first and default instruction, but fails to answer the question (0% accuracies) with the second and unseen instruction in the pretraining. This suggests that even if the instruction "What does the image describe?" is not strongly correlated with the age prediction task or other new classification tasks, we can still use it for general image classification since BiomedGPT can fully understand it from the pretraining stage. However, addressing task-aware instruction sensitivity remains an open question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Experiments</head><p>In this section, we discuss several additional experiments and their respective results. Although these experiments may not be comprehensive, they either address some limitations of the current BiomedGPT version or showcase performance comparisons with larger model scales and supplementary techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Performance with Larger Model</head><p>We evaluate the performance of BiomedGPTLarge, which has approximately 472 million parameters with 16 attention heads, 12 encoder layers, and 12 decoder layers for image classification tasks. The corresponding input size, visual backbone, embedding size, and hidden size are 480?480, ResNet152, 1024, and 4096, respectively. Table <ref type="table" target="#tab_6">6</ref> presents the results on selected MedMNIST datasets. We observe that BiomedGPTLarge generally achieves superior performance compared to smaller models, except on ChestMNIST, where all models exhibit limited performance. We are in the process of refining the BiomedGPT Large model, and potentially even a Huge-size model, by optimizing its parameters to fully exploit its potential. A comprehensive evaluation of its performance across a diverse range of biomedical tasks will be conducted, and the results will be analyzed to gain a deeper understanding of the strengths and limitations of larger models. This will contribute to the development of more robust and effective generalist biomedical AI models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Classification with High-resolution Images</head><p>In previous evaluations, we showcased BiomedGPT's proficiency in image classification tasks across different modalities using the MedMNIST v2 dataset. This dataset, however, consists of pre-processed images of dimensions 28 ? 28, which contrasts with the high-resolution images typically output by medical imaging devices. To address this, we turned to two public chest X-ray datasets released by NIH <ref type="bibr" target="#b46">(Jaeger et al., 2014)</ref> in this section: the Shenzhen chest X-ray set (SZ-CXR) and the Montgomery County chest X-ray set (MC-CXR). The SZ-CXR contains 662 images with dimensions 3000 ? 2919, and the MC-CXR has 138 images at 4892 ? 4020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Cross-modality Transferability</head><p>In this section, we explore the aspect of cross-domain transferability. Specifically, we fine-tune the BiomedGPT model, pre-trained solely with X-Ray (Radiology) data as described in Section 3.6, using datasets from other domains such as Microscopy. Additionally, we select MRI-only and CT-only imagetext pairs from SLAKE (EN), and conduct Visual Question Answering (VQA) fine-tuning. The results, measured in terms of accuracy, are presented in Table <ref type="table" target="#tab_7">8</ref>. Our findings indicate that cross-modality transfer with BiomedGPT is feasible, albeit with potentially significant performance degradation (for example, an absolute accuracy reduction of 8% with the base-size model on the DermaMNIST dataset). It's noteworthy that we had to double the training epochs as compared to the previous fine-tuning with a pre-trained model encompassing all modalities (100 vs. 50). we conclude that modality comprehensiveness is critical for a generalist biomedical model to facilitate efficient knowledge transfer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Datasets</head><p>In this section, we present the datasets utilized in our study, and we also engage in a detailed discussion on the preprocessing strategies tailored specifically for biomedical data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Pretraining Datasets</head><p>We construct pretraining datasets by incorporating vision and language data (i.e., image-text pairs), visiononly data (i.e., raw image data), and language data (i.e., plain texts). For replication, the training datasets are publicly available or easily accessed after request. For the datasets used in downstream tasks, we carefully filter our pretraining data and exclude images that appear in the validation and test sets to avoid data leakage. The statistics of the pretraining datasets are listed in Table <ref type="table" target="#tab_0">1</ref>. The detailed description of these datasets is shown as follows.</p><p>CheXpert <ref type="bibr" target="#b45">(Irvin et al., 2019</ref>) is a large dataset that contains 224,315 chest radiographs of 65,240 patients labeled for the presence of 14 common chest radiographic observations.</p><p>CytoImageNet <ref type="bibr" target="#b42">(Hua et al., 2021)</ref> contains 890K microscopy images with 894 classes, which are sourced from 40 openly available datasets such as 1) Recursion, 2) Image Data Resource <ref type="bibr" target="#b130">(Williams et al., 2017)</ref>, 3) Broad Bioimage Benchmark Collection <ref type="bibr" target="#b80">(Ljosa et al., 2012)</ref>, 4) Kaggle and 5) Cell Image Library.</p><p>International Skin Imaging Collaboration (ISIC) datasets have become a leading repository in skin cancer detection and malignancy assessment. In our pretraining, we used ISIC 2020 challenge dataset (33,126 images) that focused on melanoma detection <ref type="bibr" target="#b107">(Rotemberg et al., 2021)</ref>.</p><p>Retinal Fundus comes from Kaggle Diabetic Retinopathy competition <ref type="bibr" target="#b31">(Dugas et al., 2015)</ref>. The dataset contains 35,126 images acquired from different cameras under varying exposures with 5 categories.</p><p>MedICaT <ref type="bibr" target="#b117">(Subramanian et al., 2020)</ref>   <ref type="bibr" target="#b38">et al., 2000;</ref><ref type="bibr" target="#b50">Johnson et al., 2016)</ref>, encompassing around 1.8 million samples. We refrained from any preprocessing techniques <ref type="bibr" target="#b89">(Nuthakki et al., 2019)</ref>, which might have presented certain challenges during the pretraining process due to the presence of anonymized and irrelevant data, such as patients' names being replaced with "Xxxx". Furthermore, the language structure in clinical notes substantially deviates from the conventional domain text found in language model training corpora. Such notes frequently contain grammatical errors and use domain-specific terminology. These unique aspects result in notable disparities between clinical text and biomedical text (such as PubMed), despite both belonging to the medical domain.</p><p>Biomedical text, on the other hand, tends to be more coherent, edited, and refined. As part of our future work, we plan to improve the processing of inputs derived from clinical notes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Downstream Datasets</head><p>We verify the capability of BiomedGPT on various downstream tasks in fine-tuning and zero-shot settings.</p><p>The following lists different downstream datasets excluding ones that are shown in pretraining.</p><p>MedMNIST 2D v2. <ref type="bibr" target="#b136">(Yang et al., 2021</ref>) is a large-scale MNIST-like dataset collection of standardized biomedical images, designed to perform classification. In this work, we select ten 2D datasets, which are pre-processed into a small size of 28 ? 28 with the corresponding classification labels.</p><p>MedNLI <ref type="bibr" target="#b106">(Romanov &amp; Shivade, 2018</ref>) is annotated by doctors, and considered a natural language inference task (NLI), grounded in the medical history of patients. NLI is a task of determining whether the given "hypothesis" and "premise" logically follow (entailment) or unfollow (contradiction) or are undetermined (neutral) to each other. where M denotes the number of reference captions and g n (?) denotes an n-gram-based TF-IDF vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Detailed Experimental Outputs</head><p>In this section, we present selected experimental outputs for the tasks of text summarization and image captioning, rather than merely reporting their quantitative results. This decision is based on the recognition that existing evaluation metrics for these tasks may not always correlate well with human judgment <ref type="bibr" target="#b54">(Kilickaya et al., 2017;</ref><ref type="bibr" target="#b23">Cui et al., 2018)</ref>. We should note that the examples we present here have been randomly selected from each test set (with the exception of Peir Gross due to the potentially unsettling nature of its images), rather than carefully curated. Figure <ref type="figure" target="#fig_6">8</ref> and Figure <ref type="figure" target="#fig_7">9</ref> provide examples of image captioning from IU-Xray and ROCO, respectively, while Table <ref type="table" target="#tab_9">9</ref> illustrates examples of text summarization.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the BiomedGPT model. This showcases two examples of pretraining through image infilling using a masked image and through PrefixLM (Wang et al., 2022d) using an image-text pair. For text-only corpora, we can easily exclude the image patches and use only textual tokens. For MLM pretraining, we will mask partial tokens in the text input.</figDesc><graphic url="image-2.png" coords="4,95.40,81.86,421.21,208.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>); Raffel et al. (2020). Formally, suppose we are given a sequence of tokens x i,b as input, where i = 1, ? ? ? , I indexes the tokens in a data sample and b = 1, ? ? ? , B indexes a sample in a training batch. BiomedGPT's architecture is parametrized by ?. Then we autoregressively train the model via the chain rule as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The process example of trie-based beam search (beam size: 3, the maximum length of an output sequence: 4). Such search strategy can boost both the effectiveness and efficiency of BiomedGPT in various downstream tasks.</figDesc><graphic url="image-3.png" coords="6,306.00,431.05,234.00,97.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Examples from VQA-RAD with PubMedCLIP and our BiomedGPT. It is worth mentioning that, similar to PubMedCLIP, previous studies (Nguyen et al., 2019; Zhan et al., 2020) have also struggled to correctly interpret the question, resulting in irrelevant answers for the first three samples.</figDesc><graphic url="image-4.png" coords="9,72.00,81.86,468.00,159.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The zero-shot performance of pre-trained BiomedGPT and OFA with different model scales.Here, the model sizes are denoted by 'L', 'B', 'M', and 'S', which stand for large-, base-, medium-, and small-sized models, respectively.</figDesc><graphic url="image-5.png" coords="10,334.08,442.88,201.24,152.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>c) ? g n (S i ) ||g n (c)|| ? ||g n (S i )|| , (3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Examples of IU-Xray image captioning using BiomedGPT Base .</figDesc><graphic url="image-7.png" coords="30,107.10,135.57,397.80,519.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Examples of ROCO image captioning using BiomedGPT Base .</figDesc><graphic url="image-8.png" coords="31,107.10,145.61,397.80,499.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,72.00,447.05,468.00,203.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets for pretraining. "#Image" represents the total number of distinct images, and "#Sample" represents the number of training samples (e.g., the image-caption pair).</figDesc><table><row><cell>Type</cell><cell>Pretraining</cell><cell>Source</cell><cell>Domain / Modality</cell><cell>#Images</cell><cell>#Sample</cell></row><row><cell></cell><cell></cell><cell>MedICat</cell><cell>Radiology, histology, scope procedures, others</cell><cell>217,060</cell><cell>217,060</cell></row><row><cell></cell><cell>Captioning</cell><cell>IU X-ray</cell><cell>Chest x-ray</cell><cell>7,470</cell><cell>7,470</cell></row><row><cell>Vision &amp;</cell><cell></cell><cell>Peir Gross</cell><cell>Pathology / clinical photographs</cell><cell>7.442</cell><cell>7.442</cell></row><row><cell>Language</cell><cell>VQA</cell><cell>SLAKE PathVQA</cell><cell>Radiology (head, neck, chest, abdomen, pelvic cavity) The entire domain of pathology (He et al., 2020)</cell><cell>642 4,998</cell><cell>7,033 (EN) 32,799</cell></row><row><cell></cell><cell>Detection</cell><cell>DeepLesion OIA-DDR</cell><cell>CT (lung nodules, liver tumors, lymph nodes, etc) Fundus cameras</cell><cell>32,120 755</cell><cell>32,735 13,673</cell></row><row><cell>Vision</cell><cell>Image Filling</cell><cell>CheXpert CytoImageNet ISIC (2020)</cell><cell>Chest radiograph Microscopy Dermoscopy</cell><cell>224,315 890K 33,126</cell><cell>---</cell></row><row><cell></cell><cell></cell><cell>Retinal Fundus</cell><cell>Ophthalmology</cell><cell>5,126</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>PubMed Abstracts</cell><cell>Biomedcial articles</cell><cell>-</cell><cell>181 M</cell></row><row><cell>Language</cell><cell>MLM</cell><cell>NCBI BioNLP</cell><cell>Chemicals annotations, biomedical articles</cell><cell>-</cell><cell>52,976</cell></row><row><cell></cell><cell></cell><cell>MIMIC-III Clinic Notes</cell><cell>Medical records</cell><cell>-</cell><cell>1.8 M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Detailed model configuration of BiomedGPT. During the pretraining phase, image processing involves resizing and cropping the images to varying resolutions, corresponding to the input sizes listed in the table. It should be noted that during fine-tuning and inference stages, the input resolution of BiomedGPT can be flexibly adjusted according to the specific requirements of the task.</figDesc><table><row><cell>Model Scale</cell><cell>#Params.</cell><cell>Image Projection</cell><cell cols="2">Representation (Size)</cell><cell></cell><cell>Backbone (#)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Input Size Base Model Emb.</cell><cell>Hidden</cell><cell cols="3">Att. Head Enc. Layer Dec. Layer</cell></row><row><cell>BiomedGPT Small</cell><cell>33M</cell><cell>256 ? 256 ResNet50</cell><cell>256</cell><cell>1024</cell><cell>4</cell><cell>4</cell><cell>4</cell></row><row><cell>BiomedGPT Medium</cell><cell>93M</cell><cell>256 ? 256 ResNet101</cell><cell>512</cell><cell>2048</cell><cell>8</cell><cell>4</cell><cell>4</cell></row><row><cell>BiomedGPT Base</cell><cell>182M</cell><cell>384 ? 384 ResNet101</cell><cell>768</cell><cell>3072</cell><cell>12</cell><cell>6</cell><cell>6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Experimental results. The results of OrganMNIST consist of OrganAMNIST, OrganCMNIST and OrganSMNIST that correspond to axial / coronal / sagittal central slices. We only present state-of-the-art approaches if they provide open-sourced codes for guaranteed reproducibility.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Intra-&amp; inter-distribution transfer performance on VQA tasks in terms of accuracy. The best results are highlighted with BOLD values.</figDesc><table><row><cell>Dataset</cell><cell cols="5">Model Small Medium Base Large</cell></row><row><cell>SLAKE (EN)</cell><cell>Ours OFA</cell><cell>46.18 1.89</cell><cell>71.62 1.04</cell><cell>72.66 1.79</cell><cell>-1.60</cell></row><row><cell>PathVQA</cell><cell>Ours OFA</cell><cell>33.74 21.08</cell><cell>48.22 24.66</cell><cell cols="2">45.66 28.98 29.30 -</cell></row><row><cell>VQA-RAD</cell><cell>Ours OFA</cell><cell>30.38 31.49</cell><cell>31.49 34.81</cell><cell cols="2">33.26 30.82 33.92 -</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on holding out task groups. All the results are obtained from the small-scale model.</figDesc><table><row><cell>Model</cell><cell cols="5">Pneumonia ROCO VQA-RAD MeQSum MedNLI</cell></row><row><cell>Ours Small</cell><cell>91.8</cell><cell>13.2</cell><cell>37.5</cell><cell>42.2</cell><cell>69.3</cell></row><row><cell>w/o MLM</cell><cell>87.0</cell><cell>12.0</cell><cell>32.4</cell><cell>19.1</cell><cell>68.6</cell></row><row><cell>w/o MIM</cell><cell>88.3</cell><cell>12.2</cell><cell>33.5</cell><cell>44.3</cell><cell>69.9</cell></row><row><cell>w/o OD</cell><cell>88.3</cell><cell>12.7</cell><cell>37.7</cell><cell>44.8</cell><cell>68.2</cell></row><row><cell>w/o PTB</cell><cell>88.9</cell><cell>6.8</cell><cell>2.5</cell><cell>46.6</cell><cell>72.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Examples of image classification using BiomedGPT with different model scales.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Small Medium Base Large</cell></row><row><cell>TissueMNIST</cell><cell>36.4</cell><cell>36.4</cell><cell>53.2</cell><cell>69.7</cell></row><row><cell>OrganCMNIST</cell><cell>92.2</cell><cell>92.3</cell><cell>93.1</cell><cell>93.3</cell></row><row><cell>ChestMNIST</cell><cell>89.2</cell><cell>89.2</cell><cell>89.2</cell><cell>89.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Examples of image classification using BiomedGPT with different model scales. "Base w/ all" represents that the BiomedGPT Base pre-trained with all modalities. The VQA results are incomplete and we are working in progress for them.</figDesc><table><row><cell>Datasets</cell><cell cols="4">Small Medium Base Base w/ all</cell></row><row><cell>BloodMNIST</cell><cell>93.19</cell><cell>96.46</cell><cell>96.84</cell><cell>97.69</cell></row><row><cell cols="2">DermaMNIST 68.08</cell><cell>67.83</cell><cell>70.52</cell><cell>78.60</cell></row><row><cell>SLAKE-MRI</cell><cell>25.88</cell><cell>43.42</cell><cell>66.23</cell><cell>68.42</cell></row><row><cell>SLAKE-CT</cell><cell>41.95</cell><cell>57.20</cell><cell>65.25</cell><cell>80.51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>is created via extracting figures and captions from open-access papers in PubMed Central. 75% of its figures are compound figures, including several sub-figures. It contains over 217,000 images with captions and inline textual references.IU X-ray<ref type="bibr" target="#b26">(Demner-Fushman et al., 2016</ref>) is a set of chest X-ray images paired with their corresponding diagnostic reports. The dataset contains 7,470 pairs of images and reports.Peir Gross<ref type="bibr" target="#b49">(Jing et al., 2018)</ref> was collected with descriptions in the Gross sub-collection from PEIR digital library, resulting in 7.442 image-caption pairs from 21 different sub-categories. Each caption contains only one sentence.PathVQA<ref type="bibr" target="#b41">(He et al., 2020</ref>) is a pathology VQA dataset consisting of 4,998 images and 32,799 questionanswer pairs. There are 7 categories of questions: what, where, when, whose, how, how much/how many, and yes/no. The questions in the first 6 categories are open-ended while the rest are close-ended.DeepLesion<ref type="bibr" target="#b133">(Yan et al., 2018</ref>) is a dataset with 32,735 lesions in 32,120 CT slices from 10,594 studies of 4,427 unique patients. There are a variety of lesion types in this dataset, such as lung nodules, liver tumors, enlarged lymph nodes, etc.PubMed Abstracts are provided by BLUE benchmark<ref type="bibr" target="#b94">(Peng et al., 2019)</ref> containing 181 million sentences. We do not use PMC full articles because the prior works<ref type="bibr" target="#b39">(Gu et al., 2021)</ref>, training on both corpora of abstracts and articles surprisingly leads to a slight degradation in performance compared to sole training on PubMed abstracts.</figDesc><table><row><cell>SLAKE (Liu et al., 2021a) is a large bilingual (English &amp; Chinese) dataset with comprehensive semantic</cell></row><row><cell>labels annotated by experienced physicians. We only use English VQA samples of 642 images and 7K QA</cell></row><row><cell>pairs in our pretraining.</cell></row></table><note><p><p><p>NCBI BioNLP Corpus 8 is developed and maintained by NLM/NCBI BioNLP Research Group. For our pretraining process, we selectively employ the abstracts and full-text data, opting not to incorporate interaction-related information.</p>MIMIC-III Clinical</p>Notes are extracted and de-identified from the MIMIC-III database (Goldberger</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Examples of text summarization using BiomedGPT Base . MeQSum SUBJECT: just a question. MESSAGE: hi..just wanna ask... 1.how the aspirin can affect the ear? 2. what is the cause of suddenly ringging in the ear? isn't dangerous? tq.. :) Hi, I am a tall girl(5'8"), who wants to undergo leg shortening sugery of 2 inches for cosmetic purpose. It would be good if I can get more information about it. I would like to know the cost of this surgery, the recovery time and the risks associated with it. How long should I stay in the hospital? Thanks and regards Hi doctor, I have red circular lesions on lower extremities. Currently I am taking Amlodipine 10 mg qd, Benazepril 20 mg and Clonidine 0.2 mg. I have not yet taken any tests. Kindly suggest what can it be? Doctor: Hi. Kindly revert back with the pictures and answers to a dermatologist online -&gt; https://www.icliniq.com/ask-a-doctor-Hi doctor, During masturbation I just rub the tip of the penis and not the entire penis. Is it a wrong way of doing? I do not get excited during sex and unable to ejaculate. Only, rubbing the tip of the penis gives me excitement. Also, two weeks ago, I have undergone circumcision as my foreskin did not retract. Before circumcision too, I have the same problem. Please help. Doctor: Hi. For further doubts consult a sexologist online -&gt;https://www.icliniq.com/ask-a-doctor-I have a rash behind my ears that just developed this week. It started out on one ear and then the next day went to the other..its been a few days and I try not to touch them. But I work at a call center and have to wear headphones all day. And by the end of my shift my ears are itching and burning and inflamed. Now they are starting to seep and ooze, they are so itchy and the skin looks white and scaly. Ive read about all the dermatitis and psoriasis stuff. I find it hard for me to have that, I hav e never once had a skin problem in my life. Need help! Any answers? Doctor: Hi.As per your case history you are having allergic dermatitis.My treatment advice is -1. Avoid using any new products like soap or perfume.2. Apply a mild antibiotic plus steroid cream like fusiderm-B cream twice daily on it.3. Take an antihistamine like levocetirizine for 7-10days .4. Other treatment options are oral steroid and hydroxyzine given only after consulting a dermatologist.Thanks.Dr.Harshit Bhachech.MBBS, DDVL.</figDesc><table><row><cell>Dataset</cell><cell>Input</cell><cell>Reference</cell><cell>Hypothesis</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(Generation)</cell></row><row><cell></cell><cell></cell><cell>what causes ringing</cell><cell>what are the causes</cell></row><row><cell></cell><cell></cell><cell>in the ear, and can</cell><cell>of ringingging in the</cell></row><row><cell></cell><cell></cell><cell>aspirin affect the</cell><cell>ear?</cell></row><row><cell></cell><cell></cell><cell>ear?</cell><cell></cell></row><row><cell></cell><cell cols="2">SUBJECT: cosmetic leg shortening surgery. MESSAGE: where can i find information on leg</cell><cell>where can i find information on leg</cell></row><row><cell></cell><cell></cell><cell>shortening surgery,</cell><cell>shortening surgery?</cell></row><row><cell></cell><cell></cell><cell>including risks, cost,</cell><cell></cell></row><row><cell></cell><cell></cell><cell>and recovery time?</cell><cell></cell></row><row><cell>iCliniq</cell><cell>Patient: online/dermatologist</cell><cell>how to treat red circular lesions on lower extremities?</cell><cell>what can be the reason for red circular lesions on lower extremities?</cell></row><row><cell></cell><cell>Patient: online/sexologist</cell><cell>i masturbate only by rubbing the tip of the penis. is it a wrong way?</cell><cell>i masturbate only by rubbing the tip of the penis. is it a wrong way?</cell></row><row><cell>Healthcare</cell><cell cols="2">Patient: what causes itchy</cell><cell>suggest treatment for</cell></row><row><cell>Magic</cell><cell></cell><cell>rash with discharge</cell><cell>itchy rashes behind</cell></row><row><cell></cell><cell></cell><cell>behind the ears?</cell><cell>ears.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In this work, we build a hierarchy considering tasks, domains, and modalities. For instance, in the early-stage Alzheimer's prediction task, the task is general and requires domain-specific inputs, such as radiological imaging, which can be further specified into diverse modalities, including computed tomography (CT), magnetic resonance imaging (MRI) and others.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.cdc.gov/nchs/icd/icd10.htm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/taokz/BiomedGPT</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://pubmed.ncbi.nlm.nih.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://www.ncbi.nlm.nih.gov/pmc</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://openai.com/blog/chatgpt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://www.anthropic.com/index/introducing-claude</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://www.ncbi.nlm.nih.gov/research/bionlp/Data/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyperparameters and Sensitivity Analysis</head><p>Sensitivity, particularly hyperparameter sensitivity and instruction/prompt sensitivity, poses a significant challenge for developing a unified model, not just for biomedical generalist AI. In the following, we use examples to discuss exploiting prior knowledge for selecting hyperparameters and instructions to achieve good solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Hyperparameters for Fine-tuning</head><p>If not otherwise specified, we fine-tune BiomedGPT with 50 epochs and a learning rate of 7e-5 in terms of a batch size of 128, 64, and 32 for small-, medium-, and base-size models, respectively. The input image resolution is set to 256?256, dropout <ref type="bibr" target="#b116">(Srivastava et al., 2014)</ref> rate is set to 0.1and the other hyper-parameters remain the same as for pretraining.</p><p>Image Classification. In our model configuration, we assign a label smoothing <ref type="bibr" target="#b119">(Szegedy et al., 2016</ref>) ratio of 0.1. Adopting data augmentation strategies outlined in prior research <ref type="bibr">Wang et al. (2022b)</ref>; <ref type="bibr" target="#b9">Bao et al. (2021)</ref>, we use random resize cropping, random flipping, RandAug <ref type="bibr" target="#b22">(Cubuk et al., 2020)</ref>, and random erasing <ref type="bibr" target="#b145">(Zhong et al., 2020)</ref>. Furthermore, we incorporate Mixup <ref type="bibr" target="#b142">Zhang et al. (2018)</ref> and CutMix <ref type="bibr">(Yun et al., 2019a)</ref> augmentations, each presenting a 50% chance of being applied to every batch. We set the alpha parameters for Mixup and CutMix at 0.8 and 1.0, respectively. NLI and Text Summarization. For the NLI fine-tuning, we configure the maximum training epoch at 200, establish a learning rate of 7e-5, and apply a weight decay of 0.01. The maximum length of the target output is set at 30. For text summarization tasks, we implement variable learning rates of 1e-3, 5e-4, and 1e-4 for the MeQSum, Icliniq, and HealthCareMagic datasets, respectively, corresponding to a noise ratio of 0.2 each. We cap the maximum input text sequence length at 512 for each dataset, with a corresponding output text sequence length of 128. We also set the training epoch at 300, apply a length penalty of 0.7, and utilize a label smoothing factor of 0.1.</p><p>Image Captioning. The input image is resized to a resolution of 480 ? 480. The maximum output text lengths for IU-Xray, Peir Gross, and ROCO are set at 45, 20, and 30, respectively. The rationale behind the selection of these specific output lengths is detailed in Section A.2. By default, we set the beam search size to 10.</p><p>Visual Question Answering. In our experiments, we resize the input image to a fixed resolution of 480 ? 480. The maximum length for the output text is standardized to 30 across all datasets. Different datasets, however, necessitate varying fine-tuning epochs. Specifically, SLAKE and PathVQA are fine-tuned over 50 epochs, while VQA-RAD requires a more extensive fine-tuning period of 500 epochs due to the unseen data distribution in the pretraining and the lack of reported SOTA results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Hyperparameter Selection for Inference</head><p>We primarily discuss hyperparameter selection in the inference stage for tasks such as text summarization, image captioning, and VQA, as we observe that the model's performance is highly sensitive to beam search size and output length constraints. We use the IU X-ray dataset as an example. The default maximum target output is constrained to 20. If we select beam search sizes of 5, 10, and 20, respectively, we observe CIDEr increases from 0.164 to 0.294 and then 0.298. In most of our experiments, increasing beam search size typically improves predictions. Another component to consider is the maximum target output, as the default value may not be optimal for every dataset. Specifically, we calculate the length of answers in the training and validation sets, obtaining corresponding mean?std values of (39.57?19.95) and (39.25?20.62), respectively. Based on these statistics, we set the maximum target output as 40, 45, 50, and 60 with a beam size of 10 (the For comparison and following the approach of <ref type="bibr" target="#b120">(Tasci et al., 2021)</ref>, we employed these datasets for binary classification (normal vs. abnormal) and divided the dataset into a 70% / 10% / 20% split for training, validation, and testing, respectively. Table <ref type="table">7</ref> displays the accuracy of BiomedGPT at different scales. It is evident that BiomedGPT maintains robust performance even on high-resolution biomedical images. Particularly noteworthy is its performance on the larger downstream dataset (SZ-CXR), where it achieves approximately a 10% improvement compared to the reference model, Inception v3. BiomedGPT also exhibits competitive performance on the MC-CXR dataset, securing 26 correct predictions out of 29 samples in the testing set, compared to 27 correct predictions by Inception v3 when using augmentation techniques.</p><p>Table <ref type="table">7</ref>: We present the prediction accuracies of BiomedGPT when dealing with high-resolution medical images. Our report showcases the superior average results yielded by a single model on these two datasets, as documented in the existing literature. Moreover, the implementation of a voting-based ensemble method, as described in <ref type="bibr" target="#b120">(Tasci et al., 2021)</ref>, enhances the performance even further, achieving impressive results in excess of 95% for both datasets. For this analysis, the Inception v3 model was pre-trained.  Prompt tuning <ref type="bibr" target="#b67">(Lester et al., 2021</ref>) is a simple yet effective parameter-efficient fine-tuning (PEFT) mechanism <ref type="bibr">(Chen et al., 2023a)</ref>, which learns "soft prompts" or extra model parameters for each task instead of making a task-specific copy of the entire pre-trained model for each downstream task and inference must be performed in separate batches. We must mention that the addition of soft prompts is contrary to the design principle of the generalist model. In our experiments, we inject two prompt layers into the encoder and decoder, respectively following <ref type="bibr">(Yang et al., 2022b)</ref>, and vary the prompt length {20, 40, 60, 80, 100} to investigate the performance comparison against full-model fine-tuning using BiomedGPT Base .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>The accuracy of image classification on PathMNIST, Blood-MNIST, and PneumoniaMNIST was obtained after 100, 512, and 55 training epochs respectively, all with a consistent batch size of 512. Initial results in Figure <ref type="figure">7</ref> indicate that as the prompt length increases, the model performance tends to improve. However, despite an increased number of tuning epochs compared with fine-tuning on the original BiomedGPT, the performance after prompt tuning significantly lags behind that of model fine-tuning. Specifically, even when considering only the best results in prompt tuning, there are substantial accuracy reductions of 32.3%, 54.6%, and 32.6% on these three datasets, respectively.</p><p>What we need to say here is prompt tuning is typically used in fine-tuning large-scale models and becomes more competitive with scale: as models exceed billions of parameters, their method matches the strong performance of model tuning <ref type="bibr" target="#b67">(Lester et al., 2021)</ref>. Our implementation didn't achieve the expectation of memory efficiency, and we are working on the progress of the implementation optimization, that's why we didn't use prompt tuning for the larger model.</p><p>MeQSum, iCliniq, HealthCareMagic <ref type="bibr" target="#b0">(Abacha &amp; Demner-Fushman, 2019;</ref><ref type="bibr" target="#b140">Zeng et al., 2020)</ref> are abstractive summarization datasets and we preprocess them following BioBART <ref type="bibr" target="#b137">(Yuan et al., 2022)</ref>. Specifically, MeQSum contains 1000 refined patients' health questions selected from a collection distributed by the U.S. National Library of Medicine <ref type="bibr" target="#b55">(Kilicoglu et al., 2018)</ref>. iCliniq contains 31,062 patient-written summaries, and HealthCareMagic 226,405 abstractive samples written in a formal style. Each sample is comprised of a summary and corresponding dialogues between a patient and a doctor.</p><p>Radiology Objects in COntext (ROCO) <ref type="bibr" target="#b93">(Pelka et al., 2018</ref>) is a large-scale medical and multimodal imaging dataset. The ROCO images are from publications on the PubMed Central Open Access FTP mirror, which were automatically detected as non-compound and either radiology or non-radiology. ROCO is typically widely used in pretraining, and we are also considering incorporating it into the pretraining of our subsequent work.</p><p>VQA-RAD <ref type="bibr" target="#b64">(Lau et al., 2018)</ref> is the first manually constructed dataset where clinicians asked naturally occurring questions about radiology images and provided reference answers. We remove inappropriate characters in the original data, e.g., "slee\t n" ? "sleen".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Evaluation Metrics</head><p>In this study, we primarily employ four evaluation metrics to assess the performance of our models. These include accuracy for tasks such as image classification, natural language inference, and visual question answering; ROUGE-L <ref type="bibr" target="#b73">(Lin, 2004</ref>) for text summarization and image captioning tasks, which concentrates on the overlap of n-grams between system-generated and reference summaries; METEOR <ref type="bibr" target="#b8">(Banerjee &amp; Lavie, 2005)</ref>, a metric that calculates precision and recall using n-gram alignment between the hypothesis and the reference; and CIDEr <ref type="bibr" target="#b124">(Vedantam et al., 2015)</ref>, a metric known for its high agreement with human-assessed consensus in image captioning tasks.</p><p>In our work, our primary focus is optimizing for CIDEr. We select the model checkpoint that yields the highest CIDEr score in the validation phase for downstream tasks. Detailed mathematical expressions for captioning metrics are provided subsequently in this section.</p><p>ROUGE-L stands for recall-oriented understudy for gisting evaluation with the longest common subsequence. Given the candidate C and reference R, let LCS(C, R) be the length of the longest common subsequence, which is determined by using dynamic programming, it can be an expression as:</p><p>where R LCS = LCS(C,R) c</p><p>, P LCS = LCS(C,R) r , ? = P LCS R LCS . c and r represent the length of the candidate and reference.</p><p>METEOR stands for metric for evaluation of translation with explicit ordering. We represent precision and recall as P = m c and R = m r and let m be the number of common words in the candidate C and the reference R with the number of words of c and r, respectively. The METEOR is calculated via</p><p>where p is the penalty factor and is denoted as p = ?( ch m ) ? , ch is the number of chunks, which means a contiguous ordered block. ?, ?, ? are hyperparameters determined according to different datasets.</p><p>CIDEr stands for consensus-based image description evaluation. let c be a candidate caption, S be a set of reference captions, and CIDEr is obtained by averaging the similarity of different lengths:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the summarization of consumer health questions</title>
		<author>
			<persName><forename type="first">Asma</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abacha</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2228" to="2234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multimodal biomedical ai</title>
		<author>
			<persName><forename type="first">Guido</forename><forename type="middle">J</forename><surname>Juli?n N Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Falcone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><surname>Topol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1773" to="1784" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yana</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.14198</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Publicly available clinical BERT embeddings</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jindi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mcdermott</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-1909</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Clinical Natural Language Processing Workshop</title>
		<meeting>the 2nd Clinical Natural Language Processing Workshop<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Emily</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willie</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew Ba</forename><surname>Wa Redmond</surname></persName>
		</author>
		<author>
			<persName><surname>Mcdermott</surname></persName>
		</author>
		<title level="m">Publicly available clinical bert embeddings. NAACL HLT</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">72</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandipan</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Mckinnon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.08073</idno>
		<title level="m">Constitutional ai: Harmlessness from ai feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A cookbook of self-supervised learning</title>
		<author>
			<persName><forename type="first">Randall</forename><surname>Balestriero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Sobal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Bardes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregoire</forename><surname>Mialon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.12210</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Beit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Beit: BERT pre-training of image transformers</title>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhao</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=p-BhZSz59o4" />
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<date type="published" when="2022">April 25-29, 2022. 2022</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Michael S Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Biomedbert: A pre-trained biomedical language model for qa and ir</title>
		<author>
			<persName><forename type="first">Souradip</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaba</forename><surname>Bisong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shweta</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riley</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Mosconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="669" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adapting pretrained vision-language foundational models to medical imaging domains</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Chambon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bluethgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Chaudhari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.04133</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parameter-efficient fine-tuning design spaces</title>
		<author>
			<persName><forename type="first">Jiaao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=XSRSWxyJIC" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations, 2023a</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Towards a single unified model for effective detection, segmentation, and diagnosis of eight major cancers using a large collection of ct scans</title>
		<author>
			<persName><forename type="first">Jieneng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingda</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fakai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12291</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pix2seq: A language modeling framework for object detection</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=e42KbIw6Wb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, 2022a</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A unified sequence interface for vision tasks</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lala</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="31333" to="31346" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal masked autoencoders for medical vision-and-language pre-training</title>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinpeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Hui</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 18-22, 2022. 2022</date>
			<biblScope unit="page" from="679" to="689" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1179</idno>
		<ptr target="https://aclanthology.org/D14-1179" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10">October 2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1xMH1BtvB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to evaluate image captioning</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guandao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5804" to="5812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3965" to="3977" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Vilmedic: a framework for research at the intersection of vision and language in medical ai</title>
		<author>
			<persName><forename type="first">Jean-Benoit</forename><surname>Delbrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabri</forename><surname>Eyuboglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Chambon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Zambrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><surname>Langlotz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="23" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Preparing a collection of radiology examinations for distribution and retrieval</title>
		<author>
			<persName><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">D</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonya</forename><forename type="middle">E</forename><surname>Marc B Rosenman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laritza</forename><surname>Shooshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">R</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><forename type="middle">J</forename><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="304" to="310" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Parameter-efficient fine-tuning of large-scale pre-trained language models</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Min</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weize</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multiple metamodel quantifying for medical visual question answering</title>
		<author>
			<persName><forename type="first">Tuong</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erman</forename><surname>Binh X Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Tjiputra</surname></persName>
		</author>
		<author>
			<persName><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Quang D Tran</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="64" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Diabetic retinopathy detection</title>
		<author>
			<persName><forename type="first">Emma</forename><surname>Dugas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Cukierski</surname></persName>
		</author>
		<ptr target="https://kaggle.com/competitions/diabetic-retinopathy-detection" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Does clip benefit visual question answering in the medical domain as much as it does in the general domain</title>
		<author>
			<persName><forename type="first">Sedigheh</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Meinel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.13906</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pubmedclip: How much does clip benefit visual question answering in the medical domain?</title>
		<author>
			<persName><forename type="first">Sedigheh</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Meinel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melo</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EACL 2023</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1151" to="1163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12873" to="12883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Lloyd</forename><surname>Donald S Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">W</forename><surname>Aiello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">L</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><forename type="middle">D</forename><surname>Blankenship</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredrick</forename><forename type="middle">L</forename><surname>Cavallerano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Ferris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diabetes care</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">suppl_1</biblScope>
			<biblScope unit="page" from="84" to="87" />
			<date type="published" when="2004">2004</date>
			<publisher>American Diabetes Association</publisher>
		</imprint>
	</monogr>
	<note>Retinopathy in diabetes</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Man-Cho</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2304.04052</idno>
		<title level="m">Decoderonly or encoder-decoder? interpreting language model as a regularized encoder-decoder</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3816" to="3830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<author>
			<persName><surname>Ary L Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Plamen</forename><surname>Hausdorff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Ch Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">B</forename><surname>Mietus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Kang</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H Eugene</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="215" to="e220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computing for Healthcare (HEALTH)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards general purpose vision systems: An end-to-end task-agnostic vision-language architecture</title>
		<author>
			<persName><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amita</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="16399" to="16409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Pathvqa: 30000+ questions for medical visual question answering</title>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luntian</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10286</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Cytoimagenet: A large-scale pretraining dataset for bioimage transfer learning</title>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">M</forename><surname>Moses</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11646</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Gloria: A multimodal globallocal representation learning framework for label-efficient medical image recognition</title>
		<author>
			<persName><forename type="first">Shih-Cheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><surname>Matthew P Lungren</surname></persName>
		</author>
		<author>
			<persName><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3942" to="3951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-supervised learning for medical image classification: a systematic review and implementation guidelines</title>
		<author>
			<persName><forename type="first">Shih-Cheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><surname>Matthew P Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><forename type="middle">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><surname>Chaudhari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Medicine</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">74</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silviana</forename><surname>Ciurea-Ilcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Chute</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behzad</forename><surname>Haghgoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Shpanskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Two public chest x-ray datasets for computer-aided screening of pulmonary diseases. Quantitative imaging in medicine and surgery</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sema</forename><surname>Candemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Antani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y?-Xi?ng J</forename><surname>W?ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pu-Xuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Thoma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">475</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On the automatic generation of medical imaging reports</title>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2577" to="2586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Wei H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengling</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Anthony Celi</surname></persName>
		</author>
		<author>
			<persName><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mimic-iii, a freely accessible critical care database. Scientific data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<title level="m">Scaling laws for neural language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rethinking positional encoding in language pre-training</title>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Mmbert: multimodal bert pretraining for improved medical vqa</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Khare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viraj</forename><surname>Bagal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minesh</forename><surname>Mathew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adithi</forename><surname>Devi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Priyakumar</surname></persName>
		</author>
		<author>
			<persName><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1033" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Re-evaluating automatic metrics for image captioning</title>
		<author>
			<persName><forename type="first">Mert</forename><surname>Kilickaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aykut</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazli</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erkut</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Semantic annotation of consumer health questions</title>
		<author>
			<persName><forename type="first">Halil</forename><surname>Kilicoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yassine</forename><surname>Mrabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonya</forename><forename type="middle">E</forename><surname>Shooshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laritza</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Masterton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Vilt: Vision-and-language transformer without convolution or region supervision</title>
		<author>
			<persName><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5583" to="5594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Constituency parsing with a self-attentive encoder</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2676" to="2686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Breaking the dilemma of medical image-to-image translation</title>
		<author>
			<persName><forename type="first">Lingke</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Detian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanle</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qichao</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="1964">1964-1978, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Zeljko</forename><surname>Kraljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Shek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Bean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Bendayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName><surname>Medgpt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.03134</idno>
		<title level="m">Medical concept prediction from clinical narratives</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Self-supervised learning in medicine and healthcare</title>
		<author>
			<persName><forename type="first">Rayan</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Topol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1956" to="1981" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Toward automatic simulation of aging effects on face images</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Lanitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on pattern Analysis and machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="442" to="455" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A dataset of clinically generated visual questions and answers about radiology images</title>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Gayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Biobert: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diwakar</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Alsentzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.08091</idno>
		<title level="m">Do we still need clinical language models? arXiv preprint</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3045" to="3059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
		<ptr target="https://aclanthology.org/2020.acl-main.703" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Uni-perceiver v2: A generalist model for large-scale vision and vision-language tasks</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinguo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.09808</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A comparison of pre-trained vision-and-language models for multimodal representation learning across medical images and reports</title>
		<author>
			<persName><forename type="first">Yikuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE international conference on bioinformatics and biomedicine (BIBM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1999" to="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Scaling down to scale up: A guide to parameterefficient fine-tuning</title>
		<author>
			<persName><forename type="first">Vijeta</forename><surname>Vladislav Lialin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><surname>Rumshisky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.15647</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The unified medical language system</title>
		<author>
			<persName><forename type="first">Betsy</forename><forename type="middle">L</forename><surname>Donald Ab Lindberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexa</forename><forename type="middle">T</forename><surname>Humphreys</surname></persName>
		</author>
		<author>
			<persName><surname>Mccray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Yearbook of medical informatics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="41" to="51" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Slake: a semantically-labeled knowledge-enhanced dataset for medical visual question answering</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Ming</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Exploring and distilling posterior and prior knowledge for radiology report generation</title>
		<author>
			<persName><forename type="first">Fenglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexian</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13753" to="13762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning</title>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Muqeeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Mohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tenghao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1950" to="1965" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Annotated high-throughput microscopy image sets for validation</title>
		<author>
			<persName><forename type="first">Vebjorn</forename><surname>Ljosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><forename type="middle">L</forename><surname>Sokolnicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><forename type="middle">E</forename><surname>Carpenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="637" to="637" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Unified-io: A unified model for vision, language, and multi-modal tasks</title>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.08916</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Biogpt: generative pre-trained transformer for biomedical text generation and mining</title>
		<author>
			<persName><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Medvit: A robust vision transformer for generalized medical image classification</title>
		<author>
			<persName><forename type="first">Nejati</forename><surname>Omid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Manzari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Ahmadabadi</surname></persName>
		</author>
		<author>
			<persName><surname>Kashiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shahriar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Shokouhi</surname></persName>
		</author>
		<author>
			<persName><surname>Ayatollahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="page">106791</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Unsupervised and self-supervised deep learning approaches for biomedical text mining</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Nadif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fran?ois</forename><surname>Role</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1592" to="1603" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Overcoming data limitation in medical visual question answering</title>
		<author>
			<persName><forename type="first">Thanh-Toan</forename><surname>Binh D Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuong</forename><surname>Binh X Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erman</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quang D</forename><surname>Tjiputra</surname></persName>
		</author>
		<author>
			<persName><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2019: 22nd International Conference</title>
		<meeting><address><addrLine>Shenzhen, China</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">October 13-17, 2019. 2019</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="522" to="530" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A decade survey of transfer learning</title>
		<author>
			<persName><forename type="first">Shuteng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houbing</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="166" />
			<date type="published" when="2010">2010-2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Natural language processing of mimic-iii clinical notes for identifying diagnosis and procedures with neural networks</title>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Nuthakki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunil</forename><surname>Neela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><forename type="middle">W</forename><surname>Gichoya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saptarshi</forename><surname>Purkayastha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12397</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="27730" to="27744" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Self-evolving vision transformer for chest x-ray diagnosis through knowledge distillation</title>
		<author>
			<persName><forename type="first">Sangjoon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gwanghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujin</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon</forename><surname>Beom Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Hwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjun</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae-Kwang</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Chul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3848</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Radiology objects in context (roco): a multimodal image dataset</title>
		<author>
			<persName><forename type="first">Obioma</forename><surname>Pelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Koitka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>R?ckert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Nensa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="180" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shankai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th BioNLP Workshop and Shared Task</title>
		<meeting>the 18th BioNLP Workshop and Shared Task</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">An empirical study of multi-task learning on bert for biomedical text mining</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02799</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Alec Peltekian, and Gr?goire Altan-Bonnet. Scifive: a text-to-text transformer model for biomedical literature</title>
		<author>
			<persName><forename type="first">Long</forename><forename type="middle">N</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Anibal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaurya</forename><surname>Chanana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erol</forename><surname>Bahadroglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Psychovisual issues in the display of medical images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName><surname>Pizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pictorial information systems in medicine</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="211" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Adaptive histogram equalization and its variations. Computer vision, graphics, and image processing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Stephen M Pizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Philip Amburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Cromartie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trey</forename><surname>Geselowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Greer</surname></persName>
		</author>
		<author>
			<persName><surname>Ter Haar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">B</forename><surname>Romeny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName><surname>Zuiderveld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Transfusion: Understanding transfer learning for medical imaging</title>
		<author>
			<persName><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Med-bert: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction</title>
		<author>
			<persName><forename type="first">Laila</forename><surname>Rasmy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqian</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cui</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Degui</forename><surname>Zhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ digital medicine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">86</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Zolna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><forename type="middle">Gomez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mai</forename><surname>Gimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yury</forename><surname>Sulsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie</forename><surname>Kay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.06175</idno>
		<title level="m">Jost Tobias Springenberg, et al. A generalist agent</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Lessons from natural language inference in the clinical domain</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Shivade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1586" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">A patient-centric dataset of images and metadata for identifying melanomas using clinical context</title>
		<author>
			<persName><forename type="first">Veronica</forename><surname>Rotemberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Kurtansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brigid</forename><surname>Betz-Stablein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Caffery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanouil</forename><surname>Chousakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Combalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Dusza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Guitera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Gutman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Comparing encoder-only and encoderdecoder transformers for relation extraction from biomedical texts: An empirical study on ten benchmark datasets</title>
		<author>
			<persName><forename type="first">Mourad</forename><surname>Sarrouti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carson</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoann</forename><surname>Mamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randriamihaja</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Workshop on Biomedical Language Processing</title>
		<meeting>the 21st Workshop on Biomedical Language Processing</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="376" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2339" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Medical image captioning via generative pretrained transformers</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Selivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><forename type="middle">Y</forename><surname>Rogov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniil</forename><surname>Chesakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Shelmanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Fedulova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><forename type="middle">V</forename><surname>Dylov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">4171</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Indian movie face database: a benchmark for face recognition under wide variations</title>
		<author>
			<persName><forename type="first">Moula</forename><surname>Shankar Setty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parisa</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyothi</forename><surname>Beham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menaka</forename><surname>Gudavalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radhesyam</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidyagouri</forename><surname>Vaddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hemadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raja</forename><surname>Karure</surname></persName>
		</author>
		<author>
			<persName><surname>Raju</surname></persName>
		</author>
		<author>
			<persName><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 fourth national conference on computer vision, pattern recognition, image processing and graphics (NCVPRIPG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Transformers in medical imaging: A survey</title>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Shamshad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waqas</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munawar</forename><surname>Haris Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huazhu</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="page">102802</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Normformer: Improved transformer pretraining with extra normalization</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.09456</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Medicat: A dataset of medical images, captions, and textual references</title>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Bogin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sravanthi</forename><surname>Parasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2112" to="2120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">A voting-based ensemble deep learning method focusing on image augmentation and preprocessing variations for tuberculosis detection</title>
		<author>
			<persName><forename type="first">Erdal</forename><surname>Tasci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caner</forename><surname>Uluturk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aybars</forename><surname>Ugur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="15541" to="15555" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Medical transformer: Gated axial-attention for medical image segmentation</title>
		<author>
			<persName><forename type="first">Jeya</forename><surname>Maria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Poojan</forename><surname>Oza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilker</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2021: 24th International Conference</title>
		<meeting><address><addrLine>Strasbourg, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-10-01">September 27-October 1, 2021. 2021</date>
			<biblScope unit="page" from="36" to="46" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I 24</note>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Git: A generative image-to-text transformer for vision and language</title>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.14100</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Ofa: Unifying architectures, tasks, and modalities through a simple sequence-tosequence learning framework</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="23318" to="23340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Contrastive learning from unpaired medical images and text</title>
		<author>
			<persName><forename type="first">Zifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenbang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Medclip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-12">December 2022</date>
			<biblScope unit="page" from="3876" to="3887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Simvlm: Simple visual language model pretraining with weak supervision</title>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Image data resource: a bioimage data integration and publication platform</title>
		<author>
			<persName><forename type="first">Eleanor</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriella</forename><surname>Rustici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Tarkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anatole</forename><surname>Chessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Leo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B?lint</forename><surname>Antal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ugis</forename><surname>Richard K Ferguson</surname></persName>
		</author>
		<author>
			<persName><surname>Sarkans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="775" to="781" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">E2e-vlp: End-to-end vision-language pre-training enhanced by visual learning</title>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenming</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="503" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Clinical-bert: Vision-language pre-training for radiograph diagnosis and reports generation</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingtao</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2982" to="2990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Deeplesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medical imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36501</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">scbert as a large-scale pretrained deep language model for cell type annotation of single-cell rna-seq data</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="852" to="866" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Prompt tuning for generative multimodal pretrained models</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.02532</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">Medmnist v2: A large-scale lightweight benchmark for 2d and 3d biomedical image classification</title>
		<author>
			<persName><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donglai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zequan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilian</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.14795</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Biobart: Pretraining and evaluation of a biomedical generative language model</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruyi</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Workshop on Biomedical Language Processing</title>
		<meeting>the 21st Workshop on Biomedical Language Processing</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="97" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Graph transformer networks</title>
		<author>
			<persName><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raehyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Meddialog: Large-scale medical dialogue datasets</title>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenmian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqian</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruisi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9241" to="9250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Medical visual question answering via conditional reasoning</title>
		<author>
			<persName><forename type="first">Li-Ming</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2345" to="2354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=r1Ddp1-Rb" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<monogr>
		<title level="m" type="main">Large-scale domain-specific pretraining for biomedical vision-language processing</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaspreet</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Preston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Valluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.00915</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Contrastive learning of medical visual representations from paired images and text</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<monogr>
		<author>
			<persName><forename type="first">Ce</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiben</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifang</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.09419</idno>
		<title level="m">A comprehensive survey on pretrained foundation models: A history from bert to chatgpt</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Generalized radiograph representation learning via cross-supervision between images and free-text radiology reports</title>
		<author>
			<persName><forename type="first">Hong-Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruibang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liansheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="40" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
