<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Linked Latent Dirichlet Allocation in Web Spam Filtering *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">István</forename><surname>Bíró</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Informatics Laboratory Computer and Automation Research Institute</orgName>
								<orgName type="laboratory">Data Mining and Web search Research Group</orgName>
								<orgName type="institution">Hungarian Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dávid</forename><surname>Siklósi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Informatics Laboratory Computer and Automation Research Institute</orgName>
								<orgName type="laboratory">Data Mining and Web search Research Group</orgName>
								<orgName type="institution">Hungarian Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jácint</forename><surname>Szabó</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Informatics Laboratory Computer and Automation Research Institute</orgName>
								<orgName type="laboratory">Data Mining and Web search Research Group</orgName>
								<orgName type="institution">Hungarian Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">András</forename><forename type="middle">A</forename><surname>Benczúr</surname></persName>
							<email>benczur@ilab.sztaki.hu</email>
							<affiliation key="aff0">
								<orgName type="department">Informatics Laboratory Computer and Automation Research Institute</orgName>
								<orgName type="laboratory">Data Mining and Web search Research Group</orgName>
								<orgName type="institution">Hungarian Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Linked Latent Dirichlet Allocation in Web Spam Filtering *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A35A49ACA3E9642ABC28F80AF1AB3EA7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3 [Information Systems]: Information Storage and Retrieval; I.2.7 [Computing Methodologies]: Artificial Intelligence-Natural Language Processing text analysis</term>
					<term>feature selection</term>
					<term>document classification</term>
					<term>information retrieval Web content spam</term>
					<term>latent Dirichlet allocation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Latent Dirichlet allocation (LDA) <ref type="bibr" target="#b4">(Blei, Ng, Jordan 2003)</ref> is a fully generative statistical language model on the content and topics of a corpus of documents. In this paper we apply an extension of LDA for web spam classification. Our linked LDA technique takes also linkage into account: topics are propagated along links in such a way that the linked document directly influences the words in the linking document. The inferred LDA model can be applied for classification as dimensionality reduction similarly to latent semantic indexing. We test linked LDA on the WEBSPAM-UK2007 corpus. By using BayesNet classifier, in terms of the AUC of classification, we achieve 3% improvement over plain LDA with BayesNet, and 8% over the public link features with C4.5. The addition of this method to a log-odds based combination of strong link and content baseline classifiers results in a 3% improvement in AUC. Our method even slightly improves over the best Web Spam Challenge 2008 result.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Identifying and preventing spam is cited as one of the top challenges in web search engines in <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b22">22]</ref>. As all major search engines incorporate anchor text and link analysis algorithms into their ranking schemes, web spam appears in sophisticated forms that manipulate content as well as linkage <ref type="bibr" target="#b14">[14]</ref>.</p><p>In this paper we demonstrate the applicability of topic based natural language models for Web spam filtering. Several such generative models <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr">4]</ref> have been developed in the field of information retrieval. One of the most successful generative topic models is latent Dirichlet allocation (LDA) developed by Blei, Ng and Jordan <ref type="bibr">[4]</ref>, which is a fully generative graphical model with astonishing performance in various tasks. Several LDA extensions are known with wide range of applications in the fields of language processing, text mining and information retrieval, including categorization, keyword extraction, similarity search and statistical language modeling.</p><p>Recently several models extend LDA to exploit links between web documents or scientific papers <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b20">20]</ref>. In these models the term and topic distributions may be modified along the links. All these models have the drawback that every document is thought of either citing or cited, in other words, the citation graph is bipartite, and influence flows only from cited documents to citing ones.</p><p>In this paper we apply the recently developed linked LDA model <ref type="bibr" target="#b3">[3]</ref>, in which each document can cite to and be cited by others and thus be influenced and influence other documents. Linked LDA is very similar to the citation influence model of Dietz, Bickel and Scheffer <ref type="bibr" target="#b9">[9]</ref> with the main difference that in linked LDA the citation graph is not restricted to be bipartite. This fact and its consequences are the main advantage of linked LDA, namely, that the citation graph is homogeneous with no need for a citing and a cited copy of of each document, and finally, that influence may flow along paths of length more than one, a fact that gives power to learning over graphs <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b18">18]</ref>. In addition, linked LDA gives a flexible model of all possible aspects including cross-topic relations and link selection. The model may also distinguish between sites with strong, weak or even no influence from its neighbors. The linked LDA model is described in full detail in Section 2.1.</p><p>We demonstrate the applicability of linked LDA for Web spam filtering. The inferred topic distributions of documents are used as features. To assess the prediction power of these features, we test the linked LDA method in combination with the WEBSPAM-UK2007 public features 1 and SVM over tf.idf. Using a log-odds based random forest to aggregate these classifiers, the inclusion of linked LDA into the public and tf.idf features yields an improvement of 3% in AUC. For a detailed explanation, see Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related results</head><p>Spam hunters use a variety of content based features to detect web spam <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b21">21]</ref>; a recent measurement of their combination appears in <ref type="bibr" target="#b6">[6]</ref>. Perhaps the strongest SVM based content classification is described in <ref type="bibr" target="#b1">[1]</ref>. An efficient method for combining several classifiers is the use of log-odds averaging <ref type="bibr" target="#b19">[19]</ref>. In this paper we apply a modification of this method for combination: a random forest over the log-odds of the classifiers.</p><p>The first probabilistic models that jointly model text and link as well as the influence of topics along links is PHITS <ref type="bibr" target="#b7">[7]</ref> and the mixed membership model <ref type="bibr" target="#b10">[10]</ref>. Later, several similar link based LDA models were introduced, including the copycat and the citation influence models <ref type="bibr" target="#b9">[9]</ref> and the link-PLSA-LDA and pairwise-link-LDA models <ref type="bibr" target="#b20">[20]</ref>. These two results extend LDA over a bipartition of the corpus into citing and cited documents such that influence flows along links from cited to citing documents. They are shown to outperform earlier methods <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b20">20]</ref>. While these models generate topical relation for hyperlinked documents, in a homogeneous corpus one has to duplicate each document and infer two models for them. This is in contrast to the linked LDA model that treats citing and cited documents identically.</p><p>Our results improve over our related multicorpus LDA model <ref type="bibr" target="#b2">[2]</ref> for Web spam detection. Multicorpus LDA separately builds LDA models for the collection of spam and normal sites, then take the union of the resulting topic collections and make inference with respect to this aggregated collection of topics for every unseen document d. The total probability of spam topics in the topic distribution of d may serve as a spamicity-measure. In this paper we do not make experiments on multicorpus LDA, but make comparison to the measurements presented in <ref type="bibr" target="#b2">[2]</ref>, see Subsection 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHOD</head><p>In the classical latent Dirichlet allocation model <ref type="bibr">[4]</ref> we have a vocabulary V consisting of terms, a set T of k topics and a set D of m documents of arbitrary length. For every topic z ∈ T a distribution ϕz on V is sampled from Dir(β), where β ∈ R V + is a positive smoothing parameter. Similarly, for every document d a distribution ϑ d on T is sampled from Dir(α), where α ∈ R T + is a positive smoothing parameter. The words of the documents are drawn as follows: for every word position of document d a topic z is drawn from ϑ d , and then a term is drawn from ϕz and filled into that position. The notation is summarized in the widely used Bayesian network representation of LDA in Figure <ref type="figure" target="#fig_0">1</ref>. Inference is mostly done by Gibbs sampling or variational inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Linked LDA</head><p>The linked LDA model extends LDA to model the effect of a hyperlink between two documents on the topic and term distributions. The key idea, summarized as a Bayes net in Figure <ref type="figure" target="#fig_1">2</ref>, is to modify the topic distribution of a position on the word plate based on a link from the current document on the document plate. Linked LDA relies on the LDA distributions ϕz and ϑ d , but involves an additional distribution χ d • draw a topic z from ϑr (instead of ϑ d as in LDA),</p><p>• draw a term from ϕz and fill into the position. Note that for sake of a unified treatment, d itself can be an influencing document of itself. Note that the citation influence model <ref type="bibr" target="#b9">[9]</ref> has a nonuniform solution such that for every word a Bernoulli trial is used to decide whether the influencing document is d itself or one of its outneighbors.</p><p>For inference a Gibbs sampling procedure can be constructed (described in <ref type="bibr" target="#b3">[3]</ref>), along the same lines as for LDA <ref type="bibr" target="#b15">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>We make experiments on the WEBSPAM-UK2007 corpus <ref type="foot" target="#foot_1">2</ref> . In order to define features for hosts, we aggregate the words appearing in all HTML pages of the host to form one document per host in a bag of words model. We keep only alphanumeric characters and the hyphen but remove all words containing a hyphen not between two alphabetical words. After stemming by TreeTagger 3 and removing stop words by the Onix list 4 , the most frequent 100,000 words form the vocabulary.</p><p>Our experiments include the 6000 hosts having a WEBSPAM-UK2007 spam or normal label along with an additional 39,000 hosts linked by one of these. We weight directed links between hosts by their multiplicity, and for every site we keep only at most 10 outlinks with largest weight. After linked LDA inference is run, we have the ϑ topic distribution vectors as features to the classification.</p><p>In our experiments we perform two-class spam classification. We use the linear kernel SVM, C4.5 decision tree and Bayes net implementations of the machine learning toolkit Weka <ref type="bibr" target="#b23">[23]</ref>.</p><p>As the simplest baseline we use the public features 5 with C4.5 decision tree and the tf.idf vectors with SVM. For tf.idf, only terms appearing in at least half of the sites are kept. Another baseline is formed by the ϑ topic distributions of the original LDA model as features for classification by BayesNet. Both the baseline and the linked LDA based classifiers are trained on the WEBSPAM-UK2007 training labels (3900 sites) and are evaluated on the WEBSPAM-UK2007 test labels (2027 sites) for direct comparability with the Web Spam Challenge 2008 results.</p><p>Our combination of the classifiers is inspired by the logodds averaging by Lynam and Cormack <ref type="bibr" target="#b19">[19]</ref>. We first make a 10-fold cross validation on the WEBSPAM-UK2007 training labels to score every host by every classifier. Then for every classifier we calculate the log-odds as a feature, which is the logarithm of the fraction of the number of spams with lower score over the number of normal sites with higher score. Finally, we train a random forest over this (quite small) feature set and give predictions for the WEBSPAM-UK2007 test labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">LDA parameters</head><p>For LDA inference the following parameter settings are used. The number of topics is chosen to be k = 30 and k = 90. The Dirichlet parameter vector β is constant 200/|V |, and α is constant 50/k. For a document d, the smoothing parameter vector γ d is chosen in such a way that</p><formula xml:id="formula_0">γ d (c) ∝ w(d → c) for all c ∈ S d , c = d and γ d (d) ∝ 1 + X c∈S d ,c =d w(d → c) such that P c∈S d γ d (c) = |d|/p</formula><p>, where |d| is the number of word positions in d (the document length), w(d → c) denotes the multiplicity of the d → c link in the corpus, and p is a normalization parameter. We tried three values p = 1, 4, 10.</p><p>We have developed an own C++-code for LDA and linked LDA, which is publicly available 6 . The computations were run on Linux machines with 50GB RAM and multicore 64bit 3.2 GHz Xeon processors with 2MB cache.</p><p>We performed 50 iterations for Gibbs sampling as several measurements indicate that both the AUC value on classifying over the topic distributions and the likelihood stabilizes after 50 iterations <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b3">3]</ref>. 3 http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/ 4 http://www.lextek.com/manuals/onix/stopwords1.html 5 http://www.yr-bcn.es/webspam/datasets/uk2007/features/ 6 http://www.ilab.sztaki.hu/~ibiro/linkedLDA/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>The results of the classification can be seen in Tables <ref type="table" target="#tab_0">1</ref><ref type="table" target="#tab_1">2</ref><ref type="table" target="#tab_2">3</ref>, the evaluation metric is AUC. For linked LDA only the parameter choice p = 4, k = 30 was included in the combination, as it gave the best result. p = 1 p = 4 p = 10 k = 30 0.768 0.784 0.783 k = 90 0.764 0.777 0.773  <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref> with a log-odds based random forest. For linked LDA the parameters are chosen to be p = 4, k = 30.</p><p>The tables indicate that linked LDA slightly outperforms LDA by about 3% using BayesNet, showing the predicting power of the links in the corpus. Most notably, linked LDA achieved 8% improvement over the public link features with C4.5, and it is at par with the public content features. The addition of linked LDA to the log-odds based combination of the public and tf.idf based classifiers results in a 3% improvement in AUC.</p><p>We trained the classifiers on the WEBSPAM-UK2007 training and evaluated them on the WEBSPAM-UK2007 testing labels as in the Web Spam Challenge 2008 setup. Thus we can compare these measurements to the Challenge results <ref type="foot" target="#foot_2">7</ref> . The present methods improve a lot over the 0.796 AUC achieved by our research group using the multicorpus LDA model. The winner, Geng et al., managed to have an AUC of 0.848, and even this value is improved by our public &amp; tf.idf &amp; linked LDA combination, with AUC 0.854.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and future work</head><p>In this paper we applied the newly introduced linked LDA model <ref type="bibr" target="#b3">[3]</ref> to Web spam classification. In our experiments linked LDA outperformed LDA and other baseline classifications by about 3-8% in AUC. Combining tf.idf, the public and the linked LDA features with a log-odds based random forest we achieved an AUC of 0.854, beating the Web Spam Challenge 2008 winner (0.848). As another experiment we are currently measuring the quality of the inferred linked LDA edge weights χ, by using it in a stacked graphical classification procedure, for both the link graph and the cocitation graph.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: LDA as a Bayesian network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Linked LDA as a Bayesian network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">: Classification accuracy measured in AUC</cell></row><row><cell cols="2">for linked LDA with various parameters, classified</cell></row><row><cell>by BayesNet.</cell><cell></cell></row><row><cell>features</cell><cell>AUC</cell></row><row><cell>LDA with BayesNet</cell><cell>0.766</cell></row><row><cell>tf.idf with SVM</cell><cell>0.795</cell></row><row><cell>public (link) with C4.5</cell><cell>0.724</cell></row><row><cell cols="2">public (content) with C4.5 0.782</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy measured in AUC for the baseline methods.</figDesc><table><row><cell>features</cell><cell>AUC</cell></row><row><cell>tf.idf &amp; LDA</cell><cell>0.827</cell></row><row><cell>tf.idf &amp; linked LDA</cell><cell>0.831</cell></row><row><cell>public &amp; LDA</cell><cell>0.820</cell></row><row><cell>public &amp; linked LDA</cell><cell>0.829</cell></row><row><cell>public &amp; tf.idf</cell><cell>0.827</cell></row><row><cell>public &amp; tf.idf &amp; LDA</cell><cell>0.845</cell></row><row><cell>public &amp; tf.idf &amp; linked LDA</cell><cell>0.854</cell></row><row><cell cols="2">public &amp; tf.idf &amp; LDA&amp; linked LDA 0.854</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracy measured in AUC by combining the classifications of Tables</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.yr-bcn.es/webspam/datasets/uk2007/features/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://barcelona.research.yahoo.net/webspam/datasets/uk2007/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_2"><p>http://webspam.lip6.fr/wiki/pmwiki.php?n=Main.PhaseIIIResults</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* This work was supported by the EU FP7 project LiWA -Living Web Archives and by grants OTKA NK 72845, ASTOR NKFP 2/004/05</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">WITCH: A New Approach to Web Spam Detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Abernethy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Adversarial Information Retrieval on the Web (AIRWeb)</title>
		<meeting>the 4th International Workshop on Adversarial Information Retrieval on the Web (AIRWeb)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Latent Dirichlet Allocation in Web Spam Filtering</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bíró</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Szabó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Benczúr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>manuscript</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Very Large Scale Link Based Latent Dirichlet Allocation for Web Document Classification</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bíró</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Szabó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Benczúr</surname></persName>
		</author>
		<ptr target="http://www.ilab.sztaki.hu/~ibiro/linkedLDA/" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>manuscript</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent Dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spam Filtering Using Statistical Data Compression Models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bratko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Filipič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zupan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2673" to="2698" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Know your neighbors: web spam detection using the web topology</title>
		<author>
			<persName><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Silvestri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 30th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="423" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<title level="m">The Missing Link-A Probabilistic Model of Document Content and Hypertext Connectivity. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="430" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society of Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised prediction of citation influences</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mixed-membership models of scientific publications</title>
		<author>
			<persName><forename type="first">E</forename><surname>Erosheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fienberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spam, damn spam, and statistics -Using statistical analysis to locate spam web pages</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Workshop on the Web and Databases (WebDB)</title>
		<meeting>the 7th International Workshop on the Web and Databases (WebDB)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Detecting phrase-level duplication on the world wide web</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Research and Development in Information Retrieval (SIGIR)</title>
		<meeting>the 28th ACM International Conference on Research and Development in Information Retrieval (SIGIR)<address><addrLine>Salvador, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finding scientific topics</title>
		<author>
			<persName><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">suppl 1</biblScope>
			<biblScope unit="page" from="5228" to="5235" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Web spam taxonomy</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gyöngyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Workshop on Adversarial Information Retrieval on the Web (AIRWeb)</title>
		<meeting>the 1st International Workshop on Adversarial Information Retrieval on the Web (AIRWeb)<address><addrLine>Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Parameter estimation for text analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heinrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Challenges in web search engines</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Silverstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR Forum</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="11" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised Learning by Probabilistic Latent Semantic Analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="196" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stacked graphical models for efficient inference in markov random fields</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM 07</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On-line spam filter fusion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lynam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cheriton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 29th international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>of the 29th international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="123" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Joint Latent Topic Models for Text and Citations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 14th ACM SIGKDD international conference on Knowledge discovery and data mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting spam web pages through content analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ntoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Najork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International World Wide Web Conference (WWW)</title>
		<meeting>the 15th International World Wide Web Conference (WWW)<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Challenges in running a commercial search engine</title>
		<author>
			<persName><forename type="first">A</forename><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IBM Search and Collaboration Seminar</title>
		<imprint>
			<publisher>IBM Haifa Labs</publisher>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining: Practical Machine Learning Tools and Techniques</title>
		<title level="s">Morgan Kaufmann Series in Data Management Systems</title>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2005-06">June 2005</date>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nonparametric transforms of graph kernels for semi-supervised learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1641" to="1648" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
