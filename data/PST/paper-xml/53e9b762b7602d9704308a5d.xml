<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How effective are landmarks and their geometry for face recognition?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2006-02-20">20 February 2006</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">J</forename><surname>Shi</surname></persName>
							<email>jshi@cse.unl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering Department</orgName>
								<orgName type="institution">University of Nebraska-Lincoln</orgName>
								<address>
									<postCode>68588-0115</postCode>
									<region>NE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><surname>Samal</surname></persName>
							<email>samal@cse.unl.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering Department</orgName>
								<orgName type="institution">University of Nebraska-Lincoln</orgName>
								<address>
									<postCode>68588-0115</postCode>
									<region>NE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">D</forename><surname>Marx</surname></persName>
							<email>dmarx@unl.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Statistics Department</orgName>
								<orgName type="institution">University of Nebraska-Lincoln</orgName>
								<address>
									<postCode>68583-0712</postCode>
									<region>NE</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">How effective are landmarks and their geometry for face recognition?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2006-02-20">20 February 2006</date>
						</imprint>
					</monogr>
					<idno type="MD5">436CF867E4DD75782A712363C2F57C54</idno>
					<idno type="DOI">10.1016/j.cviu.2005.10.002</idno>
					<note type="submission">Received 12 April 2005; accepted 28 October 2005</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Face recognition</term>
					<term>Landmarks</term>
					<term>Procrustes distance</term>
					<term>Similarity measure</term>
					<term>FERET</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper evaluates how biologically meaningful landmarks and their geometry extracted from face images can be used for face recognition. The traditional Procrustes distance is studied for the landmark-based face model. By using complex principal component analysis, we propose a refined Procrustes distance that incorporates statistical correlation of landmarks. Motivated by research results from art, anthropology, and aesthetic surgery where ratios play a significant role in human face descriptions, we also investigate how well-normalized Euclidean distances (special ratios) can be exploited for face recognition. Exploiting symmetry and using principal component analysis, we reduce the number of ratios to 20. In this investigation, we analyze the effectiveness of three well-known similarity measures including the typical l 1 norm, l 2 norm, and Mahalanobis distance. We also define a new similarity measure called the eigenvalue-weighted cosine (EWC) distance. We evaluate our approach using two standard face databases: the Purdue AR and the FERET database. The former has well-controlled face images and we use it to test the reliability of landmarks. The FERET database is used for performance evaluation of face recognition algorithms. This papers use a performance measure called the cumulative match score, which indicates the percentage of faces that can be eliminated from consideration. Experimental results show that our EWC distance outperforms the l 1 and l 2 norms and even the Mahalanobis distance. Our study also finds that landmarks and their geometry-based approach can account for variations of face expression and aging very well. Thus, they can be used either in stand-alone mode or in conjunction with other approaches to reduce the search space a priori.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Humans can recognize human faces with little difficulty even in complex environments. Therefore, the human face is a natural and attractive biometric. Since the 1990s, human face recognition has attracted a great deal of attention in computer vision and pattern recognition. Three major reasons can explain this trend: the ever-increasing commercial, security, and law enforcement applications <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, significant progress in computer hardware and software technologies, and the development of face image databases and standardized evaluation benchmarks <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>.</p><p>While significant progress has been made, automated face recognition remains a challenge and an area of active research. Most approaches to automate human face recognition typically analyze face images or face-focused video sequences either automatically or semiautomatically <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Experimental results can thus be collected and approaches evaluated quickly. One common disadvantage is that the approaches are sensitive to the experimental environment such as variations in illumination, pose, expression, and aging.</p><p>The idea of face recognition based on geometry was proposed several decades ago <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. Face recognition has been receiving more and more attention, large standard face datasets have become available, and many state-of-the art face recognition systems typically combine geometrical features and texture information (e.g., active appearance model <ref type="bibr" target="#b11">[12]</ref>, local feature analysis <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, and elastic graph matching <ref type="bibr" target="#b14">[15]</ref>). Therefore, it is worthwhile to take a broad and more rigorous view of geometry-based methods. Because a face is modeled by only a limited number of biologically meaningful landmarks, face recognition performance cannot be optimal and will generally not be as good as appearance-based models that exploit significantly more information encoded in a face image. However, the study of geometry-based face recognition will contribute to advances in developing hybrid methods as well as faster methods.</p><p>This paper investigates computationally efficient approaches for face recognition from the perspective of geometric attributes of human faces. Biologically meaningful geometric features include landmarks (points on faces), proportions of distances among landmarks, and angles of landmarks. This paper focuses on two typical features: landmarks and normalized Euclidean distances, because the process of exploiting landmarks is a special case while the process for other geometric features is the same. Modeling faces using geometric features, computing similarity between faces, and determining the best geometric features for face recognition are three problems investigated in this paper.</p><p>Face modeling is an important step in automated face recognition. A face model should first be able to account for different subjects with distinct representations. A good model should also be simple and not be overly sensitive to insignificant variations of faces. A face has a large number of features, and researchers have used different sets of features based on their suitability for recognition and availability of robust algorithms to locate them automatically. A basic requirement of a face model is that it should be a one-to-one mapping from a subject space to the face model space. To reduce computational complexity and also improve robustness, a good face model should also be characterized by a low-dimensional feature space in which the features are uncorrelated. Literature in pattern recognition and data mining is replete with methods for dimension reduction <ref type="bibr" target="#b15">[16]</ref>. Our proposed face models are based on the Euclidean geometric features and defined using anthropologically relevant measurements. We first investigate how well biologically meaningful landmarks can be used for face modeling. Based on these landmarks, we also develop a ratio-based face model. Principal component analysis (PCA) is employed to reduce the dimension of face models.</p><p>Developing appropriate measures for similarity or dissimilarity is an important step in face recognition. Recognition of human faces is complex due to variations of faces such as illumination condition, viewing direction pose, facial expression, occlusion (such as glass and face hair), and face aging. A good measure should be able to extract as much dissimilarity among different subjects as possible and simultaneously be independent of the non-subject variations. For the landmark-based face model, the paper proposes a refined Procrustes distance for face recognition. The Procrustes distance has been studied widely in statistical shape analysis <ref type="bibr" target="#b16">[17]</ref> for landmark-based superposition. It is used typically to compute the mean shape of an ensemble of 2D objects. <ref type="bibr">Vaswani et al.</ref> proposed a method to detect abnormal events associated with 2D moving objects by analyzing the mean shape <ref type="bibr" target="#b17">[18]</ref>. Wang et al. <ref type="bibr" target="#b18">[19]</ref> proposed an automatic gait recognition method based on the full Procrustes similarity measure. Duta and Jain <ref type="bibr" target="#b19">[20]</ref> studied 2D shape models based on the Procrustes distance using the mean shape. The traditional Procrustes distance, however, does not incorporate statistical correlation of landmarks. Motivated by the Mahalanobis distance, we propose a refined Procrustes distance. It uses complex principal component analysis (CPCA) to reduce the correlation of landmarks and uses the eigenvalues to normalize their distribution. For the ratio-based face model, we explore three typical similarity measures: l 1 norm, l 2 norm, and Mahalanobis distance <ref type="bibr" target="#b20">[21]</ref>. In addition, we propose a new similarity measure that improves on the standard cosine similarity measure in this application. Like the Mahalanobis distance, our refined cosine distance can balance different frequency components in a measured space <ref type="bibr" target="#b20">[21]</ref>.</p><p>The methods studied in this paper are labeled as partially automated. The landmarks are first identified by persons (called readers in this paper), using a procedure similar to that described in <ref type="bibr" target="#b9">[10]</ref>. The rest of the process is fully automated, however. The repeatability of measurements from face images by different readers is a concern of this approach. Using a general linear mixed model that performs analysis of variance, we show that variation introduced in the measurements of the landmarks by readers and repetitions is significantly less than the variation in the face images themselves.</p><p>The original goal of this research was to use the landmarks and their geometry to reduce the search space for the face recognition process. Holistic face recognition approaches are likely to be more accurate since they use significantly more information about the face (e.g., the intensity and texture). However, they are also likely to be slow due to the complexity of the matching process. The landmarks from known faces in the database can be accurately captured either through a manual method or via automated means augmented with human verification. Note that the landmarks are extracted only once for each face in a database. When an unknown face is presented, its landmarks are first extracted and then matched with the stored landmarks for the faces in the database. The landmark-based approaches can thus be faster. This becomes even more significant as the size of the database increases. In fact, results of the face recognition vendor test <ref type="bibr">(FRVT 2002)</ref> indicate that performance of human face recognition algorithms degrades as the size of a face search space increases <ref type="bibr" target="#b3">[4]</ref>. Therefore, landmark-based recognition can be used as a filtering operation that removes a large fraction of faces from being considered for a more accurate and expensive matching approach. The results of geometry-based approaches using the FERET database are comparable to current face recognition algorithms <ref type="bibr" target="#b2">[3]</ref>. Thus, this approach can be used either directly or as a filtering step in a large database.</p><p>Notation. In this paper, elements of vectors and matrices are denoted by lowercase italics, vectors are denoted by lowercase bold characters, and face image spaces, sets, and matrices are denoted by uppercase bold characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N and b</head><p>N denote the original dimensions and the compressed dimension of a face model, respectively. Because this paper presents several face models, the dimension N and b</p><p>N may have different meanings depending on the context.</p><p>The rest of this paper is organized as follows. In Section 2, we provide an overview of the two most relevant face models: the holistic image-based face model and the geometric feature-based face model. Section 3 discusses how landmarks on face images can be exploited for face recognition. The traditional and refined Procrustes distances are studied in this section as well. We describe how ratios of two Euclidean distances can be used to model faces in Section 4. The similarity measures used in our research are described in this section. In Section 5, we present the experimental results. The reliability of landmarks and performance of the approach for face recognition are also described here. Finally, Section 6 presents our conclusion and directions for our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Overview of face models</head><p>TodayÕs machines still cannot recognize faces as well as human beings. One reason is that we cannot specify exactly which features play the most significant roles in face recognition. Face models that can efficiently characterize human faces are still under investigation. Representation of faces is studied extensively in the vast body of literature in different aspects of face recognition including detection (see YoungÕs recent survey <ref type="bibr" target="#b21">[22]</ref>), identification (see <ref type="bibr" target="#b1">[2]</ref>, for a recent survey), and analysis of facial expression (see PanticÕs recent survey <ref type="bibr" target="#b22">[23]</ref>). A complete review of techniques is beyond the scope of this paper. The survey articles mentioned above give a broad view of the state-of-the-art. In this section, we briefly review two face models most relevant to our research: the holistic image-based model and the geometric feature-based model. The former is more straightforward, but is more verbose and usually requires more computation. One major challenge of the latter model is the precise extraction of features using automated methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Holistic image-based face models</head><p>With a holistic image-based face model, a two-dimensional discrete face image f can be presented by</p><formula xml:id="formula_0">f ¼ ff ði; jÞ 2 R C jði; jÞ 2 Kg;<label>ð1Þ</label></formula><p>where K stands for a rectangular grid and C stands for the number of channels (e.g., color face images may have three channels and grey level face images have only one channel).</p><p>For ease of computation, the face image, f, can be ordered lexicographically by stacking either rows or columns into a vector. Thus,</p><formula xml:id="formula_1">f ¼ ½f 1 ; f 2 ; . . . ; f N T ;<label>ð2Þ</label></formula><p>where N = |f| is the number of total pixels of the face f and T denotes the vector transpose. In practice, discrimination power and computational efficiency are two important considerations in modeling of the face <ref type="bibr" target="#b23">[24]</ref>. In other words, the representation of a face should have sufficient information to differentiate each pair of dissimilar face images in a compact face space. Let X stand for the original face space spanned by X face images as</p><formula xml:id="formula_2">X ¼ spanff 1 ; f 2 ; . . . ; f X g.<label>ð3Þ</label></formula><p>The compact space b X can be mapped from X by a set of b N transform functionals as</p><formula xml:id="formula_3">fT i : R C Â R N ! R C g N . i¼1<label>ð4Þ</label></formula><formula xml:id="formula_4">Thus, the transformed face image b f i of f i is b f i ¼ ½T 1 ðf i Þ; T 2 ðf i Þ; . . . ; T Nðf i Þ T<label>ð5Þ</label></formula><formula xml:id="formula_5">and the compact face space b X is b X ¼ spanf b f 1 ; b f 2 ; . . . ; c f X g ð 6Þ with j b f i j ¼ b N ( N ¼ jf i j.</formula><p>If fT i g Ni¼1 are linear functionals, the transformation can be implemented by matrix analysis. Popular applications of this compact method include PCA <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref> and linear discriminant analysis (LDA) <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Geometric feature-based face models</head><p>Researchers have been studying feature-based face models for several decades <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. Geometric features can be derived from either the frontal view or the profile of a face. Common frontal view features include eyebrow shape, nose width, and length, and mouth position. Examples of profile face features include the notch between the brow and the nose, the tip of the nose, the notch between the nose and the upper lips, and parting of the lips and the tip of the chin <ref type="bibr" target="#b30">[31]</ref>. Mathematically, let ff i g N i¼1 be a set of N features that characterize a face f. These features can be particular points on the face, ratio of distances, angles, and other geometric measures. The face f can be modeled by an N-dimensional vector as follows:</p><formula xml:id="formula_6">f ¼ ½f 1 ; f 2 ; . . . ; f N T .<label>ð7Þ</label></formula><p>If the dimension N of the face f is very large, techniques of dimension reduction can be used to reduce the computational complexity and increase robustness. Geometric feature data can be collected using two approaches. They can be obtained using manual methods, where a reader marks the feature points on a face image. They can also be derived automatically using machine vision techniques. A useful technique for automatic feature extraction is based on horizontal and vertical integral projections. For example, Brunelli and Poggio proposed a geometric, feature-based matching algorithm <ref type="bibr" target="#b8">[9]</ref> to extract 35 geometric features on a face including the mouth, nose, eyebrows, and face outline. Goldstein et al. <ref type="bibr" target="#b5">[6]</ref> and Kaya et al. <ref type="bibr" target="#b6">[7]</ref> showed that machines can recognize human faces by using manually extracted features. For example, Craw et al. <ref type="bibr" target="#b9">[10]</ref> used 34 manually located landmarks to investigate different codings for automatic face recognition. In many biometrics applications, geometric features are also extracted manually. The goal is to guarantee precise identification of the features. For example, Ferrario et al. <ref type="bibr" target="#b31">[32]</ref> defined 22 face landmarks to study sexual dimorphism on human faces. A common approach to improving accuracy of the feature data is to have multiple readers identify the features independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Face recognition by landmarks</head><p>Good methods of face recognition are based on appropriate face models and use of effective measure to compute the similarity or distance between two faces. We propose an approach to model a face using a set of landmarks and two distance measures based on it. A second method to identify faces using proportions is described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Landmark-based face model</head><p>Landmarks are important geometric features on faces. They have been studied and applied extensively in biology, statistical shape analysis, and anthropology <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34]</ref>. Formally, ''A landmark is a point of correspondence on each object that matches between and within populations'' <ref type="bibr" target="#b16">[17]</ref>. Landmarks can be classified as anatomical, mathematical, or pseudo-landmarks. Anatomical landmarks are biologically meaningful points defined strictly by experts. Mathematical landmarks are defined according to some mathematical or geometric properties such as extreme points. Pseudo-landmarks are defined around the outline of an object or between two existing anatomical or mathematical landmarks. For face recognition, anatomical landmarks are the most important because of their biological meanings; mathematical landmarks are important for automated face recognition because they can be described by mathematical functions and located easily by machines.</p><p>Since the landmarks are mapped to two-dimensional face images, it is convenient to use a complex number notation for each landmark. Let fl i : l i 2 Cg N i¼1 be the set of N landmarks on a face image. The face f can then be modeled by a N-dimensional vector of landmarks (also called configuration) as follows:</p><formula xml:id="formula_7">f ¼ ½l 1 ; l 2 ; . . . ; l N T ;<label>ð8Þ</label></formula><p>where</p><formula xml:id="formula_8">l i ¼ x i þ ffiffiffiffiffiffi ffi À1 p y i<label>ð9Þ</label></formula><p>and (x i , y i ) are coordinates of the landmark l i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Procrustes distance</head><p>Well-defined landmarks can be employed for face recognition after the Euclidean transform (also called linear conformal transform). It maps a straight line to be straight, two parallel lines to be parallel, and a square to be square. Let (x 1i , y 1i ) and (x 2i , y 2i ) be the one-to-one corresponding landmarks of configurations f 1 = [l 11 , . . . , l 1N ] T and f 2 = [l 21 , . . . , l 2N ] T , respectively. The Euclidean transform is defined as</p><formula xml:id="formula_9">x 1i y 1i ! ¼ a b ! þ b cosðhÞ ÀsinðhÞ sinðhÞ cosðhÞ ! x 2i y 2i ! ;<label>ð10Þ</label></formula><p>where a and b represent translation, b is a positive real number corresponding to scaling (we assume that the scaling is isotropic), h 2 [0, 2p) corresponds to rotation. With complex numbers, Eq. ( <ref type="formula" target="#formula_9">10</ref>) can be rewritten as</p><formula xml:id="formula_10">l 1i ¼ ða þ ffiffiffiffiffiffi ffi À1 p bÞ þ be jh l 2i ;<label>ð11Þ</label></formula><formula xml:id="formula_11">where a þ ffiffiffiffiffiffi ffi À1 p</formula><p>b is the complex number to represent translation. The four parameter values can be determined exactly by two independent landmarks. However, if the number of landmarks (N) is more than two, bidimensional regression is required for solving optimal parameter values subject to an objective fitness criterion. Let P ¼ ff P i g P i¼1 and G ¼ ff G j g G j¼1 denote a probe face set and a gallery of known faces, respectively. We want to compute the degree of similarity between a face in the probe set, f P i ¼ ½l P i1 ; l P i2 ; . . . ; l P iN T 2 P, and a face in the face gallery,</p><formula xml:id="formula_12">f G j ¼ ½l G j1 ; l G j2 ; . . . ; l G jN T<label>2</label></formula><p>G, each of which is modeled by N landmarks. The Euclidean transform from f G j to f P i can be formulated as</p><formula xml:id="formula_13">f P i ¼ ða þ ffiffiffiffiffiffi ffi À1 p bÞ1 N þ be ih f G j þ ;<label>ð12Þ</label></formula><p>where 1 N is an N-dimensional vector of all ones and is the residual of regression <ref type="bibr" target="#b16">[17]</ref>. Without loss of generality,</p><formula xml:id="formula_14">f P i and f G j can be centralized such that a þ ffiffiffiffiffiffi ffi À1 p b ¼ 0.</formula><p>An objective fitness functional J with respect to the residual can then be defined as</p><formula xml:id="formula_15">Jðb; hÞ ¼ be ih f G j À f P i 2 2 ¼ ðbe ih f G j À f P i Þ H ðbe ih f G j À f P i Þ ¼ f P H i f P i þ b 2 f G H j f G j À f P H j f G j be ih À f G H j f P i be Àih ;<label>ð13Þ</label></formula><p>where i.i 2 is the l 2 norm and H denotes the Hermitian transpose. The Procrustes distances between the probe face f P i and gallery face f G j is defined as <ref type="bibr" target="#b34">[35]</ref> </p><formula xml:id="formula_16">d p ðf P i ; f G j Þ ¼ inf b;h Jðb; hÞ f P i À f P i 2 2 ¼ f P H i f P i À f P H i f G j f G H j f P i f G H j f G j , f P i À f P i 2 2 ;<label>ð14Þ</label></formula><p>where f P i ¼ 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N</head><p>P N d¼1 l P id is the geometric center of the N landmarks of the probe face f P i . The Procrustes distance d p can be used to measure the similarity between any two faces that have the same set of landmarks. The Procrustes distance is the smallest (d p = 0) when two faces can be superposed exactly with respect to predefined landmarks. The larger the value of Procrustes distance, the harder it is to transform the set of landmarks from one face to the other, and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Refined procrustes distance</head><p>The traditional Procrustes distance does not consider the correlation of landmarks. Motivated by the Mahalanobis distance, we propose a refined Procrustes distance. The basic idea is to remove the correlation of landmarks and build a unit sphere by the distribution for the Procrustes distance. This requires a set of faces for training. The basic steps include normalizing each face configuration into a pre-shaped space, performing a complex principal component analysis, and using a weighted similarity measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Pre-shaped space</head><p>The Procrustes distance is independent of the geometric constraints of translation, scaling, and rotation. Translation-independence can be achieved by moving the geometric center of configuration of a face to the origin of the coordinate system. In the matrix form,</p><formula xml:id="formula_17">f t ¼ ðI N À 1 N 1 T N N Þf;<label>ð15Þ</label></formula><p>where I N is a N • N identity matrix. Scaling is the process of normalizing the centerized configuration. Mathematically,</p><formula xml:id="formula_18">f s ¼ f t kf t k 2 .<label>ð16Þ</label></formula><p>A face that is free from the translation and scaling is called the pre-shape of the face <ref type="bibr" target="#b16">[17]</ref>. All faces in the gallery and probe sets are normalized into the pre-shaped space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Complex principal component analysis</head><p>PCA has been used in various scientific fields. Face recognition using eigenfaces has been studied in great detail and recognized as one of the major face recognition algorithms <ref type="bibr" target="#b1">[2]</ref>. Sirovich and Kirby <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> first introduced PCA for face recognition and defined the principal components as eigenfaces. Pentland and Turk <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> refined the method by adding preprocessing and expanding the database statistics <ref type="bibr" target="#b35">[36]</ref>. The essential idea behind PCA is to reduce dimensionality by removing the redundant and less significant components within a dataset governed by a large set of correlated variables <ref type="bibr" target="#b36">[37]</ref>. A minor concern of PCA is that we lose the direct geometric meanings associated with the landmarks.</p><p>PCA is applied typically to configurations that are modeled by vectors of real numbers. If a configuration (e.g., a digital image) is modeled by a matrix of real values, a popular method is to stack either rows or columns alphabetically into a vector of real members. For the landmark-based face model f with N landmarks, every landmark l i is two-dimensional. To use PCA in the real number domain, f can be stacked into a 2N-dimensional vector as</p><formula xml:id="formula_19">f ¼ ½x 1 ; y 1 ; x 2 ; y 2 ; . . . ; x N ; y N T .<label>ð17Þ</label></formula><p>This paper studies the PCA model in the complex number domain because complex PCA keeps the relationship of x and y coordinates. Mathematically, complex PCA is an extension of real number PCA and has the same formulation. Because pre-shaping remove translation of all configurations, the smallest eigenvalue is zero. Eigenvectors are composed of complex numbers and eigenvalues are nonnegative real numbers.</p><p>To compute the eigenvalues of the complex eigenvectors and to yield eigen-configurations, we first decorrelate the covariance matrix of landmarks of an ensemble of training face images. Mathematically, let G = [f 1 , f 2 , . . . , f G ] be an N • G matrix for a gallery G with G face images. Each face, f i (1 6 i 6 G), is modeled by an N-dimensional landmark vector, i.e., f i = [l i1 , l i2 , . . . , l iN ] T . The average face with landmarks is</p><formula xml:id="formula_20">f ¼ 1 G X G i¼1 f i .<label>ð18Þ</label></formula><p>The N • N covariance matrix C G of the gallery G can be constructed as</p><formula xml:id="formula_21">C G ¼ G 0 G 0T ;<label>ð19Þ</label></formula><formula xml:id="formula_22">where G 0 ¼ ½f 1 À f; f 2 À f; . . . ; f G À f.</formula><p>The eigen-configurations are defined as the eigenvectors of the covariance matrix C G . If N ) G, the covariance matrix C G is too large to be decomposed efficiently. A tractable method is to decompose the transpose C T G and then project the resulting eigenvectors into the space of G 0 <ref type="bibr" target="#b23">[24]</ref>. Let E be the b N Â N matrix which consists of the first b N most significant eigen-configurations with the largest eigenvalues. For any face f, its landmark-based representation can be rewritten as</p><formula xml:id="formula_23">f ¼ Eðf À fÞ ¼ ½ l1 ; l2 ; . . . ; lN T<label>ð20Þ</label></formula><p>with b N 6 N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Eigenvalue-weighted similarity measure</head><p>To incorporate statistical characteristics of a landmark distribution, we weight landmarks by their eigenvalues in the pre-shaped space. Let k 2 1 ; k 2 2 ; . . . ; k 2 values of C G for l1 ; l2 ; . . . ; lN , respectively. The eigenvalueweighted configuration is defined as</p><formula xml:id="formula_24">f ¼ l1 k 1 ; l2 k 2 ; . . . ; kN l N 2 4 3 5 T .<label>ð21Þ</label></formula><p>The refined Procrustes distance for two faces f P i and f G j is defined as</p><formula xml:id="formula_25">d ep ðf P i ; f G j Þ ¼ X N k¼1 lP ik k k À lG jk k k 2 .<label>ð22Þ</label></formula><p>4. Face recognition by ratios</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ratio-based face models</head><p>The ratios of geometric features are common in nature, the golden ratio ðU ¼ 1þ ffiffi 5 p 2 Þ being the most familiar. Many artists utilize the golden ratio to make their painting and sculpture more appealing. Scientists believe that some human faces are more attractive because their features are related by the golden ratio <ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref>. Geldart <ref type="bibr" target="#b40">[41]</ref> demonstrated that the perception of face beauty is not based entirely on cultural influences and that length of the internal features can cause different perceptions of beauty. The ratios of face features play a crucial role in the classification of faces <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>. In the past 20 years, researchers and practitioners in anthropology and aesthetic surgery have analyzed faces from a different perspective <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43]</ref>. They use a set of canonical points on a human face that are critical for face reconstruction. These points, and distances between them are used to represent a face. In fact, artists developed a set of neoclassical canons (ratios of distances) to represent faces as far back as the Renaissance period <ref type="bibr" target="#b41">[42]</ref>. All these observations motivate us to explore the role of ratios of the distances between face landmarks in face recognition.</p><p>In this paper, ratios are defined for the Euclidean distances among landmarks on human faces. They are free from translation, scaling, and 2D rotation of face images. Because ratios are determined by up to four landmarks, the total number of ratios of a face is very large, H (N 4 ), where N is the number of landmarks. An efficient way to utilize ratios while simultaneously reducing their number is to use normalized Euclidean distances. Instead of computing the ratios between all pairs of distances, we compute only the ratios with respect to one benchmark distance, e.g., the inter eye distance. These normalized distances are also independent of variations of the field of view. One must carefully chose the benchmark distance. It should not be too short in comparison to other absolute distances; otherwise, the normalized distances can be sensitive to noise if this short distance is used as the denominator. In addition, two landmarks that define the benchmark distance must be extracted reliably for a face.</p><p>The symmetry of the frontal view with respect to the vertical midline in faces can help further reduce the number of ratios <ref type="bibr" target="#b9">[10]</ref>. Many features on a face have symmetric counterparts on the left and right half of the face. A landmark on the axis of symmetry is symmetric to itself. Let (l 1 , l 2 ) be a pair of landmarks on the left half of the face and (l 3 , l 4 ) be their symmetric counterpart on the right half. The various symmetric Euclidean distances d (.,.) among l 1 , l 2 , l 3 , and l 4 are defined as</p><formula xml:id="formula_26">dðl 1 ; l 3 Þ ¼ kl 1 À l 3 k 2 ; dðl 2 ; l 4 Þ ¼ kl 2 À l 4 k 2 ; dðl 1 ; l 2 Þ ¼ dðl 3 ; l 4 Þ ¼ 1 2 ðdðl 1 ; l 2 Þ þ dðl 3 ; l 4 ÞÞ ¼ 1 2 ðkl 1 ; l 2 k 2 þ kl 3 ; l 4 k 2 Þ; dðl 2 ; l 3 Þ ¼ dðl 1 ; l 4 Þ ¼ 1 2 ðdðl 2 ; l 3 Þ þ dðl 1 ; l 4 ÞÞ ¼ 1 2 ðkl 1 ; l 3 k 2 þ kl 2 ; l 4 k 2 Þ. 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; :</formula><p>Let l a and l b be two landmarks whose Euclidean distance is defined as the benchmark distance. The normalized distances for a face are then defined as</p><formula xml:id="formula_27">rðl i ; l j Þ ¼ dðl i ; l j Þ dðl a ; l b Þ ; 8l i ; l j 2 fl 1 ; . . . ; l N g.<label>ð23Þ</label></formula><p>Let fr i : r i 2 Rg N i¼1 be a set of N ratios for a face f. Using this model, a face f can be represented by an N-dimensional vector of ratios as</p><formula xml:id="formula_28">f ¼ ½r 1 ; r 2 ; . . . ; r N T .<label>ð24Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Dimension reduction by PCA</head><p>Even with the reduction in the number of ratios, a face with N landmarks has H (N 2 ) normalized distances. To reduce the computational complexity and increase the robustness of face recognition, it is imperative that the ratio-based face model should have as few dimensions as possible. We use PCA to reduce the dimension of the ratio-based face model by converting a set of ratios to a set of eigenratios, which are essentially transform functionals as discussed in Section 2.2. The process of PCA for ratios is the same as the CPCA for the landmark-based face model in Section 3.3.2. For any face f, its ratio-based representation can be rewritten as</p><formula xml:id="formula_29">f ¼ ½r 1 ; r2 ; . . . ; rN T<label>ð25Þ</label></formula><p>with b N ( N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Similarity measures</head><p>To understand the use of ratios in face recognition, we explore several similarity measures. Two faces are similar if the distance between the two faces is small, and vice versa. We first investigate the two standard distance measures: l 1 norm and l 2 norm. We also explore the use of Mahalanobis distance, and motivated by it we define a new distance measure: eigenvalue-weighted cosine (EWC). The Mahalanobis distance and EWC distance measure similarity of whitened face configurations. (Whitening is a technique mostly used in signal detection. A whitened signal has identity covariance matrix <ref type="bibr" target="#b43">[44]</ref>.)</p><p>Again, let P ¼ ff P i g P i¼1 and G ¼ ff G j g G j¼1 denote a probe face set and a face gallery, respectively. For two faces f P i ¼ ½r i1 ; ri2 ; . . . ; ri N 2 P and f G j ¼ ½r j1 ; rj2 . . . ; r j N 2 G which are characterized by b N PCA-reduced ratios, the l 1 norm and l 2 norm are defined as</p><formula xml:id="formula_30">d l 1 ðf P i ; f G j Þ ¼ P N k¼1 jr P ik À rG jk j; d l 2 ðf P i ; f G j Þ ¼ P N k¼1 jr P ik À rG jk j 2 . 8 &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; :</formula><p>Although the two similarity measures defined above have been effectively used in many applications, they have some drawbacks. The most important of them in our context is that all dimensions are treated with equal weight, even though they have different amounts of statistical information. The Mahalanobis distance accounts for this variation in its definition</p><formula xml:id="formula_31">d md ðf P i ; f G j Þ ¼ X N k¼1 jr P ik À rG jk j k k ;<label>ð26Þ</label></formula><p>where k 2 k is the kth eigenvalue of the covariance matrix C G . The essential idea of the Mahalanobis distance is to equally treat all axes of a PCA-based measure space. Here, the Mahalanobis distance is the same as the l 1 norm if the covariance matrix C G is an identity matrix. In the literature of face recognition, the Mahalanobis distance has several variants <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b44">45]</ref>. Since our experimental results indicate that d l 1 consistently outperforms d l 2 , we define the Mahalanobis distance d md by weighting the l 1 norm d l 1 rather than the l 2 norm d l 2 .</p><p>Our initial results show that the standard cosine distance can also be a useful measure as its definition</p><formula xml:id="formula_32">d cos ðf P i ; f G j Þ ¼ X N k¼1 rP ik rG jk , ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi X N k¼1 rP ik À Á 2 X N k¼1 rG jk 2 v u u t .<label>ð27Þ</label></formula><p>However, it also suffers from the same drawbacks as the d l 1 and d l 2 distances. In the standard cosine distance d cos , each element of a vector has the same weight, which results in relative importance to the low frequency components. To overcome this problem, we define the EWC distance by normalizing the distance d cos with the eigenvalues of the covariance matrix C G as given below</p><formula xml:id="formula_33">d ewc ðf P i ; f G j Þ ¼ À X N k¼1 rP ik rG jk k 2 k , ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi X N k¼1 rP ik k k 2 X N k¼1 rG jk k k ! 2 v u u u t .<label>ð28Þ</label></formula><p>Mathematically, a similarity measure should be non-negative <ref type="bibr" target="#b45">[46]</ref>. The EWC distance d ewc , however, has a minus sign in order to uniformly use a lower value as an indicator of more similarity of two faces <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b44">45]</ref>. The range of d ewc is the interval [À1, 1] and d ewc = À1 indicates that two faces are identical. Experimental results show that the EWC distance is superior to the conventional cosine distance.</p><p>As described before, our initial goal is to examine how well geometric constraints can be used to reduce the search space, i.e., the set of possible faces that can be matches for a given unknown face. Given a probe face f P i 2 P, we match it with each face f G j 2 Gð1 6 j 6 GÞ using a designed distance, say d m 2 fd p ; d ep ; d l 1 ; d l 2 ; d cos ; d md ; d ewc g. The set of distances defines a total order. Let the face that corresponds to the test face (i.e., belongs to the same subject) in the gallery be f G e . We define the rank order of a match as</p><formula xml:id="formula_34">O d m ; f P i ; G À Á ¼ ff G j : d m f P i ; f G e À Á P d m f P i ; f G j g .<label>ð29Þ</label></formula><p>Informally, the rank is the number of faces in the gallery that have a smaller (higher similarity) or equal distance with respect to f G e . Ideally, we would like to have the highest ranked face ðO ¼ 1Þ in the gallery G be f G e . Since our goal is to reduce the search space, we would like to measure how often the actual match falls in the top K fraction, e.g., top 1%, top 10%, and so on. The relative rank K of the face normalizes its absolute rank with the number G of faces in the gallery to the range of ½ </p><formula xml:id="formula_35">If W m ð 1 G Þ ¼ 1<label>ð30Þ</label></formula><p>, then we have one-to-one face recognition, i.e., each face in the probe set P matches best with its corresponding face in the gallery G. For a given cumulative match score W m , a small relative rank K value means a better matching overall. Conversely, given a relative rank K, a higher cumulative match score W m also means a better overall matching. This clearly shows the tradeoff between accuracy of the recognition and reduction in the search space. If we want to have higher confidence that a face remains in the reduced search space, we have to increase the value of K and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental materials and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and landmarks</head><p>We use two datasets to evaluate the performance of our approach: the Purdue AR face image database <ref type="bibr" target="#b46">[47]</ref> and the FERET database <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b47">48]</ref>. The Purdue AR database contains 70 male and 56 female young adult faces. All images were taken in two sessions at 14 day intervals under strictly controlled conditions. We selected 40 frontal view and neutral expression images (20 distinct male subjects and 20 distinct female subjects). The dimension of all face images is uniform at 768 • 576 pixels. Fig. <ref type="figure" target="#fig_0">1</ref> shows several sample faces for our research. We used the Purdue AR dataset primarily to investigate if landmark coordinates collected manually were reliable.</p><p>The FERET database developed at the George Mason University is a well-known face image database used for uniform evaluation of face recognition algorithms <ref type="bibr" target="#b47">[48]</ref>. Face images in the FERET database were collected in 15 different sessions between 1993 and 1996. We used four well-defined frontal view subsets of this large database. The Fa subset is designed as a gallery G <ref type="bibr" target="#b2">[3]</ref>. We used the Fb, Dup I, and Dup II subsets as the three probe face sets, P's, for evaluating our approach. The Fb subset was used for analyzing the effects of facial expression. (The images of the same subject in Fa and Fb have slightly different facial expressions.) Both Dup I and Dup II are designed for analyzing the effects of face aging. Each has duplicate images of the same subject taken after a long interval. The Dup II subset consists of faces that are the hardest to recognize because they were taken after the longest time interval. Table <ref type="table" target="#tab_1">1</ref> summarizes the characteristics of the four face subsets.</p><p>Fig. <ref type="figure" target="#fig_1">2A</ref> and Table <ref type="table" target="#tab_2">2</ref> collectively describe the locations of 29 landmarks used in our research. These landmarks are a subset of features used for face reconstruction surgery <ref type="bibr" target="#b33">[34]</ref>. We used only the ''soft'' landmarks, i.e., the landmarks that are located on actual skin surfaces. In contrast, ''bony'' landmarks are situated on the surface of the underlying bone and are harder to decode from face images. We developed a Java-based interface to extract the landmarks manually. A method similar to <ref type="bibr" target="#b31">[32]</ref> was used to assess the reproducibility of locations of the landmarks. Fig. <ref type="figure" target="#fig_1">2B</ref> plots the distributions of the 29 biologically meaningful landmarks. This figure is composed of 994 face images in the gallery Fb. Each cross indicates the mean location of one landmark in the pre-shaped space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Reliability of the landmarks</head><p>Before conducting any analysis, it is important to evaluate the reliability of the manual feature extraction method. The two fundamental questions are:</p><p>1. Do the coordinates of the landmarks vary significantly if different persons (readers) derive them? 2. Do the coordinates of the landmarks vary significantly if the same person derives them multiple times (repetitions)?</p><p>To answer these questions we used a statistical approach and posed the following null hypothesis: H 0 : readers or repetitions have no significant impact on landmark coordinates.</p><p>We test the null hypothesis H 0 by using analysis of variance (ANOVA). To fit our statistical environment, we further categorize factors which might impact our collection of landmark coordinates into either (a) random factors whose levels are chosen only randomly from a population or (b) fixed factors whose levels are selected to observe differences between these particular levels <ref type="bibr">[49]</ref>. For example, we can sample both genders, but only a very small subset of all human faces. Therefore, gender is a fixed factor, but face is a random factor in our model. The mixed ANOVA procedure which accounts for both fixed and random effects is applicable here. It is reasonable to assume that all extracted coordinates are measured independently and errors and residuals satisfy normal distribution. Then, the mixed ANOVA can be modeled by a general linear mixed model (GLMM) as  </p><formula xml:id="formula_36">y ¼ Xb þ Zc þ ;<label>ð31Þ</label></formula><p>where the parameters are described as follows:</p><p>1. y: coordinates (including horizontal and vertical) of the landmarks 2. b: unknown parameters quantifying fixed effects 3. X: known design matrix for fixed effects 4. c: unknown parameters quantifying random effects 5. Z: known design matrix for random effects 6. : residual error accounting for statistical noise and assumed to be multivariate normally distributed.</p><p>We used the MIXED procedure of SAS, a widely used statistical software, for our analysis of variance <ref type="bibr">[49]</ref>. Possible factors that characterize the coordinates of landmarks include the landmarks themselves, the reader, the repetition, the face, and the gender. The gender was included because we wished to study the difference between male and female faces. The gender and landmarks are fixed effects and the others are random effects. Using the SAS notation, the classification of effects is Fixed effects ¼ GENDER; LANDMARK; SEX Ã LANDMARK f g Random effects ¼ READER; REPEATðREADERÞ; FACEðGENDERÞ f ; LANDMARK Ã FACEðGENDERÞg where * denotes factor interaction and (.) denotes factor nesting. For example, ÔGENDER * LANDMARKÕ is the effect that describes the interaction between the landmarks and the gender. Factor nesting is similar to factor interaction for hypothesis testing. For example, ÔFACE(GEN-  The Purdue AR dataset was used to test the null hypothesis H 0 . In our experiment, four human subjects were randomly invited as readers to collect the landmarks on all face images. They repeated landmark collection of the same face images after an interval of at least one week. Table <ref type="table" target="#tab_3">3</ref> lists the estimates of variance components and p values. The horizontal (AXIS-X) and vertical (AXIS-Y) coordinates were analyzed separately. The MIXED procedure of SAS with the restricted maximum likelihood (REML) was used for the estimation and test <ref type="bibr">[49]</ref>. The major variances of landmark coordinates (AXIS-X and AXIS-Y) were determined by face images themselves, i.e., the ÔFACE(GENDER)Õ effect. Table <ref type="table" target="#tab_3">3</ref> clearly shows that the variances introduced by a reader ÔREADERÕ and repetitions by a reader ÔREPEAT(READER)Õ are sufficiently small compared with other random effects and even the residual. The p value is the probability of obtaining the variance estimate shown in Table <ref type="table" target="#tab_3">3</ref> (or larger) if the true variance is zero. Thus, a high p value indicates that the variability for that effect is likely to be negligible, and vice versa. Therefore, the null hypotheses H 0 cannot be rejected and the landmark coordinates collected by readers can be used with confidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Standard FERET test performance</head><p>To compare direct face recognition performance, we first present the standard evaluation results of the FERET test which includes PCA and LDA appearance-based methods <ref type="bibr" target="#b2">[3]</ref>. Although there is improvement since the FERET test, the results are still good comparison references because the FERET test also used Fa as the gallery and Fb, Dup I, and Dup II as the probe sets. Fig. <ref type="figure" target="#fig_2">3</ref> illustrates the performance of partially automatic algorithms for Fb, Dup I, and Dup II. The results were edited based on <ref type="bibr" target="#b2">[3]</ref>. Here, only the top 50 ranks (ranging from 1 to 994) are presented. Figs. <ref type="figure" target="#fig_2">3A</ref> and<ref type="figure">B</ref> graph the average identification performance and the upper bound identification performance, respectively. Clearly, the algorithm performance varies for different probe sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Landmarks for face recognition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1.">Procrustes distance</head><p>Fig. <ref type="figure" target="#fig_3">4A</ref> plots the cumulative match scores W d p ðKÞ's for face recognition of the FERET datasets using the 29 landmarks and the Procrustes distance. The probe set Fb has the highest cumulative match scores overall. As noted in <ref type="bibr" target="#b2">[3]</ref>, Dup II is the hardest probe set and has the lowest score as expected. The experimental results show that variations of facial expression have less impact on the performance than variations due to aging, if the landmark-based face model and the Procrustes distance are used together. When a relative rank of K ¼ 0:01 (top 1%) is used, the cumulative match scores W dp ð0:01Þ for Fb, Dup I, and Dup II are 0.594, 0.542, and 0.496, respectively. As the relative rank K increases to 0.1 (top 10%), W dp ð0:1Þ for Fb, Dup I, and The horizontal (AXIS-X) and vertical (AXIS-Y) coordinates of the 29 landmarks are used for the experiment. Dup II increase to about 0.823, 0.784, and 0.728, respectively. Fig. <ref type="figure" target="#fig_3">4C</ref> plots the cumulative match scores for direct face identification. Here, only the top 50 ranks (ranging from 1 to 994) are presented like the FERET test. When the absolute rank is 1, which is one-to-one mapping, the cumulative match scores for Fb, Dup I, and Dup II are 0.374, 0.340, and 0.285, respectively. As the rank increases to 10, the cumulative match scores for Fb, Dup I, and Dup II are 0.594, 0.542, and 0.496, respectively. Even though Fb has low recognition performance, Dup I and Dup II reach the average identification performance of partially automatic algorithms (from Fig. <ref type="figure" target="#fig_2">3A</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2.">Refined procrustes distance</head><p>To study the performance of the refined Procrustes distance, we normalize all landmarks in Fa, Fb, Dup I, and Dup II into the pre-shaped space. The gallery Fa is used as the training set for computing covariance matrix. Using CPCA, we select the 23 most significant eigen-configurations for the refined Procrustes distance measure. Fig. <ref type="figure">6A</ref> illustrates all 28 eigenvalues. (The 29th eigenvalue is 0 in the pre-shaped space.) Fig. <ref type="figure" target="#fig_3">4B</ref> plots the cumulative match scores W dep ðKÞ's for face recognition of the FERET datasets using the refined Procrustes distance. All probe sets Fb and Dup I have almost the same cumulative match scores. This experimental result indicates that the refined Procrustes distance can account for variations of facial expression and aging, which are regarded as two challenging problems by many stateof-the-art face recognition algorithms. When a relative rank of K ¼ 0:01 (top 1%) is used, the cumulative match scores W dep ð0:01Þ for Fb, Dup I, and Dup II are 0.794, 0.780, and 0.772, respectively. As the relative rank K increases to 0.1 (top 10%), W d ep ð0:1Þ for Fb, Dup I, and Dup II increase to about 0.928, 0.933, and 0.925, respectively. These results indicate that landmark-based face model combined with the refined Procrustes distance can efficiently reduce face search space for other face recognition algorithms.</p><p>Fig. <ref type="figure" target="#fig_3">4D</ref> plots the cumulative match scores for direct face identification. Again, only the top 50 ranks (ranging from 1 to 994) are examined. When the absolute rank is 1, which is one-to-one mapping, the cumulative match scores for Fb, Dup I, and Dup II are 0.602, 0.605, and 0.592, respectively. As the rank increases to 10, the cumulative match scores for Fb, Dup I, and Dup II are 0.794, 0.780, and 0.772, respectively. The performance for Fb still is expected to be lower than those holistic-related face recognition algorithms (e.g., PCA and LDA). However, the refined landmark-based method for Dup I and Dup II performs very well. The performances are over even the upper bound identification performance of partially automatic algorithms (from Fig. <ref type="figure" target="#fig_2">3B</ref>). These results indicate that the landmark-based face model combined with the refined Procrustes distance is also a good method for direct face recognition for frontal view faces that are complicated by either expression or aging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3.">Sensitivity and specificity</head><p>To evaluate the sensitivity and specificity of the results, Fig. <ref type="figure" target="#fig_4">5A</ref> illustrates the distributions of the match and mismatch of the Procrustes distances for the Fb dataset. (The probe sets Dup I and Dup II have similar results.) We can see that the overlapping region of the match and mismatch distributions is relatively small with an area of 0.402. (Note that the area of each curve is 1.0.) Similarly, Fig. <ref type="figure" target="#fig_4">5B</ref> illustrates the distributions of the match and mismatch of the refined Procrustes distances for the Fb dataset. The overlapping region is even smaller, with an area of 0.270. Thus, it justifies the advantage of the refined Procrustes distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Normalized Euclidean distances for face recognition</head><p>Using the 29 landmarks, we derive 406 Euclidean distances (between every pair of landmarks) and using the symmetry from Eq. ( <ref type="formula" target="#formula_27">23</ref>), we reduce them to 219 symmetric distances. To remove the effect of scaling, we normalize the distances with respect to the distance between the lowest points of the two ears: landmark l 17 (SBA, L) and the landmark l 21 (SBA, R). The motivation for this choice of the benchmark distance is that both landmarks can be located easily in most applications and their Euclidean distance is closely related to a primary dimension of a face, i.e., the face width. We define the symmetric normalized Euclidean distances as   as a function of the relative rank K. The four similarity measures are: fd l 1 ; d l 2 ; d ewc ; d md g. Fig. <ref type="figure">7</ref> shows that the EWC distance d ewc and the Mahalanobis distance d md are in general superior to the popular d l 1 and d l 2 ; d l 2 clearly is the worst performer. This is consistent with results reported in literature <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b49">51]</ref>. The explanation for this behavior is that d l 2 and d l 1 , to some extent, give equal weights to both high and low frequency eigenratios, thereby distorting the importance of the more important features. The EWC distance d ewc and the Mahalanobis distance d md , however, weight the eigenratios with their eigenvalues. For a relative rank K ¼ 0:1 (top 10%), the cumulative match scores for both distances W md (0.1) and W ewc (0.2) are around 0.95 for all probe sets. This shows that the ratio-based face model combined with either the Mahalanobis distance or the EWC distance can provide good face matches and effectively reduce face image search space. Table <ref type="table" target="#tab_5">4</ref> lists W (0.01) (top 1%) values for the three probe sets. We can see that the EWC distance is at par with, and in some case superior to, the Mahalanobis distance, but worse than the refined Procrustes distance.</p><formula xml:id="formula_37">r k ðl i ; l j Þ ¼ dðl i ; l j Þ</formula><p>To further understand the ratio-based models, we use the two best similarity measures (d md and d ewc ) to analyze the effects of expression and aging. Fig. <ref type="figure" target="#fig_6">8</ref> plots the cumulative match scores for the three probe sets Fb, Dup I, and Dup II. For both the Mahalanobis distance and the EWC distance d ewc , three cumulative match score are all like the refined Procrustes distance. These results show that a ratio-based face model combined with appropriate similarity measures can also account for variations in face expression and aging, but is slightly worse than the refined Procrustes distance.    <ref type="figure" target="#fig_6">8C</ref> and<ref type="figure">D</ref>), the performances are the same as the upper bound identification performance of partially automatic algorithms (from Fig. <ref type="figure" target="#fig_2">3B</ref>). Table <ref type="table" target="#tab_6">5</ref> lists the cumulative match score for one-to-one face identification. It is slightly worse than the refined Procrustes distance. However, the ratio-based face model combined with either the Mahalanobis distance or the EWC distance is still a good method for direct face recognition for frontal view faces that are complicated by either expression or aging.</p><p>To determine the sensitivity and specificity of our approach, we use the Fb dataset again. (The probe sets Dup I and Dup II have similar results.) Fig. <ref type="figure" target="#fig_7">9A</ref> illustrates the distributions of the match and mismatch Mahalanobis distances. We can see that the overlapping region of the match and mismatch distributions is relatively small with an area of 0.270. Fig. <ref type="figure" target="#fig_7">9B</ref> illustrates the distributions of the match and mismatch of the EWC distances. Again, the overlapping region of the two distributions is small with an area of 0.203. Note that the area of each curve is 1.0. These results show that the ratio-based face model combined with either the Mahalanobis distance or the EWC distance can be used reliably in face recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Face recognition has been an active research field for over a decade. Of all the problems in face recognition, identification of a face has been studied the most due to its potential for commercial applications. As the size of datasets increases, scalability becomes an important factor. Our initial goal is to evaluate how landmarks and their geometry can be used to reduce face search spaces. Our study, however, also suggests that landmarks and their geometry can be used for direct face identification. This paper evaluates how biologically meaningful landmarks and their geometry extracted from face images can be used for face recognition. The traditional Procrustes distance is studied first. Using complex principal component analysis, we proposed the refined Procrustes distance as the similarity measure for the landmark-based face model. The refined Procrustes distance incorporates the statistical correlation of landmarks. Motivated by research results from art, anthropology, and aesthetic surgery in which ratios play a significant role in human face descriptions, we also investigated how well-normalized Euclidean distances (special ratios) can be exploited for face recognition. Exploiting symmetry and using principal component analysis, we reduced the number of ratios to 20. In this investigation, we analyzed the effectiveness of three well-known similarity measures including the typical l 1 norm, l 2 norm, and Mahalanobis distance. We also defined a new similarity measure, called the EWC distance.</p><p>The methods studied in this paper are labeled as partially automated. The landmarks are first identified by persons (called readers in this paper), using a procedure similar to the one described in <ref type="bibr" target="#b9">[10]</ref>. The rest of the process is fully automated, however. While we obtained the landmarks manually, we have shown using a mixed ANONA model that the landmark coordinates are minimally impacted by readers or repetitions.</p><p>Our investigation shows that a human face can be effectively modeled by well-defined biologically meaningful landmarks and their geometrical features. Even though the method exploits only the limited number of landmarks on face images, our experimental results indicate that the landmark-based face model combined with the refined Procrustes distance can reduce face search space for other face recognition approaches or perform face identification in stand-alone mode. For those face images complicated either by aging or expression, this method even outperforms the upper-bound identification performance of partially automatic algorithms reported in <ref type="bibr" target="#b2">[3]</ref>. Experimental results show that only a limited number (less than 20) of eigenratios play significant roles. About the similarity measures, experimental results show that the EWC distance outperforms the l 1 and l 2 norms and even the Mahalanobis distance. The results demonstrate that the ratio-based face model, along with either the Mahalanobis distance or the EWC distance can be effectively and efficiently used to reduce face image search space with satisfactory performance and to recognize human faces directly as well.</p><p>Our ongoing work is focused on developing robust approaches to automate feature extraction and combine landmark geometry and local feature appearance for better face recognition performance. Future work also could exploit all ratios determined by arbitrary four landmarks or triangular features determined by any three non-collinear landmarks. The number of complete ratios and angles, however, is huge; therefore sophisticated approaches are required for dimension reduction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example face images of the Purdue AR dataset.</figDesc><graphic coords="8,114.91,590.80,354.24,139.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Twenty-nine landmarks used for our investigation. (A) The locations of 29 landmarks. (B) The distribution of 994 face images of the gallery dataset Fa in the pre-shaped space.</figDesc><graphic coords="9,110.55,72.46,175.68,232.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Performance of partially automatic algorithms for various face datasets. The results were edited based on the standard FERET test [3]. (A) The average identification performance. (B) The upper bound identification performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Performance of face recognition by landmarks for various face datasets. (A and B) For evaluating W dp ðKÞ's and W dep ðKÞ's, respectively. (C and D) For evaluating the direct face identification by using the Procrustes distance and the refined Procrustes distance, respectively. Only top 50 ranks are graphed in (C) and (D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Evaluating the sensitivity and specificity of the results. (A) For the distributions of match and mismatch Procrustes distances. (B) For the distributions of match and mismatch refined Procrustes distances. The overlapped areas of (A) and (B) are 0.402 and 0.270, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Eigenvalues for PCA analysis. (A) Illustrates 28 eigenvalues of the covariance matrix of 29 pre-shaped landmarks. (B) Illustrates top 30 eigenvalues of the covariance matrix of the 218 symmetric normalized Euclidean distances. The gallery G ¼ Fa is used for training for both landmarks and ratios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Cumulative match scores for the three probe sets. The top row (A and B) are for the relative rank and the bottom row (C and D) are for the absolute rank (only from 1 to 50). Only the Mahanlanobis distance and the EWC distance are illustrated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Distributions of the match and mismatch similarity measures for Fb. (A) is for the Mahalanobis distance and (B) is for the EWC distance. The overlapping areas of (A) and (B) are 0.270 and 0.203, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Four FERET subsets for face recognition</figDesc><table><row><cell>Subset</cell><cell>Count</cell><cell>Annotation</cell></row><row><cell>Fa</cell><cell>994</cell><cell>Gallery for training</cell></row><row><cell>Fb</cell><cell>992</cell><cell>Analyze effect of expression</cell></row><row><cell>Dup I</cell><cell>726</cell><cell>Analyze effect of aging</cell></row><row><cell>Dup II</cell><cell>228</cell><cell>A harder subset of Dup I</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 The</head><label>2</label><figDesc></figDesc><table><row><cell>01</cell><cell>TR</cell><cell>The point on the hairline in the midline of the</cell></row><row><cell></cell><cell></cell><cell>forehand</cell></row><row><cell>02</cell><cell>SCI, L</cell><cell>Highest point on upper borderline in mid portion</cell></row><row><cell></cell><cell></cell><cell>of left eyebrow</cell></row><row><cell>03</cell><cell>G</cell><cell>Most prominent midline point between eyebrows</cell></row><row><cell>04</cell><cell>SCI, R</cell><cell>Highest point on upper borderline in mid portion</cell></row><row><cell></cell><cell></cell><cell>of right eyebrow</cell></row><row><cell>05</cell><cell>SA, L</cell><cell>Highest point on the free margin of left ear</cell></row><row><cell>06</cell><cell>OBI, L</cell><cell>Attachment of the left ear lobe to the cheek</cell></row><row><cell>07</cell><cell>OS, L</cell><cell>Highest point on lower border of left eyebrow</cell></row><row><cell>08</cell><cell>OS, R</cell><cell>Highest point on lower border of right eyebrow</cell></row><row><cell>09</cell><cell>OBI, R</cell><cell>Attachment of the right ear lobe to the cheek</cell></row><row><cell>10</cell><cell>SA, R</cell><cell>Highest point on the free margin of right ear</cell></row><row><cell>11</cell><cell>EX, R</cell><cell>Point at outer right side of the eye</cell></row><row><cell>12</cell><cell>EN, R</cell><cell>Point at inner right side of the eye</cell></row><row><cell>13</cell><cell>EN,L</cell><cell>Point at inner left side of the eye</cell></row><row><cell>14</cell><cell>EX, L</cell><cell>Point at outer left side of the eye</cell></row><row><cell>15</cell><cell>OR, L</cell><cell>Lowest point on lower margin left eye</cell></row><row><cell>16</cell><cell>OR, R</cell><cell>Lowest point on lower margin right eye</cell></row><row><cell>17</cell><cell>SBA, L</cell><cell>Lowest point of left ear</cell></row><row><cell>18</cell><cell>AL, L</cell><cell>Most lateral point on left side of nose</cell></row><row><cell>19</cell><cell>SN</cell><cell>Midpoint of nose</cell></row><row><cell>20</cell><cell>AL, R</cell><cell>Most lateral point on right side of nose</cell></row><row><cell>21</cell><cell>SBA, R</cell><cell>Lowest point of right ear</cell></row><row><cell>22</cell><cell>LS, L</cell><cell>Highest pint on left side of lip</cell></row><row><cell>23</cell><cell>LS, M</cell><cell>Midpoint on upper lip</cell></row><row><cell>24</cell><cell>LS, R</cell><cell>Highest point on right side of lip</cell></row><row><cell>25</cell><cell>CH, L</cell><cell>Left most point of closed lip</cell></row><row><cell>26</cell><cell>STO</cell><cell>Midpoint of closed lip</cell></row><row><cell>27</cell><cell>CH, R</cell><cell>Right most point of closed lip</cell></row><row><cell>28</cell><cell>SL</cell><cell>Point on lower border of lower lip or upper border</cell></row><row><cell></cell><cell></cell><cell>of chin</cell></row><row><cell>29</cell><cell>PG</cell><cell>Mid point of chin</cell></row></table><note><p>set of 29 landmarks used in the research S. No. Landmark Landmark description * FACE(GENDER).Õ</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Estimates of variance component and p values</figDesc><table><row><cell>Covariance parameter</cell><cell>Estimate</cell><cell></cell><cell>p value</cell><cell></cell></row><row><cell>(SAS notation)</cell><cell>AXIS-X</cell><cell>AXIS-Y</cell><cell>AXIS-X</cell><cell>AXIS-Y</cell></row><row><cell>READER</cell><cell>0.3365</cell><cell>0.6178</cell><cell>0.2669</cell><cell>0.2417</cell></row><row><cell>REPEAT(READER)</cell><cell>0.0536</cell><cell>0.0409</cell><cell>0.2579</cell><cell>0.3025</cell></row><row><cell>FACE(GENDER)</cell><cell>159.5344</cell><cell>257.9770</cell><cell>0.0001</cell><cell>0.0001</cell></row><row><cell>LANDMARK * FACE *</cell><cell>28.8980</cell><cell>41.7450</cell><cell>0.0001</cell><cell>0.0001</cell></row><row><cell>(GENDER)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Residual</cell><cell>15.5629</cell><cell>17.6563</cell><cell>0.0001</cell><cell>0.0001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>dðl 17 ; l 21 Þ ; 8l i ; l j 2 fl 1 ; l 2 ; . . . ; l 29 g and 1 6 k 6 218.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ð32Þ</cell></row><row><cell>150</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Probability</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Match Mismatch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>0</cell><cell>0.01</cell><cell>0.02</cell><cell>0.03</cell><cell>0.04</cell><cell>0.05</cell><cell>0.06</cell><cell>0.07</cell><cell>0.08</cell><cell>0.09</cell><cell>0.1</cell><cell>0</cell><cell>0</cell><cell>0.05</cell><cell>0.1</cell><cell>0.15</cell><cell>0.2</cell><cell>0.25</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Distance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 W</head><label>4</label><figDesc>(0.01) (top 1%) values for the three probe sets and the six similarity measures</figDesc><table><row><cell>Probe</cell><cell>d p</cell><cell></cell><cell>d ep</cell><cell></cell><cell>d l1</cell><cell></cell><cell>d l2</cell><cell></cell><cell>d ewc</cell><cell></cell><cell>d md</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fb</cell><cell>0.594</cell><cell></cell><cell>0.794</cell><cell></cell><cell>0.686</cell><cell></cell><cell>0.588</cell><cell></cell><cell cols="2">0.745</cell><cell cols="2">0.723</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dup I</cell><cell>0.542</cell><cell></cell><cell>0.780</cell><cell></cell><cell>0.720</cell><cell></cell><cell>0.613</cell><cell></cell><cell cols="2">0.756</cell><cell cols="2">0.742</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dup II</cell><cell>0.496</cell><cell></cell><cell>0.772</cell><cell></cell><cell>0.668</cell><cell></cell><cell>0.537</cell><cell></cell><cell cols="2">0.699</cell><cell cols="2">0.703</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Cumulative match score</cell><cell>0.65 0.7 0.75 0.8 0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fb DupI DupII</cell><cell></cell><cell></cell><cell></cell><cell>Cumulative match score</cell><cell>0.65 0.7 0.75 0.8 0.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.5</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell><cell>0.5</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Relative rank of the gallery set Fa</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Relative rank of the gallery set Fa</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>Rank 1 (one-to-one identification) values for the three probe sets and the six similarity measures Dup I and Dup II performs well. If the Mahalanobis distance or the EWC distance d ewc is used (from Figs.</figDesc><table><row><cell>Probe</cell><cell>d p</cell><cell>d ep</cell><cell>d l1</cell><cell>d l2</cell><cell>d ewc</cell><cell>d md</cell></row><row><cell>Fb</cell><cell>0.374</cell><cell>0.602</cell><cell>0.452</cell><cell>0.369</cell><cell>0.518</cell><cell>0.494</cell></row><row><cell>Dup I</cell><cell>0.340</cell><cell>0.605</cell><cell>0.475</cell><cell>0.381</cell><cell>0.531</cell><cell>0.516</cell></row><row><cell>Dup II</cell><cell>0.285</cell><cell>0.592</cell><cell>0.410</cell><cell>0.323</cell><cell>0.502</cell><cell>0.476</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>J. Shi et al. / Computer Vision and Image Understanding 102 (2006) 117-133</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>J. Shi et al. / Computer Vision and Image Understanding 102 (2006) 117-133</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human and machine recognition of faces: a survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sirohey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="705" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face recognition: a literature survey</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="399" to="458" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The FERET evaluation methodology for face-recognition algorithms</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1090" to="1104" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Face recognition vendor test</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Micheals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tabassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evaluation report</title>
		<imprint>
			<date type="published" when="2002">2002. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">XM2VTSDB: the extended M2VTS database, in: Audio-and Video-Based Biometric Person Authentication</title>
		<author>
			<persName><forename type="first">K</forename><surname>Messer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jonsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AVBPAÕ</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="72" to="77" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Identification of human faces</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Harmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Lesk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="748" to="760" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A Basis Study on Human Face Recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kobayashi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971</date>
			<publisher>Academic Press</publisher>
			<pubPlace>Orlando, FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Strategies of robust object recognition for automatic identification of human faces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bichsel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<pubPlace>Zurich</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Eidgenossischen, Technischen Hochschule</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Face recognition: features versus templates</title>
		<author>
			<persName><forename type="first">R</forename><surname>Brunelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1042" to="1052" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">How should we represent faces for automatic recognition</title>
		<author>
			<persName><forename type="first">I</forename><surname>Craw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Costen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akamatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="725" to="736" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A feature-based face recognition system</title>
		<author>
			<persName><forename type="first">P</forename><surname>Campadelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lanzarotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Savazzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Proc. Internat. Conf. on Image Analysis and Processing</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="68" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic face identification system using flexible appearance models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lanitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="393" to="401" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Local feature analysis: a general statistical theory for object representation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Penev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Atick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network: Comput. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="477" to="500" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recent advances in visual and infrared face recognition-a review</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Abidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="103" to="135" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Face recognition by elastic bunch graph matching</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wikott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Fellous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kru ¨ger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V D</forename><surname>Malsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="775" to="779" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<title level="m">Data Mining: Concepts and Techniques</title>
		<meeting><address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">L</forename><surname>Dreden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Mardia</surname></persName>
		</author>
		<title level="m">Statistical Shape Analysis</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Activity recognition using the dynamics of the configuration of interacting objects</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE Comput. Soc</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic gait recognition based on statistical shape analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1120" to="1131" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Duta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-P</forename><surname>Dubuisson-Jolly</surname></persName>
		</author>
		<title level="m">Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE Comput. Soc</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
	<note>Learning 2d shape models</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gabor-based kernel PCA with fractional power polynomial models for face recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="572" to="581" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Detecting faces in images: a survey</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="58" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic analysis of facial expressions: the state of the art</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J M</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1424" to="1445" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Face recognition: eigenface, elastic matching, and neural nets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lades</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="1423" to="1435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Low-dimensional procedure for the characterization of human face</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sirovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kirby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="519" to="524" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Application of the Karhunen-Loe `ve procedure for the characterization of human faces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sirovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="108" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Eigenfaces for recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cognit. Neurosci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="86" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Face recognition using eigenfaces</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Discriminant analysis of principal components for face recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Third Internat. Conf. on Automatic Face and Gesture Recognition</title>
		<meeting>Third Internat. Conf. on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="336" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discriminant analysis for recognition of human face images</title>
		<author>
			<persName><forename type="first">K</forename><surname>Etemad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am. A</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1724" to="1733" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic recognition and analysis of human faces and facial expressions: a survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Samal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Iyengar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="77" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sexual dimorphism in the human face assessed by Euclidean distance matrix analysis</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">F</forename><surname>Ferrario</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sforza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pizzini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Miani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Anat</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="page" from="593" to="600" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Bookstein</surname></persName>
		</author>
		<title level="m">Morphometric Tools for Landmark Data: Geometry and Biology</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Farkas</surname></persName>
		</author>
		<title level="m">Anthropometry of the Head and Face</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Raven Press</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A F</forename><surname>Seber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Multivariate</forename><surname>Observations</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wiley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A survey of face recognition algorithms and testing results</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Barrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal, Systems, and Computer Conference of IEEE</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="301" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<title level="m">Principal Component Analysis</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Ghyka</surname></persName>
		</author>
		<title level="m">The Geometry of Art and Life</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Dover</publisher>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">All that glitters: a review of psychological research on the aesthetics of the golden section</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="937" to="968" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust real-time face tracker for cluttered environments</title>
		<author>
			<persName><forename type="first">K</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="184" to="200" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Effects of the height of the internal features of faces on adultsÕ aesthetic ratings and 5-month-oldsÕ looking times</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geldart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="839" to="850" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Revision of neoclassical facial canons in young adult Afro-Americans</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Litsas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Aesthetic Plast. Surg</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Differences in horizontal, neoclassical facial canons in Chinese (Han) and North American caucasian populations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Farkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Aesthetic Plast. Surg</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="265" to="269" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stark</surname></persName>
		</author>
		<title level="m">Probability and Random Processes with Applications to Signal Processing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Analyzing PCA-based face recognition algorithms: eigenvector selection and distance measures</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Yambor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Evaluation Methods in Computer Vision</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Christensen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Phillips</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>World Scientific Press</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Royden</surname></persName>
		</author>
		<title level="m">Real Analysis</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The AR Face Database</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benavente</surname></persName>
		</author>
		<idno>#24</idno>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The FERET database and evaluation procedure for face-recognition algorithms</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="295" to="306" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Probabilistic visual learning for object representation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="696" to="710" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Example-based learning for view-based human face detection</title>
		<author>
			<persName><forename type="first">K.-K</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="51" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
