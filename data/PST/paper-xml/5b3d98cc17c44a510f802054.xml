<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Collaborative Learning for Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guocong</forename><surname>Song</surname></persName>
							<email>songgc@gmail.com</email>
							<affiliation key="aff0">
								<address>
									<postCode>94306</postCode>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Chai</surname></persName>
							<email>chaiwei@google.com</email>
							<affiliation key="aff1">
								<address>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Collaborative Learning for Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce collaborative learning in which multiple classifier heads of the same network are simultaneously trained on the same training data to improve generalization and robustness to label noise with no extra inference cost. It acquires the strengths from auxiliary training, multi-task learning and knowledge distillation. There are two important mechanisms involved in collaborative learning. First, the consensus of multiple views from different classifier heads on the same example provides supplementary information as well as regularization to each classifier, thereby improving generalization. Second, intermediate-level representation (ILR) sharing with backpropagation rescaling aggregates the gradient flows from all heads, which not only reduces training computational complexity, but also facilitates supervision to the shared layers. The empirical results on CIFAR and ImageNet datasets demonstrate that deep neural networks learned as a group in a collaborative way significantly reduce the generalization error and increase the robustness to label noise.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When training deep neural networks, we must confront the challenges of general nonconvex optimization problems. Local gradient descent methods that most deep learning systems rely on, such as variants of stochastic gradient descent (SGD), have no guarantee that the optimization algorithm will converge to a global minimum. It is well known that an ensemble of multiple instances of a target neural network trained with different random seeds generally yields better predictions than a single trained instance. However, an ensemble of models is too computationally expensive at inference time. To keep the exact same computational complexity for inference, several training techniques have been developed by adding additional networks in the training graph to boost accuracy without affecting the inference graph, including auxiliary training <ref type="bibr" target="#b18">[19]</ref>, multi-task learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>, and knowledge distillation <ref type="bibr" target="#b9">[10]</ref>. Auxiliary training is introduced to improve the convergence of deep networks by adding auxiliary classifiers connected to certain intermediate layers <ref type="bibr" target="#b18">[19]</ref>. However, auxiliary classifiers require specific new designs for their network structures in addition to the target network. Furthermore, it is found later <ref type="bibr" target="#b19">[20]</ref> that auxiliary classifiers do not result in obvious improved convergence or accuracy. Multi-task learning is an approach to learn multiple related tasks simultaneously so that knowledge obtained from each task can be reused by the others <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21]</ref>. However, it is not useful for a single task use case. Knowledge distillation is introduced to facilitate training a smaller network by transferring knowledge from another high-capacity model, so that the smaller one obtains better performance than that trained by using labels only <ref type="bibr" target="#b9">[10]</ref>. However, distillation is not an end-to-end solution due to having two separate training phases, which consume more training time.</p><p>In this paper, we propose a framework of collaborative learning that trains several classifier heads of the same network simultaneously on the same training data to cope with the above challenges. The method acquires the advantages from auxiliary training, multi-task learning, and knowledge distillation, such as, appending the exact same network as the target one in the training graph for a single task, sharing intermediate-level representation (ILR), learning from the outputs of other heads (peers) besides the ground-truth labels, and keeping the inference graph unchanged. Experiments have been performed with several popular deep neural networks on different datasets to benchmark performance, and their results demonstrate that collaborative learning provides significant accuracy improvement for image classification problems in a generic way. There are two major mechanisms collaborative learning benefits from: 1) The consensus of multiple views from different classifier heads on the same data provides supplementary information and regularization to each classifier. 2) Besides computational complexity reduction benefited from ILR sharing, backpropagation rescaling aggregates the gradient flows from all heads in a balanced way, which leads to additional performance enhancement. The per-layer network weight distribution shows that ILR sharing reduces the number of "dead" filter weights in the bottom layers due to the vanishing gradient issue, thereby enlarging the network capacity.</p><p>The major contributions are summarized as follows. 1) Collaborative learning provides a new training framework that for any given model architecture, we can use the proposed collaborative training method to potentially improve accuracy, with no extra inference cost, with no need to design another model architecture, with minimal hyperparameter re-tuning. 2) We introduce ILR sharing into codistillation that not only enhances training time/memory efficiency but also improves generalization error. 3) Backpropagation rescaling we propose to avoid gradient explosion when the number of heads is big is also proven able to improve accuracy when the number of heads is small. 4) Collaborative learning is demonstrated to be robust to label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>In addition to auxiliary training, multi-task learning, and distillation mentioned before, we list other related work as follows.</p><p>General label smoothing. Label smoothing replaces the hard values (1 or 0) in one-hot labels for a classifier with smoothed values, and is shown to reduce the vulnerability of noisy or incorrect labels in datasets <ref type="bibr" target="#b19">[20]</ref>. It regularizes the model and relaxes the confidence on the labels. Temporal ensembling forms a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs to improve the performance of semi-supervised learning <ref type="bibr" target="#b13">[14]</ref>. However, it is hard to scale for a large dataset since temporal ensembling requires to memorize the smoothed label of each data example. Two-way distillation. Co-distillation of two instances of the same neural network is studied in <ref type="bibr" target="#b1">[2]</ref> with a focus on training speed-up in a distributed learning environment. Two-way distillation between two networks, which can use the same architecture or different, is also studied in <ref type="bibr" target="#b22">[23]</ref>. Each of them alternatively optimizes its own network parameters. However, the developed algorithms are far from optimized. First, when different classifiers have different architectures, each of them should have a different weight associated with its loss function to balance injected backpropagation error flows. Second, multiple copies of the target network increase proportionally the memory consumption in graphics processing unit (GPU) and the training time.</p><p>Self-distillation/born-again neural networks. Self-distillation is a kind of distillation when the student network is identical to the teacher in terms of the network graph. Furthermore, the distillation process can be performed consecutively several times. At each consecutive step, a new identical model is initialized from a different random seed and trained from the supervision of the earlier generation. At the end of the procedure, additional gains can be achieved with an ensemble of multiple students generations <ref type="bibr" target="#b6">[7]</ref>. However, multiple self-distillation processes multiply the total training time proportionally; an ensemble of multiple student generations increases the inference time accordingly as well.</p><p>In comparison, the major goal of this paper is to improve the accuracy of a target network without changing its inference graph and emphasize both the accuracy and the training efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Collaborative learning</head><p>The framework of collaborative learning consists of three major parts: the generation of a population of classifier heads in the training graph, the formulation of the learning objective, and optimization  for learning a group of classifiers collaboratively. We will describe the details of each of them in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generation of training graph</head><p>Similar to auxiliary training <ref type="bibr" target="#b18">[19]</ref>, we add several new classifier heads into the original network graph during training time. At inference time, only the original network is kept and all added parts are discarded. Unlike auxiliary training, each classifier head here has an identical network to the original one in terms of graph structure. This approach leads to advantages over auxiliary training in terms of engineering effort minimization. First, it does not require to design additional networks for the auxiliary classifiers. Second, the structure symmetry for all heads does not require additional different weights associated with loss functions to well balance injected backpropagation error flows, because an equal weight for each head's objective is optimal for training.</p><p>Figure <ref type="figure" target="#fig_1">1</ref> illustrates several patterns to create a group of classifiers in the training graph. Figure <ref type="figure" target="#fig_1">1 (a</ref>) is a target network to train. The network can be expressed as z = g(x; θ), where g is determined by the graph architecture, and θ represents the network parameters. To better explain the following patterns, we assume the network g can be represented as a cascade of three functions or subnets,</p><formula xml:id="formula_0">g(x; θ) = g 3 (g 2 (g 1 (x; θ 1 ); θ 2 ); θ 3 )<label>(1)</label></formula><p>where θ = [θ 1 , θ 2 , θ 3 ] and θ i includes all parameters of subnet g i accordingly. In Figure <ref type="figure" target="#fig_1">1</ref> (b), each head is just a new instance of the original network. The output of head h is z (h) = g(x; θ (h) ), where θ (h) is an instance of network parameters for head h. Another pattern allows all heads to share ILRs in the same low layers, which is shown in Figure <ref type="figure" target="#fig_1">1 (c</ref>). This structure is very similar to multi-task learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>, in which different supervised tasks share the same input, as well as some ILR. However, collaborative learning has the same supervised tasks for all heads. It can be expressed as follows</p><formula xml:id="formula_1">z (h) = g 3 (g 2 (g 1 (x; θ 1 ); θ (h) 2 ); θ (h)</formula><p>3 ), where there is only one instance of θ 1 shared by all heads. Furthermore, multi-heads can take advantage of multiple hierarchical ILRs, as shown in Figure <ref type="figure" target="#fig_1">1 (d)</ref>. The hierarchy is similar to a binary tree in which the branches at the same levels are copies of each other. For inference, we just need to keep one head with its dependent nodes and discard the rest. Therefore, the inference graph is identical to the original graph g.</p><p>It is shown in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b4">5]</ref> that the training memory size is roughly proportional to the number of layers/operations. With the multi-instance pattern, the number of parameters in the whole training graph is proportional to the number of heads. Obviously, ILR sharing can proportionally reduce the memory consumption and speed up training, compared to multiple instances without sharing. It is more interesting that the empirical results and analysis in Section 4 will demonstrate that ILR sharing is able to boost the classification accuracy as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning objectives</head><p>The main idea of collaborative learning is that each head learns from ground-truth labels but also from the whole population through the training process. We focus on multi-class classification problems in this paper. For head h, the classifier's logit vector is represented as z = [z 1 , z 2 , . . . , z m ] tr for m classes. The associated softmax with temperature T is defined as follows,</p><formula xml:id="formula_2">σ i (z (h) ; T ) = exp z (h) i /T m j=1 exp z (h) j /T (2)</formula><p>When T = 1, (2) is just a normal softmax function. Using a higher value for T produces a softer probability distribution over classes. The loss function for head h is proposed as</p><formula xml:id="formula_3">L (h) = βJ hard (y, z (h) ) + (1 − β)J sof t (q (h) , z (h) )<label>(3)</label></formula><p>where β ∈ (0, 1]. The objective function with regard to a ground-truth label J hard is just the classification loss -cross entropy between a one-hot encoding of the label y and the softmax output with temperature of 1:</p><formula xml:id="formula_4">J hard (y, z (h) ) = − m i=1 y i log(σ i (z (h) ; 1)).</formula><p>The soft label of head h is proposed to be a consensus of all other heads' predictions as follows:</p><formula xml:id="formula_5">q (h) = σ   1 H − 1 j =h z (j) ; T  </formula><p>which combines the multiple views on the same data and contains additional information rather than the ground-truth label. The objective function with regard to the soft label is the cross entropy between the soft label and the softmax output with a certain temperature, i.e.</p><formula xml:id="formula_6">J sof t (q (h) , z (h) ) = − m i=1 q (h) i log(σ i (z (h) ; T ))</formula><p>which can be regarded as a distance measure between an average prediction from population and the prediction of each head <ref type="bibr" target="#b9">[10]</ref>. Minimizing this objective aims at transferring the information from the soft label to the logits and regularizing the training network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization for a group of classifier heads</head><p>In addition to performance optimization, another design criterion for collaborative learning is to keep the hyperparameters in training algorithms, e.g. the type of SGD, regularization, and learning rate schedule, the same as those used in individual learning. Thus, collaborative learning can be simply put on top of individual learning. The optimization here is mainly designed to take new concepts involved in collaborative learning into account, including a group of classifiers, and ILR sharing.</p><p>Simultaneous SGD. Since multiple heads are involved in optimization, it seems straightforward to alternatively update the parameters associated with each head one-by-one. This algorithm is used in both <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b1">2]</ref>. In fact, alternative optimization is popular in generative adversarial networks <ref type="bibr" target="#b7">[8]</ref>, in which a generator and discriminator get alternatively updated. However, alternative optimization has the following shortcomings. In terms of speed, it is slow because one head needs to recalculate a new prediction after updating its parameters. In terms of convergence, recent work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> reveals that simultaneous SGD has faster convergence and achieves better performance than the alternative one. Therefore, we propose to apply SGD and update all parameters simultaneously in the training graph according to the total loss, which is the sum of each head's loss as well as regularization Ω(θ).</p><formula xml:id="formula_7">L = H h=1 L (h) + λΩ(θ)<label>(4)</label></formula><p>We suggest keeping the same regularization and its hyperparameters as individual training when applying collaborative learning. It is important to avoid unnecessary hyperparameter search in practice when introducing a new training approach. The effectiveness of simultaneous SGD will be validated in Section 4.1.</p><p>Backpropagation rescaling. First, we describe an important stability issue with ILR sharing. Assume that there are H heads sharing subnet g 1 (•; θ 1 ) as shown in Figure <ref type="figure" target="#fig_3">2</ref> (a), in which θ 1 and θ</p><formula xml:id="formula_8">(h) 2</formula><p>represent the parameters of g 1 and those of g 2 associated with head h, respectively. The output of  the shared layers, x 1 , is fed to all corresponding heads. However, the backward graph becomes a many-to-one connection. According to (4), the backpropagation input for the shared layers is</p><formula xml:id="formula_9">∇ x1 L = H h=1 ∇ x1 L (h)</formula><p>. It is not hard to discover an issue that the variance of ∇ x1 L grows as the number of heads grows. Assume that the gradient of each head's loss has a limited variance, i.e., Var((∇ x1 L (h) ) i ) &lt; ∞, where i represents each element in a vector. We should make the system stable, i.e., Var((∇ x1 L) i ) &lt; ∞, even when H − → ∞. Unfortunately, the backpropagation flow of Figure <ref type="figure" target="#fig_3">2</ref> (a) is unstable in the asymptotic sense due to the sum of all gradient flows.</p><p>Note that simple loss scaling, i.e., L = 1 H h L (h) , bring another problem: resulting in very slow learning w.r.t</p><formula xml:id="formula_10">θ (h) 2 . The SGD update is θ (h) 2 ← θ (h) 2 − η 1 H ∇ θ (h) 2 L (h)</formula><p>. For a fixed learning rate η,</p><formula xml:id="formula_11">η 1 H ∇ θ (h) 2 L (h) → 0 when H → ∞.</formula><p>Therefore, backpropagation rescaling is proposed to achieve two goals at the same time -to normalize the backpropagation flow in subnet g 1 and keep that in subnet g 2 the same as the single classifier case.</p><p>The solution to add a new operation I(•) between g 1 and g 2 , shown in Figure <ref type="figure" target="#fig_3">2</ref> (b), which is</p><formula xml:id="formula_12">I(x) = x, ∇ x I = 1 H<label>(5)</label></formula><p>And then the backpropagation input for the shared layers becomes</p><formula xml:id="formula_13">∇ x1 L = 1 H H h=1 ∇ x1 L (h)<label>(6)</label></formula><p>The variance of ( <ref type="formula" target="#formula_13">6</ref>) is then always limited, which is proven in Session 1 of Supplementary material. Backpropagation rescaling is essential for ILR sharing to have better performance by just reusing a training configuration well tuned in individual learning. Its effectiveness on classification accuracy will be validated in Section 4.1.</p><p>Balance between hard and soft loss objectives. We follow the suggestion in <ref type="bibr" target="#b9">[10]</ref> that the backpropagation flow from each soft objective should be multiplied by T 2 since the magnitudes of the gradients produced by the soft targets scale as 1/T 2 . This ensures that the relative contributions of the hard and soft targets remain roughly unchanged when tuning T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Robustness to label noise</head><p>In supervised learning, it is hard to completely avoid confusion during network training either due to incorrect labels or data augmentation. For example, random cropping is a very important data augmentation technique when training an image classifier. However, the entire labeled objects or large portion of them occasionally get cut off, which really challenges the classifier. Since multiple views on the same example have diversity of predictions, collaborative learning is by nature more robust to label noise than individual learning, which will be validated in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We will evaluate the performance of collaborative learning on various network architectures for several datasets, with analysis of important and interesting observations. We use T = 2 and β = 0.5 for all experiments. In addition, the performance of any model trained with collaborative learning is evaluated using the first classifier head without head selection. All experiments are conducted with Tensorflow <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CIFAR Datasets</head><p>The two CIFAR datasets, CIFAR-10 and CIFAR-100, consist of colored natural images with 32x32 pixels <ref type="bibr" target="#b12">[13]</ref> and have 10 and 100 classes, respectively. We conduct empirical studies on the CIFAR-10 dataset with ResNet-32, ResNet-110 <ref type="bibr" target="#b8">[9]</ref>, and DenseNet-40-12 <ref type="bibr" target="#b10">[11]</ref>. ResNets and DenseNets for CIFAR are all designed to have three building blocks, residual or dense blocks. For the simple ILR sharing, the split point is just after the first block. For the hierarchical sharing, the two split points are located after the first and second blocks, respectively. Refer to Section 2 in Supplementary material for the detailed training setup. Simultaneous vs alternative optimization. We repeat an experiment that was performed in <ref type="bibr" target="#b22">[23]</ref>.</p><p>It is just a special case of collaborative learning in which we train two instances of ResNet-32 on CIFAR-100 with T = 1, β = 0.5. The only difference is that we replace the alternative optimization <ref type="bibr" target="#b22">[23]</ref> with the simultaneous one. It is shown in Table <ref type="table" target="#tab_1">2</ref> that based on the corresponding baseline, simultaneous optimization provides additional 1%+ accuracy gain compared to alternative one. With T = 2, simultaneous one has another 1% boost. Thus, simultaneous optimization substantially outperforms alternative one in terms of accuracy and speed. Backpropagation rescaling. Backpropagation rescaling is proposed to be necessary for ILR sharing theoretically in Section 3.3. We intend to confirm it by experiments on the CIFAR-10 dataset. To train a ResNet-32, we use a simple ILR sharing topology with four heads, and the split point located after the first residual block. The results in Table <ref type="table">3</ref> provide evidence that backpropagation rescaling clearly outperforms others -no scaling and loss scaling. While no scaling suffers from too large gradients in the shared layers, loss scaling results in a too small factor for updating the parameters of independent layers. We suggest backpropagation rescaling for all multi-head learning problems beyond collaborative learning.</p><p>Table <ref type="table">3</ref>: Impact of backprop rescaling. Four heads based on ResNet-32 share the low layers up to the first residual block. With no scaling, the factor for each head's loss is one. With loss scaling, the factor for each head's loss is 1/4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No scaling Loss scaling Backprop rescaling</head><p>Error (%) of ResNet-32 6.04 ± 0.17 6.09 ± 0.24 5.82 ± 0.08</p><p>Noisy label robustness. In this experiment, we aim at validating the noisy label resistance of collaborative learning on the CIFAR-10 dataset with ResNet-32. Assume that a portion of labels, whose percentage is called noise level, are corrupted with a uniform distribution over the label set.</p><p>The partition for images with corruption or not is fixed for all runs; their noisy labels are randomly generated every epoch. The results in Figure <ref type="figure" target="#fig_4">3</ref> validate that the test error rates of all collaborative learning setups are substantially lower than the baseline, and the accuracy gain becomes larger at a considerately larger noise level. It is well expected since the consensus formed from a group is able to mitigate the effect of noisy labels without knowledge of noise distribution. Another observation is that 4 heads with hierarchical ILR sharing, which constantly provides the lowest error rate at a relatively low noise level, seems worse at a high noise level. We conjecture that the diversity of predictions is more important than better ILR sharing in this scenario. Collaborative learning provides flexibility to trade off the diversity of predictions from the group with additional supervision and regularization for the common layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ImageNet Dataset</head><p>The ILSVRC 2012 classification dataset consists of 1.2 million for training, and 50,000 for validation <ref type="bibr" target="#b5">[6]</ref>. We evaluate how collaborative learning helps improve the performance of ResNet-50 network.</p><p>As following the notations in <ref type="bibr" target="#b8">[9]</ref>, we consider two heads sharing ILRs up to "conv3_x" block for simple ILR sharing. For the hierarchical sharing with four heads, two split points are located after "conv3_x" and "conv4_x" blocks, respectively. Refer to Section 3 in Supplementary material for the detailed training setup.</p><p>Classification error vs training computing resources (GPU memory consumption as well as training time). Classification error on Imagenet is particularly important because many state-of-theart computer vision problems derive image features or architectures from ImageNet classification models. For instance, a more accurate classifier typically leads to a better object detection model based on the classifier <ref type="bibr" target="#b11">[12]</ref>.  Model weight distribution and mechanisms of ILR sharing. We have plotted the statistical distribution of each layer's weights of trained ResNet-50 in Figure <ref type="figure" target="#fig_5">4</ref>, including the baseline, distilled and trained versions with hierarchical ILR sharing. Refer to Section 5 in Supplementary material for more results with other training configurations. The first finding is that the weight distribution of the baseline has a very large spike at near zero in the bottom layers. We conjecture that the gradients to many weights may be vanished so small that the weight decay part takes the major impact, which causes near-zero "dead" values eventually<ref type="foot" target="#foot_2">2</ref> . Compared to distillation, ILR sharing more effectively helps reduce the number of "dead" weights, thereby improve the accuracy. The second finding is that collaborative learning makes the weight distribution be more centralized to zero overall. Note that we also calculate per-layer model weight standard deviation values in Table <ref type="table" target="#tab_0">1</ref> in Supplementary material to additionally support this claim. The results indicate that the consensus of multiple views on the same data provides additional regularization.</p><p>ILR sharing is somewhat related to the concept of hint training <ref type="bibr" target="#b17">[18]</ref>, in which a teacher transfers its knowledge to a student network by using not only the teacher's predictions but also an ILR. In collaborative learning, ILR sharing can be regarded as an extreme case in which the ILRs of two separated classifier heads converge to the exact same one by forcing them to match. It is reported in <ref type="bibr" target="#b17">[18]</ref> that using hints can outperform distillation. To a certain extent, this provides an indirect evidence for the possibility of accuracy improvement from ILR sharing.</p><p>Again, two hyperparameters β and T are fixed in all of our experiments. It is possible that more extensive hyper-parameter searches may further improve the performance on specific datasets. We evaluate the impact of hyperparameters, β, T , and split point locations for ResNet-32 on CIFAR-10 in Section 6 in Supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed a framework of collaborative learning to train a deep neural network in a group of generated classifiers based on the target network. The consensus of multiple views from different classifier heads on the same example provides supplementary information as well as regularization to each classifier, thereby improving the generalization. By well aggregating the gradient flows from all heads, ILR sharing with backpropagation rescaling not only lowers training computational cost, but also facilitates supervision to the shared layers. Empirical results have also validated the advantages of simultaneous optimization and backpropagation rescaling in group learning. Overall, collaborative learning provides a flexible and powerful end-to-end training approach for deep neural networks to achieve better performance. Collaborative learning also opens up several possibilities for future work. The mechanism of group collaboration and noisy label resistance imply that it may potentially be beneficial to semi-supervised learning. Furthermore, other machine learning tasks, such as regression, may take advantage of collaborative learning as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Target network (b) Multiple instances (c) Simple ILR sharing (d) Hierarchical ILR sharing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Multiple head patterns for training. Three colors represent subnets g 1 , g 2 , and g 3 in (1).</figDesc><graphic url="image-1.png" coords="3,121.80,72.00,122.40,122.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Backprop rescaling. Operation I is described in<ref type="bibr" target="#b4">(5)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: No rescaling vs backpropagation rescaling</figDesc><graphic url="image-5.png" coords="5,145.11,72.00,108.00,108.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Test error on CIFAR-10 with label noise. Noise level is the percentage of corrupted labels over the all training set. The noisy labels are randomly generated every epoch.</figDesc><graphic url="image-7.png" coords="7,181.26,72.00,249.48,166.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Per-layer weight distribution in trained ResNet-50.As following the notations in<ref type="bibr" target="#b8">[9]</ref>, the two split points in the hierarchical sharing with four heads are located after "conv3_x" and "conv4_x" blocks, respectively.</figDesc><graphic url="image-8.png" coords="8,108.00,215.38,422.41,230.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test errors (%) on CIFAR-10. All experiments are performed 5 runs except for those of DenseNet-40-12 are done for 3 runs.</figDesc><table><row><cell>ResNet-32</cell><cell>ResNet-110 DenseNet-40-12</cell></row></table><note>Classification results. All results are summarized in Table1. It can be concluded from Table1that with a given training graph pattern, the more classifier heads, the lower generalization error. More important, ILR sharing reduces not only GPU memory consumption and training time but also the generalization error considerately.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Alternative optimization<ref type="bibr" target="#b22">[23]</ref> vs simultaneous optimization (ours) in terms of test errors of ResNet-32 on CIFAR-100.</figDesc><table><row><cell></cell><cell cols="3">Single instance (baseline) Head 1 in two instances Head 2 in two instances</cell></row><row><cell>[23]</cell><cell>31.01</cell><cell>28.81</cell><cell>29.25</cell></row><row><cell>Collaborative learning</cell><cell>T=1 30.52 ± 0.35 T=2</cell><cell>27.48 ± 0.37 26.36 ± 0.27</cell><cell>27.64 ± 0.36 26.32 ± 0.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Table 4 summarizes the performance of various training graph patterns Validation errors of ResNet-50 on ImageNet. Label smoothing, distillation and collaborative learning all do not affect inference's memory size and running time.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Top-1 error Top-5 error Training time Memory</cell></row><row><cell>Individual</cell><cell>Baseline</cell><cell>23.47</cell><cell>6.83</cell><cell>1x</cell><cell>1x</cell></row><row><cell>learning</cell><cell>Label smoothing (0.1)</cell><cell>23.34</cell><cell>6.80</cell><cell>1x</cell><cell>1x</cell></row><row><cell cols="3">Distillation From ensemble of two ResNet-50s 22.65</cell><cell>6.34</cell><cell>3.42x</cell><cell>1.05x</cell></row><row><cell>Collabor-</cell><cell>2 instances</cell><cell>22.81</cell><cell>6.45</cell><cell>2x</cell><cell>2x</cell></row><row><cell>ative</cell><cell>2 heads w/ simple ILR sharing</cell><cell>22.70</cell><cell>6.37</cell><cell>1.4x</cell><cell>1.32x</cell></row><row><cell>learning</cell><cell cols="2">4 heads w/ hierarchical ILR sharing 22.29</cell><cell>6.21</cell><cell>1.75x</cell><cell>1.5x</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="32" xml:id="foot_0">32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">Training time of distillation is analyzed in Section 4 in Supplementary material.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">We ran another experiment in which the weight decay was reduced by half in first three layers to verify our hypothesis. Refer to Section 5 in Supplementary material for more details.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank Qiqi Yan for many helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large scale distributed neural network training through online distillation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ormandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning internal representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baxter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Annual Conference on Computational Learning Theory, COLT &apos;95</title>
				<meeting>the Eighth Annual Conference on Computational Learning Theory, COLT &apos;95</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="311" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multitask learning: A knowledge-based source of inductive bias</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Machine Learning (ICML)</title>
				<meeting>the Tenth International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno>arXiv, abs/1604.06174</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Born again neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2018-07">July 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speed/accuracy trade-offs for modern convolutional object detectors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The numerics of gans</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gradient descent GAN optimization is locally stable</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">vDNN: Virtualized deep neural networks for scalable, memory-efficient neural network design</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clemons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zulfiqar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">49th Annual IEEE/ACM International Symposium on Microarchitecture</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep multi-task representation learning: A tensor factorisation approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno>arXiv, abs/1706.00384</idno>
		<title level="m">Deep mutual learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
