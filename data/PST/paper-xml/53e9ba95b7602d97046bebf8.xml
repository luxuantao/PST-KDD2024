<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Community-Based Bayesian Aggregation Models for Crowdsourcing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Matteo</forename><surname>Venanzi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Guiver</surname></persName>
							<email>joguiver@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Gabriella</forename><forename type="middle">Kazai</forename><surname>Microsoft</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
							<email>pkohli@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Shokouhi</forename><surname>Milad</surname></persName>
							<email>milads@microsoft.com</email>
						</author>
						<author>
							<persName><surname>Microsoft</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Southampton</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Community-Based Bayesian Aggregation Models for Crowdsourcing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">04F0A26F4C867288DF7DA49681ACE16B</idno>
					<idno type="DOI">10.1145/2566486.2567989</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.2.11 [Computing Methodologies]: Artificial Intelligence-Distributed Artificial Intelligence Models</term>
					<term>Machine learning Crowdsourcing</term>
					<term>Community detection</term>
					<term>Bayesian inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the problem of extracting accurate labels from crowdsourced datasets, a key challenge in crowdsourcing. Prior work has focused on modeling the reliability of individual workers, for instance, by way of confusion matrices, and using these latent traits to estimate the true labels more accurately. However, this strategy becomes ineffective when there are too few labels per worker to reliably estimate their quality. To mitigate this issue, we propose a novel community-based Bayesian label aggregation model, CommunityBCC, which assumes that crowd workers conform to a few different types, where each type represents a group of workers with similar confusion matrices. We assume that each worker belongs to a certain community, where the worker's confusion matrix is similar to (a perturbation of) the community's confusion matrix. Our model can then learn a set of key latent features: (i) the confusion matrix of each community, (ii) the community membership of each user, and (iii) the aggregated label of each item. We compare the performance of our model against established aggregation methods on a number of large-scale, real-world crowdsourcing datasets. Our experimental results show that our CommunityBCC model consistently outperforms stateof-the-art label aggregation methods, gaining, on average, 8% more accuracy with the same amount of labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Crowdsourcing has become one of the cornerstones of research in the development of human computation based intelligent systems. There are now a number of popular crowdsourcing platforms, like Amazon Mechanical Turk 1 , Crowd-Flower 2 or Clickworker 3 , that are increasingly used for collecting labeled training data for image or document classification and ranking <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b19">19]</ref>. However, there are a number of challenges associated with gathering labeled data from the crowd. These challenges are related to the uncertainty in the trustworthiness of individual workers and the quality of the crowdsourced labels overall. Specifically, workers might be unreliable and may provide incorrect labels depending on their skills, expertise and motivations. In addition, they may be unintentionally biased towards particular labels. For instance, in tasks where a rating is required, certain workers may be overly conservative and always give medium scores, while others may be overly opinionated and always give extreme scores. These problems make the task of aggregating labels and obtaining a consistent answer challenging, particularly when the dataset includes only a few labels per worker.</p><p>Simple solutions to the aggregation of crowd labels typically use heuristic methods such as majority voting (MV) <ref type="bibr" target="#b16">[16]</ref>. However, these approaches fail to take into account the reliabilities of different workers. To overcome this problem, probabilistic models have been proposed that do take worker reliability into account when aggregating crowd labels <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b18">18]</ref>. While these methods can model the accuracy of the workers, they still fail to consider other potential biases, e.g., the tendency for a worker to consistently over or underrate items. Such biased labels can be captured in the form of a worker's confusion matrix. That is a matrix that for a given worker, expresses the probability of each possible labeling outcome for each possible true label of the item. Indeed, a number of models have been proposed in the literature to incorporate confusion matrices into the label aggregation process and can thus represent such labeling biases <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b23">22]</ref>. These models have been shown to produce more accurate aggregations compared to heuristic alternatives <ref type="bibr" target="#b5">[5]</ref>. A common feature of these models is the simultaneous estimation of the latent confusion matrices associated with the workers, thus providing the task requester with the individual worker's reliability profile. However, these models typically need to observe a large number of labels per worker to reliably estimate their profiles (as encoded by their confusion matrices). Such a requirement can be particularly difficult to meet in a crowdsourcing setup, where often the distribution of workers to tasks follows a powerlaw curve due to most workers only contributing very few labels, while a few workers contributing a lot of labels <ref type="bibr" target="#b4">[4]</ref>.</p><p>In this paper, we propose a probabilistic model, which is able to merge sparse crowdsourced datasets more efficiently by modeling the correlations between the behaviors of different workers. The key feature that we introduce in our model is the concept of latent worker communities, which allows us to capture characteristic classes of worker behaviors. This is motivated by crowdsourcing studies, where empirical analysis of worker quality has shown that workers tend to exhibit similar behavior traits in the way they perform tasks. In particular, groups of workers can share similar confusion matrices, i.e., with similar reliabilities and biases <ref type="bibr" target="#b15">[15]</ref>. We refer to such groups of workers as communities. For example, one community may be the class of reliable workers, while other communities can represent spammers, random voters, or, more finely, groups of overly opinionated workers, who tend to give very high or very low scores, versus more conservative workers who have a tendency to pick the middle ratings. Our model encodes this information by modelling worker communities in the generative precess of inferring the crowd labels. Each community is associated with a confusion matrix, which represents the average confusion matrix of its members. Each worker belongs to a certain community where the worker 's confusion matrix is similar to (a perturbation of) the community's confusion matrix. Thus, by embedding the community model inside the generative process, our model can learn a set of key latent features: (i) the community membership of each user, (ii) the confusion matrix of each community, and (iii) the true label of each item. Furthermore, since the number of communities for a specific pool of workers is unknown a priori, our model also learns this information through marginal likelihood based model selection.</p><p>The advantage of applying our community based aggregation approach to crowdsourced data is twofold: First, the model can achieve faster convergence to accurate label aggregations with less input data by exploiting the detected worker community structures. Second, the model can accurately learn a worker's confusion matrix even when faced with sparse data, i.e., when only few labels per worker are available, by transferring learning of community profiles to the individual workers. Specifically, our model can be regarded as an extension of the Bayesian classifier combination (BCC) model <ref type="bibr" target="#b8">[8]</ref>, introduced by Kim and Ghahramani for merging categorical labels, where we incorporate latent workers community structures.</p><p>We apply our model to a number of large scale crowdsourced classification datasets which, all together, include 1,367,889 labels collected for 194,052 human intelligence tasks (HITs) of different types, such as adult query suggestion identification, document classification and relevance judgments for search results. In total, 3,948 workers contributed labels to these datasets. Experimental results demonstrate that our model produces more accurate aggregation results compared to commonly used baselines, such as MV and state-of-the-art probabilistic models, e.g., <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b8">8]</ref>. We also show that our model correctly learns the community structures and the community assignment of each worker, which we verify by comparing the model's output to the results of kmeans clustering over the workers Š profiles. Finally, we show that our method is able to provide accurate aggregated labels with less training data. In summary, our main contributions are as follows:</p><p>• We define a probabilistic Bayesian model that jointly learns latent community profiles of crowd workers, together with the individual workers' and communities' reliability profiles and the items' true labels.</p><p>• We provide a scalable implementation of our model using model decomposition techniques which allows us to process hundreds of thousands of crowd labels.</p><p>• We empirically show that our model outperforms existing non-community based aggregation methods with an exhaustive comparison against three benchmark methods on four real-world datasets for different crowdsourcing tasks.</p><p>The paper is structured as follows. Section 2 reviews related work. Section 3 introduces the preliminary notation and the BCC model, building the core of our framework. Section 4 details our community model and its probabilistic inference. Section 5 details our experimental setting and Section 6 presents the empirical evaluation of our method on real-world datasets. Section 7 concludes the paper and presents directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>In the context of compiling patient records, errors in measuring and interpreting different symptoms are not uncommon. In 1979, Dawid and Skene <ref type="bibr" target="#b2">[2]</ref> proposed an Expectation-Maximisation (EM) algorithm for modelling these error rates, even when the true values for patient responds were not available. This is analogues to crowdsourcing scenarios in which the true label of an object or task (symptom/facet) may be subject to noise and error, as shown in <ref type="bibr" target="#b5">[5]</ref>. Inspired by this classic statistical model, Kim and Ghahramani <ref type="bibr" target="#b8">[8]</ref> introduced a general framework for Bayesian Classifier Combination (BCC) in which the relationship between the -unobserved -true label and the model's output are modeled. We provide more details about BCC in the following section as it is highly related to our proposed model, and it is also one our experimental baselines.</p><p>In CrowdSynth <ref type="bibr" target="#b6">[6]</ref>, a set of Bayesian models are trained for (a) predicting the correct labels and (b) modeling the workers and predicting their votes. These models allow the system to maintain a cost-accuracy trade-off under budget constraints by deciding whether to hire a new worker or not. Similarly, Welinder et al. <ref type="bibr" target="#b19">[19]</ref> proposed an online model for reducing the cost of crowdsourcing by modeling label uncertainty and worker ability. Their method actively selects the next item for rating depending on label uncertainty and the desired level of confidence. Furthermore, the posterior distribution over the quality of workers are constantly updated as new labels are collected.</p><p>In the difficulty-ability-response estimation model (DARE) <ref type="bibr" target="#b1">[1]</ref>, the correct answers (to IQ questions), the task difficulty, and the ability of workers were jointly modeled using a probabilistic graphical model. The authors presented an active learning testing scheme for selecting the next question to be answered based on previous responses and showed that their model achieves the same level of accuracy with less judgments. Raykar et al. <ref type="bibr" target="#b14">[14]</ref> take a related approach where inference is iterative. In each iteration, a new golden set of true labels is established based on high-confidence labels, and the worker quality estimations are adjusted accordingly. In a similar fashion, the GLAD model <ref type="bibr" target="#b20">[20]</ref> also simultaneously updates its beliefs about the true label, worker quality and task difficulty in a probabilistic framework.</p><p>Zhou et al. <ref type="bibr" target="#b23">[22]</ref> considered a separate probabilistic distribution for each worker-item pair and proposed a minimax entropy principle to jointly infer the worker quality and the true labels. They argued that labels are generated by a probability distribution over workers and by maximising the entropy of this distribution the workers' quality can be naturally inferred. They compared the performance of their method against majority voting and the method proposed by Dawid &amp; Skene <ref type="bibr" target="#b2">[2]</ref> and showed that the labels inferred by their minmax entropy model are closer to the ground-truth.</p><p>Our work builds upon these empirical findings to model and simultaneously estimate worker qualities and the items' true labels, while also modelling workers' quality correlations within a probabilistic inference framework.</p><p>In the field of community detection, there are a number of graph based approaches that deal with crowdsourcing data categorizations <ref type="bibr" target="#b3">[3]</ref> or with the identification of (potentially overlapping) community structures in worker networks <ref type="bibr" target="#b13">[13]</ref>. These networks represent community members as nodes and encode distances between them by weighted (or unweighted) links. In this vein, Simpson et al. <ref type="bibr" target="#b15">[15]</ref> applied a community detection model <ref type="bibr" target="#b13">[13]</ref>, which groups workers based on the Hellinger distance between their confusion matrices. The authors illustrate that several natural clusters are formed in which workers with similar behavior are grouped together. We take this line of work to the next level by applying community detection in the generative model as opposed to previous work that applies it after inference. We show that this joint inference of community models leads to more efficient and effective label aggregations on sparse datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PRELIMINARIES</head><p>Notation. Let us start by introducing our notation. Consider that there are K workers classifying N objects into C possible classes. Let ti be the latent true class 4 of object i. Let π (k) c,j , with c, j ∈ C, be the probability that worker k will classify an object of class c as j. Finally, c (k) i is the label submitted by worker k for the object i and C be the set of all the observed labels. For simplicity of notation, in what follows we will assume a dense set of labels in which all the workers label all the objects. However, our model implementation allows us to also support sparsity in the label set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian Classifier Combination (BCC) Model. Our ap-</head><p>proach is an extension of the BCC model <ref type="bibr" target="#b8">[8]</ref> that has been successfully applied to crowdsourcing problems in previous work <ref type="bibr" target="#b15">[15]</ref>. The factor graph for the BCC model is shown in 4 In what follows, we will use the terms 'label' and 'class' of objects interchangeably.  i , submitted by worker k for object i is generated from a categorical distribution with parameters π</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N objects</head><formula xml:id="formula_0">t i Categorical p c i (k) π c (k) Dirichlet c i (k) k K workers π c (k) k C classes Categorical Dirichlet α β c (k)</formula><formula xml:id="formula_1">(k) c : c (k) i |π (k) , ti ∼ Cat ( c (k) i |π (k) t i )<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">π (k) = {π (k) 1 , . . . π (k)</formula><p>C } is the set of row probability vectors representing the confusion matrix of worker k. In other words, ti is a random variable that acts as selector of the categorical distribution, the rows of π (k) , from which the label by worker k is generated. Notice that this probabilistic relationship is visually described in the factor graph using the gate notation (dashed box) -introduced in [11] -using ti as gating variable. Further, if we assume that the labels are independent and identically distributed, then the likelihood for a given set of labels C factorises over the data points and can be expressed as follows,</p><formula xml:id="formula_3">p(C|π, t, p) = N ∏ i=1 Cat(ti|p) K ∏ k=1 Cat ( c (k) i |π (k) t i )</formula><p>Consequently, the posterior distribution over model parameters, given the data, can be written as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>p(π, t, p|C) ∝ p(c|π, t, p)p(t|p)p(p)p(π)</head><p>(2) </p><formula xml:id="formula_4">N objects t i Categorical p c i (k) π c (k) Softmax Dirichlet Gaussian s c (k) i (k) k K workers c (k) k</formula><formula xml:id="formula_5">π (k) c ∼ Dir(π (k) c |β (k) c</formula><p>). This simplifies Equation 2 to:</p><formula xml:id="formula_6">p(π, t, p|C) ∝ Dir(p|α) N ∏ i=1 { Cat(ti|p) K ∏ k=1 Cat ( c (k) i |π (k) t i )Dir(π (k) c |β) }</formula><p>The marginal posterior distribution of individual parameters can be obtained by integrating out all the remaining joint parameters. Since this integration is intractable analytically, it needs to be computed using numerical methods. In particular, our implementation of BCC uses the Expectation-Propagation (EP) message passing algorithm <ref type="bibr" target="#b10">[10]</ref> provided by the Infer.NET probabilistic programming framework <ref type="bibr" target="#b12">[12]</ref>.</p><p>A key feature of BCC is the assumption that workers are independent. On the one hand, this assumption reduces the complexity of the model and simplifies the inference. On the other hand, the model is inefficient as the relationships between the confusion matrices of workers are not modelled. Encoding these relationships, as we will show in Section 5, is key to achieving better aggregation performance with sparse sets of labels. In the next section, we describe our community-based BCC model, which encodes the feature that workers conform to a few different types, where each type represents a group of workers with similar confusion matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">COMMUNITY BCC MODEL</head><p>Our proposed Community BCC model -referred to hereafter as CommunityBCC -is an extension of the BCC model, enhanced by introducing the concept of worker communities. The factor graph of CommunityBCC can be seen in Figure <ref type="figure" target="#fig_2">2</ref>. Specifically, we assume that there are M communities of workers within the crowd. Each community m is associated with a confusion matrix π (m) . Let m (k) be the community membership of worker k. Then, we assume that m (k) is generated from a categorical distribution with parameters h:</p><formula xml:id="formula_7">m (k) |h ∼ Cat(m (k) |h)</formula><p>where h represents the proportions of community memberships across all the workers. Further, each community has a probability score s of the community m (k) (i.e., the worker k's community score), through a multivariate Gaussian draw:</p><formula xml:id="formula_8">s (k) c |s (m (k) ) c ∼ N (s (k) c |s (m (k) ) c , ν -1 I) (<label>3</label></formula><formula xml:id="formula_9">)</formula><p>where ν is the hyperparameter representing the isotropic inverse variance of the community confusion matrices. From this, π </p><formula xml:id="formula_10">p(π (k) c |s (k) c ) = δ(π (k) c -softmax(s (k) c ))</formula><p>Finally, we follow the same assumption as BCC that c k i is generated by a categorical distribution conditioned on π k and ti (see Equation <ref type="formula" target="#formula_1">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Probabilistic Inference</head><p>We now describe how inference is performed in the Com-munityBCC model. Again, for simplicity of notation this description assumes a dense set of labels. Specifically, we assume that we have observed the class assignments of all objects from all workers (denoted by C). Given C, we wish to infer the posterior distributions of the set of the parameters: Θ = { s (m) , π (m) , s (k) , π (k) , t, p }. To do so, we apply Bayes' theorem to compute the joint posterior distribution of Θ conditioned on C:</p><formula xml:id="formula_11">p(Θ|C) ∝p(C|π (k) , t)p(t|p)p(p)p(π (k) |s (k) ) p(s (k) |s (m (k) ) )p(m (k) |h)p(h)p(s (m (k) ) )<label>(4)</label></formula><p>The priors for the parameters h and sm are specified by the following conjugate distributions:</p><formula xml:id="formula_12">h|α ∼ Dir(h|α) s (k) c |µ, θ ∼ N (s (k) c |µ, θ -1 I) N t i</formula><p>Cat. That is, the prior for h is a Dirichlet distribution with parameters α, while the prior for s (k) c is a multivariate Gaussian distribution with mean µ and isotropic precision θ. If we assume that labels are generated independently, then factoring in Equation 5 in the model's distributions, we obtain the following expression for the joint posterior distribution:</p><formula xml:id="formula_13">p(Θ|C) ∝Dir(p|α) N ∏ i=1 { Cat(ti|p) K ∏ k=1 { Cat ( c (k) i |π (k) t i ) δ(π (m (k) ) t i -softmax(s (m (k) ) t i</formula><p>))Dir(h|α)</p><formula xml:id="formula_14">N (s (k) t i |s (k) t i , ν -1 )N (s (m (k) ) t i |µ, θ -1 )Cat(m (k) |h) } }</formula><p>To compute the approximate marginal distribution of each parameter, we use variational message passing algorithm <ref type="bibr" target="#b22">[21]</ref>, which is also provided by the Infer.NET inference engine <ref type="foot" target="#foot_0">5</ref> . Finally, to find the optimal number of communities M * , we use the maximum marginal likelihood model selection criterion that is computed through marginalising out all the parameters in Θ. Formally:</p><formula xml:id="formula_15">M * = arg max M ∫ Θ p(C|Θ, M )p(Θ)dΘ<label>(5)</label></formula><p>This marginal likelihood optimisation can be performed through a simple line search on the discrete set of the M values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scalable Implementation</head><p>To train our model on large-scale datasets (i.e., in the order of hundreds of thousands of labels), we provide a scalable implementation of CommunityBCC using a model decomposition implementation technique. The main idea is to split the model into three sub-models: object model, community model and a set of worker models. Then, we can run (cheaper) local inferences in each sub-model, and merge the post-inference results using standard message passing calculations. To define the user models, we split the set of workers K into a complete and disjoint partition: K = {K1, . . . , K l }. Each sub-set Ki is associated with a single worker model instance which infers the variables π (k) and s (k) for the workers in Ki. The object model infers the variables ti and p of all the objects. Similarly, the community model infers the variables h, s (k) and π (k) of all the workers. The three models are connected together through the probabilistic relationships between ti and π (k) and between s m and s k , as described in Equations 1 and 3, respectively. To run inference in this model architecture, we iteratively run the inference message passing algorithm in each of the three sub-models in a scheduled order and repeat this process until all the merged posterior distributions reach convergence.</p><p>In particular, we use the EP algorithm <ref type="bibr" target="#b10">[10]</ref> to run inference in the object model and the VMP algorithm <ref type="bibr" target="#b22">[21]</ref> to run inference in the community and the worker models.</p><p>The factor graph and the inference schedule of the scalable CommunityBCC implementation is depicted in Figure <ref type="figure" target="#fig_5">3</ref>. This implementation of CommunityBCC has several advantages in terms of scalability. First, the space complexity is reduced by running local inference in simpler sub-model and merging the results using only a few messages exchanged between the connecting factors. Second, we can parallelise the computation of each sub-model with an evident saving of time complexity. Using just the first of these techniques, we were able to train our CommunityBCC model on our largest dataset of 722,970 data points in approximately 20 minutes on a standard machine with 3.60 GHz CPU and 16GB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EMPIRICAL EVALUATION</head><p>In order to evaluate the efficacy of our CommunityBCC model, we make use of four large-scale, real-world datasets and compare performance against three state-of-the-art baselines for crowdsourcing scenarios. We measure performance based on agreement with gold standard data. Each of these aspects are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>Our four crowdsourcing datasets include a publicly available dataset, provided by CrowdFlower, and three datasets that were crowdsourced using Clickworker, a platform similar to Amazon's Mechanical Turk, where workers remain anonymous. The types of documents and the types of labels vary in the different datasets. The label types include categorical classes, with 3 or 5 classes of labels. In addition to the labels collected from workers on Clickworker, we also obtained a set of gold labels from highly trained editorial judges to be used as ground truth in our experiments. These gold labels were collected using the exact same interface as those shown to the workers. CrowdFlower's dataset includes gold labels with an undisclosed origin. In total, the four label sets contain 1, 367, 889 judgments for 194, 052 HITs from 3, 948 workers, with 38, 876 judgments by 1, 488 trained expert judges for 1, 209 gold HITs. Table <ref type="table" target="#tab_0">1</ref> provides an overview of some basic statistics over the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auto-completion Judgments (ACJ)</head><p>This task collected preference judgments over lists of query suggestions (autocompletion queries) that were shown to judges side-by-side to each other. Workers were asked to indicate their preferences over the two lists of auto-completions on a 3 point scale (-1, 0, 1), where 0 indicates no preference and 1 or -1 signal preference for one side or the other. This dataset includes 722,970 judgments for 72,142 HITs by 722 workers. Although this is the largest set in terms of volume, the number of workers is only the third largest across the four datasets. This set is thus somewhat unusual in terms of averaging 1,000 judgments per worker. Such a high perworker throughput may, however, simply be an indication of the task's popularity with these workers. Among the 72k HITs in the set, only 155 are gold tests, which were judged only by 77 workers, resulting in 3,100 judgments with known answers. Note that the relatively low number of gold tests in this task is compensated for by the high redundancy of 10 judgments per HIT.</p><p>Search Relevance Judgments (SRJ) Similarly to the ACJ set, this dataset contains preference based relevance judgments, but, this time, the preferences are over two search engine result pages (SERPs), shown side-by-side, for a given query. Judges were asked to indicate their preferences over the two SERPs, which was then, again, mapped to a 3 point scale of (-1, 0, 1). The set contains a total of 50,840 judgments for 22,623 HITs by 1,118 judges. Each HIT is a query and a pair of SERPs that may be judged multiple times by different judges. The set also contains 399 gold HITs with known labels, for which 9,352 judgments have been collected from 802 judges out of the total 1,118.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CrowdFlower (CF)</head><p>This dataset is provided by Crowd-Flower as part of the 2013 Crowdsourcing at Scale shared task challenge <ref type="foot" target="#foot_1">6</ref> . It consists of 569,375 sentiment analysis judgments for 98,980 HITs (questions) by 1,960 workers. The questions are tweets and the judgments reflect the sentiment of the tweets discussing weather. The labels can take values from four categories: negative (0), neutral (1), positive (2), tweet not related to weather (4) and cannot tell <ref type="bibr" target="#b5">(5)</ref>. The data also includes 300 gold HITs, which were judged by 461 of the 1,960 workers in the full dataset, resulting in a total of 1,720 judgments with known answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adult Relevance Judgments (ARJ) Workers in this</head><p>task were asked to rate whether a given set of query suggestions contained various types of adult content on a four point scale. For our experiments, we use the binary label whether adult content was detected <ref type="bibr" target="#b1">(1)</ref> or not (0), ignoring the actual classification. This dataset contains 24,704 judgments for 307 HITs by 148 workers. We have gold labels for all the HITs in this set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We compare our CommunityBCC model against the following set of commonly used baselines: Majority Voting (MV) The MV method, which is often used for the heuristic computation of crowd consensus <ref type="bibr" target="#b16">[16]</ref>, estimates the aggregated label as the one with the most votes, where each vote is considered with equal weight. As such, this method assumes that all workers are equally reliable. Importantly, in this paper we use a deterministic version of MV, which provides the most voted label only for unimodal vote distributions. Other stochastic interpretations of MV might use random draws for tie-breaking, e.g. weighted MV, which could marginally improve our deterministic MV in expectation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dawid&amp;Skene</head><p>The well-known crowd consensus model, presented by Dawid and Skene <ref type="bibr" target="#b2">[2]</ref>, allows the joint estimation of the items' true labels and the workers' confusion matrices. This model represent the frequentist version of BCC and is trained through the Expectation-Maximisation (EM) algorithm.</p><p>BCC The BCC model <ref type="bibr" target="#b8">[8]</ref>, which learns the true object labels and the workers' confusion matrices using the EP inference algorithm, as described in Section 3. In particular, we used uninformative priors for p and informative priors for the workers confusion matrices. Each row of π</p><formula xml:id="formula_16">(k) c</formula><p>has a Dirichlet prior with pseudo counts 1 expect for the diagonal count set to C -1. This means that workers are initially assumed to be better than random. These three methods were tested against our Community-BCC with uninformative priors for h and p, a noise precision prior for the worker score matrices ν = 100. The softmax of the community score matrices is set to be approximately equivalent to the prior of the BCC's confusion matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Metrics</head><p>The performance of each method is evaluated by two measures: (1) accuracy and (2) negative log probability density (NLPD). Accuracy is defined as the absolute estimation error given by the number of correct labels assigned, divided by the total number of assigned labels N :</p><formula xml:id="formula_17">Accuracy = num.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>correct labels num. gold labels</head><p>The NLPD provides a more comprehensive error measure, which also takes into account the uncertainty in the predictive label distribution, and is calculated as follows. Let {qi,1, . . . , qi,j} be the parameters of the predictive categorical distribution for item i. Then, the NLPD score is:</p><note type="other">MV</note><formula xml:id="formula_18">NLPD = 1 N N ∑ i=1 -log (q i,t * i )</formula><p>where t * i is the true label and ti = arg maxc ti,c is the estimated label obtained from the predictive distribution ti = {ti,c|c = 1 . . . C} for object i computed by the algorithm <ref type="foot" target="#foot_2">7</ref> .</p><p>Overall, we seek the best method with highest accuracy and lowest uncertainty, i.e., lowest NLPD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EVALUATION</head><p>In this section, we present the results of the empirical evaluation of three key features of our CommunityBCC model applied to crowdsourcing scenarios.</p><p>Robustness to Sparsity. We start by investigating the effect of sparsity on the accuracy of the different methods for estimating the true label for each object. We present the results from our accuracy evaluation against various sparsity levels within each dataset. For each of the four datasets, we simulate an adaptive learning setting in which labels are progressively selected. In detail, at each iteration, each method performs an intelligent selection of the set of n items for which to get an extra label in the next iteration by using the standard information theoretic criterion of entropy minimisation. That is, after an initial exploration phase in which one label for each item is observed, the methods select the next labels by taking the n items with the highest uncertainty in their predictive class distribution. Specifically, we used n = 3 for ACJ, n = 5 for SRJ, n = 2 for CF and n = 5 for ARJ. Figure <ref type="figure" target="#fig_6">4</ref> shows the accuracy measured for the four methods for each dataset. In particular, it shows that the accuracy of all methods improves as they get more data. The accuracy of CommunityBCC increases rapidly with the amount of data and is consistently higher compared to other methods when the size of label set is small. Specifically, looking at the points of the maximum gap between Communi-tyBCC and the other methods, CommunityBCC's accuracy is 8.4% higher than the second best method on SRJ, 7.5% higher on ACJ, 2.7% higher on CF and 3.9% higher on ARJ. Generally speaking, these results show that CommunityBCC is able to make more effective use of small sets of labels by exploiting community patterns between workers and transferring learning of community knowledge across community members. In large sets, CommunityBCC performs comparably or slightly better than the other methods. Overall, CommunityBCC offers the best trade-off between effectiveness and efficiency compared to alternative state-of-the-art baselines across all experimental testbeds. Community Detection. Figure <ref type="figure" target="#fig_8">5</ref> shows the communities that our method inferred for each dataset. It is interesting to note that CommunityBCC finds four communities for ACJ (Figure <ref type="figure" target="#fig_8">5a</ref>) and SRJ (Figure <ref type="figure" target="#fig_8">5b</ref>) although with different proportions. The first community is the group of workers who provide reliable answers and the community confusion matrix has consistently high value on the diagonal. We call this group Accurate workers. This community represents 28% and 4% of the workers in the ACJ and the SRJ dataset, respectively. The second community outlines the profile of workers who tend to provide correct answers although with lower probability (the diagonal value) compared to the Accurate community. We refer to this group of workers as Mostly accurate. The other two communities were found to correspond to biased workers. The first community of biased workers is the group of workers who mostly vote -1 or 1, who are present in 29% and 47% of the workers in the ACJ and the SRJ datasets, respectively. The second community of biased workers is the group of workers  who almost always vote 0, i.e., they tend not to pick a side in their judgment. This community is present with similar proportions within the two datasets: 31% for ACJ and 34% for SRJ. We respectively refer to these two communities as Outer class biased and Middle class biased.</p><formula xml:id="formula_19">-1 0 1 -1 0 1 1 0 -1 -1 0 1 1 0 -1 1 0 -1 1 0 -1 -1 0 1 -1 0 1 -1 0 1 0 -1 -1 0 1 0 -1 0 -1 1 0 -1 1 1 1 0 1 2</formula><p>On the CF testbed (Figure <ref type="figure" target="#fig_8">5c</ref>), there is a large community of calibrated workers (91%), who almost never use the "cannot tell" option (i.e., vote 4), and frequently select the correct judgment (i.e., using votes 1, 2 or 3), for all the tweets. The second (small) community is the one of conservative workers who mostly opt for the neutral (1) or the "not related to weather" (3) judgments. By contrast, the third community includes workers who often confidently pick negative (0) or positive (2) judgments. We refer to these communities as Calibrated, Conservative, and Decisive respectively.</p><p>On the ARJ testbed (Figure <ref type="figure" target="#fig_8">5e</ref>), there is a large majority of workers that are Accurate (99%), and there is only 1% workers biased towards outer classes (referred to as the Outer class biased community). We will later provide more insights about how the reported community percentages relate to the quality of the predicted label.</p><p>To evaluate the community detection accuracy of Com-munityBCC, we run a kmeans clustering algorithm (with k=4, that is the number of communities found by Commu-nityBCC) on the confusion matrices estimated by BCC for SRJ <ref type="foot" target="#foot_3">8</ref> . Then we compare the centroid of each cluster to our community matrices. Similar to <ref type="bibr" target="#b15">[15]</ref>, we first calculate the cumulative pairwise distance (CPD) for the confusion matrix of each worker defined as:</p><formula xml:id="formula_20">CPD(k) = ∑ k ′ ̸ =k HD(π (k) , π (k ′ ) )</formula><p>where HD(π (k) , π (k ′ ) ) is the Hellinger distance between the  <ref type="figure" target="#fig_9">6</ref>, it can be seen that the four communities of CommunityBCC clearly match the profiles of the kmeans clusters. In particular, Figure <ref type="figure" target="#fig_9">6b</ref> shows the actual confusion matrix of the worker corresponding to the centroid of each cluster. The centroid worker's confusion matrices are similar to the estimated community matrices. Furthermore, the proportions of workers in each communities reported in brackets are equivalent to the ones of computed by kmeans as showed in Figure <ref type="figure" target="#fig_9">6b</ref>. This means that our method correctly learns both the community profiles and the worker's community memberships.</p><p>Accuracy. Table <ref type="table" target="#tab_1">2</ref> reports the accuracy scores of all methods evaluated on the subset of gold items of each of the four datasets. Importantly, we use only gold items as training set in order to evaluate the user's accuracy profile only based on labels for items with gold labels. The scores show that the three confusion matrix-based methods (Dawid&amp;Skene, BCC and CommunityBCC) are generally more accurate than MV.</p><p>In particular, CommunityBCC is the best method which outperforms the best benchmark (BCC) by up to 3% on the CF dataset. On average, its performance is slightly higher than BCC. From the NLPD scores reported in Table <ref type="table" target="#tab_2">3</ref>, it can also be seen that, except for SRJ, CmmunityBCC is close to the best NLPD scoring method which means that its predictions also have low uncertainty, hence it is able to provide informative predictions. These accuracy scores reflect also the difficulty of the label aggregation task for each dataset. For instance, all the methods are consistently more accurate on ACJ (avg. 0.975) and less accurate on SRJ (avg. 0.689). Notice that the scores also relate to the proportions of accurate users available in the crowd, as discussed earlier in the community detection results. Indeed, SRJ has only 19% of accurate or mostly accurate workers while this percentage increases up to 40% in ACJ (Figure <ref type="figure" target="#fig_8">5</ref>). Consequently, all the methods are able to produce higher quality results for sets where more accurate labels are available. In summary, we conclude that our method performs comparably or slightly higher than the tested baselines in predicting the true labels, while it is much more efficient and less prone to error when only sparse data is available per worker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>In this paper, we presented a novel Bayesian model for the efficient aggregation of crowdsourced labels for classification tasks based on modelling worker's profiles as latent communities. The key innovation of our method is to model the worker communities within the label aggregation pro- More specifically, we model workers' community structures within the generative model of crowd labels and we apply Bayesian inference to learn (i) the accuracy profile, i.e., the confusion matrix of each community, (ii) the community membership of each worker, and the true label of each object. We also discover the optimal number of communities within the crowd using marginal likelihood optimisation. Thus, the key capabilities of our model are to (i) abstract inferred information about workers' reliability from a single individual to the general community and (ii) transfer knowledge about the community profile to workers. We also provided a scalable implementation of CommunityBCC using model decomposition techniques that allow us to train the model on hundred of thousands of labels within minutes on a standard desktop machine. This makes our new method particularly suitable for large-scale crowdsourcing applications.</p><p>We ran an extensive experimental evaluation with more than one million real-word crowdsourced labels for different tasks contributed by more than four thousand workers. We compared the performance of our method against three state-ofthe-art methods as baselines. We empirically showed that CommunityBCC is more accurate than the baseline methods, and is more effective on sparse datasets where, on average, it gains 8% more accuracy with the same amount of sparse labels compared to the baseline methods.</p><p>While this work is a step forward towards relaxing the assumption of independence between workers in crowdsourcing by incorporating the notion of communities, there are several other aspects related to dynamic and task-specific worker behaviours that have not yet been addressed and will be considered as future work. In future work, we will investigate the application of our community-based aggregation approach to other crowdsourcing problems, such as ranking or to continuous labels. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The factor graph of BCC. The users plate includes the user's confusion matrix π (k) while the objects plate includes the true object class variable ti. The observed (shaded) user's vote c (k) i lies at their intersection .</figDesc><graphic coords="3,431.41,145.07,118.98,132.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The BCC model assumes that the true class, ti, of an object i is generated from a categorical distribution with parameters p: ti|p ∼ Cat(ti|p) where p denotes the class proportions for all the objects. The label, c (k)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Factor graph of CommunityBCC. The extension to the BCC model (Figure 1) is depicted as the community plate which includes the community score s (m) c . This is connected to the user score s (k) c</figDesc><graphic coords="4,204.11,148.95,88.70,98.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(m) c representing the log probability vector of the c th row of the confusion matrix π (m) of community m. We apply a softmax operator to the community score s (m) c for all the classes c to derive a normalised exponentiated version of s (m) c , which we refer to as the community confusion matrix π (m) c : p(π (m) c |s (m) c ) = δ(π (m) c -softmax(s (m) c )) here δ is the Dirac delta function imposing the equality constraint between π (m) c and the softmax of s (m) c . In contrast to the standard BCC model where the confusion matrices of workers are independent, the Community-BCC model encodes the relationship between the confusion matrix of the community and the workers that belong to it. That is, each worker k has an individual score vector s (k) c , one for each class c, generated from the score vector s (m (k) ) c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Scalable CommunityBCC implementation. The figure shows the three sub-models and their cyclic schedule for the inference computation.</figDesc><graphic coords="5,58.27,198.69,232.19,128.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Accuracy of the four methods measured with increasing proportions of labels for each dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Communities of workers and members proportions (in brackets) inferred from CommunityBCC in each dataset. The bar plots are the communities confusion matrices inferred for each dataset. The values on the x, y axis are the object classes and values on the z axis are the probabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison between CommunityBCC and Kmeans clustering for community detection on the SRJ dataset. On the left, there is the comparison between the CommunityBCC communities (first row) and the kmeans clusters (second row). On the right, there is the plot of the cumulative pairwise distances and kmeans cluster assignments of each user.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Datasets: Avg-J (Avg-GJ) is the aver- age number of judgments (on gold HITs) per judge, Avg-H (Avg-GH) is the average number of judg- ments per (gold) HIT. Gold judges are the judges who were tested on gold HITs.</head><label>1</label><figDesc></figDesc><table><row><cell>Dataset:</cell><cell>ACJ</cell><cell>SRJ</cell><cell>CF</cell><cell>ARJ</cell></row><row><cell>HITs</cell><cell cols="2">72,142 22,623</cell><cell>98,980</cell><cell>307</cell></row><row><cell>Judgments</cell><cell cols="4">722,970 50,840 569,375 24,704</cell></row><row><cell>Judges</cell><cell>722</cell><cell>1,118</cell><cell>1,960</cell><cell>148</cell></row><row><cell>Avg-J</cell><cell>1,001.34</cell><cell>45.47</cell><cell cols="2">290.5 166.92</cell></row><row><cell>Avg-H</cell><cell>10.02</cell><cell>2.25</cell><cell>5.75</cell><cell>80.47</cell></row><row><cell>Gold HITs</cell><cell>155</cell><cell>399</cell><cell>300</cell><cell>307</cell></row><row><cell>Gold judgments</cell><cell>3,100</cell><cell>9,352</cell><cell cols="2">1,720 24,704</cell></row><row><cell>Gold judges</cell><cell>77</cell><cell>802</cell><cell>461</cell><cell>148</cell></row><row><cell>Avg-GJ</cell><cell>40.26</cell><cell>11.66</cell><cell cols="2">3.73 166.92</cell></row><row><cell>Avg-GH</cell><cell>20.00</cell><cell>23.44</cell><cell>5.73</cell><cell>80.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 : The accuracy scores of the four methods. The numbers for each dataset (rows) are the abso- lute estimation error of each method (column). The best performing run in each dataset is highlighted in bold.</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>MV</cell><cell cols="3">Dawid&amp;Skene BCC CommunityBCC</cell></row><row><cell cols="2">ACJ 0.967</cell><cell>0.972</cell><cell>0.987</cell><cell>0.974</cell></row><row><cell cols="2">SRJ 0.646</cell><cell>0.670</cell><cell>0.72</cell><cell>0.711</cell></row><row><cell>CF</cell><cell>0.840</cell><cell>0.830</cell><cell>0.860</cell><cell>0.886</cell></row><row><cell cols="2">ARJ 0.934</cell><cell>0.960</cell><cell>0.964</cell><cell>0.976</cell></row><row><cell cols="5">two confusion matrices. Then, kmeans is run on the set of</cell></row><row><cell cols="4">CPD values. From results showed in Figure</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 : The NLPD scores (the lower the better) of the three probabilistic methods: Dawid&amp;Skene, BCC and CommunityBCC computed from the pre- dictive object class distribution. The best perform- ing run in each dataset is highlighted in bold.</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Dawid&amp;Skene BCC CommunityBCC</cell></row><row><cell>ACJ</cell><cell>0.322</cell><cell>0.076</cell><cell>0.137</cell></row><row><cell>SRJ</cell><cell>0.791</cell><cell>1.226</cell><cell>1.414</cell></row><row><cell>CF</cell><cell>0.459</cell><cell>0.437</cell><cell>0.526</cell></row><row><cell>ARJ</cell><cell>0.150</cell><cell>0.157</cell><cell>0.157</cell></row><row><cell>cess.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_0"><p>The messages for the softmax factor (which is not conjugate) make use of the Taylor expansion in[9].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1"><p>https://sites.google.com/site/crowdscale2013/shared-task</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_2"><p>To avoid infinite errors, we threshold the NLPD with a with minimum probability of 0.001.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_3"><p>for brevity we report only the results of SRJ, however the conclusions for the other datasets are similar.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGMENTS</head><p>The authors gratefully thank Tom Minka for the support and discussion about the model. Matteo Venanzi would also like to thank Oliver Parson for early discussions about this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">How to grade a test without knowing the answers. a bayesian graphical model for adaptive crowdsourcing and aptitude testing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guiver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 29th Int. Conf. on Machine Learning</title>
		<meeting>of the 29th Int. Conf. on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1183" to="1190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of observer error-rates using the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Statistics</title>
		<imprint>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><surname>Crowdclustering</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Analyzing the Amazon Mechanical Turk marketplace</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">XRDS</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="16" to="21" />
			<date type="published" when="2010-12">December 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quality management on Amazon Mechanical Turk</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD Workshop on Human Computation, HCOMP &apos;10</title>
		<meeting>the ACM SIGKDD Workshop on Human Computation, HCOMP &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="64" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Combining human and machine intelligence in large-scale crowdsourcing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 11th Int. Conf. on Autonomous Agents and Multiagent Systems</title>
		<meeting>of the 11th Int. Conf. on Autonomous Agents and Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="467" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">In search of quality in crowdsourcing for search engine evaluation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in information retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="165" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<title level="m">Bayesian classifier combination. Int. Conf. on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="619" to="627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Correlated topic models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="147" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A family of algorithms for approximate Bayesian inference</title>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Winn</forename><surname>Gates</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1073" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Infer .net 2.5. Microsoft Research Cambridge</title>
		<author>
			<persName><forename type="first">T</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guiver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Knowles</surname></persName>
		</author>
		<ptr target="http://research.microsoft.com/infernet" />
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">115</biblScope>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Overlapping community detection using bayesian non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Psorakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ebden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sheldon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">66114</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning from crowds</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1297" to="1322" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic bayesian combination of multiple imperfect classifiers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Simpson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Psorakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Decision Making and Imperfection</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient budget allocation with accuracy guarantees for crowdsourcing classification tasks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tran-Thanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Venanzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Jennings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2013 Int. Conf. on Autonomous Agents and Multiagent Systems</title>
		<meeting>of the 2013 Int. Conf. on Autonomous Agents and Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="901" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Max algorithms in crowdsourcing environments</title>
		<author>
			<persName><forename type="first">P</forename><surname>Venetis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Polyzotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 21st international Conf. on World Wide Web</title>
		<meeting>of the 21st international Conf. on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="989" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The multidimensional wisdom of crowds</title>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2424" to="2432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Online crowdsourcing: rating annotators and obtaining cost-effective labels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conf. on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-F</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Whose vote should count more: Optimal integration of labels from labelers of unknown expertise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Movellan</surname></persName>
		</author>
		<author>
			<persName><surname>Ruvolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2035" to="2043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Variational message passing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="661" to="694" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning from the wisdom of crowds by minimax entropy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
