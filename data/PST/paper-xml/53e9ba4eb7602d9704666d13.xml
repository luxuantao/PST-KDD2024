<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human activity recognition using multi-features and multiple kernel learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013-12-17">17 December 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Salah</forename><surname>Althloothi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Denver</orgName>
								<address>
									<postCode>80208</postCode>
									<region>CO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
							<email>mmahoor@du.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Denver</orgName>
								<address>
									<postCode>80208</postCode>
									<region>CO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
							<email>xzhang62@du.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Denver</orgName>
								<address>
									<postCode>80208</postCode>
									<region>CO</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><forename type="middle">M</forename><surname>Voyles</surname></persName>
							<email>rvoyles@purdue.edu</email>
							<affiliation key="aff1">
								<orgName type="department">College of Technology</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<postCode>47907</postCode>
									<settlement>West Lafayette</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Human activity recognition using multi-features and multiple kernel learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013-12-17">17 December 2013</date>
						</imprint>
					</monogr>
					<idno type="MD5">E73D069C0866E5F6A24121381E3B8293</idno>
					<idno type="DOI">10.1016/j.patcog.2013.11.032</idno>
					<note type="submission">Received 17 April 2013 Received in revised form 8 November 2013 Accepted 30 November 2013</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Human activity recognition Multiple kernel learning Spherical harmonics coefficients Distal limb segments Support vector machines</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents two sets of features, shape representation and kinematic structure, for human activity recognition using a sequence of RGB-D images. The shape features are extracted using the depth information in the frequency domain via spherical harmonics representation. The other features include the motion of the 3D joint positions (i.e. the end points of the distal limb segments) in the human body. Both sets of features are fused using the Multiple Kernel Learning (MKL) technique at the kernel level for human activity recognition. Our experiments on three publicly available datasets demonstrate that the proposed features are robust for human activity recognition and particularly when there are similarities among the actions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human activity recognition has remained as an interesting and challenging topic in the field of computer vision and pattern recognition. This research topic is motivated by many applications such as surveillance systems, video browsing, and human-computer interfaces (HCI) design. In the past two decades, a significant amount of research has been done in the area of human activity recognition using a sequence of 2D images. Most published research is based on either shape features or motion features. Recently, researchers have paid more attention to using 3D spatio-temporal features for describing and recognizing human activities <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> due to easy access to depth information via new consumer technologies such as Microsoft's Kinect sensor.</p><p>In general, 3D spatio-temporal features look at the changes in the human body shape based on dominant motions in the human limbs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>. The variations in the body shape can be detected and represented with 3D spatio-temporal features as space-time volumes. Those features mainly focus on the representation of the shape and motion as a function of time. The main idea behind the methods that utilize the spatio-temporal features is to recognize human activity by detecting/describing the changes in human limbs either by describing the motion of human limbs or through measuring the similarities among different space-time volumes.</p><p>Recently, the developed commodity depth sensors such as Kinect <ref type="bibr" target="#b5">[6]</ref> have opened up new possibilities of dealing with 3D data. The Kinect sensor has given the computer vision community the opportunity to acquire RGB images as well as depth maps simultaneously at a good frame rate with a good resolution. As we can see in Fig. <ref type="figure" target="#fig_0">1</ref>, the depth map provides additional information as 3D data which is expected to be helpful in distinguishing different poses of silhouettes. Furthermore, compared with RGB images, the depth map increases the amount of information that can be used to detect 3D joint positions.</p><p>The research in human activity recognition based on a sequence of depth maps has been motivated with the release of the Kinect Windows SDK, which is utilized to estimate the 3D joint positions of the human body. Although Kinect produces better quality 3D motion than those estimated from regular RGB sensors (e.g., Stereo vision systems for 3D estimation), the estimated 3D joint positions are still noisy and fail when there are occlusions among human limbs such as two limbs crossing each other. Furthermore, the motion of 3D joint positions alone is insufficient to distinguish similar activities such as eating and drinking. Therefore, extra information needs to be included in the feature level to enhance the classification performance. In this context, we need to develop a method to fuse multiple types of features in order to discriminate similar activities and to enhance the recognition rate of the system. For instance, the motion features of human limbs, such as forearms and shins, may be augmented with the shape features that describe the silhouette structure to improve the accuracy of the action classification.</p><p>Consequently, fusion techniques can be used to enhance the classification performance of human activity recognition. In this context, some researchers have conducted fusion at the feature level such as <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> where they combined multiple types of features extracted from 2D images into a fused feature vector and then used a single classifier for action recognition. In particular, Liu et al. <ref type="bibr" target="#b6">[7]</ref> and Wang et al. <ref type="bibr" target="#b7">[8]</ref> combined spatio-temporal volume features and a deformation of the human silhouette obtained from sequences of 2D images to derive action descriptors. In another work, Liu et al. <ref type="bibr" target="#b8">[9]</ref> fused local spatio-temporal volumes and statistical models of interest points (Cuboids and 2D SIFT) obtained from 2D images for action recognition using hypersphere multi-class Support Vector Machines (SVM). Fusion can also be performed at the classifier level. For instance <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> designed multiple classifiers for two types of features extracted from 2D images, and their final decision was made by taking into account the complementaries among classifiers. In our work, two sets of features are extracted from a depth map (3D data) and are fused at the kernel level instead of the feature level in order to select useful features based on the weights using the MKL technique.</p><p>Recently, MKL techniques <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b46">47]</ref> have been proposed for feature fusion within kernel-based classifiers. The works presented in <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref> show that the MKL technique can enhance the discrimination power and improve the performance of classifiers. The idea behind MKL is to optimally combine different kernel matrices calculated from multiple types of features with multiple kernel functions. Within this framework, the problem of multifeature representation with a single kernel function in the canonical SVM is transferred to set the optimal value of kernel combination weights for multiple kernel matrices. These works empirically show that the MKL-based multiclass SVM outperforms the canonical multiclass SVM.</p><p>This paper presents a method to recognize human activities using a sequence of RGB-D data. The basic idea of our method is illustrated in Fig. <ref type="figure">2</ref>. Based on the surface representation and the kinematic structure of the human body, we propose a method that can characterize shapes and motions. In our approach the shape features, extracted from the depth map using spherical harmonics representation, are used to describe the 3D silhouette structure. The motion features, extracted from the estimated 3D joint positions, are used to describe the movement of the human body. The distal limb segments of the human body are utilized in our method to describe the motion because we believe that segments such as forearms and shins provide sufficient and compact information for human activity recognition. Therefore, each distal limb segment is described by the orientation and translation distance with respect to the initial frame in order to create motion features. Both sets of features are fused using the MKL technique <ref type="bibr" target="#b15">[16]</ref> to produce an optimally combined kernel matrix within SVM for activity classification. This kernel matrix has more discriminating power than a single kernel function due to the utility of multiple features within different kernel functions.</p><p>Compared with the aforementioned 2D-based approaches for multi-feature fusion, our work is based on features extracted from 3D data (depth map). Also, our approach is based on multiple kernel functions and multiple features which have more advantages over single kernel function with multiple features. In fact, a single kernel cannot perform well when the nature of the features are different and incompatible. Furthermore, combining multiple features into one feature vector introduces the curse of dimensionality problem.</p><p>In summary, the contributions of this paper are summarized as follows: (1) A novel 3D shape feature using spherical harmonics transformation to represent the body silhouette is proposed. (2) The human body motions (i.e. kinematic structure) are described using only the distal limb segments. (3) These two types of features are fused at the kernel level as a novel methodology in order to differentiate similar activities and enhance the classification rate of the system.</p><p>The remainder of this paper is organized as follows. A brief review of related work is presented in Section 2. Section 3 explains our proposed frame work for activity recognition using 3D spatiotemporal features and feature fusion using the simple MKL approach. Our experimental results are presented in Section 4. Finally, Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>In the last decade, the shape-based methods using 2D images (captured by a regular RGB camera) have been widely used for action recognition. There are several different shape-based  methods which are based on silhouettes for statistical shape analysis as a function of time. The shape analysis approaches aim to describe and locate the changes in the human body shape. For instance, Blank et al. <ref type="bibr" target="#b16">[17]</ref> analyzed 3D shapes of silhouettes in a space-time volume to create 3D spatio-temporal features for human activity recognition. Cohen and Li <ref type="bibr" target="#b17">[18]</ref> presented a 3D spatio-temporal feature for classifying and identifying human posture using SVM. They proposed global shape features that are invariant to rotation and translation. The main advantage of their approach is in its ability to capture human shape variationsallowing for the identification of body posturesbut it needs to use multiple cameras to create 3D spatio-temporal features.</p><p>Another example is the work of Yilmaz and Shah <ref type="bibr" target="#b18">[19]</ref> which represented the action as a spatio-temporal feature defined on structure of contour across successive video frames. The action features are described by analyzing the differential geometry properties of the surface volume. The limitation of this method is the exhaustive search required for finding the correspondences between two volumes. The motion-based methods, which are based on motion trajectory, recognize the action by employing either human limb positions or interest points on the human body. For instance, Chen et al. <ref type="bibr" target="#b19">[20]</ref> and Fujiyoshi and Lipton <ref type="bibr" target="#b20">[21]</ref> proposed a star skeleton representation based on the shape geometry to recognize human action. They detected the extremities of the silhouette with respect to the centroid and assumed that these points represent the head, hands, and feet. Chun et al. <ref type="bibr" target="#b21">[22]</ref> proposed a 3D star spatio-temporal pattern based on the shape boundary information of the human posture using eight projection maps from different views. They detected the extremities of the silhouette with respect to the centroid as a shape feature. They assumed that these extremities represent the head, hands, and feet. They accumulated the motion history of these features in order to create a spatio-temporal pattern. Although the star shape representation is simple and fast for computation, its accuracy for detecting limbs needs further improvement. In the method proposed by Althloothi et al. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, skeleton models were fitted to silhouettes to capture the positions of distal limb segments (i.e. arms, legs and head). The positions of distal limb segments were used as features. Then, Gaussian mixture models were used to model the spatio-temporal distribution of the distal limb segments over the period of an action. Sun et al. <ref type="bibr" target="#b24">[25]</ref> extracted trajectories through pairwise Scale Invariant Feature Transform (SIFT) correspondences between two consecutive frames. The stationary distribution of a Markov chain model was then used to compute motion features. Tian et al. <ref type="bibr" target="#b25">[26]</ref> employed the Harris detector and local HOG descriptor on Motion History Images (MHI) to perform action recognition and detection. The fundamental difference between the aforementioned and recent methods described in the next paragraphs is that all the extracted features are captured by regular RGB cameras, instead of an RGB-D sensor which represents the depth map of the human body shape.</p><p>Recently, with the release of the Microsoft Kinect sensor, research on human activity recognition based on a sequence of depth maps and kinematic structure has resurged. Wang et al. <ref type="bibr" target="#b0">[1]</ref> proposed a model for human actions, called the Actionlet Ensemble Model which is learned using the MKL technique to represent each action and to capture the intra-class variance. Wang et al. <ref type="bibr" target="#b0">[1]</ref> added a temporal pyramid to capture the temporal in order to improve the accuracy. Xia et al. <ref type="bibr" target="#b2">[3]</ref> developed a method based on 3D joint positions estimated from a depth map to create a histogram of 3D joint positions using a spherical coordinate system. Then, they modeled the temporal evolutions of 3D joint positions by discrete hidden Markov models in order to train/ classify the action. Li et al. <ref type="bibr" target="#b1">[2]</ref> proposed a Bag-of-3D-Points model for action recognition. They first sampled 2D points at equal distances along the contours of projections formed by mapping the depth map onto three orthogonal Cartesian planes, i.e. XY, XZ, and YZ planes. Then, the sampled 2D points were used to characterize the posture in each frame.</p><p>Similarly, Yang et al. <ref type="bibr" target="#b3">[4]</ref> generated the Depth Motion Maps (DMM) from three orthogonal planes and accumulated global activities through entire video sequences. Then, Histograms of Oriented Gradients (HOG) are computed from the DMM as the representation of an action model. Yang et al. <ref type="bibr" target="#b3">[4]</ref> collapsed the whole sequence of frames into one image DMM, which eliminates the temporal order of shape/motion cues. Sung et al. <ref type="bibr" target="#b26">[27]</ref> compute a set of features based on human pose and motion of the 3D joint positions provided by Prime Sense with Kinect. They proposed a hierarchical maximum entropy Markov model (MEMM), which considers the human activity as composed of two-layered graph structure.</p><p>Yang and Tian <ref type="bibr" target="#b4">[5]</ref> proposed a method based on position differences of 3D joint positions. They applied Principal Component Analysis (PCA) to joint differences to obtain EigenJoints by reducing redundancy and noise. Then, they employed the Naive Bayes Nearest Neighbor (NBNN) classifier for multi-class action classification. Zhang and Parker <ref type="bibr" target="#b27">[28]</ref> proposed a 4D local spatial-temporal descriptor that combines both intensity and depth information. The proposed descriptor computes and concatenates the intensity and depth gradients within a 4D hyper cuboid, which is centered at the detected feature point, as a feature. Zhao et al. <ref type="bibr" target="#b28">[29]</ref> extract the feature vector of each video clip by combining the RGB-based descriptor and depth-map based descriptor. They used three types of features; Local Depth Pattern (LDP), which describes the local region of interest points in the depth map, HOG and HOF features in RGB data. All these features are concatenated in one vector in order to classify human action as multi-class classification using LibSVM <ref type="bibr" target="#b29">[30]</ref>. Omar et al. <ref type="bibr" target="#b30">[31]</ref> presented the HON4D approach that can describe a sequence of depth maps using a histogram capturing the distribution of the surface normal orientation in a 4D space of time, depth, and spatial coordinates. This process needs to create 4D projectors, which quantize the 4D space and represent the possible directions for the 4D normal. The limitation of the HON4D approach is the quantization process used to build the histogram. Holte et al. <ref type="bibr" target="#b31">[32]</ref> proposed an approach for view-invariant gesture recognition. They focused only on arm gestures by segmenting the arms (when they move) using optical flow to represent the motion context which represents the velocity of the arms by using the location of motion, the amount of motion, and its direction. Also, they utilized spherical harmonics representation to make the motion context to be rotation invariant through the use of a depth map. Compared to our method, they did not use spherical harmonics as shape features for human action recognition.</p><p>From the literature review, we have observed that the motion and shape features alone have their own limitations in representing human activities. The motion features are not robust in capturing the change in velocity among the frames. While the shape features can capture some pose information of the human body, but without motion features the capability of describing human activity is limited. Also, the depth map captured from an RGB-D sensor contains rich shape information of the human body compared with the RGB images. In addition, the distal limb segments such as forearms and shins provide sufficient and compact features for human action recognition. The movements of distal limb segments are important cues for estimating the 3D motion of the human limbs. These observations form the core concept of the method proposed in this paper. Therefore, the main idea of our work is to fuse the motion features of the distal limb segments with the shape features extracted from the depth map in a novel way to enhance the classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed method</head><p>In our work, we utilized spherical harmonics representation to extract 3D shape features of the silhouette and the kinematic structure of the human body to extract the 3D motion features. These features are fused using the SimpleMKL algorithm <ref type="bibr" target="#b15">[16]</ref> with different kernel functions where both the kernel combination weights and discriminate hyper planes of multiclass-SVMs are optimized. In the end, human action classification based on the learned kernel weights and discriminated hyper planes of multiclass-SVM are used to address the issues of recognizing complex activities. The general framework of our proposed method is demonstrated in Fig. <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spatio-temporal features</head><p>This section gives a detailed description of the two proposed 3D spatio-temporal features for human activity representation and classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Motion features</head><p>The distal limb segments are employed to extract the motion features of the human movement. In fact, the positions of the distal limb segments (four limbs) provided by a Kinect sensor, as illustrated in Fig. <ref type="figure" target="#fig_1">3</ref>, are utilized to extract the motion features which represent the orientation and the translation distance of the distal limb segments. Our key observation is that the change in the positions of the distal limb segments provide sufficient information to represent the human body movement as discriminative features to classify the actions.</p><p>In our approach, the end points of the distal limb segments are used to characterize the motion features. Thus, each distal limb segment L k is described by its end points as a 3D vector with respect to the body center (hip) in order to create the 3D unit vector which represents the orientation of the distal limb segment.</p><p>The orientation of the 3D vector is defined by a unit vector U ! ab for frame F t . Therefore, the 3D coordinates ðx i ; y i ; z i Þ of end points J(t)</p><formula xml:id="formula_0">of distal segments L k in frame F t is J L k i ðtÞ ¼ fj a ; j b g,</formula><p>where k is a distal limb number and N is the number of distal limb segments.</p><formula xml:id="formula_1">V ! ab ðtÞ ¼ fJ L k b ðtÞÀJ L k a ðtÞ j k ¼ 1; 2; …; N; a a bg ð 1Þ V ! ab ðtÞ ¼ ðx b À x a Þ î þðy b À y a Þ ĵ þðz b À z a Þ k<label>ð2Þ</label></formula><formula xml:id="formula_2">U ! ab ðtÞ ¼ V ! ab ðtÞ J V ! ab ðtÞ J ¼ A x î þ A y ĵ þ A z k<label>ð3Þ</label></formula><p>Since all the measurements are relative to the center of the body in one frame, it is necessary to add more information about the motion of the human limbs between the current frame F t and the initial frame F t 0 , which represents to the neutral human pose. Thus, the translation, which gives the difference in position for the distal limb segments between two frames, is computed in order to create a 3D spatio-temporal feature and to make the classifier discriminate between the actions that have similar orientation but different positions. Practically, each human subject has four distal limb segments which are tracked by the skeleton tracker <ref type="bibr" target="#b32">[33]</ref>, each distal limb L k is represented by 3D unit vectors U ! ab and translation vectors D tt 0 in each frame.</p><formula xml:id="formula_3">D tt 0 ¼ fJ J t a À J t 0 a J j J t a εF t ; J t 0 a εF t 0 g ð<label>4Þ</label></formula><p>The 3D unit vector U ! ab and translation vector D tt 0 with respect to the initial frame F t 0 represent motion features in one frame. In order to represent the motion features that include t number of frames, our motion features for all the frames are concatenated together to build a spatio-temporal feature vector for motion features. This yields to a 3-D spatio-temporal feature with precise orientation and translation data for each human limb in all the frames. These motion features define the movement of distal limbs in the video. In addition, all of the vectors which represent the 3D distal limb segments were normalized to reduce intra-class variations among subjects and to be invariant to the body size. We use a linear normalization scheme to scale the features in the range ½À1 to þ1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Shape features</head><p>Due to the motion similarity in some activities, it is insufficient to only use the motion features to fully describe and model an activity. We must create another feature that can describe the human body shape in order to increase the accuracy of the classifier. We use spherical harmonics coefficients <ref type="bibr" target="#b33">[34]</ref> as shape features for representation and recognition of human actions from a sequence of depth maps. Over the last decade, spherical harmonics coefficients have been applied to several computer vision applications such as 3-D shape descriptors <ref type="bibr" target="#b34">[35]</ref> as well as 3-D model retrieval <ref type="bibr" target="#b35">[36]</ref>, medical image analysis <ref type="bibr" target="#b33">[34]</ref> and rotation estimation <ref type="bibr" target="#b36">[37]</ref>. According to <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b37">38]</ref>, spherical harmonics coefficients are suitable for shape comparison because it can deal with protrusions and intrusions. Also, it can be used as an abstract features that can characterize the 3D object with different resolution depending on the spherical harmonics band. Consequently, a 3D surface object with thousands of vertices can be represented using spherical harmonics coefficients up to a user specified maximum band L max .</p><p>In our approach, the body silhouette is described by a large number of 3D geometric cloud points obtained from the depth map. These points are down-sampled to reduce the number of mesh points and to pare down the number of vertices that can represent the human body. Spherical harmonics decomposition is then applied to these vertices and a set of spherical harmonics coefficients are extracted. These coefficients are used to build spatio-temporal features that describe the human body shape in the spherical harmonics domain.</p><p>Spherical harmonics decomposition was originally used as a type of parametric surface representation for radial surfaces and later extended to more general shapes by representing the shape surface using a spherical function Sðθ; ϕÞ ¼ ðS x ðθ; ϕÞ; S y ðθ; ϕÞ; S z ðθ; ϕÞÞ with θ as the polar angle and ϕ as the azimuth angle <ref type="bibr" target="#b38">[39]</ref>.</p><p>In the spherical harmonics decomposition a spherical parametrization algorithm <ref type="bibr" target="#b39">[40]</ref> is first used to establish a one-to-one mapping between 3D points on a human body surface (vertices) and 3D points on the unit sphere. The result of the spherical parametrization process is a bijective mapping between each 3D point on a human body surface and a pair of spherical coordinates, θ and ϕ. Therefore, the human body surface is represented as a spherical function: Sðθ; ϕÞ which specifies the distance from a specified origin to each point on the sphere surface for all three coordinates x ¼ R cos ϕ sin θ; y ¼ R sin ϕ sin θ, and z ¼ R cos θ.</p><p>Fig. <ref type="figure" target="#fig_2">4</ref> shows a voxel representation of the human body and its mapping onto a unit sphere.</p><p>After spherical parametrization, the spherical harmonic expansion <ref type="bibr" target="#b33">[34]</ref> can be used to expand the human surface represented as a spherical function into a complete set of spherical harmonics coefficients. Spherical harmonics expansion is essentially a Fourier transform technique that defines a 3D surface using a spherical function and transforms them into a set of spherical harmonics coefficients ðc m lx ; c m ly ; c m lz Þ in the frequency domain. These coefficients can be calculated up to a user-desired band L max by solving a system of linear equations. Thus, given spherical function Sðθ; ϕÞ defined on the surface of the unit sphere, we can transform it into a set of spherical harmonics coefficients ðc m l ¼ c m lx ; c m ly ; c m lz Þ using the following equations:</p><formula xml:id="formula_4">Sðθ; ϕÞ ¼ ∑ Lmax l ¼ 0 ∑ l m ¼ Àl c m l Y m l ðθ; ϕÞ ð<label>5Þ</label></formula><p>where Y l m denotes the orthogonal spherical harmonics base of degree l and order m, and L max is the band-width of the spherical harmonics, l and m are integers with 0 r l o L max and jmjr l:</p><formula xml:id="formula_5">Y m l ðθ; ϕÞ ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi ð2l þ1Þðl À mÞ! ð4πðl þ mÞ!Þ 2 s P m l ð cos θÞe ðimϕÞ<label>ð6Þ</label></formula><p>P m l ðxÞ is the associated Legendre polynomial defined by the differential equation:</p><formula xml:id="formula_6">P m l ðxÞ ¼ ðÀ1Þ m ð2 l l!Þ ð1 þ x 2 Þ m=2 d l þ m dx l þ m ðx 2 À 1Þ l<label>ð7Þ</label></formula><p>Then, the spherical harmonics coefficients ðc m lx ; c m ly ; c m lz Þ, which are related to Sðθ; ϕÞ can be independently decomposed in terms ðS x ; S y ; S z Þ of the spherical harmonics.</p><p>Clearly, calculating the spherical harmonics coefficients linearly depends on the spherical harmonics band L max and number of surface points. In particular, these coefficients are 3D vectors which are computed up to a user specified maximum band to construct the 3D shape features. Therefore, based on spherical harmonics decomposition, we can extract our 3D shape features with different frequency harmonics. These 3D shape features define the body shape in one frame. In order to represent an action that includes t number of frames, our 3D spatio-temporal features for all the frames are concatenated together to build a single vector for all shape features.</p><p>In our work, we modified the algorithm in <ref type="bibr" target="#b33">[34]</ref> that works with 3D closed surface (genus-zero surface) for modeling and enhancing 3D complex morphological structures. First, we convert the mesh surface into a voxel surface which has uniform vertex sampling in order to fill all the holes on the surface. Then, we adapt only the initial mapping method to the voxel surface in order to accelerate the process of the spherical parametrization and to establish a one-to-one mapping on the unit sphere. Afterwards, spherical harmonics expansion is used to calculate spherical harmonics coefficients that represent the human body surface in the frequency domain. Our algorithm is used only to calculate the spherical harmonics coefficients from a 3D surface without any smoothing, enhancement or optimization process on the surfaces compared with the Shen et al. <ref type="bibr" target="#b33">[34]</ref> algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-feature fusion using SimpleMKL</head><p>The MKL algorithms have recently received great attention in the field of computer vision and pattern recognition. The idea behind MKL is to optimally combine and utilize multiple kernels and features instead of using a single kernel in learning kernel based classifiers. In this work, SimpleMKL algorithm <ref type="bibr" target="#b15">[16]</ref> based SVM is employed to fuse multiple types of features with multiple kernel functions to create discriminative weights d m , m ¼ 1; …; M for generalizing a multiple kernel matrix K in multiclass-SVM based classifiers. Furthermore, combining multiple types of features in MKL helps the classifier to achieve a high recognition rate since different features reflect different pieces of useful information. Practically, two types of features are extracted from each frame t: the motion features x i from 3D distal limbs segments, and the shape features z i from spherical harmonics coefficients.</p><p>Let us assume that we are given a set of N training samples fðx i ; z i ; y i Þg N i ¼ 1 , where x i is a feature vector of the ith sample in the training set X , x i A R D 1 , and z i A R D 2 is another feature vector of that sample in the set Z, and y i A fÀ1; þ 1g is their corresponding class label. kðÁ; ÁÞ is a kernel function that maps two feature vectors to be a positive scalar. The SimpleMKL algorithm was proposed to address the MKL-based SVM problem by solving the convex problem defined as</p><formula xml:id="formula_7">min d max β Lðd; βÞ ¼ À 1 2 β T ∑ M m ¼ 1 d m K m β þ y T β s:t: ∑ N i ¼ 1 β m ¼ 0; 0 rβ i y i r C; i ¼ 1; 2; …; N ∑ M i ¼ 1 d m ¼ 1; d m Z 0; m ¼ 1; 2; …; M<label>ð8Þ</label></formula><p>Here, y ¼ ðy 1 ; y 2 ; …; y N Þ T , and β ¼ ðβ 1 ; β 2 ; …; β N Þ T is known as the vector of Lagrangian coefficients in SVM. The multiple kernel matrix K is generated as</p><formula xml:id="formula_8">K ¼ ∑ M m ¼ 1 d m K m</formula><p>, where K m is the kernel matrix calculated based on single type of features with single kernel function. d ¼ ðd 1 ; d 2 ; …; d M Þ T is the kernel combination weight vector, and M is the total number of kernel functions in use.</p><p>In our work, suppose we are given a pair of samples (e.g., the ith and jth registered images), the fusion of the extracted shape (fx i ; x j g) and motion features (fz i ; z j g) at kernel level within the SimpleMKL framework is handled as follows:</p><formula xml:id="formula_9">K i;j ¼ ∑ M m ¼ 1 ðd m k m ðx i ; x j Þþd m þ M k m ðz i ; z j ÞÞ<label>ð9Þ</label></formula><p>Hence in this fusion work, the dimensionality of kernel combination weight vector d is 2M.</p><p>In the SimpleMKL algorithm, the Two-Step method <ref type="bibr" target="#b40">[41]</ref> is used to solve the optimization problem defined in Eq. ( <ref type="formula" target="#formula_7">8</ref>), where two nested iterative loops are set to optimize both the classifier and kernel combination weights. In the inner iteration a solver of SVM is implemented by fixing the vector of kernel combination weights while in the outer iteration a reduced gradient descent algorithm <ref type="bibr" target="#b41">[42]</ref>, along with the golden section search method <ref type="bibr" target="#b42">[43]</ref>, is used to update the combination weights with the fixed parameters of the SVM classifier. Therefore, once given a new test sample, its label (determined by y 0 ) can be calculated according to the following function:</p><formula xml:id="formula_10">y 0 ¼ sgn ∑ N i ¼ 1 ∑ M m ¼ 1 β i ðd m k m ðx i ; x 0 Þþd m þ M k m ðz i ; z 0 ÞÞ " #<label>ð10Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">SimpleMKL based multiclass-SVM</head><p>In our work, we employed SimpleMKL framework for multiclass classification. Suppose we want to classify U classes using binary classifiers. Two techniques are commonly used in the literature: oneagainst-one and one-against-rest. In the one-against-one technique, UðU À 1Þ=2 binary classifiers are built from all pairs of distinct classes, whereas in the one-against-rest technique U binary classifiers are built for each class of data.</p><p>The authors of <ref type="bibr" target="#b15">[16]</ref> presented a structure of MKL based multiclass-SVM using the SimpleMKL algorithm in the outer iteration of the Two-Step method <ref type="bibr" target="#b40">[41]</ref>. In their structure, a single kernel combination weight vector is jointly learned for all binary classifiers in the multiclass-SVM, and the general objective function L(d) is defined as</p><formula xml:id="formula_11">LðdÞ ¼ ∑ u A Φ L u ðdÞ ð<label>11Þ</label></formula><p>where Φ is the set of all pairs of distinct classes considered in the multiclass-SVM, and L u (d) is the object function of a binary MKLbased SVM defined in Eq. ( <ref type="formula" target="#formula_7">8</ref>) with fixed β for each binary classifier.</p><p>By this definition, the inner loop of the SimpleMKL based multiclass-SVM is meant to solve the multiclass-SVM while in the outer loop a single kernel weight vector is learned to minimize the summation of the objective functions from all binary classifiers. Therefore, the learned optimal kernel weight vector can be used for all binary classifiers, which generally increases the recognition result of multiclass-SVM. In our work, the oneagainst-one technique is used based on the experiences of the work in <ref type="bibr" target="#b43">[44]</ref>, and the classification of novel samples is done by a max-wins voting strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>In our experiments, we used three challenging publicly available datasets for human activity recognition, MSR-Action 3D dataset <ref type="bibr" target="#b1">[2]</ref>, MSR-Daily Activity 3D dataset <ref type="bibr" target="#b0">[1]</ref>, and 3D Action-Pairs dataset <ref type="bibr" target="#b30">[31]</ref>, to evaluate our proposed method. The content of each dataset and the experimental results are described in the following subsections.</p><p>Also, for each type of feature two kernel functions are used: Gaussian function and polynomial function, each with different parameters are linearly combined to classify the action using a multiclass-SVM classifier. We used the following configuration for kernel functions to fuse multiple types of features with different kernel function parameters based on the SimpleMKL framework as defined in the following equations:</p><formula xml:id="formula_12">k Gaussian ðx; yÞ ¼ e À J x À y J 2 =s 2<label>ð12Þ</label></formula><formula xml:id="formula_13">k poly ðx; yÞ ¼ 〈x; y〉 d ; d A N<label>ð13Þ</label></formula><p>where s is the kernel parameters of the Gaussian function and d is the parameter for varying the order of the polynomial function. In all experiments, we set different values of parameters for the two kernel functions with the criterion that they fill a proper range of the defined domain. For the Gaussian function, we set s 2 A f0:01; 0:1; 1; 5; 10; 60; 500g, and for the polynomial function, we set d A f1; 2; 3g. Thus, we obtained 10 alternatives for parameterizing the two defined kernel functions. Hence, given any pair of samples (e.g., the ith and jth), the fusion of extracted shape features (fx i ; x j g) and motion features (fz i ; z j g) at the kernel level within our framework is handled as follows:</p><formula xml:id="formula_14">K i;j ¼ ∑ 10 m ¼ 1 ½d m k m ðx i ; x j Þþd m þ 10 k m ðz i ; z j Þ<label>ð14Þ</label></formula><p>where k m ðÁ; ÁÞ is one of the 10 optional kernel functions, and</p><formula xml:id="formula_15">d ¼ ðd 1 ; d 2 ; …; d 20 Þ T (‖d‖ p ¼ 1; p Z 1)</formula><p>is the kernel combination vector to be optimized during MKL. Within this frame, several experiments were conducted using different number of training samples in order to evaluate the performance of our proposed activity recognition method. The empirical results show that our proposed method is comparable with the state-of-the-art-methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">MSR-Action 3D dataset</head><p>The MSR-Action 3D dataset <ref type="bibr" target="#b1">[2]</ref> consists of depth map sequences captured by a depth camera (RGB-D sensor). It contains different human actions: high arm wave, horizontal arm wave, hammer, hand catch, forward punch, high throw, draw x, draw tick, draw circle, hand clap, two hand wave, side boxing, bend, forward kick, side kick, jogging, tennis swing, tennis serve, golf swing, pick up and throw. It includes 20 actions performed by 10 subjects. Each action was performed 2 or 3 times by each subject. The size of the depth map is 320 Â 240. All the subjects were facing the camera during the performance, and were given a freedom to perform the actions at their own place in front of the camera. For instance, Fig. <ref type="figure" target="#fig_3">5</ref> shows a sequence of depth maps for Tennis Serve action. Furthermore, most of the actions in this dataset involve only the movement of limbs (i.e. arm and leg) in one place, which makes most of the actions highly similar to each other. In fact, this dataset is more challenging compared with the previous datasets such as the Weizmann (Blank et al. <ref type="bibr" target="#b16">[17]</ref>) and KTH (Schuldt et al. <ref type="bibr" target="#b44">[45]</ref>) datasets in human activity recognition.</p><p>In the first experiment, we used the end points of distal limb segments to calculate the orientation as a unit vector and the translation distance with respect to the initial frame as explained in Section 3. In addition, the surface points of the silhouette are employed to calculate the spherical harmonics coefficients. We trained two thirds of samples in total and the other third of the samples were used to test the recognition rate of our proposed method in classifying the performed actions. We repeated this experiment three times, each time we change the training and testing samples, and the results of classifying actions were averaged. Fig. <ref type="figure" target="#fig_4">6</ref> illustrates the result of action classification in the form of a confusion matrix. From Fig. <ref type="figure" target="#fig_4">6</ref>, we note that the proposed method achieves an average recognition rate of 90.7% for all actions together. Classification errors occur if there is a high rate of similarity among the actions, such as "forward punch" and "draw x" or if the occlusion occurs among human limbs making the skeleton tracker fail as in the tennis swing action. Therefore, since the skeleton tracker sometimes fails and because of the high rate of similarity among the actions, we considered the 90.7% recognition rate for 20 actions a success comparable with other methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>In addition, we compared the performance of our proposed method with the state-of-the-art methods which are evaluated on the same dataset (MSR-Action 3D dataset <ref type="bibr" target="#b1">[2]</ref>). The first approach was developed by Xia et al. <ref type="bibr" target="#b2">[3]</ref>, where they modeled the histograms of 3D skeleton joint locations by discrete hidden Markov models in order to train/classify the action. The second approach proposed by Li et al. <ref type="bibr" target="#b1">[2]</ref> is a Bag-of-3D-Points model for action recognition.</p><p>In order to make a fair comparison, we follow the same experimental settings as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref> by dividing the twenty actions into three subsets, each subset contains eight actions, as listed in Table <ref type="table" target="#tab_1">1</ref>. We compare our method with the state-of-the-art methods in two tests, where different number of training samples are chosen. In the first test (Test-1), since each subject has three samples, two third of the samples were used as training samples and the rest as testing samples. In the second test (Test-2), which is a cross subject test, half of the subjects were used for training and the rest subjects were used for testing (all the samples were used in Test-2). Then, the results of action classification were averaged over three different training/testing subsets. Table <ref type="table" target="#tab_2">2</ref> reports the average recognition rates for the state-of-the-art methods.   As Table <ref type="table" target="#tab_2">2</ref> shows, in the second test (Test-2), the overall accuracies are lower than Test-1 for all the methods because some of the actions are very similar (e.g. forward punch, high throw, draw x, draw tick, and draw circle) and also the number of training samples are lower than the number of samples in Test-1. Furthermore, Test-2 is conducted across subjects, whereas in Test-1 the same subjects may be in both training/testing (2/3 of the samples were used as training and 1/3 of the samples were used as testing). Overall, our approach outperforms <ref type="bibr" target="#b1">[2]</ref> in both tests. However, compared with <ref type="bibr" target="#b2">[3]</ref>, it gains 1% in Test-2 but degrades by 2.7% in Test-1. This degradation is due to the fact that <ref type="bibr" target="#b2">[3]</ref> requires building a large posture vocabulary set and manually encoding the action sequences which improve the accuracy. Whereas, all the processes of our method are done automatically starting from depth map and ending with the action classification.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">3D action-pairs dataset</head><p>The 3D Action-Pairs dataset <ref type="bibr" target="#b30">[31]</ref> contains new styles of activities which are selected in pairs such that the two activities of each pair are similar in motion (have similar trajectories) and shape (have similar objects). For instance, "Pick up" and "Put down" actions have similar motion and shape. This dataset has six pairs of activities: "Pick up a box/Put down a box", "Lift a box/Place a box", "Push a chair/Pull a chair", "Wear a hat/Take off a hat", "Put on a backpack/Take off a backpack", and "Stick a poster/Remove a poster". A few samples of the 3D Action Pairs Dataset are shown in Fig. <ref type="figure">7</ref>. The dataset includes 12 activities performed by 10 different subjects. Each activity was performed three times by each subject. We used this dataset in order to emphasize two points: First, to evaluate the performance of our proposed method in the case of activities that have similar trajectories and objects. Second, to show the advantage of using the feature fusion at the kernel level to enhance the recognition rate.</p><p>In order to verify the performance of our method, Leave-One-Subject-Out (LOSO) cross validation was applied. In the LOSO test, one subject is removed from the training set and the other subjects were utilized to train the multiclass-SVM classifier. The excluded subject is used to test the accuracy of our method in classifying the performed activities. This process is repeated for all the subjects, and the results are averaged for all test subjects. Fig. <ref type="figure">8</ref> illustrates the classification results in terms of a confusion matrix. From Fig. <ref type="figure">8</ref> we note that the proposed method achieves an average recognition rate of 90.8%. Also, we observe that most of the classification errors occur in the activity pairs because some subjects have a low accuracy due to the noisy skeleton data. Fig. <ref type="figure" target="#fig_6">9</ref> illustrates the variation in the accuracy among the 10 subjects. We can observe that most of the subjects have a high accuracy (over than 80%) except subject 6 (the accuracy is 70%) due to the noisy skeleton data for this subject. By excluding subject 6 from the experiment, the recognition rate of our method is increased from 90.8% to 93%. Overall, these results show a significant improvement in the recognition rate when multiple types of features are used for activity classification.</p><p>In Table <ref type="table">3</ref>, we compare the performance of our method with the state-of-the-art methods which used the 3D Action-Pairs dataset <ref type="bibr" target="#b30">[31]</ref>). The first method <ref type="bibr" target="#b0">[1]</ref> utilizes three types of features: the skeleton-Table <ref type="table">3</ref> Recognition accuracy comparison using 3D Action-Pairs dataset <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy  based pair-wise feature, Local Occupancy Pattern (LOP) feature, and a temporal pyramid feature. The other works are the motion map method (DMM-HOG) proposed by Yang et al. <ref type="bibr" target="#b3">[4]</ref> and HON4D method presented by Omar et al. <ref type="bibr" target="#b30">[31]</ref>. As Table <ref type="table">3</ref> shows, our method significantly outperforms <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b3">[4]</ref> but is comparable with the HON4D method <ref type="bibr" target="#b30">[31]</ref>. The HON4D approach <ref type="bibr" target="#b30">[31]</ref> uses only the surface normals (shape features) extracted from the depth map and requires to create a 4D projector in order to quantize the 4D space. Although, this process is complex, it is superior to other motion-based methods in describing activities with similar silhouette shapes. However, it may fail in describing shapes with different poses such as the actions in the MSR-Daily Activity dataset (see the discussion in 4.3).</p><formula xml:id="formula_16">(Skeleton þ LOP þ</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">MSR-Daily Activity 3D dataset</head><p>In another experiment, the MSR-Daily Activity 3D dataset <ref type="bibr" target="#b0">[1]</ref> was utilized to evaluate the performance of our method for recognizing different human activities. The MSR-Daily Activity 3D dataset contains 16 different human activities: drink, eat, read book, call cellphone, write on a paper, use laptop, vacuum cleaner use, cheer up, sit still, toss paper, play game, lay down on sofa, walk, play guitar, stand-up, sit-down, and each subject performs an activity in two different poses: a standing pose and a sitting on sofa pose. Each pose has 160 total samples, with each subject performing one sample per activity in each pose. A few samples of our MSR-Daily Activity 3D dataset are shown in Fig. <ref type="figure" target="#fig_0">10</ref>. This dataset is created to cover daily activities and human-object interactions in the living room. These tests are more challenging than the other datasets because of frequent human-object interactions. Furthermore, the segmentation of the dataset is not clean and the skeleton tracker is more noisy in the sitting pose.</p><p>We used both the standing and sitting poses in our experiments. Since the silhouette shape is different in the sitting pose, we restricted our activity recognition to motion features only. Shape features had to be avoided because of the differences in the silhouette shape between the sitting pose and the standing pose. In this context, the end points of distal limb segments were used to calculate the orientation and the translation distance of the distal limbs as motion features.</p><p>Since this dataset has one sample per subject, LOSO cross validation was applied in our experiments. The average recognition rate is 93.1%. From the confusion matrix in Fig. <ref type="figure" target="#fig_8">11</ref>, we can observe that there are confusions among some activities such as write on a paper, use laptop, and read book.</p><p>Table <ref type="table" target="#tab_3">4</ref> presents the accuracy of our proposed method and the state-of-art-methods which used the MSR-Daily Activity 3D dataset <ref type="bibr" target="#b0">[1]</ref>. These methods are the actionlet ensemble model <ref type="bibr" target="#b0">[1]</ref>, the restricted Graph-based Genetic Programming (RGGP) <ref type="bibr" target="#b45">[46]</ref>, the Fourier Temporal Pyramid method <ref type="bibr" target="#b0">[1]</ref>, and the HON4D method <ref type="bibr" target="#b30">[31]</ref>. It is clear that our proposed method significantly outperforms the stateof-art methods that rely on 3D joint positions. In fact, in MSR-Daily Activity dataset, each subject performs an activity in two different poses: a standing pose and a sitting on sofa pose. In particular, the method in <ref type="bibr" target="#b30">[31]</ref> utilizes only the surface normals of the silhouette shape. These shape features do not have the same characteristics between the two poses while motion-based features such as ours can better discriminate between the standing pose and sitting pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion</head><p>In order to evaluate the performance of using multiple types of features with a SimpleMKL based multiclass-SVM classifier for human activity recognition, three independent SVM classifiers are adopted in our experiments. Two classifiers C 1 and C 2 are used as C-SVM classifiers with a single kernel function and a group of kernel parameters to classify the activities using either shape features or motion features separately. Classifier C 1 is used for spherical harmonics coefficients as shape features and classifier C 2 is used for the motion features. The other classifier C 3 is built based on a SimpleMKL based multiclass-SVM classifier used for the fused features. Classifier C 3 is designed with two kernel functions (Gaussian and polynomial) and a group of kernel parameters. This classifier C 3 is used to create one kernel weight vector for activity classification.   In our work, we tuned 20 alternative kernel function parameters for Gaussian kernel functions and polynomial kernel functions. By fusing shape features and motion features, our proposed method can automatically learn the optimal kernel combination weights. This advantage comes from utilizing multiple types of features and different kernel functions, which enhances the performance of the classifier system in the kernel level.</p><p>Figs. 12, 13 and 8 illustrate the calculated confusion matrix for the three classifiers C 1 , C 2 and C 3 using MSR-Action 3D dataset <ref type="bibr" target="#b1">[2]</ref>. By comparing the results, we can observe that our proposed method using a SimpleMKL-based multiclass-SVM can generally boost the recognition rate of human activity recognition by fusing multiple types of features with different kernel functions and parameters as shown in Fig. <ref type="figure" target="#fig_4">6</ref>. Specifically, the recognition rate for most actions have been increased from 74.1% for classifier C 1 , and 83.7% for classifier C 2 to 90.7% using classifier C 3 which represents the fused features using the MKL technique.</p><p>By comparing the results in the three Figs. 12, 13 and 6, we can observe that the feature fusion technique at the kernel level enhances the recognition rate especially if there is a big difference in the accuracy between the two sets of features (Classifiers C 1 and C 2 ). In other words, there are some actions such as side boxing, side kick, tennis serve, golf serve, pick up and throw that are better described and recognized using kinematic structure than the shape features and vice versa. Hence, combining those two sets of features can enhance the classification performance of similar activities.</p><p>Figs. 14, 15 and 8 show the confusion matrix for each classifier C 1 , C 2 and C 3 with shape features, motion features and fused features respectively using the 3D Action-Pairs dataset <ref type="bibr" target="#b30">[31]</ref>. By comparing the results in the confusion matrices, we can observe that there is an improvement in the recognition rate by fusing two types of features with two kernel functions. Specifically, the recognition rate for most activities has been increased from 74.4% for classifier C 1 , and 88.8% for classifier C 2 to 90.8% for classifier C 3 which represents the fused features. In addition, we note that the MKL technique works well in improving the performance if the two sets of features have a big difference in the recognition rate for each activity. Otherwise, the enhancement will be small if the difference in the recognition rate is small between the two sets of features (Classifiers C 1 and C 2 ). Based on the comparison among the experimental results of the three classifiers, we observed that the SimpleMKL-based multiclass-SVM (MKL technique) enhances the classification performance and outperforms single kernel function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and future work</head><p>We presented two sets of features obtained from RGB-D data acquired using a Kinect sensor for human activity recognition. The features are (1) a novel 3D spatio-temporal feature obtained from the end points of the distal limb segments and (2) a novel shape feature using spherical harmonics coefficients extracted from the surface points. These features were fused via the SimpleMKL algorithm and multiclass-SVM using two kernel functions (Gaussian and polynomial functions) in order to enhance the classification performance for similar activities. Our experimental results on three challenging public datasets have shown the significance of the presented features in human activity recognition. In the future, we will exploit the efficiency of 3D data for view invariant action recognition and in dealing with more complex human object interactions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Sample of depth maps with 3D joint positions for Tennis Serve action.</figDesc><graphic coords="2,140.54,58.64,324.00,124.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. The 3D distal limb segments (four limbs) are used to extract the motion features.</figDesc><graphic coords="4,47.31,482.89,241.68,241.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example of the voxel surface and its spherical parametrization.</figDesc><graphic coords="5,77.01,58.64,431.52,188.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. A sequence of depth maps for Tennis Serve action.</figDesc><graphic coords="7,130.61,58.64,324.24,105.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The confusion matrix for MSR-Action 3D dataset (Classifier C 3 ).</figDesc><graphic coords="7,111.93,192.15,361.68,202.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. Example frames for four pairs from 3D Action Pairs dataset; each column shows two frames from a pair of activities. The activities from left to right: "Pick up a box/Put down a box", "Lift a box/Place a box", "Push a chair/Pull a chair", and "Wear a hat/Take off a hat".</figDesc><graphic coords="8,68.00,314.09,480.06,173.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. The accuracy of each subject for 3D Action-Pairs dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>908 Fig. 10 .</head><label>90810</label><figDesc>Fig. 10. A few sample frames of MSR-Daily Activity 3D dataset; the activities from left to right: drink, eat, read book, play game, use laptop, use vacuum cleaner, and play guitar.</figDesc><graphic coords="9,112.73,337.62,360.00,124.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. The confusion matrix for MSR-Daily Activity 3D dataset.</figDesc><graphic coords="9,112.01,505.43,361.44,227.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Confusion matrix of classifier C 2 using only the motion features with single kernel function (average recognition rate: 83.7%).</figDesc><graphic coords="11,81.98,58.64,421.68,198.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 .Fig. 15 .</head><label>1415</label><figDesc>Fig. 14. Confusion matrix of classifier C 1 using only the shape features with single kernel function (average recognition rate: 74.4%).</figDesc><graphic coords="11,122.66,299.23,340.32,189.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,86.54,515.80,432.00,216.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>The three subsets of actions used in comparison.</figDesc><table><row><cell>Action set 1 (AS1)</cell><cell>Action set 2 (AS2)</cell><cell>Action set 3 (AS3)</cell></row><row><cell>Horizontal wave</cell><cell>High wave</cell><cell>High throw</cell></row><row><cell>Hammer</cell><cell>Hand catch</cell><cell>Forward kick</cell></row><row><cell>Forward punch</cell><cell>Draw X</cell><cell>Side kick</cell></row><row><cell>High throw</cell><cell>Draw tick</cell><cell>Jogging</cell></row><row><cell>Hand clap</cell><cell>Draw circle</cell><cell>Tennis swing</cell></row><row><cell>Bend</cell><cell>Hands wave</cell><cell>Tennis serve</cell></row><row><cell>Tennis serve</cell><cell>Forward kick</cell><cell>Golf swing</cell></row><row><cell>Pickup throw</cell><cell>Side boxing</cell><cell>Pickup throw</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Recognition accuracy comparison using MSR-Action3-D dataset.</figDesc><table><row><cell>Subset</cell><cell cols="2">Xia et al. [3]</cell><cell>Li et al. [2]</cell><cell></cell><cell cols="2">Our method</cell></row><row><cell></cell><cell>Test-1</cell><cell>Test-2</cell><cell>Test-1</cell><cell>Test-2</cell><cell>Test-1</cell><cell>Test-2</cell></row><row><cell>AS1</cell><cell>0.986</cell><cell>0.879</cell><cell>0.934</cell><cell>0.729</cell><cell>0.932</cell><cell>0.743</cell></row><row><cell>AS2</cell><cell>0.979</cell><cell>0.854</cell><cell>0.926</cell><cell>0.719</cell><cell>0.945</cell><cell>0.768</cell></row><row><cell>AS3</cell><cell>0.949</cell><cell>0.634</cell><cell>0.963</cell><cell>0.792</cell><cell>0.956</cell><cell>0.867</cell></row><row><cell>Overall</cell><cell>0.971</cell><cell>0.789</cell><cell>0.941</cell><cell>0.747</cell><cell>0.944</cell><cell>0.797</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Recognition accuracy comparison using MSR-Daily Activity dataset.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell></row><row><cell>Restricted graph-based genetic</cell><cell>0.904</cell></row><row><cell>programming (RGGP) IJCAI 2013 (Liu et al. [46])</cell><cell></cell></row><row><cell>Actionlet Ensemble Model CVPR 2012 (Wang et al. [1])</cell><cell>0.857</cell></row><row><cell>HON4D CVPR 2103 (Omar et al. [31])</cell><cell>0.800</cell></row><row><cell>Our method</cell><cell>0.931</cell></row></table><note><p>Fig. 12. Confusion matrix of classifier C 1 using only the shape features with single kernel function (average recognition rate: 74.1%).</p></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest statement</head><p>None declared.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mining actionlet ensemble for action recognition with depth cameras</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1290" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
	<note>Action recognition based on a bag of 3d points</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2012-06">2012. June 2012</date>
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognizing actions using depth motion maps-based histograms of oriented gradients</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM international conference on Multimedia, MM &apos;12</title>
		<meeting>the 20th ACM international conference on Multimedia, MM &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1057" to="1060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Eigenjoints-based action recognition using Naive-Bayesnearest-neighbor</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2012-06">2012. June 2012</date>
			<biblScope unit="page" from="14" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Corporation</surname></persName>
		</author>
		<ptr target="〈http://www.microsoft.com/en-us/kinectforwindows/〉" />
		<title level="m">Kinect for windows</title>
		<imprint>
			<date type="published" when="2013-04">April 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recognizing human actions using multiple features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Action recognition via multi-feature fusion and gaussian process classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leckie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Action recognition by multiple features and hyper-sphere multi-class SVM</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th International Conference on Pattern Recognition (ICPR) 2010, IEEE</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3744" to="3747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust human action recognition scheme based on high-level feature fusion</title>
		<author>
			<persName><forename type="first">R</forename><surname>Benmokhtar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimed. Tools Appl</title>
		<imprint>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fusion of human posture features for continuous action recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision Workshop on Sign, Gesture, and Activity</title>
		<meeting>the European Conference on Computer Vision Workshop on Sign, Gesture, and Activity<address><addrLine>Crete, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multiple kernel learning algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gönen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alpaydın</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2211" to="2268" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A surf-based spatio-temporal feature for feature-fusionbased action recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Noguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV Workshop on Human Motion: Understanding Modeling Capture and Animation</title>
		<meeting>ECCV Workshop on Human Motion: Understanding Modeling Capture and Animation</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Object, scene and actions: combining multiple features for human action recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comput. Vis.-ECCV 2010</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="494" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fusing appearance and distribution information of interest points for action recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bregonzio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1220" to="1234" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><surname>Simplemkl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2491" to="2521" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth IEEE International Conference on Computer Vision, 2005. ICCV 2005</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1395" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inference of human postures by classification of 3d human body shape</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Analysis and Modeling of Faces and Gestures</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003. 2003. 2003</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A differential geometric approach to representing the human actions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="335" to="351" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human action recognition using star skeleton</title>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM International Workshop on Video Surveillance and Sensor Networks, VSSN &apos;06</title>
		<meeting>the 4th ACM International Workshop on Video Surveillance and Sensor Networks, VSSN &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="171" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time human motion analysis by image skeletonization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fujiyoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE WACV98</title>
		<meeting>IEEE WACV98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="15" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d star skeleton for fast human posture representation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Academy of Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="273" to="282" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fitting distal limb segments for accurate skeletonization in human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Althloothi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Voyles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Ambient Intell. Smart Environ</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="121" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">2d human skeleton model from monocular video for human activity recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Althloothi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Voyles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2010 International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical spatio-temporal context modeling for action recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-F</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009-06">2009. June 2009</date>
			<biblScope unit="page" from="2004" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical filtered motion for action recognition in crowded videos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. Part C: Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="313" to="323" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unstructured human activity detection from RGBD images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
			<biblScope unit="page" from="842" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">4-dimensional local spatio-temporal features for human activity recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Parker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2011-09">September 2011</date>
			<biblScope unit="page" from="2044" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Combing RGB and depth map features for human activity recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia-Pacific Signal Information Processing Association Annual Summit and Conference (APSIPA ASC)</title>
		<imprint>
			<date type="published" when="2012-12">2012. December 2012</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Redmond</surname></persName>
		</author>
		<title level="m">Hon4d: Histogram of oriented 4d normals for activity recognition from depth sequences</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition (CVPR)</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">View-invariant gesture recognition using 3d optical flow and harmonic motion context</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Holte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fihl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1353" to="1361" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling three-dimensional morphological structures using spherical harmonics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mcpeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolution</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1003" to="1016" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">An improvement of rotation invariant 3d-shape based on functions on concentric spheres</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Vranic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2003 International Conference on Image Processing</title>
		<meeting>2003 International Conference on Image Processing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">757</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d model retrieval with spherical harmonics and moments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Vranić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="392" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A robust method for rotation estimation using spherical harmonics representation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Althloothi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Voyles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2306" to="2316" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fourier method for large-scale surface modeling and registration</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Saykin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="311" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Parametrization of closed surfaces for 3-d shape description</title>
		<author>
			<persName><forename type="first">C</forename><surname>Brechbühler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gerig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kübler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="170" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spherical mapping for processing of 3d closed surfaces</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Makedon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Second order optimization of kernel parameters</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS Workshop on Kernel Learning: Automatic Selection of Optimal Kernels</title>
		<meeting>the NIPS Workshop on Kernel Learning: Automatic Selection of Optimal Kernels</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Luenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<title level="m">Linear and Nonlinear Programming</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">116</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Numerical Optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A comparison of methods for multiclass support vector machines</title>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="415" to="425" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recognizing human actions: a local SVM approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Pattern Recognition</title>
		<meeting>the 17th International Conference on Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004. 2004</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="32" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<title level="m">Proceedings of International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Learning discriminative representations from RGB-D video data</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Voyles</surname></persName>
		</author>
		<title level="m">Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on Facial expression recognition using HessianMKL based multiclass-SVM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Salah Althloothi received the B.S. degree in electrical and computer engineering from Tripoli University, Tripoli, Libya, in 1993, the M.S. degree in computer systems engineering from University Putra Malaysia</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<pubPlace>Malaysia; Denver, CO, USA</pubPlace>
		</imprint>
	</monogr>
	<note>His current research interests include image processing. pattern classification and human activity recognition</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">He joined the University of Denver (DU) as assistant professor of computer engineering in September 2008. He has authored or co-authored over 60 refereed research publications. He is the director of image processing and computer vision laboratory at DU. His research interests include affective computing and particularly developing automated systems for facial expression recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">S&apos;03-M&apos;07) received the B.S. degree in Electronics from Abadan Institute of Technology, Iran, in 1996, the M.S. degree in biomedical engineering from Sharif University of Technology, Iran, in 1998, and the Ph.D. in electrical and computer engineering from University of</title>
		<meeting><address><addrLine>Miami, Florida</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">He is currently a Ph.D. candidate and graduate research assistant in the Department of Electrical and Computer Engineering, University of Denver, Denver. His research interests include automatic analysis of facial expression and action units, pattern recognition, and machine learning</title>
	</analytic>
	<monogr>
		<title level="m">2008 and the M.S. degree from the Beijing Institute of Technology</title>
		<meeting><address><addrLine>Beijing, China; Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>Beijing Institute of Petrochemical Technology</orgName>
		</respStmt>
	</monogr>
	<note>Xiao Zhang received the B.S. degree from the</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">His current research interests include robotics and artificial intelligence. Specifically, he is interested in the development of small resourceconstrained robots and robot teams for urban search and rescue and surveillance. He has additional expertise in sensors and sensor calibration, particularly haptic and force sensors, and real-time control. His industrial experience includes Dart Controls</title>
	</analytic>
	<monogr>
		<title level="m">1989, and the Ph.D. degree in robotics from the School of Computer Science</title>
		<meeting><address><addrLine>West Lafayette, IN, USA; Stanford, CA, USA; Pittsburgh, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IBM Corp., Integrated Systems, Inc., and Avanti Optics</publisher>
			<date type="published" when="1983">1983. 1997</date>
		</imprint>
		<respStmt>
			<orgName>Purdue University ; Department of Mechanical Engineering, Stanford University ; Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note>the M.S. degree in manufacturing systems engineering from the</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
