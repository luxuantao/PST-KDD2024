<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Designing Random Graph Models Using Variational Autoencoders With Applications to Chemical Design</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-05-23">23 May 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bidisha</forename><surname>Samanta</surname></persName>
							<email>bidisha@iitkgp.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">IIT Kharagpur</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abir</forename><surname>De</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Niloy</forename><surname>Ganguly</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IIT Kharagpur</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manuel</forename><surname>Gomez-Rodriguez</surname></persName>
							<email>manuelgr@mpi-sws.org</email>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">W2</forename><surname>W2</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Software Systems</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Designing Random Graph Models Using Variational Autoencoders With Applications to Chemical Design</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-05-23">23 May 2018</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1802.05283v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep generative models have been praised for their ability to learn smooth latent representation of images, text, and audio, which can then be used to generate new, plausible data. However, current generative models are unable to work with graphs due to their unique characteristics-their underlying structure is not Euclidean or grid-like, they remain isomorphic under permutation of the nodes labels, and they come with a different number of nodes and edges. In this paper, we propose NeVAE, a novel variational autoencoder for graphs, whose encoder and decoder are specially designed to account for the above properties by means of several technical innovations. In addition, by using masking, the decoder is able to guarantee a set of local structural and functional properties in the generated graphs. Experiments reveal that our model is able to learn and mimic the generative process of several well-known random graph models and can be used to discover new molecules more effectively than several state of the art methods. Moreover, by utilizing Bayesian optimization over the continuous latent representation of molecules our model finds, we can also identify molecules that maximize certain desirable properties more effectively than alternatives.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphs emerge as a fundamental data structure in a wide range of biological and chemical networked systems. In each of these systems, the nodes and edges of the corresponding graphs have different, distinctive meanings-they represent different types of entities and relationships. For example, in protein interaction networks, nodes and edges represent proteins and physical interactions between proteins. In chemical networks, nodes are atoms and there is an edge between two nodes if there is a chemical bond between the corresponding atoms. In all these cases, the underlying generative processes that determine the absence or presence of nodes and edges are highly complex, domain dependent, and typically stochastic.</p><p>In this context, there have been a surge of interest in developing deep generative models for graphs <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>, partly fueled by the success of generative adversarial networks (GANs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref> and variational autoencoders (VAEs) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b30">31]</ref> in image, audio and text generation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b28">29]</ref>. Such models take a fundamental departure from traditional random graph models, such as Barab?si-Albert <ref type="bibr" target="#b3">[4]</ref>, stochastic block models <ref type="bibr" target="#b17">[18]</ref>, Kronecker graph models <ref type="bibr" target="#b24">[25]</ref> or exponential random graphs (ERGM) <ref type="bibr" target="#b12">[13]</ref>, which make strong (parametric) assumptions about the underlying generative process of the graphs they model and, as a consequence, have a limited expressive power. However, current deep generative models for graphs typically share one or more of the following limitations, which preclude them from realizing all their potential: (i) they can only generate (and be trained on) graphs with the same number of nodes, while in practice, graphs often come with a different number of nodes and edges; (ii) they are not invariant to permutations of the node labels, however, graphs remain isomorphic under permutation of their node labels; (iii) their training procedure suffers from a quadratic complexity with respect to the number of nodes in the graph; (iv) they are validated on downstream machine learning tasks such as node classification or link prediction, similarly as in network representation learning <ref type="bibr" target="#b11">[12]</ref>, rather than by their ability to generate new, plausible data, as in deep generative models of images, audio and text.</p><p>In this paper, we develop NeVAE, a deep generative model for (undirected) graphs based on variational autoencoders that overcomes the above limitations. To do so, it relies on several technical innovations, which distinguish us from previous work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>:</p><p>I Our probabilistic encoder learns to aggregate information (e.g., node and edge features) from a different number of hops away from a given node and then map this aggregate information into a continuous latent space, as in inductive graph representation learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>. However, in contrast with inductive graph representation learning, the aggregator functions are learned via variational inference so that the resulting aggregator functions are especially well suited to enable the probabilistic decoder to generate new, plausible graphs rather than other downstream machine learning tasks. Moreover, by using (symmetric) aggregator functions, it can encode graphs with a variable number of nodes and is invariant to permutations of the node labels, as opposed to most of the existing graph generative models, with the notable exception of those based on GCNs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref>. II Our probabilistic decoder jointly represents all edges as an unnormalized log probability vector (or 'logit'), which then feeds a single multinomial edge distribution. Such scheme allows for an efficient inference algorithm with O(l) complexity, where l is the number of (true) edges in the graph. In contrast, previous work typically models the presence and absence of each potential edge using a Bernoulli distribution and this leads to inference algorithms with O(n 2 ) complexity, where n is the number of nodes in the graph. Note that, since most real-world graphs are sparse, i.e., l n 2 , the complexity of our inference algorithm compares favorably to alternatives. III Our probabilistic decoder is able to guarantee a set of local structural and functional properties in the generated graphs by using a mask in the edge distribution definition, which can prevent the generation of certain undesirable edges during the decoding process. While masking have been increasingly used to account for prior (expert) knowledge in deep learning models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref>, their use in deep generative models for graphs has been lacking. We evaluate our model using both synthetic and real-world data. In a first set of experiments, we show that our model is able to learn and mimic the underlying processes that determine the absence or presence of nodes and edges in two well-known random graph models, Barab?si-Albert <ref type="bibr" target="#b3">[4]</ref> and Kronecker graphs <ref type="bibr" target="#b24">[25]</ref>, and the continuous latent representations that our model finds can be used to smoothly interpolate between model parameter values. In a second set of experiments, we use our model for molecular design, a high impact application of increasing popularity in the machine learning community <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>. More specifically, we train our variational autoencoder using molecules from two publicly available datasets, ZINC <ref type="bibr" target="#b14">[15]</ref> and QM9 <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b29">30]</ref>, and show that the trained autoencoders are able to find more valid and novel molecules than the state of the art. Moreover, the resulting latent space representations of molecules exhibit powerful semantics-we can smoothly interpolate between molecules-and, by utilizing Bayesian optimization over this latent representation, we can also identify molecules that maximize certain desirable properties more effectively than alternatives. We are releasing an open source implementation of our model in Tensorflow as well as synthetic and real-world data used in our experiments <ref type="foot" target="#foot_0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background on Variational Autoencoders</head><p>Variational autoencoders are characterized by a probabilistic generative model p ? (x|z) of the observed variables x ? R N given the latent variables z ? R M , a prior distribution over the latent variables p(z) and an approximate probabilistic inference model q ? (z|x). In this characterization, p ? and q ? are arbitrary distributions parametrized by two (deep) neural networks ? and ? and one can think of the generative model as a probabilistic decoder, which decodes latent variables into observed variables, and the inference model as a probabilistic encoder, which encodes observed variables into latent variables. Ideally, if we use the maximum likelihood principle to train a variational autoencoder, we should optimize the marginal log-likelihood of the observed data, i.e., E D [log p ? (x)], where p D is the data distribution. Unfortunately, computing log p ? (x) requires marginalization with respect to the latent variable z, which is typically intractable. Therefore, one resorts to maximizing a variational lower bound or evidence lower bound (ELBO) of the log-likelihood of the observed data, i.e.,</p><formula xml:id="formula_0">max ? max ? E D -KL(q ? (z|x)||p(z)) + E q ? (z|x) log p ? (x|z) .</formula><p>Finally, note that the quality of this variational lower bound (and thus the quality of the resulting VAE) depends on the expressive ability of the approximate inference model q ? (z|x), which is typically assumed to be a normal distribution whose mean and variance are parametrized by a (deep) neural network ? with the observed data x as an input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NeVAE: A Variational Autoencoder for Graphs</head><p>In this section, we first give a high-level overview of the design of our variational autoencoder for graphs, starting from the data it is designed for. Then, we elaborate on the key technical aspects of its individual components. Finally, we conclude by further elaborating on the training procedure, scalability and implementation details.</p><p>High-level overview. We observe a collection of N graphs</p><formula xml:id="formula_1">{G i = (V i , E i )} i?[N ]</formula><p>, where V i and E i denote the corresponding set of vertices and edges, respectively, and this collection may contain graphs with a different number of nodes and edges. Moreover, for each graph G = (V, E), we also observe a set of node features F = {f u } u?V and edge weights<ref type="foot" target="#foot_1">2</ref> Y = {y uv } (u,v)?E . Our goal is then to design a variational autoencoder for graphs that, once trained on this collection of graphs, has the ability of creating plausible new graphs, including node features and edge weights. In doing so, it will also provide a latent representation of any graph in the collection (or elsewhere) with (hopefully) meaningful semantics.</p><p>Following the above background on variational autoencoders, we can characterize our variational autoencoder by means of:</p><formula xml:id="formula_2">-Prior : p(z 1 , . . . , z n ), where |V| = |F| = n ? Poisson(? n ) -Inference model (encoder): q ? (z 1 , . . . , z n |V, E, F, Y) -Generative model (decoder): p ? (E, F, Y|z 1 , . . . , z n ), where |E| = |Y| = l ? Poisson(? l )</formula><p>In the above characterization, note that: (i) we define one latent variable per node, i.e., we have a node-based latent representation; and (ii) the number of nodes and edges are random variables and, as a consequence, both the latent representation as well as the graph can vary in size. Next, we formally define the functional form of the inference model, the generative model, and the prior.</p><p>Inference model (probabilistic encoder). Given a graph G = (V, E) with node features F and edge weights Y, our inference model q ? defines a probabilistic encoding for each node in the graph by aggregating information from different distances. More formally, for each node u, the inference model is defined as follows:</p><formula xml:id="formula_3">q ? (z u |V, E, F, Y) ? N (? u , diag(? u ))<label>(1)</label></formula><p>where z u is the latent variable associated to node u, [? u , diag(? u )] = ? enc (c u (1), . . . , c u (K)), and c u (k) aggregates information from k hops away from u, i.e.,</p><formula xml:id="formula_4">c u (k) = r(W k f u ) if k = 1 r W k f u ? ? v?N (u) y uv g(c v (k -1) if k &gt; 1.<label>(2)</label></formula><p>In the above, W k are trainable weight matrices, which propagate information between different search depths, ?(.) is a (possibly nonlinear) symmetric aggregator function in its arguments, g(?) and r(?) are (possibly nonlinear) differentiable functions, ? enc is a neural network, and denotes pairwise product. Fig. <ref type="figure" target="#fig_0">1</ref> summarizes our encoder architecture.</p><p>The above node embeddings, defined by Eq. 2, are very similar to the ones used in several graph representation learning algorithms such as GraphSAGE <ref type="bibr" target="#b10">[11]</ref>, column networks <ref type="bibr" target="#b27">[28]</ref>, and GCNs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, the main difference with our work is the way we will train the weight matrices W k . Here, we will use variational inference so that the resulting embeddings are especially well suited to enable our probabilistic decoder to generate new, plausible graphs while the above algorithms use non variational approaches to compute general purpose embeddings to feed downstream machine learning tasks. Moreover, the following proposition highlights several desirable theoretical properties of our probabilistic encoder (proven in Appendix A), which distinguishes our design from most existing generative models of graphs <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref>: Proposition 1 The probabilistic encoder defined by Eqs. 1 and 2 has the following properties:</p><p>(i) For each node u, its corresponding embedding c u (k) is invariant to permutations of the node labels of its neighbors. (ii) The weight matrices W 1 , . . . , W k do not depend on the number of nodes and edges in the graph and thus a single encoder allows for graphs with a variable number of nodes and edges.</p><p>Generative model (probabilistic decoder). Given a set of of n nodes with latent variables Z = {z u } u?[n] , our generative model p ? is defined as follows:</p><formula xml:id="formula_5">p ? (E, Y, F|Z) = u?V p ? (f u |Z) ? ? k?[l] p ? (e k |E k-1 , F, Z)p ? (y u k v k |Y k-1 , F, Z) ? ? ,<label>(3)</label></formula><p>where the ordering for the edge and edge weights is arbitrary, e k and y u k v k denote the k-th edge and edge weight under this arbitrary ordering,</p><formula xml:id="formula_6">E k-1 = {e 1 , . . . , e k-1 } and Y k-1 = {y u1v1 , . . . , y u k-1 v k-1 } denote the k -1</formula><p>previously generated edges and edge weights respectively. Moreover, the model characterizes the conditional probabilities on the right hand side of the above equation as follows. For each node, it represents all potential node feature values as an unnormalized log probability vector (or 'logits'), feed this logit into a softmax distribution and sample the node features. Then, it represents all potential edges as a logit and, for each edge, all potential edge weights as another logit, and it feeds the former vector into a single softmax distribution and the latter vectors each into a different softmax distribution. The edge distribution and the corresponding edge weight distributions depend on a set of binary masks, which may depend on the sampled node features and also get updated every time a new ) and it samples a latent vector z u per node u ? V from N (0, I). Then, for each node u, it represents all potential node feature values as an unnormalized log probability vector (or 'logits'), where each entry is given by a nonlinearity ? dec ? of the corresponding latent representation z u , feeds this logit into a softmax distribution and samples the node features. Thereafter, on the top row, it constructs a logit for all potential edges (u, v), where each entry is given by a nonlinearity ? dec ? of the corresponding latent representations (z u , z v ). Then, it samples the edges one by one from a soft max distribution depending on the logit and a mask x e (E k-1 ), which gets updated every time it samples a new edge e k . On the bottom row, it constructs a logit per edge (u, v) for all potential edge weight values m, where each entry is given by a nonlinearity ? dec ? of the latent representations of the edge and edge weight value (z u , z v , m). Then, every time it samples an edge, it samples the edge weight value from a soft max distribution depending on the corresponding logit and mask x m (u, v), which gets updated every time it samples a new y u k v k . edge and edge weight are sampled. By doing so, it prevents the generation of certain undesirable edges and edges weights, allowing for the generated graph to fulfill a set of predefined local structural and functional properties. For example, in molecule design, masking facilitates the generation of molecules with a valid structure, as shown in Section 5. More formally, the distributions of each node, edge and edge weight are given by:</p><formula xml:id="formula_7">p ? (f u = q|Z) = exp(? dec ? (z u , q)) q exp(? dec ? (z u , q )<label>(4)</label></formula><formula xml:id="formula_8">p ? (e = (u, v)|E k-1 , Z) = x e exp(? dec ? (z u , z v )) e =(u ,v ) / ?E k-1 x e exp(? dec ? (z u , z v ))<label>(5)</label></formula><formula xml:id="formula_9">p ? (y uv = m|Y k-1 , Z) = x m (u, v) exp(? dec ? (z u , z v , m)) m =m x m (u, v) exp(? dec ? (z u , z v , m )) ,<label>(6)</label></formula><p>where x e is the binary mask for edge e and x m (u, v) is the binary mask for feature edge value m, and ? dec ? , ? dec ? and ? dec ? are neural networks. Note that the parameters of the neural networks do not depend on the number of nodes or edges in the network and the dependency of the binary masks x e and x m (u, v) on the node features and the previously generated edges E k-1 and edge weights Y k-1 is deterministic and domain dependent. Fig. <ref type="figure" target="#fig_1">2</ref> summarizes our decoder architecture.</p><p>Note that, by using a softmax distribution, it is only necessary to account for the presence of an edge, not its absence, and this, in combination with negative sampling, will allow for efficient training and decoding, as it will become clear later in this section. This is in contrast with previous generative models for graphs <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33]</ref>, which need to model both the presence and absence of each potential edge. Moreover, we would like to acknowledge that, while masking may be useful to account for prior (expert) knowledge, it may be costly to check for some local (or global) structural and functional properties on-the-fly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prior. Given a set of n nodes with latent variables</head><formula xml:id="formula_10">Z = {z u } u?[n] , p z (Z) ? N (0, I).</formula><p>Training, scalability and implementation details. Given a collection of N graphs</p><formula xml:id="formula_11">{G i = (V i , E i )} i?[N ] ,</formula><p>each with n i nodes, m i edges, a set of node features F i and set of edge weights Y i , we train our variational autoencoder for graphs by maximizing the evidence lower bound (ELBO), as described in Section 2, plus the log-likelihood of two Poisson distributions p ?n and p ?m modeling the number of nodes and edges in each graph, i.e.,</p><formula xml:id="formula_12">1 N i?[N ] -KL(q ? ||p z ) + E q ? (Zi|Vi,Ei,Fi,Yi) log p ? (E i , Y i , F i |Z i ) + log p ?n (n i ) + log p ?m (m i ). (<label>7</label></formula><formula xml:id="formula_13">)</formula><p>The following proposition points out the key property of our objective function (proven in Appendix B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 2</head><p>The parameters learned by maximizing the objective in Eq. 7 are invariant to the permutations of the node labels.</p><p>In terms of scalability, the major bottleneck is computing the gradient of the second term in the above equation during training, rather than encoding and decoding graphs once the model is trained. More specifically, an exact computation of the per edge partition function of the log-likelihood of the edges, i.e.,</p><formula xml:id="formula_14">e =(u ,v ) / ?E k-1 x e exp(? dec ? (z u , z v )), requires O(|V| 2</formula><p>) computations, similarly as in most inference algorithms for existing generative models of graphs, and hence is costly to compute for medium and large networks. Fortunately, in practice, we can approximate such partition function using negative sampling <ref type="bibr" target="#b26">[27]</ref> and this reduces the complexity to O(l), where l = |E| is the number of (true) edges in the graph. Here, note that most real-world graphs are sparse and thus l = |E| |V| 2 . In terms of implementation, we used Tensorflow <ref type="bibr" target="#b0">[1]</ref> and Adam <ref type="bibr" target="#b18">[19]</ref> for parameter tuning. Appendix C provides additional details for our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments on Synthetic Graphs</head><p>In this section, we first demonstrate that our model is able to generate graphs with a predefined local topological property, i.e., graphs without triangles. Then, we show that our model is able to learn smooth latent representations of a popular type of random graphs, Kronecker graphs <ref type="bibr" target="#b24">[25]</ref>. Appendix D contains additional quantitative results on the ability of our model to learn and mimic the generative processes of Kronecker graphs and Barab?si-Albert graphs <ref type="bibr" target="#b3">[4]</ref> and a scalability analysis.</p><p>Experimental setup. We first generate two sets of synthetic networks, each containing 100 graphs, with up to n = 1000 number of nodes. The first set contains triangle free graphs and the second set contains a 50%-50% mixture of Kronecker graphs with initiator matrices: ? 1 = [0.9, 0.6; 0.3, 0.2], and ? 2 = [0.6, 0.6; 0.6, 0.6]. For each dataset, we train our variational autoencoder for graphs by maximizing the corresponding evidence lower bound (ELBO). Then, we use the trained models to generate three sets of 1000 graphs by sampling from the decoders, i.e., G ? p ? (G|Z), where Z ? p(Z).</p><p>Graphs with a predefined local topological property. We evaluate the ability of our model to generate triangle free graphs by measuring the validity of the generated graphs, i.e.</p><formula xml:id="formula_15">Validity := |{G i ? G | G i has no triangles}|/|G|,</formula><p>where G is the set of 1000 graphs generated using the trained model. We experiment both with and without masking during training and during test time. We observe that, if we train and test our model with masking, it achieves a validity of 100%, i.e., it always generates triangle free graphs. If we only use masking during training, our model is able to achieve a validity of 68%, and, if we do not use masking at all, our models achieves a validity of 57%. Moreover, while using masking during test does not lead to significant increase in the time it takes to generate a graph, using masking during training does lead to an increase of 18% in training time. Fig. <ref type="figure">6</ref> in Appendix D shows several example of triangle free graphs generated by our model.  Generalization ability. We evaluate the ability of our model to learn smooth latent representations of Kronecker graphs as follows. First, we select two graphs (G 0 and G 1 ) from the training set, one generated using an initiator matrix ? 0 = [0.9, 0.6; 0.5, 0.1] and the other using ? 1 = [0.6, 0.6; 0.6, 0.6]. Then, we sample the latent representations Z 0 and Z 1 for G 0 and G 1 , respectively, and sample new graphs from latent values Z in between these latent representations (using a linear interpolation), i.e., G ? p ? (G|Z), where Z = aZ 0 + (1a)Z 1 and a ? [0, 1], and the node labels, which define the matching between pairs of nodes in both graphs, are arbitrary. Fig. <ref type="figure" target="#fig_3">3</ref> provides an example, which shows that, remarkably, as Z moves towards Z 0 (Z 1 ), the sampled graph becomes similar to that of G 0 (G 1 ) and the inferred initiator matrices along the way smoothly interpolate between both initiator matrix. Here, we infer the initiator matrices of the graphs generated by our trained decoder using the method by Leskovec et al. <ref type="bibr" target="#b24">[25]</ref>. Table <ref type="table" target="#tab_3">4</ref> in Appendix D provides a quantitative evaluation of the quality of the generated graphs, i.e., it shows that the graphs our model generates are indistinguishable from true Kronecker graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments on Molecule Design</head><p>In this section, we utilize our variational autoencoder for molecular design. First, we show that it can generate a higher percentage of valid and novel molecules than several state of the art machine learning models for molecule design <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26]</ref>. Then, by applying Bayesian optimization over the latent space of molecules provided by our encoder, we also show that our model can find molecules that maximize certain desirable properties. Appendix E contains additional experiments showing that the continuous latent representations of molecules that our model finds are smooth.</p><p>Experimental setup. We sample ?10,000 drug-like commercially available molecules from the ZINC dataset <ref type="bibr" target="#b14">[15]</ref> with E[n] = 44 atoms and ?10,000 molecules from the QM9 dataset <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref> with E[n] = 21 atoms. For each molecule, we construct a molecular graph, which is nothing but a weighted undirected graph, where nodes are the atoms, the node features are the type of the atoms i.e. f u ?{C, H, N, O}<ref type="foot" target="#foot_2">3</ref> , edges are the bonds between two atoms, and the weight associated to an edge is the type of bonds (single, double or triple) <ref type="foot" target="#foot_3">4</ref> . Then, for each dataset, we train our variational autoencoder for graphs using batches comprised of molecules with the same number of nodes <ref type="foot" target="#foot_4">5</ref> . Finally, we sample ?5,000 molecular graphs from each of the (two) trained variational autoencoders using: (i) G ? p ? (G|Z), where Z ? p(Z) and (ii) G ? p ? (Z|G = G T ), where G T is a molecular graph from the corresponding (training) dataset. In the above procedure, we only use masking on  <ref type="table">1</ref>: Quality of the molecules generated using our variational autoencoder for graphs, GraphVAE <ref type="bibr" target="#b32">[33]</ref>, GrammarVAE <ref type="bibr" target="#b22">[23]</ref>, CVAE <ref type="bibr" target="#b7">[8]</ref> and SDVAE <ref type="bibr" target="#b6">[7]</ref>. The sign * indicates no masking.</p><p>the weight (i.e., type of bond) distributions both during training and sampling to ensure that the valence of the nodes at both ends are valid at all times, i.e.,</p><formula xml:id="formula_16">x m (u, v) = I(m+n k (u) ? m max (u)?m+n k (v) ? m max (v)),</formula><p>where n k (u) is the current valence of node u and m max (u) is the maximum valence of node u, which depends on its type f u . Moreover, during sampling, if there is no valid weight value for a sampled edge, we reject it.</p><p>To assess to which extent masking help, we also train and sample from our model without masking. Here, we would like to highlight that, while using masking during test does not lead to significant increase in the time it takes to generate a graph, using masking during training does lead to an increase of 5% in training time.</p><p>We compare the quality of the molecules generated by our trained models and the molecules generated by several state of the art competing methods: (i) GraphVAE <ref type="bibr" target="#b32">[33]</ref>, (ii) GrammarVAE <ref type="bibr" target="#b22">[23]</ref>, (iii) CVAE <ref type="bibr" target="#b7">[8]</ref>, and (iv) SDVAE <ref type="bibr" target="#b6">[7]</ref> Among them, GraphVAE uses molecular graphs, however, the rests use chemical SMILES, a domain specific textual representation of molecules. To that aim, we use the following evaluation metrics: (ii) Novelty: we use this metric to evaluate to which degree a method generates novel molecules, i. (i) Validity: we use this metric to evaluate to which degree a method generates chemically valid molecules. <ref type="foot" target="#foot_5">6</ref>That is, Validity = |C s |/n s where n s is the number of generated molecules, C s is the set of generated molecules which are chemically valid, and note that Validity ? [0, 1].</p><p>Quality of the generated molecules. Table <ref type="table">1</ref> compares our trained models to the state of the art methods above in terms of novelty and validity. For GraphVAE and SDVAE, we just report the results reported in the papers since there is no public domain implementation of these methods at the time of writing. For CVAE and GrammarVAE, we run their public domain implementations in the same set of molecules that we used. In terms of novelty, both our trained models and all competing methods except for the GraphVAE, which assumes a fixed number of nodes, are able to (almost) always generate novel molecules. However, we would also like to note that novelty is only defined over chemically valid molecules. Therefore, despite having (almost) perfect novelty scores, both GrammarVAE and CVAE generate fewer novel molecules than our method. In terms of validity, our trained models significantly outperform all competing methods, even without the use of masking. In contrast to our model, GrammarVAE, CVAE and SDVAE use SMILES, a domain specific string based representation, and thus they may be constrained by its limited expressiveness. Among them, GrammarVAE and SDVAE achieve better performance by using a grammar to favor valid molecules. GraphVAE generates molecular graphs, as our model, however, its performance is inferior to our method because it assumes a fixed number of nodes, it samples edges independently from a Bernoulli distribution, and is not permutation invariant.</p><p>Bayesian optimization. Here, we leverage our model to discover novel molecules with desirable properties. Similarly as in previous work <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref>  with a high value of the octanol-water partition coefficient (logP) y(m), penalized by synthetic accessibility (SA) score and number of long cycles. More specifically, we first sample 3,000 molecules from our ZINC dataset, which we split into training (90%) and test (10%) sets. Then, for our model and each competing model with public domain implementations, we train a sparse Gaussian process (SGP) <ref type="bibr" target="#b33">[34]</ref> with the latent representations and y(m) values of 500 inducing points sampled from the training set. The SGPs allow us to make predictions for the property values of new molecules in the latent spaces. Then, we run 5 iterations of batch Bayesian optimization (BO) using the expected improvement (EI) heuristic <ref type="bibr" target="#b16">[17]</ref>, with 50 (new) latent vectors (molecules) per iteration. Here, we compare the performance of all models using several quality measures: (a) the predictive performance of the trained SGPs in terms of log-likelihood (LL) and root mean square error (RMSE) on the test set and (b) the average value E [y(m)], fraction of valid molecules and fraction of good molecules, i.e., y(m) &gt; 0, among the molecules found using EI. Table <ref type="table" target="#tab_1">2</ref> and Fig. <ref type="figure" target="#fig_5">4</ref> summarize the results. In terms of predictive performance (log-Likelihood and RMSE), the SGP trained using the latent representations provided by our model outperforms all the alternatives. In terms of average property values E [y(m)] of the discovered molecules and fraction of valid and good molecules, our model outperforms both alternative methods. However, GrammarVAE does find a molecule with a larger value than our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we have introduced a variational autoencoder for graphs, whose encoder and decoder are specially designed to account for the non Euclidean structure of graphs, be invariant to the permutation of the nodes labels of the graphs they are trained with, and allow for graphs with different number of nodes and edges. Moreover, by using masking, the decoder is able to guarantee a set of local structural and functional properties in the generated graphs. Finally, we have shown that our variational autoencoder can learn and mimic the generative process of well-known random graph models and can also be used to discover valid and diverse molecules with certain desirable properties more effectively than several state of the art methods.</p><p>Our work also opens many interesting venues for future work. For example, in the design of our variational autoencoder, we have assumed graphs to be static, however, it would be interesting to augment our design to dynamic graphs by, e.g., incorporating a recurrent neural network. In our design, each node has a latent representation, however, it would be interesting to consider latent representations for multiple nodes and edges, e.g., graphlets, which could improve the scalability further. Finally, we have performed experiments on a single real-world application, e.g., automatic chemical design, however, it would be very interesting to use our autoencoder in other applications <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Proposition 1</head><p>Proof of (i). Consider ?, a permutation of the node labels, i.e. for each u, we have ?(u) ? V; and the set of all shuffled labels {?(w)|w ? V} = V. Let us denote u := ?(u). Now we need to prove</p><formula xml:id="formula_17">c u (k) = c u (k) ?k ? 1, ?u ? V<label>(8)</label></formula><p>We proof this by induction. Since the feature f u is independent of node label of u, we have f u = f u which proves Eq. 8 for k = 1, ?u ? V. Now assume that Eq. 8 is true for k ? k -1, with k &gt; 1. That is, we have,</p><formula xml:id="formula_18">c v (k -1) = c v (k -1) ?v ? V<label>(9)</label></formula><p>Also, since the edge-weight y uv between nodes does not depend on their labels, we have</p><formula xml:id="formula_19">y uv = y u, v .<label>(10)</label></formula><p>This, along with Eq. 9 gives {? v?N (u) y uv g(c v (k -1)} = {? v?N ( u) y u v g(c v (k -1)} which, due to the symmetric property of ?(.), implies</p><formula xml:id="formula_20">? ? v?N (u) y uv g(c v (k -1) = ? ? v?N ( u) y u v g(c v (k -1)<label>(11)</label></formula><p>The above equation, together with the fact that f u = f u proves Eq. 8 for k = k .</p><p>Proof of (ii). We re-write that,</p><formula xml:id="formula_21">c u (k) = r(W k f u ) if k = 1 r W k f u ? ? v?N (u) y uv g(c v (k -1) if k &gt; 1. (<label>12</label></formula><formula xml:id="formula_22">)</formula><p>where all the functions r(.), g(.) and ?(.) are defined term-wise. Note that, to make sure that f u ? ? v?N (u) y uv g(c v (k -1) is well defined, we need to have c v (k -1) ? R D?1 for all k &gt; 1 and v ? V. Then, by matching the dimension of vectors in both sides of Eq. 12, we have</p><formula xml:id="formula_23">W k ? R D?D .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Proposition 2</head><p>We define the component of the objective function for a training graph</p><formula xml:id="formula_24">G i = (V i , E i ) as L Gi (?) = -KL(q ? ||p z ) + E q ? (Zi|Vi,Ei,Fi,Yi) log p ? (E i , Y i , F i |Z i ) + log p ?n (n i ) + log p ?m (m i ),</formula><p>where ? is the set of trainable parameters. So the actual objective function is</p><formula xml:id="formula_25">L(?) = 1 N N i=1 L Gi (?).</formula><p>In order to prove that the parameters ? estimated by maximizing L(?) are invariant to the permutations of the labels of all V i 's, it is enough to prove that L Gi (?) is invariant to the permutation of V i for any i ? [N ], and for any ?. To do so, we note that log p ?n (n i ) + log p ?m (m i ) depends on the total number of nodes and edges, and therefore is node permutation invariant. Therefore, it is enough to prove the permutation invariance property of the first two components, i.e. KL(q ? ||p z ) and</p><formula xml:id="formula_26">E q ? (Zi|Vi,Ei,Fi,Yi) log p ? (E i , Y i , F i |Z i ).</formula><p>Since q ? and p z are both normal distribution,we have:</p><formula xml:id="formula_27">KL(q ? ||p z ) = 1 2 tr(? -1 p ? q ) + (? p -? q ) T ? -1 p (? p -? q ) -kn i + log det ? p det ? q<label>(13)</label></formula><p>which, in our case, reduces to:</p><formula xml:id="formula_28">1 2 u?Vi (1 T ? 2 u + 1 T ? 2 u ) -kn i -1 T log ? 2 u .</formula><p>Note that, from Proposition 1, we know that the values of c u (k) are invariant to the permutation of node labels. Now, since [? u , diag(? u )] = ? enc (c u (1), . . . , c u (K)), KL(q ? ||p z ) is also invariant to the permutation of node labels. Now, to prove that E q ? (Zi|Vi,Ei,Fi,Yi) log p ? (E i , Y i , F i |Z i ), we rely on a reparameterization trick for the normal distribution.</p><formula xml:id="formula_29">E q ? (Zi|Vi,Ei,Fi,Yi) log p ? (E i , Y i , F i |Z i ) = E u?V i ?N (0,I) log p ? E i , Y i , F i |(? u + diag(? u ) u ) u?Vi<label>(14)</label></formula><p>Note that, u does not depend on u since it is sampled from N (0, I). This, along with the permutation invariance property of ? u and diag(? u ), prove the proposition.</p><p>Generative Process 1: Training with Minibatches</p><formula xml:id="formula_30">1: Input: Training graphs {Gi(Vi, Ei) i?[N ] }, hyperparameters ? = {D, K, L, lr}, ? 2: Output: Inferred parameters ?. 3: ? ? Initialize(?) 4: B ? CreateBatches({Gi(Vi, Ei) i?[N ] }) 5: for B k ? B do 6:</formula><p>NeVAE ? ? BuildComputationalGraph(Nodes(B k ), ?)</p><p>? ? Train(NeVAE ?, B k )</p><p>8: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Implementation Details</head><p>Architecture details. Table <ref type="table" target="#tab_2">3</ref> provides additional details on the architecture of our variational autoencoder for graphs, where it is important to notice that the parameters to be learned do not depend on the size of the graphs (i.e., the number of nodes and edges). Note that, r and g are linear forms and the aggregator function ? is a sum, which is a symmetric function, for simplicity Hyperparameter tuning. At the very outset, to train NeVAE, we implemented stochastic gradient descent (SGD) using the Adam optimizer. Therein, we had to specify four hyperparameters: (i) D -the dimension of z u , (ii) K -the maximum number of hops used in encoder to aggregate information, (iii) Lthe number of negative samples, (iv) l r -the learning rate. Note that, all the parameters W ? 's and b ? 's in the input, hidden and output layers depend on D and K. We selected these hyperparameters using cross validation. More specifically, we varied l r in a logarithmic scale, i.e., {0.0005, 0.005, 0.05, 0.5}, and the rest of the hyperparameters in an arithmetic scale, and chose the hyperparameters maximizing the value of the objective function in the validation set. For synthetic (real) data, the resulting hyperparameter values were D = 7(5), K = 3(5), L = 10(10) and l r = 0.005(0.005). To run the baseline algorithms, we followed the instructions in the corresponding repository (or paper).</p><p>Training with minibatch. We implemented stochastic gradient descent (SGD) using minibatches, where each batch contained graphs with the same number of nodes. More specifically, we first group the training graphs</p><formula xml:id="formula_32">G i 's into batches B = {B k } such that |V i | = |V j | for all G i , G j ? B k .</formula><p>Then, at each iteration, we select a batch at random, build a computation graph for the number of nodes corresponding to the batch using the parameters estimated in the previous iteration, and update the parameters using the computation graph and the batch of graphs. Such a procedure helps to reduce the overhead time for building the computational graph, from per sample to per batch. This batching and training process is summarizedd in Algorithm 1, where "CreateBatches(...)" group the training graphs into batches, "BuildComputationalGraph(...)" builds the computation graph "NeVAE" using the parameters from the previous iteration and a given number of nodes, "Nodes(...)" returns the number of nodes of the graphs in a batch, and "Train(...)" updates the parameters given the computation graph and the parameters from the previous iteration.</p><p>Hardware and software specifications. We carried out our experiments for NeVAE using Tensorflow 1.4.1, on a 64 bit Debian OS with 16 core Intel Xenon CPU (E5-2667 v4 @3.20 GHz) and 512GB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Experiments on Synthetic Graphs</head><p>Quality of the generated graphs. In addition to the set of Kronecker graphs reported in the main paper, we create an additional set of 100 graphs with up to n = 1000 number of nodes sampled from the Barab?si-Albert graph model with generation parameter m = 1. Then, for both Barab?si-Albert and Kronecker graphs, we evaluate the quality of the generated graphs using two quantitative evaluation metrics:</p><p>(i) Rank correlation: we use this metric to test to which extent the models we trained using Barab?si-Albert and Kronecker graphs do generate plausible Barab?si-Albert and Kronecker graphs, respectively. Intuitively, if the trained models generate plausible graphs, we expect that a graph G with a very high value of likelihood under the true model, p(G|P), should also have a high value of likelihood, E Z?p(Z) log p ? (G|Z), and ELBO under our trained model. For a set of graphs, we verify this expectation by computing the rank correlation between lists of graphs as follows. First, for each set of generated graphs G, we order them in decreasing order of p(G|P) and keep the top 10% in a ranked list<ref type="foot" target="#foot_7">8</ref> , which we denote as T p . Then, we take the graphs in T p and create two ranked lists, one in decreasing order of E Z?p(Z) log p ? (G|Z), which we denote as T p ? , and another one in decreasing order of ELBO, which we denote as T ELBO . Finally, we compute two Spearman's rank correlation coefficients between these lists:</p><formula xml:id="formula_33">? p ? := Cov(T p , T p ? ) ? Tp ? Tp ? ? ELBO := Cov(T p , T pELBO ) ? Tp ? TELBO where ? p ? , ? ELBO ? [-1, 1].</formula><p>(ii) Precision: we use this metric, which we compute as follows, as an alternative to the rank correlation above for Barab?si-Albert and Kronecker graphs. For each set of generated graphs G, we also order them in decreasing order of p(G|P) and create an ordered list T p , and select T ? p as the top 10% and T ? p as the bottom 10% of T p . Then, we re-rank this list in decreasing order of E Z?p(Z) log p ? (G|Z) and ELBO to create two new ordered lists, T p ? and T ELBO . Here, if the trained models generate plausible graphs, we expect that each of the top and bottom halves of T p ? and T ELBO should have a high overlap with T ? p and T ? p , respectively. Then, we define top and bottom precision as:</p><formula xml:id="formula_34">? ? = |T ? x ? T ? p | |T ? p | ? ? = |T ? x ? T ? p | |T ? p |</formula><p>where ? ? , ? ? ? [0, 1] and T ? x (T ? x ) is the top (bottom) half of either x = T p ? or x = T ELBO . Table <ref type="table" target="#tab_3">4</ref> summarizes the results, which show that our model is able to learn the generative process of Barab?si-Albert more accurately than Kronecker graphs. This may be due to the higher complexity of the generative process Kronecker graph use. That being said, it is remarkable that our model is able to achieve correlation and precision values over 0.4 in both cases. Effect of K (search-depth in encoder) on model performance. Here, we investigate the behavior of our model with respect to the search depths K used in the decoder. Figure <ref type="figure" target="#fig_7">5</ref> summarizes the results, which show that, for Barab?si-Albert graphs, our model performs consistently well for low values of K, however, for Kronecker graphs, the performance is better for high values of K. A plausible explanation for this is that Barab?si-Albert networks are generated sequentially using only local topological features (only node-degrees), whereas the generation process of Kronecker graphs incorporates global topological features. ) and bottom precision (? ? ) achieved by our variational autoencoder trained with either Barab?si-Albert or Kronecker graphs. In both cases, dim(z i ) = 7 and K = 3. Here, the higher the value of rank correlation and (top and bottom) precision, the more accurately the trained models mimic the generative processes for Barab?si-Albert and Kronecker graphs.  Visualization of generated triangle free graphs. In Figure <ref type="figure">6</ref>, we show a few triangle free graphs that the decoder of our model is able to generate, which shows that it can produce various type of triangle free networks. Figure <ref type="figure">6</ref>: Graphs sampled using our variational autoencoder trained with a set of triangle free graphs. By using masking, our variational autoencoder is able to always generate triangle free graphs.</p><formula xml:id="formula_35">? p ? ? K ? dim(z z zi) = 3 dim(z z zi) = 4 dim(z z zi) = 5 dim(z z zi) = 6<label>0</label></formula><p>Scalability. Here, we first compute the running time of our variational inference procedure against the size of the graphs in the training set and then compute the running time of our probabilistic decoder against the size of the sampled (generated) graphs. Figure <ref type="figure" target="#fig_10">7</ref> summarizes the results, which show that both in terms of inference and sampling, our model easily scales to ?1,000 nodes. For example, for graphs with 1000 nodes (average degree 3), our inference procedure takes 67 + 20 seconds to run one iteration of SGD with a batch size of 10 graphs and, for graphs with 50 nodes, our inference procedure takes less than 10 seconds per iteration. Moreover, our probabilistic decoder can sample a graph with 1000 (50) nodes (average degree 3) in only 5 (0.5) seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Experiments on Molecule Design</head><p>Molecule generation from the posterior distribution. In this section, we demonstrate (qualitatively) that the latent space of molecules inferred by our model is smooth. Given a molecule, along with its associated graph G, node features F and edge weights Y, we first sample its latent representation Z using our probabilistic encoder, i.e., Z ? q ? (Z|G, F, Y). Then, given this latent representation, we generate various molecular graphs by sampling from our probabilistic decoder, i.e., G i ? p ? (G|Z). Figure <ref type="figure">8</ref> summarizes the  results for one molecule from the ZINC dataset, which show that the sampled molecules are topologically similar to the given molecule.</p><p>Figure <ref type="figure">8</ref>: Molecules sampled using the probabilistic decoder, i.e. G i ? p ? (G|Z), given the (sampled) latent representation Z of a given molecule G from the ZINC dataset. The sampled molecules are topologically similar to each other as well to the given molecule. This provides qualitative evidence that the latent space of molecules provided by our variational autoencoder for graphs is smooth.</p><p>Smooth latent space of molecules. In this section, we show that our encoder, once trained, creates a latent space representation of molecules with powerful semantics. In particular, since each node in a molecule has a latent representation, we can make fine-grained changes to the structure of a molecule by perturbing the latent representation of single nodes. To this aim, we proceed as follows.</p><p>First, we select one molecule with n nodes from the ZINC (QM9) dataset. Given its corresponding graph, node features and edge weights, G, F and Y, we sample its latent representation Z 0 . Then, we sample new molecular graphs G from the probabilistic decoder G ? p ? (G|Z), where Z = {z i + a i z i | z i ? Z 0 , a i ? 0} and a i are given parameters. Figures 9 provides several examples across both datasets, which show that the latent space representation is smooth and, as the distance from the initial molecule increases in the latent space, the resulting molecule differs more from the initial molecule.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The encoder of our variational autoencoder for graphs. From left to right, given a graph G with a set of node features F and edge weights Y, the encoder aggregates information from a different number of hops j ? K away for each node v ? G into an embedding vector c v (j). These embeddings are fed into a differentiable function ? enc which parameterizes the posterior distribution q ? , from where the latent representation of each node in the input graph are sampled from.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The decoder of our variational autoencoder for graphs. From left to right, the decoder first samples the number of nodes n = |V| and edges l = |E| from two Poisson distributions p n (? n ) and p l (? l) and it samples a latent vector z u per node u ? V from N (0, I). Then, for each node u, it represents all potential node feature values as an unnormalized log probability vector (or 'logits'), where each entry is given by a nonlinearity ? dec</figDesc><graphic url="image-20.png" coords="5,80.62,72.00,444.60,155.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Graph generation by sampling graphs from our probabilistic decoder whose latent representation lies in between the latent representation of two Kronecker graphs, using a linear interpolation. Each column corresponds to a graph, the top row shows the latent representation of all nodes for the graphs in the bottom row, and the middle row shows the (inferred) initiator matrix for the Kronecker graph model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>e., molecules which were not present in the (training) dataset, i.e. Novelty = 1 -|C s ? D|/|C s |, where C s is the set of generated molecules which are chemically valid, D is the training dataset, and Novelty ? [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Best molecules found by Bayesian Optimization (BO) using our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Rank correlation (? p ? ) with respect to the search depths K used in the decoder for Barab?si-Albert graphs, small values of K achieve better performance, whereas for Kronecker graphs, a larger K provides better performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Graph sampling time vs. # of nodes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Scalability of our inference procedure and probabilistic decoder. Panel (a) shows the time per iteration of our variational inference procedure against the size of the graphs in the training set using batches of 10 graphs with average degree 3. Panel (b) shows the time our probabilistic decoder takes to sample an entire graph with average degree 3 against the size of the graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>, we use Bayesian optimization (BO) to identify novel molecules m Property prediction performance (LL and RMSE) using Sparse Gaussian processes (SGPs) and property maximization using Bayesian Optimization (BO)</figDesc><table><row><cell>Objective</cell><cell cols="3">NeVAE GrammarVAE [23] CVAE [8]</cell></row><row><cell>LL</cell><cell>-1.631</cell><cell>-1.980</cell><cell>-2.153</cell></row><row><cell>RMSE</cell><cell>1.231</cell><cell>1.407</cell><cell>1.504</cell></row><row><cell cols="2">Fraction of valid molecules 1.00</cell><cell>0.77</cell><cell>0.53</cell></row><row><cell>Avg. scores</cell><cell>0.36</cell><cell>-4.30</cell><cell>-14.40</cell></row><row><cell cols="2">Fraction of good molecules 0.66</cell><cell>0.27</cell><cell>0.23</cell></row><row><cell>Best scores</cell><cell>2.32</cell><cell>2.82</cell><cell>1.46</cell></row><row><cell>y(m) = 2.32 (1st)</cell><cell cols="2">y(m) = 1.89 (2nd)</cell><cell>y(m) = 1.68 (3rd)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>7 . Details on the architecture of NeVAE.</figDesc><table><row><cell>Layer</cell><cell cols="2">Architecture Inputs</cell><cell>Type of non-linearity</cell><cell>Parameters to be learned</cell><cell>Output</cell></row><row><cell>Input</cell><cell>Feedforward (K layers)</cell><cell>E, F, Y</cell><cell>r(?): Linear ?(?): Sum g(?): Linear</cell><cell>W1, ..., WK</cell><cell>c1, . . . , cn</cell></row><row><cell>Encoder</cell><cell>Feedforward (Two layers)</cell><cell>c1, . . . , cn</cell><cell>Softplus Softplus Softplus</cell><cell>Wh, bh W?, b?, W?, b?</cell><cell>?1, . . . , ?n ?1, . . . , ?n</cell></row><row><cell>Decoder</cell><cell>Feedforward (One layer)</cell><cell>Z</cell><cell>Softplus</cell><cell>W, b</cell><cell>E, Y, f</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Rank correlation (?), top precision (? ?</figDesc><table><row><cell></cell><cell>?p ?</cell><cell>?ELBO</cell><cell>? ? p ?</cell><cell>? ? p ?</cell><cell>? ? ELBO</cell><cell>? ? ELBO</cell></row><row><cell>Barab?si-Albert</cell><cell>0.69</cell><cell>0.72</cell><cell cols="2">0.98 0.98</cell><cell>1.00</cell><cell>1.00</cell></row><row><cell>Kronecker</cell><cell>0.50</cell><cell>0.21</cell><cell cols="2">0.47 0.47</cell><cell>0.70</cell><cell>0.70</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>See https://github.com/Networks-Learning/nevae.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>For ease of exposition, we assume the node features and edge weights take discrete values, however, our model could be augmented to continuous values.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>During encoding we use the one hot representation of the node-type as feature vector</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>We have not selected any molecule whose bond types are others than these three.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>We batch graphs with respect to the number of nodes for efficiency reasons since, every time that the number of nodes changes, we need to change the size of the computational graph in Tensorflow.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>We have used the opensource cheminformatics suite RDkit (http://www.rdkit.org) to check whether a generated molecules is chemically valid.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>We did experiment with other symmetric aggregator functions such as pooling, as in the inductive graph representation learning<ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>, and did not notice significant gains in practice.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>We discard the remaining graphs since their likelihood is very similar.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>and a i are given parameters. In each row, we use a different starting molecule (and thus different latent representation Z 0 ), set a i &gt; 0 for a single arbitrary node i (denoted as ?) and set a j = 0, j = i for the remaining nodes.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to represent programs with graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khademi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barab?si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5439</biblScope>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Netgan: Generating graphs via random walks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Z?gner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06349</idno>
		<title level="m">Generating sentences from a continuous space</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Syntax-directed variational autoencoder for structured data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Automatic chemical design using a data-driven continuous representation of molecules</title>
		<author>
			<persName><forename type="first">R</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02415</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Generative adversarial nets. In NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Graphite: Iterative generative modeling of graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10459</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>IEEE Data Engineering Bulletin</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An exponential family of probability distributions for directed graphs</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the american Statistical association</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">373</biblScope>
			<biblScope unit="page" from="33" to="50" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Zinc: a free tool to discover chemistry for biology</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Mysinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Bolstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1768" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04364</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient global optimization of expensive black-box functions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schonlau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Welch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global optimization</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="455" to="492" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels and community structure in networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">16107</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<title level="m">Variational graph auto-encoders</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01925</idno>
		<title level="m">Grammar variational autoencoder</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deriving neural architectures from sequence and graph kernels</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kronecker graphs: An approach to modeling networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="985" to="1042" />
			<date type="published" when="2010-02">Feb. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning deep generative models of graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03324</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Column networks for collective classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2485" to="2491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Von Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">140022</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Enumeration of 166 billion organic small molecules in the chemical universe database gdb-17</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ruddigkeit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Deursen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Reymond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2864" to="2875" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Graphvae: Towards generation of small graphs using variational autoencoders</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03480</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sparse gaussian processes using pseudo-inputs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Snelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Improved variational autoencoders for text modeling using dilated convolutions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08139</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graphrnn: A deep generative model for graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
