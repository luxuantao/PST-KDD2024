<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A TRANSFORMER-BASED FRAMEWORK FOR MULTI-VARIATE TIME SERIES REPRESENTATION LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-06">6 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">George</forename><surname>Zerveas</surname></persName>
							<email>george@brown.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Brown University Providence</orgName>
								<address>
									<region>RI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Srideepika</forename><surname>Jayaraman</surname></persName>
							<email>j.srideepika@ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research Yorktown Heights</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dhaval</forename><surname>Patel</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IBM Research Yorktown Heights</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anuradha</forename><surname>Bhamidipaty</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">IBM Research Yorktown Heights</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
							<email>carsten@brown.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">Brown University Providence</orgName>
								<address>
									<region>RI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A TRANSFORMER-BASED FRAMEWORK FOR MULTI-VARIATE TIME SERIES REPRESENTATION LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-06">6 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2010.02803v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we propose for the first time a transformer-based framework for unsupervised representation learning of multivariate time series. Pre-trained models can be potentially used for downstream tasks such as regression and classification, forecasting and missing value imputation. By evaluating our models on several benchmark datasets for multivariate time series regression and classification, we show that not only does our modeling approach represent the most successful method employing unsupervised learning of multivariate time series presented to date, but also that it exceeds the current state-of-the-art performance of supervised methods; it does so even when the number of training samples is very limited, while offering computational efficiency. Finally, we demonstrate that unsupervised pre-training of our transformer models offers a substantial performance benefit over fully supervised learning, even without leveraging additional unlabeled data, i.e., by reusing the same data samples through the unsupervised objective.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Multivariate time series (MTS) are an important type of data that is ubiquitous in a wide variety of domains, including science, medicine, finance, engineering and industrial applications. As the name suggests, they typically represent the evolution of a group of synchronous variables (e.g., simultaneous measurements of different physical quantities) over time, but they can more generally represent a group of dependent variables (abscissas) aligned with respect to a common independent variable, e.g., absorption spectra under different conditions as a function of light frequency. Despite the recent abundance of MTS data in the much touted era of "Big Data", the availability of labeled data in particular is far more limited: extensive data labeling is often prohibitively expensive or impractical, as it may require much time and effort, special infrastructure or domain expertise. For this reason, in all aforementioned domains there is great interest in methods which can offer high accuracy by using only a limited amount of labeled data or by leveraging the existing plethora of unlabeled data.</p><p>There is a large variety of modeling approaches for univariate and multivariate time series, with deep learning models recently challenging and at times replacing the state of the art in tasks such as forecasting, regression and classification <ref type="bibr" target="#b6">(De Brouwer et al., 2019;</ref><ref type="bibr">Tan et al., 2020a;</ref><ref type="bibr">Fawaz et al., 2019b)</ref>. However, unlike in domains such as Computer Vision or Natural Language Processing (NLP), the dominance of deep learning for time series is far from established: in fact, non-deep learning methods such as TS-CHIEF <ref type="bibr" target="#b28">(Shifaz et al., 2020)</ref>, HIVE-COTE <ref type="bibr" target="#b19">(Lines et al., 2018)</ref>, and ROCKET <ref type="bibr" target="#b7">(Dempster et al., 2020</ref>) currently hold the record on time series regression and classification dataset benchmarks <ref type="bibr">(Tan et al., 2020a;</ref><ref type="bibr" target="#b0">Bagnall et al., 2017)</ref>, matching or even outperforming sophisticated deep architectures such as InceptionTime <ref type="bibr">(Fawaz et al., 2019a)</ref> and ResNet <ref type="bibr">(Fawaz et al., 2019b)</ref>.</p><p>In this work, we investigate, for the first time, the use of a transformer encoder for unsupervised representation learning of multivariate time series, as well as for the tasks of time series regression and classification. Transformers are an important, recently developed class of deep learning models, which were first proposed for the task of natural language translation <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref> but have since come to monopolize the state-of-the-art performance across virtually all NLP tasks <ref type="bibr" target="#b26">(Raffel et al., 2019)</ref>. A key factor for the widespread success of transformers in NLP is their aptitude for learning how to represent natural language through unsupervised pre-training <ref type="bibr" target="#b4">(Brown et al., 2020;</ref><ref type="bibr" target="#b26">Raffel et al., 2019;</ref><ref type="bibr" target="#b8">Devlin et al., 2018)</ref>. Besides NLP, transformers have also set the state of the art in several domains of sequence generation, such as polyphonic music composition <ref type="bibr" target="#b14">(Huang et al., 2018)</ref>.</p><p>Transformer models are based on a multi-headed attention mechanism that offers several key advantages and renders them particularly suitable for time series data (see Appendix section A.2 for details).</p><p>Inspired by the impressive results attained through unsupervised pre-training of transformer models in NLP, as our main contribution, in the present work we develop a generally applicable methodology (framework) that can leverage unlabeled data by first training a transformer model to extract dense vector representations of multivariate time series through an input denoising (autoregressive) objective. The pre-trained model can be subsequently applied to several downstream tasks, such as regression, classification, imputation, and forecasting. Here, we apply our framework for the tasks of multivariate time series regression and classification on several public datasets and demonstrate that transformer models can convincingly outperform all current state-of-the-art modeling approaches, even when only having access to a very limited amount of training data samples (on the order of hundreds of samples), an unprecedented success for deep learning models. To the best of our knowledge, this is also the first time that unsupervised learning has been shown to confer an advantage over supervised learning for classification and regression of multivariate time series without utilizing additional unlabeled data samples. Importantly, despite common preconceptions about transformers from the domain of NLP, where top performing models have billions of parameters and require days to weeks of pre-training on many parallel GPUs or TPUs, we also demonstrate that our models, using at most hundreds of thousands of parameters, can be practically trained even on CPUs; training them on GPUs allows them to be trained as fast as even the fastest and most accurate non-deep learning based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Regression and classification of time series: Currently, non-deep learning methods such as TS-CHIEF <ref type="bibr" target="#b28">(Shifaz et al., 2020)</ref>, HIVE-COTE <ref type="bibr" target="#b19">(Lines et al., 2018)</ref>, and ROCKET <ref type="bibr" target="#b7">(Dempster et al., 2020)</ref> constitute the state of the art for time series regression and classification based on evaluations on public benchmarks <ref type="bibr">(Tan et al., 2020a;</ref><ref type="bibr" target="#b0">Bagnall et al., 2017)</ref>, followed by CNN-based deep architectures such as InceptionTime <ref type="bibr">(Fawaz et al., 2019a)</ref> and ResNet <ref type="bibr">(Fawaz et al., 2019b)</ref>. ROCKET, which on average is the best ranking method, is a fast method that involves training a linear classifier on top of features extracted by a flat collection of numerous and various random convolutional kernels. HIVE-COTE and TS-CHIEF (itself inspired by Proximity Forest <ref type="bibr" target="#b21">(Lucas et al., 2019)</ref>), are very sophisticated methods which incorporate expert insights on time series data and consist of large, heterogeneous ensembles of classifiers utilizing shapelet transformations, elastic similarity measures, spectral features, random interval and dictionary-based techniques; however, these methods are highly complex, involve significant computational cost, cannot benefit from GPU hardware and scale poorly to datasets with many samples and long time series; moreover, they have been developed for and only been evaluated on univariate time series.</p><p>Unsupervised learning for multivariate time series: Recent work on unsupervised learning for multivariate time series has predominantly employed autoencoders, trained with an input reconstruction objective and implemented either as Multi-Layer Perceptrons (MLP) or RNN (most commonly, LSTM) networks. As interesting variations of the former, <ref type="bibr" target="#b16">Kopf et al. (2019) and</ref><ref type="bibr">Fortuin et al. (2019)</ref> additionally incorporated Variational Autoencoding into this approach, but focused on clustering and the visualization of shifting sample topology with time. As an example of the lat-ter, <ref type="bibr" target="#b24">Malhotra et al. (2017)</ref> presented a multi-layered RNN sequence-to-sequence autoencoder, while <ref type="bibr" target="#b22">Lyu et al. (2018)</ref> developed a multi-layered LSTM with an attention mechanism and evaluated both an input reconstruction (autoencoding) as well as a forecasting loss for unsupervised representation learning of Electronic Healthcare Record multivariate time series.</p><p>As a novel take on autoencoding, and with the goal of dealing with missing data, Bianchi et al.</p><p>(2019) employ a stacked bidirectional RNN encoder and stacked RNN decoder to reconstruct the input, and at the same time use a user-provided kernel matrix as prior information to condition internal representations and encourage learning similarity-preserving representations of the input. They evaluate the method on the tasks of missing value imputation and classification of time series under increasing "missingness" of values. For the purpose of time series clustering, <ref type="bibr" target="#b17">Lei et al. (2017)</ref> also follow a method which aims at preserving similarity between time series by directing learned representations to approximate a distance such as Dynamic Time Warping (DTW) between time series through a matrix factorization algorithm. A distinct approach is followed by <ref type="bibr" target="#b33">Zhang et al. (2019)</ref>, who use a composite convolutional -LSTM network with attention and a loss which aims at reconstructing correlation matrices between the variables of the multivariate time series input. They use and evaluate their method only for the task of anomaly detection.</p><p>Finally, <ref type="bibr" target="#b15">Jansen et al. (2018)</ref> rely on a triplet loss and the idea of temporal proximity (the loss rewards similarity of representations between proximal segments and penalizes similarity between distal segments of the time series) for unsupervised representation learning of non-speech audio data. This idea is explored further by <ref type="bibr" target="#b12">Franceschi et al. (2019)</ref>, who combine the triplet loss with a deep causal CNN with dilation, in order to make the method effective for very long time series. Although on the task of univariate classification the method is outperformed by the aforementioned supervised stateof-the-art methods, it is the best performing method leveraging unsupervised learning for univariate and multivariate classification datasets of the UEA/UCR archive <ref type="bibr" target="#b0">(Bagnall et al., 2017)</ref>.</p><p>Transformer models for time series: Recently, a full encoder-decoder transformer architecture was employed for univariate time series forecasting: <ref type="bibr" target="#b18">Li et al. (2019)</ref> showed superior performance compared to the classical statistical method ARIMA, the recent matrix factorization method TRMF, an RNN-based autoregressive model (DeepAR) and an RNN-based state space model (DeepState) on 4 public forecasting datasets, while <ref type="bibr" target="#b32">Wu et al. (2020)</ref> used a transformer to forecast influenza prevalence and similarly showed performance benefits compared to ARIMA, an LSTM and a GRU Seq2Seq model with attention. Finally, <ref type="bibr" target="#b23">Ma et al. (2019)</ref> use an encoder-decoder architecture with a variant of self-attention for imputation of missing values in multivariate, geo-tagged time series and outperform classic as well as the state-of-the-art, RNN-based imputation methods on 3 public and 2 competition datasets for imputation.</p><p>By contrast, our work aspires to generalize the use of transformers from solutions to specific generative tasks (which require the full encoder-decoder architecture) to a framework which allows for unsupervised pre-training and with minor modifications can be readily used for a wide variety of downstream tasks; this is analogous to the way BERT <ref type="bibr" target="#b8">(Devlin et al., 2018</ref>) converted a translation model into a generic framework based on unsupervised learning, an approach which has become a de facto standard and established the dominance of transformers in NLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BASE MODEL</head><p>At the core of our method lies a transformer encoder, as described in the original transformer work by <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref>; however, we do not use the decoder part of the architecture. A schematic diagram of the generic part of our model, common across all considered tasks, is shown in Figure <ref type="figure" target="#fig_0">1</ref>. We refer the reader to the original work for a detailed description of the transformer model, and here present the proposed changes that make it compatible with multivariate time series data, instead of sequences of discrete word indices.</p><p>In particular, each training sample X ? R w?m , which is a multivariate time series of length w and m different variables, constitutes a sequence of w feature vectors</p><formula xml:id="formula_0">x t ? R m : X ? R w?m = [x 1 , x 2 , . . . , x w ].</formula><p>The original feature vectors x t are first normalized (for each dimension, we sub- We mask a proportion r of each variable sequence in the input independently, such that across each variable, time segments of mean length l m are masked, each followed by an unmasked segment of mean length l u =<ref type="foot" target="#foot_0">1</ref>-r r l m . Using a linear layer on top of the final vector representations z t , at each time step the model tries to predict the full, uncorrupted input vectors x t ; however, only the predictions on the masked values are considered in the Mean Squared Error loss.</p><p>tract the mean and divide by the variance across the training set samples) and then linearly projected onto a d-dimensional vector space, where d is the dimension of the transformer model sequence element representations (typically called model dimension):</p><formula xml:id="formula_1">u t = W p x t + b p (1)</formula><p>where W p ? R d?m , b p ? R d are learnable parameters and u t ? R d , t = 0, . . . , w are the model input vectors 1 , which correspond to the word vectors of the NLP transformer. These will become the queries, keys and values of the self-attention layer, after adding the positional encodings and multiplying by the corresponding matrices.</p><p>We note that the above formulation also covers the univariate time series case, i.e., m = 1, although we only evaluate our approach on multivariate time series in the scope of this work. We additionally note that the input vectors u t need not necessarily be obtained from the (transformed) feature vectors at a time step t: because the computational complexity and the number of parameters of the model scale as O(w 2 ) with the input sequence length w, to obtain u t in case the granularity (temporal resolution) of the data is very fine, one may instead use a 1D-convolutional layer with 1 input and d output channels and kernels K i of size (k, m), where k is the width in number of time steps and i the output channel:</p><formula xml:id="formula_2">u t i = u(t, i) = j h x(t + j, h)K i (j, h), i = 1, . . . , d<label>(2)</label></formula><p>In this way, one may control the temporal resolution by using a stride or dilation factor greater than 1. Moreover, although in the present work we only used equation 1, one may use equation 2 as an input to compute the keys and queries and equation 1 to compute the values of the self-attention layer. This is particularly useful in the case of univariate time series, where self-attention would otherwise match (consider relevant/compatible) all time steps which share similar values for the independent variable, as noted by <ref type="bibr" target="#b18">Li et al. (2019)</ref>.</p><p>Finally, since the transformer is a feed-forward architecture that is insensitive to the ordering of input, in order to make it aware of the sequential nature of the time series, we add positional encodings</p><formula xml:id="formula_3">W pos ? R w?d to the input vectors U ? R w?d = [u 1 , . . . , u w ]: U = U + W pos .</formula><p>Instead of deterministic, sinusoidal encodings, which were originally proposed by <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref>, we use fully learnable positional encodings, as we observed that they perform better for all datasets presented in this work. Interestingly, we note that similar to the case of word embeddings, the positional encodings generally appear not to interfere with the numerical information of the time series; we hypothesize that this is because they are learned so as to occupy a different, approximately orthogonal, subspace to the one in which the projected time series samples reside. This approximate orthogonality condition is much easier to satisfy in high dimensional spaces (here, this is the model dimension d).</p><p>An important consideration regarding time series data is that individual samples may display considerable variation in length. This issue is effectively dealt with in our framework: after setting a maximum sequence length w for the entire dataset, shorter samples are padded with arbitrary values, and we generate a padding mask which adds a large negative value to the attention scores for the padded positions, before computing the self-attention distribution with the softmax function. This forces the model to completely ignore padded positions, while allowing the parallel processing of samples in large minibatches.</p><p>Transformers in NLP use layer normalization after computing self-attention and after the feedforward part of each encoder block, leading to significant performance gains over batch normalization, as originally proposed by <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref>. However, here we instead use batch normalization, because it can mitigate the effect of outlier values in time series, an issue that does not arise in NLP word embeddings. Additionally, the inferior performance of batch normalization in NLP has been mainly attributed to extreme variation in sample length (i.e., sentences in most tasks) <ref type="bibr" target="#b27">(Shen et al., 2020)</ref>, while in the datasets we examine this variation is much smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">UNSUPERVISED PRE-TRAINING</head><p>As a task for the unsupervised pre-training of our model we consider the autoregressive task of denoising the input: specifically, we set part of the input to 0 and ask the model to predict the masked values. The corresponding setup is depicted in the right part of Figure <ref type="figure" target="#fig_0">1</ref>. A binary noise mask M ? R w?m , is created independently for each training sample, and the input is masked by elementwise multiplication: X = M X. On average, a proportion r of each mask column of length w (corresponding to a single variable in the multivariate time series) is set to 0 by alternating between segments of 0s and 1s. We choose the state transition probabilities such that each masked segment (sequence of 0s) has a length that follows a geometric distribution with mean l m and is succeeded by an unmasked segment (sequence of 1s) of mean length l u = 1-r r l m . We chose l m = 3 for all presented experiments. The reason why we wish to control the length of the masked sequence, instead of simply using a Bernoulli distribution with parameter r to set all mask elements independently at random, is that very short masked sequences (e.g., of 1 masked element) in the input can often be trivially predicted with good approximation by replicating the immediately preceding or succeeding values or by the average thereof; in order to obtain enough long masked sequences with relatively high likelihood, a very high masking proportion r would be required, which would render the overall task unnecessarily challenging. Following the process above, at each time step on average r ? m variables will be masked. We chose r = 0.15 for all presented experiments. This input masking process is different from the "cloze type" masking used by NLP models such as BERT, where a special token and thus word embedding vector replaces the original word embedding, i.e., the entire feature vector at affected time steps. We chose this masking pattern because it encourages the model to learn to attend both to preceding and succeeding segments in individual variables, as well as to existing contemporary values of the other variables in the time series, and thereby to learn to model inter-dependencies between variables.</p><p>Using a linear layer with parameters W o ? R m?d , b o ? R m on top of the final vector representations z t ? R d , for each time step the model concurrently outputs its estimate xt of the full, uncorrupted input vectors x t ; however, only the predictions on the masked values (with indices in the set M ? {(t, i) : m t,i = 0}, where m t,i are the elements of the mask M), are considered in the Mean Squared Error loss for each data sample:</p><formula xml:id="formula_4">xt = W o z t + b o (3) L MSE = 1 |M | (t,i)?M (x(t, i) -x(t, i)) 2<label>(4)</label></formula><p>This objective differs from the one used by denoising autoencoders, where the loss considers reconstruction of the entire input, under (typically Gaussian) noise corruption. Also, we note that the approach described above differs from simple dropout on the input embeddings, both with respect to the statistical distributions of masked values, as well as the fact that here the masks also determine the loss function. In fact, we additionally use a dropout of 10% when training all of our supervised and unsupervised models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">REGRESSION AND CLASSIFICATION</head><p>The base model architecture presented in Section 3.1 and depicted in Figure <ref type="figure" target="#fig_0">1</ref> can be used for the purposes of regression and classification with the following modification: the final representation vectors z t ? R d corresponding to all time steps are concatenated into a single vector z ? R d?w = [z 1 ; . . . ; z w ], which serves as the input to a linear output layer with parameters</p><formula xml:id="formula_5">W o ? R n?(d?w) , b o ? R n ,</formula><p>where n is the number of scalars to be estimated for the regression problem (typically n = 1), or the number of classes for the classification problem:</p><formula xml:id="formula_6">? = W o z + b o (5)</formula><p>In the case of regression, the loss for a single data sample will simply be the squared error L = ?y 2 , where y ? R n are the ground truth values. In the case of classification, the predictions ? will additionally be passed through a softmax function to obtain a distribution over classes, and its cross-entropy with the categorical ground truth labels will be the sample loss.</p><p>Finally, when fine-tuning the pre-trained models, we allow training of all weights; instead, freezing all layers except for the output layer would be equivalent to using pre-extracted time-series representations of the time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS &amp; RESULTS</head><p>In the experiments reported below we use the predefined training -test set splits of the benchmark datasets and train all models long enough to ensure convergence. We do this to account for the fact that training the transformer models in a fully supervised way typically requires more epochs than fine-tuning the ones which have already been pre-trained using the unsupervised methodology of Section 3.2. Because the benchmark datasets are very heterogeneous in terms of number of samples, dimensionality and length of the time series, as well as the nature of the data itself, we observed that we can obtain better performance by a cursory tuning of hyperparameters (such as the number of encoder blocks, the representation dimension, number of attention heads or dimension of the feedforward part of the encoder blocks) separately for each dataset. However, a set of hyperparameters which has consistently good performance on all datasets is shown in Table <ref type="table">6</ref> of the Appendix. We use the Rectifying Adam optimizer <ref type="bibr" target="#b20">(Liu et al., 2020)</ref> to obtain insensitivity in terms of the optimal learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">REGRESSION</head><p>We select a diverse range of 6 datasets from the Monash University, UEA, UCR Time Series Regression Archive <ref type="bibr">(Tan et al., 2020a)</ref> in a way so as to ensure diversity with respect to the dimensionality and length of time series samples, as well as the number of samples (see Appendix Table <ref type="table">3</ref> for dataset characteristics). Table <ref type="table" target="#tab_0">1</ref> shows the Root Mean Squared Error achieved by of our models, named TST for "Time Series Transformer", including a variant trained only through supervision, and one first pre-trained on the same training set in an unsupervised way. We compare them with the currently best performing models as reported in the archive. Our transformer models outperform all other baselines on all but two of the examined datasets, for which they rank second. They thus achieve an average rank of 1.33, setting them clearly apart from all other models; the overall second best model, XGBoost, has an average rank of 3.5, ROCKET (which outperformed ours on one dataset) on average ranks in 5.67th place and Inception (which outperformed ours on the second dataset) also has an average rank of 5.67. On average, our models attain approx. 16% lower RMSE than the overall second best model (XGBoost), with absolute improvements varying among datasets from approx. 4% to 36%. Importantly, we also observe that the pre-trained transformer models outperform the fully supervised ones in 3 out of 6 datasets. This is interesting, because no additional samples are used for pre-training: the benefit appears to originate from reusing the same training samples for learning through an unsupervised objective. To further elucidate this observation, we investigate the following questions:</p><p>Q1: Given a partially labeled dataset of a certain size, how will additional labels affect performance? This pertains to one of the most important decisions that data owners face, namely, to what extent will further annotation help. To clearly demonstrate this effect, we choose the largest dataset we have considered from the regression archive (12.5k samples), in order to avoid the variance introduced by small set sizes. The left panel of Figure <ref type="figure" target="#fig_1">2</ref> (where each marker is an experiment) shows how performance on the entire test set varies with an increasing proportion of labeled training set data used for supervised learning. As expected, with an increasing proportion of available labels performance improves both for a fully supervised model, as well as the same model that has been first pre-trained on the entire training set through the unsupervised objective and then fine-tuned. Interestingly, not only does the pre-trained model outperform the fully supervised one, but the benefit persists throughout the entire range of label availability, even when the models are allowed to use all labels; this is consistent with our previous observation on Table <ref type="table" target="#tab_0">1</ref> regarding the advantage of reusing samples.</p><p>Q2: Given a labeled dataset, how will additional unlabeled samples affect performance? In other words, to what extent does unsupervised learning make it worth collecting more data, even if no additional annotations are available? This question differs from the above, as we now only scale the availability of data samples for unsupervised pre-training, while the number of labeled samples is fixed. The right panel of Figure <ref type="figure" target="#fig_1">2</ref> (where each marker is an experiment) shows that, for a given number of labels (shown as a percentage of the totally available labels), the more data samples are used for unsupervised learning, the lower the error achieved (note that the horizontal axis value 0 corresponds to fully supervised training only, while all other values to unsupervised pre-training followed by supervised fine-tuning). This trend is more linear in the case of supervised learning on 20% of the labels (approx. 2500). Likely due to a small sample (here, meaning set) effect, in the case of having only 10% of the labels (approx. 1250) for supervised learning, the error </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">CLASSIFICATION</head><p>We select a set of 11 multivariate datasets from the UEA Time Series Classification Archive <ref type="bibr" target="#b1">(Bagnall et al., 2018)</ref> with diverse characteristics in terms of the number, dimensionality and length of time series samples, as well as the number of classes (see Appendix Table <ref type="table">4</ref>). As this archive is new, there have not been many reported model evaluations; we follow <ref type="bibr" target="#b12">Franceschi et al. (2019)</ref> and use as a baseline the best performing method studied by the creators of the archive, DTW D (dimension-Dependent DTW), together with the method proposed by Franceschi et al. ( <ref type="formula">2019</ref>) themselves (a dilation-CNN leveraging unsupervised and supervised learning). Additionally, we use the publicly available implementations <ref type="bibr">(Tan et al., 2020b)</ref> of ROCKET, which is currently the top performing model for univariate time series and one of the best in our regression evaluation, and XGBoost, which is one of the most commonly used models for univariate and multivariate time series, and also the best baseline model in our regression evaluation (Section 4.1). Finally, we did not find any reported evaluations of RNN-based models on any of the UCR/UEA archives, possibly because of a common perception for long training and inference times, as well as difficulty in training <ref type="bibr">(Fawaz et al., 2019b)</ref>; therefore, we implemented a stacked LSTM model and also include it in the comparison. The performance of the baselines alongside our own models are shown in Table <ref type="table" target="#tab_1">2</ref> in terms of accuracy, to allow comparison with reported values.</p><p>It can be seen that our models performed best on 7 out of the 11 datasets, achieving an average rank of 1.7, followed by ROCKET, which performed best on 3 datasets and on average ranked 2.3th. The dilation-CNN <ref type="bibr" target="#b12">(Franceschi et al., 2019)</ref> and XGBoost, which performed best on the remaining 1 dataset, tied and on average ranked 3.7th and 3.8th respectively. Interestingly, we observe that all datasets on which ROCKET outperformed our model were very low dimensional (specifically, 3-dimensional). Although our models still achieved the second best performance for UWaveGes-tureLibrary, in general we believe that this indicates a relative weakness of our current models when dealing with very low dimensional time series. As discussed in Section 3.1, this may be due to the problems introduced by a low-dimensional representation space to the attention mechanism, as well as the added positional embeddings; to mitigate this issue, in future work we intend to use a 1Dconvolutional layer to extract more meaningful representations of low-dimensional input features (see Section 3.1). Conversely, our models performed particularly well on very high-dimensional datasets (FaceDetection, HeartBeat, InsectWingBeat, PEMS-SF), and/or datasets with relatively more training samples. As a characteristic example, on InsectWingBeat (which is by far the largest dataset with 30k samples and contains time series of 200 dimensions and highly irregular length) our model reached an accuracy of 0.687, while all other methods performed very poorly, with the possible exception of XGBoost (0.369). However, we note that our model performed exceptionally well also on datasets with only a couple of hundred samples, which in fact constitute 8 out of the 11 examined classification datasets.</p><p>Finally, we observe that the pre-trained transformer models performed better than the fully supervised ones in 9 out of 11 datasets, although the advantage in most cases was relatively small.Again, no additional samples were available for unsupervised pre-training, so the benefit appears to originate from reusing the same samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ADDITIONAL POINTS &amp; FUTURE WORK</head><p>Execution time for training: While a precise comparison in terms of training time is well out of scope for the present work, in Section A.3 of the Appendix we demonstrate that our transformerbased method is economical in terms of its use of computational resources. Alternative self-attention schemes, such as sparse attention patterns <ref type="bibr" target="#b18">(Li et al., 2019)</ref>, recurrence <ref type="bibr" target="#b5">(Dai et al., 2019)</ref> or compressed (global-local) attention <ref type="bibr" target="#b2">(Beltagy et al., 2020)</ref>, can help drastically reduce the O(w 2 ) complexity of the self-attention layers with respect to the time series length w, which is the main performance bottleneck.</p><p>Imputation and forecasting: The model and training process described in Section 3.2 is exactly the setup required to perform imputation of missing values, without any modifications, and we observed that it was possible to achieve very good results following this method (see representative examples in Figure <ref type="figure">5</ref> of the Appendix); as a rough quantitative indication, our models could reach Root Mean Square Errors very close to 0 when asked to perform the input denoising (autoregressive) task on the test set, after being subjected to unsupervised pre-training on the training set. However, we defer a systematic quantitative comparison with the state of the art on commonly used imputation datasets to future work. Furthermore, we note that one may simply use different patterns of masking to achieve different objectives, while the rest of the model and setup remain the same. For example, using a mask which conceals the last part of all variables simultaneously, one may perform forecasting (see Figure <ref type="figure" target="#fig_2">4</ref> in Appendix), while for longer time series one may additionally perform this process within a sliding window. Again, we defer a systematic investigation to future work.</p><p>Extracted representations: The representations z t extracted by the transformer models can be used directly for evaluating similarity between time series, clustering, visualization and any other use cases where time series representations are used in practice. A valuable benefit offered by transformers is that representations can be independently addressed for each time step; this means that, for example, a greater weight can be placed at the beginning, middle or end of the time series, which allows to selectively compare time series, visualize temporal evolution of samples etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In Transformer models are based on a multi-headed attention mechanism that offers several key advantages and renders them particularly suitable for time series data:</p><p>? They can concurrently take into account long contexts of input sequence elements and learn to represent each sequence element by selectively attending to those input sequence elements which the model considers most relevant. They do so without position-dependent prior bias; this is to be contrasted with RNN-based models: a) even bi-directional RNNs treat elements in the middle of the input sequence differently from elements close to the two endpoints, and b) despite careful design, even LSTM (Long Short Term Memory) and GRU (Gated Recurrent Unit) networks practically only retain information from a limited number of time steps stored inside their hidden state (vanishing gradient problem <ref type="bibr" target="#b13">(Hochreiter, 1998;</ref><ref type="bibr" target="#b25">Pascanu et al., 2013)</ref>), and thus the context used for representing each sequence element is inevitably local. ? Multiple attention heads can consider different representation subspaces, i.e., multiple aspects of relevance between input elements. For example, in the context of a signal with two frequency components, 1/T 1 and 1/T 2 , one attention head can attend to neighboring time points, while another one may attend to points spaced a period T 1 before the currently examined time point, a third to a period T 2 before, etc. This is to be contrasted with attention mechanisms in RNN models, which learn a single global aspect/mode of relevance between sequence elements. ? After each stage of contextual representation (i.e., transformer encoder layer), attention is redistributed over the sequence elements, taking into account progressively more abstract representations of the input elements as information flows from the input towards the output. By contrast, RNN models with attention use a single distribution of attention weights to extract a representation of the input, and most typically attend over a single layer of representation (hidden states).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 EXECUTION TIME</head><p>On the same CPU machine, we recorded the times required for training our models, as well as the currently fastest and top performing (in terms of classification accuracy and regression error) baseline methods, ROCKET and XGBoost. These have been shown to be orders of magnitude faster than methods such as TS-CHIEF, Proximity Forest, Elastic Ensembles, DTW and HIVE-COTE, but also deep learning based methods <ref type="bibr" target="#b7">(Dempster et al., 2020)</ref>. As can be seen in Table <ref type="table">5</ref> in the Appendix, although XGBoost and ROCKET are on average must faster than the transformer on a CPU, ROCKET is typically faster only by a factor smaller than 10, and for higher dimensional datasets performs on par or even slower than the transformer. However, running our transformer models on a single GPU with a batch size of 64 -256 (depending on the size of each time series sample) offers a speedup of at least a factor of 10 (typically much higher). This means that exploiting commercial GPUs and the parallel processing capabilities of a transformer allows for at least as fast training times as the fastest currently available methods. In practice, although allowing for several . . . </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left: Generic model architecture, common to all tasks. The feature vector x t at each time step t is linearly projected to a vector u t of the same dimensionality d as the internal representation vectors of the model and is fed to the first self-attention layer to form the keys, queries and values after adding a positional encoding. Right: Training setup of the unsupervised pre-training task.We mask a proportion r of each variable sequence in the input independently, such that across each variable, time segments of mean length l m are masked, each followed by an unmasked segment of mean length l u = 1-r r l m . Using a linear layer on top of the final vector representations z t , at each time step the model tries to predict the full, uncorrupted input vectors x t ; however, only the predictions on the masked values are considered in the Mean Squared Error loss.</figDesc><graphic url="image-1.png" coords="4,108.00,81.86,395.99,225.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: Root Mean Squared Error of a fully supervised transformer (orange circles) and the same model pre-trained (blue diamonds) on the training set through the unsupervised objective and then fine-tuned on available labels, versus the proportion of labeled data in the training set. Right: Root Mean Squared Error of a given model as a function of the number of samples (here, shown as a proportion of the total number of samples in the training set) used for unsupervised pretraining. For supervised learning, two levels of label availability are depicted: 10% (blue circles) and 20% (orange squares) of all training data labels. Note that a horizontal axis value of 0 means fully supervised learning only, while all other values correspond to unsupervised pre-training followed by supervised fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Masking scheme for implementation of forecasting objective within our transformer encoder framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance on multivariate regression datasets, in terms of Root Mean Squared Error. Bold indicates best values, underlining indicates second best.</figDesc><table><row><cell>Root MSE</cell></row><row><cell>Ours</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accuracy on multivariate classification datasets. Bold indicates best and underlining second best values. A dash indicates that the corresponding method failed to run on this dataset.</figDesc><table><row><cell></cell><cell cols="2">Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="7">TST (pretrained) TST (sup. only) Rocket XGBoost LSTM Frans. et al DTW D</cell></row><row><cell>EthanolConcentration</cell><cell>0.323</cell><cell>0.319</cell><cell>0.452</cell><cell>0.437</cell><cell>0.323</cell><cell>0.289</cell><cell>0.323</cell></row><row><cell>FaceDetection</cell><cell>0.687</cell><cell>0.678</cell><cell>0.647</cell><cell>0.633</cell><cell>0.577</cell><cell>0.528</cell><cell>0.529</cell></row><row><cell>Handwriting</cell><cell>0.356</cell><cell>0.328</cell><cell>0.588</cell><cell>0.158</cell><cell>0.152</cell><cell>0.533</cell><cell>0.286</cell></row><row><cell>Heartbeat</cell><cell>0.776</cell><cell>0.757</cell><cell>0.756</cell><cell>0.732</cell><cell>0.722</cell><cell>0.756</cell><cell>0.717</cell></row><row><cell>JapaneseVowels</cell><cell>0.997</cell><cell>0.994</cell><cell>0.962</cell><cell>0.865</cell><cell>0.797</cell><cell>0.989</cell><cell>0.949</cell></row><row><cell>InsectWingBeat</cell><cell>0.687</cell><cell>0.684</cell><cell>-</cell><cell>0.369</cell><cell>0.176</cell><cell>0.16</cell><cell>-</cell></row><row><cell>PEMS-SF</cell><cell>0.890</cell><cell>0.919</cell><cell>0.751</cell><cell>0.983</cell><cell>0.399</cell><cell>0.688</cell><cell>0.711</cell></row><row><cell>SelfRegulationSCP1</cell><cell>0.918</cell><cell>0.925</cell><cell>0.908</cell><cell>0.846</cell><cell>0.689</cell><cell>0.846</cell><cell>0.775</cell></row><row><cell>SelfRegulationSCP2</cell><cell>0.600</cell><cell>0.611</cell><cell>0.533</cell><cell>0.489</cell><cell>0.466</cell><cell>0.556</cell><cell>0.539</cell></row><row><cell>SpokenArabicDigits</cell><cell>0.998</cell><cell>0.993</cell><cell>0.712</cell><cell>0.696</cell><cell>0.319</cell><cell>0.956</cell><cell>0.963</cell></row><row><cell>UWaveGestureLibrary</cell><cell>0.913</cell><cell>0.903</cell><cell>0.944</cell><cell>0.759</cell><cell>0.412</cell><cell>0.884</cell><cell>0.903</cell></row><row><cell>Avg Accuracy (excl. InsectWingBeat)</cell><cell>0.745</cell><cell>0.743</cell><cell>0.725</cell><cell>0.659</cell><cell>0.486</cell><cell>0.703</cell><cell>0.669</cell></row><row><cell>Avg Rank</cell><cell>1.7</cell><cell></cell><cell>2.3</cell><cell>3.8</cell><cell>5.4</cell><cell>3.7</cell><cell>4.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>this work we propose a novel framework for multivariate time series representation learning based on the transformer encoder architecture. The framework includes an unsupervised pre-training scheme, which we show that can offer substantial performance benefits over fully supervised learning, even without leveraging additional unlabeled data, i.e., by reusing the same data samples. By evaluating our framework on several public multivariate time series datasets from various domains and with diverse characteristics, we demonstrate that it is currently the best performing method for regression and classification, even for datasets where only a few hundred training samples are available.</figDesc><table><row><cell>Dataset</cell><cell cols="5">TrainSize TestSize NumDimensions SeriesLength NumClasses</cell></row><row><cell cols="2">EthanolConcentration 261</cell><cell>263</cell><cell>3</cell><cell>1751</cell><cell>4</cell></row><row><cell>FaceDetection</cell><cell>5890</cell><cell>3524</cell><cell>144</cell><cell>62</cell><cell>2</cell></row><row><cell>Handwriting</cell><cell>150</cell><cell>850</cell><cell>3</cell><cell>152</cell><cell>26</cell></row><row><cell>Heartbeat</cell><cell>204</cell><cell>205</cell><cell>61</cell><cell>405</cell><cell>2</cell></row><row><cell>InsectWingbeat</cell><cell>30000</cell><cell>20000</cell><cell>200</cell><cell>30</cell><cell>10</cell></row><row><cell>JapaneseVowels</cell><cell>270</cell><cell>370</cell><cell>12</cell><cell>29</cell><cell>9</cell></row><row><cell>PEMS-SF</cell><cell>267</cell><cell>173</cell><cell>963</cell><cell>144</cell><cell>7</cell></row><row><cell>SelfRegulationSCP1</cell><cell>268</cell><cell>293</cell><cell>6</cell><cell>896</cell><cell>2</cell></row><row><cell>SelfRegulationSCP2</cell><cell>200</cell><cell>180</cell><cell>7</cell><cell>1152</cell><cell>2</cell></row><row><cell>SpokenArabicDigits</cell><cell>6599</cell><cell>2199</cell><cell>13</cell><cell>93</cell><cell>10</cell></row><row><cell cols="2">UWaveGestureLibrary 120</cell><cell>320</cell><cell>3</cell><cell>315</cell><cell>8</cell></row><row><cell></cell><cell cols="4">Table 4: Multivariate Classification Datasets</cell><cell></cell></row><row><cell cols="3">A.2 ADVANTAGES OF TRANSFORMERS</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Although equation 1 shows the operation for a single time step for clarity, all input vectors are embedded concurrently by a single matrix-matrix multiplication</p></note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of labels</head><p>Unsup. pretrained Supervised</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Published as a conference paper at ICLR 2021 first decreases rapidly as we use more samples for unsupervised pre-training, and then momentarily increases, before it decreases again (for clarity, the same graphs are shown separately in Figure <ref type="figure">3</ref> in the Appendix). Consistent with our observations above, it is interesting to again note that, for a given number of labeled samples, even reusing a subset of the same samples for unsupervised pretraining improves performance: for the 1250 labels (blue diamonds of the right panel of Figure <ref type="figure">2</ref> or left panel of Figure <ref type="figure">3</ref> in the Appendix) this can be observed in the horizontal axis range [0, 0.1], and for the 2500 labels (blue diamonds of the right panel of Figure <ref type="figure">2</ref> or right panel of Figure <ref type="figure">3</ref>    </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bostrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Large</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="606" to="660" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The UEA multivariate time series classification archive</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Large</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bostrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eamonn</forename><surname>Southam</surname></persName>
		</author>
		<author>
			<persName><surname>Keogh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00075</idno>
		<idno>arXiv: 1811.00075</idno>
		<imprint>
			<date type="published" when="2018-10">2018. October 2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<idno>arXiv: 2004.05150</idno>
		<ptr target="http://arxiv.org/abs/2004.05150" />
		<title level="m">The Long-Document Transformer</title>
		<imprint>
			<date type="published" when="2020-04">April 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning representations of multivariate time series with missing data</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Filippo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Livi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>?yvind Mikalsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kampffmeyer</surname></persName>
		</author>
		<author>
			<persName><surname>Jenssen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2019.106973</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<idno type="ISSN">0031-3203</idno>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">106973</biblScope>
			<date type="published" when="2019-12">December 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kr?ger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tom Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<idno>ArXiv, abs/2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<idno>arXiv: 1901.02860</idno>
		<ptr target="http://arxiv.org/abs/1901.02860" />
		<title level="m">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</title>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">GRU-ODE-Bayes: Continuous Modeling of Sporadically-Observed Time Series</title>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">De</forename><surname>Brouwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaak</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Arany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yves</forename><surname>Moreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>D\textquotesingle Alch?-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="7379" to="7390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ROCKET: exceptionally fast and accurate time series classification using random convolutional kernels</title>
		<author>
			<persName><forename type="first">Angus</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franccois</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-020-00701-z</idno>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR, abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">4805</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">eprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Fawaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlotte</forename><surname>Pelletier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Idoumghar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Alain</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franccois</forename><surname>Petitjean</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-020-00710-y</idno>
	</analytic>
	<monogr>
		<title level="j">InceptionTime: Finding AlexNet for Time Series Classification. ArXiv</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning for time series classification: a review</title>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Fawaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germain</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lhassane</forename><surname>Idoumghar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Alain</forename><surname>Muller</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-019-00619-1</idno>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="917" to="963" />
			<date type="published" when="2019-07">July 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Vincent Fortuin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>H?ser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName><surname>R?tsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOM-VAE: Interpretable Discrete Representation Learning on Time Series</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised Scalable Representation Learning for Multivariate Time Series</title>
		<author>
			<persName><forename type="first">Jean-Yves</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aymeric</forename><surname>Dieuleveut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>D\textquotesingle Alch?-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4650" to="4661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="DOI">10.1142/S0218488598000094</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Uncertain. Fuzziness Knowl.-Based Syst</title>
		<idno type="ISSN">0218-4885</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="1998-04">April 1998</date>
			<publisher>World Scientific Publishing Co., Inc</publisher>
			<pubPlace>NJ, USA Publisher</pubPlace>
		</imprint>
	</monogr>
	<note>Place: River Edge</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Music transformer: Generating music with long-term structure</title>
		<author>
			<persName><forename type="first">Cheng-Zhi Anna</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Semantic Audio Representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ratheet</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2018.8461684</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Mixture-of-Experts Variational Autoencoder for clustering and generating from similarity-based representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><surname>Vincent Fortuin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ram</forename><surname>Vignesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Somnath</surname></persName>
		</author>
		<author>
			<persName><surname>Claassen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Similarity Preserving Representation Learning for Time Series Analysis</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vacul?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting</title>
		<author>
			<persName><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyou</forename><surname>Yao Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5243" to="5253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Time Series Classification with HIVE-COTE</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">J</forename><surname>Bagnall</surname></persName>
		</author>
		<idno type="DOI">10.1145/3182382</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowl. Discov. Data</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<idno>arXiv: 1908.03265</idno>
		<ptr target="http://arxiv.org/abs/1908.03265" />
		<title level="m">On the Variance of the Adaptive Learning Rate and Beyond</title>
		<imprint>
			<date type="published" when="2020-04">April 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Proximity Forest: An effective and scalable distancebased classifier for time series</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Shifaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlotte</forename><surname>Pelletier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Lachlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nayyar</forename><surname>Neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Goethals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName><surname>Webb</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-019-00617-3</idno>
		<idno type="arXiv">arXiv:1808.10594</idno>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<idno type="ISSN">1384-5810</idno>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1573" to="1756" />
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving Clinical Predictions through Unsupervised Time Series Representation Learning</title>
		<author>
			<persName><forename type="first">Xinrui</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hueser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><forename type="middle">L</forename><surname>Hyland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Zerveas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Raetsch</surname></persName>
		</author>
		<idno>1812.00490</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NeurIPS 2018 Workshop on Machine Learning for Health</title>
		<meeting>the NeurIPS 2018 Workshop on Machine Learning for Health</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">eprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cdsa: Cross-dimensional self-attention for multivariate, geo-tagged time series imputation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vetro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<idno>ArXiv, abs/1905.09904</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">TimeNet: Pre-trained deep recurrent neural network for time series classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Malhotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vishnu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puneet</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shroff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ESANN</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>III-1310-III-1318</idno>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on International Conference on Machine Learning<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-06">June 2013</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
	<note>ICML&apos;13</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv, abs/1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><surname>Powernorm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07845</idno>
		<idno>arXiv: 2003.07845</idno>
		<title level="m">Rethinking Batch Normalization in Transformers</title>
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">TS-CHIEF: a scalable and accurate forest algorithm for time series classification</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Shifaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlotte</forename><surname>Pelletier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-020-00679-8</idno>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bergmeir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Franc ?ois Petitjean</surname></persName>
		</author>
		<author>
			<persName><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UEA</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>Monash University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Chang</forename><surname>Wei Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Bergmeir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12672</idno>
		<title level="m">Time series regression</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep transformer models for time series forecasting: The influenza prevalence case</title>
		<author>
			<persName><forename type="first">Neo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn O'</forename><surname>Banion</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Deep Neural Network for Unsupervised Anomaly Detection and Diagnosis in Multivariate Time Series Data</title>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuncong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lumezanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33011409</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
