<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generalizable Person Re-identification with Relevance-aware Mixture of Experts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-19">19 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yongxing</forename><surname>Dai</surname></persName>
							<email>yongxingdai@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Lab for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaotong</forename><surname>Li</surname></persName>
							<email>lixiaotong@stu.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Lab for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
							<email>liu@sutd.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zekun</forename><surname>Tong</surname></persName>
							<email>zekuntong@u.nus.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Engineering Lab for Video Technology</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generalizable Person Re-identification with Relevance-aware Mixture of Experts</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-19">19 May 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2105.09156v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain generalizable (DG) person re-identification (ReID) is a challenging problem because we cannot access any unseen target domain data during training. Almost all the existing DG ReID methods follow the same pipeline where they use a hybrid dataset from multiple source domains for training, and then directly apply the trained model to the unseen target domains for testing. These methods often neglect individual source domains' discriminative characteristics and their relevances w.r.t. the unseen target domains, though both of which can be leveraged to help the model's generalization. To handle the above two issues, we propose a novel method called the relevance-aware mixture of experts (RaMoE), using an effective voting-based mixture mechanism to dynamically leverage source domains' diverse characteristics to improve the model's generalization. Specifically, we propose a decorrelation loss to make the source domain networks (experts) keep the diversity and discriminability of individual domains' characteristics. Besides, we design a voting network to adaptively integrate all the experts' features into the more generalizable aggregated features with domain relevance. Considering the target domains' invisibility during training, we propose a novel learning-to-learn algorithm combined with our relation alignment loss to update the voting network. Extensive experiments demonstrate that our proposed RaMoE outperforms the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, the research on person re-identification (ReID) has been appealing to academia and industry. The goal of ReID is to identify a person across different camera views. Many works on fully supervised ReID <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b7">8]</ref> have achieved quite promising performances when train- ing and testing under the same domain (dataset). However, when applying these well-trained ReID models to other domains, the performance often drops significantly because of the domain biases <ref type="bibr" target="#b51">[52]</ref>. To tackle this problem, some researchers have studied unsupervised domain adaptation (UDA) methods <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10]</ref>, which utilize the unlabeled target data to finetune and adapt the source-trained model to the target domain. However, existing UDA ReID methods are often not powerful enough to deal with practical application scenarios, because it is sometimes hard to collect target domain training data and time-consuming to finetune the model on these unlabeled samples. As a result, domain generalizable (DG) ReID <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> has been appealing to researchers recently. Generally, DG ReID methods utilize labeled data from multiple source domains to learn a generalizable model for new unseen target domains, without using any target domain data for training. To obtain more generalizable models for unseen target domains, we are devoted to the problem of DG ReID in this paper.</p><p>Almost all the existing DG ReID methods <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30</ref>] follow the same pipeline, where they collect all source domain data into a hybrid dataset and train a single model on it, as shown in Fig. <ref type="figure" target="#fig_0">1 (a)</ref>. During testing, they usually use the same well-trained model to extract features for any unseen target domain. However, there can be two potential problems in such a pipeline: (1) They learn a common feature space for different domains, which may neglect individual domains' discriminative characteristics. Such diverse domain-specific characteristics have been shown to be able to provide complementary information for better generalization on target domains, as mentioned in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b66">67]</ref>. <ref type="bibr" target="#b1">(2)</ref> Conventional DG ReID methods often ignore the specific target domain's inherent relevance w.r.t. different source domains. They are difficult to generalize the model to the unseen target domain because the model trained on the more relevant source domains can provide more discriminative and meaningful information than those less relevant domains. However, such relevance is often not explicitly considered by existing works <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Recently, works <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b20">21]</ref> on the mixture of experts <ref type="bibr" target="#b27">[28]</ref> (MoE) show that MoE can improve the overall model's capability by mixing multiple networks (i.e., leveraging expterts' complementary information) with a voting procedure. Inspired by this, we propose a novel approach called Relevance-aware Mixture of Experts (RaMoE), as shown in Fig. <ref type="figure" target="#fig_0">1</ref> (b), to handle the above two issues (i.e., complementary information and domain relevance). We argue that, instead of learning a single model on the hybrid domains, we can train a domain-specific network (domain expert) for each source domain to exploit individual domains' discriminative and powerful characteristics. Thus, these domain experts' mixture can keep source domains' diversity and provide rich complementary information, improving the generalization on target domains. Subsequently, we propose an adaptive voting network to calculate the unseen target domain's relevance w.r.t. all source domains. Based on the domain relevance, we can adaptively integrate those source experts' features into the aggregated features by voting. The voting network will assign the more relevant domain experts with higher weights. Thus, those more relevant experts will provide more complementary information to improve the aggregated features' generalizability on the target domain.</p><p>Specifically, in our RaMoE method, we propose a decorrelation loss to encourage source domain experts to keep their domains' diverse characteristics, and thus they can provide complementary and discriminative information. Such a decorrelation loss is implemented by minimizing the correlation among the source domain experts because the lower correlation among experts will bring about more com-plementary information, as mentioned in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">41]</ref>. Because the target domain is totally unseen during training in DG ReID, it is challenging for the adaptive voting network to well learn the target domain's correct relevance w.r.t. source domains. Inspired by meta-learning (learning-to-learn) that can improve the model's generalization <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23]</ref> for the unseen target domains in an episodic training paradigm, we propose a novel learning-to-learn algorithm to learn our adaptive voting network. At the beginning of each episodic training iteration, we randomly split source domains into the meta-train (simulated "source domains") and the metatest (simulated "unseen target domains") to simulate the adaptive voting procedure for the unseen target domain. During each episodic training iteration, the meta-test first obtains the relevance w.r.t. the meta-train using the adaptive voting network. The meta-test can then get two kinds of features: one is the features extracted by the meta-test domain expert, and the other is the aggregated features integrated from multiple meta-train domain experts with the relevance. We propose the relation alignment loss to push the aggregated features to be as discriminative as the features extracted by the meta-test expert. As a result, our RaMoE method can generate very discriminative and generalizable aggregated features for the unseen target domains by adaptively integrating diverse domain experts with the domain relevance.</p><p>Our major contributions can be summarized as follows: <ref type="bibr" target="#b0">(1)</ref> We propose a novel RaMoE method to tackle the problem of DG ReID by exploiting source domains' complementary information and their relevance w.r.t. the unseen target domain. <ref type="bibr" target="#b1">(2)</ref> We propose the decorrelation loss to keep source domains' diversity and encourage source domain experts to provide more complementary and discriminative information. <ref type="bibr" target="#b2">(3)</ref> To make the model more generalizable to target domains, we propose a voting network to adaptively integrate source domain experts' features into the aggregated features. Specially, the adaptive voting network is updated with the relation alignment loss in a novel learning-tolearn way. ( <ref type="formula" target="#formula_4">4</ref>) Extensive experiments demonstrate that our method outperforms state-of-the-art DG ReID approaches by a large margin.</p><p>To the best of our knowledge, this is the first work that treats DG ReID as a novel mixture-of-experts paradigm via an effective voting-based mixture mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Person Re-Identification. Deep supervised person ReID has made great progress in recent years, including but not limited to deep metric learning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b46">47]</ref>, part-based methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b21">22]</ref>, and attention network learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b58">59</ref>]. To handle the problem of domain biases <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b51">52]</ref> in ReID, researchers proposed unsupervised domain adaptation (UDA) methods <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61]</ref>. ) and meta-train (e.g., D2, ..., DK ). A meta-test image can obtain K features (one feature from its own domain expert and K − 1 features from meta-train domain experts), together with a query feature obtained by the voting network. The meta-test domain's relevance w.r.t. the meta-train can be obtained by calculating the mean similarity between the meta-test query feature and the meta-train prototypes. We can obtain the weighted aggregated feature by adaptively integrating meta-train experts' features with the relevance. " " is the operation of weighting features with the domain relevance. The relation alignment loss is proposed to push the weighted aggregated feature as discriminative as the meta-test domain-specific feature. " " is the features' operation: concatenation or element-wise summation.</p><p>Very recently, researchers started to study the topic of domain generalization (DG) in ReID <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>, which learns the generalizable ReID models on multi-source domains without using any target training data, and tests on unseen target domains. Song et al. <ref type="bibr" target="#b44">[45]</ref> proposed the problem of domain generalization in ReID and designed the Domain-Invariant Mapping Network combined with a memory bank to learn domain-invariant features. Jia et al. <ref type="bibr" target="#b28">[29]</ref> utilized Instance Normalization <ref type="bibr" target="#b48">[49]</ref> to learn a more generalizable model. Jin et al. <ref type="bibr" target="#b29">[30]</ref> proposed Style Normalization and Restitution modules to disentangle the identity-relevant and identity-irrelevant features. Different from all the above DG ReID works, we propose a novel RaMoE method by utilizing individual source domains' diverse and discriminative characteristics and the unseen target domain's relevance w.r.t. source domains in order to adaptively improve the model's generalization on unseen target domains. Domain Generalization. The goal of general DG is to improve the model generalization in an arbitrary domain for image classification by training from multi-source domains. Existing DG methods can be mainly categorized into three aspects. (1) Learning domain-invariant features <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34]</ref>: These methods assume that minimizing the domain discrepancy between multi-source domains can help learn domain-invariant features which are robust for unseen target domains. (2) Augmenting source data <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b65">66]</ref>: These methods augment the source domain data to increase the domain diversity, thus the source-trained model will be more robust to unseen target domains. (3) Optimizing with meta-learning <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b11">12]</ref>: These methods adopted the episodic training paradigm to split the source domains into meta-train and meta-test to simulate the domain bias, so as to improve the model generalization. The above general DG methods mainly focus on image classification where the target domains and the source domains share the same label space. Thus, these methods can not be directly applied to the task of DG ReID, since in ReID, the identities/classes of the target domains are usually totally different from source domains.</p><p>Mixture of Experts. Jacobs et al. <ref type="bibr" target="#b27">[28]</ref> first introduced the mixture of experts (MoE). MoE aims to learn a system composed of many separated networks (experts), where each expert learns to handle a subset of the whole dataset. Recently, deep MoE methods have shown their superiority in image recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b50">51]</ref>, machine translation <ref type="bibr" target="#b43">[44]</ref>, scene parsing <ref type="bibr" target="#b13">[14]</ref> and so on. Unlike these works, we design a learnable voting network that can be updated with a novel meta-learning algorithm. By integrating all the experts using our designed voting network, we can well leverage the complementary information of those relevant domains' experts to improve the features' generalization in DG ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this work, we aim to train a group of experts that are capable of learning discriminative features from their indi-vidual domains. When facing an unseen target domain, the mixture of these domain experts can be trained to vote based on their relevances w.r.t. the target domain. By adaptively integrating all the source experts' features into aggregated features with the relevance, our RaMoE can achieve an optimal generalization performance on the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>The pipeline of our proposed RaMoE is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. During training, we can access K source domains' labeled datasets</p><formula xml:id="formula_0">D = {D k } K k=1 , where D k = {(x k n , y k n )} N k n=1 , (x k n , y k n</formula><p>) is a labeled sample and N k is the number of labeled images in the k-th domain. After the backbone F ψ (e.g., ResNet50), we design K branch networks (termed as "source domain experts") {M φ k } K k=1 , and a voting network Q θ . The metric loss L metric makes each expert focus on learning its domain-specific features. The decorrelation loss L decor is used to keep source domains' diverse characteristics and encourage all the domain experts to provide complementary information. We use the k-th domain's class centers C k = {c k l } L k l=1 as the prototypes to represent the k-th domain's characteristics, where L k is the number of person identities in the k-th domain.</p><p>We propose a novel meta-learning algorithm combined with the relation alignment loss L relation to update the voting network. K source domains are randomly split into a meta-test domain and K − 1 meta-train domains at each episodic training iteration. For a meta-test image, it can obtain K + 1 features, including (1) a feature extracted by the meta-test expert, (2) K − 1 features extracted by the meta-train experts, (3) a query feature extracted by the voting network. The meta-test domain's relevance w.r.t. the meta-train domains can be calculated by the mean similarity between the query feature and the meta-train domains' prototypes. We can obtain the weighted aggregated feature by integrating K − 1 meta-train expert features based on their relevance. The relation alignment loss is proposed to push the weighted aggregated feature to be as discriminative as the meta-test feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Optimizing Domain-specific Experts</head><p>As mentioned in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b66">67]</ref>, exploiting the complementary information of discriminative experts helps improve the overall model's generalization on target domains. Thus, the domain experts should satisfy two properties: discriminability and complementarity. We use the metric loss to improve every domain-specific expert's discriminability. Similar to <ref type="bibr" target="#b40">[41]</ref>, we mutually reduce all the domain experts' correlation to improve the complementarity among them. Specifically, we propose a decorrelation loss to decorrelate all these domain experts' features.</p><p>Metric Loss. Similar to <ref type="bibr" target="#b37">[38]</ref>, we use the classification loss L cls , triplet loss <ref type="bibr" target="#b24">[25]</ref> L tri , and center loss <ref type="bibr" target="#b53">[54]</ref> L cent to optimize K domain-specific experts {M φ k } K k=1 , the domain-specific prototypes {C k } K k=1 , and the backbone network F ψ . We combine the above metric losses as:</p><formula xml:id="formula_1">Lmetric = L cls + Ltri + λLcent,<label>(1)</label></formula><p>where λ (set as 5 × 10 −4 ) is the weighting hyper-parameter. Decorrelation Loss. For an image x k n (where n = 1, 2, ..., N k ) from the k-th domain, we use all the experts to extract K features {m j n } K j=1 that are characterized by individual domains, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. To improve the aggregated features' generalization, we encourage these experts to provide more complementary and discriminative information. Specifically, we propose the decorrelation loss by reducing the correlation among different domain experts. We formulate the decorrelation loss as follows:</p><formula xml:id="formula_2">L decor = 1 N k N k n=1 ( 1 K − 1 j =k ||m k n m j n ||),<label>(2)</label></formula><p>where features {m j n } K j=1 are all L2-normalized, means the point-wise product and • is the L2-norm of a vector.</p><p>We combine Eq. ( <ref type="formula" target="#formula_1">1</ref>) (2) into the domain loss by:</p><formula xml:id="formula_3">L domain = Lmetric + L decor .<label>(3)</label></formula><p>Thus, by alternating k from 1 to K, we can obtain a group of representative and complementary domain experts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimizing the Voting Network</head><p>To make the model more generalizable to the unseen target domain, we leverage the specific target domain's relevance w.r.t. all source domains. Specifically, we propose a voting network to calculate the domain relevance adaptively. By integrating all the source domain experts' features into a weighted aggregated feature with relevance, we can achieve more generalizable features for an unseen target domain during testing. Because the target domain data is unavailable during training, we propose a learningto-learn algorithm to simulate integrating multi-source experts' features with the relevance. The voting network can be updated with a relation alignment loss introduced below. Thus, we can learn a generalizable voting network for an unseen target domain, integrating multi-source experts' features adaptively. Specifically, we split the K source domains into meta-train (simulated "source domains") D s including K − 1 domains, and the meta-test (simulated "the unseen target domain") D u including the remaining domain, at every episodic training iteration.</p><p>Relation Alignment Loss. As mentioned before, for a k-th domain's image x k n (where n = 1, 2, ..., N k ), we can obtain K features {m j n } K j=1 extracted by K experts {M φj (•)} K j=1 , and a query feature q k n extracted by the voting network Q θ (•), as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>We use the query feature q k n to calculate the domain relevance score of the k-th domain's image x k n w.r.t. the j-th domain (j = k) by:</p><formula xml:id="formula_4">s j n = 1 Lj L j l=1 q k n , c j l ,<label>(4)</label></formula><p>where q k n , c j l is the inner product between the query feature q k n and the l-th class prototype c j l (where l = 1, 2, ..., L j ) in the j-th domain. Both q k n and c j l are L2normalized. As a result, we can get the relevance set {s j n } K j=1,j =k of the image x k n w.r.t. all other K − 1 domains. Thus, for a k-th domain image x k n , we can then integrate other K − 1 irrelevant experts' features {m j n } K j=1,j =k into the weighted aggregated feature v n with the relevance s j n by:</p><formula xml:id="formula_5">vn = j =k σ(s j n ) • m j n ,<label>(5)</label></formula><p>where σ(•) is the non-linear function (e.g., sigmoid or softmax) to normalize the relevance between 0 and 1. Softmax-triplet function <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b55">56]</ref> has been shown to be a powerful tool to measure the metric relationship in the feature space (i.e., inter-sample discriminability). Thus we use it to measure the metric relationship of the weighted aggregated feature v n as below:</p><formula xml:id="formula_6">R(vn) = exp( vn − v + n ) exp( vn − v + n ) + exp( vn − v − n ) ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_7">R(•) ∈ [0, 1],</formula><p>• is the L2-norm of a vector, and v + n and v − n are the selected features of the hardest positive and negative samples within a mini-batch. Similarly, for the kth expert's feature m k n we can also use Eq. ( <ref type="formula" target="#formula_6">6</ref>) obtain the metric relationship R(m k n ). Compared with other K − 1 domain experts, the k-th domain expert should be able to generate more discriminative feature for the sample x k n , while such metric relationship R(m k n ) reflects the k-th domain-specific discriminative characteristics. Thus, we push the weighted aggregated feature v n to be as discriminative as the k-th domainspecific feature m k n , and meanwhile, enable the weighted aggregated feature to be characterized by the k-th domain, we propose the relation alignment loss below:</p><formula xml:id="formula_8">L relation = 1 N k N k n=1 L bce (R(vn), R(m k n )),<label>(7)</label></formula><p>where L bce is the binary cross-entropy loss. By minimizing Eq. ( <ref type="formula" target="#formula_8">7</ref>), the voting network is pushed to learn to produce reliable relevance scores. Thus, the model can learn powerful generalization capabilities for unseen target domains, by exploiting how to integrate source domains. Meta Optimizing. Since we cannot access the unseen target domain samples, we design a meta-learning scheme to optimize the above losses. At the meta-training stage, we use the meta-train D s to compute the domain loss with Eq. ( <ref type="formula" target="#formula_3">3</ref>) and the relation alignment loss with Eq. ( <ref type="formula" target="#formula_8">7</ref>) as: where ψ is the parameter of the backbone, φ s is the parameter of the domain-specific experts of D s , C s is the prototypes set of D s , and θ is the parameter of the voting network. Similar to <ref type="bibr" target="#b53">[54]</ref>, prototypes can be updated with the center loss in Eq. (1). Next, the updated parameters of the voting network is obtained by: θ ← θ − α∇ θ L s relation (θ), where α is the learning rate hyper-parameter. At the metatesting stage, we use the meta-test D u to compute the domain loss and relation alignment loss with Eq. (3) <ref type="bibr" target="#b6">(7)</ref>, which is formulated as follows:</p><formula xml:id="formula_9">L s =</formula><formula xml:id="formula_10">L u = L u domain (Du; ψ, φu, Cu) + L u relation (Du; ψ, φu, Cu, θ ),<label>(9)</label></formula><p>where φ u is the parameter of the D u expert, C u is the prototypes set of D u , and θ is the updated parameter with Eq. ( <ref type="formula">8</ref>). At the meta-optimizing stage, we optimize the voting network with the second-order gradient as follows:</p><formula xml:id="formula_11">θ ← θ − γ((1 − η)∇ θ L s relation (θ) + η∇ θ L u relation (θ )), (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where γ is the learning rate and η (set as 0.5) is the hyperparameter to balance the gradient of meta-train and metatest. The overall training procedure is shown in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Testing Procedure</head><p>During testing, for the unseen target domain dataset consisting of N samples {x n } N n=1 , we use Eq. ( <ref type="formula" target="#formula_4">4</ref>) to obtain the relevance of each target sample x n w.r.t. all K source domains, i.e., {s k n } K k=1 . Then, we can obtain the relevance of the unseen target domain w.r.t. the k-th source domain by</p><formula xml:id="formula_13">s k = 1 N N n=1 s k n .</formula><p>Each target sample x n can achieve K features {m k n } K k=1 using K domain experts. Similar to Eq. ( <ref type="formula" target="#formula_5">5</ref>), we adaptively integrate all K source domains' features with the relevance {s k n } K k=1 by:</p><formula xml:id="formula_14">vn = K k=1 σ(s k ) • m k n ,<label>(11)</label></formula><p>where the weighted aggregated features {v n } N n=1 are all L2normalized for evaluating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We use ResNet50 <ref type="bibr" target="#b23">[24]</ref> pretrained on ImageNet as our backbone. Similar to <ref type="bibr" target="#b37">[38]</ref>, the last residual layer's stride size is set as 1. After the global pooling layer we add an Embedding layer (i.e., FC: 2048d→512d) followed by batch normalization (BN) to get the ReID feature. The identity classifier (Classifier) followed by softmax function is added after BN to optimize with the classification loss. The above network is the structure of our Baseline. For efficiency, in our method, we make all the source domains share the same backbone and add a branch network (expert) for each source domain. Specifically, the structure of every domain expert is Embedding→BN→Classifier. The voting network can be easily implemented with FC→ReLU→BN, where FC is 2048d→512d. We resize the person image size to 256 × 128. For data augmentation, we perform random cropping, random flipping, and color jittering. Similar to <ref type="bibr" target="#b29">[30]</ref>, we discard random erasing (REA) because REA will degenerate the cross-domain ReID performance <ref type="bibr" target="#b37">[38]</ref>. The batch size is set to 64, including 16 identities and four images per identity. For our Baseline, we combine all the source domains into a hybrid dataset and only use the metric loss L metric for training. In our RaMoE method, we sample each source domain evenly at every training iteration. We optimize the model with the Adam optimizer. We train the model for 120 epochs and use the warmup strategy in the first ten epochs. The learning rate (i.e., α, β, γ in Alg. 1) is initialized as 3.5 × 10 −4 and divided by 10 at the 40th and 70th epochs respectively. We conduct all the experiments with PyTorch and train the model on four 1080Ti GPUs. The training and testing are efficient in our multi-head RaMoE method where the training and inference time of each batch are 0.708s and 0.312s respectively (batch size is 64).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets and Evaluation Settings.</head><p>Datasets and Evaluation Metrics. Following the previous works <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref> on DG ReID, we conduct our experiments on the public ReID or Pearson-Search datasets, including Market1501 <ref type="bibr" target="#b61">[62]</ref>, DukeMTMC-reID <ref type="bibr" target="#b62">[63]</ref>, CUHK02 <ref type="bibr" target="#b34">[35]</ref>, CUHK03 <ref type="bibr" target="#b35">[36]</ref>, MSMT17 <ref type="bibr" target="#b51">[52]</ref>, Table <ref type="table">1</ref>. Different evaluation protocols. The leave-one-out setting for M+D+C3+MT means selecting one domain for testing and the remaining three domains for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setting</head><p>Training Data Testing Data Protocol-1 M+D+C2+C3+CS PRID, GRID, VIPeR, iLIDs Protocol-2 M+D+C3+MT Protocol-3</p><p>Leave-one-out for M+D+C3+MT CUHK-SYSU <ref type="bibr" target="#b54">[55]</ref>, and four small ReID datasets including PRID <ref type="bibr" target="#b25">[26]</ref>, GRID <ref type="bibr" target="#b36">[37]</ref>, VIPeR <ref type="bibr" target="#b18">[19]</ref>, and iLIDs <ref type="bibr" target="#b52">[53]</ref>.</p><p>For CUHK03, we use the "labelled" dataset for training and adopt the protocol used in <ref type="bibr" target="#b63">[64]</ref> for testing. For simplicity, in the next sections we denote Market1501 as M, DukeMTMC-reID as D, CUHK02 as C2, CUHK03 as C3, MSMT17 as MT, and CUHK-SYSU as CS. We use the mean average precision (mAP) and Cumulative Matching Characteristics (CMC) for evaluation. Evaluation Protocols. There exist two evaluation protocols for DG ReID, as shown in Tab. 1. Under the setting of Protoco1-1 <ref type="bibr" target="#b44">[45]</ref>, all the images in these datasets M+D+C2+C3+CS (including the training and testing sets) are used for training. Four small ReID datasets (i.e., PRID, GRID, VIPeR, and iLIDs) are tested respectively, where the final performances of these small ReID datasets are evaluated on the average of 10 repeated random splits of gallery and probe sets. Under Protocol-2 <ref type="bibr" target="#b29">[30]</ref>, all the images in M+D+C3+MT (including the training and testing sets) are used for training and the testing sets are the same as Protocol-1. However, two disadvantages may lie in Protocol-1 and Protocol-2: (1) Compared with the existing ReID datasets, the number of images per identity in the CS dataset is much smaller, which will limit the learning of discriminative ReID features. <ref type="bibr" target="#b1">(2)</ref> The images' quality of the four small ReID datasets is low. The small datasets' performances can not correctly evaluate the model's generalizability in real scenarios, where the latter needs to be evaluated on large-scale datasets. As a result, we set a new protocol (i.e., Protocol-3 in Tab. 1) of the leave-one-out setting for the existing large-scale public datasets M+D+C3+MT. Specifically, the leave-one-out setting of M+D+C3+MT is selecting one domain from M+D+C3+MT for testing (only the testing set in this domain) and all the remaining domains for training (including the training and testing sets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with the State-of-the-Arts</head><p>Our proposed RaMoE can outperform the state-of-thearts methods by a large margin in the task of Domain Generalization (DG) ReID, as shown in Tab. 2. The Baseline method is training on the hybrid dataset including all source domains with only the metric loss L metric .</p><p>Comparison with DG ReID methods under the Protocol-1 and Protocol-2. We compare our method with the existing DG ReID methods under two different evaluation protocols. All the other methods directly apply the model trained on source domains to the unseen target do-Table <ref type="table">2</ref>. Comparison with state-of-the-arts methods in DG ReID under the setting of protocol-1 and protocol-2. We report the performances of the methods marked by " * " from <ref type="bibr" target="#b44">[45]</ref>. The best results are highlighted with bold. Comparison under the Protocol-3. We compare our proposed RaMoE with the Baseline method under the protocol-3 in Tab. 3. The performances on these large-scale ReID datasets have shown our method's superiority in integrating source domains' characteristics adaptively for better domain generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Effectiveness of the domain decorrelation. We propose the decorrelation loss L decor to encourage source domain experts to keep their diverse and discriminative characteristics. Thus, integrating these experts can provide complementary information to improve the aggregated features' generalization. As shown in Tab. 4, our method outperforms ours w/o decorrelation by 1.3% in Rank-1 on PRID. If learning source experts without the decorrelation loss, the experts will provide less complementary information and thus reduce the generalization of the aggregated features.</p><p>Effectiveness of the voting network. The voting network learned with meta-learning can adaptively provide the relevance of the target domain w.r.t. source domains, making those more relevant source domains provide more complementary information to improve the generalization of the weighted aggregated features. As shown in Tab. 4, our method outperforms ours w/o voting (Experts-ensemble) by 1.7%, 1.6%, 1.6%, 0.9% in mAP on PRID, GRID, VIPeR, and iLIDs respectively. Experts-ensemble means that the relevance of the target domain w.r.t. source domains is set 1, and all the experts' features are directly concatenated into the ensemble features. However, our method uses the domain relevance to integrate adaptively. Take the performances on iLIDs as an example, the Expert-M performs worst compared with other three experts (i.e., Expert-D/C3/MT) and the Expert-D performs best. Though directly mixing all these experts (i.e., Experts-ensemble) can bring about great performance gain, the methods w/o voting is inferior to our RaMoE significantly. It can demonstrate that the voting mechanism using the domain relevance can adaptively leverage those more relevant experts' complementary information and alleviate the influence of those less relevant experts.</p><p>Can individual domain experts provide complementary information to improve the features' generalization? We can keep all the source domains' diverse and discriminative characteristics using the decorrelation loss. Thus, all the source domain experts are encouraged to provide more complementary information. As shown in Tab. <ref type="bibr" target="#b3">4</ref>, almost all the experts (i.e., Expert-M/D/C3/MT) do not perform very well on different target domains. However, when integrating these experts' features, the aggregated features are superior to those extracted by individual experts. Thus, we can improve the overall features' generalization for unseen target domains by leveraging individual source domains' complementary information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How to integrate different source domain features?</head><p>As shown in Tab. 5, we compare different combinations of non-linear functions σ(•) and feature integrating types. The results show that the types of the non-linear function σ(•) in Eq. (5) will not bring about significant performance fluctuations. When concatenating features obtained by different source domain experts, the performance is better than summing features along with the corresponding dimensions, because the type of concatenating will keep more information   about the different feature dimensions. Thus, we choose the combination of "softmax" and "concat" to integrate source domains' features for our method in all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Extension</head><p>Evaluation on source domains. We use M, D, C3, and MT as source datasets, where only their training sets are used to train, and their testing sets are only used to test. In Tab. 6, Single-source Baseline means training and testing on the single domain; Multi-source Baseline means training on a hybrid dataset of all domains and testing on each domain separately. Comparing with them, the accuracy of RaMoE on source datasets does not drop but increases.</p><p>Extension to the online setting. We can easily extend our testing procedure to a more practical setting where the query set samples are given online, i.e., only the gallery samples are used to calculate the domain relevance in testing. We compare our method with this online setting in Tab. 7 and there is only very negligible performance drop when only using gallery to calculate the domain relevance.</p><p>Visualization. As shown in Fig. <ref type="figure">3</ref>, we visualize the domain relevance between the target domains (i.e., PRID, GRID, VIPeR, and iLIDs) w.r.t. the source domains (i.e., Market, Duke, CUHK03, and MSMT17), where the domain relevance is calculated with the manner mentioned in Sec. 3.4. In Fig. <ref type="figure">3</ref>, we can see that the unseen target domain's relevance w.r.t. all the source domains are different, and there exist some more relevant source domains for the unseen target domain. For the unseen target domain dataset iLIDs, its style is more similar to MSMT17 and Duke, and thus their relevances are higher than the other two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper proposes a novel approach called Relevanceaware Mixture of Experts (RaMoE) to tackle the problem of domain generalizable person ReID (DG ReID). By considering both the source domains' individual discriminative characteristics and the relevance of the unseen target domain w.r.t. source domains, we can obtain more generalizable features adaptively for the unseen target domain in DG ReID. Specifically, we propose the decorrelation loss to keep source domains' diverse and discriminative characteristics. Thus, these experts can provide more complementary information to improve the aggregated features' generalization. To obtain more accurate domain relevance of the unseen target domain w.r.t. source domains, we propose the voting network learned with the relation alignment loss in a meta-learning way. Extensive experiments show the effectiveness of our proposed RaMoE method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Differences between our method and the conventional DG ReID pipeline. (a) Conventional DG ReID methods generally train a single model on the hybrid dataset from multi-source domains and then apply the trained model to the unseen target domain for testing, which neglects individual domains' discriminative characteristics and target domain's relevance w.r.t. source domains. (b) Our method leverages the complementary information provided by all the source domain networks (also termed as "domain experts"). In testing, we integrate features obtained by source domain experts into an adaptive voting process based on the unseen target domain's relevance w.r.t. source domains.</figDesc><graphic url="image-1.png" coords="1,320.68,255.61,212.63,128.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Illustration of our method. The k-th branch network serves as the expert of the domain D k , and it is learned with the metric loss Lmetric and the decorrelation loss L decor . We use the learning-to-learn algorithm combined with the relation alignment loss L relation to update the voting network. At each episodic training iteration, we split K domains into the meta-test (e.g.,D1) and meta-train (e.g., D2, ..., DK ). A meta-test image can obtain K features (one feature from its own domain expert and K − 1 features from meta-train domain experts), together with a query feature obtained by the voting network. The meta-test domain's relevance w.r.t. the meta-train can be obtained by calculating the mean similarity between the meta-test query feature and the meta-train prototypes. We can obtain the weighted aggregated feature by adaptively integrating meta-train experts' features with the relevance. " " is the operation of weighting features with the domain relevance. The relation alignment loss is proposed to push the weighted aggregated feature as discriminative as the meta-test domain-specific feature. " " is the features' operation: concatenation or element-wise summation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Table 7 .Figure 3 .</head><label>73</label><figDesc>Figure 3. Visualization on the domain relevance.</figDesc><graphic url="image-44.png" coords="8,417.54,282.97,85.06,89.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Source domains D = {D k } K k=1 ; Learning rate hyperparameters α, β, γ; Balance hyperparameter η; MaxIters; MaxEpochs. Output: Backbone feature extractor F ψ ; Domain-specific experts {M φ k } K k=1 ; Prototypes {C k } K k=1 ; Voting network Q θ . 1 // For simplicity, we denote L domain and L relation as L d and Lr respectively.</figDesc><table><row><cell cols="2">2 for epoch = 1 to MaxEpochs do</cell></row><row><cell>3</cell><cell>for iter = 1 to MaxIters do</cell></row><row><cell>4</cell><cell>Sample K − 1 domains as meta-train Ds and the</cell></row><row><cell></cell><cell>remaining as meta-test Du;</cell></row><row><cell>5</cell><cell>Meta-training:</cell></row><row><cell>6</cell><cell>Compute losses for Ds: L s = L s d + L s r (θ);</cell></row><row><cell>7</cell><cell>Update the voting network parameters by:</cell></row><row><cell></cell><cell>θ ← θ − α∇ θ L s r (θ);</cell></row><row><cell>8</cell><cell>Meta-testing:</cell></row><row><cell>9</cell><cell>Compute losses for Du: L u = L u d + L u r (θ );</cell></row><row><cell>10</cell><cell>Optimizing:</cell></row><row><cell>11</cell><cell>ψ ← ψ − β∇ ψ (L s d + L u d );</cell></row><row><cell>16</cell><cell>end</cell></row><row><cell>17 end</cell><cell></cell></row></table><note>L s domain (Ds; ψ, φs, Cs) + L s relation (Ds; ψ, φs, Cs, θ), (8) Algorithm 1: Training Procedure of RaMoE Input: 12 (φs, Cs) ← (φs, Cs) − β∇ φs,Cs L s d ; 13 (φu, Cu) ← (φu, Cu) − β∇ φu,Cu L u d ; 14 Meta-optimizing 15 θ ← θ − γ((1 − η)∇ θ L s r (θ) + η∇ θ L u r (θ ));</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Comparisons under the setting of protocol-3.</figDesc><table><row><cell>Setting</cell><cell cols="2">Method</cell><cell>Reference</cell><cell></cell><cell cols="7">Target: PRID mAP Rank-1 mAP Rank-1 mAP Rank-1 mAP Rank-1 Target: GRID Target: VIPeR Target: iLIDs</cell></row><row><cell></cell><cell cols="2">Agg Align* [58]</cell><cell>arXiv 2017</cell><cell></cell><cell>25.5</cell><cell>17.2</cell><cell>24.7</cell><cell>15.9</cell><cell>52.9</cell><cell>42.8</cell><cell>74.7</cell><cell>63.8</cell></row><row><cell></cell><cell cols="2">Reptile* [40]</cell><cell>arXiv 2018</cell><cell></cell><cell>26.9</cell><cell>17.9</cell><cell>23.0</cell><cell>16.2</cell><cell>31.3</cell><cell>22.1</cell><cell>67.1</cell><cell>56.0</cell></row><row><cell></cell><cell cols="2">CrossGrad* [43]</cell><cell cols="2">ICLR 2018</cell><cell>28.2</cell><cell>18.8</cell><cell>16.0</cell><cell>8.96</cell><cell>30.4</cell><cell>20.9</cell><cell>61.3</cell><cell>49.7</cell></row><row><cell></cell><cell cols="2">Agg PCB* [48]</cell><cell cols="2">TPAMI 2019</cell><cell>32.0</cell><cell>21.5</cell><cell>44.7</cell><cell>36.0</cell><cell>45.4</cell><cell>38.1</cell><cell>73.9</cell><cell>66.7</cell></row><row><cell></cell><cell cols="2">MLDG* [32]</cell><cell cols="2">AAAI 2018</cell><cell>35.4</cell><cell>24.0</cell><cell>23.6</cell><cell>15.8</cell><cell>33.5</cell><cell>23.5</cell><cell>65.2</cell><cell>53.8</cell></row><row><cell>Protocol-1</cell><cell cols="2">PPA* [42]</cell><cell cols="2">CVPR 2018</cell><cell>45.3</cell><cell>31.9</cell><cell>38.0</cell><cell>26.9</cell><cell>54.5</cell><cell>45.1</cell><cell>72.7</cell><cell>64.5</cell></row><row><cell></cell><cell cols="2">DIMN* [45]</cell><cell cols="2">CVPR 2019</cell><cell>52.0</cell><cell>39.2</cell><cell>41.1</cell><cell>29.3</cell><cell>60.1</cell><cell>51.2</cell><cell>78.4</cell><cell>70.2</cell></row><row><cell></cell><cell>SNR [30]</cell><cell></cell><cell cols="2">CVPR 2020</cell><cell>66.5</cell><cell>52.1</cell><cell>47.7</cell><cell>40.2</cell><cell>61.3</cell><cell>52.9</cell><cell>89.9</cell><cell>84.1</cell></row><row><cell></cell><cell cols="2">Baseline RaMoE (Ours)</cell><cell cols="2">CVPR 2021</cell><cell>60.4 67.3</cell><cell>47.3 57.7</cell><cell>49.0 54.2</cell><cell>39.4 46.8</cell><cell>58.0 64.6</cell><cell>49.2 56.6</cell><cell>84.0 90.2</cell><cell>77.3 85.0</cell></row><row><cell></cell><cell>SNR [30]</cell><cell></cell><cell cols="2">CVPR 2020</cell><cell>60.0</cell><cell>49.0</cell><cell>41.3</cell><cell>30.4</cell><cell>65.0</cell><cell>55.1</cell><cell>91.9</cell><cell>87.0</cell></row><row><cell>Protocol-2</cell><cell cols="2">Baseline RaMoE (Ours)</cell><cell cols="2">CVPR 2021</cell><cell>58.9 66.8</cell><cell>47.2 56.9</cell><cell>47.7 53.9</cell><cell>38.1 43.4</cell><cell>63.8 72.2</cell><cell>54.7 63.4</cell><cell>89.2 92.3</cell><cell>84.2 88.4</cell></row><row><cell>Target: Market</cell><cell cols="5">mAP Rank-1 Rank-5 Rank-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>49.9</cell><cell>75.4</cell><cell>86.9</cell><cell>91.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RaMoE (Ours)</cell><cell>56.5</cell><cell>82.0</cell><cell>91.4</cell><cell>94.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Target: Duke</cell><cell cols="5">mAP Rank-1 Rank-5 Rank-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>49.4</cell><cell>65.8</cell><cell>79.0</cell><cell>83.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RaMoE (Ours)</cell><cell>56.9</cell><cell>73.6</cell><cell>85.3</cell><cell>88.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Target: CUHK03</cell><cell cols="5">mAP Rank-1 Rank-5 Rank-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>32.6</cell><cell>32.9</cell><cell>52.9</cell><cell>63.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RaMoE (Ours)</cell><cell>35.5</cell><cell>36.6</cell><cell>54.3</cell><cell>64.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Target: MSMT17</cell><cell cols="5">mAP Rank-1 Rank-5 Rank-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>9.9</cell><cell>24.5</cell><cell>35.4</cell><cell>40.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RaMoE (Ours)</cell><cell>13.5</cell><cell>34.1</cell><cell>46.0</cell><cell>51.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">main without considering the domain relevance. Compared</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">with them, our RaMoE can outperform them significantly.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>We study ablation studies on individual components of our method under the Protocol-2. Voting means learning the voting network with meta-learning by L relation and decorrelation means decorrelating source domain experts by L decor . Expert-M/D/C3/MT means using the feature extracted by Market/Duke/CUHK03/MSMT17 domain expert. Experts-ensemble means concatenating source domain experts' features directly without learning the domain relevance.</figDesc><table><row><cell>Method</cell><cell cols="8">Target: PRID mAP Rank-1 mAP Rank-1 mAP Rank-1 mAP Rank-1 Target: GRID Target: VIPeR Target: iLIDs</cell></row><row><cell>w/o voting (Expert-M)</cell><cell>62.2</cell><cell>53.4</cell><cell>50.4</cell><cell>39.8</cell><cell>66.1</cell><cell>56.9</cell><cell>87.9</cell><cell>82.7</cell></row><row><cell>w/o voting (Expert-D)</cell><cell>61.6</cell><cell>51.6</cell><cell>48.4</cell><cell>38.5</cell><cell>67.0</cell><cell>57.5</cell><cell>90.3</cell><cell>85.5</cell></row><row><cell>w/o voting (Expert-C3)</cell><cell>62.5</cell><cell>53.7</cell><cell>51.0</cell><cell>41.4</cell><cell>68.5</cell><cell>59.7</cell><cell>88.7</cell><cell>84.0</cell></row><row><cell>w/o voting (Expert-MT)</cell><cell>63.6</cell><cell>54.9</cell><cell>49.9</cell><cell>40.0</cell><cell>66.9</cell><cell>57.3</cell><cell>89.0</cell><cell>84.5</cell></row><row><cell>w/o voting (Experts-ensemble)</cell><cell>65.1</cell><cell>56.1</cell><cell>52.3</cell><cell>42.2</cell><cell>70.6</cell><cell>61.6</cell><cell>91.4</cell><cell>86.7</cell></row><row><cell>w/o decorrelation</cell><cell>66.0</cell><cell>55.6</cell><cell>53.2</cell><cell>42.9</cell><cell>71.3</cell><cell>62.8</cell><cell>91.2</cell><cell>87.0</cell></row><row><cell>RaMoE (Ours)</cell><cell>66.8</cell><cell>56.9</cell><cell>53.9</cell><cell>43.4</cell><cell>72.2</cell><cell>63.4</cell><cell>92.3</cell><cell>88.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Evaluating on different non-linear functions σ(•) and feature integrating types under the setting of Protocol-2.</figDesc><table><row><cell>Non-linear σ(•)</cell><cell cols="2">Integrating type</cell><cell cols="2">Target: GRID</cell></row><row><cell>softmax sigmoid</cell><cell>concat</cell><cell>sum</cell><cell>mAP</cell><cell>R1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>53.9</cell><cell>43.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>53.7</cell><cell>43.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>52.3</cell><cell>41.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>51.9</cell><cell>40.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table</head><label></label><figDesc>Evaluation of mAP within source domains. Baseline 81.8 71.6 62.0 46.6 Multi-source Baseline 82.6 74.4 64.3 48.0 RaMoE (Ours) 83.8 74.6 65.6 49.1</figDesc><table><row><cell>Method</cell><cell>M</cell><cell>D</cell><cell>C3</cell><cell>MT</cell></row><row><cell>Single-source</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work was supported by the National Natural Science Foundation of China under Grant 62088102, and in part by the PKU-NTU Joint Research Institute (JRI) sponsored by a donation from the Ng Teng Fong Charitable Foundation, and was partially supported by SUTD Project PIE-SGP-Al2020-02.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Network of experts for large-scale image categorization</title>
		<author>
			<persName><forename type="first">Karim</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Haris Baig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="516" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D'</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2229" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mixed highorder attention network for person re-identification</title>
		<author>
			<persName><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="371" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-critical attention learning for person reidentification</title>
		<author>
			<persName><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunze</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangliang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9637" to="9646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep credible metric learning for unsupervised domain adaptation person re-identification</title>
		<author>
			<persName><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="643" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep meta metric learning</title>
		<author>
			<persName><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianren</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9547" to="9556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Abdnet: Attentive but diverse person re-identification</title>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaojin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8351" to="8361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="403" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dual-refinement: Joint label and feature refinement for unsupervised domain adaptive person re-identification</title>
		<author>
			<persName><forename type="first">Yongxing</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.13689</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification</title>
		<author>
			<persName><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="994" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain generalization via model-agnostic learning of semantic features</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Coelho De Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain generalization with domain-specific aggregation modules</title>
		<author>
			<persName><forename type="first">D'innocente</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Moe-spnet: A mixture-of-experts scene parsing network</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Elsevier PR</publisher>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="226" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-similarity grouping: A simple unsupervised cross domain adaptation approach for person re-identification</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6112" to="6121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mutual meanteaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification</title>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-paced contrastive learning with hybrid memory for domain adaptive object re-id</title>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjie</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2551" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="262" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hard mixtures of experts for large scale weakly supervised vision</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">'</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6865" to="6873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation with mixture of experts</title>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darsh</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Beyond human parts: Dual partaligned representations for person re-identification</title>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Ge</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3642" to="3651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning meta face recognition in unseen domains</title>
		<author>
			<persName><forename type="first">Jianzhu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6163" to="6172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Person re-identification by descriptive and discriminative classification</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scandinavian conference on Image analysis</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="91" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Interaction-and-aggregation network for person re-identification</title>
		<author>
			<persName><forename type="first">Ruibing</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinqian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9317" to="9326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Frustratingly easy person re-identification: Generalizing person re-id in practice</title>
		<author>
			<persName><forename type="first">Jieru</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuqi</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2019. 1, 2, 3, 6</date>
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Style normalization and restitution for generalizable person re-identification</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2020. 1, 2, 3, 6, 7</date>
			<biblScope unit="page" from="3143" to="3152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName><forename type="first">Dangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Episodic training for domain generalization</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinno</forename><surname>Jialin Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5400" to="5409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Locally aligned feature transforms across views</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3594" to="3601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Timedelayed correlation analysis for multi-camera activity understanding</title>
		<author>
			<persName><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer IJCV</publisher>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="106" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A strong baseline and batch normalization neck for deep person re-identification</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hao Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youzhi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuxu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenqi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep metric learning with bier: Boosting independent embeddings robustly</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Waltner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horst</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fewshot image recognition by predicting parameters from activations</title>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7229" to="7238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generalizing across domains via cross-gradient training</title>
		<author>
			<persName><forename type="first">Shiv</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vihari</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preethi</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixtureof-experts layer</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generalizable person reidentification by domain-invariant mapping network</title>
		<author>
			<persName><forename type="first">Jifei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2019. 1, 2, 3, 6, 7</date>
			<biblScope unit="page" from="719" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Part-aligned bilinear representations for person re-identification</title>
		<author>
			<persName><forename type="first">Yumin</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Circle loss: A unified perspective of pair similarity optimization</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changmao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6398" to="6407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning part-based convolutional features for person re-identification</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5334" to="5344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep mixture of experts via shallow embedding</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Dunlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-An</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruth</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="552" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person reidentification</title>
		<author>
			<persName><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Associating groups of people</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wei-Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename><surname>Shaogang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="23" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">End-to-end deep learning for person search</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bochao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.01850</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Distilling crosstask knowledge via relationship matching</title>
		<author>
			<persName><forename type="first">Han-Jia</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De-Chuan</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="12396" to="12405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Ad-cluster: Augmented discriminative clustering for domain adaptive person re-identification</title>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuebo</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9021" to="9030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Alignedreid: Surpassing human-level performance in person reidentification</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiqi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08184</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Relation-aware global attention for person reidentification</title>
		<author>
			<persName><forename type="first">Zhizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3186" to="3195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Exploiting sample uncertainty for domain adaptive person re-identification</title>
		<author>
			<persName><forename type="first">Kecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizheng</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Group-aware label transfer for domain adaptive person re-identification</title>
		<author>
			<persName><forename type="first">Kecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3754" to="3762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identification</title>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="598" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning to generate novel domains for domain generalization</title>
		<author>
			<persName><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07325</idno>
		<title level="m">Domain adaptive ensemble learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
