<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute for Systems Research and Department of Electrical Engineering</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1CEFDCB393663F858D9FE7189F1A63B2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Normalization and Noise-Robustness in Early Auditory Representations</head><p>Kuansan Wang, Member, IEEE, and Shihab Shamma, Member, IEEE Abstract-A common sequence of operations in the early stages of most sensory systems is a multiscale transform followed by a compressive nonlinearity. In this paper, we explore the contribution of these operations to the formation of robust and perceptually significant representation in the early auditory system. It is shown that auditory representation of the acoustic spectrum is effectively a self-normqlized spectral analysis, i.e., the auditory system computes a spectrum divided by a smoothed version of itself. Such a self-normalization induces significant effects such as spectral shape enhancement and robustness against scaling and noise corruption. Examples using synthesized signals and a natural speech vowel are presented to illustrate these results. Furthermore, the characteristics of auditory representation are discussed in the context of several psychoacoustical findings, together with the possible benefits of this model for various engineering applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>OUND signals undergo a series of complex transforma-S tions in the early auditory system. These transformations convert the acoustic spectrum of the stimulus into an internal representation that we shall call the auditory spectrum. Higher central auditory stages analyze further the auditory spectrum into more elaborate representations, interpret them, and eventually form corresponding sound percepts. Therefore, determining the characteristics of the auditory spectrum and the underlying auditory processing that give rise to it is critical for a deeper understanding of the basic perceptual elements of sound. This in turn would inspire novel signal processing algorithms that are perceptually oriented, and suggest new representations of sound signals that might prove useful in such applications as automatic speech recognition (ASR) systems and underwater acoustics.</p><p>It is with these hopes that many efforts were launched over the years to develop auditory-based front-ends for ASR systems [8], [13], [18], <ref type="bibr">[26]</ref>. It is fair to say, however, that despite repeated demonstrations of enhanced performance (ranging from increased noise-robusmess [SI, better generalizations across speakers <ref type="bibr">[4]</ref>, to improved phonemic representations <ref type="bibr" target="#b28">[28]</ref>), auditory front-ends did not progress beyond the re-search laboratory. Instead, most ASR systems continue for various reasons to use more traditional representations such as LPC-based cepstral coefficients, or perhaps slightly modified versions of them. This is partly due to the apparent complexity and nonunifonnity of the proposed auditory models, and the heavy computational loads they demand compared to the benefits gained.</p><p>From a signal processing perspective, the succession of linear, nonlinear, adaptive, and cross channel processing stages that are known to occur in the auditory system makes it difficult to come up with a clear coherent statement of the exact nature of the auditory spectral representations. Instead, it is often necessary to resort to simulations and large speech recognition experiments to demonstrate overall system improvements. In such an exercise, one often loses sight of the exact mechanisms or stages that give rise to the improvements and ways to further enhance the performance.</p><p>With these problems in mind, we proposed earlier a "minimal" auditory model <ref type="bibr" target="#b38">[38]</ref> in which a few of the most important stages were preserved and certain simplifying analyses were introduced. The goals of the work were two folds: (1) to show what the auditory spectrum looked like, and (2) to demonstrate that, despite the nonlinearities and apparent distortions in the auditory spectrum compared to the acoustic spectrum, no information was lost since fairly accurate reconstructions of the original signal were still possible.</p><p>There were two shortcomings of that work that we seek to tackle in this paper. The first is that the deterministic analytical framework of the earlier model is still too difficult to manipulate. For instance, it is difficult to use it to predict quantitatively such things as the enhanced peak-to-trough ratios in the auditory spectrum, the expected amount of noise suppression, or simply what the auditory spectrum is in relation to the acoustic spectrum. The second problem is the simplifying omission in the earlier paper of a normalizing factor in the auditory spectrum, which made it look less distorted when compared to the original acoustic spectrum.</p><p>In this paper, we shall continue to use the earlier "minimal" model of the early auditory system (critically reviewed in Section I1 below). This work, however, proposes a new stochastic framework for the analysis of the early auditory system that employs a random source model. Our goal is to explain and predict the general trends and properties of early auditory processing, rather than to pursue an exact and deterministic description on the model output for any specific signal. This approach is motivated by several factors. First, many signals are random in nature and can usually be 1063-6676/94$04.00 0 1994 IEEE well-described by some stochastic quantities, e.g., the autocorrelation and power spectrum <ref type="bibr">[2]</ref>, 1241. Second, the analysis task of a signal processing system is usually significantly simplified if the system performance is described in terms of a more general underlying signal space that is associated with certain probability measure (i.e., a probability space). The signal, as argued in <ref type="bibr">[lo]</ref>, can always be viewed as a sample from a signal space of which parameters are randomly chosen before the signal starts. The performance of system, described in terms of the responses to the signal space as a whole, may roughly be applied for the sample in a statistical sense. For example, any single tone of frequency w belongs to a space {Rsin(wt + 0 ) ) in which the amplitude R and phase 0 are random variables. If R has a Rayleigh distribution and 0 is uniformly distributed over [0,27r], the signal is indeed a zero mean, stationary Gaussian process <ref type="bibr">[20]</ref>. While it is difficult to obtain a detailed closed-form description for the system responses to a specific single tone, a concise expression is usually possible for a stationary Gaussian process. The usefulness of this approach is well-demonstrated, for example, by the characterization of zero crossings and spectral analysis in [3], [16].</p><p>The organization of this paper is as follows. First, we shall review in Section I1 the basic outlines of the auditory model. In Section 111, we present the stochastic framework for the analysis and discuss how the auditory spectrum is interperted under various conditions and simplifications. Next we illustrate the salient properties of the auditory spectrum and compare them to the analytical predictions. Four specific examples of acoustic signals are next considered in Section IV: A single tone in broadband noise, a harmonic series in noise, a pre-emphasized speech vowel, and a speech vowel in broadband noise. Finally, in Section V, we discuss the implications of the auditory processing, especially its effects on the representation of acoustic power spectrum, and elaborate briefly on its relationship to several psychoacoustical measures and to commonly used representations and signal processing techniques in ASR systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">BRIEF REVIEW OF THE MATHEMATICAL FORMULATIONS OF THE AUDITORY MODEL</head><p>There have been numerous descriptions of the early auditory system, ranging from detailed biophysical models to schematic computational algorithms [I], 171, [8], [ 121. Despite this diversity, all models are generally composed of three major stages: analysis, transduction, and reduction (Fig. <ref type="figure" target="#fig_0">1</ref>). In the following, we briefly review a minimal model that is both mathematically tractable and biophysically defensible, at least for the case of broadband signals (such as speech) and at moderate to high levels of intensity. This model was previously described in more detail in <ref type="bibr" target="#b38">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Analysis Stage</head><p>Sound pressure waves impinging on the ear drum cause vibrations that are transmitted to the fluids of the cochlea via the oscicles of the middle ear. These vibrations induce pressure differences across the basilar membrane. They in tum produce mechanical displacements in the form of traveling waves whose amplitudes peak at specific locations along the cochlea in an ordered manner depending on the frequency of the stimulus. Thus, for high frequencies the maximum response occurs near the base of the cochlea, while for lower frequencies it occurs near the apex. In this way, the spatial axis of the cochlea may be associated with a tonotopically ordered (i.e., a frequency) axis. One simple way to describe the response characteristics of the basilar membrane is to associate each point on it with a transfer function, i.e., to model the basilar membrane as a bank of filters. Suppose the sound signal is described by ~( t ) , then the basilar membrane response is given by where h(t, s) denotes the cochlear filter at a specific location s on the basilar membrane and *t denotes the convolution in the time domain.</p><p>In describing the basilar membrane responses in linear terms as above, we have chosen to ignore a host of additional simple and complex details that may be critical in some applications. For instance, we have ignored the spectral filtering induced by the outer ear that is important for auditory localization tasks. We have also ignored the relatively slow adaptive (AGC-like) action and lowpass filtering of the middle ear muscles and bones that are useful in protecting against very loud sounds. Finally, we have ignored all the nonlinearities and adaptive mechanisms in the basilar membrane that are attributable to the motion of the outer hair cells and the feedback communicated via the efferent system. These mechanisms are particularly important in enhancing the sensitivity of the basilar membrane at very low sound levels. All these simplifications, however, can be justified if we restrict the domain of applicability of this model to the processing of broadband signals (such as speech) at moderate to high levels of intensity. In this case, it has been repeatedly shown that the cochlear filters behave largely as linear and relatively broadly tuned filters 171.</p><p>For the analysis presented in this paper, the exact shape of the cochlear filters is unimportant. Rather, it is understood that the filters satisfy certain gross characteristic features typical of the shapes inferred from available physiological and psychoacoustical data. For instance, cochlear filters are generally relatively broadly tuned and significantly asymmetric in shape, with a steep roll-off on their high-frequency sides. The center frequency of each filter is called the characteristic frequency (CF). For filters with CF's larger than about 800 Hz, the bandwidth is linearly proportional to the CF, i.e., the filters have constant Q's; for lower CF's, the filter's become gradually less tuned. The constant tuning over most of the cochlear length implies that the CF's of the filters are logarithmically mapped. As such, the cochlear filters can be related by a simple dilation and the basilar membrane response yl(t, s ) in ( <ref type="formula">1</ref>) is a waiielet transform of the sound signal ~( t ) .</p><p>For the analysis presented here, this property of the filters is not essential. Instead, filter shapes can be designed to satisfy arbitrary application-dependent criteria. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Transduction Stage</head><p>The mechanical vibrations on the basilar membrane are traduced into electrical activity along a dense, topographically ordered array of auditory nerve fibers. At each point on the basilar membrane, the motion of the membrane causes a local fluid flow that bends small filiments (cilia) that are attached to the so-called (inner) hair cells. The bending of the cilia controls the ionic currents through a nonlinear channel into the hair cells. This ionic flow, in turn, generates electrical potentials across the hair cell membranes and these potentials are finally conveyed by the auditory nerve fibers in the form of neural spikes to the cochlear nucleus.</p><p>The above biophysical process transforming the motion on the basilar membrane into the neural spikes in the auditory nerves can be modeled by the following three steps <ref type="bibr" target="#b30">[30]</ref>. First, since the ionic current is velocity driven, a temporal derivative is employed to convert instantaneous membrane displacement into velocity (Fig. <ref type="figure" target="#fig_0">1</ref>). The nonlinear channel through the hair cell is then modeled by a sigmoid-like function, and the leakage of the cell membranes is accounted for by a low-pass filter. The hair cell responses can therefore be described by</p><formula xml:id="formula_0">(2)</formula><p>where g(.) denotes the compressive nonlinear function and w(-) is a temporal smoothing window. The lowpass filter w(-) Y2(t, 3) = g(atYl(4 3)) *t w(t) in effect filters out all response frequencies 24 kHz. Consequently, for the very high-frequency cells, the intracellular potential response exhibits only a steady dc increase in level with little or no fluctuations. Much of the theory we discuss here is only relevant for the lower frequency ranges where much of speech signal energy is and phase-locked activity is still present. Thus, the w(t) plays a minor role and will be usually ignored.</p><p>The hair cell nonlinearity g(.) is a compressive function described by a monotonically nondecreasing sigmoid function. Its response may be divided into a dynamic (linear) region, a cutoff region, and a saturated region. The so-called dynamic range usually refers to the area where the gain of the nonlinear function, g'(-), is large. The dynamic range of the hair cell nonlinearity is about 30-40 dB. Beyond this level, the hair cell is driven into saturation and the fine detail of the signal is heavily compressed and poorly represented.</p><p>The receptor potentials (or enhanced versions thereof) generated at the end of these stages are conveyed via the auditorynerve fibers to the cochlear nucleus, the first station of the central auditory system. This is achieved through a series of transformations in which the receptor potentials are converted into stochastic trains of electrical impulses (firings) on the auditory nerve. Detailed biophysical models of these transformations can be found in <ref type="bibr">[7]</ref>, <ref type="bibr">[12]</ref>, <ref type="bibr" target="#b37">[37]</ref>. More abstractly, the stochastic firings can be modeled as nonstationary point processes with instantaneous rates that approximately reflect the underlying receptor potentials [3 11. Recipient neurons in the cochlear nucleus may then reconstruct estimates of the receptor potentials by effectively computing the ensemble averages of activity in locally adjacent fibers <ref type="bibr" target="#b29">[29]</ref>.</p><p>From an information processing point of view, these complex transformations merely convey the receptor potentials to the cochlear nucleus. Consequently, in a functional model, they can all be bypassed. Such a simplified view ignores two types of effects that have figured prominently in several classical models of auditory processing. The first is the adaptive mechanisms operative at the hair cell/auditory-nerve junctions that might be important in describing the responses to the onset of sound <ref type="bibr">[21]</ref>. To a limited extent, the enhancing action of these mechanisms can be modeled by a linear stage and incorporated into the form of the filter in (2) above.</p><p>The second simplification concems the range of thresholds and spontaneous rates of firings observed in the responses of the auditory nerve. In most fibers (&gt; 85%), thresholds are lower than typical conversational sound levels, while their spontaneous rates are relatively high. These fibers exhibit a limited dynamic range between threshold and saturation (approximately 30 dB), and are consistently found to be almost totally saturated when driven by broadband sounds at moderate levels (60-70 dB SPL). The remaining fibers exhibit low spontaneous rates, a wider range of thresholds, and sloping saturations. Consequently, they are less likely to be saturated at normal sound levels. To model these two populations, it is possible to use two or more nonlinearities with appropriately weighted outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C . The Reduction Stage</head><p>The auditory nerve transmits the hair cell responses g 2 ( t , s) to the central auditory system where the information about various attributes of the sound, such as timber, pitch, and other spatial and temporal characteristics are extracted and processed. In this report, we focus on the extraction and representation of the acoustic spectrum, a fundamental cue for the perception of timber and the recognition of speech signals.</p><p>There are many ways by which the spectral contents of the stimulus may be estimated from the pattems of auditory nerve responses. A detailed review can be found in <ref type="bibr" target="#b29">[29]</ref>. The scheme discussed here is a particularly simple one found in all sensory systems and is implemented by a neural network commonly known as the lateral inhibitory network (LIN). In early vision, such network exists in the retina and functions like an edge detectodmagnifier [ 191. In audition, the LIN is presumed to exist in the cochlear nucleus that receives direct inputs from the auditory nerve and exhibits physiological and anatomical characteristics consistent with the structure and functions of LIN <ref type="bibr" target="#b29">[29]</ref>. The simplest model of the LIN consists of a single layer of nonlinear neurons that are mutually inhibited either in a feed forward or feedback manner <ref type="bibr">[27]</ref>. From a mathematical point of view, the LIN operations can be effectively divided into three steps as depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. The first two steps are a derivative with respect to the tonotopic axis that mimics the lateral interaction among LIN neurons, followed by a halfwave rectifier modeling the nonlinearity of the LIN neurons. More realistically, the derivative is leaky in nature, i.e., is accompanied by a local smoothing due to the finite spatial extent of the lateral interactions <ref type="bibr" target="#b29">[29]</ref>. Generally speaking, this operation essentially detects spatial discontinuities in the y2(t, s) responses along the cochlear axis s. The last step of LIN models primarily the fact that the central auditory neurons, unlike the auditory nerve fibers, are unable to follow rapid temporal modulations higher than, for example, a few hundred hertz. Rather, they signal a temporally integrated version of the outputs. The LIN operations then can be described by where IT(.) is a temporal integration window and v(.) is a spatial smoothing function. At the final output of the LIN we thus obtain a representation of the sound that, as we shall elaborate, approximately reflect a short-time spectral profile of the signal. This pattem will be referred to as the auditory spectrum of the signal.</p><p>The characteristics of this auditory spectrum and its application for speech signal representation have already been empirically studied. For example, it was shown in <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b38">[38]</ref> that the formant structure of various phonemes are wellpreserved and that simple speech recognition systems can be designed to distinguish most phonemes based on these features. In another series of experiments [6], it was demonstrated that the auditory representation has a significant advantage over conventional representations in noise robustness when employed as a front-end for ASR systems. The issue of noise robustness was also studied in <ref type="bibr" target="#b38">[38]</ref>, where it was observed that the speech signal reconstructed from the auditory spectrum exhibit noise suppression. The exact reasons for this phenomenon were not determined then because of the difficult nature of the deterministic analysis of the model. In the next section, we present an altemative stochastic analysis of the model to overcome these difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">STOCHASTIC ANALYSIS OF THE EARLY AUDITORY MODEL</head><p>In this section, a stochastic interpretation of the auditory processing described above is developed. The fundamental goal of this analysis is to describe accurately the nature of the spectral analysis performed by the auditory system, and to explain the enhancements and noise suppression effects previously observed in the auditory spectrum <ref type="bibr" target="#b38">[38]</ref>. For simplicity, we shall ignore the temporal and spatial smoothing w(t) and v(s) as in <ref type="bibr" target="#b38">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Suppose the sound signal ~( t )</head><p>can be modeled by a zero mean random process. Accordingly, so are the signals yl(t, s) . . . y5(t, s) random processes (not necessarily zero mean). Since the auditory spectrum ys(t,s) is a filtered process, the variance of the process yys(t, s) is limited by the smoothness, or the bandwidth of the lowpass filter n(t) <ref type="bibr">[lo]</ref>. If this bandwidth is narrow enough, the variance of yS(t, s) would be very small for any instant t. According to Chebyshev inequality [lo], this implies that y5(t,s) can be approximated by its expectancy</p><formula xml:id="formula_1">E[Y5(t, 311 = E[Y4(t, s) *t l-I(t)l = E[~4(t, s)] *t n(t) ( 6 ) where E [ Y ~( ~, 311 = qm=(y3(t, S),O&gt;I = E[mu(g'(&amp;Yl(t, s))as&amp;yi(t,s),0)]. (7)</formula><p>Given II(.), <ref type="bibr">(6)</ref> suggests that the auditory representation can be characterized by examining E[y4(t, s)]. If y4(t, s) satisfies an ergodic theorem within a duration comparable to the time constant of n(t), then ys(t, s) will indeed converge to a timeinvariant constant although the signal and y4(t, s) may well be nonstationary [ 101. A stochastic process whose sample mean converges (along with the number of samples) is said to satisfy an ergodic theorem. Such a condition is not contingent on whether the process is stationary or not [lo]. In this report, we refer to the ergodic mean of y4(t, s), toward which yg(t, s) converges, as the auditory spectrum. With such a definition, the auditory spectrum is a first-order statistic of g5(t,s). In contrast, conventional definition of power spectrum requires the second order statistic (autocorrelation or autocovariance function) to be a Teoplitz function, i.e., R(t,T) = R(t -7).</p><p>which in tum has to base on a (weakly) stationary assumption on the stochastic process.</p><p>For notational simplicity, let U = &amp;yl(t,s) and V = &amp;&amp;yl(t, a). Assume the hair cell nonlinearity is monotonically nondecreasing, i.e., g'(.) 2 0. Quation (7) can then be rewritten as</p><formula xml:id="formula_2">E[Y4(t, 23)) = J J m=(g'(+, O)fUO(U, v)dudv = E[g'(U)E[m=(V, 0)IUll (8)</formula><p>where fu(.) and fuw(., .) denote the probability density function (pdf) of U and joint density function of U and V , respectively.</p><p>A. The "Linear" Auditory Spectrum nonlinearity, ( <ref type="formula">8</ref>) is reduced to</p><p>In the case that g'(x) = 1, i.e., there is no hair cell x ( t ) is zero mean, so is V. Hence the quantity E[max( V, O)] is proportional (though not necessarily linearly) to the standard deviation 0 , or the instantaneous energy of V.' Thus in this linear case, the signal is analyzed into various frequency bands defined by the differential cochlear filters, and the scheme is no different from a filter-bank based frequency analysis framework, using the differential filters (as opposed to the cochlear filters) as analyzing filters. While the cochlear filters are broad and highly asymmetric, the differential filters are narrowly tuned and centered around the same frequencies (Fig. <ref type="figure">2</ref>). As such, E[ye(t, s)] = E[max(V, O)] is simply the spectral energy profile of the sound signal z ( t ) across the channels indexed by s . ~</p><formula xml:id="formula_3">E[Y4@, 41 = E[E[m=(V, 0 ) l ~I l = E["(V, 0)l implying E[y4(t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Effects of the Hair Cell Nonlinearity</head><p>When the compressive hair cell nonlinearity is taken into consideration, it transforms the estimate of the spectral energy profile into a conditional measure. Thus, while still being estimated through the dc component of the half-wave rectified waveform, the spectral energy becomes modulated by the instantaneous gain of the hair cell nonlinearity, as described by (8). In the following, we elaborate on the implications of this nonlinear effect.</p><p>First consider the nonlinear gain g'(U). As mentioned earlier in Section 11, the hair cell nonlinearity has a finite dynamic range so that it is occasionally driven into saturation. In practice, this occurs when the sound intensity is at or above a moderate level. Since the nonlinearity is characterized by a negligible gain outside the dynamic range, this implies that the estimation of the energy resolved by the differential filters is not made equally at all instants as in the linear case. Instead, the estimate now takes place only when U = &amp;yl(t, s ) lies I The physical term "energy" usually refers to the variance U' of a random process rather than the standard deviation U. However, due to the one-toone correspondence between the two quantities, their minor difference is not distinguished throughout this paper.</p><p>2Note that this narrowband filtering leading to the LIN auditory spectrum is fundamentally identical to the operations invoked to obtain the so-called ALSR spectrum in <ref type="bibr">[25]</ref>.</p><p>within the dynamic range of the nonlinearity. Therefore, the estimate is also affected by the "rate" of U being within the dynamic range. When U has a large variance, it is less probable to restrict its value within a certain range. This "rate" decreases as the energy of U increases, and vice versa. Therefore, conceptually, E[y4(t, s)] contains a term that is inversely proportional to the energy of U.</p><p>The second term in (8), the conditional expectation E [ max( V, 0) I U], suggests an additional property of the nonlinearity. As is in the linear case, the dc component of the half-wave rectified waveform of V is evaluated. However, the evaluation now only takes place when U is in the dynamic range of the nonlinearity. Effectively, the compressive nonlinearity introduces a sampling process imposed on V, the output waveform from the differential filter.</p><p>The energy profile estimation is then carried out based on these samples instead of the whole waveform. In the case that U and</p><formula xml:id="formula_4">V are independent, E[max(V,O) I U] = E[max(V,O)], i.e.,</formula><p>V is unbiasedly sampled and therefore the estimation of its energy is left unaffected. In general, however, the dependency between U and V affects the energy estimation, and hence the auditory spectrum (see later for an example). Since the two processes U and V are generated by passing the same source, &amp;z(t), through the cochlear and differential filters, respectively, the dependency between U and V is basically dependent on the design of the filters. In summary, E[y4(t,s)] is a quantity that is proportional to the energy of V, and inversely proportional to the energy of U and their dependency. As established earlier, the auditory spectrum is an ergodic average of y4(t,s). Thus, by the definitions of U and V, this suggests that y ~( t , s) is an averaged ratio of the signal energy passing through the differential and cochlear filters. That is, the auditory spectrum is a self normalized spectral profile. Before elaborating on the significance of such self-normalization, we consider the following special cases to exemplify our arguments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C . The Special Case of a High-Gain Nonlinearity</head><p>The arguments above were based on the premise that the compressive hair cell nonlinearity has a finite dynamic range. However, the above analytical expressions can be dramatically simplified by pushing the nonlinearity to the extreme where it becomes a Heaviside (step) function. The dynamic range consequently is reduced to a singleton at the origin and the gain is infinite, described by g'(z) = 6(z), the Dirac delta function. In this case, (8) can be rewritten as</p><formula xml:id="formula_5">(10) E[Y4(t, s)] = E[m=(V, 0) I U = OIfu(0).</formula><p>a zero mean Gaussian process. Since U and V are obtained by linear filtering, U,V are also zero mean Gaussian. Let T , au, (T, denote the correlation coefficient and the standard deviations of U and V, respectively. Note that all these quantities, like U and V themselves, are indexed by ( t , s ) , which we drop throughout for notational simplicity. As shown in <ref type="bibr">[20]</ref>, a special property of the Gaussian distribution is that the conditional distribution fVlu(vlu) is also Gaussian with mean U and variance (T' = 0 : ( 1</p><formula xml:id="formula_6">-T ' ) . Hence E[max(V,O)IU = U] = vf,(,(v/u)dv I"</formula><p>where a(.) is the cumulative distribution function (cdf) of a</p><p>Gaussian random variable with zero mean and unit variance.</p><p>By ( <ref type="formula">10</ref>) and (1 l), E[y4(t, s)] for the high-gain case becomes This estimate can be decomposed into three terms corresponding to the nonlinear effects described earlier: IS,,/- is the energy profile of the linear case (i.e., E[max( V, O)]), 1 / 6 0 , is the rate of U entering the dynamic range of the nonlinearity (i.e., fu( <ref type="formula">0</ref>)), and d m ' accounts for the decrease in energy estimation due to the conditioning. Note that in this expression, nV is normalized by nu, which demonstrates the self-normalizing nature of the auditory spectrum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. The High-Gain Case for a Weakly Stationary Source</head><p>Now we consider the additional assumption that the source is weakly stationary and has a spectral density function (s.d.f.) Sz(w). For a Gaussian source, weak stationarity also implies strict stationarity. The statistics mentioned above all become time-invariant constants and can be obtained from the following</p><formula xml:id="formula_7">(TE = -IwH(w,s)12Sz(w)dw 27r ' J</formula><p>The first term in the above equation illustrates that V is where H* denotes the complex conjugate of H . Let the at the zero crossings Of The second term fU(O)* cochlear band Q , ( s ) and the differential band Qtd(s) be the passbands of the (preemphasized) cochlear filter lwH(w,s)l the probability density Of = ' 9 is generally proportional to the standard deviation of U since U is assumed to have zero mean.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The High-Gain Auditory Spectrum for a Gaussian Source</head><p>In order to obtain an explicit expression for the joint distribution of U and V, let us assume the source z(t) is and differential filter Iwd,H(w, and ( T : can be restated as the spectral energy within the cochlear band and the differential band, respectively. If the differential band is very narrow, for example, w a s ~( w , s) = f w s ) , U," = S,(fw,). That is, ( T : is indeed the power of the signal at frequency w,.</p><p>respectively.</p><p>The dependency of U and V is described by T in (15). Since </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Summary</head><p>To sum up, E[y4(t, s)] is a normalized power spectrum of the sound stimulus (6), and so is the auditory spectrum (12).</p><p>The normalization here is neither uniform nor predetermined.</p><p>Rather, it is driven by the energy distribution of the signal. For each channel s, the output reflects approximately the ratio of the energy of its differential filter to that of its cochlear filter. The spectral components of the sound signal &amp; ( U ) therefore receive unproportional scaling: a spectral peak resolved by the differential filter receives a relatively small normalization factor since the cochlear filter in its vicinity integrates energy from the valleys surrounding the pgak. The opposite is the case for a spectral valley. Effectively, this difference in the normalization further enlarges the spectral peak to valley ratio, a phenomenon referred to here as spectral enhancement or noise-suppression depending on what is being suppressed in the spectral valley. Generally speaking, the broader the cochlear band is relative to the differential band, the more enhancement effects can be expected.</p><p>In the following section, we shall illustrate these nonlinear effects with a series of examples, and then in Section V further elaborate their analytical significances and psychoacoustical implications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iv. EXAMPLES OF SPECTRAL ENHANCEMENT AND NOISE SUPPRESSION</head><p>We illustrate in this section the properties of the auditory spectrum in the high-gain case for four signals: tone in noise, a harmonic series in noise, a natural vowel, and a natural vowel in noise. Our aim here is to highlight some of the characteristic features of the auditory spectrum that result from self-normalization property discussed earlier. Throughout this section, we assume that ys(.,s) is well-behaved so that the auditory spectrum E[y5(t,s)] can be approximated by an ensemble of yS(t, 8). We start with a description of the implementational details of the model used in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . Implementation of the Auditory Model</head><p>Although the theory developed in the previous section does not depend on specific H ( w , s ) , the design of the cochlear filter plays an important role in the application of the auditory model. This issue has been discussed in some detail earlier in <ref type="bibr" target="#b36">[36]</ref> (see also Section 11). Motivated partly by biophysical and psychophysical considerations, the filters chosen here are related by a dilation, i.e. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H(w,</head><p>i.e., the differential filters are also related by a dilation;</p><p>So are &amp;(s) and R,(s). If the differential band fl,(s) is reasonably narrow, the spatial axis s can be thought of as being labeled by logarithmic frequencies, which is roughly equivalent to the tonotopic structure of the auditory system. One convenient property of the filter dilations is that, on a logarithmic frequency axis, the filters have the same shape and width as the seed filter. They are merely translations of each other. The filter design problems are thus simplified to a single seed filter rather than the whole filter bank. Fig. <ref type="figure">2</ref> shows an example of a cochlear filter and its corresponding differential filter used in the computations discussed below. They are obtained following the criteria and design principles outlined in <ref type="bibr" target="#b36">[36]</ref>. Generally speaking, the cochlear filter is designed to be relatively broad and asymmetric so that its differential version is reasonably narrow. In this implementation, the 3-dB bandwidth of the differential filter is roughly 5% of the peak frequency, and Od(s) C O,(s). The seed filter is then dilated 2 octaves down and 2.8 octaves up to cover the frequency range from 250 Hz to 7 kHz. The spatial channel is discretized at a resolution of 20 channels/octave, causing the differential filters to overlap at their 3-dB points. Finally, the smoothing window n(t) is implemented by a leaky integrator with a time constant 7 = 20 msec, i.e., n(t) is a single-pole low-pass filter with the corner frequency at 50 rad/sec. With these, if the source is stationary</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J O</head><p>Furthermore, if the source is Gaussian, by (lo), we have where all the parameters can be obtained from (13)-(15).</p><p>These equations, which give a stochastic description of the auditory spectrum, are not difficult to evaluate in computer simulations. However, a simpler and a more direct approach from a biological and hardware implementation point of view is to approximate the spatial derivative a, (Fig. <ref type="figure" target="#fig_0">1</ref>) by a difference and simply subtract the heavily saturated outputs of adjacent cochlear filters, and then half-wave rectify the result and integrate it over II(t). Unless stated otherwise, the auditory spectra discussed below are obtained in this manner. Fig. <ref type="figure" target="#fig_3">3</ref> demonstrates the closeness of the auditory spectra for a vowel /aa/ computed by the two methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Noise Suppression in the Auditory Spectrum</head><p>Consider first the auditory spectrum for white Gaussian noise. Regardless of the level of the noise, the filters in ( <ref type="formula">13</ref>)-( <ref type="formula">15</ref>) can be replaced by those in (18x19) to obtain <ref type="bibr">(22)</ref> The fact that both of these two quantities are independent of s suggests that the auditory spectrum E[ys(-, s)] (20) of a white noise is constant (over s). According to the above equations, this constant 2 4 n -J IwH:,(w)l2dw U:,,</p><formula xml:id="formula_9">J I W ~? n ( W ) l 2 d W ' , = l V l n d R (23) 2~ Duln</formula><p>does not depend on the level of the noise and is determined solely by the shape of the seed filter, a direct reflection of the normalized nature of the auditory spectrum. Since K does not vary with the noise power, it will be refered to in *e following as the baseline of the auditory spectrum, and the noise will be assumed to have unit power.</p><p>When a tone of frequency WO (in the sense defined in Section I) is added to the noise, the s.d.f. of the signal ~( t )</p><formula xml:id="formula_10">i s S,(W) = a27rS(w f WO) + 1</formula><p>where c2 here is the signal-to-noise ratio (SNR). For s such that WO $Z' s2,(s), E[y5(-,s)] is still equal to the baseline. However, for chynels whose cochlear band covers the tone frequency but the differential band does not, i.e., WO E R,(s)n R ~( s ) " , substituting the s.d.f. into (13)-( <ref type="formula">15</ref> Note that (24) indicates that P 5 1, which implies that a 5 1 and the equality holds only if P = 1 (SNR or D = 0). Also note that a is roughly proportional to P when P is small. Equation ( <ref type="formula">25</ref>) thus suggests that in the auditory spectrum, the noise level at the channels close to the tone frequency (WO E G,(s)) is suppressed by a factor a. As the SNR increases, so does the suppression. This suppression effect is demonstrated in Fig. <ref type="figure" target="#fig_4">4</ref>, which compares the computed (dashed) and predicted (solid) auditory spectra of a 1 kHz single tone corrupted by a white Gaussian noise at various SNR's. Since the suppression occurs at s where WO E fl,(s) n R ~( s ) ~, the suppression effect in this implementation takes place mostly at channels higher in frequency than WO due to the asymmetric shape of the cochlear filters (Fig. <ref type="figure">2</ref>). All these features closely resemble findings from psychoacoustical experiments employing tone in noise stimuli <ref type="bibr">[23]</ref>, <ref type="bibr">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C . Enhancement of Spectral Peaks</head><p>To demonstrate the spectral enhancement in the peak to valley ratio, we compare the auditory spectrum to the linear power spectrum of a harmonic series in white Gaussian noise.</p><p>The linear power spectrum is obtained by merely removing the hair cell nonlinearity (i.e., by letting g(z) = z). Note that the last two stages in the LIN, a halfwave rectifier followed by a leaky integration, is an envelope detector. Such a scheme can therefore be viewed as estimating the energy resolved by the differential filters alone without self-normalization. The harmonic series has a fundamental frequency 150 Hz, and the phase for each component is randomly chosen. It can be generalized from the argument for a single tone (Section I) that such a harmonic series, with the phase of each component independently randomized, is a weakly stationary signal. In this demonstration, each component in this series is set to be 6 dB stronger than the white Gaussian noise (within the band of analysis). The two computed spectra are compared in Fig. <ref type="figure" target="#fig_5">5</ref>.</p><p>As predicted, the valleys in the auditory spectrum are deeper than in the linear power spectrum. Note that in both cases, the low-frequency harmonics are well-resolved compared to the higher ones since they are more separated on a logarithmic frequency axis. The enhancement effect within this region is similar to the single tone in noise case and can be theoretically predicted in the same fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Robustness Against Scaling Effects</head><p>Another property of a self-normalized spectrum is its relative stability with respect to an overall scaling. This follows directly from the fact that the auditory spectrum is an energy ratio, and hence the scaling factor will appear in both the numerator and denominator in, for example, (20). More generally, the auditory spectrum is relatively insensitive to broadband changes in the spectral shape of the signal as long as the responses from the differential and cochlear filters are affected similarly.</p><p>To quantify these effects, consider an overall distortion of the acoustic spectrum (A(w)), or equivalently in our implementation, a distortion in the overall gain of the auditory filters (A(s)) as follows</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R ( w , s ) = A(s)H(w, s)</head><p>The differential filter is therefore &amp; 8 ( w , s ) = A(s)asiY(w, s) + A'(s)H(w, s)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A(s)&amp;H(w, s)</head><p>when A'(s) is small relative to A(s), i.e., A ( s ) is a broadband adjustment. In this case, the gain term A ( s ) scales the cochlear and differential filter responses apprqximately equally, and hence the net auditory spectrum remains unchanged.</p><p>Depending on the nature of the signal and the application, A(s) may represent intentional adjustments (e.g., pre-emphasis in speech processing) or an undesirable unpredictable distortion (e.g., spectral tilt reflecting inter-speaker variability, or a distortion id a signal caused by a noisy transmission channel).</p><p>In all cases, it is anticipated that the auditory spectrum should exhibit more stability against the distortion so long as it is broadband in the sense defined above.</p><p>As an example, consider the effects of applying preemphasis. This is normally achieved by pre-filtering the signal with a transfer function equivalent to 1 -cz-l, where c is the preemphasis factor. The effective distortion is</p><formula xml:id="formula_11">~~ A(w) = Jl + c2 -2ccosw</formula><p>where w denotes the normalized frequency (with respect to the sampling rate) and is logarithmically mapped by s in our model.  <ref type="figure" target="#fig_7">6(c</ref>). As can be seen in the previous example, the harmonics of this vowel are clearly resolved in the lowfrequency channels, while for the high-frequency channels, only the formant structures are visible. Despite the large modification on the input spectrum, the auditory spectrum remains relatively stable, with the pre-emphasis effects mostly concentrated in the high-frequency channels. These occur because of relative steepness of the A ( s ) function there, i.e., the distortion is not as broadband, and A'(s) is not small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Noise Robustness of a Natural Vowel Stimulus</head><p>One way to demonstrate the rioise robusmess of the auditory spectrum compared to the linear spectrum is to examine how fast it deteriorates with decreasing SNR. The S N R here refers to the ratio of the total energy of the clean speech to the energy of the additive noise. Fig. <ref type="figure">7</ref> shows the auditory spectrum of the vowel /aa/ corrupted by various levels of white background noise. Note that the auditory spectrum approaches the baseline K. as the noise gradually overwhelms the signal. Consequently, it can be viewed that the acoustic features in the auditory sljkctrum of the signal are represented by F ( s ) = E[yS(t,s)] -K. and llFll -0 as noise level increases. The spectral features conveyed in F ( s ) will be further discussed in the next section. To quantify the effects of noise, we define a distortion measure based on the Euclidean distance as</p><formula xml:id="formula_12">D = 2010g l l F ( ~, SNR) -F ( s , . . ) \ I (27)</formula><p>where F ( s , z ) is the pattem when SNR = x and F ( s , m ) is the clean speech pattem. Fig. <ref type="figure">8</ref> shows a comparison in the amount of distortion between the auditory and the linear power spectra. As in this example, the two representations have the comparable performance for low noise level. At high noise level (SNR &lt; 7 dB), however, the linear power spectrum degrades at a faster pace than the auditory spectrum.</p><p>v . RELATING THE AUDITORY REPRESENTATION TO THE PERCEPTION OF THE ACOUSTIC SPECTRUM In the previous section, we illustrated with several isolated examples how the local self-normalization of the auditory spectrum manifests itself as feature enhancement and noise robustness. In this section, we attempt to integrate these phenomena within a more specific extension of the theoretical framework developed earlier in Section 111. This will allow us to demonstrate how the transformation from an acoustic (power) spectrum into an auditory pattern in the early auditory system may possibly be linked to several important psychoacoustical phenomena.</p><p>In the following, the framework for the analysis is first developed (Section V-A) and then several of its perceptual implications are discussed. These include: invariance to broad scale adjustments (Section V-B), the representation of spectral slopes (Section V-C), the transfer characteristics of rippled spectra (Section V-C), linearity and superposition in the auditory representation (Section V-D), and the dominance principle and noise robustness (Section V-E). Fig. <ref type="figure">8</ref>. The distortion incurred by noise for both the auditory spectrum (solid)</p><p>and the linear power spectrum (dotted trace) of the vowel /aa/. Note that the abcissa is labeled according to the relative noise to signal level. At low noise level, the two representations have comparable performance. However, the distortion in the auditory spectrum does not increase as fast as the linear power spectrum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Transforming the Acoustic Power Spectrum into an Auditory Pattern</head><p>Consider a source power spectrum &amp;(U) applied to the auditory model. In general, this pattem can be linearly decomposed into where {l,z, z2, -..} is a collection of orthogonal bases and { a0 , a1 ,a2, . . } are the associated coefficients.</p><p>There are many ways in which this decomposition can be useful. For instance, if we let z = ejnow, the above analysis is a Fourier transformation on the power spectrum, and a, denotes the intensity for component having a "quefrency" nR,. Such {a}, is very similar to the well-known cepstral coefficients of the source signal3.</p><p>A slightly different and more useful decomposition of the power spectrum results if we let z = ejuos in <ref type="bibr" target="#b28">(28)</ref>, where s = log w. This is essentially a Fourier transformation of the logfrequency power spectrum S, (3) = S, (log U ) since frequency in the auditory model is logarithmically mapped on the saxis (see Section IV). Such a decomposition simply analyzes S,(s) into ripples, or sinusoidally-modulated (against log w ) power spectra, of different frequencies [14]. The coefficient a, denotes the intensity of the component that has a ripple frequency nu,,.</p><p>Another useful decomposition, usually known as orthogonal polynomial decomposition, is to let z, denote an nth-degree polynomial (with the leading coefficient being 1) from an orthogonal bases [17]. a, then denotes the nth-order characteristics of the power spectrum, e.g., al, a2 are the overall slope and quadratic curvature of the spectrum, respectively.</p><p>In all cases, the transformation of S,(w) into S,(z) can be viewed as a mapping similar to a "change of coordinate" that merely provides an alternative description for the power spectrum of the source signal. Note that for all the decomposition methods discussed above, the coefficient a0 represents the overall level of the sound signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">) Decomposing the Auditory Spectrum:</head><p>Now consider the corresponding description for the auditory power spectrum in the high-gain case. From (20) we have -S&amp;) 4 u + 3&gt;Sz(u&gt;&amp; J&amp;) 4 0 + s)Sz(u)du -A ( s ) <ref type="bibr" target="#b29">(29)</ref> in which we refer to the fact that the spatial index s is interchangeable with logarithmic frequency due to the dilation relationship. From (18) and (19), ~( 3 ) and u ( s ) can also respectively. They can also be decomposed into be shown to be Ja2sHm(aswm)12 and l ~~~d S H m ( ~~w m ) J ~,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(30) (3 1)</head><p>.(z) = U0 + u1z + U222 + . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>U ( . )</head><p>= WO + VIZ + w2z2 + * . . . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ( z ) =</head><p>= U:, + ai2 + a y + . . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(32)</head><p>Note that the cross terms all vanish since z" and zm for n # m are orthogonal to each other. For ripple bases z = ejuos, {a'}, are the Fourier coefficients for the auditory power spectrum, which are analogous to the cepstral coefficients. It should be re-emphasized here that the analysis of the auditory pattem here is a local operation. This is highlighted by the integral 3Generally, the cepstrum coefficients refer to a Fourier analysis on the logarithmic power spectrum. Though not exactly the same as conventional definition, the term "cepstral analysis" is used here to emphasize the significance of analyzing the power spectrum in its Fourier domain.</p><p>ranges R ~( s ) and R,(s), and the translation of w(. + s) and U ( .+s) in <ref type="bibr" target="#b29">(29)</ref>. Therefore, the decomposition coefficients { U } , and {U}, are actually indexed by s implicitly.</p><p>2) Relating the {a'}, Coefficients to the {a},'s: With the above formulation, the information conveyed in the auditory spectrum and the effects of auditory processing on source spectral characteristics can be further studied by examing { u ' } ~. By applying long division to (32), we obtain the first two coefficients as ab = wo/uo <ref type="bibr" target="#b33">(33)</ref> a; = ('Ul/UOvoul/u;)(al/ao).</p><p>(</p><formula xml:id="formula_13">)<label>34</label></formula><p>For high-order coefficients, there is a trade-off between the generality and simplicity of the analysis by approximating the self-normalization, namely, reducing the degree of the denominator in (32). Recall that this denominator is simply the acoustic power spectrum smoothed by the relatively broad cochlear filter (29). The justification for such an approximation depends on the bases into which the source or auditory power spectrum are analyzed. For instance, in the ripple decomposition, one may analyze locally the auditory spectrum in terms of the harmonics of a fundamental ripple 00 (i.e., z = ej"O) whose period is commensurate (in a mean square sense) with the width of the cochlear filter (Fig. <ref type="figure" target="#fig_10">9</ref>). In this case, u ( z ) x uo + u1z and the high-order terms u2, u g , . . . are relatively insignificant in comparison with uo. Using the first order approximation for the denominator</p><p>and hence the nth-order coefficients are recursively defined as</p><formula xml:id="formula_15">(36)</formula><p>Furthermore, when Iu1 In &lt;&lt; luol for n &gt; 1, the feed-back type of recursion (36) can be rewritten into a feed-forward fashion (37) Given the above relationships between the coefficients of the acoustic and the auditory power spectral decompositions, we can now examine in more detail how various spectral features are manifested in the early auditory system. voao + W l U l Z + v2a2z* + . . . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Level-Tolerance and Robustness to Wide-Band Adjustment</head><p>As pointed out in the previous section, level tolerance is an immediate consequence of self-normalization. This is also evident in the context of the present analysis since dividing both the numerator and denominator in (32) by a0 (the overall level of the sound signal) does not affect any of the coefficients { u ' } ~. Therefore, the auditory spectrum represents only the level-independent attributes of the acoustic features.</p><p>Level-independent (or scale-invariant) features have been the focus of many psychoacoustical experiments, such as those known as profile analysis <ref type="bibr">[ 111.</ref> In other studies, it has been established that the absolute sound level plays only a minor role in many aspects of sound perception. For example, it is shown in [ 171 that when the temporal variation of sound level is artificially limited or "compressed," the perception of speech for human subjects is not only unimpeded, but improved. Similarly, experiments based on noise masking paradigms suggest that it is the relative (not absolute) intensity of the signal and noise that dominates the perception of the signal [22]. Finally, in many applications where speech and other acoustic signals are employed, level-independent information is often implicitly extracted or emphasized by the use of logarithmic or decibel signal spectra.</p><p>In the auditory spectrum, overall level-tolerance is only a special case of a general insensitivity to broad scale spectral adjustments (as illustrated in Section IV-D). The limits of this insensitivity can be deduced from (29) by noting that spectral components that can be resolved by both the cochlear and differential filters are subject to cancelation. Hence, all spectral fluctuations that are of a similar or a broader scale than the bandwidth of the cochlear filters will be attenuated in the auditory spectrum. Equivalently, in terms of the decompositions described above ( 3 9 , spectral adjustments characterized by coefficients a, for n 2 2 will be undistortedly represented in A ( z ) ; those characterized by a1 will be attenuated and the overall level described by a0 will be eliminated. The perceptual implications of these statements are discussed in more detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C . Characteristics of Low-Order Coefficients ab and U$</head><p>The two low-order coefficients of the auditory spectrum ab and U; are exactly computable from (33) and (34).</p><p>1) Interpretation of the ab: From (33), ab is independent of the sound signal and only reflects the relative gains of the differential and cochlear filters. The auditory spectrum, therefore, provides a common dc reference for all signals. For example, in Fig. <ref type="figure">7</ref>, the auditory spectra for the vowel in various levels of noise maintain roughly the same level despite the increase of total energy in the stimulus. The auditory spectrum barely looks flattened with the increasing noise power. This property is useful for many engineering applications especially when combined with the limited dynamic range in the auditory spectrum. Note that for the white noise, the power spectrum has only the constant term a0 and therefore A ( s ) = V O / U O , which is equal to tc2 other than a constant scaling factor (K. is defined in Section IV).</p><p>2) Interpretations ofthe ai: Unlike the ab, the exact meaning of the ai coefficient depends on the type of decomposition employed. For instance, in terms of a polynomial decomposition, ai denotes the spectral tilt in the auditory spectrum. This tilt, however, is proportional to the normalized spectral slope in the source power spectrum, i.e., it represents a levelindependent spectral tilt (see (34)). This is a desirable attribute since without the normalization, the spectral tilt would vary with the overall sound level, and hence would not serve as a distinguishing cue for phoneme classifications, such as among stop consonants [5], <ref type="bibr" target="#b32">[32]</ref>. Another observation from (34) is that the auditory spectral tilt is attenuated (by the factor w1/uo -wou1/ui) relative to that of the power spectrum, a property for many speech recognition applications [ 151.</p><p>In the case of ripple decomposition, ai represents the relative intensity of the fundamental ripple component in the auditory power spectrum. A perceptually important special case of this decomposition occurs when the source signal contains only one ripple, i.e., S,(z) = a0 + alz. The auditory power spectrum becomes a harmonic series, with the nth component weighted by a; (see <ref type="bibr" target="#b32">(32)</ref>). The largest term, Iv~/uowoul/uil from (34). can therefore roughly be interpreted as the 'gain' of the ripple since a\ is linearly proportional to al/ao. This term (as a function of ripple frequency) is closely analogous to the contrast sensitivity function, commonly measured in psychophysical studies of the visual system [33]. It has recently been psychoacoustically measured for a wide range of ripple frequencies [14]. Fig. <ref type="figure" target="#fig_0">10</ref> shows the experimental results and the prediction from the auditory model. Both curves display the bandpass nature of the ripple sensitivity, namely, subjects are most sensitive to intermediate ripple frequencies around 2 cycles/octave. In the auditory model, the high-ripple frequency roll-off is caused by the finite bandwidth of the differential filters, whereas the low-ripple frequency roll-off is due to the self-normalization, which in turn reflects the broad bandwidth of the cochlear filtef'. Note that, if the coefficients of the ripple decomposition here are viewed as analogous to the commonly used cepstral coefficients, then the transfer function of Fig. <ref type="figure" target="#fig_0">10</ref> is very similar to the bandpass liftering often employed in ASR systems [15]. As yet, there have been no neurophysiological experiments carried out in the early auditory system with rippled spectral stimuli to test these hypotheses directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Characteristics of the High-Order Coejj'icients</head><p>Higher-order coefficients capture the rapidly varying features of the power spectral shape. Under the first degree   where the a and P coefficients belong to two superimposed spectra (e.g., of two vowels), and ao and a1 are the low-order coefficients of the composite. Then it directly follows from (37) that</p><formula xml:id="formula_16">= CY:, + p:, (<label>39</label></formula><formula xml:id="formula_17">)</formula><p>where a; (or PA) is the nth coefficient for the auditory spectrum due to one signal without the presence of the other. Therefore, the model demonstrates that auditory processing possesses a quasi-linear property on the level-independent acoustic features. This conclusion is consistent with the fundamental assumptions underlying many psychoacoustical models and experiments that treat the auditory periphery as a bank of linear filters applied to a logarithmic spectrum (i.e., with level-independent features) [9], [22], [23].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Dominance Principle and Noise Robustness</head><p>The previous superposition principle is obtained by fixing the composite ai during the derivation. It can actually be shown that the first order components also abide by a superposition principle, though not a strictly linear one. Suppose the source signal consists of two (unrelated) spectra that can be described by { a } , and {P}, respectively, i.e.</p><p>Then, according to (34), the first-order coefficients are The dominance principle described above can now be employed to reinterpret the noise robustness property that was explained as a suppression phenomena in the previous section. For instance, consider the example of a signal ({a},) in white noise (with level Po) as in Section IV-E, i.e. Finally, note that for general nonwhite noise, the effect of noise on the source power spectrum can be viewed as adding a jitter on the spectral shape. Therefore, interpreting (37) (or (36)) as a feedforward (feedback) one-step estimation on a, from a,-1 means that the auditory spectrum a; simply encodes the scaled prediction error. Consequently, the auditory spectrum smooths out the added noise through the prediction process. In addition, it will have the nice properties common to prediction errors, such as having a smaller dynamic range (which in this case is roughly bounded by the ratio of the gains of the differential and cochlear filters). This suggests that auditory model may be a suitable front-end for a variety fields of engineering applications such as recognition and coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. SUMMARY AND CONCLUSION</head><p>In summary, we have analyzed and interpreted the properties of a simple analytic model of the early auditory system. The article had three specific objectives: (1) To provide a tractable theoretical framework for this and future analysis of early auditory processing; (2) To interpret the results in the context of basic known psychoacoustical findings; (3) To help justify the advantages of auditory-like and other representations as front-ends for speech recognition systems. Within each of these objectives, the following issues were discussed:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Theoretical Analysis</head><p>stochastic analysis of the model (Section 111) specific extension of the analysis through spectral decomposition to highlight the acoustic-to-auditory transformation of power spectral shape (Section V-A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B . Relation to Psychoacoustics</head><p>the origin of level-tolerance in the auditory spectrum, and its psychoacoustical correlates (Section V-B) the representation of normalized spectral slopes in the auditory pattem, and its relation to speech perception (Section V-C) the auditory representation of rippled spectra, and its relation to the contrast sensitivity function (Section V-C) the linearity of the auditory representation, and its justification of basic assumptions of the psychoacoustical models of the auditory periphery (Section V-D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C . Relevance to Engineering Applications</head><p>For auditory-like models, the analysis explains and quantifies the origins of their experimentally observed noiserobustness as suppression (Sections IV-B and IV-E), as a dominance principle (Section V-E), or as a linear prediction process (Section V-E). The auditory spectrum exhibits (and hence justifies) very similar deformations of the spectrum as those implied by commonly used algorithms in ASR systems. They include the attenuation of spectral slopes (Section V-C), bandpass liftering of cepstral coefficients (Section V-C).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic description of the auditory model (adapted from [38]). (a) Block diagram of the three basic stages in the early auditory system. (b) Quasi-anatomic sketches of the auditory stages. (c) Mathematical model of the different stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>&amp; ( U ) is real and positive, (15) can be rewritten as (w%H(w, s ) a " w H ( w , 4dm4) denotes the inner product in the w domain. The parameter, T , is therefore a quantity measuring the similarity of the two functions, w &amp; H ( w , s ) d m and wH(w, s) d m . According to Cauchy-Schwartz inequality, T 5 1 and the equality holds if and only if the two functions are equal. If the differential and cochlear filters are designed to be dissimilar, T becomes small for broadband S,(w) and dsz 1. For the filters shown in Fig. 2 (used in the following section), Jm usually ranges between 0.76 to 0.96, and is equal to 0.84 for white noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>3) = Hm(uSw) (18)for some constant a and seed filter H,(w). It can be shown that dsH(bJ, 3) = (lOgU)Ud,H(w, 3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The predicted (solid) and computed (dotted) auditory spectrum for a naturally spoken vowel /ad. The computed spectrum is scaled before being superimposed on the predicted pattem. The predicted pattern is obtained by using (20). The channels are labeled with the CF of the corresponding differential filters. Roughly, the relationship between the channel number and the CF is described by s = 2010g2(f/250). The .logarithmic ordinate is labeled by arbitrary units.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example of noise suppression in the auditory spectrum. The solid lines are the computed auditory spectrum for the single tone in noise stimulus. The SNR's (see text for definition) are -m, -6, 0, and 6 dB from top to bottom. The dotted lines are the predicted amount of suppression obtained from (26). The computed auditory spectra are scaled so that the baseline IF is 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Spectral peak enhancement in the auditory spectrum. The solid (dotted) trace is the linear power (auditory) spectrum for the stimulus consisting of a harmonic series and white noise. The computed auditory spectrum is scaled before being superimposed on the power spectrum. Since the two patterns are shown in log scale, this adjustment does not affect the reading of the peak to valley ratio. As predicted, the peak to valley ratio is larger in the auditory spectrum due to the spectral enhancement effect discussed in text. The logarithmic ordiiate is labeled by arbitrary units.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Fig. S(b). For comparison, the preemphasized auditory spectra are shown in Fig.6(c). As can be seen in the previous example, the harmonics of this vowel are clearly resolved in the lowfrequency channels, while for the high-frequency channels, only the formant structures are visible. Despite the large modification on the input spectrum, the auditory spectrum remains relatively stable, with the pre-emphasis effects mostly concentrated in the high-frequency channels. These occur because of relative steepness of the A ( s ) function there, i.e., the distortion is not as broadband, and A'(s) is not small.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Stability of the auditory spectrum against broadband spectral distortion. In (a), the spectral distortion A ( w ) is shown for c = 0.4 (solid), 0.7 (dashed), and 0.9 (dotted). (b) and (c) illustrate the corresponding distorted linear power spectrum and auditory spectrum of the vowel /ad, respectively. Most distortion in the auditory spectrum is located at the high-frequency channels where A ' ( s ) is not small. See text for detailed discussion. The ordinates are labeled by arbitrary units.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>By rewriting<ref type="bibr" target="#b29">(29)</ref> into a z-domain, it directly follows that voao + quiz + . . . uoao+u1a1z+*..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>= (vn/uoao)an -(ulal/uoao)a:,-l. a:, = ( v n /uo)(an/ao) -( % I -1 U1 /.;)(a1 /ao) (an-1 /ao).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. An illustration of first order approximation. The u ( s ) = laZJ H m ( a S ~, , , ) 1 * is shown in solid trace, and, in this example, is approximated by a ripple with 1/3 cycles/octave (dashed line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>4Since the cochlear filter is much broader than the differential filter, 1u1 I &lt;&lt; It~i1 at high ripple frequencies. Hence the gain term( V ~/ U O -V O U 1 / u ~) isdominated by the characteristics of the differential filter, which is itself a lowpass filter for ripples. Therefore the gain drops at the same rate as the ripple transform of the differential filter t i l . On the other hand, when the ripple frequency is low, 1 ~1 + r~o and 111 + i t ] . Accordingly, the gain decreases with decreasing ripple frequency being subject to self-normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Fig. 10. The contrast sensitivity function obtained from psychoacoustic experiments (dashed trace, adapted from (141, Fig. 3.26) and model prediction (solid). The psychoacoustic experiments used single ripple stimuli and the thresholds for detecting the ripple, in peak-to-valley ratio, were recorded for various ripple frequencies. The measured thresholds are then converted from dB to linear scale and inverted to yield the 'gain' depicted in this figure. The model prediction, It11 /uo -uou1/uiI is scaled to fit the psychoacoustic curve.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>a; = ( V l / U O -VO~1/U;)(al/ao) = ( V l / U O -VOUI/U$)(Qll + Pl)/(.O + P o ) i.e., they satisfy a weighted superposition principle. When the absolute level of one component is overwhelmed by the other, say PO &lt;&lt; QO, ai approaches the coefficient of the dominant component (ai),. Such a dominance principle can be generalized for signals with several individual spectra, and the resultant ai would be the centroid of the corresponding normalized a 1 ' s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Y</head><label></label><figDesc>and the corresponding auditory coefficients for A,(z) are given by a; = (A) a; a 0 + Po 2 a:, = ( A ) . : , a0 + Po + ( z ) a0 + Po where {a'}, denote the auditory coefficients for the clean speech A,(z). Note that ao/(ao + PO) = 1/(1+ Po/ao), in which P o / a o is the reciprocal of SNR. When SNR is large, U; x a;, the coefficient for clean speech. However, when SNR is small, the distortion, measured by Euclidean distance between A,(z) and A,(z), roughly increases by the order of 1 -~O / ( Q O +Po) = ( P o / a o ) / ( l +Po/ao). In comparison, the Euclidean distortion for the linear power spectrum increases by the order of P o / a o . Therefore, the distortion of linear power spectrum increases at a rate (l+Po/ao) faster than the auditory spectrum. This trend was previously demonstrated in Fig. 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>The auditory spectra for vowel /aa/ corrupted by white noise at various SNR's (see text for definition). The SNR's (from top to bot") are -24, -12, -6, 0, 6, and 00 dB. The ordinate is labeled by arbitrary units.</head><label></label><figDesc>...... ..... . .. ..i ....................... : ............ .......... 4 . L ................ .i ..... ..............</figDesc><table><row><cell>3.5 c</cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.5 ' 2 5</cell><cell>.35</cell><cell>.S</cell><cell>.I</cell><cell>1</cell><cell>1.4</cell><cell>2</cell><cell>2.8</cell><cell>4</cell><cell>5.6</cell><cell>I 8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">FrapuencY Q e)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fig. 7. '</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ndst lcvd (dB)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>0 .............__...... ..... ... ............. j .............. ........ i. . ......... ......... i ....................... i ........... .......... $1. j /' .................. ,.' i j , . : j ,-: j ,*-: .-? -10 ........ ........ . i... ................... :</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>approximation on the self-normalization,<ref type="bibr" target="#b37">(37)</ref> and<ref type="bibr" target="#b36">(36)</ref> indicate that, if al/ao is regarded as constant, a high-order coefficient {a'}, is also linearly related to the coefficients an/ao of the source power spectrum. This important finding is intuitively surprising given the many nonlinearities of the model. It implies that linearly filtering (i.e., weighting and/or linearly combining the coefficients of) an arbitrary (normalized) source power spectrum is a justified and easily computable procedure that has well-understood effects on the auditory representation. For instance, this is the most important reason why the contrast sensitivity function (Fig.lo), which is measured with a single ripple at a time, is still relevant for the analysis of complex spectral shapes (e.g.. of a vowel) composed of many ripples.The linearity of the high-order coefficients manifests itself as a superposition principle. More specifically, suppose</figDesc><table><row><cell>n &gt; l</cell><cell>n &gt; l</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Dr. L. Lee and E. T. Gan for their enlightening discussions, and three anonymous reviewers for their constructive suggestions regarding the manuscript.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Portions of this article have been presented at ICASSP-93 in April, 1993 at Minneapolis, MN. This work is supported by grants from the Office of Naval Research, the Air Force Office of Scientific Research, and by NSF's Engineering Research Centers Program NSFD CD-88030 12. The associate editor coordinating the review of this paper and approving it for publication was Prof. James M. Kates.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Kuansan <ref type="bibr">Wang (M'94)</ref>   <ref type="bibr">Stanford, CA, in 1977</ref><ref type="bibr">, 1980</ref><ref type="bibr">, and 1980</ref><ref type="bibr">, respectively. From 1981</ref> to 1984, he performed postdoctoral research into the physiology of hearing and the mathematical modeling of the nervous system at Stanford and the National Institute of Health. From 1982 to 1985, he was an Adjunct Professor in the Biomedical Engineering Department at Johns Hopkins University. In 1984, he joined the Department of the Electrical Engineering, University of Maryland, College Park, where he is currently an Associate Professor. His research interests have focused on processing and representation of sound in the auditory nervous system.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cochlear modeling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ASSP Mag</title>
		<imprint>
			<date type="published" when="1985-01">Jan. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Autocorrelogram models of the segregation of competing voices</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Assmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Paschall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Winter ARO Meeting</title>
		<meeting>15th Winter ARO Meeting</meeting>
		<imprint>
			<date type="published" when="1992-02">Feb. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Zero-crossing rates of functions of Gaussian processes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Bamett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kedem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using auditory models for speaker normalization in speech recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bladon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Symp. Speech Recognition</title>
		<meeting>Symp. Speech Recognition<address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Perceptual invariance and onset spectra for stop consonants in different vowel environments</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Blumstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Stevens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Acoustic. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="648" to="4562" />
			<date type="published" when="1980-02">Feb. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The auditory processing and recognition of speech</title>
		<author>
			<persName><forename type="first">W</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Speech, Natural Language Workshop</title>
		<meeting>Speech, Natural Language Workshop</meeting>
		<imprint>
			<date type="published" when="1989-10">Oct. 1989</date>
			<biblScope unit="page" from="325" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A composite model of the auditory periphery for the processing of speech</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Geisler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phonetics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">I</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Temporal non-place information in the auditory-nerve firing pattems as a front-end for speech recognition in a noisy environment</title>
		<author>
			<persName><surname>Ghitza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="109" to="204" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Auditory filter shapes in subjects with unilateral and bilateral cochlear impairments</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Glasherg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C J</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Acoustic. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1020" to="1033" />
			<date type="published" when="1986-04">Apr. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Davisson</surname></persName>
		</author>
		<title level="m">Random Processes</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Green</surname></persName>
		</author>
		<title level="m">P rofrle Analysis: Auditory Intensify Discrimination</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Acoustic transduction in the auditory periphery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Phonetics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptually based processing in automatic speech recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsuga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Makino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wakita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP -86</title>
		<meeting>ICASSP -86<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Auditory processing of sinusoidal spectral envelopes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hillier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<pubPlace>St. Louis, MO</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Washington University, Sever Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the use of bandpass liftering in speech recognition</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Wilpon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="947" to="954" />
			<date type="published" when="1987-07">July 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spectral analysis and discrimination by zero-crossings</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kedem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc IEEE</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1477" to="1493" />
			<date type="published" when="1986-11">Nov. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluation of orthognal polynomial compression</title>
		<author>
			<persName><forename type="first">H</forename><surname>Levitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neuman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoustic. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="241" to="252" />
			<date type="published" when="1991-07">July 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computational models of neural auditory processing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust., Speech, Signal Processing</title>
		<meeting>IEEE Int. Conf. Acoust., Speech, Signal essing<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-07">July 1991</date>
			<biblScope unit="page" from="1188" to="1194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ca)</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984-03">Mar. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Vision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Freeman</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Meyer</surname></persName>
		</author>
		<title level="m">Introductory Probability and Statistical Applications</title>
		<meeting><address><addrLine>Reading, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Representation of stop consonants in discharge patterns of auditory nerve fibers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Sachs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoustic. Soc. Am</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The use of nonsimultaneous masking to measure frequency selectivity and suppression</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C J</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>O'loughlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frequency Selectivify in Hearing</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">C J</forename><surname>Moore</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="179" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Auditory filters and excitation patterns as representations of frequency resolution</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C J</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frequency Selectivity in Hearing</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">C J</forename><surname>Moore</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="123" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Schafer</surname></persName>
		</author>
		<title level="m">Digital Processing of Speech Signals</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>hntice-Hall</publisher>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Effects of nonlinearities on speech encoding in the auditory nerve</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoustic. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="858" to="875" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A joint synchronyhnean-rate model of auditory processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seneff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Phonetics</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">I</biblScope>
			<biblScope unit="page" from="55" to="76" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Speech processing in the auditory system 11: Lateral inhibition and the central processing of speech evoked activity in the auditory nerve</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoustic. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="502" to="517" />
			<date type="published" when="1983">Nov. 1985. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The acoustic features of speech sounds in a model of auditory processing: vowels and voiceless fricatives</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phonetics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spatial and temporal processing in central auditory networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Methods in Neural Modelling</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Segev</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Biophysical model of cochlear processing: Intensity dependence of pure tone responses</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Chadwick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Wiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Monish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rinzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoustic. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1986-07">July 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Frequency discrimination in the auditory system: Place or periodicity mechanisms?</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Siebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1970">1970</date>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="723" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Invariant cues for place of articulation in stop consonants</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Blumstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoustic. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Valois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K De</forename><surname>Valois</surname></persName>
		</author>
		<title level="m">Spatial Vision</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Neural networks that recognize phonemes by their acoustic features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989-12">Dec. 1989</date>
			<pubPlace>College Park, MD</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A functional model of the early auditory system</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. Time-Frequency</title>
		<meeting>Int. Symp. Time-Frequency<address><addrLine>Time-Scale Anal (Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-10">Oct. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Zero-crossings and noise suppression in auditory wavelet transformations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<pubPlace>College Park, MD</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Institute for Systems Research, University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. TR 92-94</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rapid and short term adaptation in auditory nerve responses</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Westerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hearing Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="249" to="260" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Auditory representations of acoustic signals</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory, Spec. Issue Wavelet Transforms, Multiresolution Signal Anal</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1358" to="1368" />
			<date type="published" when="1978">Mar. 1992. 16. 1988. 1978</date>
			<publisher>NOV</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
