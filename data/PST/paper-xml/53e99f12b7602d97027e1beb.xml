<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning from the Wisdom of Crowds by Minimax Entropy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft</orgName>
								<address>
									<addrLine>Research 1 Microsoft Way</addrLine>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
							<email>jplatt@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft</orgName>
								<address>
									<addrLine>Research 1 Microsoft Way</addrLine>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sumit</forename><surname>Basu</surname></persName>
							<email>sumitb@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft</orgName>
								<address>
									<addrLine>Research 1 Microsoft Way</addrLine>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Mao</surname></persName>
							<email>yimao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft</orgName>
								<address>
									<addrLine>Research 1 Microsoft Way</addrLine>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning from the Wisdom of Crowds by Minimax Entropy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1449A9B1D9683E462508D5386CEC4DB2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An important way to make large training sets is to gather noisy labels from crowds of nonexperts. We propose a minimax entropy principle to improve the quality of these labels. Our method assumes that labels are generated by a probability distribution over workers, items, and labels. By maximizing the entropy of this distribution, the method naturally infers item confusability and worker expertise. We infer the ground truth by minimizing the entropy of this distribution, which we show minimizes the Kullback-Leibler (KL) divergence between the probability distribution and the unknown truth. We show that a simple coordinate descent scheme can optimize minimax entropy. Empirically, our results are substantially better than previously published methods for the same problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There is an increasing interest in using crowdsourcing to collect labels for machine learning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12]</ref>. Currently, many companies provide crowdsourcing services. Amazon Mechanical Turk (MTurk) <ref type="bibr" target="#b1">[2]</ref> and CrowdFlower <ref type="bibr" target="#b3">[4]</ref> are perhaps the most well-known ones. An advantage of crowdsourcing is that we can obtain a large number of labels at the low cost of pennies per label. However, these workers are not experts, so the labels collected from them are often fairly noisy. A fundamental challenge in crowdsourcing is inferring ground truth from noisy labels by a crowd of nonexperts.</p><p>When each item is labeled several times by different workers, a straightforward approach is to use the most common label as the true label. From reported experimental results on real crowdsourcing data <ref type="bibr" target="#b18">[19]</ref> and our own experience, majority voting performs significantly better on average than individual workers. However, majority voting considers each item independently. When many items are simultaneously labeled, it is reasonable to assume that the performance of a worker is consistent across different items. This assumption underlies the work of Dawid and Skene <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17]</ref>, where each worker is associated with a probabilistic confusion matrix that generates her labels. Each entry of the matrix indicates the probability that items in one class are labeled as another. Given the observed responses, the true labels for each item and the confusion matrices for each worker can be jointly estimated by a maximum likelihood method. The optimization can be implemented by the expectation-maximization (EM) algorithm <ref type="bibr" target="#b6">[7]</ref>.</p><p>Dawid and Skene's method works well in practice. However, their method only contains a perworker probabilistic confusion model of generating labels. In this paper, we assume a separate probabilistic distribution for each worker-item pair. We propose a novel minimax entropy principle to jointly estimate the distributions and the ground truth given the observed labels by workers in Section 2. The theoretical justification of minimum entropy is given in Section 2.1. To prevent overfitting, we relax the minimax entropy optimization in Section 3. We describe an easy-to-implement technique to carry out the minimax program in Section 4 and link minimax entropy to a principle of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Minimax Entropy Principle</head><p>We propose a model illustrated in Figure <ref type="figure">1</ref>. Each row corresponds to a crowdsourced worker indexed by i (from 1 to m). Each column corresponds to an item to be labeled, indexed by j (from 1 to n).</p><p>Each item has an unobserved label represented as a vector y jl , which is 1 when item j is in class l (from 1 to c), and 0 otherwise. More generally, we can treat y jl as the probability that item j is in class l. We observe a matrix of labels z ij by workers. The label matrix can also be represented as a tensor z ijk , which is 1 when worker i labels item j as class k , and 0 otherwise. We assume that z ij are drawn from π ij , which is the distribution for worker i to generate a label for item j. Again, π ij can also be represented as a tensor π ijk , which is the probability that worker i labels item j as class k. Our method will estimate y jl from the observed z ij .</p><p>We specify the form of π ij through the maximum entropy principle, where the constraints on the maximum entropy combine the best ideas from previous work. Majority voting suggests that we should be constraining the π ij per column, with the empirical observation of the number of votes per class per item i z ijk should match i π ijk . Dawid and Skene's method suggests that we should be constraining the π ij per row, with the empirical confusion matrix per worker j y jl z ijk should match j y jl π ijk . We thus have the following maximum entropy model for π ij given y jl :</p><formula xml:id="formula_0">max π - m i=1 n j=1 c k=1 π ijk ln π ijk s.t. m i=1 π ijk = m i=1 z ijk , ∀j, k, n j=1 y jl π ijk = n j=1 y jl z ijk , ∀i, k, l,<label>(1a)</label></formula><formula xml:id="formula_1">c k=1 π ijk = 1, ∀i, j, π ijk ≥ 0, ∀i, j, k.<label>(1b)</label></formula><p>We propose that, to infer y jl , we should choose y jl to minimize the entropy in Equation (1). Intuitively, making π ij "peaky" means that z ij is the least random given y jl . We make this intuition rigorous in Section 2.1. Thus, the inference for y jl can be expressed by a minimax entropy program:  </p><formula xml:id="formula_2">c k=1 π ijk = 1, ∀i, j, π ijk ≥ 0, ∀i, j, k, c l=1 y jl = 1, ∀j, y jl ≥ 0, ∀j, l.<label>(2a)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Justification for Minimum Entropy</head><p>Now we justify the principle of choosing y jl by minimizing entropy. Think of y jl as a set of parameters to the worker-item label models π ij . The goal in choosing the y jl is to select π ij that are as close as possible to the true distributions π * ij . To find a principle to choose the y jl , assume that we have access to the row and column measurements on the true distributions π * ij . That is, assume that we know the true values of the column measurements φ jk = i π * ijk and row measurements ϕ ikl = j y jl π * ijk , for a chosen set of y jl values. Knowing these true row and column measurements, we can apply the maximum entropy principle to generate distributions π ij :</p><formula xml:id="formula_4">max π - m i=1 n j=1 c k=1 π ijk ln π ijk s.t. m i=1 π ijk = φ jk , ∀j, k, n j=1 y jl π ijk = ϕ ikl , ∀i, k, l.<label>(3)</label></formula><p>Let D KL (• •) denote the KL divergence between two distributions. We can choose y jl to minimize a loss of π ij with respect to π * ij given by</p><formula xml:id="formula_5">(π * , π) = m i=1 n j=1 D KL (π * ij π ij ).<label>(4)</label></formula><p>The minimum loss can be attained by choosing y jl to minimize the entropy of the maximum distributions π ij . This can be shown by writing the Lagrangian of program ( <ref type="formula" target="#formula_4">3</ref>):</p><formula xml:id="formula_6">L = - m i=1 n j=1 c k=1 π ijk ln π ijk + m i=1 n j=1 λ ij c k=1 π ijk -1 + n j=1 c k=1 τ jk m i=1 (π ijk -π * ijk ) + m i=1 c k=1 c l=1 σ ikl n j=1 y jl (π ijk -π * ijk ),</formula><p>where the newly introduced variables τ jk and σ ikl are the Lagrange multipliers. For a solution to be optimal, the Karush-Kuhn-Tucker (KKT) conditions must be satisfied <ref type="bibr" target="#b2">[3]</ref>. Thus,</p><formula xml:id="formula_7">∂L ∂π ijk = -ln π ijk -1 + λ ij + c l=1 y jl (τ jk + σ ikl ) = 0, ∀i, j, k,</formula><p>which can be rearranged as</p><formula xml:id="formula_8">π ijk = exp c l=1 y jl (τ jk + σ ikl ) + λ ij -1 , ∀i, j, k.<label>(5)</label></formula><p>For being a probability measure, the variables π ijk have to satisfy</p><formula xml:id="formula_9">c k=1 π ijk = c k=1 exp c l=1 y jl (τ jk + σ ikl ) + λ ij -1 = 1, ∀i, j.<label>(6)</label></formula><p>Eliminating λ ij by jointly considering Equations ( <ref type="formula" target="#formula_8">5</ref>) and ( <ref type="formula" target="#formula_9">6</ref>), we obtain a labeling model in the exponential family:</p><formula xml:id="formula_10">π ijk = exp c l=1 y jl (τ jk + σ ikl ) c s=1 exp c l=1 y jl (τ js + σ isl ) , ∀i, j, k.<label>(7)</label></formula><p>Plugging Equation ( <ref type="formula" target="#formula_10">7</ref>) into (4) and performing some algebraic manipulations, we prove Theorem 2.1 Let π ij be the maximum entropy distributions in (3). Then,</p><formula xml:id="formula_11">(π * , π) = m i=1 n j=1 c k=1 (π * ijk ln π * ijk -π ijk ln π ijk ).</formula><p>The second term is the only term that depends on y jl . Therefore, we should choose y jl to minimize the entropy of the maximum entropy distributions.</p><p>The labeling model expressed by Equation ( <ref type="formula" target="#formula_10">7</ref>) has a natural interpretation. For each worker i, the multiplier set {σ ikl } is a measure of her expertise, while for each item j, the multiplier set {τ jk } is a measure of its confusability. A worker correctly labels an item either because she has good expertise or because the item is not that confusing. When the item or worker parameters are shifted by an arbitrary constant, the probability given by Equation ( <ref type="formula" target="#formula_10">7</ref>) does not change. The redundancy of the constraints in (2a) causes the redundancy of the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Constraint Relaxation</head><p>In real crowdsourcing applications, each item is usually labeled only a few times. Moreover, a worker usually only labels a small subset of items rather than all of them. In such cases, it is unreasonable to expect that the constraints in (2a) hold for the true underlying distributions π ij . As in the literature of regularized maximum entropy <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b8">9]</ref>, we relax the optimization problem to prevent overfitting:</p><formula xml:id="formula_12">min y max π,ξ,ζ - m i=1 n j=1 c k=1 π ijk ln π ijk - n j=1 c k=1 ξ 2 jk 2α j - m i=1 c k=1 c l=1 ζ 2 ikl 2β i s.t. m i=1 (π ijk -z ijk ) = ξ jk , ∀j, k, n j=1 y jl (π ijk -z ijk ) = ζ ikl , ∀i, k, l,<label>(8a)</label></formula><formula xml:id="formula_13">c k=1 π ijk = 1, ∀i, j, π ijk ≥ 0, ∀i, j, k, c l=1 y jl = 1, ∀j, y jl ≥ 0, ∀j, l,<label>(8b)</label></formula><p>where α j and β i are regularization parameters. It is obvious that program ( <ref type="formula" target="#formula_12">8</ref>) is reduced to program (2) when the slack variables ξ jk and ζ ikl are set to zero. The two 2 -norm based regularization terms in the objective function force the slack variables to be not far away from zero. Other vector or matrix norms, such as the 1 -norm and the trace norm, can be applied as well <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b8">9]</ref>. We choose the 2 -norm only for the sake of simplicity in computation.</p><p>The justification for minimum entropy in Section 2.1 can be extended to the regularized minimax entropy formulation <ref type="bibr" target="#b7">(8)</ref> with minor modifications. Instead of knowing the exact marginals, we need to choose π ij based on noisy marginals:</p><formula xml:id="formula_14">φ jk = m i=1 π * ijk + ξ * jk , ∀j, k, ϕ ikl = n j=1 y jl π * ijk + ζ * ikl , ∀i, k, l.</formula><p>We thus maximize the regularized entropy subject to the relaxed constraints:</p><formula xml:id="formula_15">m i=1 π ijk + ξ jk = φ jk , ∀j, k, n j=1 y jl π ijk + ζ ikl = ϕ ikl , ∀i, k, l.<label>(9)</label></formula><p>Lemma 3.1 To be the regularized maximum entropy distributions subject to <ref type="bibr" target="#b8">(9)</ref>, π ij must be represented as in Equation <ref type="bibr" target="#b6">(7)</ref>. Moreover, we should have</p><formula xml:id="formula_16">ξ jk = α j τ jk , ζ ikl = β i σ ikl .</formula><p>Proof The first part of the result can be verified as before. By using the labeling model in Equation <ref type="bibr" target="#b6">(7)</ref>, the Lagrangian of the regularized maximum entropy program can be written as </p><formula xml:id="formula_17">L = -</formula><p>We cannot minimize the loss by minimizing the right side of Equation ( <ref type="formula" target="#formula_18">10</ref>) since the random noise is unknown. However, we can consider minimizing an upper bound instead. Note that</p><formula xml:id="formula_19">ξ * jk ξ jk ≤ (ξ * 2 jk + ξ 2 jk )/2, ∀j, k, ζ * ikl ζ ikl ≤ (ζ * 2 ikl + ζ 2 ikl )/2, ∀i, k, l.<label>(11)</label></formula><p>Denote by Ω(π, ξ, ζ) the objective function of the regularized minimax entropy program <ref type="bibr" target="#b7">(8)</ref>. Substituting the inequalities in <ref type="bibr" target="#b10">(11)</ref> into Equation ( <ref type="formula" target="#formula_18">10</ref>), we have</p><formula xml:id="formula_20">(π * , π) ≤ Ω(π, ξ, ζ) -Ω(π * , ξ * , ζ * ).<label>(12)</label></formula><p>So minimizing the regularized maximum entropy leads to minimizing an upper bound of the loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Optimization Algorithm</head><p>A typical approach to constrained optimization is to covert the primal problem to its dual form. By Lemma 3.1, the Lagrangian of program ( <ref type="formula" target="#formula_12">8</ref>) can be written as The dual problem minimizes L subject to the constraints ∆ = {y jl | c l=1 y jl = 1, ∀j, y jl ≥ 0, ∀j, l}. It can be solved by coordinate descent with the variables being split into two groups: {y jl } and {τ jk , σ ikl }. It is easy to check that, when the variables in one group are fixed, the optimization problem on the variables in the other group is convex. When the y jl are restricted to be {0, 1}, that is, deterministic labels, the coordinate descent procedure can be simplified. Let Plugging the last line into the Lagrangian L, we obtain an upper bound of L, called F . It can be shown that we must have y jl = µ jl at any stationary point of F. Our optimization algorithm is a coordinate descent minimization of this F <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b6">7]</ref>. We initialize y jl with majority vote in Equation <ref type="bibr" target="#b12">(13)</ref>. In each iteration step, we first optimize over τ jk and σ ikl in (14a), which can be solved by any convex optimization procedure, and next optimize over y jl using a simple closed form in (14b). The optimization over y jl is the same as applying Bayes' theorem where the result from the last iteration is considered as a prior. This algorithm can be shown to produce only deterministic labels.</p><formula xml:id="formula_21">L = -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Minimax Entropy Learning from Crowds</head><p>input:</p><formula xml:id="formula_22">{z ijk } ∈ {0, 1} m×n×c , {α j } ∈ R n + , {β i } ∈ R m + initialization: y 0 jl = m i=1 z ijl m i=1 c k=1 z ijk , ∀j, l<label>(13)</label></formula><p>for t = 1, 2, . . . </p><formula xml:id="formula_23">y t jl ∝ y t-1 jl m i=1 exp c k=1 z ijk (τ t jk + σ t ikl ) c s=1 exp τ t js + σ t isl , ∀j, l<label>(14b)</label></formula><p>output: {y t jl }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Measurement Objectivity Principle</head><p>The measurement objectivity principle can be roughly stated as follows: (1) a comparison of labeling confusability between two items should be independent of which particular workers are included for the comparison; (2) symmetrically, a comparison of labeling expertise between two workers should be independent of which particular items are included for the comparison. The first statement is about the objectivity of item confusability. The second statement is about the objectivity of worker expertise. In what follows, we mathematically define the measurement objectivity principle. For deterministic labels, we show that the labeling model in Equation ( <ref type="formula" target="#formula_10">7</ref>) can be recovered from the measurement objectivity principle.</p><p>From Equation <ref type="bibr" target="#b6">(7)</ref>, given item j in class l, the probability that worker i labels it as class k is</p><formula xml:id="formula_24">π ijkl = exp (τ jk + σ ikl ) c s=1 exp (τ js + σ isl ) . (<label>15</label></formula><formula xml:id="formula_25">)</formula><p>Assume that a worker i has labeled two items j and j both of which are from the same class l. With respect to the given worker i, for each item, we measure the confusability for class k by</p><formula xml:id="formula_26">ρ ijk = π ijkl π ijll , ρ ij k = π ij kl π ij ll . (<label>16</label></formula><formula xml:id="formula_27">)</formula><p>For comparing the item confusabilities, we compute a ratio between them. To maintain the objectivity of confusability, the ratio should not depend on whichever worker is involved in the comparison. Hence, given another worker i , we should have</p><formula xml:id="formula_28">π ijkl π ijll π ij kl π ij ll = π i jkl π i jll π i j kl π i j ll .<label>(17)</label></formula><p>It is straightforward to verify that the labeling model in Equation ( <ref type="formula" target="#formula_24">15</ref>) indeed satisfies the objectivity requirement given by Equation <ref type="bibr" target="#b16">(17)</ref>. We can further show that a labeling model which satisfies Equation <ref type="bibr" target="#b16">(17)</ref> has to be expressed by Equation <ref type="bibr" target="#b14">(15)</ref>. Let us rewrite Equation <ref type="bibr" target="#b16">(17)</ref> as</p><formula xml:id="formula_29">π ijkl π ijll = π ij kl π ij ll π i jkl π i jll π i j ll π i j kl .</formula><p>Without loss of generality, choose i = 0 and j = 0 as the fixed references such that</p><formula xml:id="formula_30">π ijkl π ijll = π i0kl π i0ll π 0jkl π 0jll π 00ll π 00kl .<label>(18)</label></formula><p>Assume that the referenced worker 0 chooses a class uniformly at random for the referenced item 0. So we have π 00ll = π 00kl = 1/c. Equation ( <ref type="formula" target="#formula_30">18</ref>) implies π ijkl ∝ π i0kl π 0jkl . Reparameterizing with Symmetrically, we can also start from the objectivity of worker expertise to recover the labeling model in <ref type="bibr" target="#b14">(15)</ref>. Assume that two workers i and i have labeled a common item j which is from class l. With respect to the given item j, for each worker, we measure the confusion from class l to k by</p><formula xml:id="formula_31">ρ ijk = π ijkl π ijll , ρ i jk = π i jkl π i jll .<label>(19)</label></formula><p>For comparing the worker expertises, we compute a ratio between them. To maintain the objectivity of expertise, the ratio should not depend on whichever item is involved in the comparison. Hence, given another item j in class l, we should have</p><formula xml:id="formula_32">π ijkl π ijll π i jkl π i jll = π ij kl π ij ll π i j kl π i j ll .<label>(20)</label></formula><p>We can see that Equation ( <ref type="formula" target="#formula_32">20</ref>) is actually just a rearrangement of Equation ( <ref type="formula" target="#formula_28">17</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental Validation</head><p>We compare our method with majority voting and Dawid &amp; Skene's method <ref type="bibr" target="#b4">[5]</ref> using real crowdsourcing data. One is multiclass image labeling, and the other is web search relevance judging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Image Labeling</head><p>We chose the images of 4 breeds of dogs from the Stanford dogs dataset <ref type="bibr" target="#b7">[8]</ref>: Norfolk Terrier (172), Norwich Terrier (185), Irish Wolfhound (218), and Scottish Deerhound (232) (see Figure <ref type="figure" target="#fig_6">2</ref>). The numbers of the images for each breed are in the parentheses. There are 807 images in total. We submitted them to MTurk, and received the labels from 109 MTurk workers. A worker labeled an image at most once, and each image was labeled 10 times. It is difficult to evaluate a worker if she only labeled few images. We thus only consider the workers who labeled at least 40 images, which yields a label set that contains 7354 labels by 52 workers. Each image has at least 4 labels and around 95% of the images have at least 8 labels. The average accuracy of the workers is 70.60%. The best worker achieved an accuracy of 88.24% while only labeled 68 images. The worker who labeled the most labeled 345 images and achieved an accuracy of 68.99%. The average worker confusion matrix between breeds is shown in Table <ref type="table" target="#tab_1">2</ref>. As expected, it consists of two blocks. One block contains Norfolk Terrier and Norwich Terrier, and the other block contains Irish Wolfhound and Scottish Deerhound. For our method, the regularization parameters are set as α j = 100/(number of labels for item j), β i = 100/(number of labels by worker i). The performance of various methods on this image labeling task is summarized in Table <ref type="table">1</ref>. For this problem, our method is somewhat better than Dawid and Skene's method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Web Search Relevance Judging</head><p>In another experiment, we asked workers to rate a set of 2665 query-URL pairs on a relevance rating scale from 1 to 5. The larger the rating, the more relevant the URL. using consensus from 9 experts. The noisy labels were provided by 177 nonexpert workers. Each pair was judged by around 6 workers, and each worker judged a subset of the pairs. The average accuracy of workers is 37.05%. Seventeen workers have an accuracy of 0 and they judged at most 7 pairs. The worker who judged the most judged 1225 pairs and achieved an accuracy of 76.73%. For our method, the regularization parameters are set as α j = 200/(number of labels for item j), β i = 200/(number of labels by worker i). The performance of various methods on this relevance judging task is summarized in Table <ref type="table">1</ref>. In this case, our method is substantially better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>This paper can be regarded as a natural extension to Dawid and Skene's work <ref type="bibr" target="#b4">[5]</ref>, discussed in Section 1. Our approach can be reduced to Dawid and Skene's by setting the regularization parameters to be α j = ∞, β i = 0. The essential difference between our work and Dawid and Skene's work is that, in addition to worker expertise, we also take item confusability into account.</p><p>In computer vision, a minimax entropy method was proposed for estimating the probability density of certain visual patterns such as textures <ref type="bibr" target="#b21">[22]</ref>. The authors compute empirical marginal distributions through various features, then construct a density model that can reproduce all empirical marginal distributions. Among all models satisfying the constraints, the one with maximum entropy is preferred. However, one wants to select the features which are most informative: the constructed model should approximate the underlying density by minimizing a KL divergence. The authors formulate the combined density estimation and feature selection as a minimax entropy problem.</p><p>The measurement objectivity principle is inspired by the Rasch model <ref type="bibr" target="#b15">[16]</ref>, used to design and analyze psychological and educational measurements. In the Rasch model, given an examinee and a test item, the probability of a correct response is modeled as a logistic function of the difference between the examinee ability and the item difficulty. Rasch defined "specific objectivity": the comparison of any two subjects can be carried out in such a way that no other parameters are involved than those of the two subjects. The specific objectivity property of the Rasch model comes from the algebraic separation of examinee and item parameters. If the probability of a correct response is modeled with other forms, such as a logistic function of the ratio between the examinee ability and the item difficulty <ref type="bibr" target="#b20">[21]</ref>, objective measurements cannot be achieved. The most fundamental difference between the Rasch model and our work is that we must infer ground truth, rather than take them as given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have proposed a minimax entropy principle for estimating the true labels from the judgements of a crowd of nonexperts. We have also shown that the labeling model derived from the minimax entropy principle uniquely satisfies an objectivity principle for measuring worker expertise and item confusability. Experimental results on real-world crowdsourcing data demonstrate that the proposed method estimates ground truth more accurately than previously proposed methods. The presented framework can be easily extended. For example, in the web search experiment, the multilevel relevance scale is treated as multiclass. By taking the ordinal property of ratings into account, the accuracy may be further improved. The framework could be extended to real-valued labels. A detailed discussion on those topics is beyond the scope of this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>π</head><label></label><figDesc>ijk ln π ijk s.t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>z</head><label></label><figDesc>ijk , ∀j, k, n j=1 y jl π ijk = n j=1 y jl z ijk , ∀i, k, l,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>yy 3 . 2</head><label>32</label><figDesc>jl (τ js + σ isl )ijk + (ξ jk -ξ * jk ) + jl π * ijk + (ζ ikl -ζ * ikl ) .For fixed τ jk and σ ikl , maximizing the Lagrange dual over ξ jk and ζ ikl provides the proof.By Lemma 3.1 and some algebraic manipulations, we obtainTheorem Let π ij be the regularized maximum entropy distributions subject to<ref type="bibr" target="#b8">(9)</ref>. Then,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>jl (τ jk + σ ikl )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>µµ</head><label></label><figDesc>ijk (τ jk + σ ikl ) c s=1 exp (τ js + σ isl ) . For any set of real-valued numbers {µ jl | c l=1 µ jl = 1, ∀j, µ jl &gt; 0, ∀j, l}, we have the inequality jl (τ jk + σ ikl ) c s=1 exp c l=1 y jl (τ js + σ isl ) jl ln(y jl p jl )jl ln µ jl .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>js + σ isl ) -c k=1 z ijk (τ jk + σ ikl )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sample images of four breeds of dogs from the Stanford dogs dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The true labels were derived by Average worker confusion (%)</figDesc><table><row><cell>Method</cell><cell>Dogs</cell><cell>Web</cell><cell></cell><cell cols="4">Norfolk Norwich Irish Scottish</cell></row><row><cell cols="3">Minimax Entropy 84.63 88.05</cell><cell>Norfolk</cell><cell>71.04</cell><cell>27.35</cell><cell>1.03</cell><cell>0.58</cell></row><row><cell>Dawid &amp; Skene</cell><cell cols="2">84.14 83.98</cell><cell>Norwich</cell><cell>31.99</cell><cell>66.71</cell><cell>1.13</cell><cell>0.18</cell></row><row><cell>Majority Voting</cell><cell cols="2">82.09 73.07</cell><cell>Irish</cell><cell>1.19</cell><cell>0.55</cell><cell>69.35</cell><cell>28.91</cell></row><row><cell>Average Worker</cell><cell cols="2">70.60 37.05</cell><cell>Scottish</cell><cell>1.20</cell><cell>0.38</cell><cell>26.77</cell><cell>71.65</cell></row><row><cell cols="3">Table 1: Accuracy of methods (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Daniel Hsu, Xi Chen, Chris Burges and Chris Meek for helpful discussions, and Gabriella Kazai for generating the web search dataset.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unifying divergence minimization and statistical inference via convex duality</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Altun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Annual Conference on Learning Theory</title>
		<meeting>the 19th Annual Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Amazon Mechanical</forename><surname>Turk</surname></persName>
		</author>
		<ptr target="https://www.mturk.com/mturk" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="http://crowdflower.com/" />
		<title level="m">CrowdFlower</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Maximum likeihood estimation of observer error-rates using the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vox populi: Collecting high-quality labels from a crowd</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual Conference on Learning Theory</title>
		<meeting>the 22nd Annual Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Maximum entropy density estimation with generalized regularization and an application to species distribution modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dudik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1217" to="1260" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Approximating the wisdom of the crowd</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ertekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hirsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Computational Social Science and the Wisdom of Crowds</title>
		<meeting>the Workshop on Computational Social Science and the Wisdom of Crowds</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Quality management on Amazon Mechanical Turk</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD Workshop on Human Computation</title>
		<meeting>the ACM SIGKDD Workshop on Human Computation</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="64" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combining human and machine intelligence in largescale crowdsourcing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems</title>
		<meeting>the 11th International Conference on Autonomous Agents and Multiagent Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="467" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Iterative learning for reliable crowdsourcing systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1953" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Boosting and maximum likelihood for exponential models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lebanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="447" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A view of the EM algorithm that justifies incremental, sparse, and other variants</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning in Graphical Models</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On general laws and the meaning of measurement in psychology</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rasch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the 4th Berkeley Symposium on Mathematical Statistics and Probability<address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1961">1961</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="321" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning from crowds</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1297" to="1322" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inferring ground truth from subjective labelling of venus images</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1085" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cheap and fast-but is it good? Evaluating non-expert annotations for natural language tasks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The multidimensional wisdom of crowds</title>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2424" to="2432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Whose vote should count more: optimal integration of labels from labelers of unknown expertise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ruvolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2035" to="2043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Minimax entropy principle and its applications to texture modeling</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1627" to="1660" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
