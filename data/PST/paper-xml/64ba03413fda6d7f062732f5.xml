<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reparameterized Policy Learning for Multimodal Trajectory Optimization</title>
				<funder ref="#_jafG5Nw">
					<orgName type="full">Qualcomm AI and AI Institute for Learning-Enabled Optimization at Scale</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-07-20">20 Jul 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
							<email>&lt;z2huang@ucsd.edu&gt;.</email>
							<affiliation key="aff0">
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Litian</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhan</forename><surname>Ling</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuanlin</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">UMass Amherst</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reparameterized Policy Learning for Multimodal Trajectory Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-07-20">20 Jul 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2307.10710v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate the challenge of parametrizing policies for reinforcement learning (RL) in highdimensional continuous action spaces. Our objective is to develop a multimodal policy that overcomes limitations inherent in the commonlyused Gaussian parameterization. To achieve this, we propose a principled framework that models the continuous RL policy as a generative model of optimal trajectories. By conditioning the policy on a latent variable, we derive a novel variational bound as the optimization objective, which promotes exploration of the environment. We then present a practical model-based RL method, called Reparameterized Policy Gradient (RPG), which leverages the multimodal policy parameterization and learned world model to achieve strong exploration capabilities and high data efficiency. Empirical results demonstrate that our method can help agents evade local optima in tasks with dense rewards and solve challenging sparse-reward environments by incorporating an object-centric intrinsic reward. Our method consistently outperforms previous approaches across a range of tasks. Code and supplementary materials are available on the project page https: //haosulab.github.io/RPG/</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Reinforcement learning (RL) with high-dimensional continuous action space is notoriously hard despite its fundamental importance for many application problems such as robotic manipulation <ref type="bibr" target="#b49">(OpenAI et al., 2019;</ref><ref type="bibr" target="#b45">Mu et al., 2021)</ref>. In practice, popular frameworks <ref type="bibr" target="#b67">(Silver et al., 2014;</ref><ref type="bibr" target="#b18">Haarnoja et al., 2018;</ref><ref type="bibr" target="#b64">Schulman et al., 2017)</ref> of deep RL formulate the continuous policy as a neural network that outputs a single-modal density function over the action space (e.g., a Gaussian distribution over actions). This formulation, however, breaks the promise of RL being a global optimizer of the return function because the single-modality policy parameterization introduces local minima that are hard to escape using gradients w.r.t. distribution parameters. Besides, a single-modality policy will significantly weaken the exploration ability of RL algorithms because the sampled actions are usually concentrated around the modality.</p><p>Although there are other candidates beyond the Gaussian distribution for policy parameterization, they often have limitations when used for continuous policy modeling. For example, Gaussian mixture models can only accommodate a limited number of modes; normalizing flow methods <ref type="bibr" target="#b61">(Rezende &amp; Mohamed, 2015)</ref> can compute density values, but they may not be as numerically robust due to their dependency on the determinant of the network Jacobian; furthermore, normalizing flows must apply continuous transformations onto a continuously connected distribution, making it difficult to model disconnected modes <ref type="bibr" target="#b59">(Rasul et al., 2021)</ref>. Option-critic <ref type="bibr" target="#b2">(Bacon et al., 2017)</ref> represents policies with options and temporal structure, but it often requires specially designed option spaces for efficient learning, which motivates research on hierarchical imitation learning that uses demonstrations to avoid exploration problems <ref type="bibr" target="#b53">(Peng et al., 2022;</ref><ref type="bibr" target="#b10">Fang et al., 2019)</ref>. Skill discovery methods learn a population of skills without demonstrations or rewards by optimizing for diversity <ref type="bibr" target="#b8">(Eysenbach et al., 2018)</ref>. However, the separation of optimization and skill learning can be nonefficient as it expends effort on learning task-irrelevant skills and may ignore more important ones that would benefit a specific task. This paper presents a principled framework for learning the continuous RL policy as a multimodal density function through multimodal action parameterization. We adopt a sequence modeling perspective <ref type="bibr" target="#b7">(Chen et al., 2021)</ref> and view the policy as a density function over the entire trajectory space (instead of the action space) <ref type="bibr" target="#b81">(Ziebart, 2010;</ref><ref type="bibr" target="#b34">Levine, 2018)</ref>. This allows us to sample a population of trajectories that cover multiple modalities, enabling concurrent exploration of distant regions in the solution space. Additionally, we use a generative model to parameterize the multimodal policies, drawing inspiration from their success in modeling highly complex distributions such as natural images <ref type="bibr" target="#b13">(Goodfellow et al., 2016;</ref><ref type="bibr" target="#b80">Zhu et al., 2017;</ref><ref type="bibr" target="#b62">Rombach et al., 2022;</ref><ref type="bibr" target="#b57">Ramesh et al., 2021)</ref>. We condition the policy on a latent variable z and use a powerful function approximator to "reparameterize" the random distribution z into the multimodal trajectory distribution <ref type="bibr" target="#b31">(Kingma &amp; Welling, 2013)</ref>, from which we can sample trajectories ? . This policy parameterization leads us to adopt the variational method <ref type="bibr" target="#b31">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b18">Haarnoja et al., 2018;</ref><ref type="bibr" target="#b44">Moon, 1996)</ref> to derive a novel framework for modeling the posterior of the optimal trajectory using variational inference, which enables us to model multimodal trajectories and maximize the reward with a single objective.</p><p>This framework allows us to build Reparameterized Policy Gradient (RPG), a model-based RL method for multimodal trajectory optimization. The framework has two notable features: First, RPG combines the multimodal policy parameterization with a learned world model, enjoying the sample efficiency of the learned model and gradient-based optimization while providing the additional ability to jump out of the local optima; Second, we equip RPG with a novel density estimator to help the multimodal policy explore in the environments by maximizing the state entropy <ref type="bibr">(Hazan et al., 2019)</ref>. We verify the effectiveness of our methods on several robot manipulation tasks. These environments only provide sparse rewards when the agent successfully fully finishes the task, which is challenging for single-modal policies even when they are guided by intrinsic motivations. In comparison, our method is able to explore different modalities, improve the exploration efficiency, and outperform single-modal policies, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Notably, our method is more robust than single-modal policies and consistently outperforms previous approaches across different tasks.</p><p>Our contributions are multifold: 1. We propose a variational policy learning framework that models the posterior of multimodal optimal trajectories for reward optimization. 2. We demonstrate that multimodal parameterization can help the policy escape local optima and accelerate exploration in continuous policy optimization. 3. When combined with a learned world model and a delicate density estimator, our method, RPG, is able to solve these challenging sparsereward tasks more efficiently and reliably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Policy as Sequential Generative Model. Maximum entropy reinforcement learning <ref type="bibr" target="#b70">(Todorov, 2006;</ref><ref type="bibr" target="#b71">2008;</ref><ref type="bibr" target="#b72">Toussaint, 2009;</ref><ref type="bibr" target="#b81">Ziebart, 2010;</ref><ref type="bibr" target="#b29">Kappen et al., 2012)</ref> can be viewed as variational inference in probabilistic graphical models <ref type="bibr" target="#b34">(Levine, 2018)</ref> with optimality as an observed variable and sampled trajectories as latent variables. When the demonstration or a fixed dataset is provided in the offline RL setting <ref type="bibr" target="#b7">(Chen et al., 2021;</ref><ref type="bibr" target="#b60">Reed et al., 2022)</ref>, policy learning is simplified as a sequence modeling task <ref type="bibr" target="#b7">(Chen et al., 2021;</ref><ref type="bibr" target="#b79">Zheng et al.;</ref><ref type="bibr" target="#b60">Reed et al., 2022)</ref>. They use autoregressive models to learn the distribution of the whole trajectory, including actions, states, and rewards, and use the action prediction as policy. In our work, we learn a sequential generative model of policy for online RL via the variational method.</p><p>Variational Skill Discovery Under additional assumptions of rewards, our method degenerates to skill discovery methods. However, previous skill discovery methods focus on unsupervised reinforcement learning <ref type="bibr" target="#b8">(Eysenbach et al., 2018;</ref><ref type="bibr" target="#b0">Achiam et al., 2018;</ref><ref type="bibr">Campos et al., 2020)</ref> or diverse skill learning <ref type="bibr" target="#b33">(Kumar et al., 2020;</ref><ref type="bibr" target="#b51">Osa et al., 2022)</ref>. These methods build latent variable policy and encourage the policy to reach states that are consistent with the sampled latent variables through a mutual information term as a reward. These methods do not consider reward maximization or exploration when learning the skills, making them differ from our method vastly. For example, <ref type="bibr" target="#b8">Eysenbach et al. (2018)</ref>; <ref type="bibr" target="#b0">Achiam et al. (2018)</ref> does not optimize the learned skill for the environment rewards; <ref type="bibr" target="#b51">Osa et al. (2022)</ref> does not optimize the mutual information along trajectories; <ref type="bibr" target="#b33">Kumar et al. (2020)</ref> needs to solve the optimization problem first before finding a diverse set of solutions. Moreover, these methods fix the latent distributions, limiting their ability to achieve optimality when rewards are given. <ref type="bibr" target="#b41">Mazzaglia et al. (2022)</ref> also learns skills within a learned world model. However, it decouples the exploration and skill learning and needs offline data or data generated from other exploration policies to train the model. In contrast, we are motivated by the parameterization problems in online RL and jointly optimize the latent representation to model optimal trajectories. We show that learning a latent variable model benefits optimization and exploration and they can be considered together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical Methods</head><p>The hierarchical methods, e.g., option-critic <ref type="bibr" target="#b2">(Bacon et al., 2017)</ref>, can be regarded as a special way of policy parameterization by conditioning the lower-level policy over a sequence of latent variables z = (z 1 , ? ? ? , z T ). Usually, most hierarchical RL methods need special designs for the latent space, e.g., state-based subgoals <ref type="bibr" target="#b32">(Kulkarni et al., 2016;</ref><ref type="bibr" target="#b47">Nachum et al., 2018b;</ref><ref type="bibr">a)</ref> or predefined skills <ref type="bibr" target="#b35">(Li et al., 2020)</ref> to avoid mode-collapse. <ref type="bibr" target="#b50">Osa et al. (2019)</ref> regularized options to maximize the mutual information between the action and the options, which are very relevant to ours. However, it does not model temporal structures as ours to ensure consistency along the trajectories. Goal-conditioned RL <ref type="bibr" target="#b1">(Andrychowicz et al., 2017;</ref><ref type="bibr" target="#b42">Mendonca et al., 2021;</ref><ref type="bibr" target="#b47">Nachum et al., 2018b)</ref> can also be considered a special hierarchical method that uses states or goals to help parameterize the policy and has been proven efficient in exploration, but designing the goal space, sampling and generating goals in high-dimensional space is non-trivial. The specific reward design of goal-reaching tasks also makes extending goal-conditioned policies to general reward functions not easy.</p><p>Hierarchical imitation learning <ref type="bibr" target="#b16">(Gupta et al., 2019;</ref><ref type="bibr" target="#b54">Pertsch et al., 2021;</ref><ref type="bibr" target="#b66">Shankar &amp; Gupta, 2020;</ref><ref type="bibr" target="#b28">Jiang et al., 2022;</ref><ref type="bibr">Lynch et al., 2020;</ref><ref type="bibr" target="#b11">Fang et al., 2020)</ref> extracts temporal abstractions from demonstrations using generative models. For example, InfoGAN <ref type="bibr" target="#b36">(Li et al., 2017)</ref> and ASE <ref type="bibr" target="#b53">(Peng et al., 2022)</ref> use adversarial training <ref type="bibr" target="#b14">(Goodfellow et al., 2020;</ref><ref type="bibr" target="#b25">Ho &amp; Ermon, 2016)</ref> to imitate demonstrations. These works all rely on demonstrations rather than rewards to learn abstractions. <ref type="bibr" target="#b8">Co-Reyes et al. (2018)</ref> learns representation on the collected dataset with variational inference and then utilizes the trained model for planning or policy learning. The separation of the representation learning and reward maximization makes it differ from our methods: first, it requires a state reconstruction module to supervise the generative model, which is challenging for high-dimensional observations; second, it optimizes neither the latent distribution nor the actions for the reward directly, thus requires additional planning procedure during the execution to find suitable actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminary</head><p>Markov decision process A Markov decision process (MDP) is a tuple of (S, A, P, R), where S is the state space and A is the action space. p(s ? |s, a) is the transition probability that transits state s to another state s ? after taking action a. The function R(s, a, s ? ) computes a reward per transition. A policy ?(a|s) outputs an action distribution according to the state s. Executing a policy ? starting from the initial state s 1 with density p(s 1 ) will result in a trajectory ? , which is a sequence of states and actions {s 1 , a 1 , s 2 , . . . , s t , a t , . . . } where a t ? ?(a|s = s t ), s t+1 ? p(s|s = s t , a = a t ). We also use the terminology environment to refer to an MDP in an RL problem. The discounted reward of a trajectory is R ? (? ) = ? t=1 ? t R(s t , a t , s t+1 ) where 0 &lt; ? &lt; 1 is the discount factor to ensure the series converges. The goal of reinforcement learning (RL) is to find a parameterized policy ? ? that maximizes the expected reward</p><formula xml:id="formula_0">E s1?p(s1) [V ? ? (s 1 )] = E ? ?? ? ,s1?p(s1) [R ? (? )],</formula><p>where V ? ? is the value function. Many environments have an observation space O that is not the same to the state space. In this case the agent may need to identify the state s t from the observation o t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RL as probabilistic inference</head><p>The RL as inference framework <ref type="bibr" target="#b70">(Todorov, 2006;</ref><ref type="bibr" target="#b71">2008;</ref><ref type="bibr" target="#b72">Toussaint, 2009;</ref><ref type="bibr" target="#b81">Ziebart, 2010;</ref><ref type="bibr" target="#b29">Kappen et al., 2012;</ref><ref type="bibr" target="#b34">Levine, 2018)</ref> defines optimality p(O|? ) ? e R(? )/T , where T is a temperature scalar and R(? ) is the total rewards of the trajectory ? . It further defines a prior distribution of the trajectory p(? ) = p(s 1 ) T t=1 p(a t |s t )p(s t+1 |s t , a t ), where p(a t |s t ) is a known prior action distribution, e.g., a Gaussian distribution. Thus, it can compute the density of optimality p(O) = p(O|? )p(? )d? . The goal of the framework is to approximate the posterior distribution of optimal trajectories p(? |O) = p(O|? )p(? ) p(O|? )p(? )d? . In the maximum entropy framework <ref type="bibr" target="#b17">(Haarnoja et al., 2017)</ref>, one can apply evidence lower bound <ref type="bibr" target="#b31">(Kingma &amp; Welling, 2013)</ref> </p><formula xml:id="formula_1">log p(O) ? E ? ?? [log p(O|? ) + log p(? ) -log ?(? )] to train the model.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>To overcome the limitations of single modality policies, we propose to use latent variables to parameterize multimodal policies in Sec. 4.1. We then propose a novel variational bound as the optimization objective to approximate the posterior of optimal trajectories in Sec. 4.2. The variational bound naturally combines maximum entropy RL and includes a term to encourage consistency <ref type="bibr" target="#b80">(Zhu et al., 2017)</ref> between the latent distribution and the sampled trajectories, preventing the policy from mode collapse. To optimize this objective in hard continuous control problems, we propose to learn a world model and build the Reparameterized Policy Gradient, a model-based latent variable policy learning framework in Sec. 4.3.1. We design intrinsic rewards in Sec. 4.3.2 to facilitate exploration. Figure <ref type="figure" target="#fig_3">3</ref> illustrates the whole pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Reparameterize Latent Variables for Multimodal Policy Learning</head><p>Policy parameterization matters. In continuous RL, it is popular to model action distribution with a unimodal Gaussian distribution. However, theoretically, to make sure that the optimal policy will be captured by RL, the function class of continuous RL policies has to include density functions of arbitrary probabilistic distributions (Sutton &amp; Barto, ) by the common practice in literature, we will have trouble -as shown in Figure <ref type="figure" target="#fig_1">2</ref>(C), even if its standard deviation is so large to well cover both modalities, the policy gradient can push it towards the local optimum on the right side, causing it to fail to converge to the global optimum. To address the issue, a more flexible policy parameterization is needed for continuous RL problems, one that is simple to sample and optimize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multimodal policy by reparameterizing latent variables</head><p>Motivated by recent developments in generative models that have shown superiority in modeling complex distributions <ref type="bibr" target="#b31">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b26">Ho et al., 2020;</ref><ref type="bibr" target="#b62">Rombach et al., 2022;</ref><ref type="bibr" target="#b57">Ramesh et al., 2021)</ref>, we propose to parameterize policies using latent variables, as illustrated in <ref type="bibr">Figure 2(D)</ref>. Instead of adding random noise to perturb network outputs to generate an action distribution, we build a generative model of policy distribution by taking random noise as input and relying on powerful neural networks to transform it into actions of various modalities.</p><p>Formally, let z ? Z be a random variable, which can be either continuous or categorical. We design our "policy" as a joint distribution ? ? (z, ? ) of the latent z and the trajectory ? . This paper considers a particular factorization of ? ? (z, ? ) that samples z in the beginning of each episode and then sample trajectory ? conditioning on z:</p><formula xml:id="formula_2">? ? (z, ? ) = p(s1)? ? (z|s1) T t=1 p(st+1|st, at)? ? (at|z, st) (1)</formula><p>where T is the length of the sampled trajectory.</p><p>One can use the policy gradient theorem <ref type="bibr" target="#b69">(Sutton &amp; Barto, 2018)</ref>, i.e., ?J(?) = E ? [R(? )? log p(? )] to optimize the generative model policy. However, computing p(? ) needs to marginalize over z, i.e., computing z p(z, ? ) dz, which is often intractable when z is continuous. Besides, optimizing the marginal distribution log p(? ) by gradient descent suffers from local optimality issues (e.g., using gradient descent to optimize Gaussian mixture models which have latent variables is not effective, so EM is often used instead <ref type="bibr" target="#b48">(Ng, 2000)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Variational Inference for Optimal Trajectory Modeling</head><p>To overcome these obstacles, following <ref type="bibr" target="#b70">Todorov (2006;</ref><ref type="bibr" target="#b71">2008)</ref>  <ref type="formula">2018</ref>), we adopt variational method (maximum entropy RL) to directly optimize the joint distribution of the optimal policy without hassles of integrating over z.</p><p>The evidence lower bound We learn ? ? (z, ? ) using variational inference <ref type="bibr" target="#b31">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b18">Haarnoja et al., 2018;</ref><ref type="bibr" target="#b44">Moon, 1996)</ref>. Like an EM algorithm, we define an auxiliary distribution p ? (z|? ) to approximate the posterior distribution of z conditioning on ? using function approximators. This auxiliary distribution p ? (z|? ) helps to factorize the joint distribution of optimality O, latent z, and the trajectory ? as p ? (O, z, ? ) = p(O|? )p ? (z|? )p(? ). Treating ? ? (z, ? ) as the variational distribution, we can write the Evidence Lower Bound (ELBO) for the optimality O:</p><formula xml:id="formula_3">log p(O) = Ez,??? ? [log p ? (O, z, ? ) -log ? ? (z, ? )] ELBO + DKL(? ? (z, ? )||p ? (z, ? |O)) ? Ez,??? ? [log p ? (O, ?, z) -log ? ? (z, ? )] = Ez,??? ? [log p(O, ? ) + log p ? (z|? ) -log ? ? (z, ? )] = Ez,? ? ? ?log p(O|? ) reward + log p(? ) prior + log p ? (z|? ) cross entropy -log ? ? (z, ? ) entropy ? ? ? (2)</formula><p>If we optimize ? ? (z, ? ) and p ? (z|? ) using the gradient of the variational bound, the variational distribution ? ? (z, ? ) learns to model the optimal trajectory distribution p(? |O).</p><p>How it works ELBO contains four parts that can all be computed directly given the sampled z and ? (the environment probability p(s t+1 |s t , a t ) is canceled as in <ref type="bibr" target="#b34">(Levine, 2018)</ref>). The first two parts are the predefined reward log p(O|? ) = R(? )/T + c, where T is the temperature scalar, and c is the normalizing constant that can be ignored in optimization. The prior distribution p(? ) is assumed to be known. The third part is the log-likelihood of z, defined by our auxiliary distribution p ? (z|? ). It is easy to see that if we fix ? ? , maximize p ? alone will minimize the cross-entropy E z,? ?? ? [-log p ? (z|? )], similar to the supervised learning of predicting z given ? . This achieves optimality when p ? (z|? ) = p ? (z|? ) = ? ? (z,? ) z ? ? (z,? )dz , modeling the posterior of z for ? sampled from ? ? . On the other hand, by fixing ?, the policy ? ? is encouraged to generate trajectories that are easy to identify or classify; this helps to increase diversity and enforce consistency to avoid mode collapse, letting the network not ignore the latent variables. The fourth part is the policy entropy that enables maximum entropy exploration. Maximizing all terms together for the parameters ? and ? will minimize</p><formula xml:id="formula_4">D KL (? ? (z, ? )||p ? (z, ? |O)) = D KL (? ? (z, ? )||p ? (z|? )p(? |O))</formula><p>. The optimality can be achieved when p ? (z|? ) equals to p(z|? ), the true posterior of z. Then,</p><formula xml:id="formula_5">p ? (? ) = p ? (z|? )p(? |O)/p(z|? ) = p(? |O) where p ? (? ) = ? ? (?, z)dz is the marginal distribution of ? sampled from ? ? .</formula><p>Relationship with other methods Our method is closely related to skill discovery methods <ref type="bibr" target="#b8">(Eysenbach et al., 2018;</ref><ref type="bibr" target="#b41">Mazzaglia et al., 2022)</ref>. A skill discovery method usually uses mutual information I(?, z) = H(? ) -H(? |z) or H(z) -H(z|? ) ? E z,? [log p ? (z|? ) -log p(z)] to encourage diversity. For example, <ref type="bibr">DIYAN (Eysenbach et al., 2018)</ref> directly optimizes mutual information to learn various skills without reward. Dropping out the reward term in Eq. 2 shows that the skill learning objective can be seamlessly embedded into the "RL as inference" framework with external reward, and there is no need to introduce the mutual information term manually. Furthermore, the framework suggests we can model the posterior of the optimal trajectories, which enables us to unify generative modeling and trajectory optimization in a single framework. As for the relationship of our method with other generative models, we refer readers to a more thorough discussion in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Reparameterized Policy Gradient for Model-based Exploration</head><p>We now describe Reparameterized Policy Gradient (RPG), a model-based RL method with intrinsic motivation for sample efficient exploration in continuous control environments.</p><p>We first simplify the right side of Eq. 2 using the factorization in Eq. 1 and assuming log p ? (z|? ) = t&gt;0 log p(z|s t , a t ). Thus, the ELBO becomes -log ? ? (z|s 1 ) + ? t=1 R(s t , a t )/Tlog ? ? (a t |s t , z) + log p ? (z|s t , a t ), which can be optimized with an RL algorithm by maximizing the reward</p><formula xml:id="formula_6">R(s t , a t )/T rt -? log ? ? (a t |s t , z) + ? log p ? (z|s t , a t ) r ? t ,</formula><p>where scalars ?, ? control the exploration and consistency. We use neural networks to model log p ? (z|s t , a t ) and ? ? (a t |s t , z).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">MODEL-BASED RL WITH LATENT VARIABLES</head><p>In our method Reparameterized Policy Gradient (RPG), we train a differentiable world model <ref type="bibr" target="#b19">(Hafner et al., 2019;</ref><ref type="bibr" target="#b63">Schrittwieser et al., 2020;</ref><ref type="bibr" target="#b76">Ye et al., 2021;</ref><ref type="bibr" target="#b21">Hansen et al., 2022)</ref> to improve data efficiency. The world model contains the following components: observation encoder</p><formula xml:id="formula_7">s t = f ? (o t ), reward predictor r t = R ? (s t , a t ), Q value Q t = Q ? (s t , a t , z) and dynamics s t+1 = h ? (s t , a t ).</formula><p>Given any z and latent state s t0 = f ? (o t0 ) at time step t 0 , the learned dynamics network can generate an imaginary trajectory for any action sequence. If we sample actions from the policy ? ? (a t |s t , z) for t ? t 0 and execute them in the latent model, it will produce a Monte-Carlo estimate for the value of s t0 for optimizing the policy ? ? :</p><formula xml:id="formula_8">Vest(ot 0 , z) ? ? K (Qt 0 +K + r ? t 0 +K ) + t 0 +K-1 t=t 0 ? t-t 0 (rt + r ? t )<label>(3)</label></formula><p>We self-supervise the dynamics network to ensure state consistency without reconstructing observations as in <ref type="bibr" target="#b76">(Ye et al., 2021;</ref><ref type="bibr" target="#b21">Hansen et al., 2022)</ref>. For any latent variable z and trajectory segments of length K + 1 ? t0:t0+K = {o t0 , a gt t0 , r gt t0 , o t0+1 , . . . , o t0+K } sampled from the replay buffer, we execute actions {a gt t } in the world model and use the following loss function to train the world model, as well as the Q function:</p><formula xml:id="formula_9">L ? (? ) = t 0 +K-1 t=t 0 L1?st+1 -ng(f ? (ot+1))? 2 + L2(rt -r gt t ) 2 + L3(Qt -ng(r gt t + ?Vest(ot+1, z))) 2<label>(4)</label></formula><p>where ng(x) means stopping gradient and L 1 = 1000, L 2 = L 3 = 0.5 are constants to balance the loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">MAXIMIZE STATE ENTROPY WITH OBJECT-CENTRIC RANDOMIZED NETWORK DISTILLATION</head><p>For challenging continuous control tasks with sparse rewards, policies that maximize the action entropy of ? ? (a|s, z) usually have trouble obtaining a meaningful reward, making its exploration inefficient. We follow <ref type="bibr">(Hazan et al., 2019)</ref> to let the policy additionally maximize the entropy of the discounted stationary state distribution</p><formula xml:id="formula_10">d ? (s) = (1 -?)</formula><p>? t=1 ? t P (s t = s|?). We use the object-centric Randomized Network Distillation (RND) <ref type="bibr" target="#b4">(Burda et al., 2018)</ref> as a simple and effective method to approximate the state density in continuous control tasks. RND uses a network g ? (o t ) to distill the output of a random network g ? (o t ) by minimizing the difference ?g ? (o t ) -g ? (o t )? 2 over states sampled by the current agent and treat the difference as the negative density of each observation o t .</p><p>We make several modifications to the vallina RND to improve its performance for state vector observations in control problems. First, we inject object-prior to the RND estimator to make the policy sensitive to regions that include objects' position change. Specifically, before feeding objects' coordinates into the network, we apply positional encoding <ref type="bibr" target="#b73">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b43">Mildenhall et al., 2021)</ref> to turn all scalars x to a vector of {sin(2 i x), cos(2 i x)} i=1,2,... for objects of interest (e.g., in robot manipulation, the end effector of the robot and the object). Second, we use a large replay buffer to store past states to avoid catastrophic forgetting <ref type="bibr" target="#b78">(Zhang et al., 2021)</ref>. We verified that it is necessary to normalize the RND's output to stabilize the training and make it an approximated density estimator. Lastly, to account for the latent world model, we relabel trajectories' rewards sampled from the replay buffer instead of estimating them directly in the latent model by reconstructing the observation.</p><p>An implicit benefit of a latent variable policy model is its ability to maximize the state entropy better, as will be shown in the experiments of Sec. 5.1. When combined with our RND method, RPG achieves much better state coverage while single modality policy cannot stabilize. The combination of multimodal policy learning and state entropy maximization accelerates the exploration of continuous control tasks with sparse rewards. We describe the whole algorithm in Alg. 1 and implementation details in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first illustrate the potential of RPG in optimization and exploration through two example tasks. We then show that our method can help solve hard continuous control problems, even with only sparse rewards. We ablate essential design choices and provide additional experiments in section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Illustrative Experiments</head><p>Can multimodal policies help escape local optima? We study the effects of our method on a 1D bandit problem as shown in Fig. <ref type="figure">4</ref>. It has a 1d action space and a non-convex reward landscape with an additional discontinuous point. Fig. <ref type="figure">4</ref> compares the performance of our method with a single modality Gaussian policy optimized by REINFORCE. Notice that we do not add the intrinsic reward for dense reward maximization tasks. The Gaussian policy, initialized at 0 with a large standard deviation, can cover the whole solution space. However, the gradient w.r.t ? is positive, which means the action probability density will be pushed towards the right, as the expected return on the right side is larger than the left side, although the left side contains a higher extreme value. As a result, the policy will move right and get stuck at the local optimum with a low chance of jumping out. In contrast, under the entropy maximization formulation, our method maximizes the reward while seeking to increase diversity, providing more chances for the policy to explore the whole solution space. Furthermore, by turning the latent variables into action distribution, our method can build a multimodal policy distribution that fits the multimodal rewards, explore both modalities simultaneously, and eventually stabilize at the global optimum. This experiment suggests that a multimodal policy is necessary for reward maximization, and our method can help the policy better handle local optima.</p><p>Can multimodal policies accelerate exploration? We argue that maintaining a multimodal policy is beneficial even in the existence of an intrinsic reward to guide the exploration. We illustrate it in a 2D maze navigation task shown in Figure <ref type="figure">5</ref>. The maze consists of 5 ? 5 grids. Each of them is connected with neighbors with a narrow passage. The agent starts in the center grid and can move in four directions. The action space is its position change in two directions (?x, ?y).</p><p>We apply RPG and single-modality model-based SAC <ref type="bibr" target="#b18">(Haarnoja et al., 2018)</ref> on this environment to maximize the intrinsic reward described in Sec. 4.3.2. We count the areas covered by the two policies during exploration with respect to the number of samples in Fig. <ref type="figure">5(D)</ref>. The curve suggests that our method explores the domain much faster, quickly reaching most grids, while the Gaussian agent only covers the right part of the maze within a limited sample budget.</p><p>To understand their differences, we visualize states sampled at different training steps of the two policies in Fig. <ref type="figure">5</ref> (A-B). Our policy below quickly finds four directions to move and gradually expands the state distribution until it fully occupies all grids. Fig. <ref type="figure">5</ref>(C) shows the historic state visitation count. It is easy to see that our multimodal policy induces a more uniform distribution over the whole state space, generating a higher state distribution entropy. The optimization procedure of single-modality policy, as shown in the first row of Fig. <ref type="figure">5</ref>, suffers from its policy parameterization. It can only explore one modality every time and has to switch modalities one by one, where modalities refer to different regions of the state space. It is hard to predict when it switches modality, making algorithms behave vastly differently in different environments with different random seeds. Sometimes it moves slowly from one direction to another because it has to wait for samples for density estimators to generate enough momentum. As a result, it never explores the left side in Fig. <ref type="figure">5(C</ref>). While sometimes, it switches too fast due to the fast updates of the network and does not exploit some modalities enough, missing far-end grids of certain directions that it has explored once. This also causes issues when maximizing external rewards. Even if a single-modal policy finds the optimal solution, it may switch to another modality to continue exploration and it is hard to guarantee that it would come back in the end. In contrast, our method is more like Monte-Carlo sampling, which samples all candidates while converging to solutions of high rewards with high probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Continuous Control Problems</head><p>We now verify if our method can scale up and help solve challenging continuous control problems. We take 8 representative environments from standard RL benchmarks, including 2 table-top environments from MetaWorld <ref type="bibr" target="#b77">(Yu et al., 2020)</ref>, 2 dexterous hand manipulation tasks from Rajeswaran et al. ( <ref type="formula">2017</ref>), 1 navigation problems from <ref type="bibr" target="#b47">Nachum et al. (2018b)</ref>, and 2 articulated object manipulation from ManiSkill <ref type="bibr" target="#b45">(Mu et al., 2021)</ref>. We show environment examples and provide a detailed environment description in Appendix C. Only Cabinet (Dense) and AntPush contain dense rewards that lead to local optima. The remaining 6 environments all only provide sparse rewards, which means the agents receive a reward 1 when it succeeds to finish the task and 0 otherwise. This change dramatically increases the difficulty of these environments and disastrously hurts the performance of classical RL methods like SAC <ref type="bibr" target="#b18">(Haarnoja et al., 2018)</ref> and PPO <ref type="bibr" target="#b64">(Schulman et al., 2017)</ref>.</p><p>We evaluate our methods against the following baselines: DreamerV2 + Plan2Explore <ref type="bibr" target="#b65">(Sekar et al., 2020)</ref>, abbreviated as DreamerV2 (P2E), a model-based exploration method based on the disagreement of learned models' prediction.</p><p>We also consider 3 baselines, TDMPC, MBSAC, and SAC using the same intrinsic rewards as ours. The suffix (R) means that when we apply these methods to a sparse-reward environment, we will add RND intrinsic rewards that are the same as in our method. For all results evaluated on densereward environments in Figure <ref type="figure">6</ref>, the exploration method of the corresponding algorithm is disabled. The standard SAC without intrinsic rewards validates the difficulty of our tasks. Details of the baseline implementations are in Appendix D.</p><p>Fig. <ref type="figure">6</ref> and 7 plots the learning progress of each algorithm in all environments (x-axis: number of environment interaction steps in million, y-axis: task success rate). For all environments, we run each algorithm for at least five trials. The curve and the shaded region shows the average and the standard deviation of performance over trials. MBSAC shares almost the same implementation as our method, except that it does not condition its policy on latent variables..</p><p>We first observe that, for dense reward tasks, our method largely improves the success rate on tasks with local optima (Fig. <ref type="figure">6</ref>). We can see that in both AntPush and Cabinet (Dense) tasks, our method outperforms all baselines. Our method consistently finds solutions, regardless of the local optima in the environments. For example, in the task of opening the cabinets' two doors and going to the two sides of the block, our method usually explores the two directions simultaneously and converges at the global optima. In contrast, other methods' performance highly depends on their initialization. If the algorithm starts by opening the wrong doors or pushing the block in the wrong direction, it will not escape from the local minimums; thus, its success rates are low.</p><p>Our methods successfully solve the 6 sparse reward tasks as shown in Fig. <ref type="figure">7</ref>. Especially, it consistently outperforms the MBSAC(R) baseline, which is a method that only differs from ours by the existence of latent variables to parameterize the policy. Our method reliably discovers solutions in environments that are extremely challenging for other methods (e.g., the StickPull environment), clearly demonstrating the advantages of our method in exploration. Notably, we find that MBSAC(R), which is equipped with our object-centric RND, is a strong baseline that can solve AdroitHammer and AdroitDoor faster than DreamerV2(P2E), proving the effectiveness of our intrinsic reward design. TDMPC(R) has a comparable performance with MBSAC(R) on several environments. We validate that it has a faster exploration speed in Adroit Environments thanks to latent planning. We find that the Dreamer(P2E) does not perform well except for the BlockPush environment without the object prior and is unable to explore the state space well. We visualize modalities explored by our method in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Additional Experiments</head><p>Ablation study We analyze various factors influencing the performance of our method in the Maze navigation task in Section 5.1. More detailed discussion and experiment results are in Appendix B. Experimental comparisons between different latent spaces show that a Gaussian distribution of dimension 12 outperforms the categorical latent space, both surpassing a baseline that does not use latent variables. A moderate latent space size ? 6 is found to be sufficient, with performance declining if the latent dimensions are too small. In terms of reward maximization, the weight of the crossentropy term (?) is crucial, with results indicating an ideal range between 0.001 and 0.01 for the RND design. Furthermore, the performance from RND is tied to maintaining a large replay buffer and using positional embedding, with a lack of either resulting in degraded exploration. A comparative analysis of policy parameterization methods shows the superiority of the vanilla Gaussian policy over the Gaussian Mixture Models (GMM) and CEM-based policy. The latter two display several optimization issues; GMM struggles with log-likelihood maximization, and CEM, despite its proficiency at finding local optima, tends to sacrifice its explorative capabilities. Finally, normalizing flow showed initial promise but soon encountered numerical instabilities, highlighting the need for further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation on locomotion environments</head><p>We modified the HalfCheetah-v3 environment in OpenAI Gym <ref type="bibr" target="#b3">(Brockman et al., 2016)</ref> to study the performance of our methods in locomotion tasks, shown in Figure <ref type="figure" target="#fig_0">11</ref> in the appendix. The cheetah robot moves backward for a certain distance to receive a sparse reward of 1 to succeed. Our exploration method was able to effectively aid the exploration of the Cheetah robot and solve the task easily while removing the exploration term that led to the agent getting stuck. However, in this particular task, modeling multi-modal exploration did not increase the sampling efficiency, as there were only two modalities (moving forward and backward), and modelbased SAC could exploit the two modes one by -one and solve the task. This made the advantage of our method negligible in this case. We also evaluated our method compared to SAC <ref type="bibr" target="#b18">(Haarnoja et al., 2018)</ref> on the standard Mujoco environments. Results are shown in Fig. <ref type="figure" target="#fig_1">12</ref>.</p><p>Vision-based RL As a proof of concept, we illustrate, in Fig. <ref type="figure" target="#fig_3">13</ref>, the potential of our method for image observations in a single-block pushing environment: the observation consists of two consecutive 64x64 RGB images; the agent needs to control the red block to push the purple box into the target region. We use 4-layer convolutional networks as the encoder for both the policy network and RND estimator. We compare our method with model-based SAC (RND), which has an intrinsic reward to guide exploration but only models single modality policies, and model-based SAC without RND. The result validates our method's effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitation and Future Work</head><p>Our approach capitalizes on the advantages offered by multiple components, effectively addressing complex exploration issues in continuous spaces. However, it also introduces certain hurdles and constraints. For instance, our intrinsic reward is predicated on assumptions regarding the recognition of objects and their spatial positioning. This approach may be unsuitable in environments with unidentified objects or where observations don't plainly reveal object-related information, akin to scenarios in vision-based RL; Learning the world model typically results in a slower pace of gradient updates; Incorporating a cross-entropy network adds an extra layer of complexity to the network design and training. Therefore, it is worth discussing potential future directions that might address these limitations.</p><p>Object-centric learning for vision-based RL While the Random Network Distillation (RND) is initially tailored for image observations, integrating object-centric design to accelerate exploration in vision-based RL will be an interesting direction. This suggests two typical strategies to apply our method to tasks with vision observations: (1) The first involves directly encoding observations without considering object information. It proves effective in scenarios with no occlusion and a static background, wherein objects emerge as the sole salient feature of the input. We provide a proof-of-concept experiment in Section 5.3. (2) The second approach harnesses computer vision techniques to identify objects for object-centric exploration. This includes applying recent large-scale vision foundation models, which possess zero-shot object detection capabilities as outlined in <ref type="bibr" target="#b78">(Zhang et al., 2022)</ref> or leveraging slot-attention for object discovery as described in <ref type="bibr" target="#b38">(Locatello et al., 2020)</ref>.</p><p>Combining with previous model-based control and planning methods Instead of learning the world model from on-policy data, we can pre-train a physical world model <ref type="bibr" target="#b37">(Li et al., 2019)</ref> or use analytical models <ref type="bibr" target="#b55">(Posa et al., 2014;</ref><ref type="bibr" target="#b27">Huang et al., 2021)</ref> to gain generalizability and efficiency. Moreover, we drew inspiration from RRT-like motion planners <ref type="bibr" target="#b30">(Karaman &amp; Frazzoli, 2011)</ref> to derive our policy to sample over the configuration space and bias the exploration towards significant kinematics changes. Thus, an exciting direction is incorporating structures in model-based control into RL algorithms, including temporal structures like dynamics motion primitives <ref type="bibr" target="#b68">(Stulp &amp; Sigaud, 2013)</ref> and semantic information from TAMP <ref type="bibr" target="#b12">(Garrett et al., 2021)</ref>.</p><p>Extending to other probabilistic models Our method can be viewed as variational inference <ref type="bibr" target="#b58">(Ranganath et al., 2014)</ref> over a particular stochastic computation graph <ref type="bibr" target="#b74">(Weber et al., 2019)</ref>. The computation graph contains hidden variables, and we use the Bellman equation and a learned model to estimate its gradient. This provides a new perspective that bridges online Reinforcement Learning (RL) with generative models and sequence modeling. In the future, we are interested in exploring how sequence-modeling techniques, such as transformers and hierarchical methods, can be used to model the policy in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We derive a framework that models the policy of continuous RL by a multimodal distribution in the variational inference framework. The method reparameterizes latent variables into trajectories like generative models. Under this framework, we learn a world model to help learn multimodal policy data efficiently. Incorporating an object-centric intrinsic reward, our method can solve challenging continuous control problems with little to no reward signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Model-based Reparameterized Policy Gradient</head><p>Input: p ? , ? ? , h ? , R ? , f ? , Q ? and an optional density estimator g ? Initialize p ? , ? ? , construct the replay buffer B. while time remains do Sample start state o 1 and encode it as s 1 = f ? (o 1 ). Select z from ? ? (z|s 1 ).</p><p>Execute the policy ? ? (a|s, z) and store transitions into the replay buffer B. Sample a batch of trajectory segment of length K {? i t:t+K , z} from the buffer B. Optional: update and estimate the density estimator g ? and relabel transitions with the negative density as the intrinsic reward.</p><p>Optimize ? using Equation <ref type="formula" target="#formula_9">4</ref>. Optimize ? ? (a|s, z) with gradient descent to maximize the value estimate in Equation 3 for s, z sampled from the buffer.</p><p>Optimize ? ? (z|s 1 ) with policy gradient to maximize V estimate (s 1 , z) -? log ? ? (z|s 1 ) for s 1 sampled from the buffer. Optimize ?, ? if necessary . end while The policy will ignore the latent variable if the ? is too small, e.g., 0., 1e -4. But if the ? is too large, though the policy generates diverse solutions, it may explore too much without exploiting past experiences. This ? plays a similar role as ? in ?-VAE <ref type="bibr" target="#b24">(Higgins et al., 2017)</ref>. In experiments, we find that ? from 0.001 to 0.01 works well in the case of our RND design. Fig. <ref type="figure" target="#fig_5">8(C)</ref> shows the effects of the latent dimensions. For tasks like 2D maze, a moderate latent space size d ? 6 is sufficient. But the performance will degrade when it is too small. Fig. <ref type="figure" target="#fig_5">8</ref>(D) ablates our design for the RND. When the RND estimator does not maintain a large replay buffer or does not use the positional embedding, the exploration will suffer a lot. We further compare various policy parameterization methods in Fig. <ref type="figure" target="#fig_5">8(E)</ref>. We find that in our implementation, Gaussian mixture models (GMM) and CEM-based policy do not perform as well as the vanilla Gaussian policy. GMM may have trouble in log-likelihood maximization. We noticed several numerical issues in optimizing GMM and Flow when we applied them with RND in sparse reward tasks. Specifically, we have encountered some instability when optimizing the log prob for GMM due to its non-convex nature and the need for sampling to estimate entropy. Similarly, our experiments with Flow have revealed significant parameter divergence and instabilities, warranting further investigation to pinpoint the root cause. CEM has a stronger ability to find local optima and generates actions with less randomness, which may sacrifice its ability to do exploration. Besides, we find the policy parameterized by a normalizing flow distribution behaves well initially but soon meets numerical instabilities and fails to proceed with optimization, suggesting more investigations are needed in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Environment Details</head><p>Cabinet (Dense) <ref type="bibr" target="#b15">(Gu et al., 2023)</ref>. The agent controls the movement of a 12 dof mobile robot arm and gripper robot to open both cabinet doors. The agent receives a dense reward for reaching its nearest door's handle. Besides, it receives a higher reward when it opens the right door than the left door. The agent succeeds when it fully opens the right door while the dense reward will typically drive the agent close to the handle of the left door. The episode length is 60.</p><p>AntPush <ref type="bibr" target="#b47">(Nachum et al., 2018b)</ref>. The agent controls an ant robot with action dimension 8 to go to the upper room. The reward is the l 2 distance between the agent and a point in the upper room. The optimal path is to go to the left of the red block and push it to the right and go to the upper room. However, agents often get stuck at the local optima, which pushes the block forward or moves to go to the right side. The episode length is 400.</p><p>Door <ref type="bibr" target="#b56">(Rajeswaran et al., 2017)</ref>. The agent controls a dexterous hand with action dimension 26 to open a door. The only receives reward of 1 when it successfully undoes latch and opens the door. The episode length is 100 with an action repeat 2. Objects of interest include the hand's palm, the latch, and the door.</p><p>Hammer <ref type="bibr" target="#b56">(Rajeswaran et al., 2017)</ref>. The agent controls a dexterous hand with action dimension 26 to force drive a nail into the board. The agent only receives reward of 1 when has driven the nail all the way in. Action repeat is 2. The episode length is 125. We encode the position of the hand's palm, the hammer, and the nail.</p><p>BlockPush <ref type="bibr" target="#b75">(Xiang et al., 2020)</ref>. The agent controls the movement of the red block with action dimension 2 to push the green block (middle) to the green destination (above) and the blue block (middle) to the blue destination (above). The agent only receives a reward of 1 when it has successfully pushed both blocks to the exact destination with a small tolerance. The objects of interest contain the location of the three blocks. The environment horizon is 60.</p><p>Cabinet (Sparse) <ref type="bibr" target="#b15">(Gu et al., 2023)</ref>. The agent controls the movement of a 9 dof robot arm and gripper robot to open both doors of the cabinet. The agent only receives a reward of 1 when both cabinet doors are fully opened. We encode the position of the robot's end effector and the location of the cabinet's door. Its episode length is 60. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Connection with Other Generative Models</head><p>Our method is based on the same variational bound shared with many other generative models log p(x) = E z?q(z) [log p(x, z) -log q(z)] + KL(q(z)?p(z|x)).</p><p>By different choices of latent space, posterior q(z|x), joint distribution p(x, z), we can obtain different generative models. For example, VAE models p ? (x, z) = p ? (x|z)p(z) and q(z) = q ? (z|x) using neural networks and then optimize ?, ? jointly to maximize the ELBO bound. By doing so, q ? (z|x) will align with the true posterior of p ? (z|x). Thus log p(x) ? E z?q ? (z|x) [log p ? (x|z) + log p(z) -log q ? (z|x)]</p><p>The Expectation-maximization algorithm (EM) <ref type="bibr" target="#b9">(Dempster et al., 1977)</ref> for learning Gaussian mixture models assumes that we have p ? (x, z) = p ? (x|z)p ? (z) where z is a categorical representation. E-step: finding q ? (z|x) by solving max ? log p ? (x) -D KL (q ? (z|x)||p ? (z|x)) where p ? (z|x) = p ? (x, z)/ p ? (x, z)dz. M-step: fixing ?, find max ? E q ? [log p ? (x, z)] -E q ? [log q ? (z|x)] which is exactly maximizing the ELBO.</p><p>In Maximum Entropy RL <ref type="bibr" target="#b34">(Levine, 2018)</ref>, we have optimality p(O, ? ) = p(O|? )p(? ) defined by the reward, and we optimize ? ? (? |O) only. The ELBO bound becomes a maximum entropy term E ? ?? [log p(O|? ) + log p(? ) -log ?(? )] . Our method differs from it by introducing an additional variable z.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (A) Our method reparameterizes latent variables into multimodal policy to facilitate exploitation and exploration in continuous policy learning; (B) Average performance on 6 hard exploration tasks. Our method outperforms previous methods.</figDesc><graphic url="image-1.png" coords="1,315.09,175.27,218.69,84.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (A) rewards; (B); soft max policy over discrete action space; (C) single-modality Gaussian policy; (D) our methods reparameterize a random variable into multimodal distributions with neural networks.</figDesc><graphic url="image-2.png" coords="4,87.39,67.06,170.09,162.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>; Toussaint (2009); Ziebart (2010); Kappen et al. (2012); Levine (2018); Haarnoja et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. An overview of our model pipeline: A) a reparameterized policy from which we can sample latent variable z and action a given the latent state s; B) a latent dynamics model which can be used to forward simulate the dynamic process when a sequence of actions is known. C) an exploration bonus provided by a density estimator. Our Reparameterized Policy Gradient do multimodal exploration with the help of the latent world model and the exploration bonus.</figDesc><graphic url="image-3.png" coords="6,104.04,67.06,388.79,137.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .Figure 6 .Figure 7 .</head><label>467</label><figDesc>Figure 4. Illustrative experiment on continuous bandit Figure 5. Illustrative experiment on 2D maze navigation problem</figDesc><graphic url="image-5.png" coords="7,298.44,67.07,157.93,97.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Comparing different factors in our methods.</figDesc><graphic url="image-10.png" coords="15,55.44,324.92,486.00,97.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Exploration on several environments; The first column shows the initial state. The right 5 figures of the same row plot states sampled from a single agent.</figDesc><graphic url="image-22.png" coords="18,55.44,231.04,486.00,81.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table 2 compares various generative models.</figDesc><table><row><cell></cell><cell>Latent</cell><cell>Encoder q(z|x)</cell><cell>Joint p(x, z)</cell><cell>MLE objective</cell></row><row><cell>VAE</cell><cell>z</cell><cell>p ? (z|x)</cell><cell>p ? (x|z)p(z)</cell><cell>p(x)</cell></row><row><cell>EM Diffusion</cell><cell>z {x t } t?1</cell><cell>max ? log p ? (x) -D KL (q ? (z|x)||p ? (z|x)) T i=1 N (x t ; ? 1 -? t x t-1 , ? t I)</cell><cell>p ? (x|z)p ? (z) p(x T ) t?1 p ? (x t-1 |x t )</cell><cell>p(x) p(x 0 )</cell></row><row><cell>MaxEntRL</cell><cell>?</cell><cell>? ? (? )</cell><cell>p(O|? )p(? )</cell><cell>p(O)</cell></row><row><cell>RPG</cell><cell>?, z</cell><cell>?</cell><cell></cell><cell></cell></row></table><note><p>? (z, ? ) p(O|? )p ? (z|? )p(? ) p(O)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of different algorithms that optimize ELBO bounds for inference</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work is in part supported by <rs type="funder">Qualcomm AI and AI Institute for Learning-Enabled Optimization at Scale</rs> (<rs type="grantNumber">TI-LOS</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jafG5Nw">
					<idno type="grant-number">TI-LOS</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>Network architecture We use the following two-layer MLP to model policy ? ? , value Q ? , state encoder f ? , and the encoder p ? (z|s). The network structures are shown in the pytorch's convention <ref type="bibr" target="#b52">(Paszke et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequential(</head><p>(0): Linear(in_features=inp_dim, out_features=256, bias=True) (1): ELU(alpha=1.0) (2): Linear(in_features=256, out_features=256, bias=True)</p><p>(3): ELU(alpha=1.0) (4): Linear(in_features=256, out_features=out_dim, bias=True) )</p><p>The dynamics network is a single-layer GRU with a hidden dimension 256. The RND network g ? we use is a 3 layer MLP network with hidden dimension 512 and leaky ReLU as its activation function.</p><p>We maintain target networks like the standard double Q learning. The hyperparameters for training the network are listed in N (0, 1) for sparse reward tasks Table <ref type="table">1</ref>. RPG hyperparameters. We here list the hyper-parameters used in the experiments. The hyper-parameters keep the same for our MBSAC baseline except that MBSAC has no latent space. Notice that for dense reward tasks, the entropy of ? ? (z|s1) is linearly decayed starting from 3 ? 10 5 environment steps to 1M steps to ensure optimality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Study</head><p>We study and compare various factors in our methods in Fig. <ref type="figure">8</ref> on the Maze navigation task described in Sec. 5.1. Fig. <ref type="figure">8(A</ref>) compares different latent spaces to use. The continuous latent space modeled by a Gaussian distribution of dimension 12 outperforms the categorical latent space, while both are better than the one without latent variables, i.e., the MBSAC baselines. <ref type="bibr">Fig. 8(B)</ref> shows the effects of our method when using a Gaussian distribution as the latent space with different ? values. The ? controls the scale of the cross entropy term log p ? (z|s, a) in reward maximization, as mentioned in Sec. 4.3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Environments and Results in Additional Experiments</head><p>Cheetah Back  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.10299</idno>
		<title level="m">Variational option discovery algorithms</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hindsight experience replay</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pieter Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The option-critic architecture</title>
		<author>
			<persName><forename type="first">P.-L</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Harb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01540</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Openai gym. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12894</idno>
		<title level="m">Exploration by random network distillation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Explore, discover and learn: Unsupervised discovery of state-covering skills</title>
		<author>
			<persName><forename type="first">V</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gir?-I Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1317" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Decision transformer: Reinforcement learning via sequence modeling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Laskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="15084" to="15097" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Eysenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1009" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.2517-6161.1977.tb01600.x</idno>
		<idno type="arXiv">arXiv:1802.06070</idno>
		<ptr target="https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1977.tb01600" />
	</analytic>
	<monogr>
		<title level="m">Diversity is all you need: Learning skills without a reward function</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Eysenbach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1977">1977. 2018</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dynamics learning with cascaded variational inference for multi-step manipulation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13395</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamics learning with cascaded variational inference for multi-step manipulation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="42" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Integrated task and motion planning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chitnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Holladay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lozano-P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">robotics, and autonomous systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="265" to="293" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Annual review of control</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Maniskill2: A unified benchmark for generalizable manipulation skills</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hausman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11956</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Soft actorcritic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1861" to="1870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Dream to control: Learning behaviors by latent imagination</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01603</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Mastering atari with discrete world models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02193</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Temporal difference learning for model predictive control</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.04955</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Provably efficient maximum entropy exploration</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Soest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="2681" to="2691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">betavae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on learning representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generative adversarial imitation learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><surname>Plasticinelab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03311</idno>
		<title level="m">A soft-body manipulation benchmark with differentiable physics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.10291</idno>
		<title level="m">Efficient planning in a compact latent action space</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optimal control as a graphical model inference problem</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kappen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>G?mez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="182" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sampling-based algorithms for optimal motion planning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frazzoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The international journal of robotics research</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="846" to="894" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">One solution is not all you need: Few-shot extrapolation via structured maxent rl</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8198" to="8210" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Reinforcement learning and control as probabilistic inference: Tutorial and review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00909</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hrl4in: Hierarchical reinforcement learning for interactive navigation with mobile manipulators</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martin-Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="603" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Infogail: Interpretable imitation learning from visual demonstrations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tedrake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Object-centric learning with slot attention</title>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11525" to="11538" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning latent plans from play</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on robot learning</title>
		<imprint>
			<biblScope unit="page" from="1113" to="1132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Mazzaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Verbelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dhoedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName><surname>Choreographer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.13350</idno>
		<title level="m">Learning and adapting skills in imagination</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Discovering and achieving goals via world models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mendonca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rybkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="24379" to="24391" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Nerf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Representing scenes as neural radiance fields for view synthesis</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="99" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The expectation-maximization algorithm</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="47" to="60" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><surname>Maniskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.14483</idno>
		<title level="m">Generalizable manipulation skill benchmark with large-scale demonstrations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Near-optimal representation learning for hierarchical reinforcement learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01257</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Data-efficient hierarchical reinforcement learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cs229 lecture notes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CS229 Lecture notes</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Solving rubik&apos;s cube with a robot hand</title>
		<author>
			<persName><forename type="first">Akkaya</forename><surname>Openai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chociej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Petron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ribas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<idno>CoRR, abs/1910.07113</idno>
		<ptr target="http://arxiv.org/abs/1910.07113" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Hierarchical reinforcement learning via advantage-weighted information maximization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Osa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tangkaratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01365</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Discovering diverse solutions in deep reinforcement learning by maximizing state-action-based mutual information</title>
		<author>
			<persName><forename type="first">T</forename><surname>Osa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tangkaratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="90" to="104" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><surname>Pytorch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Large-scale reusable adversarial skill embeddings for physically simulated characters</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Halper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><surname>Ase</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.01906</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Accelerating reinforcement learning with learned skill priors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Pertsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Lim</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on robot learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="188" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A direct method for trajectory optimization of rigid bodies through contact</title>
		<author>
			<persName><forename type="first">M</forename><surname>Posa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cantu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tedrake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="81" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Learning complex dexterous manipulation with deep reinforcement learning and demonstrations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.10087</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Zero-shot textto-image generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8821" to="8831" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Black box variational inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gerrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="814" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Seward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8857" to="8868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zolna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Novikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sulsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.06175</idno>
		<title level="m">A generalist agent</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Mastering atari, go, chess and shogi by planning with a learned model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">588</biblScope>
			<biblScope unit="issue">7839</biblScope>
			<biblScope unit="page" from="604" to="609" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06347</idno>
		<title level="m">Proximal policy optimization algorithms</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Planning to explore via self-supervised world models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rybkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v119/sekar20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning robot skills with temporal variational inference</title>
		<author>
			<persName><forename type="first">T</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8624" to="8633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Deterministic policy gradient algorithms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Degris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="387" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Robot skill learning: From reinforcement learning to evolution strategies. Paladyn</title>
		<author>
			<persName><forename type="first">F</forename><surname>Stulp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sigaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Behavioral Robotics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="61" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
	<note>Linearly-solvable markov decision problems</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">General duality between optimal control and estimation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Todorov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="4286" to="4292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Robot trajectory optimization using approximate inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Toussaint</surname></persName>
		</author>
		<idno type="DOI">10.1145/1553374.1553508</idno>
		<ptr target="https://doi.org/10.1145/1553374.1553508" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning, ICML &apos;09</title>
		<meeting>the 26th Annual International Conference on Machine Learning, ICML &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1049" to="1056" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery. ISBN 9781605585161</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Credit assignment techniques in stochastic computation graphs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2650" to="2660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Sapien: A simulated part-based interactive environment</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11097" to="11107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Mastering atari games with limited data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kurutach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="25476" to="25488" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Quillen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on robot learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1094" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Glipv2: Unifying localization and vision-language understanding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2022/file/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Oh</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2022. 2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="25217" to="25230" />
		</imprint>
	</monogr>
	<note>Noveld: A simple yet effective exploration criterion</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Online decision transformer</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.05607</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Modeling purposeful adaptive behavior with the principle of maximum causal entropy</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">The agent controls the movement of a gripper with a 4 dof controller to move the ball into the basket. The agent only receives a reward of 1 when the ball is sufficiently close to the basket. The locations of the ball and the location of robots&apos; fingertips are what we are concerned about. The episode length is 100, including 2 action repeats</title>
		<author>
			<persName><forename type="first">Meta-World</forename><surname>Baseketball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">(</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Meta-World StickPull</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>The agent controls the movement of a gripper with a 4 dof controller to pull the container with a blue stick. The agent receives a reward of 1 only when the stick is inserted inside the handle, and the container is already pulled sufficiently close to the green dot. We encode the positions of the fingertips, the stick, and the handle of the cup for computing intrinsic rewards. The remaining setup is the same as BasketBall</note>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Baseline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tdmpc (</forename><surname>Hansen</surname></persName>
		</author>
		<ptr target="https://github.com/nicklashansen/tdmpc" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>we used the publically available official implementation and default hyperparameters provided by the</note>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">we implemented according to the original paper and used the default hyperparameter provided by the authors. We use the abbreviation TDMPC(R), SAC(R) to represent that we add an intrinsic reward with scale 0.1 for exploration in environments with only sparse rewards</title>
		<author>
			<persName><surname>Sac (haarnoja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">we used the publically available official implementation and default hyperparameters provided by the authors at</title>
		<author>
			<persName><forename type="first">(</forename><surname>Dreamerv2</surname></persName>
		</author>
		<author>
			<persName><surname>Hafner</surname></persName>
		</author>
		<ptr target="https://github.com/danijar/dreamerv2" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">2explore with hyperparameters provided by the authors of the paper. For all baseline algorithms, we only change model update frequency to once every 5 environment steps. E. Visualization of the Multimodal Exploration We plot the trajectory of the agent in AntPush environment, evaluated at different numbers of training stages in Fig. 9. The agent learned to move forward and explored all directions that would decrease the l 2 distance. It found the left side was easier for moving up in the beginning, but at episode 360, it learned to explore all directions. Ultimately, it explored the left path to the upper room and converged on it. Figure 9. Exploration of AntPush</title>
		<author>
			<persName><surname>Sekar</surname></persName>
		</author>
		<ptr target="https://github.com/ramanans1/plan" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
		<respStmt>
			<orgName>Plan2Explore</orgName>
		</respStmt>
	</monogr>
	<note>we run DreamerV2 according to the instructions provided by. which has the dense reward to guide the agent to move forward. We also plot the sampled states during exploration for Block, Cabinet, and Stickpull Envs in Fig</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
