<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NoC-Sprinting: Interconnect for Fine-Grained Sprinting in the Dark Silicon Era</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jia</forename><surname>Zhan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
							<email>yuanxie@cse.psu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guangyu</forename><surname>Sun</surname></persName>
							<email>gsun@pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">DAC &apos;14</orgName>
								<address>
									<postCode>01 -05 2014</postCode>
									<settlement>June, San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NoC-Sprinting: Interconnect for Fine-Grained Sprinting in the Dark Silicon Era</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/2593069.2593165</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>C.2 [Computer-Communication Networks]: Network Architecture and Design Performance</term>
					<term>Design Network-on-Chip</term>
					<term>Dark Silicon</term>
					<term>Computational Sprinting</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The rise of utilization wall limits the number of transistors that can be powered on in a single chip and results in a large region of dark silicon. While such phenomenon has led to disruptive innovation in computation, little work has been done for the Network-on-Chip (NoC) design. NoC not only directly influences the overall multi-core performance, but also consumes a significant portion of the total chip power. In this paper, we first reveal challenges and opportunities of designing power-efficient NoC in the dark silicon era. Then we propose NoC-Sprinting: based on the workload characteristics, it explores fine-grained sprinting that allows a chip to flexibly activate dark cores for instantaneous throughput improvement. In addition, it investigates topological/routing support and thermal-aware floorplanning for the sprinting process. Moreover, it builds an efficient network power-management scheme that can mitigate the dark silicon problems. Experiments on performance, power, and thermal analysis show that NoC-sprinting can provide tremendous speedup, increase sprinting duration, and meanwhile reduce the chip power significantly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The continuation of technology scaling leads to a utilization wall challenge <ref type="bibr" target="#b20">[21]</ref>: to maintain a constant power envelope, the fraction of a silicon chip that can be operated at full frequency is dropping exponentially with each generation of process technology. Consequently, a large portion of silicon chips will become dark or dim silicon, i.e., either idle or significantly under-clocked. However, most previous work focuses on energy-efficient core/cache design while the impact of on-chip interconnect is neglected. In fact, Network-on-chip (NoC) plays a vital role in message passing and memory access that directly influences the overall performance of many-core processors. Moreover, network components dissipate 10% -36% of total chip power <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref>. Therefore, how to design the interconnection network is critical to tackle the challenges of multicore scaling in the dark silicon age.</p><p>Recently, Raghavan et al. <ref type="bibr" target="#b16">[17]</ref> proposed computational sprinting, in which a chip improves its responsiveness to short-burst of computations through temporarily exceeding its sustainable thermal design power (TDP) budget.</p><p>All the cores will be operated at the highest frequency/voltage to provide instant throughput during sprinting, and after that the chip must return to the single-core nominal operation to cool down. While such mechanism sheds light upon how "dark" cores can be utilized for transient performance enhancement, it exposes two major design issues: First, the role of interconnect is neglected. NoCs consume a significant portion of chip power when all cores are in sprinting mode. When switching back to the nominal mode, only a single core is active. However, the network routers and links cannot be completely powered down, otherwise a gated-off node would block packet-forwarding and the access of the local but shared resources (e.g., cache and directory). As a result, the ratio of network power over chip power rises substantially and may even lead to higher NoC power than that of the single active core. Second, the mode-switching lacks flexibility and only provides two options: nominal single-core operation and maximum all-core sprinting. Depending on the workload characteristics, an intermediate number of active cores may provide the optimal performance speedup with less power dissipation.</p><p>To address these two issues, we propose fine-grained sprinting, in which the chip can selectively sprint to any intermediate stages instead of directly activating all the cores in response to short-burst computations. The optimum number of cores to be selected depends on the application characteristics. Scalable applications may opt to a large number of cores that can support highly parallel computation, whereas other applications may mostly consist of sequential programs and would rather execute on a small number of cores. Apparently, fine-grained sprinting can flexibly adapt to a variety of workloads. In addition, landing on intermediate sprinting stages can save chip power and slow down the heating process by power-gating the remaining inactive on-chip resources, which is capable of sustaining longer sprint duration for better system performance.</p><p>Fine-grained sprinting opens up an opportunity to better utilize the on-chip resources for power-efficiency, but it also poses challenges on designing the interconnect backbone. Inherently it incurs three major concerns: (1) how to form the topology which connects the selected number of cores during sprinting when dark cores and active cores co-exist? (2) how to construct a thermal-aware floorplan of the on-chip resources (cores, caches, routers, etc.) for such sprinting-based multicores? <ref type="bibr" target="#b2">(3)</ref> what would be an appropriate NoC power-management scheme? To answer these questions, we propose a topological sprinting mechanism with deadlock-free routing support, and a fast heuristic floorplanning algorithm to address the thermal problem during sprinting. Moreover, this sprinting scheme naturally enables power gating on network resources in the dark silicon region.</p><p>In summary, we propose NoC-sprinting, which provides topological/routing support for fine-grained sprinting and employs network power-gating techniques for combating dark silicon. Overall, this paper makes the following contributions:</p><p>? Explores challenges and opportunities of designing NoC in the dark silicon era, from the perspectives of both performance and power.</p><p>? Investigates the pitfalls of the conventional all-core sprinting which fails to fulfill diverse workload characteristics, and proposes fine-grained sprinting for better power-efficiency.</p><p>? Proposes NoC support to enable fine-grained sprinting, including topology construction, routing, floorplanning, and power management.</p><p>? Conducts thermal analysis to evaluate how NoC-sprinting correlates with the sprint duration.</p><p>Dark Silicon and Computational Sprinting. Conventionally in multi-core scaling, the power gain due to the increase of transistor count and speed can be offset by the scaling of supply voltage and transistor capacitance. However, in today's deep sub-micron technology, leakage power depletes the power budget. We cannot scale threshold voltage without exponentially increasing leakage. Consequently, we have to hold a constant supply voltage, and hence produce a shortfall of energy budget to power a chip at its full frequency. This gap accumulates through each generation and results in an exponential increase of inactive chip resources -Dark Silicon.</p><p>Instead of shrinking the chip or sacrificing transistor density, computational sprinting <ref type="bibr" target="#b16">[17]</ref> embraces dark silicon by leveraging the extra transistors transiently when performance really counts. Special phase change materials should be used as heat storage to support such temporary intense sprinting, leveraging the property that temperature stays constant during the melting phase of the material. Figure <ref type="figure" target="#fig_1">1</ref> demonstrates the nominal single-core operation as well as the sprint mode for a 16-core system. The temperature rises from the ambient environment when the sprint starts at t sprint , and then extra thermal energy is absorbed by the melting process of the phase change material, which keeps the temperature at T melt . After the material is completely melted, the temperature rises again until T max where the system terminates all but one core (t one ) to sustain the operation. The system starts to cool after all work is done at t cool . Note that numbers in the curve mark different sprint phases and will be analyzed in Section IV. Conventional computational sprinting still focuses on computation, whereas the role of interconnect is neglected. Here we demonstrate two key challenges that require careful consideration when designing NoC for sprinting-based multicores in the dark silicon age.</p><p>NoC Power Gating. Power gating is an efficient power-saving technique by completely shutting down the idle cores to reduce leakage. However, as more and more cores turn "dark", the network components such as routers and links also become under-utilized. As mentioned, NoC dissipates 10% -36% of total chip power <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20]</ref>; additionally, the more cores become dark, the larger the ratio of network power over the total chip power. This observation also points out the flaw of the conventional computational sprinting which turns off all but one core during nominal operation, while neglecting the impact of NoC.</p><p>To give a brief overview of network power, we simulate a classic wormhole router with a network power tool DSENT <ref type="bibr" target="#b18">[19]</ref>. The flit width is 128 bits. Each input port of a router comprises two virtual channels (VC) and each VC is 4-flit deep. The power value are estimated with an average injection rate of 0.4 flits/cycle. Figure <ref type="figure" target="#fig_2">2</ref> shows the router power breakdown when varying the operating voltage (1v, 0.9v, 0.75v) and frequency (2GHz, 1.5GHz, 1.0GHz) under 45 nm technology. We can see that leakage power contributes a significant portion to the total network power. In addition, the ratio of leakage power increases as we scale down the supply voltage and frequency, and even exceeds that of dynamic power in some cases. Sprinting-based multicores activate a single core during nominal operation whereas the rest are turned off. Figure <ref type="figure" target="#fig_3">3</ref> shows the chip power breakdown when scaling the number of cores based on the Niagara2 <ref type="bibr" target="#b15">[16]</ref> processor. We evaluate the power dissipation with McPAT <ref type="bibr" target="#b12">[13]</ref> for cores, L2 caches, memory controllers (MC), NoC, and others (PCIe controllers, etc.). We assume idle cores can be gated-off (dark silicon) while other on-chip resources stay active or idle. As shown in Figure <ref type="figure" target="#fig_3">3</ref>, NoC accounts for 18%, 26%, 35%, and 42% of chip power respectively for 4-core, 8-core, 16-core, 32-core CMP chips when they are operating at nominal mode. In contrast, the power ratio for the single active core keeps decreasing as the "dark silicon" grows. Therefore in this scenario, it is inappropriate to only measure core power when power budget is the design limitation.</p><p>NoC power gating is heavily dependent on the traffic. In order to benefit from power gating, an adequate idle period (namely, "break-even time") of routers should be guaranteed to make sure they are not frequently woken up and gated off. Recently researchers have proposed various schemes <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref> to mitigate the latency overhead caused by frequent router wake-up. However, these techniques do not account for the underlying core status and will result in sub-optimal power gating decisions.</p><p>Workloads-Dependent Sprinting. A straightforward sprinting mechanism is to transiently activate all the dark cores at once. However, this scheme fails to explore the sporadic workload parallelism and thus may waste power without sufficient performance gain, especially for multi-threaded applications that have various scalability. Here we use PARSEC 2.1 <ref type="bibr" target="#b1">[2]</ref> as an example. We simulate CMP systems using gem5 <ref type="bibr" target="#b2">[3]</ref> and observe the performance speedup when varying the core count. For clarity, Figure <ref type="figure" target="#fig_4">4</ref> selects a few results that can represent different workload characteristics. The detailed evaluation methodology is described in Section IV.</p><p>As shown in Figure <ref type="figure" target="#fig_4">4</ref>, some benchmarks (e.g. blackscholes and bodytrack) achieve significant performance speedup as the number of cores increases. In contrast, for freqmine, the execution time is almost identical at different configurations, which implies its serial program benefits little from the extra cores. In addition, there are some applications (e.g. vips and swaptions) that achieve obvious speedup as the core count increases within a small range but then slow down gradually, and further suffer from delay penalty after exceeding a certain number. This is because adding more cores than required by the application parallelism may incur significant overheads that may offset and even hurt performance. The overheads include thread scheduling, synchronization, and long interconnect delay due to the spread of computation resources. Therefore, for sprinting-based multicores, activating all the dark cores is not a universal solution for all applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method: NoC-Sprinting</head><p>As illustrated above, an efficient sprinting mechanism should be able to provide different levels of parallelism desired by different applications. Depending on workload characteristics, the optimal number of cores required to provide maximal performance speedup varies. This also raises challenges in designing a high performance and low power interconnect to support the sprinting process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fine-Grained Sprinting</head><p>We first propose fine-grained sprinting, a flexible sprint mechanism that can activate a subset of network components to connect a certain number of cores for different workloads.</p><p>Specifically, during execution, the CMP system may experience a short burst of computation due to the abrupt fluctuation of a running program. As such, the system will quickly react to such intense computation and determine the optimal number of cores that should be offered for instantaneous responsiveness. Then the system will activate the required number of cores while the others remain "dark". There are some existing work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref> on adapting system configurations like core count/frequency to meet runtime application requirements. Since our focus is on how to design interconnect under such circumstances, we assume that these application parallelism can be learnt in advance or monitored during run-time execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Irregular Topological Sprinting and Deadlock-Free Routing</head><p>Under the nominal operation, only a single core (namely master core) remains active. There are different choices of placement for the master core. We list a few examples here, but real implementations not be limited by these mentioned conditions. Firstly it could be placed in the center of the chip to reduce the transmission latency for thread migration. Another example is to select the core running the OS as the master core since it is always activated. The core next to the memory controller is also a good candidate if the application generates intensive memory accesses. Without loss of generality, we choose the top-left corner node as the master node which is closest to the memory controller, i.e., Node 0 as shown in Figure <ref type="figure" target="#fig_7">5a</ref>.   After the system transfers to the sprinting mode, a number of cores will be activated and keep running for a short duration. Fine-grained sprinting requires topological support from the following aspects:</p><p>? Pay-as-you-go: fine-grained activation of any number of cores.</p><p>? Short communication delay between different nodes, especially to the master node where the memory controller resides.</p><p>? Routing should be simple and deadlock-free which does not incur significant control complexity or hardware overhead.</p><p>To achieve these goals, we propose to start from the master node, and connect other nodes to the network in ascending order of their Euclidean distances to the master node. For example, the red nodes in Figure <ref type="figure" target="#fig_7">5a</ref> demonstrate the topology of a 8-core sprinting. Note that we use Euclidean distances instead of Hamming distances here. The latter may guarantee a shortest routing distance between the newly-added node to the master node, but would generate longer inter-node communication to other nodes. For example, both cases would choose node 0, 1, and 4 as 3-core sprinting. But if 4-core sprinting is triggered, the method with Hamming distance may possibly choose node 2 whereas the method with Euclidean distance would generate a better choice by accommodating node 5. Algorithm 1 generates the order of N nodes used for topological sprinting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ALGORITHM 1. Irregular Topological Sprinting</head><p>Result: A linked-list L of routers to be activated Initialize:</p><formula xml:id="formula_0">D[i] = 0, i = 0, 1, 2...N -1. The coordinate for R k is (x k , y k ). for i ? 1 to N -1 do D[i] = (x i -x 0 ) 2 + (y i -y 0 ) 2 ; end Sort R[i](i = 0, 1...N -1</formula><p>) in ascending order of D[i](i = 0, 1...N -1) and put them into a linked-list L. Break ties according to the order of indexes.</p><p>The fine-grained sprinting process will generate an irregular network topology to connect active cores. Meanwhile, it guarantees that chosen nodes would form a convex set in the Euclidean space, i.e., the topology region contains all the line segments connecting any pair of nodes inside it. Flich et al. <ref type="bibr" target="#b6">[7]</ref> proposed a distributed routing algorithm for irregular NoC topologies but their algorithm requires twelve extra bits per switch. Adapted from their approach, we extend the Dimension-Order-Routing (specifically, X-Y routing) algorithm for such convex topologies (CDOR). Specifically, two connectivity bits (C w and C e ) are leveraged to indicate whether a router is connected to its western or eastern neighbors. As in conventional DOR, we assume that X and Y coordinates of the final destination are stored in the packet header (X des and Y des ), and each switch knows its X and Y coordinates (through X cur and Y cur registers at each switch). The origin of the coordinate system is located at the top-left corner of the 2D mesh. Messages are routed from the current router to the destination router, according to the offsets of coordinates and the two connectivity bits per router. Figure <ref type="figure" target="#fig_7">5a</ref> shows a routing path from the source to its destination. The detailed routing algorithm is described in Algorithm 2. Furthermore, Figure <ref type="figure">6</ref> depicts the routing logic design, which includes two comparators per switch and the routing circuit for computing the North port. The routing logic for other ports can be designed similarly based on Algorithm 2. We implemented CDOR on behavioral Verilog. Synthesized results using Synopsys Design Compiler (45nm technology) show that it adds less than 2% area overhead compared to a conventional DOR switch. N and E represent south, west, north and east, respectively). In our CDOR, although the NE turn may happen, it is deadlock-free. For example, as shown in Figure <ref type="figure" target="#fig_7">5a</ref>, a NE turn happens at Node 5 but this also indicates the east output port of its southern neighbor 9 is not connected. Therefore a WN turn cannot happen and thus eliminate a cycle that may cause a deadlock.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Thermal-Aware Floorplanning</head><p>The key design constraint of fine-grained sprinting is the thermal design power (TDP). The above topological sprinting process does not consider thermal behavior to simplify control and routing logic. Therefore, here we propose a design-time floorplanning algorithm that can be seamlessly integrated with the topological sprinting process while providing better thermal distribution to avoid hot spots. Thus, it sustains a longer sprint duration by slowing down the heating process.</p><p>Consider a 4-core sprinting in a 16-node mesh network as shown in Figure <ref type="figure" target="#fig_7">5a</ref>. We may opt to choose the top-left four nodes for better performance, but alternatively may prefer the four scattered corner nodes from the thermal point of view. To overcome this dilemma, we still maintain the original logic connectivity of the mesh network in consistent of the topological sprinting process, but propose a heuristic algorithm that reallocates the physical location of each node.</p><p>As shown in Algorithm 3, our floorplanning algorithm treats the 2D mesh network as a graph, and allocates the nodes based on the list generated from Algorithm 1. In our annotations, G represents the original logical network, S contains the set of nodes that have already been explored in G, G represents the physical floorplan, and S is the corresponding set of occupied nodes in G . At each iteration, it picks up a node R k in G -S, and maps it to a node in G -S that has the maximum weighted sum of Euclidean distances to all the nodes in S for the optimal thermal distribution. This process is described in Function MaxWeightedDistance as in Algorithm 4. Note that the weight of a distance is inversely proportional to the Hamming distance between R k and the node in S. The rationale behind this scheme is that, the longer the Hamming distance in logical connectivity, the less chance these two nodes would be selected together during sprinting and accumulate heat dissipation, thus they can be placed closer in the physical floorplan. </p><formula xml:id="formula_1">f : {R 0 , R 1 ... R N-1 }. Transformed floorplan f : {R 0 , R 1 ... R N-1 }. The coordinate for R k or R k is (x k , y k ). Set S = ? , S ={R 0 ,R 1 ... R N-1 }. Queue Q = ? . List L from Algorithm 1 Goal: Find the mapping Pos() from f to f . Pos(R 0 ) = 0(Master Node); Put R 0 in S; Delete R 0 from S ; Put all unexplored adjacent nodes of R 0 into Q based on List L; while Q = ? do R k = Q[0]; Delete Q[0] from Q; Pos(R k ) = MaxWeightedDistance(S, S , R k ); Delete R Pos(R k ) from S ; Put R k in S; Put all unexplored adjacent nodes of R k into Q based on List L; end</formula><p>The floorplanning algorithm frees the sprinting process and routing algorithm from the thermal concern, i.e., only the logical connectivity of mesh network needs to be considered during topological sprinting. Figure <ref type="figure" target="#fig_7">5b</ref> shows the final floorplan of the physical network and only links for four-core sprinting are shown for clarity. Note that the floorplanning algorithm will increase the wiring complexity and generate long links. A standard method of reducing delay of long wires is to insert repeaters in the wire at regular intervals. Recently, </p><formula xml:id="formula_2">d i j = (x i -x Pos(R j ) ) 2 + (y k -y Pos(R j ) ) 2 ; s i j = w i j * d i j ; end Sum = ? s i j ; if Sum &gt; Max then Max = Sum; Pos(R k ) = i; end end Return Pos(R k );</formula><p>Krishna et al. <ref type="bibr" target="#b10">[11]</ref> have validated such clockless repeated wires that allow multi-hop traversals to be completed in a single clock cycle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Network Power Gating</head><p>With our proposed fine-grained sprinting, the network power gating scheme becomes straightforward. Since the topological sprinting algorithm activates a subset of routers and links to connect the active cores, we gate off the other network components as shown in the shaded nodes of Figure <ref type="figure" target="#fig_7">5a</ref>. Moreover, the proposed CDOR algorithm routes packets within the active network and thus avoids unnecessary wakeup of intermediate routers for packet forwarding. This further increases the idle period of the dark region for longer power gating.</p><p>However, we still need to consider the Last-Level-Cache (LLC) architecture for network power gating. For private per-core LLC, centralized shared LLC, or distributed shared LLC connected with a separate network (NUCA), our power gating mechanism works perfectly without the need for any further hardware support. However, for tile-based multicores where each tile comprises of a shared bank of LLC, there may be some packet accesses to dark nodes for cache resources. Therefore, some complimentary techniques such as bypass paths <ref type="bibr" target="#b3">[4]</ref> can be leveraged to avoid completely isolating cache banks from the network. We accommodate this method in our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Architectural Evaluation</head><p>We use the gem5 <ref type="bibr" target="#b2">[3]</ref> full system simulator to setup a sprinting-based multicore architecture with 16 ALPHA CPUs. We use Garnet <ref type="bibr" target="#b0">[1]</ref> to model a 4 ? 4 mesh network and DSENT <ref type="bibr" target="#b18">[19]</ref> for network power analysis. The detailed system configurations are listed in Table <ref type="table" target="#tab_0">1</ref>. We evaluate NoC-sprinting with multi-threaded workloads from PARSEC <ref type="bibr" target="#b1">[2]</ref> by assuming the chip can sustain computational sprinting for one second in the worst case, which is consistent with <ref type="bibr" target="#b16">[17]</ref>. Later we will analyze how NoC-sprinting influences the sprint duration. We first start running the benchmarks in a simple mode and take checkpoints when reaching the parallel portion of the program. Then, the simulation restores from checkpoints and we record the execution time of running a total of one billion instructions for each benchmark. In addition, we construct synthetic traffic for further network analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Performance Evaluation</head><p>Here we evaluate how NoC-sprinting improves the system responsiveness.</p><p>In comparison, one naive baseline design (non-sprinting) is to always operate with one core under TDP limit. Another extreme case (full-sprinting) is to activate all the 16 cores during sprinting. While the methods to predict the application parallelism <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref> is beyond the scope of this paper, we conduct off-line profiling on PARSEC to capture the internal parallelism of the benchmarks. Figure <ref type="figure" target="#fig_11">7</ref> shows the execution time of PARSEC workloads with different sprinting schemes. We can see that NoC-sprinting cuts down the execution time substantially compared to non-sprinting. It achieves 3.6x speedup on average for all the applications. In comparison, full-sprinting fails to provide the maximal speedup in some cases with an average 1.9x speedup. It is because increasing the active core count in some programs would incur overheads that may outweigh achievable benefits after a saturating point is reached. These overheads come from OS scheduling, synchronization, and long interconnect delay due to the spread of computation resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Core Power Dissipation</head><p>Instead of waking up all the dark cores for quick response, NoC-sprinting provides better power-efficiency by allocating just enough power to support the maximal performance speedup. Since the triggered topology directly determines the number of cores to be powered on, here we first explore its impact on the core power dissipation. Apart from full-sprinting, we also compare NoC-sprinting with a naive fine-grained sprinting scheme which does not employ any power gating techniques, i.e. the system only cares about selecting the optimal number of cores for actual execution and leaves the others idle. As shown in Figure <ref type="figure" target="#fig_12">8</ref>, except for blackscholes and bodytrack which achieve the optimal performance speedup in full-sprinting and hence leave no space for power-gating, NoC-sprinting cuts down the most power across all the other applications. Compared to full-sprinting, fine-grained sprinting saves 25.5% power even though power gating is not applied. More promisingly, NoC-sprinting achieves 69.1% core power saving on average for all applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of On-Chip Networks</head><p>NoC-sprinting provides customized topology, routing, floorplanning, and efficient power-gating support for fine-grained sprinting. Therefore in this subsection, we evaluate network performance and power to see how the NoC behaves during the sprinting process.</p><p>Network Latency: Full-sprinting activates the entire network and would possibly lose some performance speedup in the long interconnect. In contrast, NoC-sprinting uses a subset of routers to directly connect the active cores, which avoids unnecessary network traversals in the dark nodes with the support of CDOR routing algorithm. As an example, Figure <ref type="figure" target="#fig_13">9</ref> shows the average network latency for running PARSEC with different sprinting schemes. Apparently, NoC-sprinting shortens the communication latency for most applications. Overall, it cuts down the network latency by 24.5%. Network Power: As Figure <ref type="figure" target="#fig_3">3</ref> shows, network power becomes more and more significant as cores turn dark. Therefore, optimizing NoC power dissipation becomes an urgent issue in order to combat the power shortage in the dark silicon age.</p><p>Figure <ref type="figure" target="#fig_14">10</ref> shows the total network power consumption during the sprint phase of running PARSEC. As we can see, NoC-sprinting successfully cuts down the network power if an intermediate level of sprinting is selected. On average, it saves 71.9% power compared to full-sprinting. This is because NoC-sprinting can adapt the network topology according to workload characteristics and only operates on a subset of nodes. In comparison, full-sprinting activates a fully-functional network and loses opportunities for power-gating. More Analysis with Synthetic Traffic: Furthermore, we construct some synthetic traffic on a network simulator booksim 2.0 <ref type="bibr" target="#b9">[10]</ref> to test NoC-sprinting under different traffic scenarios. For full-sprinting, we consider traffic to be randomly mapped in the fully-functional network and results are averaged over ten samples. We compare full-sprinting with NoC-sprinting and observe the differences in performance and power while varying the network load. As an example, Figure <ref type="figure" target="#fig_1">11</ref> shows the results of 4-core and 8-core sprinting for a 16-core system under uniform-random traffic. There are a few key observations:</p><p>? As shown in Figure <ref type="figure" target="#fig_1">11a</ref> and Figure <ref type="figure" target="#fig_1">11c</ref>, NoC-sprinting cuts down the average flit latency by 45.1% and 16.1% before saturation for 4-core and 8-core sprinting, respectively, because it uses a dedicated region of network for more efficient communication without traversing the dark region. The latency benefit drops when switching to a higher level of sprinting because less routers/links are wasted as intermediate forwarding stations like full-sprinting.</p><p>? Correspondingly, NoC-sprinting decreases the total network power consumption by 62.1% and 25.9% for 4-core and 8-core sprinting, respectively, as indicated by the gap between the two power curves in both Figure <ref type="figure" target="#fig_1">11b</ref> and Figure <ref type="figure" target="#fig_1">11d</ref>. The extra routers/links used in full-sprinting not only consume leakage power but also generate dynamic power from packet traversals. As expected, the lower sprint level, the more power saving NoC-sprinting can achieve.</p><p>? The downside of NoC-sprinting is that the network saturates earlier than that of full-sprinting. This is because NoC-sprinting uses a subset of network where each node is generating and accepting packets. Differently, full-sprinting spreads the same amount of traffic among a fixed fully-functional network where some nodes are simply used for intermediate packet forwarding. However, this usually would not affect the network performance in real cases. For example, in the PARSEC benchmarks we have evaluated, the average network injection rate never exceeds 0.3 flits/cycle, which is far from saturating the network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Thermal Analysis</head><p>NoC-sprinting heavily relies on the sprint duration to sustain the desired parallelism. Figure <ref type="figure" target="#fig_1">1</ref> in Section II demonstrated the sprinting process which includes three phases. The duration of each phase is dependent on the property of the phase change material placed close to the die. However, we can still conduct some qualitative analyses to evaluate how NoC-sprinting affects the sprint duration. Phase 1 indicates that the temperature rises abruptly when sprinting starts, and so does the phase 3 after the melting phase ends. Intuitively, the more power-on components, the faster the temperature will increase. Therefore, NoC-sprinting can slow down the heating process by allocating just enough power for the maximum performance speedup. As an example, we analyze dedup (one of the PARSEC benchmarks) whose optimal level of sprinting is 4. We collect the power densities using McPAT and feed them into a thermal modeling tool HotSpot <ref type="bibr" target="#b8">[9]</ref> as the power trace. As for the floorplan, we abstract the 16-core CMP system as 16 blocks placed in a 2D grid, where each block comprises the Alpha CPU, local caches, and other network resources. We use a fine-grained grid model to observe the stable temperatures of the whole chip. Figure <ref type="figure" target="#fig_2">12</ref> shows the heat maps for full-sprinting and NoC-sprinting.</p><p>As shown in Figure <ref type="figure" target="#fig_2">12a</ref>, though power is almost uniformly distributed across the chip, full-sprinting results in an overheated spot in the center (358.3 ? K). In contrast, fine-grained sprinting only activates four nodes as shown in Figure <ref type="figure" target="#fig_2">12b</ref> and the corresponding peak temperature drops (347.79 ? K). Furthermore, our thermal-aware floorplanning generates better temperature profile (343.81 ? K) as shown in Figure <ref type="figure" target="#fig_2">12c</ref>.</p><p>Phase 2 is the most critical phase that determines the capability of sprinting. Placing phase change materials close to the die elongates this period by increasing the thermal capacitance. Temperature remains stable during melting and the duration of melting is mostly determined by its latent heat of fusion -the amount of energy to melt a gram of such material. As such, based on the power results we collected from PARSEC, NoC-sprinting increases the duration by 55.4% on average.</p><p>As a summary, NoC-sprinting reduces the slopes of temperature rise in phase 1 &amp; 3, and enhances the melting duration in phase 2 by slowing down thermal capacitance depletion. Thus, it guarantees a longer sprint for intense parallel computation and further increases the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we reveal the challenges and opportunities in designing NoC in the dark silicon age. We present NoC-sprinting: it provides topology/routing support, thermal-aware floorplanning, and network power gating for fine-grained sprinting. Our experiments show that NoC-sprinting outperforms conventional full-sprinting which always activate all the dark cores.</p><p>It is able to provide tremendous performance speedup, longer sprint duration, and reduces the chip power significantly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Zhan and Xie were supported in part by NSF 0905365 and by the Department of Energy under Award Number DE -SC0005026. Sun was supported by NSFC (No. 61202072), 863 Program of China (No. 2013AA013201), and AMD grant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: During nominal operation, only one core is active under the TDP constraint, whereas the rest cores are dark silicon. In sprinting mode, all the cores will be activated to provide instantaneous throughput.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Router power breakdown (dynamic power vs leakage power) when varying the operating voltage and frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Chip power breakdown during nominal operation (single active core) in sprinting-based multicores. The percentages denote the component power (core, cache, NoC, MC, and others) over the total chip power.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Execution time of running PARSEC benchmarks when increasing the number of available cores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Logical connection of a 16-node mesh network. The irregular topology and convex DOR routing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Physical allocation for the original network in (a). Only links for 4 nodes are shown for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Topology, routing, and floorplan for fine-grained sprinting</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>ALGORITHM 2 .Figure 6 :</head><label>26</label><figDesc>Figure 6: Routing logic for convex DOR algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>ALGORITHM 3 .</head><label>3</label><figDesc>Thermal-aware heuristic floorplanning algorithm Result: Positions for all nodes Initialize: Original floorplan</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>ALGORITHM 4 .</head><label>4</label><figDesc>MaxWeightedDistance(S, S , R k ) Initialize: Sum = 0; Max = 0; for every node R i in S do for every node R j in S do w i j = 1/(|x kx j | + |y ky j |);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Execution time comparison with different sprint mechanisms.</figDesc><graphic url="image-184.png" coords="5,68.57,58.12,171.50,52.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Core power dissipation with different sprinting schemes. Here fine-grained sprinting does not include any power-gating schemes to the idle cores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Comparisons of average network latency after running PARSEC with full-sprinting and NoC-sprinting</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Comparisons of total network power after running PARSEC with full-sprinting and NoC-sprinting</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Figure 11: Performance and power analysis on full-sprinting and NoC-sprinting with synthetic uniform-random traffic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>System and Interconnect configuration</figDesc><table><row><cell>core count/freq.</cell><cell>16, 2GHz</cell><cell>topology</cell><cell>4 ? 4 2D Mesh</cell></row><row><cell>L1 I &amp; D cache</cell><cell>private, 64KB</cell><cell>router pipeline</cell><cell>classic five-stage</cell></row><row><cell>L2 cache</cell><cell>shared &amp; tiled, 4MB</cell><cell>VC count</cell><cell>4 VCs per port</cell></row><row><cell>cacheline size</cell><cell>64B</cell><cell>buffer depth</cell><cell>4 buffers per VC</cell></row><row><cell>memory</cell><cell>1GB DRAM</cell><cell>packet length</cell><cell>5 flits</cell></row><row><cell>cache-coherency</cell><cell>MESI protocol</cell><cell>flit length</cell><cell>16 bytes</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Garnet: A detailed on-chip network model inside a full-system simulator</title>
		<author>
			<persName><forename type="first">N</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-S</forename><surname>Peh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPASS</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bienia</surname></persName>
		</author>
		<title level="m">Benchmarking Modern Multiprocessors</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>Princeton University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The gem5 simulator</title>
		<author>
			<persName><forename type="first">N</forename><surname>Binkert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Nord: Node-router decoupling for effective power-gating of on-chip routers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Pinkston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-45</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="270" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Catnap: Energy proportional multiple network-on-chip</title>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dreslinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="320" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A helper thread based EDP reduction scheme for adapting application execution in CMPs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPDPS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An efficient implementation of distributed routing algorithms for nocs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Flich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rodrigo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NoCs</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A 5-ghz mesh interconnect for a teraflops processor</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hoskote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vangal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Borkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="51" to="61" />
			<pubPlace>Micro, IEEE</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">HotSpot: A compact thermal modeling methodology for early-stage VLSI design</title>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on VLSI</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="501" to="513" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A detailed and flexible cycle-accurate network-on-chip simulator</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISPASS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="86" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Single-Cycle Multihop Asynchronous Repeated Traversal: A SMART Future for Reconfigurable On-Chip Networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">40</biblScope>
			<biblScope unit="page" from="48" to="55" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic power-performance adaptation of parallel computation on chip multiprocessors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">McPAT: an integrated power, area, and timing modeling framework for multicore and manycore architectures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-42</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Run-time power gating of on-chip routers using look-ahead routing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Matsutani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koibuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Amano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASP-DAC</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The 48-core scc processor: the programmer&apos;s view</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Mattson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Implementation of an 8-core, 64-thread, power-efficient sparc server on a chip</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">G</forename><surname>Nawathe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE JSSC</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="20" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Computational sprinting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Energy-efficient interconnect via router parking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Samih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="508" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DSENT-a tool connecting emerging photonics with electronics for opto-electronic networks-on-chip modeling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NoCS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The Raw microprocessor: A computational fabric for software circuits and general-purpose programs</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="25" to="35" />
			<date type="published" when="2002">2002</date>
			<pubPlace>Micro</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Conservation cores: reducing the energy of mature computations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="205" to="218" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
