<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Local Augmentation for Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Songtao</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanze</forename><surname>Dong</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Hong Kong University of Science and Technol</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Lanqing</forename><surname>Li</surname></persName>
							<email>&lt;lanqingli1993@gmail.com&gt;</email>
							<affiliation key="aff4">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dinghao</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Pennsylvania State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab</orgName>
								<orgName type="institution">Tencent AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Local Augmentation for Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have achieved remarkable performance on graph-based tasks. The key idea for GNNs is to obtain informative representation through aggregating information from local neighborhoods. However, it remains an open question whether the neighborhood information is adequately aggregated for learning representations of nodes with few neighbors. To address this, we propose a simple and efficient data augmentation strategy, local augmentation, to learn the distribution of the node representations of the neighbors conditioned on the central node's representation and enhance GNN's expressive power with generated features. Local augmentation is a general framework that can be applied to any GNN model in a plug-and-play manner. It samples feature vectors associated with each node from the learned conditional distribution as additional input for the backbone model at each training iteration. Extensive experiments and analyses show that local augmentation consistently yields performance improvement when applied to various GNN architectures across a diverse set of benchmarks. For example, experiments show that plugging in local augmentation to GCN and GAT improves by an average of 3.4% and 1.6% in terms of test accuracy on Cora, Citeseer, and Pubmed. Besides, our experimental results on large graphs (OGB) show that our model consistently improves performance over backbones. Code is available at https://github.com/ SongtaoLiu0823/LAGNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph Neural Networks (GNNs) and their variants <ref type="bibr" target="#b30">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b23">Hamilton et al., 2017;</ref><ref type="bibr" target="#b55">Veliƒçkoviƒá et al., 2018)</ref> have achieved state-of-the-art performance on a variety of graph-based tasks, including recommendation system <ref type="bibr" target="#b70">(Ying et al., 2018)</ref>, drug discovery <ref type="bibr" target="#b7">(Dai et al., 2019)</ref> and traffic prediction <ref type="bibr" target="#b22">(Guo et al., 2019)</ref>. The core of GNNs is to employ a message-passing mechanism that passes and aggregates information from the local neighborhood to generate informative representations.</p><p>Recent development of deep GNNs, such as JKnet <ref type="bibr" target="#b66">(Xu et al., 2018)</ref>, GCNII <ref type="bibr" target="#b4">(Chen et al., 2020b)</ref>, and RevGNN-Deep <ref type="bibr" target="#b33">(Li et al., 2021)</ref> adds the output of shallow layers to the deep layers with a residual-style design, to preserve the locality information of node representations <ref type="bibr" target="#b4">(Chen et al., 2020b)</ref>. Moreover, recent studies <ref type="bibr" target="#b72">(Zeng et al., 2021;</ref><ref type="bibr" target="#b73">Zhang &amp; Li, 2021;</ref><ref type="bibr" target="#b62">Wijesinghe &amp; Wang, 2022)</ref> utilize structural information of the local neighborhood to design efficient message-passing aggregation schemes to enhance the expressive power of GNNs. These works demonstrate that local information plays a significant role in training GNN models and designing powerful GNNs.</p><p>Despite advances of GNNs in learning node representations from the local neighborhood, it remains an open problem whether the local neighborhood information is sufficient to obtain effective node representations, especially for nodes with limited number of neighbors. We argue that the limited number of neighbors in the local neighborhood restricts the expressive power of GNNs and hinders their performance, especially in sample-starving cases where some nodes have very few neighbors. Stacking graph layers to enlarge the receptive field can incorporate multi-hop neighboring information but leads to over-smoothing <ref type="bibr" target="#b34">(Li et al., 2018)</ref> without residual connection to the input, and is not a direct solution to address this issue. Existing works on GNN model architecture cannot tackle the problem that the very limited neighbors are unfavorable to learning node representations. Therefore, here we focus on enriching the local information for low-degree nodes to obtain effective representations.</p><p>One promising solution is to generate more samples in the local neighborhood via data augmentation. Data augmentation has been well-studied in computer vision <ref type="bibr" target="#b49">(Shorten &amp; Khoshgoftaar, 2019;</ref><ref type="bibr" target="#b6">Cubuk et al., 2019;</ref><ref type="bibr" target="#b74">Zhao et al., 2019;</ref><ref type="bibr" target="#b11">Dong et al., 2022)</ref> and natural language processing <ref type="bibr" target="#b12">(Fadaee et al., 2017;</ref><ref type="bibr" target="#b42">S ¬∏ahin &amp; Steedman, 2019;</ref><ref type="bibr" target="#b64">Xia et al., 2019)</ref>, but remains under-explored on graph-structured data. Existing graph data augmentation approaches only perturb at the topology-level and feature-level from a global perspective, which can be divided into two categories: topology-level augmentation <ref type="bibr" target="#b40">(Rong et al., 2020;</ref><ref type="bibr" target="#b75">Zhao et al., 2021)</ref> and feature-level augmentation <ref type="bibr" target="#b10">(Deng et al., 2019;</ref><ref type="bibr" target="#b14">Feng et al., 2019;</ref><ref type="bibr" target="#b32">Kong et al., 2020;</ref><ref type="bibr" target="#b13">Fang et al., 2021)</ref>. Topology-level augmentation perturbs the adjacency matrix, yielding different graph structures. On the other hand, existing featurelevel augmentation <ref type="bibr" target="#b10">(Deng et al., 2019;</ref><ref type="bibr" target="#b14">Feng et al., 2019;</ref><ref type="bibr" target="#b32">Kong et al., 2020)</ref> exploits perturbation of node attributes guided by adversarial training to boost generalization. These augmentation techniques have a prominent drawback: they focus on global augmentation concerning the properties of the whole distribution of the graph rather than a single node, and neglect the local information of the neighborhood.</p><p>Present Work. In this work, in order to promote the aggregation scheme with more generated samples in the local neighborhood, we propose a novel and efficient data augmentation framework: Local Augmentation for Graph Neural Networks (LA-GNNs). The term "local augmentation" refers to generating neighborhood features via a generative model conditioned on local structures and node features. Specifically, our proposed framework includes a pre-training step, which learns the conditional distribution of the connected neighbors' node features given one center node's features via a generative model. As shown in Fig. <ref type="figure">1</ref>, we then exploit this distribution to generate feature vectors associated with this center node as additional input at each training iteration. Furthermore, we decouple the pre-training of the generative model and downstream GNN training, allowing our data augmentation model to be applied to any GNN model in a plug-and-play manner.</p><p>We verify the effectiveness of LAGNNs on three standard citation networks (Cora, Citeseer, Pubmed) and Open Graph Benchmark (OGB) <ref type="bibr" target="#b27">(Hu et al., 2020)</ref>. Extensive experimental results on semi-supervised node classification show that our local augmentation achieves new stat-of-the-art performance: LAGCN and LAGAT achieve up to by an average of 3.4% and 1.6% in terms of test accuracy over GCN and GAT respectively on Cora, Citeseer, and Pubmed. LAGNN also obtains superior performance on large-scale OGB datasets. We show that our model improves 1.7% and 0.2% of test accuracy on Pubmed for nodes with degrees in <ref type="bibr">[2, 5] and [6, 20]</ref> respectively. Besides, our local augmentation model outperforms other feature/topology-level augmentation models, such as G-GNN <ref type="bibr" target="#b76">(Zhu et al., 2020)</ref>, DropEdge <ref type="bibr" target="#b40">(Rong et al., 2020)</ref>, GRAND <ref type="bibr" target="#b15">(Feng et al., 2020)</ref>, and GAUG <ref type="bibr" target="#b75">(Zhao et al., 2021)</ref> on node classification tasks, which demonstrates the superiority of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>‚Ä¢ We propose a general augmentation strategy to generate more features in the local neighborhood to enhance the expressive power of existing GNNs;</p><p>‚Ä¢ We explore a new direction on pre-training generative models for graphs to improve downstream task performance;</p><p>‚Ä¢ Our proposed framework is flexible and can be applied to various popular backbones. Extensive experimental results demonstrate that our proposed framework could improve the performance of GNN variants on different benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>Notations. Let G = (V, E) represent the graph, where</p><formula xml:id="formula_0">V is the set of vertices {v 1 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , v N } with |V| = N and</formula><p>E is the set of edges. The adjacency matrix is defined as A ‚àà {0, 1} N √óN , and</p><formula xml:id="formula_1">A i,j = 1 if and only if (v i , v j ) ‚àà E. Let N i = {v j |A i,j = 1}</formula><p>denotes the neighborhood of node v i and D denote the diagonal degree matrix, where</p><formula xml:id="formula_2">D i,i = n j=1 A i,j</formula><p>. The feature matrix is denoted as X ‚àà R N √óF where each node v is associated with a Fdimensional feature vector X v . Y ‚àà {0, 1} N √óC denote the one-hot label matrix, where Y i ‚àà {0, 1} C is a one-hot vector and C j=1 Y i,j = 1 for any v i ‚àà V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Neural Networks. Many popular Graph Neural</head><p>Networks (GNNs) directly operate on the graph structure and capture the dependence of graphs via message passing between the nodes of a graph. They repeatedly aggregate the representations of immediate neighbors N v of node v and combine the aggregated information and its representation vector to obtain a representation vector h v . The k-th layer of the GNN message-passing scheme is:</p><formula xml:id="formula_3">h (k) v = COM h (k‚àí1) v , AGG h (k‚àí1) u , e u,v |u ‚àà N v ,<label>(1)</label></formula><p>where COM(‚Ä¢) and AGG(‚Ä¢) denotes COMBINE and AG-GREGATE functions respectively, h (k) v is the representation vector of node v in the k-th layer, and e u,v is the edge vector between node u and node v. Specifically,</p><formula xml:id="formula_4">h (0) v = X v .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Local Augmentation for Graph Neural Networks (LAGNN)</head><p>In this section, we first present how to generate more samples in the local neighborhood via a generative model. Then we show how to decouple the pre-training of the generative</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Neural Network</head><p>The original features</p><p>The original and generated feature matrices</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated features</head><p>Sampled from local neighborhood distribution</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>‚Ä¶‚Ä¶</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated features</head><p>Figure <ref type="figure">1</ref>. A schematic depiction of our local augmentation. The yellow circles on the graph correspond to the neighbor nodes. Assume we have learned the distribution of the local neighborhood. We generate features from the local neighborhood distribution. And then we take the original features and the generated features as input for downstream GNNs.</p><p>model and downstream GNN training from a probabilistic perspective, so that our local augmentation model can be applied to any GNN model in a plug-and-play manner. We then introduce the architecture of LA-GNNs and the training details. The overall framework is illustrated in Fig. <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Local Augmentation</head><p>Motivation. Existing GNNs focus on designing a message-passing scheme to exploit local information to obtain node representations. We explore a new direction in that we can generate more samples in the local neighborhood, especially for nodes with few neighbors, to enhance the expressive power of various GNNs. In order to produce more samples in a node v's neighborhood N v , we need to know the distribution of the node representation of its neighbors. As this distribution is related to the center node v, we can learn it conditioned on the center node's representation via a generative model. Approach. We exploit the conditional variational autoencoder (CVAE) <ref type="bibr" target="#b28">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b51">Sohn et al., 2015)</ref> to learn the conditional distribution of the node features of connected neighbors u(u ‚àà N v ) given the center node v. In our CVAE setting, we use X v as a condition since the distribution of X u (u ‚àà N v ) is related to X v . Following <ref type="bibr" target="#b51">Sohn et al. (2015)</ref>, the latent variable z is generated from the prior distribution p Œ∏ (z|X v ) and the data X u is generated by the generative distribution p Œ∏ (X|X v , z) conditioned on z and X v : z ‚àº p Œ∏ (z|X v ), X u ‚àº p Œ∏ (X|X v , z v ). Let œï denote the variational parameters and Œ∏ represent the generative parameters, we have</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benefits</head><formula xml:id="formula_5">log p Œ∏ (X u |X v ) = q œï (z|X u , X v ) log p Œ∏ (X u , z|X v ) q œï (z|X u , X v ) dz + KL (q œï (z|X u , X v )‚à•p Œ∏ (z|X u , X v )) ‚â• q œï (z|X u , X v ) log p Œ∏ (X u , z|X v ) q œï (z|X u , X v ) dz,</formula><p>and the evidence lower bound (ELBO) can be written as:</p><formula xml:id="formula_6">L(X u , X v ; Œ∏, œï) = ‚àí KL(q œï (z|X u , X v )‚à•p Œ∏ (z|X v )) + 1 L L l=1 log p Œ∏ (X u |X v , z (l) )<label>(2)</label></formula><p>where z (l) = g œï (X v , X u , œµ (l) ), œµ (l) ‚àº N (0, I) and L is the number of neighbors of node v. Note that as we have discussed before, we just train one CVAE for all nodes. In the training stage, the objective is to use the neighboring pairs (X v , X u , u ‚àà N v ) as input to maximize the ELBO, i.e., Eq. ( <ref type="formula" target="#formula_6">2</ref>). In the generation stage, we use the node feature X v as the condition and sample a latent variable z ‚àº N (0, I) as input for the decoder. Then we can get generated feature vector X v associated with node v.</p><p>Discussion. When learning the distribution of the neighbors' feature conditioned on the central node, we do not consider the effect of other nodes connected to each neighbor on the neighbors' feature. If we regard the center node as a parent node, and its neighbors as the children nodes of the center node, then our assumption is similar to the Causal Markov Condition in a Bayesian network <ref type="bibr" target="#b25">(Hausman &amp; Woodward, 1999)</ref>: the distribution of the neighbor's features is independent of its non-descendants given its parent node. This assumption is important and common in the literature of the probabilistic graphical model. The advantage is that this assumption avoids the exponential complexity of conditioning on multi-hop neighbors, significantly improving scalability. Our experimental results show that our method still achieves remarkable performance across all benchmarks, thanks to the expressive power of deep generative models (similar to how the assumption of variational inference does not limit the performance of deep VAEs in real datasets).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Decoupling the Generative Model Training from Downstream Graph Learning</head><p>Most existing GNN models follow the message passing mechanism <ref type="bibr" target="#b19">(Gilmer et al., 2017)</ref> and can be regarded as a learned classification or regression function. In order to make predictions, GNN models need to estimate the posterior distribution P Œò (‚Ä¢|A, X) with respect to the graph structure A and feature matrix X. For example, ‚Ä¢ can be class labels Y on the node classification task. We can use Maximum Likelihood Estimation (MLE) to estimate the parameter Œò by optimizing the following likelihood function:</p><formula xml:id="formula_7">max i P Œò (‚Ä¢|A, X) ,<label>(3)</label></formula><p>where i represents the i-th data point in the training dataset.</p><p>In our local augmentation model, in order to further improve the expressive power of GNNs, we introduce a generated feature vector X v for the center node v by using X v as input condition and sampling from the generative model. Let X denote the generated feature matrix where the j-th row corresponds to the generated feature vector X j . We incorporate X in Eq. ( <ref type="formula" target="#formula_7">3</ref>) and rewrite it as follows:</p><formula xml:id="formula_8">max i X P Œò ‚Ä¢, X|A, X .<label>(4)</label></formula><p>For Bayesian tractability, we decompose P Œò in Eq.( <ref type="formula" target="#formula_8">4</ref>) as a product of two posterior probabilities:</p><formula xml:id="formula_9">P Œò,Œ¶ (‚Ä¢, X|A, X) := P Œò (‚Ä¢|A, X, X)Q Œ¶ (X|A, X),<label>(5)</label></formula><p>where P Œò (‚Ä¢|A, X, X) and Q Œ¶ (X|A, X) denote the probabilistic distributions approximated by the GNN models and the generative model respectively, parameterized by Œò and Œ¶. By doing this, we can decouple our proposed local augmentation and the specific graph learning, allowing our augmentation model to be applied to various GNN models with only one-time pre-training for the generative model. Therefore, local augmentation can be regarded as an unsupervised pre-training model prior to the GNN training. The representation power of Eq. ( <ref type="formula" target="#formula_9">5</ref>) is superior than that of a single predictor P Œò (Y k |A, X) since we provide GNN models with more generated samples in the local neighborhood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Architecture</head><p>In this section, we present the details of how to train GNNs with generated samples from our local augmentation model as additional input. To illustrate the effectiveness of our local augmentation model, we provide two different ways of exploiting our generated samples, leading to average and concatenation design of the architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LAGCN.</head><p>For GCN, we only make a small change on the first graph convolution layer with</p><formula xml:id="formula_10">H (1) = œÉ P XW (1) 0 ‚à•œÉ P XW (1) 1 ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_11">P = D ‚àí 1 2 A D ‚àí 1 2 .</formula><p>The notation ‚à• means concate- nation of the matrices on the second dimension. The suband super-scripts in the weight matrix W denote the layers' and the parameters' ordinal numbers. In order not to change the parameter size of the GCN model, the sum of the second dimension of W</p><p>(1) 0</p><p>and W</p><p>(1) 1</p><p>is equal to the second dimension of W (1) of GCN. For other architectures (LAGAT, LASAGE, LAGCNII) which we will discuss later, we keep the same setting on the parameter size in the first layer as LAGCN. For GraphSAGE and GCNII, they have similar architectures to GCN and we employ the same modification strategy as LAGCN for LASAGE and LAGCNII. In addition to the concatenation-style design, we can also average X and X as input for GNNs and do not change the architecture.</p><p>LAGAT. Similarly, the first layer of LAGAT is defined as follows:</p><formula xml:id="formula_12">H (1) = Ô£´ Ô£¨ Ô£≠ K/2 ‚à• k=1 œÉ u‚ààNv Œ± k vu W (1) k X u Ô£∂ Ô£∑ Ô£∏ Ô£´ Ô£¨ Ô£≠ K ‚à• k=K/2+1 œÉ u‚ààNv Œ± k vu W (1) k X u Ô£∂ Ô£∑ Ô£∏ ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_13">Œ± k vu is computed on X (1 ‚â§ k ‚â§ K/2) or X (K/2 + 1 ‚â§ k ‚â§ K). Note that the second dimension of W (1) k</formula><p>is the same as GAT. We just replace the input of the half of the attention heads with X. And we also provide the average-style design for GAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss Functions</head><p>In this section, we explain two loss functions to train LAGNNs and the underlying motivation: supervised loss and consistency loss.</p><p>Supervised Loss. Once we have completed the pretraining of the local augmentation model, we then use the generated feature matrix X produced by it as additional input to enhance the expressive power of GNN models. Given the training labels K and S augmented feature matrix X (s) , we can write the supervised loss function for node classification tasks as follows:</p><formula xml:id="formula_14">L sup = ‚àí 1 S S s=1 i‚ààK Y i log Z (s) i ,<label>(8)</label></formula><p>where s) , Œò). Note that we just provide a type of supervised loss function. For other graph learning tasks such as link prediction and graph classification, the supervised loss functions can be adjusted accordingly.</p><formula xml:id="formula_15">Z (s) = f LAGN N (A, X, X<label>(</label></formula><p>Consistency Regularization Loss. Inspired by the huge success of consistency training <ref type="bibr" target="#b61">(Wang et al., 2020c;</ref><ref type="bibr" target="#b15">Feng et al., 2020;</ref><ref type="bibr" target="#b43">Sajjadi et al., 2016;</ref><ref type="bibr" target="#b46">Samuli &amp; Timo, 2017;</ref><ref type="bibr" target="#b1">Berthelot et al., 2019;</ref><ref type="bibr" target="#b57">Verma et al., 2019)</ref> on semisupervised learning tasks, we provide an optional loss function for specific GNNs and graph learning tasks. Intuitively, the consistency regularization encourages invariant prediction of different inputs at each training iteration <ref type="bibr" target="#b57">(Verma et al., 2019)</ref>. Specifically, the consistency regularization loss function has the following form:</p><formula xml:id="formula_16">L con = 1 S S s=1 N i=1 Z ‚Ä≤ i ‚àí Z (s) i 2 2 , (<label>9</label></formula><formula xml:id="formula_17">)</formula><p>where</p><formula xml:id="formula_18">Z i = 1 S S s=1 Z (s) i , Z ‚Ä≤ i = Z 1 T i / C c=1 Z 1 T</formula><p>ic is the sharpening trick <ref type="bibr" target="#b1">(Berthelot et al., 2019)</ref>, and T is a hyperparameter which adjusts the "temperature" of this categorical distribution. The sharpening trick can reduce the entropy of the predictions. Generate the augmented feature matrix:</p><formula xml:id="formula_19">X (s) ‚àº Q Œ¶ . 7:</formula><p>Obtain the prediction using LAGNN P Œò :</p><formula xml:id="formula_20">Z (s) = f LAGN N (A, X, X (s) , Œò) 8:</formula><p>end for 9:</p><p>Compute supervised classification loss L sup via Eq. ( <ref type="formula" target="#formula_14">8</ref>) 10:</p><p>Optionally compute the consistency regularization loss L con via Eq. ( <ref type="formula" target="#formula_16">9</ref>).</p><p>11:</p><p>Update the parameters Œò by gradients descending:</p><formula xml:id="formula_21">Œò = Œò ‚àí Œ∑‚àá Œò (L sup (+Œ≤L con )) 12:</formula><p>Regenerate the augmented feature matrix: X ‚àº Q Œ¶ .</p><p>13:</p><p>Compute the validation loss function or the validation accuracy via Z = f LAGN N (A, X, X, Œò) and Eq. ( <ref type="formula" target="#formula_14">8</ref>). 14: end while 15: Predict via: Z = f LAGN N (A, X, X, Œò), where we select X with the smallest validation loss function or the highest validation accuracy.</p><p>additional input at each training iteration to train the GNN models. But for GRAND <ref type="bibr" target="#b15">(Feng et al., 2020)</ref>, we just sample one feature matrix during training stage since we find we can get better performance with such a sample strategy. Supervised loss functions are computed on the initial feature matrix X and the generated feature matrix X. Besides, we optionally compute the consistency regularization loss function  </p><formula xml:id="formula_22">L con based on f LAGN N (A, X, X<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Semi-supervised Learning</head><p>Datasets. We utilize three public citation network datasets Cora, Citeseer, and Pubmed <ref type="bibr" target="#b47">(Sen et al., 2008)</ref> for semisupervised node classification. All the dataset statistics can be found in Appendix D.</p><p>Baselines. We consider three popular graph neural networks: GCN <ref type="bibr" target="#b30">(Kipf &amp; Welling, 2017)</ref>, GAT <ref type="bibr" target="#b55">(Veliƒçkoviƒá et al., 2018)</ref>, and GCNII <ref type="bibr" target="#b4">(Chen et al., 2020b)</ref> as our implemented backbones. GCN and GAT are representatives of the state-of-the-art GNN architectures, while GCNII is a deep GNN model with a skip connection design. For each of these backbones, we employ the concatenation-style design discussed in Sec. 3.3 as our LAGNN architecture. But we keep the size of the learnable weight matrix of LAGNN the same as the corresponding GNN model, which is detailed in the Appendix D. We also combine our method with other data augmentation models -GRAND <ref type="bibr" target="#b15">(Feng et al., 2020)</ref>. To evaluate our proposed framework, we compare our model against state-of-the-art models of four categories:</p><p>‚Ä¢ Backbone models: Chebyshev <ref type="bibr" target="#b8">(Defferrard et al., 2016)</ref>, GCN <ref type="bibr" target="#b30">(Kipf &amp; Welling, 2017)</ref>, GAT <ref type="bibr" target="#b55">(Veliƒçkoviƒá et al., 2018)</ref>, APPNP <ref type="bibr" target="#b31">(Klicpera et al., 2019)</ref>, Graph U-net <ref type="bibr" target="#b17">(Gao &amp; Ji, 2019)</ref>, MixHop (Abu-El-Haija et al., 2019), GC-NII <ref type="bibr" target="#b4">(Chen et al., 2020b)</ref>, GSNN-M <ref type="bibr" target="#b58">(Wang et al., 2020a)</ref>, and S 2 GC <ref type="bibr" target="#b77">(Zhu &amp; Koniusz, 2021)</ref> ‚Ä¢ Feature-level augmentation models: G-GNNs <ref type="bibr" target="#b76">(Zhu et al., 2020)</ref>, and GRAND <ref type="bibr" target="#b15">(Feng et al., 2020)</ref>.</p><p>‚Ä¢ Topology-level augmentation modes: DropEdge <ref type="bibr" target="#b40">(Rong et al., 2020)</ref> and GAUG-O <ref type="bibr" target="#b75">(Zhao et al., 2021)</ref>.</p><p>‚Ä¢ Subgraph GNN: GraphSNN <ref type="bibr" target="#b62">(Wijesinghe &amp; Wang, 2022)</ref>.</p><p>The choice of baselines aims to show that existing GNNs benefit from our proposed local data augmentation, and our model outperforms other data augmentation models.</p><p>Experimental setup. We apply the standard fixed splits <ref type="bibr" target="#b69">(Yang et al., 2016)</ref> on Cora, Citeseer, and Pubmed, with 20 nodes per class for training, 500 nodes for validation, and 1,000 nodes for testing. See more details on experimental setup and hyper-parameters in the Appendix D.</p><p>Comparison with SOTA. We report the mean node classification accuracy after 100 runs in Table <ref type="table" target="#tab_3">1</ref>. We reuse the metrics of the baselines already reported in the corresponding papers. The results demonstrate that the backbone models equipped with our method achieve better performance across all three datasets. Specifically, local augmentation can improve upon GCN by a margin of 3.1%, 4.4%, and 2.7% on Cora, Citeseer, and Pubmed respectively, while the improved margins of LAGAT over GAT are 1.7%, 1.2%, and 2.0% respectively. Moreover, when combined with other data augmentation methods -GRAND, we can still improve by a margin of 0.3%, 0.4%, and 0.7% respectively. Furthermore, based on the std information of the experimental results of GRAND and our LA-GRAND, we compute the p-value by t-test to verify the improvements. Except for LA-GCN v.s. GRAND-GCN on Cora (with p-value 0.046), all the p-value ‚â™ 0.01 by t-test (the same test is also employed by GRAND), which shows the improvements of LAover GRAND-are statistically significant. Compared with other data augmentation models <ref type="bibr" target="#b76">(Zhu et al., 2020;</ref><ref type="bibr" target="#b40">Rong et al., 2020;</ref><ref type="bibr" target="#b75">Zhao et al., 2021)</ref>, LA-GNN achieves the best performance on two popular backbones GCN and GAT, showing local information is indeed better than the augmentation approaches from a global perspective, such as DropEdge <ref type="bibr" target="#b40">(Rong et al., 2020)</ref> and GAUG <ref type="bibr" target="#b75">(Zhao et al., 2021)</ref>.</p><p>Both our model and GraphSNN start from the perspective </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Full-supervised Learning</head><p>Datasets. To demonstrate the effectiveness of our model on large graphs for full-supervised node and link classification tasks, we utilize ogbn-products, ogbn-proteins, ogbnarxiv, and ogbl-collab datasets from Open Graph Benchmark (OGB) <ref type="bibr" target="#b27">(Hu et al., 2020)</ref> for evaluation. All the dataset statistics can be found in the Appendix D.</p><p>Baselines. We consider four popular message-passing GNNs: GCN <ref type="bibr" target="#b30">(Kipf &amp; Welling, 2017)</ref>, GAT <ref type="bibr" target="#b55">(Veliƒçkoviƒá et al., 2018)</ref>, and GraphSAGE <ref type="bibr" target="#b23">(Hamilton et al., 2017)</ref> as backbones. For each of these backbones, we apply the concatenation-style or average-style design discussed in Sec. 3.3 as our LAGNN architecture, which is detailed in the Appendix D. For node classification on arxiv, proteins, and products, we compare it against MLP, Node2vec (Grover &amp; Leskovec, 2016), GCN, GAT, GraphSAGE, FLAG <ref type="bibr" target="#b32">(Kong et al., 2020)</ref>, GraphSNN <ref type="bibr" target="#b62">(Wijesinghe &amp; Wang, 2022)</ref>, GraphZoom <ref type="bibr" target="#b9">(Deng et al., 2020)</ref>, and CoLinkDistMLP <ref type="bibr" target="#b36">(Luo et al., 2021)</ref>. Besides, we use ogbl-collab to evaluate the performance of our model on the link prediction task, and compare it against MLP, Node2vec, GCN, GraphSAGE. Experimental Setup and Results. We follow the experimental setup as in OGB <ref type="bibr" target="#b27">(Hu et al., 2020)</ref>. For the detailed setup, such as the split ratio and evaluation metric, we just follow the same setting from the OGB implementation. Note that the test results of the baselines are from the official OGB leaderboard website<ref type="foot" target="#foot_0">1</ref> or corresponding papers. For a fair comparison, we implement our models on OGB tasks from the open-resource codes with only touching the first layer. From the OGB leaderboard, we can know that the test results are sensitive concerning model size and various tricks. So we do not change the model size of the backbones as suggested in Sec. 3.3 and do not add other tricks. The details of our LAGNN architecture can be found in the Appendix D. Results are summarized in Table <ref type="table" target="#tab_4">2</ref> and Table <ref type="table" target="#tab_5">3</ref>. Following common practice, we report the test accuracy associated with the best validation performance.</p><p>The results on node and link prediction demonstrate that our augmentation model consistently improves performance over backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Inductive Learning</head><p>To evaluate the effectiveness of our model on inductive learning tasks, we take ogbg-molhiv and ogbg-molpcba datasets from OGB for evaluation. For the experimental setup, we just follow the official OGB implementation. We consider GCN and GIN <ref type="bibr" target="#b67">(Xu et al., 2019b)</ref> as backbones and the architecture of LAGCN and LAGIN can be found in the Appendix D. The results are summarized in Table <ref type="table">4</ref>. The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>To demonstrate the effectiveness of our proposed local augmentation model, we conduct experiments of LAGCN on Pubmed that compare it to several of its ablated variants.</p><p>The results are shown in Table <ref type="table" target="#tab_6">5</ref>. "+ concatenation" means that we only apply our concatenation-style design architecture of LAGCN in Sec. 3.3 with the original feature matrix as additional concatenated input. The improvement is 0.3%, which shows that our modification of architecture does not have a lot of effect on the result. "+ local augmentation" means we use the generated feature matrix as additional concatenated input without consistency training. Although we do not use consistency training, the generated feature matrix as additional input improves the GCN's test accuracy by a margin of 1.8%. With the consistency training and sharpening trick, we can enhance the performance further.</p><p>From the ablation study, it is evident that the performance gain is due to our proposed generative local augmentation framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Robustness to Missing Information</head><p>In this section, we conduct experiments to verify that our proposed framework is robust against missing information in the feature attributes. Specifically, we mask a certain percentage of the attributes of each feature vector and use the same pipeline to do augmentation for the masked feature matrix. As shown in Table <ref type="table" target="#tab_7">6</ref>, we can see that as the mask ratio increases, the gap of the performance between the GCN and LA-GCN enlarges in most cases in Citeseer, which corroborates our insight that our local augmentation can complement the contextual information of the local neighborhood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Case Study</head><p>In this section, we explore the change in test accuracy of different nodes after applying our local augmentation method. Note that we only apply local augmentation without consistency training, and we set S to 1. The results are summarized in Table <ref type="table" target="#tab_8">7</ref>. From the results, we can draw the following conclusions: 1) The degree of most nodes on the Pubmed test set is relatively small, of which with degree fewer than 6 accounts for about 76.1%. 2) Nodes with smaller degree tend to have lower test accuracy. However, our local augmentation can enrich local information for these nodes and thus enhance their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Over-smoothing Analysis</head><p>It is well known that stacking GNN layers leads to oversmoothing <ref type="bibr" target="#b34">(Li et al., 2018)</ref>. In this section, we discuss how our proposed approach prevents the over-smoothing issue in GNNs compared to existing approaches. We utilize the MADgap <ref type="bibr" target="#b3">(Chen et al., 2020a)</ref> metric to compare our method with existing GNNs. Table <ref type="table" target="#tab_9">8</ref> reports the MADgap metric of LAGCN and GCN on Cora (on different layers). We can observe that the MADgap metric of LAGCN is larger than or the same as that of GCN on different layers. Although our approach is not to address over-smoothing, our method can enrich the local neighborhood information and thus can improve the locality of node representations. Therefore, we can alleviate over-smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Unsupervised Representation Learning on Graphs. In general, unsupervised representation learning methods on graphs include contrastive-based self-supervision methods <ref type="bibr" target="#b56">(Velickovic et al., 2019;</ref><ref type="bibr" target="#b52">Sun et al., 2020;</ref><ref type="bibr" target="#b24">Hassani &amp; Khasahmadi, 2020;</ref><ref type="bibr" target="#b71">You et al., 2020)</ref>, graph embedding methods <ref type="bibr" target="#b18">(Garc√≠a-Dur√°n &amp; Niepert, 2017;</ref><ref type="bibr" target="#b23">Hamilton et al., 2017)</ref>, and random walk methods <ref type="bibr" target="#b39">(Perozzi et al., 2014;</ref><ref type="bibr" target="#b54">Tang et al., 2015;</ref><ref type="bibr" target="#b21">Grover &amp; Leskovec, 2016)</ref>. Contrastive learning works <ref type="bibr" target="#b24">(Hassani &amp; Khasahmadi, 2020;</ref><ref type="bibr" target="#b71">You et al., 2020)</ref> employ contrastive loss functions to enforce minimizing the representation distance of the positive pairs and maximizing the distance of the negative pairs. Random walk methods get sentences by taking random walks across nodes and use NLP word embedding models to learn node representations. Our local augmentation is also an unsupervised method for learning local neighborhood information.</p><p>Graph Generative Models. Generative models <ref type="bibr" target="#b20">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b28">Kingma &amp; Welling, 2013)</ref> are powerful tools of learning data distribution through unsupervised learning. Recently, researchers have proposed several interesting generative models for graph data generation. Variational graph auto-encoder (VGAE) <ref type="bibr" target="#b29">(Kipf &amp; Welling, 2016)</ref> exploits the latent variables to learn interpretable representations for undirected graphs. <ref type="bibr" target="#b44">Salha et al. (2019)</ref> make use of a simple linear model to replace the GCN encoder in VGAE and reduce the complexity of encoding schemes. <ref type="bibr" target="#b65">Xu et al. (2019a)</ref> propose a generative GCN model to learn node representations for growing graphs. ConDgen <ref type="bibr" target="#b68">(Yang et al., 2019)</ref> exploits the GCN encoder to handle the invariant permutation for conditional structure generation. Besides, some methods have been proposed to apply the graph generative models in various applications such as graph matching (Simonovsky &amp; Komodakis, 2018), and molecule design <ref type="bibr" target="#b35">(Liu et al., 2018)</ref>, retrosynthesis prediction <ref type="bibr" target="#b48">(Shi et al., 2020)</ref> and chemical design <ref type="bibr" target="#b45">(Samanta et al., 2018)</ref>. Compared with these approaches mainly focusing on structure generation, our model takes full use of the power of the generative model for feature representation generation, which can serve as an enhanced technique for the downstream backbone models.</p><p>Concatenation-style Design. In this work, we use concatenation to concatenate the original features and the dif-ferent generated features at each training iteration to enrich the neighborhood information through local augmentation.</p><p>Concatenation-style design is a general technique that many works employ such as GAT <ref type="bibr" target="#b55">(Veliƒçkoviƒá et al., 2018)</ref> and SIGN <ref type="bibr" target="#b41">(Rossi et al., 2020)</ref>. SIGN focuses on the scalable training of GNN models on large graph and augments the feature by multi-hop information through powers of adjacency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We propose local augmentation, a new technique that exploits the generative model to learn the conditional distribution of the central node's neighbors' features given the central node's features. We feed the generated feature matrix from a well-trained generative model to some modified backbone GNN models to enhance their performance. Experiments show that our model can improve performance across various GNN architectures and benchmark datasets. Besides, our model achieves new state-of-the-art results on various semi-supervised node classification tasks. One limitation of our proposed framework is that we do not exploit the 2-hop neighbors or use the random walk to find more related neighbors for the central node. One future work is to extract more 2/3-hop neighbors if the central node's degree is small and learn the conditional distribution for random sampling nodes if the graph is large.</p><formula xml:id="formula_23">= q œï (z|X u , X v ) log p Œ∏ (X u , X v , z) q œï (z|X u , X v )p Œ∏ (X v ) dz = q œï (z|X u , X v ) log p Œ∏ (X u |X v , z)p Œ∏ (X v , z) q œï (z|X u , X v )p Œ∏ (X v ) dz = q œï (z|X u , X v ) log p Œ∏ (X u |X v , z)p Œ∏ (z|X v ) q œï (z|X u , X v ) dz = q œï (z|X u , X v ) log p Œ∏ (z|X v ) q œï (z|X u , X v ) dz + q œï (z|X u , X v ) log p Œ∏ (X u |X v , z)dz = ‚àíKL(q œï (z|X u , X v )||p Œ∏ (z|X v )) + q œï (z|X u , X v ) log p Œ∏ (X u |X v , z)dz = ‚àíKL(q œï (z|X u , X v )‚à•p Œ∏ (z|X v )) + 1 L L l=1 log p Œ∏ (X u |X v , z (l) ) B. Pretraining details B.1. Framework</formula><p>We build CVAE based on MLP. The encode and decoder are two-layer MLP where each layer has 256 hidden units for all the graph datasets. For a node v and its neighbors N v , we extract neighboring-paris (X v , X u ) as input for CVAE during the training stage, where u ‚àà N v . In the inference stage, we extract a latent variable z from N (0, I) and the center node v's feature vector X v as input for the decoder of CVAE. Thus, we can obtain the generated feature vector X v . The detail of CVAE is illustrated in Fig. <ref type="figure">2</ref>.</p><formula xml:id="formula_24">ùúá ùúé ùí©(ùúá, ùúé) Concat(ùëã ! , ùëã " ) Concat(z, ùëã " ) ùëã * ! ùëã ! close</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample Encoder Decoder</head><p>Figure <ref type="figure">2</ref>. A schematic depiction of CVAE. The yellow and brown circles on the graph correspond to the center node and its neighbors respectively. We extract their feature vectors as input for CVAE.</p><p>Algorithm 2 The framework to obtain the Generator Q Œ¶ with active learning trick on Cora, Citeseer, and Pubmed</p><p>Input: Adjacency matrix A, feature matrix X Output: Generator QŒ¶ 1: Initialize U =-inf, QŒ¶, and Q ‚Ä≤ Œ¶ 2: for i = 1 to the number of pre-training epochs do 3: Update the parameters of generator QŒ¶ 4: Generate feature matrix X using QŒ¶ 5: Compute U (X) using Eq.( <ref type="formula" target="#formula_26">10</ref>). 6: if U (X) &gt; U then 7: </p><formula xml:id="formula_25">U = U (X) 8: Q ‚Ä≤ Œ¶ =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Active Learning Trick</head><p>We introduce a trick for the pre-training of CVAE on Cora, Citerseer, and Pubmed. Since the generator may generate some features from the long tail of the distribution. This critical question makes the generation inefficient. Inspired by <ref type="bibr" target="#b37">Nielsen &amp; Okoniewski (2019)</ref>, we introduce active learning to address this issue. During active learning, the probability of each feature is proportional to its uncertainty evaluated by an acquisition function. We adopt the Bayesian Active Learning by Disagreement (BALD) acquisition function <ref type="bibr" target="#b26">(Houlsby et al., 2011)</ref> with the approximation from the Monte Carlo (MC) dropout samples, which is defined as follows:</p><formula xml:id="formula_26">U (X) ‚âà H 1 N N n=1 P Y |X, œâ n ‚àí 1 N N n=1 H P Y |X, œâ n , (<label>10</label></formula><formula xml:id="formula_27">)</formula><p>where N is the number of MC samples and œâ n are the parameters of the network sampled for the n-th MC dropout sample.</p><p>A high BALD score indicates a network with high uncertainty about the generated feature matrix. So it tends to be selected to improve the GNN model. Finally, the CVAE training procedure is summarized in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Connection to Existing works C.1. Connection to EP-B and GraphSAGE</head><p>We discuss how our proposed model distinguishes from the classical graph embedding models. EP-B (Garc√≠a-Dur√°n &amp; Niepert, 2017) and GraphSAGE <ref type="bibr" target="#b23">(Hamilton et al., 2017)</ref> rely on reconstruction loss function between the center node and its neighbors' embeddings. EP-B aims to minimize their defined reconstruction error and make the attribute vector representation reconstructed by the neighbor node from message passing mechanism is as close as possible to the original attribute vector.  GraphSAGE exploits the negative sampling to differentiate the representations of remote node-pairs. GraphSAGE enforce nearby nodes to have similar representations and to enforce disparate nodes to be distinct by minimizing their proposed objective function. The two graph embedding models build upon the assumption that nearby nodes share similar attributes.</p><formula xml:id="formula_28">X !" X !# X $ X !% (a) The Original Graph X !" X !# X $ X !% re c o n st u c t re c o n st u c t X * $ loss (b) EP-B X !" X !# X $ X !% s i m i l a r s i m i l a r dissimilar (c) GraphSAGE X !" X !# X $ X !% i n f e r i n f e r<label>(</label></formula><p>In contrast, our model does not rely on such assumption and generates more feature vectors of the connected neighbors from a well-learned distribution. A comparison between the reconstruction-based representation learning on graphs and our proposed framework is illustrated in Figure <ref type="figure">3</ref>. And our local augmentation method is the third paradigm to exploit neighbors in a generative way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Reproducibility</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Datasets Details</head><p>Cora, Citeseer, and Pubmed are standard citation network benchmark datasets <ref type="bibr" target="#b47">(Sen et al., 2008)</ref>. In these datasets, nodes represent documents, and edges denote citations; node feature corresponds to elements of a bag-of-words representation of a document, and node label corresponds to one of the academic topics. Besides, we utilize six large graph datasets:ogbnproducts, ogbn-proteins, ogbn-arxiv, ogbl-collab, ogbg-molhiv, and ogbg-molpcba from OGB <ref type="bibr" target="#b27">(Hu et al., 2020)</ref> for evaluation.</p><p>The goal of OGB is to support and catalyze research in graph ML. Specifically, ogbn-products is an Amazon products copurchasing network <ref type="bibr" target="#b2">(Bhatia et al., 2016)</ref> originally developed by <ref type="bibr" target="#b5">Chiang et al. (2019)</ref>, where node features are dimensionalityreduced bag-of-words of the product descriptions. The ogbn-arxiv dataset is extracted from the Microsoft Academic Graph (MAG) <ref type="bibr" target="#b59">(Wang et al., 2020b)</ref>, where each node is an ARXIV paper and each directed edge indicates that one paper cites another one and its node representation is a 128-dimensional feature vector obtained by averaging the embeddings of words in its title and abstract. ognb-proteins dataset is a protein-protein association network <ref type="bibr" target="#b53">(Szklarczyk et al., 2019)</ref>, where nodes represent proteins, and edges indicate different types of biologically meaningful associations between proteins. We use edge embedding as node embedding as suggested in PyG <ref type="bibr" target="#b16">(Fey &amp; Lenssen, 2019)</ref> implementation. ogbl-collab is an author collaboration network <ref type="bibr" target="#b59">(Wang et al., 2020b)</ref>, where each node represents an author, edges indicate the collaboration between authors, and the node representation is a 128-dimensional feature vector, obtained by averaging the word embeddings of papers that are published by the authors. The ogbg-molhiv and ogbg-molpcba datasets are two molecular property prediction datasets adopted from <ref type="bibr" target="#b63">Wu et al. (2018)</ref>, where each graph represents a molecule. ogbn-arxiv, ogbn-proteins, ogbn-products are for node classification, ogbl-collab is for link prediction, and ogbg-molhiv and ogbg-molpcba are for graph prediction.</p><p>All the dataset statistics are summarized in Table <ref type="table" target="#tab_10">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Implementation Details</head><p>We use Pytorch <ref type="bibr" target="#b38">(Paszke et al., 2019)</ref>, PyG <ref type="bibr" target="#b16">(Fey &amp; Lenssen, 2019)</ref>, and DGL <ref type="bibr" target="#b60">(Wang et al., 2019)</ref> to implement LA-GNNs.</p><p>The codes of LA-GCN, LA-GAT, LA-GCNII, LA-GRAND on Cora, Citeseer, and Pubmed are implemented referring to Pytorch implementation of GCN<ref type="foot" target="#foot_1">2</ref>  <ref type="bibr" target="#b30">(Kipf &amp; Welling, 2017)</ref>, PyG implementation of GAT<ref type="foot" target="#foot_2">3</ref>  <ref type="bibr" target="#b55">(Veliƒçkoviƒá et al., 2018)</ref>, Pytorch implementation of GCNII<ref type="foot" target="#foot_3">4</ref>  <ref type="bibr" target="#b4">(Chen et al., 2020b)</ref>, and Pytorch implementation of GRAND<ref type="foot" target="#foot_4">5</ref>  <ref type="bibr" target="#b15">(Feng et al., 2020)</ref> respectively. The codes of LA-GCN, LA-GRAND, and LA-GIN on OGB datasets are implemented referring to official OGB implementation<ref type="foot" target="#foot_5">6</ref> . The codes of LA-GAT on ogbn-products and ogbn-arxiv are implemented referring to official OGB implementation<ref type="foot" target="#foot_6">7</ref> and DGL implementation<ref type="foot" target="#foot_7">8</ref> . All the experiments in this work are conducted on a single NVIDIA Tesla V100 with 32GB memory size. The software that we use for experiments are Python 3.6.8, pytorch 1.9.0, pytorch-cluster 1.5.9, pytorch-scatter 2.0.9, pytorch-sparse 0.6.12, pyg 2.0.3, ogb 1.3.2, dgl 0.7.2, numpy 1.19.2, torchvision 0.10.0, CUDA 10.2.89, and CUDNN 7.6.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Hyperparameter Details</head><p>As we have discussed in Sec. 3.3, we provide two design for our LAGNNs -average and concatenation. For concatenationstyle design, LA-GNNs introduce an additional aggregation over our generated feature matrix X before concatenation. The details of two style design architectures of LA-GCN can be found in Figure <ref type="figure" target="#fig_2">4</ref>. Specifically, we use 3 additional generated feature matrix in the first layer for the Pubmed dataset. More details about hyparatemeters can be found in Table <ref type="table" target="#tab_11">10 and 11</ref>.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>QŒ¶ 9: end if 10: end for 11: QŒ¶ = Q ‚Ä≤ Œ¶ 12: Return: Generator QŒ¶</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 3. (a) The original graph. (b) EP-B exploits the neighbors to reconstruct the central node's embedding. (c) GraphSAGE encourages nearby nodes to have similar embeddings. (d) Given the representation of the central node, our aim is to infer the representations of the connected neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Average-style and Concatenation-style LAGCN architectures. The difference between the two architectures is that the concatenation-style LAGCN has an additional convolutional layer for X and it uses a concatenation operation to mix the hidden representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>2000, lr: 0.01, weight-decay: 5e-4, hidden: 8, layers: 2, dropout: 0.5, S: 4, consistency training: True, Œ≤: 1.0 T : 0.5 LAGAT Concatenation 1 epochs: 1000, lr: 0.01, weight-decay: 5e-4, hidden: 8, layers: 2, heads: [4, 1], dropout: 0.6, alpha: 0.2, S: 4, consistency training: True, Œ≤: 1.0 T : 0.5 LAGCNII Concatenation 1 lr: 0.01, L2 c : 0.01, L2 d : 5e-4, layers: 32, hidden: 128, Œ± l : 0.1, Œª: 0.6, dropout: 0.7, S: 4, consistency training: True, Œ≤: 1.0, T : 0.5, early stopping patience: 200 LAGRAND --lr: 0.01, weight-decay: 5e-4, input dropout rate: 0.2, hidden dropout rate: 0.1, dropNode probability: 0, propagation step: 2, hidden: 32, S: 4, consistency training: True, Œ≤: 0.7, T : 0.2, early stopping patience: 200, batch normalization: False Pubmed LAGCN Concatenation 3 epochs: 300, lr: 0.02, weight-decay: 5e-4, hidden: 4, layers: 2, dropout: 0.5, S: 4, consistency training: True, Œ≤: 1.0 T : 0.5 LAGAT Concatenation 3 epochs: 1000, lr: 0.01, weight-decay: 5e-4, hidden: 8, layers: 2, heads: [2, 1], dropout: 0.6, alpha: 0.2, S: 4, consistency training: True, Œ≤: 1.0 T : 0.5 LAGCNII Concatenation 3 lr: 0.01, L2 c : 5e-4, L2 d : 5e-4, layers: 16, hidden: 64, Œ± l : 0.1, Œª: 0.4, dropout: 0.5, S: 4, consistency training: True, Œ≤: 1.0, T : 0.5, early stopping patience: 200 LAGRAND --lr: 0.2, weight-decay: 5e-4, input dropout rate: 0.7, hidden dropout rate: 0.8, dropNode probability: 0.7, propagation step: 5, hidden: 32, S: 4, consistency training: True, Œ≤: 1.2, T : 0.2, early stopping patience: 200, batch normalization: TrueTable 11. The hyper-parameters for each backbone on OGB datasets. 500, lr: 0.01, hidden: 128, layers: 3, dropout: 0.5, S: 2, consistency training: True, Œ≤: 1.0 T : 0.5 LAGAT Concatenation 1 epochs: 2000, lr: 0.01, n-hidden: 125, n-layers: 3, nheads: 3, dropout: 0.75, mask-rate: 0.5, no-attn-dst: True, use-norm: True, use-labels: True, input-drop: 0, attn-drop: 0, edge-drop: 0, wd: 0, S: 2, consistency training: False LASAGE Concatenation 1 epochs: 500, lr: 0.01, hidden: 128, layers: 3, dropout: 0.5, S: 4, consistency training: True, Œ≤: 1.0 T : 0300, lr: 0.01, hidden: 256, layers: 3, dropout: 0.5, S: 1, consistency training: False LAGAT Average -epochs: 100, lr: 0.001, hidden: 128, layers: 3, heads: 4, S: 2, consistency training: False LASAGE Average -epochs: 20, lr: 0.001, hidden: 128, layers: 3, dropout: 0.5, heads: 4, S: 2, consistency training: True, Œ≤: 400, lr: 0.001, hidden: 128, layers: 3, dropout: 0.0, S: 1, consistency training: False LASAGE Concatenation 1 epochs: 400, lr: 0.001, hidden: 128, layers: 3, dropout: 0.0, S: 1, consistency training: False ogbg-molhiv LAGCN Average -epochs: 100, lr: 0.001, hidden: 300, layers: 5, dropout: 0.0, S: 1, consistency training: False, batch size: 32 LAGIN Average -epochs: 100, lr: 0.001, hidden: 300, layers: 5, dropout: 0.0, S: 1, consistency training: False, batch size: 32 ogbg-molpcba LAGCN Average -epochs: 100, lr: 0.001, hidden: 300, layers: 5, dropout: 0.0, S: 1, consistency training: False, batch size: 32 LAGIN Average -epochs: 100, lr: 0.001, hidden: 300, layers: 5, dropout: 0.0, S: 1, consistency training: False, batch size: 32</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Training and Inference. The details of our training and inference process are outlined in Algorithm 1. First, we train CVAE i.e. our local augmentation model. And then we sample a different feature matrix generated by CVAE as Algorithm 1 Local Augmentation for Graph Neural Networks 1: Input: Adjacency matrix A, feature matrix X 2: Output: Prediction Z 3: Pre-train the generative model Q Œ¶ using Eq. (2), given A and X as input. 4: while not convergence do</figDesc><table><row><cell>5:</cell><cell>for s = 1 : S do</cell></row><row><cell>6:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Classification results on three citation networks (%)</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>Chebyshev</cell><cell>81.2</cell><cell>69.8</cell><cell>74.4</cell></row><row><cell>APPNP</cell><cell cols="3">83.8¬±0.3 71.6¬±0.5 79.7¬±0.3</cell></row><row><cell>MixHop</cell><cell cols="3">81.9¬±0.4 71.4¬±0.8 80.8¬±0.6</cell></row><row><cell>Graph U-net</cell><cell cols="3">84.4¬±0.6 73.2¬±0.5 79.6¬±0.2</cell></row><row><cell>GSNN-M</cell><cell cols="3">83.9¬±0.5 72.2¬±0.5 79.1¬±0.3</cell></row><row><cell>S 2 GC</cell><cell cols="3">83.5¬±0.02 73.6¬±0.09 80.2¬±0.02</cell></row><row><cell>GCN</cell><cell cols="3">81.5¬±0.5 70.3¬±0.7 79.0¬±0.5</cell></row><row><cell>G-GCN</cell><cell>83.7</cell><cell>71.3</cell><cell>80.9</cell></row><row><cell>DropEdge-GCN</cell><cell>82.8</cell><cell>72.3</cell><cell>79.6</cell></row><row><cell>GAUG-O-GCN</cell><cell cols="3">83.6¬±0.5 73.3¬±1.1 79.3¬±0.4</cell></row><row><cell cols="4">GraphSNN GCN 83.1¬±1.8 72.3¬±1.5 79.8¬±1.2</cell></row><row><cell>GRAND-GCN</cell><cell cols="3">84.5¬±0.3 74.2¬±0.3 80.0¬±0.3</cell></row><row><cell>LA-GCN</cell><cell cols="3">84.6¬±0.5 74.7¬±0.5 81.7¬±0.7</cell></row><row><cell>GAT</cell><cell cols="3">83.0¬±0.7 72.5¬±0.7 79.0¬±0.3</cell></row><row><cell>GAUG-O-GAT</cell><cell cols="2">82.2¬±0.2 71.6¬±1.1</cell><cell>OOM</cell></row><row><cell>GraphSNN GAT</cell><cell cols="3">83.8¬±1.2 73.5¬±1.6 79.6¬±1.4</cell></row><row><cell>GRAND-GAT</cell><cell cols="3">84.3¬±0.4 73.2¬±0.4 79.2¬±0.6</cell></row><row><cell>LA-GAT</cell><cell cols="3">84.7¬±0.4 73.7¬±0.5 81.0¬±0.4</cell></row><row><cell>GCNII</cell><cell cols="3">85.5¬±0.5 73.4¬±0.6 80.2¬±0.4</cell></row><row><cell>LA-GCNII</cell><cell cols="3">85.7¬±0.3 74.1¬±0.5 80.6¬±0.7</cell></row><row><cell>GRAND</cell><cell cols="3">85.4¬±0.4 75.4¬±0.4 82.7¬±0.6</cell></row><row><cell>LA-GRAND</cell><cell cols="3">85.7¬±0.3 75.8¬±0.5 83.4¬±0.6</cell></row><row><cell>4. Experiments</cell><cell></cell><cell></cell><cell></cell></row></table><note>In this section, we evaluate the performance of our local augmentation model on various tasks including node classification, link prediction, and graph classification. All the experiments are conducted on open graph datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Test performance (%) averaged over 10 runs on node property prediction. Blank denotes no statistics on the leaderboard or in the paper.</figDesc><table><row><cell></cell><cell cols="3">products proteins arxiv</cell></row><row><cell>Model</cell><cell>Acc</cell><cell>ROC-AUC</cell><cell>Acc</cell></row><row><cell>MLP</cell><cell cols="3">61.06¬±0.08 72.04¬±0.48 55.50¬±0.23</cell></row><row><cell cols="2">CoLinkDistMLP 62.59¬±0.10</cell><cell>-</cell><cell>56.38¬±0.16</cell></row><row><cell>Node2vec</cell><cell cols="3">72.49¬±0.10 68.81¬±0.65 70.07¬±0.13</cell></row><row><cell>GraphZoom</cell><cell>74.06¬±0.26</cell><cell>-</cell><cell>71.18¬±0.18</cell></row><row><cell>GCN</cell><cell cols="3">75.64¬±0.21 72.51¬±0.35 71.74¬±0.29</cell></row><row><cell>+FLAG</cell><cell>-</cell><cell cols="2">71.71¬±0.50 72.04¬±0.20</cell></row><row><cell>+GraphSNN</cell><cell>-</cell><cell>-</cell><cell>72.20¬±0.90</cell></row><row><cell>+LA</cell><cell cols="3">76.11¬±0.09 73.25¬±0.51 72.08¬±0.14</cell></row><row><cell>GraphSAGE</cell><cell cols="3">78.70¬±0.36 77.68¬±0.20 71.49¬±0.27</cell></row><row><cell>+FLAG</cell><cell cols="3">79.36¬±0.57 76.57¬±0.75 72.19¬±0.21</cell></row><row><cell>+GraphSNN</cell><cell>-</cell><cell>-</cell><cell>71.80¬±0.70</cell></row><row><cell>+LA</cell><cell cols="3">79.44¬±0.25 77.86¬±0.37 72.30¬±0.12</cell></row><row><cell>GAT</cell><cell>79.45¬±0.59</cell><cell>-</cell><cell>73.65¬±0.11</cell></row><row><cell>+FLAG</cell><cell>81.76¬±0.45</cell><cell>-</cell><cell>73.71¬±0.13</cell></row><row><cell>+LA</cell><cell>80.46¬±0.54</cell><cell>-</cell><cell>73.77¬±0.12</cell></row></table><note>of the subgraph. Results show that local augmentation is more effective than GraphSNN in capturing the feature information of the local neighborhood, which demonstrates that it's better to consider feature and structure information in designing subgraph-related GNNs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Test performance (%) averaged over 10 runs on link prediction.</figDesc><table><row><cell></cell><cell></cell><cell>ogbl-collab</cell></row><row><cell cols="2">Model</cell><cell>Hits@50 (%)</cell></row><row><cell>MLP</cell><cell></cell><cell>19.27¬±1.29</cell></row><row><cell cols="2">Node2vec</cell><cell>48.88¬±0.54</cell></row><row><cell>GCN</cell><cell></cell><cell>44.75¬±1.07</cell></row><row><cell>+LA</cell><cell></cell><cell>47.49¬±1.40</cell></row><row><cell cols="2">GraphSAGE</cell><cell>48.10¬±0.81</cell></row><row><cell>+LA</cell><cell></cell><cell>49.23¬±0.55</cell></row><row><cell cols="3">Table 4. Test performance (%) averaged over 10 runs on graph</cell></row><row><cell cols="2">property prediction.</cell><cell></cell></row><row><cell cols="3">ogbg-molhiv ogbg-molpcba</cell></row><row><cell>Model</cell><cell>ROC-AUC</cell><cell>AP</cell></row><row><cell>GCN</cell><cell>76.06¬±0.97</cell><cell>20.20¬±0.24</cell></row><row><cell>+LA</cell><cell>76.18¬±1.11</cell><cell>20.28¬±0.16</cell></row><row><cell>GIN</cell><cell>75.58¬±1.40</cell><cell>22.66¬±0.28</cell></row><row><cell>+LA</cell><cell>75.20¬±1.74</cell><cell>22.38¬±0.24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Effects of different components of our LAGCN evaluated on Pubmed dataset.</figDesc><table><row><cell>Technique</cell><cell cols="3">Accuracy (%) ‚àÜ Cumu ‚àÜ</cell></row><row><cell>GCN</cell><cell>79.0</cell><cell>0</cell><cell>0</cell></row><row><cell>+ Concatenation</cell><cell>79.3¬±0.4</cell><cell>0.3</cell><cell>0.3</cell></row><row><cell>+ Local Augmentation</cell><cell>81.1¬±0.5</cell><cell>1.8</cell><cell>2.1</cell></row><row><cell>+ Consistency Training</cell><cell>81.4¬±0.5</cell><cell>0.3</cell><cell>2.4</cell></row><row><cell>+ Sharpening Trick</cell><cell>81.7¬±0.7</cell><cell>0.3</cell><cell>2.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Summary of results of GCN evaluated on Citeseer on recovering study in terms of classification accuracy (%). ‚Üì means a decrease compared with the accuracy if features are not masked.</figDesc><table><row><cell>Mask Ratio</cell><cell>0.1</cell><cell>0.2</cell><cell>0.4</cell><cell>0.8</cell></row><row><cell>GCN</cell><cell cols="4">70.4(‚Üë0.1) 69.2(‚Üì1.1) 67.2(‚Üì3.1) 61.1(‚Üì9.2)</cell></row><row><cell>LAGCN</cell><cell cols="4">73.8(‚Üì0.9) 74.0(‚Üì0.7) 71.8(‚Üì2.9) 68.7(‚Üì6.0)</cell></row></table><note>experimental results show that our model still works for GCN on inductive learning tasks. Our generative model is only trained on the training dataset. As long as the graphs on the test dataset and the training dataset have similar distributions, i.e., similar subgraph structures and feature vectors, our generative model can make reasonable inferences and generate effective augmented feature vectors.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Summary of results of GCN and LAGCN evaluated on Pubmed on case study in terms of classification accuracy (%).</figDesc><table><row><cell cols="2">Node degree is computed on D ‚àí 1 2 A D ‚àí 1 2 .</cell><cell></cell></row><row><cell>Degree</cell><cell>[2, 5]</cell><cell>[6, 20]</cell></row><row><cell>#Nodes</cell><cell>761</cell><cell>189</cell></row><row><cell>GCN</cell><cell>78.2</cell><cell>82.0</cell></row><row><cell>LAGCN</cell><cell>79.9</cell><cell>82.2</cell></row><row><cell>‚àÜ</cell><cell>1.7</cell><cell>0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>The MADgap metric of LAGCN v.s. GCN of 10 runs on Cora (on different layers).</figDesc><table><row><cell>Layer</cell><cell>GCN</cell><cell>LAGCN</cell></row><row><cell>Layer2</cell><cell>0.63¬±0.02</cell><cell>0.68¬±0.02</cell></row><row><cell>Layer3</cell><cell>0.61¬±0.08</cell><cell>0.61¬±0.07</cell></row><row><cell>Layer4</cell><cell>0.55¬±0.06</cell><cell>0.64¬±0.05</cell></row><row><cell>Layer5</cell><cell>0.39¬±0.23</cell><cell>0.61¬±0.06</cell></row><row><cell>Layer6</cell><cell>0.24¬±0.47</cell><cell>0.31¬±0.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>Data statistics</figDesc><table><row><cell>Category</cell><cell>Name</cell><cell>#Graphs</cell><cell>Average #Nodes</cell><cell>Average #Edges</cell><cell cols="2">#Features #Classes</cell><cell>Split Ratio</cell><cell>Task Type</cell><cell>Metric</cell></row><row><cell>Node citation-</cell><cell>cora citeseer</cell><cell>1 1</cell><cell>2,708 3,327</cell><cell>5,429 4,732</cell><cell>1,433 3,703</cell><cell>7 6</cell><cell cols="3">8.5/30.5/61 7.4/30.9/61.7 Multi-class class. Accuracy Multi-class class. Accuracy</cell></row><row><cell></cell><cell>pubmed</cell><cell>1</cell><cell>19,717</cell><cell>44,338</cell><cell>500</cell><cell>3</cell><cell cols="3">3.8/32.1/64.1 Multi-class class. Accuracy</cell></row><row><cell>Node ogbn-</cell><cell>products proteins</cell><cell>1 1</cell><cell cols="2">2,449,029 61,859,140 132,534 39,561,252</cell><cell>100 8</cell><cell>47 2</cell><cell>8/2/90 65/16/19</cell><cell cols="2">Multi-class class. Accuracy Binary class. ROC-AUC</cell></row><row><cell></cell><cell>arxiv</cell><cell>1</cell><cell>169,343</cell><cell>1,166,243</cell><cell>128</cell><cell>40</cell><cell>54/18/28</cell><cell cols="2">Multi-class class. Accuracy</cell></row><row><cell>Link ogbl-</cell><cell>collab</cell><cell>1</cell><cell>235,868</cell><cell>1,285,465</cell><cell>128</cell><cell>-</cell><cell>92/4/4</cell><cell>Link prediction</cell><cell>Hits@50</cell></row><row><cell>Graph</cell><cell>molhiv</cell><cell>41,127</cell><cell>25.5</cell><cell>27.5</cell><cell>9</cell><cell>2</cell><cell>80/10/10</cell><cell>Binary class.</cell><cell>ROC-AUC</cell></row><row><cell>ogbg-</cell><cell>molpcba</cell><cell>437,929</cell><cell>26.0</cell><cell>28.1</cell><cell>9</cell><cell>2</cell><cell>80/10/10</cell><cell>Binary class.</cell><cell>AP</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 .</head><label>10</label><figDesc>The hyper-parameters for each backbone on three citation datasets. L2 c : 0.01, L2 d : 5e-4, layers: 64, hidden: 32, Œ± l : 0.1, Œª: 0.5, dropout: 0.6, S: 4, consistency training: False, early stopping patience: 200 LAGRAND --lr: 0.01, weight-decay: 5e-4, input dropout rate: 0.5, hidden dropout rate: 0.5, dropNode probability: 0.5, propagation step: 8, hidden: 32, S: 4, consistency training: True, Œ≤: 1.0, T : 0.5, early stopping patience: 200, batch normalization: False</figDesc><table><row><cell>Dataset</cell><cell>Backbone</cell><cell>Architecture</cell><cell>Additional aggregation Hyper-parameters</cell></row><row><cell></cell><cell>LAGCN</cell><cell>Concatenation</cell><cell>1 epochs: 2000, lr: 0.01, weight-decay: 5e-4, hidden:</cell></row><row><cell></cell><cell></cell><cell></cell><cell>8, layers: 2, dropout: 0.5, S: 4, consistency training:</cell></row><row><cell></cell><cell></cell><cell></cell><cell>True, Œ≤: 1.0 T : 0.5</cell></row><row><cell>Cora</cell><cell>LAGAT</cell><cell>Concatenation</cell><cell>1 epochs: 1000, lr: 0.01, weight-decay: 5e-4, hidden: 8, layers: 2, heads: [4, 1], dropout: 0.6, alpha: 0.2, S: 4,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>consistency training: True, Œ≤: 1.0 T : 0.5</cell></row><row><cell></cell><cell>LAGCNII</cell><cell>Concatenation</cell><cell>1 lr: 0.01,</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://ogb.stanford.edu/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/tkipf/pygcn</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/pyg-team/pytorch geometric/blob/master/examples/gat.py</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/chennnM/GCNII</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://github.com/THUDM/GRAND</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://github.com/snap-stanford/ogb/blob/master/examples</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://github.com/pyg-team/pytorch geometric/blob/master/examples/ogbn products gat.py</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">https://github.com/dmlc/dgl/blob/master/examples/pytorch/ogb/ogbn-arxiv/gat.py</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank all the reviewers for their helpful comments and suggestions, the support of Tencent AI Lab and the helpful discussion from Yulai Cong. Specially, Songtao Liu is also thankful for the encouragement from Hao Yin. Liu and Wu were supported partially by an NSF grant CNS-1652790 and a seed grant from Penn State Center for Security Research and Education (CSRE).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName><surname>Mixhop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><surname>Mixmatch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<title level="m">A holistic approach to semi-supervised learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The extreme classification repository: Multi-label datasets and code</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dahiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<ptr target="http://manikvarma.org/downloads/XC/XMLRepository.html" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning augmentation strategies from data</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Autoaugment</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Retrosynthesis prediction with conditional graph logic network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graphzoom: A multi-level spectral approach for accurate and scalable graph embedding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Batch virtual adversarial training for graph convolutional networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09192</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning the compositional domains for generalized zero-shot learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">103454</biblScope>
		</imprint>
		<respStmt>
			<orgName>Computer Vision and Image Understanding</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data augmentation for low-resource neural machine translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fadaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bisazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mathematical models of overparameterized neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
				<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="683" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph adversarial training: Dynamically regularizing based on graph structure</title>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph random neural network for semi-supervised learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning graph representations with embedding propagation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Garc√≠a-Dur√°n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention based spatial-temporal graph convolutional networks for traffic flow forecasting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Contrastive multiview representation learning on graphs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Khasahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Independence, invariance and the causal markov condition. The British journal for the philosophy of science</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Hausman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Woodward</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bayesian active learning for classification and preference learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Husz√°r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lengyel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1112.5745</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G√ºnnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><surname>Flag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09891</idno>
		<title level="m">Adversarial data augmentation for graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Training graph neural networks with 1000 layers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>M√ºller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Constrained graph variational autoencoders for molecule design</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Distilling self-knowledge from contrastive links to classify graph nodes without passing messages</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08541</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gan data augmentation through active learning inspired sample acquisition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Okoniewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName><surname>Deepwalk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11198</idno>
		<title level="m">Sign: Scalable inception graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Data augmentation via dependency tree morphing for low-resource languages</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>S ¬∏ahin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.09460</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Keep it simple: Graph autoencoders without graph convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hennequin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Designing random graph models using variational autoencoders with applications to chemical design</title>
		<author>
			<persName><forename type="first">B</forename><surname>Samanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05283</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Samuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Timo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>AI magazine</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A graph to graphs framework for retrosynthesis prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graphvae: Towards generation of small graphs using variational autoencoders</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization</title>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">String v11: protein-protein association networks with increased coverage, supporting functional discovery in genome-wide experimental datasets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Szklarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Gable</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Junge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huerta-Cepas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simonovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">T</forename><surname>Doncheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D607" to="D613" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
				<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veliƒçkoviƒá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li√≤</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03825</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Graph stochastic neural networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Microsoft academic graph: When experts are not enough</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quantitative Science Studies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="396" to="413" />
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Deep graph library: A graphcentric, highly-performant package for graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Nodeaug: Semi-supervised node classification with data augmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020c</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A new perspective on &quot;how graph neural networks go beyond weisfeiler-lehman?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wijesinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Generalized data augmentation for low-resource translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Generative graph convolutional network for growing graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Korpeoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Achan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-I</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Conditional structure generation through graph variational generative adversarial nets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Decoupling the depth and scope of graph neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Malevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prasanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Nested graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Data augmentation using learned transformations for one-shot medical image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Data augmentation for graph neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Pre-train and learn: Preserve global information for graph neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Simple spectral graph convolution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
