<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WARP: Word-level Adversarial ReProgramming</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-01-01">1 Jan 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Karen</forename><surname>Hambardzumyan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">YerevaNN</orgName>
								<address>
									<settlement>Yerevan</settlement>
									<country key="AM">Armenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hrant</forename><surname>Khachatrian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">YerevaNN</orgName>
								<address>
									<settlement>Yerevan</settlement>
									<country key="AM">Armenia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Yerevan State University</orgName>
								<address>
									<settlement>Yerevan</settlement>
									<country key="AM">Armenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">USC Information Sciences Institute</orgName>
								<address>
									<addrLine>Marina Del Rey</addrLine>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WARP: Word-level Adversarial ReProgramming</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-01">1 Jan 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2101.00121v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transfer learning from pretrained language models recently became the dominant approach for solving many NLP tasks. While fine-tuning large language models usually gives the best performance, in many applications it is preferable to tune much smaller sets of parameters, so that the majority of parameters can be shared across multiple tasks. The main approach is to train one or more task-specific layers on top of the language model. In this paper we present an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation. It attempts to learn taskspecific word embeddings that, when concatenated to the input text, instruct the language model to solve the specified task. We show that this approach outperforms other methods with a similar number of trainable parameters on SST-2 and MNLI datasets. On SST-2, the performance of our model is comparable to the fully fine-tuned baseline, while on MNLI it is the best among the methods that do not modify the parameters of the body of the language model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Language model pretraining has had a tremendous impact on solving many natural language processing tasks <ref type="bibr" target="#b6">(Peters et al., 2018;</ref><ref type="bibr" target="#b8">Radford, 2018;</ref><ref type="bibr" target="#b1">Devlin et al., 2019;</ref><ref type="bibr" target="#b5">Liu et al., 2019)</ref>. The most popular two approaches take a pretrained model and use a straightforward supervised learning objective. In the first approach, the parameters of the language model are frozen and a task-specific head is trained on top of them <ref type="bibr" target="#b6">(Peters et al., 2018)</ref>. The second approach fine-tunes all model parameters <ref type="bibr" target="#b8">(Radford, 2018)</ref>. The latter can sometimes yield better results <ref type="bibr" target="#b6">(Peters et al., 2019)</ref>, while the first one usually offers better stability for smaller datasets. The approach based on frozen features does not require storing task-specific language models.</p><p>A recent alternative is based on so called adapters <ref type="bibr" target="#b4">(Houlsby et al., 2019;</ref><ref type="bibr" target="#b7">Pfeiffer et al., 2020)</ref>, a technique that adds new weights at every layer of the pretrained language model while the original parameters are kept frozen. This enables a smaller set of task-specific parameters while achieving results comparable to the fine-tuning approach.</p><p>Another approach of leveraging pretrained language models for downstream tasks has been introduced in GPT-2 <ref type="bibr" target="#b9">(Radford et al., 2019)</ref> through providing "task descriptions" without using any labeled examples. GPT-3 <ref type="bibr" target="#b0">(Brown et al., 2020)</ref> demonstrated an impressive few-shot learning performance with priming: by providing the language model a few inputs and outputs ("analogies") as a context. The language model "learns" from these examples and outputs the answer with a single forward pass without any trainable parameters. These methods, however, require huge language models <ref type="bibr">(1.5B and 175B parameters, respectively)</ref>.</p><p>The success of task-reformulation based approaches suggest that language models are capable of solving various natural language processing tasks given a well-crafted prompt. We hypothesize that it is possible to find prompts -extra tokens that have to be added to the input, that can exploit language model capabilities better than the manuallydesigned ones.</p><p>In this paper, we introduce a novel technique to find the best prompt that can be added to the input to make a language model output the desired answer. We call our method WARP -Word-level Adversarial RePrograming. The method is inspired by Adversarial <ref type="bibr">Reprogramming (Elsayed et al., 2019)</ref>-a method of adding adversarial perturbations to the input image that reprograms the pretrained neural network to perform classification on a task other than the one it was originally trained for.</p><p>We evaluate on sentence classification and natural language inference (NLI) tasks and show that our method outperforms the frozen-features approach on SST-2 <ref type="bibr" target="#b12">(Socher et al., 2013)</ref> and MultiNLI <ref type="bibr" target="#b13">(Williams et al., 2018)</ref> tasks and has comparable performance with fine-tuning on SST-2. We also compare with AutoPrompt <ref type="bibr" target="#b11">(Shin et al., 2020)</ref>, another method that automatically constructs prompts, and show significant improvement on top of that for SST-2.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Towards Fewer Trainable Parameters</head><p>Jiao et al. <ref type="bibr">(2020)</ref> show that knowledge distillation may help reduce the size of their model 7.5 times while almost preserving the performance, but finetuning such models still requires storage of separate task-specific models. As seen in Section 5, this approach does not scale when we want to apply it to many tasks at once. Another approach, called Adapters <ref type="bibr" target="#b4">(Houlsby et al., 2019;</ref><ref type="bibr" target="#b7">Pfeiffer et al., 2020)</ref>, introduces new task-specific parameters that are added at every layer of the Transformer network. Only these newly initialized weights are trained, which allows separation of general and task-specific knowledge. In contrast, our method does not inject task-specific knowledge inside the pretrained language model. Instead it focuses on learning task-specific input-level prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Task Reformulation</head><p>In GPT-2, <ref type="bibr" target="#b9">Radford et al. (2019)</ref> introduced a completely unsupervised way for transferring knowledge to downstream tasks by reformulating various natural language understanding tasks into language modeling problems. This approach does not make use of the available training examples. Later, <ref type="bibr" target="#b0">Brown et al. (2020)</ref> demonstrate an effective fewshot transfer by reformulating downstream tasks into input-output analogies in the context without a need for further fine-tuning. Nonetheless, the number of training examples is limited to the context size and is not scalable to a traditional supervised learning scenario. <ref type="bibr" target="#b10">Schick and Schutze (2020)</ref> show the effectiveness of reformulating a number of tasks into Clozestyle tasks by fine-tuning masked language models <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref>. The method, called Pattern Exploited Training (PET), additionally uses training samples and performs few-shot learning even without huge models such as GPT-3.</p><p>Our method is also based on masked language models, but unlike PET, we focus on finding the best prompt from scratch using the training examples. This eliminates the need for manuallydesigned prompts. Our method can also benefit from similar prior knowledge about the task by careful initialization of the prompts, although our initial experiments did not demonstrate any substantial improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Adversarial Reprogramming</head><p>Adversarial Reprogramming <ref type="bibr" target="#b3">(Elsayed et al., 2019)</ref> demonstrates the reprogramming of pretrained Im-ageNet classifiers by adding input-level adversarial perturbations to make them perform well on MNIST and CIFAR-10 image classification tasks. The adversarial perturbation is designed to be image padding added to the original input. Then the perturbation parameter is trained to optimize the target classification task objective using the annotated image data. While in case of image classification it is not obvious why adversarial reprogramming should ever work, e.g. why the network trained on ImageNet should have the capacity to solve MNIST, in NLP tasks it is more intuitive, as many NLP tasks can have reformulations in language models.</p><p>More recently, AutoPrompt <ref type="bibr" target="#b11">(Shin et al., 2020</ref>) attempts to find prompts automatically without adding any parameters to the model. Unlike Auto-Prompt, we perform gradient-based optimization in the space of word embeddings which gives our model more degrees of freedom and eventually better performance on the downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">WARP</head><p>We follow a setup similar to Elsayed et al. ( <ref type="formula">2019</ref>) with slight NLP-specific modifications. Our goal is to find the best prompt that will make a pretrained masked language model predict the desired answer for a training example's masked token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Method</head><p>Similar to Elsayed et al. ( <ref type="formula">2019</ref>), we employ stochastic gradient descent to find the best adversarial perturbation on the text that will minimize the task objective. First, we insert special prompt tokens [P 1], [P 2], ... [P K] and an additional [MASK] token into the input sequence. These tokens might be placed before or after the sentences, depending on the prompt template.</p><p>We set the optimization objective to a crossentropy loss between the head output of the masked language model and the special tokens</p><formula xml:id="formula_0">[V 1], [V 2], ..., [V C] for classes 1...C ac- cordingly.</formula><p>The only trainable parameters are the word embeddings for [P 1], ..., [P K] and</p><formula xml:id="formula_1">[V 1], ... [V C].</formula><p>In case we want to train models for multiple tasks, these are the only task-specific parameters we need to store. The entire "body" of the large language model (all attention layers) remains untouched. Note that, unlike most adversarial attacks, we do not update the embeddings of the original tokens of the input. This follows the intuition from Elsayed et al. ( <ref type="formula">2019</ref>), when the pixels of MNIST or CIFAR images are left untouched, and only padding pixels are updated.</p><p>We train these parameters by minimizing the loss on the training set of the downstream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We use the roberta-large <ref type="bibr" target="#b5">(Liu et al., 2019)</ref> model from the huggingface transformers <ref type="bibr" target="#b14">(Wolf et al., 2020)</ref> library.</p><p>Both the prompt and answer special tokens are initialized similar to the vectors from the word embedding layer.</p><p>For the answer prompts, we use the masked language model head, which usually consists of a feed-forward network and a decoder on top of it, where the weights of the decoder are shared with the token embeddings used for the input. We calculate the softmax over the tokens</p><formula xml:id="formula_2">[V 1], ... [V C].</formula><p>We choose the Adam optimizer with no weight decay, and a slanted triangular schedule for the learning rate with 6% warm-up steps. We train for 10 epochs on each task. Each batch consists of examples containing at most 1024 tokens. In order to speedup the training, we disable the dropout of the pretrained language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We test our method on sentiment classification and natural language inference tasks. We use the 2class version of the Stanford Sentiment Treebank (SST-2) and MultiNLI datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SST-2</head><p>SST-2 is the binary sentence classification version of the Stanford Sentiment Treebank <ref type="bibr" target="#b12">(Socher et al., 2013)</ref> consisting of short movie reviews from Rotten Tomatoes and human-annotation sentiment labels. The task is also part of the GLUE benchmark. We use standard dev/test splits. We test multiple configurations of prompt templates for WARP and report the scores in Table <ref type="table" target="#tab_0">1</ref>. The prompt template for the WARP K model is as follows:</p><formula xml:id="formula_3">[P 1] [P 2] ... [P K] [MASK]</formula><p>. This sequence is concatenated to the input tokens from the left side.</p><p>With only 6K trainable parameters, the performance of WARP is less than the fine-tuned baseline by only 0.4 percentage points. WARP significantly outperforms AutoPrompt and has similar performance with Adapters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MultiNLI</head><p>The Multi-Genre Natural Language Inference (MultiNLI / MNLI) is a corpus of sentence pairs with human-annotated textual entailment information <ref type="bibr" target="#b13">(Williams et al., 2018)</ref>. The task is part of the GLUE benchmark. As MultiNLI is formulated as sentence pair classification with 3 classes, we need to apply minimal adjustments.</p><p>WARP allows prompt templates, where prompt and mask tokens can be placed before, after and between the sentences. In Table <ref type="table" target="#tab_1">2</ref>  As seen in Table <ref type="table" target="#tab_1">2</ref>, Adapters and the fully finetuned baseline outperform WARP, but require 300 and 3 • 10 4 more trainable parameters, respectively. Models that keep RoBERTa weights frozen, or train only the last layer, perform much worse. In general, NLI tasks are known to be hard for non fine-tuned language models. Even GPT-3 failed to beat the BERT-base baseline on another NLI dataset <ref type="bibr" target="#b0">(Brown et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>The importance of NLP systems like WARP can be demonstrated by the following application. Suppose we want to build a system that needs to serve N &gt;&gt; 1 classification tasks simultaneously. Let the number of classes for each task be bounded by C. The system can be based on a large pretrained language model with M parameters, using subword embedding size E. How many parameters should the system store in memory to be able to serve all N tasks?</p><p>If we take the approach with frozen features, we can reuse M parameters for all tasks and store additional ECN task-specific parameters. This is optimal in terms of storage but will not perform well. The other extreme is to fine-tune the whole model for each task and store at least M N parameters. Table <ref type="table">3</ref> shows the trade-offs offered by the other solutions. Methods like TinyBERT decrease the number of parameters from M N by only M . WARP, on the other hand, needs to store only M + N E(C + K) parameters, where K is the number of trainable prompt tokens.</p><p>In practice, WARP additionally allows performing inference on inputs for different tasks in parallel, using samples of multiple tasks in the same</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p># of parameters to store Linear probing M + ECN Full fine-tuning M N Single layer</p><formula xml:id="formula_4">M + N E(E + C) TinyBERT M 0 N Adapters M + N EE ′ WARP M + N E(C + K)</formula><p>Table <ref type="table">3</ref>: The number of parameters to be stored to serve N text classification tasks with at most C classes each, using a pretrained language model with M parameters. E is the dimension of embeddings (1024 in case of RoBERTa). In TinyBERT, M 0 can be up to 10 times less than M . In Adapters, E ′ is roughly equal to E, as the number of layers to which adapters are attached roughly compensates the smaller size of the bottleneck layer. In WARP, K is the number of prompts (usually less than 10).</p><p>batch. Every input sentence can be concatenated with task-specific pretrained prompts in advance.</p><p>Then, the forward pass of the network is identical for all tasks. The final task-specific linear layers can be concatenated to form a single large linear layer with at most N C output neurons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we have proposed an alternative way to transfer knowledge from large pretrained language models to downstream tasks by appending carefully optimized embeddings to the input text.</p><p>The method outperforms existing methods with a similar number of trainable parameters on two tasks. On the sentiment analysis task the performance is comparable to the fully fine-tuned language models. We plan to extend this approach to work in few-shot and cross-lingual transfer settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy on SST-2 development and test sets. The last column shows the number of trainable parameters only. All methods use RoBERTa-large if not stated otherwise. WARP K corresponds to a prompt consisting of K tokens. The subscript next to TinyBERT corresponds to the number of layers in the model.</figDesc><table><row><cell></cell><cell cols="3">Dev Test # of parameters</cell></row><row><cell>BERT-base</cell><cell cols="2">92.7 93.5</cell><cell>110 • 10 6</cell></row><row><cell>TinyBERT 6</cell><cell cols="2">93.0 93.1</cell><cell>67 • 10 6</cell></row><row><cell>TinyBERT 4</cell><cell cols="2">92.6 92.6</cell><cell>15 • 10 6</cell></row><row><cell>Fine-tuned</cell><cell cols="2">96.4 96.7</cell><cell>335 • 10 6</cell></row><row><cell>Adapters</cell><cell>96.3</cell><cell>-</cell><cell>3 • 10 6</cell></row><row><cell>Single layer</cell><cell cols="2">88.4 89.2</cell><cell>10 6</cell></row><row><cell cols="3">Linear probing 87.9 88.8</cell><cell>2048</cell></row><row><cell>AutoPrompt</cell><cell cols="2">91.2 91.4</cell><cell>-</cell></row><row><cell>WARP 1</cell><cell cols="2">92.1 93.5</cell><cell>3072</cell></row><row><cell>WARP 2</cell><cell cols="2">92.2 93.0</cell><cell>4096</cell></row><row><cell>WARP 3</cell><cell cols="2">94.8 95.7</cell><cell>5120</cell></row><row><cell>WARP 4</cell><cell cols="2">95.7 96.3</cell><cell>6144</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>for our WARP experiments we use a prompt template added be-The accuracy on MultiNLI development and test sets, and the number of trainable parameters.</figDesc><table><row><cell>Approach</cell><cell cols="4">Dev Matched Mismatched Matched Mismatched Test</cell><cell># of parameters</cell></row><row><cell>BERT-base</cell><cell>84.4</cell><cell>−</cell><cell>84.6</cell><cell>83.4</cell><cell>110 • 10 6</cell></row><row><cell>TinyBERT 6</cell><cell>84.5</cell><cell>84.5</cell><cell>84.6</cell><cell>83.2</cell><cell>67 • 10 6</cell></row><row><cell>TinyBERT 4</cell><cell>82.8</cell><cell>82.9</cell><cell>82.5</cell><cell>81.8</cell><cell>15 • 10 6</cell></row><row><cell>Fine-tuned</cell><cell>90.2</cell><cell>90.2</cell><cell>90.8</cell><cell>90.2</cell><cell>335 • 10 6</cell></row><row><cell>Adapters</cell><cell>90.4</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>3 • 10 6</cell></row><row><cell>Single layer</cell><cell>63.1</cell><cell>64.6</cell><cell>−</cell><cell>−</cell><cell>10 6</cell></row><row><cell>Linear probing</cell><cell>59.1</cell><cell>60.0</cell><cell>−</cell><cell>−</cell><cell>3048</cell></row><row><cell>WARP 6</cell><cell>87.2</cell><cell>87.3</cell><cell>86.8</cell><cell>87.2</cell><cell>9216</cell></row><row><cell cols="3">tween the premise and hypothesis, consisting of</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">[P 1], [P 2], [P 3], [P 4], [MASK], [P 5], [P 6].</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<idno>ArXiv, abs/2005.14165</idno>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and Dario Amodei</title>
				<meeting><address><addrLine>Scott Gray, Benjamin Chess, J. Clark, Christopher Berner, Sam McCandlish, A. Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<title level="m">Kenton Lee, and Kristina Toutanova</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pre-training of deep bidirectional transformers for language understanding</title>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
		<respStmt>
			<orgName>BERT</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Gamaleldin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno>ArXiv, abs/1806.11146</idno>
		<title level="m">Adversarial reprogramming of neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Tinybert: Distilling bert for natural language understanding</title>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno>ArXiv, abs/1909.10351</idno>
		<editor>ICML. Xiaoqi Jiao, Y. Yin, L. Shang, Xin Jiang, X. Chen, Linlin Li, F. Wang, and Qun Liu</editor>
		<imprint>
			<date type="published" when="2019">2019. 2020</date>
		</imprint>
	</monogr>
	<note>Parameter-efficient transfer learning for nlp</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>ArXiv, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">To tune or not to tune? adapting pretrained representations to diverse tasks</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4302</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana; Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018. 2019. RepL4NLP-2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7" to="14" />
		</imprint>
	</monogr>
	<note>Proceedings of the 4th Workshop on Representation Learning for NLP</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adapterfusion: Non-destructive task composition for transfer learning</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Rücklé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno>ArXiv, abs/2005.00247</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schutze</surname></persName>
		</author>
		<idno>ArXiv, abs/2009.07118</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Autoprompt: Eliciting knowledge from language models with automatically generated prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Iv Robertllogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<idno>ArXiv, abs/2010.15980</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding throug</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
