<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Software Trace Cache</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alex</forename><surname>Ram</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Architecture Department</orgName>
								<orgName type="institution">Universitat Polit ecnica de Catalunya</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Josep-L</forename><surname>Larriba-Pey</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Architecture Department</orgName>
								<orgName type="institution">Universitat Polit ecnica de Catalunya</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Carlos</forename><surname>Navarro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Architecture Department</orgName>
								<orgName type="institution">Universitat Polit ecnica de Catalunya</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Josep</forename><surname>Torrellas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Architecture Department</orgName>
								<orgName type="institution">Universitat Polit ecnica de Catalunya</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mateo</forename><surname>Valero</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Architecture Department</orgName>
								<orgName type="institution">Universitat Polit ecnica de Catalunya</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Software Trace Cache</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To prevent instruction fetch bandwidth from becoming a major limiting factor to aggressive superscalar performance, it is crucial to develop techniques to supply many useful instructions to the processor every cycle. Unfortunately, this is hard to do for large integer codes like databases or several SPEC applications that have few loops, high instruction-cache miss rates and very large code sizes. Existing hardware techniques like large trace caches do not work very well. Furthermore, while compile-time code reordering can help, past techniques have largely focused on simply minimizing instruction-cache miss rates. Such an approach, while ideal for simple, less aggressive processors, is insu cient for wider-issue superscalars, where the key is to provide to the processor as many consecutive useful instructions per cycle as possible.</p><p>In this paper, we present a code reordering technique that focuses on maximizing the sequentiality of instructions, while still trying to minimize instruction cache misses. We call it the Software Trace Cache (STC), and apply it to the PostgreSQL database, an arcade game and the integer and oatingpoint SPEC'95 applications. Our results show that for large codes with few loops and deterministic execution sequences like databases, and some SPEC-INT codes the STC o ers similar, or better, results than a hardware trace cache (HTC). Moreover, if we combine the STC and the HTC, we obtain encouraging results: the STC and a small HTC often perform similarly to a large HTC alone.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aggressive wide-issue superscalars will continue to increase their demands for instruction bandwidth to satisfy execution requirements. As a result, there is the danger that the instruction fetch unit may become a major limiting factor to the performance of aggressive processors. Consequently, it is crucial to develop techniques that deliver multiple basic blocks worth of useful instructions to the processor every cycle.</p><p>Unfortunately, for many important codes, this is hard to do. Indeed, database codes and several integer SPEC applications have frequent control ow transfers, few loops, high instruction-cache miss rates and, frequently, large code sizes. The combination of these characteristics makes supplying a high bandwidth of useful instructions a di cult task, even in the presence of aiding devices like a hardware trace cache (HTC) <ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b3">4]</ref>. Indeed, a trace cache does not work very well with very large codes with few loops like database or compiler codes. The reason is that the important sections of these codes are so long and so relatively infrequently reused that even a large trace cache is unable to store them for reuse.</p><p>For these applications, the compiler can reorder the code in memory so that it is easier to supply useful instructions to the fetch unit. Code reordering can target the elimination of cache con icts and, therefore, reduce instruction cache miss rates. In addition, it can also map sequentially-executed basic University of Illinois at Urbana Champaign, USA.</p><p>blocks in consecutive memory positions and, therefore, increase the number of useful instructions fetched per access.</p><p>Unfortunately, past work on code reordering techniques has largely focused on simply reducing the instruction cache miss rate <ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8]</ref>. This approach made sense in the context of the simple, less aggressive processors for which past work was done. However, in the modern, wide-issue superscalars, ensuring that sequentially-executed instructions are mapped in consecutive memory positions can be more crucial than keeping the number of misses low. Furthermore, those past works that focused on increasing the spatial locality of codes, often ignored trying to reduce the instruction cache miss rate <ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref>. Finally, to our knowledge, code reordering techniques have not been applied to challenging codes like databases and very large integer SPEC applications.</p><p>In this paper, we address all these issues. We present a fully-automated, compile-time code reordering technique that focuses on maximizing the sequentiality of instructions, while still trying to minimize instruction cache misses. Such a technique is targeted to wide-issue superscalars, allowing the fetching of multiple basic blocks worth of useful instructions every cycle. We call the technique the Software Trace Cache (STC). We apply it to the PostgreSQL database management system, an arcade game and the integer and oating-point SPEC'95 applications.</p><p>Our results show that for large codes with few loops and deterministic execution sequences like the Postgres95 database management system, the STC o ers similar, or better, results than a hardware trace cache (HTC). Moreover, if we combine the STC and the HTC, we obtain very encouraging results. Speci cally, the number of fetched instructions per cycle obtained with a combination of the STC and a small HTC is comparable to that of a HTC of double size alone. Finally, the STC can be useful even in combination with a large HTC due to its miss reduction e ects.</p><p>This paper is structured as follows: Section 2 describes the STC technique; Section 3 characterizes the instruction reference stream for a variety of workloads; Section 4 uses simulations to evaluate various combinations of the STC and the HTC; Section 5 discusses related work; and Section 6 concludes and presents future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Software Trace Cache</head><p>The number of useful instructions per cycle provided to the processor is determined by three factors: branch prediction accuracy, instruction cache miss rate and the execution of non-contiguous instructions. To deal with the last two problems, we propose a code reordering technique which uses the whole memory space as a Software Trace Cache to store the most popular sequences of basic blocks. To avoid sequence breaks we inline the most popular functions, and move unexecuted basic blocks out of the execution path, mapping basic blocks executed sequentially in consecutive memory positions. To reduce the instruction cache miss rate we map the most popular traces in a reserved area of the i-cache, and other popular sequences are mapped close to other equally popular ones.</p><p>Our algorithm is based on pro le information. Running the training set on each benchmark, we obtain a directed graph with weighted edges. An edge connects two basic blocks p and q if q is executed after p. The weight of an edge pq is equal to the total number of times q has been executed after p. The number of times a basic block has been executed can be obtained by adding the weight of all outgoing edges. All unexecuted basic blocks and transitions are pruned from the graph.</p><p>In the following, we describe how we select the seeds or starting basic blocks for these sequences of code, the algorithm which builds the basic block traces from the selected seeds, increasing the code sequentiality, and the mapping algorithm to allocate these traces minimizing con ict misses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Seed selection</head><p>We obtain an ordered list of seeds by ordering the entry points of all functions in decreasing frequency of execution. This tries to expose the maximum temporal locality, as the rst traces built will start on the most frequently referenced functions.</p><p>It is possible to obtain better results with a seed selection based on the internal structure of the code, as we show <ref type="bibr">in 12]</ref>. However access to the source code of applications is not always granted, and gaining such deep understanding of the code is a time consuming task, which may not o er an improvement large enough to compensate for the e ort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Trace building</head><p>Using the weighted graph obtained running the training set, and starting from the selected seeds, we implement a greedy algorithm to build our basic block traces targeting an increase in the code sequentiality.</p><p>Given a basic block, the algorithm follows the most frequently executed path out of it. This implies visiting a subroutine called by the basic block, or following the control transfer out of it with the highest probability of being used. With this, we often end up including the basic blocks of a called function between the basic blocks of the caller. This guided function inlining increases the sequentiality of code by crossing the procedure boundary and mapping sequences of basic blocks spanning several functions.</p><p>For this algorithm we use four parameters called the Exec Threshold, the Exec Replication Threshold, the Branch Threshold, and the Branch Replication Threshold. The weight of a node divided by the total weight of all nodes gives a ratio that we compare to the Exec and Exec Replication thresholds; the weight of an outgoing edge divided by the weight of the node it leaves gives a ratio we compare to the Branch and Branch Replication thresholds.</p><p>The trace building algorithm stops when all the successor basic blocks have a weight lower than the Exec Threshold, or all the outgoing arcs have a weight less than the Branch Threshold.</p><p>When the algorithm visits a node which has already been included in a previous sequence, and it is the seed of a previously formed trace, we check the Replication thresholds. If the basic block and the arc which led to it pass both thresholds, the found trace is replicated at that point. This creates larger sequences and allows a given function to be inlined at several points in the code, further increasing code sequentiality. If the found basic block is not a starting point for another trace or it does not pass the Replication thresholds, the algorithm also stops.</p><p>In that case, we start again from the next acceptable basic block, building secondary execution paths for that seed. Once all basic blocks reachable from the given seed have been included in the sequence, we proceed to the next seed.</p><p>We have a loop that repeatedly selects a set of values for the four thresholds and generates the resulting traces for each pass. The basic blocks included in previous passes are pruned from the newly formed traces. By iteratively selecting less and less restrictive values for the thresholds, we build our traces grouped in passes of decreasing frequency of execution.</p><p>The values selected for the Exec and Branch threshold will determine the number of basic blocks included in each pass of the algorithm, generating larger or smaller groups of traces. The target is to pack in a given pass those traces with a similar popularity, while keeping the total number of instructions under control. The values for the Replication thresholds will determine the amount of code we allow our algorithm to replicate, more restrictive thresholds will be more selective as to when to replicate a given trace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Trace mapping</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the mapping scheme we developed. We de ne a logical array of caches, equal in size and address alignment to the physical cache. We map the found traces in sequential order, one pass at a time. If the number of instructions in a pass r in a logical cache, there will not be con ict misses among the traces in that pass. The rst KB of all logical caches but the rst one are to be kept free of code, thus creating a con ict free area (CFA) for the most popular traces. The size of this CFA is xed to a number of KB. When all the traces have been mapped in the cache, we map all the basic blocks not included in any trace in the original order. Optionally, we could map these basic blocks using the entire address space instead of leaving the CFA gap. This rarely executed code is expected not to produce many con icts with the sequences placed in the CFA.</p><p>While the increase in code sequentiality does not depend on the CFA size, it is the most important factor regarding the miss rate reduction o ered by our technique. Intuitively, an increase in the CFA size causes both positive and negative e ects. A larger CFA shields more routines from interference and, as a result, eliminates misses in those routines. On the other hand, however, less area is left for the rest of the routines, which will su er more con ict misses. Once the CFA size reaches certain value, the second e ect will dominate. Also, when the instructions in the CFA represent most of the i-cache references, there is little point in further increasing it, as we will be taking out cache area that may be better used to avoid con ict misses in other code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Locality Study</head><p>We analyzed the instruction reference stream for a wide set of workloads, characterizing those parameters of program execution which a ect the performance o ered by the STC. The reference locality will a ect the i-cache miss rate reduction o ered by our technique, and the basic block size, the number of loops and the determinism of program execution will in uence the increase in code sequentiality accomplished by the basic block reordering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Workloads</head><p>We have used four classes of workloads, trying to cover a wide range of applications: common integer and oating point codes, commercial workloads, and arcade games.</p><p>Recent studies have shown that commercial workloads do not behave like common integer codes, like the SPEC-INT set. Also, it is well known that oating point codes have a di erent behavior than integer codes. Our workloads include the whole SPEC 95 benchmarks. We use the PostgreSQL 6.3.2 database management system as our commercial workload and XBlast 2.2, an arcade game, as an example of a little studied workload.</p><p>All the executions and simulations needed to develop this work, have been done using the Alpha 21164 processor, DEC ATOM tool and trace driven simulation. The Training set used to obtain the pro le information consisted of the train input set for the SPEC benchmarks, TPC-D queries 3,4,5,6,9 and 15 for PostgreSQL, and a 5-player game to 3 wins for XBlast. As a Test set used for simulation, we used the ref input set for the SPEC benchmarks, TPC-D queries 2,3,4,6,11,12,13,14,15 and 17 for PostgreSQL, and di erent 5-player games to 3 wins for XBlast. The TPC-D data set was generated for a scale factor of 0.1 (100MB of data). All benchmarks were run to completion for both training and simulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Code Analysis</head><p>Examining the pro le information obtained running the training set, we intend to classify the workloads attending at those characteristics which a ect the performance of our technique: code locality, the amount of loops, conditional branches and subroutine calls, the basic block size and the number of sequence breaks.</p><p>The rst parameter we examined is code locality. This will determine the cache size needed to work with that workload, the right CFA size, and the amount of code we can allow our method to replicate. Table <ref type="table" target="#tab_0">1</ref> shows the number of instructions which concentrate 75, 90 and 99% of the references, the total code size for each benchmark and the CFA size we selected for 32 and 64KB instruction caches. We selected the CFA size so that the instructions in it gathered as many dynamic references as possible, while leaving reasonable space for the rest of the code. For most workloads, the CFA captures 75{90% of the references. For example, xblast concentrates 90% of the dynamic instruction references in 2362 instructions (9448 bytes), which almost t in an 8KB CFA. We observe that some codes have very large working sets, like applu, apsi, fpppp, gcc and postgres which do not t even in 32KB caches. Furthermore, some codes exhibit very little temporal locality, like gcc, which can not t 75% of the references in a 32KB cache. We chose the CFA size looking at the number of instructions which gathered between 75 and 90% of the references. In some cases, like apsi, a larger i-cache allows the use of a larger CFA without much risk of increasing con ict misses in the rest of the cache. For some codes, like mgrid, there is no point in increasing the CFA size for the 64KB cache, as a 4KB CFA already captures 99% of the references. Most codes will work ne with a CFA size of about 25% of the total cache size.</p><p>Next we examined the code sequentiality for the original layout. Table <ref type="table" target="#tab_1">2</ref> shows the percentage of basic block transitions which break the sequential execution of the code and the average number of instructions per basic block for each benchmark. The last column shows the average number of consecutive instructions executed, which can be obtained as a function of the rst two ones. This last gure limits the potential fetch bandwidth of a sequential fetch unit. We observe that all oating point benchmarks have very large basic blocks, leading to large code sequences. Instruction fetch is not a problem for these codes, as fetching a single basic block already provides enough instructions to feed even the more aggressive processors. Fetching a single basic block from applu already provides an average of 23 instructions, and most of the FP codes already execute over 32 consecutive instructions before breaking the execution sequence. Meanwhile, the average sequence length for the integer benchmarks is usually under 12 instructions, as less than 2 consecutive basic blocks are executed, and the typical basic block size is around 5{7 instructions.</p><p>To maximize the number of instructions provided by a sequential fetch unit, we will try to increase the number of sequentially executed instructions, removing the need to fetch instructions from non-consecutive basic blocks. The sequence length can be improved in two ways. We can either enlarge the basic blocks or reduce the number of sequence breaks. We will reorder the code so that the number of sequence breaks is kept to a minimum, decreasing the number of taken branches and inlining the most popular procedures as explained in Section 2. Finally, we examined a classi cation of the dynamic basic blocks executed by each benchmark. The di erent types of basic block considered are shown in Table <ref type="table" target="#tab_2">3</ref>. The percentage of basic blocks of each type executed is shown in Table <ref type="table" target="#tab_3">4</ref>. The last two columns show the percentage of branch and loop basic blocks which behave in a xed way (FB,FL), that is, they are always taken or always not taken. A low proportion of xed loop branches means that each loop executes few iterations.</p><p>By changing the order of the basic blocks in a program we can reduce the number of unconditional branches (J), and change taken conditional branches (B) for not taken ones. Also, by inlining the most popular functions we can eliminate subroutine calls (S) and returns (R), and increment the number of sequentially executed instructions. But, the number of sequence breaks due to loop branches (L) and unpredictable conditional branches does not depend on the organization of the code. We consider the indirect jumps (j,s) in a separate way because they can not be eliminated, as the target address is unknown, and may jump to an unexpected address. But we can reorder the code so that the most frequent target address does not break the execution sequence.</p><p>To reduce the number of loops, compiler optimizations like loop unrolling can be used, but it is not yet included in our work. Consequently, the STC as it is now will o er little advantage to codes with lots of loops and few xed conditional branches. Also, codes with few subroutine calls will not bene t from the increased code sequentiality o ered by function inlining. The number of predictable basic block transitions is determined by fall-through basic blocks, PC-relative unconditional branches, xed conditional branches and subroutine calls. The percentage of fallthrough basic blocks is around 10{20% for most codes, so we will look more closely to the rest of the basic block classes. With this criteria, we expect postgres, with only a 3.4% of loop basic blocks, 12.8% of subroutine calls and 43% of conditional branches (76.2% of which behave in a xed way) to be the one which will bene t the most from the STC. On the other hand, 32% of the basic blocks executed by ijpeg end with a loop branch, and barely 30% of its conditional branches behave in a xed way, which makes it di cult to enlarge the execution sequences. Among the FP codes, apsi looks as the best candidate, with a large proportion of xed conditional branches, few loops and a few subroutine calls.</p><p>4 Evaluation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Core Fetch Unit</head><p>In order to evaluate the actual increase in instructions per cycle obtained using the Software Trace Cache, we simulated an aggressive sequential fetch unit similar to that described in 13]. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, the core fetch unit is composed of an interleaved instruction cache (i-cache), a multiple branch predictor (BP), an interleaved branch target bu er (BTB) and a return address stack (RAS).  This fetch unit is designed to fetch as many contiguous instructions as possible. The limits are posed by the width of the datapath and the branch predictor throughput. In this work, we will assume a limit of 16 instructions and 3 branches per cycle. Two consecutive i-cache lines are accessed per cycle, allowing us to fetch sequential code crossing the cache line boundary. The BTB is accessed in parallel with the i-cache, and is used to predict the address of indirect jumps and subroutine calls. The return address of subroutines can be accurately predicted using the RAS. It is assumed that instructions will be predecoded, allowing branches and other control transfers to be detected. The target of PC-relative branches is calculated, not obtained from the BTB. Using the outputs from the BP, the BTB and the information regarding which instructions represent control transfers, we obtain an instruction mask to select the valid instructions from the fetched i-cache lines, and generate the fetch address for the next cycle.</p><p>To fully exploit the increased code sequentiality, we can not allow our fetch unit to stop at indirect jumps if those do not break the execution sequence. We propose to add a breaking bit to the BTB, which informs the fetch unit when the predicted jump address will break the sequence.</p><p>Summarizing, instruction fetch stops in one of these conditions:</p><p>16 instructions are fetched 3 branches are fetched A branch is predicted taken An indirect jump is predicted to break the execution sequence A system call is fetched On any misprediction or BTB miss</p><p>For high branch prediction accuracy we use a 4KB GAg correlated branch predictor, with 14-bit history length, extended to allow multiple branches to be predicted in a single cycle, a 256-entry BTB enhanced with the breaking bit, and a 256-entry RAS.</p><p>We also simulated our core fetch unit in conjunction with a trace cache (t-cache). We implement the basic trace cache model described in 13]. The t-cache stops fetching on the same conditions as the core fetch unit, except for the case of sequence breaks, as it is able to dynamically store non-contiguous instructions in contiguous memory positions. On a t-cache miss, the line-ll bu er is fed using future knowledge, and the new trace will be available for fetching the next cycle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Simulation Results</head><p>After selecting an appropriate CFA size for each benchmark (see Table <ref type="table" target="#tab_0">1</ref>), we measured the sequence break reduction obtained with the STC. Table <ref type="table" target="#tab_6">5</ref> shows the percentage of basic block transitions which break the sequence, and the average number of consecutive instructions executed for each benchmark for the original code and our proposed layout, measured running the Test set. The results obtained in Table <ref type="table" target="#tab_1">2</ref> (Section 3) correspond to the Training set.</p><p>The number of consecutive instructions executed represents the performance limit of a sequential fetch unit. Even if we were not limited by the bus width, the branch predictor throughput, and branch mispredictions, we would still be limited by taken branches. Table <ref type="table" target="#tab_6">5</ref> shows how the STC improves this performance limit.</p><p>As expected, the FP benchmarks barely reduced the percentage of sequence breaks. The best results are obtained by hydro2d and apsi, with reductions between 20{25%, which translate to sequence length increases of 24{31%. This was to be expected due to the reduced proportion of loops executed (see Table <ref type="table" target="#tab_3">4</ref>). On the other hand, mgrid actually increased the percentage of sequence breaking BB transitions from 89% to 90%. It is the FP benchmark with a higher proportion of loop basic blocks executed.</p><p>For the integer codes, we obtain sequence length increases above 100% for m88ksim, vortex, postgres and xblast. Meanwhile, ijpeg did not experience any noticeable improvement, and compress actually increased the percentage of sequence breaks. This roughly corresponds to what we expected from Section 3, with somewhat unexpected results for vortex and compress, which points to the existence of some other factor we did not consider in our study.</p><p>In general terms, most integer codes experience signi cant reductions in the percentage of sequence breaking BB transitions. After reordering, most codes execute 2{3 consecutive basic blocks, raising the average performance limit to 15.8 instructions.</p><p>Table <ref type="table" target="#tab_7">6</ref> shows simulation results for the above described fetch unit, using a 32KB instruction cache. We simulated both code layouts on the core fetch unit, and in combination with 16 and 32KB t-caches. The code layout is either the original code (Base), or the optimized layout corresponding to a CFA of the speci ed size (Opt x ). We present separate results for the number of Fetched Instructions per Access (FIPA) for the core fetch unit (i-cache) and the t-cache, and the global performance for the combination of both. Also, the i-cache and t-cache miss rates are presented in terms of misses per line access. There  are two i-cache line accesses and one t-cache line access for each fetch unit access. Finally, we also present the branch misprediction rate.</p><p>The nal performance metric is the number of Fetched Instructions per Cycle (FIPC). The FIPC was obtained dividing the FIPA for an estimated number of cycles per access (CPA). We used a xed number of cycles for each i-cache miss, and assumed that if both i-cache lines missed, they could be served simultaneously. We also assumed an average number of penalty cycles for each branch misprediction. The fetch unit does not necessarily stop during these cycles, but it will be fetching wrong path instructions. As i-cache miss penalties we used 3 and 6 cycles, and branch misprediction penalties of 4, 8 and 12 cycles, as it will depend on the execution core of the processor.</p><p>The dynamic nature of the HTC makes the FIPA provided independent of the layout of the code executed and the number of entries of the t-cache. On the other hand, the FIPA provided by the core fetch unit increases for the reordered code. For example, reordering the code for postgres increases the FIPA for the core fetch unit from 7.5 to 10.3 instructions. These 10.3 instructions per access are still far away from the 18.3 shown in Table <ref type="table" target="#tab_6">5</ref>, but that was a maximum performance. Now we are also limited by the bus width and the branch predictor throughput and accuracy.</p><p>The global FIPA performance of the STC alone comes quite close to the HTC for some codes. For example, reordering the code for xblast we provide an average of 10.1 instructions per access, while a 16KB t-cache provides 10.8 instructions per access. The best results are obtained when combining both STC and HTC, as the core fetch unit will be able to provide more instructions on a t-cache miss. For example, a combination of STC and HTC for li provides an equal number of instructions than a t-cache of double size alone. For the larger codes, the t-cache can not remember all the executed code sequences, and the core fetch unit is used extensively. It is in these cases, like gcc, where the STC proves more useful, combining with a small t-cache to provide better results than a t-cache of double size alone. Combining a STC with a large t-cache still improves the results for the larger codes, but the FIPA increase is minimum for the small ones. We also observe that the STC drastically reduces the i-cache miss rate for both the FP and the integer codes. Large codes like xblast and postgres obtain miss rate reductions around 90%, and a nal i-cache miss rate around 1% on a 32KB cache. The t-cache miss rate does not seem to depend on the code layout, but on the t-cache size itself. Techniques like partial matching 4] target a t-cache miss rate reduction, but it also reduces the FIPA. With the increased performance of the core fetch unit due to the STC, this FIPA reduction may cause the t-cache to provide less instructions than would be fetched form the i-cache. It may not be worth adding such functionality to the HTC, as a compile-time optimization can obtain similar results.</p><p>The branch misprediction rate increases slightly with the reordered code. It is so because the reordered code reduces the number of taken branches, introducing more zeroes in the history register of the GAg predictor, leading to a worse utilization of the history table. For example, the branch misprediction rate for m88ksim increases from 4.9% to 7.9% when the code is reordered, mainly due to the reduction of sequence breaks from 61% to 27% (see Table <ref type="table" target="#tab_6">5</ref>).</p><p>The nal performance metric, the FIPC, depends on all three previous ones. Instruction cache misses and branch mispredictions cause the fetch engine to stall and fetch instruction from the wrong execution path increasing the CPA.</p><p>Both the STC and the HTC improve the FIPA, but the STC also targets a reduction of the CPA by minimizing i-cache misses. For small codes, like hydro2d which already have a very low i-cache miss rate, the STC is not too useful, with the HTC providing much better performance. In some cases, like li, the increased branch misprediction rate can actually hinder the performance of the HTC.</p><p>On the other hand, for the largest codes, like postgres and xblast, the STC alone can o er similar, or better results than a HTC alone. In these cases, combining the compile-time and the run-time techniques o ers the best results, raising the FIPC from 5.9 with the STC, or 4.6 with the 16KB HTC, to 6.5 with a combination of both. In most cases, using a combination of STC and a small HTC o ers similar or better results than a HTC of double size.</p><p>The bene ts of the STC are more obvious when the i-cache miss penalty increases and the branch misprediction penalty is small, while the HTC proves most useful when the i-cache miss penalty is low.</p><p>We conclude that for large codes with few loops, the STC can provide better results than the HTC alone, and that a combination of the STC with a small HTC provides similar or better results than a much larger HTC alone. When combined with a large t-cache, the STC is still able to provide performance improvements, due to the reduced i-cache miss rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>There has been much on code mapping algorithms to optimize the instruction cache miss rate. These works were targeted at less aggressive processors, which do not need to fetch instructions from multiple basic blocks per cycle.</p><p>Hwu and Chang 9] use function inline expansion, and group into traces those basic blocks which tend to execute in sequence as observed on a pro le of the code. Then, they map these traces in the cache so that the functions which are executed close to each other are placed in the same page. Our approach does a more intelligent function inlining, including only those procedures which pass the thresholds, and only the main execution path of the procedure, instead of the whole procedure. In addition, we also protect the most popular sequences in a reserved area of the cache.</p><p>Pettis &amp; Hansen 11] propose a pro le based technique to reorder the procedures in a program, and the basic blocks within each procedure. Their aim is to minimize the con icts between the most frequently used functions, placing functions which reference each other close in memory. They also reorder the basic blocks in a procedure, moving unused basic blocks to the bottom of the function code, even splitting the procedures in two, and moving away the unused basic blocks. Their algorithm did not consider the target cache information, and their basic block reordering was limited to the basic blocks within a function body.</p><p>Torrellas et al 14] designed a basic block reordering algorithm for Operating System code, running on a very conservative vector processor. They map the code in the form of sequences of basic blocks spanning several functions, and keep a section of the cache address space reserved for the most frequently referenced basic blocks.</p><p>A performance comparison between the Pettis &amp; Hansen method, the Torrellas et al method and the STC can be found in 12], with the STC obtaining better results across all tested setups. Gloy et al. 5] extend the Pettis &amp; Hansen placement algorithm at the procedure level to consider the temporal relationship between procedures in addition to the target cache information and the size of each procedure. Hashemi et al 6] and Kalamaitianos et al 8] use a cache line coloring algorithm inspired in the register coloring technique to map procedures so that the resulting number of con icts is minimized. Their algorithm is based on either a dynamic pro le of the code or on static estimations based on heuristics.</p><p>To further increase the number of useful instructions provided to the processor, we need either to enlarge the basic block or to fetch instructions from multiple basic blocks per cycle. On the software side, we have techniques like trace scheduling 3] and superblock scheduling 7], which use static branch prediction to identify sequences of basic blocks and join them, creating larger basic blocks, which avoids the need for extra hardware to fetch them and enlarging the scope of the compiler to schedule instructions.</p><p>On the hardware side, techniques like the Branch Address Cache 15], the Collapsing Bu er 2] and the Trace <ref type="bibr">Cache 13,</ref><ref type="bibr" target="#b3">4]</ref> approach the problem of fetching multiple, non-contiguous basic blocks each cycle. Both the Branch Address Cache and the Collapsing Bu er access non-consecutive cache lines from an interleaved i-cache each cycle and then merge the required instructions from each accessed line. The Trace Cache does not require fetching of non-consecutive basic blocks from the i-cache as it stores dynamically constructed sequences of basic blocks in a special purpose cache.</p><p>These techniques do not target an i-cache miss rate reduction, relying on other techniques for it. Some works have examined the interaction of compile-time and run-time techniques regarding the instruction fetch mechanism. Chen et al. 1] examined the e ect of the code expanding optimizations (loop unrolling and function inlining) on the instruction cache miss rate. Also, as an example of software and hardware cooperation, Patel et al 10] identify branches with a xed behavior and avoid making prediction on them, increasing the potential of the Trace Cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>Instruction fetch bandwidth may become a major limiting factor to aggressive superscalar performance. Consequently, it is crucial to develop techniques to supply many useful instructions to the processor every cycle. To address this problem, this paper has presented a code reordering technique that focuses on maximizing the sequentiality of instructions, while still trying to minimize instruction cache misses. We call this technique Software Trace Cache (STC), and apply it to the PostgreSQL database, an arcade game and the integer and oating-point SPEC'95 applications.</p><p>Our results are very encouraging. For large codes with few loops and deterministic execution sequences like databases or large integer SPEC applications, the STC o ers similar, or better, results than a hardware trace cache (HTC). Moreover, if we combine the STC and the HTC, we obtain even better results: the STC and a small HTC often perform similarly to a HTC of double size alone. Finally, the STC improves even a system with a large HTC.</p><p>We are extending this work in many promising directions. First, we are modifying the HTC design to exploit the code layout generated by the STC; we think that a combined STC-HTC design can provide better results. Secondly, we are incorporating advanced compiler optimizations in our STC, like loop unrolling to more e ectively prepare the code for the HTC. Finally, we are examining the e ect of the STC on the branch prediction hardware, since we have observed that the branch misprediction rate increases after the STC reduces the number of taken branches. Overall, we feel that the STC has many interesting interactions with the instruction fetch hardware which can be exploited.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Trace mapping into a direct mapped cache</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sequential fetch unit model used for simulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Number of static instructions needed to accumulate 75, 90 and 99% of the dynamic references, and the total code size, including unreferenced instructions. Selected CFA size for each benchmark for 32 and 64KB instruction caches. The size for xblast includes dynamically linked libraries. Numbers for the Training set.</figDesc><table><row><cell>Dynamic references 75% 90% 99% code size 32KB 64KB Total CFA size 223 308 1328 108237 8 8 148 232 763 110350 4 4 979 1839 4197 129741 8 16 104.hydro2d 1223 1977 5371 125946 Benchmark 101.tomcatv 102.swim 103.su2cor 8 16 107.mgrid 147 218 1029 112421 4 4 110.applu 2407 5060 10509 132803 16 24 125.turb3d 1065 1771 2828 121181 8 8 141.apsi 3099 5694 9883 156479 16 24 145.fpppp 8985 8985 9879 124970 8 32 146.wave5 1116 1919 5506 154987 8 24 124.m88ksim 458 1006 2863 51341 8 16 126.gcc 9595 22098 57878 349382 8 16 129.compress 243 338 525 21991 4 8 130.li 325 563 1365 38126 8 8 132.ijpeg 862 1489 3271 67646 8 16 134.perl 987 1582 3006 108227 8 16 147.vortex 751 1486 5128 172690 8 24 postgres 2716 5221 11748 374399 16 16 xblast 1100 2362 6326 430664 8 16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Benchmark 101.tomcatv 102.swim 103.su2cor 104.hydro2d 107.mgrid 110.applu 125.turb3d 141.apsi 145.fpppp 146.wave5 FP Average 124.m88ksim 126.gcc 129.compress 130.li 132.ijpeg 134.perl 147.vortex INT Average postgres xblast</cell><cell>% Sequence Average Sequence breaks BB size length 73.7 37.4 50.7 77.2 31.3 40.5 52.6 19.8 37.6 67.6 14.0 20.7 78.1 54.0 69.1 48.7 23.3 47.8 47.3 21.9 46.3 44.9 18.5 41.2 39.4 72.1 183.0 58.2 18.5 31.8 58.8 31.1 56.9 63.9 6.2 9.7 54.2 5.3 9.8 63.1 7.5 11.9 51.3 4.3 8.4 69.6 15.1 21.7 55.0 6.2 11.3 54.7 4.8 8.8 58.8 7.1 11.7 50.6 4.6 9.1 62.8 5.5 8.8</cell></row></table><note>Percentage of basic block transitions which break the execution sequence, average number of instructions per basic block and average number of consecutive instructions executed for each benchmark. Numbers obtained from the pro le information.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Basic block types considered.</figDesc><table><row><cell>BB Type F J j B L S s R</cell><cell>Description Fall-through Unconditional branches Unconditional branches Conditional branches Loop branches Subroutine call Subroutine call Subroutine returns</cell><cell>Target Next instruction PC relative Indirect PC relative PC relative PC relative Indirect Indirect</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Percent of the dynamic basic blocks of each type for each workload. Percent of conditional and loop branches with a xed behavior.</figDesc><table><row><cell>Basic Block Type B L 3.3 4.1 3.1 28.2 59.0 0.2 1.0 1.2 92.0 Fixed branches F J j S s R FB FL 49.3 7.1 5.9 2.0 19.4 57.5 0.0 3.9 3.9 54.7 100.0 11.8 5.9 2.0 44.4 25.3 0.1 5.2 5.3 65.9 92.4 9.8 1.6 0.6 46.3 39.1 0.1 1.3 1.3 66.5 88.1 5.2 0.1 0.1 13.2 81.1 0.0 0.1 0.1 81.1 0.0 11.1 0.0 0.0 43.3 45.4 0.0 0.0 0.0 58.8 13.0 11.8 6.4 0.4 46.9 29.3 2.2 0.4 2.6 81.5 14.8 17.8 2.9 0.0 44.7 23.7 3.2 2.2 5.4 77.9 18.8 19.8 2.7 2.7 56.3 9.6 0.0 4.5 4.5 46.2 16.9 15.2 1.7 0.8 35.2 31.9 2.5 5.1 7.6 90.5 88.3 124.m88ksim 11.7 6.9 0.4 47.3 9.4 2.0 10.2 12.2 62.7 Benchmark 101.tomcatv 102.swim 103.su2cor 104.hydro2d 107.mgrid 110.applu 125.turb3d 141.apsi 145.fpppp 146.wave5 53.6 126.gcc 9.5 4.2 2.2 58.8 11.3 4.0 3.0 7.0 39.9 21.7 129.compress 12.3 5.9 0.0 37.0 16.5 14.1 0.0 14.1 48.2 44.7 130.li 21.1 3.2 1.9 39.8 11.6 7.3 3.9 11.2 44.4 50.8 132.ijpeg 13.8 5.0 0.0 43.7 32.1 1.6 1.1 2.7 29.8 35.1 134.perl 21.1 3.6 2.6 45.9 9.2 2.1 6.7 8.8 69.0 53.4 147.vortex 16.0 6.2 0.5 44.4 17.8 0.1 7.5 7.6 63.7 32.6 postgres 22.1 2.5 1.2 43.4 3.4 12.8 0.9 13.6 76.2 26.3 xblast 20.8 1.0 0.1 50.1 11.2 2.2 6.2 8.4 65.1 13.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Percentage of basic block transitions and average number of consecutive instructions executed for the original and the reordered code. The average BB size is the same for both code layouts. Numbers for the Test set.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Simulation results for a 32KB instruction cache.</figDesc><table><row><cell>Setup t-cache Layout i-cache t-cache global i-cache t-cache mispr. (%) 6/8 6/12 3/8 6/4 FIPA Miss rate (%) Branch FIPC miss pen,br pen] 0 Base 11.7 | 11.7 0.08 | 0.3 11.5 11.4 11.5 11.6 0KB Opt8 12.2 | 12.2 0.01 | 0.4 11.9 11.8 11.9 12.0 16KB Base 11.4 15.5 15.4 0.09 3.3 0.3 15.0 14.8 15.0 15.2 16KB Opt8 11.1 15.5 15.5 0.01 3.9 0.4 15.0 14.8 15.0 15.2 32KB Base 9.1 15.5 15.5 0.08 0.5 0.3 15.0 14.8 15.1 15.2 32KB Opt8 9.9 15.5 15.5 0.01 0.7 0.4 15.0 14.8 15.0 15.2 0KB Base 13.8 | 13.8 1.3 | 2.8 11.8 11.4 12.4 12.3 0KB Opt16 14.2 | 14.2 0.2 | 3.3 12.8 12.2 12.8 13.4 16KB Base 13.5 15.9 15.3 1.5 23.1 2.8 12.9 12.4 13.5 13.4 16KB Opt16 13.7 15.9 15.4 0.2 21.0 3.3 13.7 13.1 13.8 14.4 32KB Base 13.3 15.9 15.5 1.4 16.5 2.8 13.0 13.5 13.7 13.6 32KB Opt16 13.7 15.9 15.5 0.2 16.5 3.3 13.8 13.2 13.9 14.5 0KB Base 6.9 | 6.9 11.6 | 4.9 3.2 3.0 4.1 3.5 0KB Opt8 10.0 | 10.0 0.06 | 7.9 5.5 4.5 5.5 7.1 m88ksim 16KB Bench. hydro2d apsi Base 6.5 13.8 11.1 14.5 36.9 4.9 4.4 4.0 5.7 5.0 16KB Opt8 8.8 13.7 11.6 0.06 42.7 7.9 6.0 4.8 6.0 7.9 32KB Base 6.4 13.7 11.9 13.5 25.4 4.9 4.7 4.2 6.1 5.4 32KB Opt8 8.1 13.8 12.0 0.06 31.5 7.9 6.1 4.9 6.1 8.1 0KB Base 7.7 | 7.7 10.2 | 10.0 3.2 2.7 3.7 3.8 0KB Opt8 8.8 | 8.8 7.3 | 11.8 3.4 2.8 3.9 4.3 gcc 16KB Base 7.1 13.6 9.8 12.4 59.4 10.0 3.5 2.9 4.2 4.2 16KB Opt8 8.2 13.3 10.5 8.3 56.0 11.8 3.7 3.0 4.2 4.7 32KB Base 6.9 13.6 10.1 12.4 51.6 10.0 3.6 3.0 4.2 4.4 32KB Opt8 8.0 13.3 10.7 8.3 48.6 11.8 3.7 3.0 4.2 4.8 0KB Base 7.4 | 7.4 0.1 | 5.0 5.3 4.6 5.3 6.1 0KB Opt4 8.4 | 8.4 0.1 | 5.8 5.5 4.7 5.6 6.6 li 16KB Base 6.4 13.5 10.6 0.2 40.4 5.0 6.8 5.8 6.8 8.2 16KB Opt4 7.3 13.2 11.1 0.2 36.4 5.8 6.6 5.5 6.6 8.2 32KB Base 6.1 13.4 11.1 0.2 32.0 5.0 7.0 5.9 7.0 8.5 32KB Opt4 6.8 13.4 11.5 0.2 29.0 5.8 6.8 5.6 6.8 8.5 0KB Base 11.7 | 11.7 0.07 | 9.7 8.3 7.2 8.3 9.7 0KB Opt8 11.6 | 11.6 0.01 | 10.1 8.2 7.1 8.2 9.6 ijpeg 16KB Base 9.7 15.5 14.5 0.1 17.4 9.7 9.6 8.2 9.6 11.4 16KB Opt8 10.3 15.4 14.5 0.01 18.4 10.2 9.5 8.1 9.5 11.5 32KB Base 8.8 15.5 14.5 0.1 14.4 9.7 9.6 8.2 9.6 11.5 32KB Opt8 9.0 15.4 14.6 0.01 14.6 10.1 9.5 8.1 9.6 11.5 0KB Base 7.6 | 7.6 7.8 | 7.4 3.5 3.1 4.2 4.1 0KB Opt8 10.2 | 10.2 1.1 | 6.2 5.9 5.0 6.1 7.2 vortex 16KB Base 6.7 13.8 10.8 8.1 42.1 7.4 4.6 3.9 5.3 5.5 16KB Opt8 9.2 13.8 12.2 1.3 34.0 6.2 6.5 5.4 6.7 8.2 32KB Base 6.7 13.7 11.3 8.2 34.7 7.4 4.7 4.0 5.4 5.7 32KB Opt8 8.6 13.8 12.3 1.3 27.7 6.2 6.5 5.4 6.8 8.2 0KB Base 7.5 | 7.5 9.3 | 3.0 4.1 3.9 5.2 4.3 0KB Opt16 10.3 | 10.3 1.0 | 8.4 5.7 4.8 5.9 7.2 postgres 16KB Base 7.2 14.2 10.6 10.0 51.8 3.0 5.3 5.0 6.8 5.7 16KB Opt16 9.7 14.0 11.9 1.1 49.6 8.4 6.2 5.1 6.4 7.9 32KB Base 7.0 14.2 11.6 9.9 35.7 3.0 5.8 5.4 7.4 6.2 32KB Opt16 8.9 14.1 12.3 1.2 33.7 8.4 6.3 5.2 6.5 8.1 0KB Base 7.1 | 7.1 4.8 | 7.7 3.9 3.4 4.4 4.5 0KB Opt8 10.1 | 10.1 0.6 | 7.9 5.7 4.8 5.8 7.2 xblast 16KB Base 7.0 13.7 10.8 6.8 43.4 7.7 4.9 4.2 5.6 5.9 16KB Opt8 9.2 13.7 11.5 1.5 48.7 8.8 5.8 4.8 6.0 7.4 32KB Base 6.8 13.7 11.1 7.2 37.9 7.9 4.9 4.2 5.6 5.9 32KB Opt8 8.6 13.8 12.1 0.9 33.1 7.8 6.4 5.2 6.5 8.2</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>Authors from Universitat Politecnica de Catalunya are supported by CICYT grant TIC-0511-98. Josep Lluis Larriba-Pey and Josep Torrellas are supported by Generalitat de Catalunya Grant ACI-002. Josep Lluis Larriba-Pey, Josep Torrellas and Mateo Valero are supported by the Commission for Cultural, Educational and Scienti c Exchange between the United States of America and Spain. A. Ram rez wants to thank all his fellow PBC's for their time and e orts. The authors want to thank Xavi Serrano for all his help setting up and analyzing PostgreSQL.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The e ect of code expanding optimizations on instruction cache design</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pohua</forename><forename type="middle">P</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Conte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwu</forename></persName>
		</author>
		<idno>1045{ 1057</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="1993-09">September 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimization of instruction fetch mechanism for high issue rates</title>
		<author>
			<persName><forename type="first">T</forename><surname>Conte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Menezes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Patell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22th Annual Intl. Symposium on Computer Architecture</title>
				<meeting>the 22th Annual Intl. Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="page" from="333" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Trace scheduling: A technique for global microcode compaction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers, C</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="478" to="490" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Alternative fetch and issue techniques from the trace cache mechanism</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Friendly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Anual ACM/IEEE International Symposium on Microarchitecture</title>
				<meeting>the 30th Anual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1997-12">December 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Procedure placement using temporal ordering information</title>
		<author>
			<persName><forename type="first">Nikolas</forename><surname>Gloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Anual ACM/IEEE International Symposium on Microarchitecture</title>
				<meeting>the 30th Anual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1997-12">December 1997</date>
			<biblScope unit="page" from="303" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">E cient procedure mapping using cache line coloring</title>
		<author>
			<persName><forename type="first">H</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Kaeli</surname></persName>
		</author>
		<author>
			<persName><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGPLAN&apos;97 Conf. on Programming Languaje Design and Implementation</title>
				<meeting>ACM SIGPLAN&apos;97 Conf. on Programming Languaje Design and Implementation</meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The superblock: An e ective technique for vliw and superscalar compilation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mahlke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Water</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Ouellette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Hank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kiyohara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Haab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Hold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Lavery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Supercomputing</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="9" to="50" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Temporal-based procedure reordering for improved instruction cache performance</title>
		<author>
			<persName><forename type="first">John</forename><surname>Kalamaitianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Kaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Intl. Conference on High Performance Computer Architecture</title>
				<meeting>the 4th Intl. Conference on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1998-02">February 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Achieving high instruction cache performance with an optimizing compiler</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Mei Hwu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pohua</forename><forename type="middle">P</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Intl. Symposium on Computer Architecture</title>
				<meeting>the 16th Annual Intl. Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1989-06">June 1989</date>
			<biblScope unit="page" from="242" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving trace cache e ectiveness with branch promotion and trace packing</title>
		<author>
			<persName><forename type="first">Jeram</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Evers</surname></persName>
		</author>
		<author>
			<persName><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual Intl. Symposium on Computer Architecture</title>
				<meeting>the 25th Annual Intl. Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1998-06">June 1998</date>
			<biblScope unit="page" from="262" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pro le guided code positioning</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pettis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGPLAN&apos;99 Conf. on Programming Languaje Design and Implementation</title>
				<meeting>ACM SIGPLAN&apos;99 Conf. on Programming Languaje Design and Implementation</meeting>
		<imprint>
			<date type="published" when="1990-06">June 1990</date>
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Code reordering of decision support systems for optimized instruction fetch</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Ll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavi</forename><surname>Navarro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Torrellas</surname></persName>
		</author>
		<author>
			<persName><surname>Valero</surname></persName>
		</author>
		<idno>UPC-DAC-1998-56</idno>
		<imprint>
			<date type="published" when="1998-12">December 1998</date>
		</imprint>
		<respStmt>
			<orgName>Universitat Politecnica de Catalunya</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Trace cache: a low latency aprroach to high bandwith instruction fetching</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rottenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Benett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Anual ACM/IEEE International Symposium on Microarchitecture</title>
				<meeting>the 29th Anual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1996-12">December 1996</date>
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimizing instruction cache performance for operating system intensive workloads</title>
		<author>
			<persName><forename type="first">Josep</forename><surname>Torrellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Daigle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Intl. Conference on High Performance Computer Architecture</title>
				<meeting>the 1st Intl. Conference on High Performance Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1995-01">January 1995</date>
			<biblScope unit="page" from="360" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Increasing the instruction fetch rate via multiple branch prediction and a branch address cache</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Marr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th Intl. Conference on Supercomputing</title>
				<imprint>
			<date type="published" when="1993-07">July 1993</date>
			<biblScope unit="page" from="67" to="76" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
