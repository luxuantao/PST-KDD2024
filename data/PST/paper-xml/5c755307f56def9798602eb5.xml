<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">300 Faces In-The-Wild Challenge: database and results ଝ,ଝଝ</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Christos</forename><surname>Sagonas</surname></persName>
							<email>c.sagonas@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Epameinondas</forename><surname>Antonakos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Nottingham</orgName>
								<address>
									<settlement>Nottingham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maja</forename><surname>Pantic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Electrical Engineering, Mathematics, and Computer Science</orgName>
								<orgName type="institution">University of Twente</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">300 Faces In-The-Wild Challenge: database and results ଝ,ଝଝ</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">12D9AB6E372F898D50C254860E0C97A1</idno>
					<idno type="DOI">10.1016/j.imavis.2016.01.002</idno>
					<note type="submission">Received 19 March 2015 Received in revised form 2 October 2015 Accepted 4 January 2016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Facial landmark localization Challenge Semi-automatic annotation tool Facial database</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Computer Vision has recently witnessed great research advance towards automatic facial points detection. Numerous methodologies have been proposed during the last few years that achieve accurate and efficient performance. However, fair comparison between these methodologies is infeasible mainly due to two issues. (a) Most existing databases, captured under both constrained and unconstrained (in-the-wild) conditions have been annotated using different mark-ups and, in most cases, the accuracy of the annotations is low. (b) Most published works report experimental results using different training/testing sets, different error metrics and, of course, landmark points with semantically different locations. In this paper, we aim to overcome the aforementioned problems by (a) proposing a semi-automatic annotation technique that was employed to re-annotate most existing facial databases under a unified protocol, and (b) presenting the 300 Faces In-The-Wild Challenge (300-W), the first facial landmark localization challenge that was organized twice, in 2013 and 2015. To the best of our knowledge, this is the first effort towards a unified annotation scheme of massive databases and a fair experimental comparison of existing facial landmark localization systems. The images and annotations of the new testing database that was used in the 300-W challenge are available from http://ibug.doc.ic.ac.uk/resources/300-W_IMAVIS/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>During the last decades we notice a wealth of scientific research in computer vision for the problem of facial landmark points localization using visual deformable models. The main reason behind this are the countless applications that the problem has in human-computer interaction and facial expression recognition. Numerous methodologies have been proposed that are shown to achieve great accuracy and efficiency. They can be roughly divided into two categories: generative and discriminative. The generative techniques, which aim to find the parameters that maximize the probability of the test image being generated by the model, include Active Appearance Models (AAMs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, their improved extensions <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> and Pictorial Structures <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. The discriminative techniques can be further divided to those that use discriminative response map functions, such as Active Shape Models (ASMs) <ref type="bibr" target="#b13">[14]</ref>, Constrained Local Models (CLMs) <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref> and Deformable Part Models (DPMs) <ref type="bibr" target="#b17">[18]</ref>, those that learn a cascade of regression functions, such as Supervised Descent Method (SDM) <ref type="bibr" target="#b18">[19]</ref> and others <ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>, and, finally, those that employ random forests <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Arguably, the main reason why many researchers of the field focus on the problem of face alignment is the plethora of publicly available annotated facial databases. These databases can be separated in two major categories: (a) those captured under controlled conditions, e.g. Multi-PIE <ref type="bibr" target="#b24">[25]</ref>, XM2VTS <ref type="bibr" target="#b25">[26]</ref>, FRGC-V2 <ref type="bibr" target="#b26">[27]</ref>, and AR <ref type="bibr" target="#b27">[28]</ref>, and (b) those captured under totally unconstrained conditions (in-the-wild), e.g. LFPW <ref type="bibr" target="#b28">[29]</ref>, HELEN <ref type="bibr" target="#b29">[30]</ref>, AFW <ref type="bibr" target="#b17">[18]</ref>, AFLW <ref type="bibr" target="#b30">[31]</ref>, and IBUG <ref type="bibr" target="#b31">[32]</ref>. All of them cover large variations, including different subjects, poses, illumination conditions, expressions and occlusions. However, for most of them, the provided annotations appear to have several limitations. Specifically:</p><p>• The majority of them provide annotations for a relatively small subset of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>• The annotation mark-up of each database consists of different number of landmark points with semantically different locations.</p><p>• The accuracy of the provided annotations in some cases is limited.</p><p>The above issues are due to the fact that manual annotation of large databases is a highly time consuming procedure that requires enormous workload and a trained expert. Moreover, factors like fatigue and lack of concentration are among the reasons why, in some cases, annotations are inaccurate. This highlights the need of creating a (semi-) automatic annotation tool.</p><p>Furthermore, by going through the published works of the last years, one can easily notice that the setup of the experiments is not always correct. Researchers employ different databases, experimental protocols and performance metrics, which lead to unfair comparisons between existing methods. Some characteristic such examples are the following:</p><p>• Authors compare their techniques against other state-of-theart, but they do so by using, in many cases, completely different databases for training compared to the ones that the other methods were originally trained on. • Authors compare their techniques on specific databases by replicating the originally presented curves and not the experiment. • In some cases, authors report results on databases from which only a part can be used by the community, as some of the training/testing images are no longer publicly available.</p><p>Evidence shows that there is a lack of access to properly evaluate existing methods. Even though there exist open-source implementations of various state-of-the-art techniques (the most characteristic example is Menpo <ref type="bibr" target="#b32">[33]</ref>), researchers still do not employ a unified benchmark. Since we are unaware of the achieved performances, it is impossible to investigate how far we are from attaining satisfactory performance. Therefore, a new evaluation needs to be carried out, using a unified experimental protocol.</p><p>Various methods have been proposed in the literature for the task of landmark localization under semi-supervised or weaklysupervised settings <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref>. However, there are two major limitations of these methods. Firstly, most existing methodologies require additional information regarding the input images. Specifically, Jia et al. <ref type="bibr" target="#b33">[34]</ref> employ the corresponding facial mask for each of the training images. The purpose of these masks is to indicate which pixels belong to the facial area and the only way to produce them is by manually annotating each image. In <ref type="bibr" target="#b34">[35]</ref>, the training procedure requires as input the orientation of each face depicted in the training images. Secondly, and most importantly, existing methods, such as <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b36">[37]</ref>, have only been applied on images that are captured under controlled conditions. The aforementioned issues, make the existing methods incapable for the task of semi-automatic annotation of large databases with in-the-wild images (most of the images are downloaded from the web with simple search queries), which is a much more challenging task.</p><p>Semi-automatic annotation systems can greatly benefit from the employment of generative models. Let us assume that we have cohorts of both annotated and non-annotated images. By training a generative model, such as AAMs, using the annotated images, we get a parametric model that describes the facial shape and appearance. Most importantly, the model can naturally generate novel instances of human face, by combining the shape and appearance variance of the training annotated images. This could enable the generation of instances that resemble accurately with the shape and appearance of the subjects in the non-annotated images. For instance, by training a model using images from one view (e.g. pose 15 • ) with neutral expression and images from another view (e.g. pose 0 • ) with a nonneutral expression, one can fit the model to an instance that has the non-neutral expression with pose of 15 • . However, the fitting procedure of a generative deformable model is a very tedious task, mainly because many of the models that have been proposed till now do not generalize well to unseen images. One of the AAM variants that has satisfactory generalization properties is Active Orientation Models (AOMs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. AOMs are shown to be robust in cases with large variations, such as occlusions and extreme illumination conditions, and outperform discriminative methodologies, such as CLMs <ref type="bibr" target="#b15">[16]</ref>, DPMs <ref type="bibr" target="#b17">[18]</ref> and SDM <ref type="bibr" target="#b18">[19]</ref>.</p><p>Motivated by the success of AOMs in generic face alignment, we propose, in this paper, a semi-automatic technique for annotating in a time efficient manner massive facial databases. We employed the proposed tool to re-annotate all the widely used databases, i.e. Multi-PIE <ref type="bibr" target="#b24">[25]</ref>, XM2VTS <ref type="bibr" target="#b25">[26]</ref>, FRGC-V2 <ref type="bibr" target="#b26">[27]</ref>, AR <ref type="bibr" target="#b27">[28]</ref>, LFPW <ref type="bibr" target="#b28">[29]</ref>, HELEN <ref type="bibr" target="#b29">[30]</ref> and AFW <ref type="bibr" target="#b17">[18]</ref>. The resulting annotations<ref type="foot" target="#foot_0">1</ref> are, in many cases, more accurate than the original ones and employ a unified mark-up scheme, thus overcome the limitations explained above.</p><p>Furthermore, in order to offer to the research community the ability to carry out rational comparisons between existing and future proposed methods, we organized two versions of the 300 Faces In-The-Wild Challenge (300-W), the first automatic facial landmark detection in-the-wild challenge. The first challenge<ref type="foot" target="#foot_1">2</ref> was organized in 2013 in conjunction with the IEEE International Conference on Computer Vision (ICCV'13) <ref type="bibr" target="#b31">[32]</ref>. The second conduct 3 of the challenge was completed in the beginning of 2015. In both conducts, the training set consisted of the XM2VTS, FRGC-V2, LFPW, HELEN, AFW and IBUG databases that were annotated using the proposed semi-automatic procedure. Additionally, we collected and annotated a new challenging in-the-wild database that was used for testing. 4  The 300-W database consists of 300 Indoor and 300 Outdoor images downloaded from the web, thus captured under totally unconstrained conditions. The performance of the submitted methods was evaluated using the same fitting accuracy metric. The major difference between the two conducts of the challenge is that in the first version we provided the bounding boxes of the testing images to be used as initializations, while in the second version the participants were required to submit systems that performed both face detection and alignment. Additionally, contrary to the first version, in the second one the submitted methods were also compared with respect to their computational costs.</p><p>The contribution of this paper can be summarized as follows:</p><p>1. We propose a semi-automatic methodology for facial landmark points annotation. The proposed tool was employed in order to re-annotate large facial databases and overcome the major issues of the original annotations. 2. We present and analyze the results of the 300 Faces In-The-Wild Challenge (300-W), the first facial landmark localization challenge, that was conducted twice, in 2013 and 2015. The challenge is the first attempt towards a fair comparison of existing methods using a unified experimental protocol. 3. We make the very challenging 300-W dataset publicly available to the research community. It was employed as testing set in both conducts of the 300-W competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>The rest of the paper is organized as follows: Section 2 gives an overview of the available facial databases. Section 3 presents the proposed semi-automatic methodology for facial landmark points annotations along with the re-annotated databases. The 300-W challenge and the results are described in details in Section 4. Finally, Section 5 summarizes the results of this work and draws conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Overview of existing facial databases</head><p>There exist numerous facial databases which partially justifies the research advances for the task of face alignment. These databases exhibit large variations in resolution, image quality, identity, head pose, facial expression, lighting conditions and partial occlusion. As mentioned before, the existing databases can be split in two major categories. The first category includes databases that are captured under controlled conditions, normally within special indoor laboratories/studios in which the camera position and the lighting source and intensity can be controlled. In most of these databases, each subject is asked to perform a posed facial expression, thus we find more than one images per subject. The most popular such databases are Multi-PIE <ref type="bibr" target="#b24">[25]</ref> (used for face recognition,expression recognition, landmark points localization), FRGC-V2 <ref type="bibr" target="#b26">[27]</ref> (used for face recognition), XM2VTS <ref type="bibr" target="#b25">[26]</ref> and AR <ref type="bibr" target="#b27">[28]</ref> (both used for face recognition and landmark points localization). The facial databases of the second major category consist of images that are captured under totally unconstrained conditions (in-the-wild). In most cases, these images are downloaded from the web by making face-related queries to various search engines. The most notable databases of this category are LFPW <ref type="bibr" target="#b28">[29]</ref>, HELEN <ref type="bibr" target="#b29">[30]</ref>, AFW <ref type="bibr" target="#b17">[18]</ref>, AFLW <ref type="bibr" target="#b30">[31]</ref> and IBUG <ref type="bibr" target="#b31">[32]</ref> (all used for facial landmark points localization).</p><p>The majority of the aforementioned databases provide annotations for a relatively small subset of images. Moreover, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, they all have different annotation schemes between them, leading in different numbers of points with semantically different locations. There are also cases in which the accuracy of the provided annotations is limited. Sections 2.1 and 2.2 and Table <ref type="table" target="#tab_0">1</ref> provide an overview of the characteristics of all the commonly-used existing databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Facial databases under controlled conditions 2.1.1. Multi-PIE</head><p>The CMU Multi-Pose Illumination, and Expression (Multi-PIE) Database <ref type="bibr" target="#b24">[25]</ref> contains around 750,000 images of 337 subjects captured under laboratory conditions in four different sessions. For each subject, there are available images for 15 different poses, 19 illumination conditions and 6 different expressions (neutral, scream, smile, squint, surprise, disgust). The accompanying facial landmark annotations consist of a set of 68 points (Fig. <ref type="figure" target="#fig_0">1a</ref>) for images in the range [-45 • , 45 • ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">XM2VTS</head><p>The Extended Multi Modal Verification for Teleservices and Security applications (XM2VTS) <ref type="bibr" target="#b25">[26]</ref> database contains 2360 frontal images of 295 different subjects. Each subject has two available images for each of the four different sessions. All subjects are captured under the same illumination conditions and in the majority of images the subject has neutral expression. Facial landmark annotations of the whole database are available, where 68 points are provided for each image (Fig. <ref type="figure" target="#fig_0">1b</ref>). However, the accuracy of the annotations in some cases is limited and the locations of the provided points do not correspond to ones of Multi-PIE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">FRGC-V2</head><p>The Face Recognition Grand Challenge Version 2.0 (FRGC-V2) database <ref type="bibr" target="#b26">[27]</ref> consists of 4950 facial images of 466 different subjects. Each subject session consists of images captured under wellcontrolled conditions (i.e., uniform illumination, high resolution) and images captured under fairly uncontrolled conditions such as nonuniform illumination and poor quality. The provided annotations consist of 5 landmark points (Fig. <ref type="figure" target="#fig_0">1c</ref>) only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4.">AR</head><p>The AR Face Database <ref type="bibr" target="#b27">[28]</ref> contains over 4000 images corresponding to 126 subjects (70 males, 56 females). The images were captured in two sessions per subject and have frontal pose with variations in facial expressions, illumination conditions and occlusions (sunglasses and scarf). The images are annotated using 22 landmark points (Fig. <ref type="figure" target="#fig_0">1d</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Facial databases under in-the-wild conditions 2.2.1. LFPW</head><p>The Labeled Face Parts in the Wild (LFPW) database <ref type="bibr" target="#b28">[29]</ref> contains 1287 images downloaded from the internet (i.e., google.com, flickr.com, and yahoo.com). This database provides only the web URLs and not the actual images. We were therefore able to download only a subset of 811 out of 1100 training images and 224 out of 300 test images, due to broken links. These images contain large variations in pose, expressions, illumination conditions and occlusions. The provided ground truth annotations consist of 35 landmark points (Fig. <ref type="figure" target="#fig_0">1e</ref>) and low accuracy is observed in several cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">HELEN</head><p>The HELEN <ref type="bibr" target="#b29">[30]</ref> database consists of 2330 images downloaded from flickr.com web service, that contain a broad range of appearance variation, including pose, illumination conditions, expression, occlusion and identity. The approximate face size of each image is 500 × 500 pixels. The provided annotations are very detailed and contain 194 landmark points (Fig. <ref type="figure" target="#fig_0">1f</ref>), but the accuracy is limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">AFW</head><p>The Annotated Faces in-the-wild (AFW) <ref type="bibr" target="#b17">[18]</ref> database consists of 250 images with 468 faces, that is, more than one faces are annotated in each image. The images exhibit similar variations with those in the aforementioned in-the-wild databases. Facial landmark annotations are available for the whole database, but the annotation mark-up consists of only 6 points (Fig. <ref type="figure" target="#fig_0">1g</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4.">AFLW</head><p>The Annotated Facial Landmarks in the Wild (AFLW) <ref type="bibr" target="#b30">[31]</ref> database consists of 25,993 images gathered from Flickr, exhibiting a large variety in appearance (e.g., pose, expression, ethnicity, age, gender) as well as general imaging and environmental conditions. However, the employed annotation scheme only includes 21 landmark points (Fig. <ref type="figure" target="#fig_0">1h</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.5.">IBUG</head><p>The IBUG database was released as part of the first version of the 300-W challenge <ref type="bibr" target="#b31">[32]</ref>. It consists of 135 images downloaded from the web, with large variations in expression, illumination conditions, and pose. The provided facial landmark annotations are produced by employing the annotation scheme of Multi-PIE (Fig. <ref type="figure" target="#fig_0">1a</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Semi-automatic annotation tool</head><p>In this section, we propose a technique for semi-automatic annotation of large databases, which takes advantage of the good generalization properties of AOMs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Active orientation models</head><p>AOMs is a variant of AAMs <ref type="bibr" target="#b1">[2]</ref>. Similar to AAMs, they consist of parametric statistical shape and appearance models, and a deformation model. However, the difference is that AOMs employ kernel PCA based on a similarity criterion that is robust to outliers. Specifically, the appearance model of AOMs consists of the principal components of image gradient orientations <ref type="bibr" target="#b37">[38]</ref>, which makes them generalize well to unseen face instances.</p><p>Let us assume that we have a set of D training images, {I 1 , . . . , I D }, annotated with N landmark points that represent the ground truth shape of each image. A shape instance is defined as the 2N × 1 vector s = [x 1 , y 1 , . . . , x N , y N ] T , where (x i , y i ) are the coordinates of the i-th fiducial point. The shape model is constructed by first aligning all training shapes using Generalized Procrustes Analysis in order to remove global similarity transformations and then applying Principal Component Analysis (PCA) on the aligned shapes to retrieve: where s is the mean shape and U S consists of the first N S eigenvectors with the highest variance. A novel shape instance can be generated as:</p><formula xml:id="formula_0">s, U S ∈ R 2N×N S ,<label>( 1 )</label></formula><formula xml:id="formula_1">s = s + U S p,<label>( 2 )</label></formula><p>where p = [p 1 , . . . , p N S ] T denotes the N S × 1 vector of shape parameters. The deformation model consists of a warp function, denoted as W(p), which maps all the pixels that belong into a shape instance generated from Eq. ( <ref type="formula" target="#formula_1">2</ref>) with parameters p to their corresponding locations in the mean shape s. We employ the Piecewise Affine Warp, which evaluates the mapping using the barycentric coordinates of the triangles extracted with Delaunay triangulation.</p><p>The appearance model of an AOM is based on normalized gradients <ref type="bibr" target="#b37">[38]</ref>. Let us denote an image in vectorial form as i with size L × 1, thus L is the number of pixels. Moreover, we denote g x and g y to be the image gradients and 0 = arctan(g x /g y ) the corresponding gradient orientation vector. The normalized gradient extraction function is defined as:</p><formula xml:id="formula_2">Z(i) = 1 √ L [cos 0 T , sin 0 T ] T ,<label>( 3 )</label></formula><p>where cos 0 = [cos0(1), . . . , cos 0(L)] T and sin 0 = [sin 0(1), . . . , sin 0(L)] T . By employing the deformation model, we can define the shape-free normalized gradients of an image i as the 2L Z × 1 vector:</p><formula xml:id="formula_3">z(p) ≡ Z(i(W(p))), (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where L Z is the number of pixels that belong to the mean shape s, which has the role of the reference shape.  we construct an appearance model of the form:</p><formula xml:id="formula_5">U Z ∈ R 2L Z ×N Z , (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where U Z stores the first N Z eigenvectors with the highest variance. Note that in order to preserve the robust property of the normalized gradient kernel, we don't subtract the mean appearance vector from the training set, so it ends up as the first eigenvector. A novel appearance instance can be generated as:</p><formula xml:id="formula_7">z = U Z c,<label>( 6 )</label></formula><p>where c = [c 1 , . . . , c N Z ] T denotes the N Z × 1 vector of appearance parameters.</p><p>Given a testing image t in vectorized form and the trained shape, appearance and deformation models, the fitting procedure aims to minimize:</p><formula xml:id="formula_8">arg min p,c z(p) -U Z c 2 , (<label>7</label></formula><formula xml:id="formula_9">)</formula><p>where z(p) denotes the normalized gradients of t, as defined in Eq. ( <ref type="formula" target="#formula_3">4</ref>). This optimization can be efficiently solved in an inverse compositional alternating manner, as shown in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b38">39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Method</head><p>The main idea behind the proposed tool is to take advantage of the generalization qualities of AOMs by building a model using annotated images with various poses and expressions and generate the annotations on images with different poses and expressions. Specifically, let us denote a database that consists of N subj subjects as DB. We assume that for each subject, images with different expressions {E j }, j ∈ {1, 2, . . . , N exp }, and poses {P k }, k ∈ {1, 2, . . . , N pos } are available. Let V be a subset of annotated images and Q a subset of non-annotated images of DB. The goal of our tool is to (1) generate annotations for the subjects in Q which appear in V with different expressions and poses, and (2) generate annotations for the subjects of Q that are not included in V. For example, in Multi-PIE, the annotations for subjects with expressions "disgust" at 0 • and "neutral" at 15 • are provided and we want to produce the annotations for subjects with expression "disgust" at 15 • . In this case the annotated and non-annotated subsets are defined as</p><formula xml:id="formula_10">V = {Disgust, 0 • , Neutral, 15 • } and Q = {Disgust, 15 • }, respectively.</formula><p>In order to annotate the images in Q, we first train an AOM using the images in V. The trained model is employed within an iterative fitting procedure which aims to augment the set of correctly annotated images in V and build a more powerful AOM. Specifically, we fit the trained AOM to each image in Q and manually classify the fitting results into two sets: Good denoted as Q and Bad denoted as W = Q\Q. After this procedure is completed, the initial set of annotated images is augmented with Q, i.e. V ← V ∪Q, a new AOM is built using the updated V and the fitting procedure is repeated. This iterative process is repeated until the cardinality of the subset W has not changed between two consecutive iterations i.e., |W t |-|W t-1 | == 0, thus we end up with fitting results for all the images in Q. Note that we employ DPMs <ref type="bibr" target="#b17">[18]</ref> to estimate the initial landmark locations for the first iteration of the above procedure.</p><p>In case Q has multiple images per subject (e.g. Multi-PIE, XM2VTS, FRGC-V2, AR), the above method can be extended to further improve the generated annotations. Specifically, let us assume that we have a subset of images for each subject Q p ⊆ Q with N p number of images each, where p ∈ {1, 2, . . . , N subj }. For each such subset, we build and fit a Person Specific Model (PSM) <ref type="bibr" target="#b39">[40]</ref> in an "one-vs-rest" manner, that is we fit each image i ∈ Q p using the PSM trained on the rest N p -1 images. This person-specific adaptation further improves the results, especially since we employ person-specific AOMs. The generated annotations of the images in Q can be further manually improved, as a final step, although the above methodology ensures that minor corrections will be required. Fig. <ref type="figure" target="#fig_1">2</ref> and Algorithm 1 present the flowchart and pseudocode, respectively, of the proposed semi-automatic annotation technique. Finally, the above method can be readily applied to annotate a database DB 1 using an already annotated database DB 2 by setting V = DB 2 and Q = DB 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Annotations</head><p>In this section, we present how the proposed tool was used in order to re-annotate the databases presented in Section 2. The advantages of the generated annotations 1 are twofold: (1) They all have the same landmark configuration, i.e. the one employed in Multi-PIE (Fig. <ref type="figure" target="#fig_0">1a</ref>), and (2) in many cases they are more accurate than the original ones.  ] and multiple non-neutral expressions with pose 0 • . We employed the proposed tool to annotate 12,570 images for 6 expressions, all 337 subjects and poses in range [-30 • , 30 • ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Semi-automatic database annotation tool</head><formula xml:id="formula_11">Require: Annotated subset V, Non-annotated subset Q Ensure: Annotations of Q 1: Initialize landmarks locations of Q. 2: Initialize Q 1 = ∅, V 1 = V, and W 1 = Q. 3: t =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">XM2VTS</head><p>The images of XM2VTS's first session were semi-automatically annotated by setting V to be the subjects of Multi-PIE with neutral expression and [-15 • , 15 • ] poses. Subsequently, the annotated images of the first session were employed to annotate the images of the second session, and so on for all four available sessions. This procedure resulted in annotating 2360 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">AR</head><p>A procedure similar to the one used for XM2VTS was used to generate annotations for the neutral images of AR. For images having a specific expression E j , we used the annotated neutral images of AR and the images with the corresponding expression and frontal pose from the Multi-PIE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4.">FRGC-V2</head><p>In the case of FRGC-V2, we first annotated a subset consisting of two images per subject with two different illumination conditions. This subset was annotated by employing images from Multi-PIE with six expressions and [-15 • , 15 • ] poses as V. The rest of FRGC-V2 was annotated using this initial subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.5.">LFPW</head><p>Since LFPW database does not provide information regarding pose and expression characteristics for any image, we manually clustered the images in different poses {P k } in the range [-30 • , 30 • ]. The images of each such pose cluster were semi-automatically annotated using images from Multi-PIE with the same pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.6.">HELEN, AFW, IBUG</head><p>The rest of in-the-wild databases were annotated using a common procedure. Specifically, Q consisted of the non-annotated database DB i , and V was set equal to all the rest annotated in-thewild databases DB j , j = {1, 2, . . . , i -1}. Fig. <ref type="figure">3</ref> shows examples for each database with the original annotations and the annotations produced using the proposed semiautomatic methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Efficiency</head><p>In order to assess the efficiency of the semi-automatic tool, we conducted the following experiment. We used the testing set of Helen database (330 images) as the annotated subset V while the non-annotated set Q was formed by randomly selecting 1450 images from the training set of the same database. Note that the selected images exhibit significant variations in pose, illumination conditions, and occlusion. Then, we applied the proposed semi-automatic tool in order to generate the annotations. Fig. <ref type="figure">4</ref> visualizes the cardinality of W and V at each iteration until the termination of the procedure. The tool managed to generate annotations of good quality for 1393 out of 1450 images. The annotations for the rest 57 images were not adequately good mainly due to the existence of extreme poses, occlusions and illumination conditions. Given that an expert human annotator needs around 5 min to manually annotate from scratch one image, we have to spend 7250 min in order to annotate all the images. Instead, by using the proposed tool we dropped the requirement time for the creation of annotations in 1671 min. More specifically, we spent 820 min for the manual classification of fittings in Good and Bad, 549 min in order to refine the automatically created 1393 annotations, and 285 min for the manually annotation of the rest 57 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Discussion</head><p>In order to assess the variance of the manually annotated landmarks, we considered the simplest case of annotating images with frontal faces without any occlusion or expression. To this end, we selected such images of N = 80 different subjects with frontal pose from the Multi-PIE database. All these images were manually annotated by three expert annotators. Fig. <ref type="figure" target="#fig_3">5</ref> plots the variance of the manual annotations for each landmark point using an ellipse. Note that the ellipses are colored based on the standard deviation of the annotations, normalized by the size of the face.</p><p>This experiment shows that the agreement level among the annotators is high for the landmarks that correspond to the eyes and mouth. This is due to the fact that these landmarks are located to facial features which are very distinctive across all human faces. Instead, the standard deviation is high for landmarks that do not have a clear semantic meaning. The chin is the most characteristic example of this category, as it demonstrates the highest variance. Finally, the result of this experiment suggests that it is more reliable to report the performance of landmark localization techniques using the 51-point mark-up (after removing the points of the face's boundary), as done in both 300-W competitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">300 Faces In-The-Wild Challenge</head><p>In this section, we present the 300 Faces In-The-Wild Challenge (300-W), the first facial landmark localization challenge that was held twice, in 2013 and 2015. The ultimate goal of the challenge is to provide a fair comparison between different automatic facial landmark detection methods. To this end, the 300-W database was collected and annotated using the same unified annotation scheme described in Section 3, in order to be used as testing set. Section 4.1 gives more details about the database and Sections 4.2 and 4.3 analyse the results of the two conducts of the competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">300-W database</head><p>The 300-W database 4 is a newly-collected challenging dataset that consists of 300 Indoor and 300 Outdoor in-the-wild images. It covers a large variation of identity, expression, illumination conditions, pose, occlusion and face size. The images were downloaded from google.com by making queries such as "party", "conference", "protests", "football" and "celebrities". Compared to the rest of inthe-wild datasets, the 300-W database contains a larger percentage of partially-occluded images and covers more expressions than the common "neutral" or "smile", such as "surprise" or "scream". We annotated the images with the 68-point mark-up of Fig. <ref type="figure" target="#fig_0">1a</ref>, using the semi-automatic methodology presented in Section 3. The images of the database were carefully selected so that they represent a characteristic sample of challenging but natural face instances under totally unconstrained conditions. Thus, methods that achieve accurate performance on the 300-W database can demonstrate the same accuracy in most realistic cases. Consequently, the experimental results on this database indicate how far the research community is from an adequately good solution to the problem of automatic facial landmark localization.</p><p>Table <ref type="table" target="#tab_4">2</ref> summarizes the characteristics of the database. Many images of the database contain more than one annotated faces (293 images with 1 face, 53 images with 2 faces and 53 images with <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref> faces). Consequently, the database consists of 600 annotated face instances, but 399 unique images. Finally, there is a large variety of face sizes. Specifically, 49.3% of the faces have size in the range [48.6k, 2.0M] and the overall mean size is 85k (about 292 × 292) pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">300-W challenge: first conduct (2013)</head><p>The first conduct of the 300-W challenge 2 was held in conjunction with IEEE International Conference on Computer Vision (ICCV) in 2013 <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Training</head><p>LFPW, AFW, HELEN, XM2VTS and FRGC-V2 were provided for training, along with the corrected annotations produced with the semi-automatic annotation tool (Section 3). The fact that only a very small proportion of images in LFPW and HELEN have expressions different than "smile" motivated us to collect and annotate Please cite this article as: C. Sagonas, E. Antonakos, G. Tzimiropoulos, S. Zafeiriou, M. Pantic, 300 Faces In-The-Wild Challenge: database and results, Image and Vision Computing (2016), http://dx.doi.org/10.1016/j.imavis.2016.01.002 the IBUG database. It consists of 135 images with highly expressive faces under challenging poses and was provided to the participants as an additional option for training. Furthermore, we computed the bounding boxes of all the aforementioned databases by using our in-house face detector, the one that is also employed in <ref type="bibr" target="#b16">[17]</ref>, which is a variant of <ref type="bibr" target="#b17">[18]</ref>. Both the annotations and the bounding boxes were made publicly available at the challenge's website 2 . Note that the participants were encouraged but not restricted to use only the provided training sets and annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Testing</head><p>To ensure a fair comparison between the submitted methodologies, participants did not have access to the 300-W testing database. They were requested to send us the compiled (binary) files of their pre-trained systems. On our behalf, we extracted the face's bounding box for each of the testing images using the same methodology as the one employed for the training images 4 . These bounding boxes were passed in to the submitted executables as initializations. The accuracy of the fitting results was measured by the point-to-point RMS error between each fitted shape and the ground truth annotations, normalized by the face's interoccular distance, as proposed in <ref type="bibr" target="#b17">[18]</ref>. Specifically, by denoting the fitted and ground truth shapes</p><formula xml:id="formula_12">as s f = [x f 1 , y f 1 , . . . , x f N , y f N ] T and s g = [x g 1 , y g 1 , . . . , x g N , y</formula><p>g N ] T respectively, then the error between them is computed as: where d outer is the interoccular distance computed as the Euclidean distance between the outer points of each eye, as shown in Fig. <ref type="figure">6</ref>.</p><formula xml:id="formula_13">RMSE = N i=1 x f i -x g i 2 + y f i -y g i 2 d outer N , (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>For the employed landmark configuration of Fig. <ref type="figure" target="#fig_0">1a</ref>, the interoccular distance is defined as d outer = x </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Participants</head><p>In total, there were six participants in this version of the challenge. Below is a brief description of the submitted methods:</p><p>• Baltrusaitis et al. <ref type="bibr" target="#b40">[41]</ref> propose a probabilistic patch expert technique that learns non-linear and spatial relationships between the pixels and the probability of a landmark being aligned. To fit the model they propose a novel non-uniform regularized landmark mean-shift optimization technique which takes into account the reliabilities of each patch expert. • Hasan et al. <ref type="bibr" target="#b41">[42]</ref> first apply a nearest neighbor search using global descriptors and, then, aim to align local neighbors by dynamically fitting a locally linear model to the global keypoint configurations of the returned neighbors. Neighbors are also used to define restricted areas of the input image in which they apply local discriminative classifiers. Finally, an energy minimization approach is applied in order to combine the local classifier predictions with the dynamically estimated joint keypoint configuration model. • Jaiswal et al. <ref type="bibr" target="#b42">[43]</ref> use Local Evidence Aggregated Regression <ref type="bibr" target="#b43">[44]</ref>, in which local patches provide evidence of the location of the target facial point using Support Vector Regressors. • Milborrow et al. <ref type="bibr" target="#b44">[45]</ref> approach the problem with Active Shape Models (ASMs) that incorporate a modified version of SIFT descriptors <ref type="bibr" target="#b45">[46]</ref>. They employ multiple ASMs and utilize the one that best estimates the face's yaw pose. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>multiple hypotheses and learn to rank or combine them in order to get the final results. They estimate the parameters in both "learn to rank" and "learn to combine" using a structural Support Vector Machine framework. • Zhou et al. <ref type="bibr" target="#b47">[48]</ref> propose a four-level Convolutional Network</p><p>Cascade, where each level is trained to locally refine the outputs of the previous network levels. Moreover, each level predicts an explicit geometric constraint (face region and component position) to rectify the inputs of the next levels, which improves the accuracy and robustness of the whole network structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Results</head><p>The performance of the submitted systems was assessed based on both the 68 and 51 points. As shown in Fig. <ref type="figure">6</ref>, the 51 points are a subset of the 68 points after removing the 17 points of the face's boundary. Fig. <ref type="figure">7</ref> shows the Cumulative Error Distribution (CED) curves using the error metric of Eq. ( <ref type="formula" target="#formula_13">8</ref>). The plots are divided based on the number of points (68 and 51) as well as the image subsets (Indoor, Outdoor and Indoor + Outdoor). Table <ref type="table" target="#tab_6">3</ref> reports the median absolute deviation of the results and Fig. <ref type="figure" target="#fig_8">10</ref> shows some indicative fitting shapes.</p><p>All methodologies demonstrated a lower performance on Outdoor scenes. The main reason for this is the illumination condition variance which is much smaller within an Indoor environment. However, another factor affecting the performance is that the Outdoor images have larger variation in facial expressions compared to the Indoor ones. This is because we picked specific keywords for the selection of Outdoor images, such as "sports" and "protest", which ended up in a big number of images with various expressions, such as "surprise" and "scream", that are much more challenging than the expressions that are commonly seen in the Indoor ones, such as "smile" and "neutral". We decided to announce two winners: one from an academic institution and one from industry. Based on the results, the winners were (a) Yan et al. <ref type="bibr" target="#b46">[47]</ref> from The National Laboratory of Pattern Recognition at the Institute of Automation of the Chinese Academy of Sciences, and (b) Zhou et al. <ref type="bibr" target="#b47">[48]</ref> from Megvii company. It is worth to mention that all groups achieved better results in the case of 51 points.</p><p>In order to show whether there is any room for further improvement on the performance, we also report an Oracle curve. We built a statistical shape model using the shapes of the training databases, as explained in Eq. ( <ref type="formula" target="#formula_0">1</ref>), and kept the first 25 components. Using this model, we compute and plot the reconstruction error for each shape of the 300-W database. The reconstruction of a shape s is achieved by first projecting as p r = U T S (s -s), and then reconstructing as s r = s + U S p r . The resulting curve shows that the 300-W dataset is not saturated and there is considerable room for further improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">300-W challenge: second conduct (2015)</head><p>The second conduct of the 300-W challenge was completed in the beginning of 2015. The biggest difference compared to the previous conduct is that we were no longer providing the bounding boxes of the images to the fitting methods. On the contrary, the participants were required to submit systems that perform both face detection and landmark localization. The three main reasons that led us to this change are:</p><p>1. Various techniques perform differently when initialized with bounding boxes that cover different facial regions. For example, DPMs <ref type="bibr" target="#b17">[18]</ref> tend to return bounding boxes that only include facial texture and not any of the subject's hair, as usually done by the Viola-Jones detector <ref type="bibr" target="#b48">[49]</ref>.  <ref type="bibr" target="#b41">[42]</ref> 0.0543 0.0551 Jaiswal et al. <ref type="bibr" target="#b42">[43]</ref> 0.0527 0.0506 Milborrow et al. <ref type="bibr" target="#b44">[45]</ref> 0.1126 0.1145 Yan et al. <ref type="bibr" target="#b46">[47]</ref> 0.0211 0.0199 Zhou et al. <ref type="bibr" target="#b47">[48]</ref> 0.0205 0.0182 Oracle 0.0038 0.0040 2. There are methods, like DPMs <ref type="bibr" target="#b17">[18]</ref> and Pictorial Structures <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, that do not require any initialization. 3. There are algorithms for which the training is coupled with the face detector, such as SDM <ref type="bibr" target="#b18">[19]</ref>.</p><p>Of course, this change made the task even more challenging than before, since the search region of each image became much larger with a lot of background information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">300-W images cropping</head><p>As mentioned in Section 4.1, many of the 300-W images contain more than one faces, which are not necessarily annotated. Consequently, we cropped the images so that they all included only one face. The cropping was performed in such a way to ensure that (1) only a single face is included in each image and (2) DPMs <ref type="bibr" target="#b17">[18]</ref> and Viola-Jones <ref type="bibr" target="#b48">[49]</ref> achieve the best true positive rate that they possibly can. Table <ref type="table" target="#tab_7">4</ref> reports the characteristics of the cropped images. Naturally, the only thing that changes compared to the ones of the initial images in Table <ref type="table" target="#tab_4">2</ref> is the image size (resolution). The mean size of the cropped images is 0.4 M pixels, which is much smaller than the 3.3 M pixels of the non-cropped images. Fig. <ref type="figure" target="#fig_5">8</ref> shows some representative examples of the way that the images were cropped. Note that the cropped images are provided along with the original images of the 300-W database 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Training</head><p>The training instructions were the same as in the previous conduct. The authors were encouraged, but not restricted, to use LFPW, AFW, HELEN, IBUG, FRGC-V2 and XM2VTS databases with the provided annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Testing</head><p>The testing procedure followed the same rules as in the previous version of the challenge. The participants were required to submit compiled pre-trained systems, the performance of which was evaluated using the metric of Eq. ( <ref type="formula" target="#formula_13">8</ref>). The submitted systems could return nothing in case no face was detected or the detected face was estimated to be a false positive. Consequently, in order to facilitate the participants and make the competition less dependent to a face detector's performance, we suggested them to use one of the face detection methods that took part in the Face Detection Data Set and Benchmark (FDDB) <ref type="bibr" target="#b49">[50]</ref>. Finally, in this conduct of the competition, the submitted methods were also assessed with respect to their computational costs and a maximum limit of 2 min per image was typically set.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4.">Participants</head><p>In total, there were five participants in this version of the challenge. Below is a brief description of the submitted methods:</p><p>• Čech et al. <ref type="bibr" target="#b50">[51]</ref> propose an algorithm where the sum of individual landmark scoring functions, that are trained by a structured output SVM classifier, is maximized with respect to the camera pose by fitting a parametric 3D shape model. They explicitly handle self-occlusions by excluding the corresponding contributions from the data term, which allows their algorithm to operate correctly for a large range of viewing angles. Their landmark localization procedure is initialized with a manually-engineered framework that involves a commercial face detector and the estimation of the initial 3D pose. • Deng et al. <ref type="bibr" target="#b51">[52]</ref> use a multi-view, multi-scale and multicomponent cascade shape regression model. Their model learns view-specific cascaded shape regressors using multiscale HOG features as shape-index features and is optimized with a multi-scale strategy that eliminates the risk of getting stuck on local minima. • Fan et al. <ref type="bibr" target="#b52">[53]</ref> approach the problem by proposing a deep learning system that consists of a cascade of multiple Convolutional Neural Networks (CNNs) that are optimized in a coarse-to-fine strategy in order to improve accuracy. Their fitting procedure is initialized with their implementation of the Viola-Jones face detector <ref type="bibr" target="#b48">[49]</ref>. • Martinez et al. <ref type="bibr" target="#b53">[54]</ref> employ a cascaded regression method that makes use of the L 2,1 norm in order to increase the robustness to poor initializations or partial occlusions compared to the commonly used least-squares regressor. They also attempt to improve the results by using multiple initializations with different spatial translations and four different head pose rotations.</p><p>• Uřičář et al. <ref type="bibr" target="#b54">[55]</ref> propose a real-time multi-view methodology based on DPMs. They utilize different view-specific models in order to deal with the problem of self-occlusions and cover a large range of head poses. The model parameters are learned through structured output SVM. The dynamic programming optimization is performed in a coarse-to-fine search strategy that allows real-time performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.5.">Results</head><p>The number of participants in this version of the competition was 5. Fig. <ref type="figure" target="#fig_7">9</ref> shows the CED curves of the submitted methodologies. Table <ref type="table" target="#tab_8">5</ref> reports the number of images for which an estimation of the landmarks was returned, the mean absolute deviation of the results, as well as the mean computational costs. The common subset of images for which all methods returned a detection consists of 519 images. Fig. <ref type="figure" target="#fig_11">11</ref> shows some indicative fitting results.</p><p>Similar to the first conduct of the competition, we selected to announce two winners: one from academia and one from industry. Based on the results, the winners of the second conduct of the competition are (a) Deng et al. <ref type="bibr" target="#b51">[52]</ref> from the B-DAT Laboratory at the Nanjing University of Information and Technology, and (b) Fan et al. <ref type="bibr" target="#b52">[53]</ref> from Megvii company. Even though the technique of Fan et al. <ref type="bibr" target="#b52">[53]</ref> is slightly more accurate, it returns results for 526 images, as opposed to the one of Deng et al. <ref type="bibr" target="#b51">[52]</ref> that detects the landmarks in 599 images and has a small mean absolute deviation. It is worth to notice that some systems employed an unreliable face detector. This seems to be the case with Čech et al. <ref type="bibr" target="#b50">[51]</ref> and Uřičář et al. <ref type="bibr" target="#b54">[55]</ref>. Their systems returned an output for about 98.5% of the images, however about 50% of the images resulted with very high fitting error partially because of false positive face detections. This is the reason why their mean absolute deviation is high. Moreover, only the submission by Martinez et al. <ref type="bibr" target="#b53">[54]</ref>   not the most accurate ones, they are very robust with small mean absolute deviations. Finally, the systems of Deng et al. <ref type="bibr" target="#b51">[52]</ref> and Fan et al. <ref type="bibr" target="#b52">[53]</ref> were also the fastest ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and conclusions</head><p>The results of both conducts of the 300-W challenge shown in Figs. <ref type="figure">7</ref> and<ref type="figure" target="#fig_7">9</ref> clearly prove that, even though much progress has been made during the last years, the research community is still far from accurately solving the problem of face alignment and that there is much room for further improvement. This is indicated by the gap between the participants' curves and the Oracle, which is the minimum error that can be achieved using the specific training databases. Table <ref type="table" target="#tab_9">6</ref> reports the percentage of images with error less than {0.02, 0.03, 0.04, 0.05, 0.06} for the top techniques of both competitions as well as the Oracle and makes it obvious that the gap is still huge, especially for small error values.</p><p>Table <ref type="table" target="#tab_9">6</ref> also shows that there was a small improvement on the state-of-the-art performance between the first and the second conduct of the challenge. The top performing methodologies have relatively small differences and are close to each other. One of the main reasons behind this progress is the plethora of training data from which discriminative methods can greatly benefit. For example, techniques like Yan et al. <ref type="bibr" target="#b46">[47]</ref> (cascade regression framework) and Zhou et al. <ref type="bibr" target="#b47">[48]</ref> (convolutional network framework), can continually achieve better results with continuous rise in the amount of training data.</p><p>Additionally, the 300-W challenge was only focused on the task of sparse facial landmark points detection. Alignment using dense Please cite this article as: C. Sagonas, E. Antonakos, G. Tzimiropoulos, S. Zafeiriou, M. Pantic, 300 Faces In-The-Wild Challenge: database and results, Image and Vision Computing (2016), http://dx.doi.org/10.1016/j.imavis.2016.01.002</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>(a) Baltrusaitis et al. <ref type="bibr" target="#b40">[41]</ref> (b) Hasan et al. <ref type="bibr" target="#b41">[42]</ref> (c) Jaiswal et al. <ref type="bibr" target="#b42">[43]</ref> (d) Milborrow et al. <ref type="bibr" target="#b44">[45]</ref> (e) Yan et al. <ref type="bibr" target="#b46">[47]</ref> (f) Zhou et al. <ref type="bibr" target="#b47">[48]</ref> (g) Ground truth landmark mark-ups is much more difficult and the performance would get worse. This is because the more landmarks exist in the shape, there is more ambiguity about the semantic locations at which they are located. Consider for example the 41 boundary landmark points of the HELEN mark-up in Fig. <ref type="figure" target="#fig_0">1f</ref>. Their locations have no special semantic discrimination. On the contrary they are just located with an approximately equal distance between them. Consequently, it is very hard to accurately detect such points since there is no discriminative texture information that describes them and which could drive the fitting procedure. This highlights the need to further research how to select a relatively high number of landmark points that are capable to describe all the characteristic areas of an object.  performance. The results presented in the Face Detection Data Set and Benchmark (FDDB) <ref type="bibr" target="#b49">[50]</ref> show that current state-of-the-art techniques achieve very good true positive rates. However, there is still room for further improvement especially on images with in-the-wild conditions. Finally, most current research effort focuses on detecting the facial landmarks and not tracking them within video sequences. We strongly believe that more attention should be given towards developing techniques that can track facial points in a robust manner,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>even under difficult conditions such as camera movement, disappearance and re-appearance of the face, challenging background and lighting, etc. Consequently, we believe that one promising step towards this direction would be the organization of a challenge, similar to the 300-W one, that focuses on facial landmark points tracking. The biggest difficulty of such a competition would be the annotation of the thousands of frames of the videos. However, using semi-automatic annotation tools as the one proposed in this paper, the task would be simplified and annotations could be efficiently generated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Landmark configurations of existing databases. Note they all have different numbers of landmark points with semantically different locations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Flowchart of the proposed tool. Given a set of landmarked images V with various poses and expressions, we aim to annotate a set of non-annotated images Q (1) with the same subjects and different poses and expressions, or (2) with different subjects but similar pose and expressions. warped normalized gradients of the training images, i.e. {z 1 , . . . , z D },</figDesc><graphic coords="5,96.02,58.02,413.40,116.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Examples of the annotated images. For each database, the image on the left has the original annotations and the one on the right shows the annotations generated by the proposed tool. Note that in the case of Multi-PIE, even though the original and generated annotations have the same configuration, the generated ones are more accurate.</figDesc><graphic coords="6,108.20,283.86,368.71,416.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Each ellipse denotes the variance of each landmark point with regard to three expert human annotators. The colors of the points rank them with respect to their standard deviation normalized by the face size. (For interpretation of the references to color in this figure, the reader is referred to the web version of this article.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. The 51-point mark-up is a subset of the 68-points one after removing the 17 points of the face's boundary. The interoccular distance is defined between the outer points of the eyes.</figDesc><graphic coords="8,341.24,459.06,171.77,250.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Indicative examples of the way the images were cropped for the second conduct of the 300-W challenge.</figDesc><graphic coords="11,132.02,58.14,340.69,265.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>managed to return a detection for all 600 images and the results indicate that even though their methodologies are Please cite this article as: C. Sagonas, E. Antonakos, G. Tzimiropoulos, S. Zafeiriou, M. Pantic, 300 Faces In-The-Wild Challenge: database and results, Image and Vision Computing (2016), http://dx.doi.org/10.1016/j.imavis.2016.01.002 -point Normalized RMS Error (a) Indoor + Outdoor, 68 points (b) Indoor + Outdoor, 51 points (c) Indoor, 68 points (d) Indoor, 51 points (e) Outdoor, 68 points (f) Outdoor, 51 points</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Fitting results of the second conduct of the 300-W challenge in 2015. The plots show the Cumulative Error Distribution (CED) curves with respect to the landmarks (68 and 51 points) and the conditions (indoor, outdoor or both).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Fitting examples of the first conduct of the 300-W challenge in 2013. Each row shows the fitted landmarks for each participating method.</figDesc><graphic coords="13,118.04,58.20,368.71,528.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Moreover, another factor that contributed towards creating more accurate and efficient alignment techniques is the great progress in the task of face detection. Most landmark localization methodologies are very sensitive to the initialization, thus the face detection Please cite this article as: C. Sagonas, E. Antonakos, G. Tzimiropoulos, S. Zafeiriou, M. Pantic, 300 Faces In-The-Wild Challenge: database and results, Image and Vision Computing (2016), http://dx.doi.org/10.1016/j.imavis.2016.01.002</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(a) Cech et al.<ref type="bibr" target="#b50">[51]</ref> (b) Deng et al.<ref type="bibr" target="#b51">[52]</ref> (c) Fan et al.<ref type="bibr" target="#b52">[53]</ref> (d) Martinez et al.<ref type="bibr" target="#b53">[54]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Fitting examples of the second conduct of the 300-W challenge in 2015. Each row shows the fitted landmarks for each participating method.</figDesc><graphic coords="14,108.20,258.54,368.74,450.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,122.72,58.50,340.25,256.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Overview of the characteristics of existing facial databases.</figDesc><table><row><cell>Database</cell><cell>Conditions</cell><cell># of faces</cell><cell># of subjects</cell><cell># of points</cell><cell>Pose</cell></row><row><cell>Multi-PIE</cell><cell></cell><cell cols="2">∼ 750, 000 337</cell><cell>68</cell><cell>[-45 • , 45 • ]</cell></row><row><cell>XM2VTS</cell><cell>Controlled</cell><cell>2360</cell><cell>295</cell><cell>68</cell><cell>0 •</cell></row><row><cell>FRGC-V2</cell><cell></cell><cell>4950</cell><cell>466</cell><cell>5</cell><cell>0 •</cell></row><row><cell>AR</cell><cell></cell><cell>∼ 4000</cell><cell>126</cell><cell>22</cell><cell>0 •</cell></row><row><cell>LFPW</cell><cell></cell><cell>1035</cell><cell></cell><cell>35</cell><cell></cell></row><row><cell>HELEN</cell><cell></cell><cell>2330</cell><cell></cell><cell>194</cell><cell></cell></row><row><cell>AFW</cell><cell cols="2">In-the-wild 468</cell><cell>-</cell><cell>6</cell><cell>[-45 • , 45 • ]</cell></row><row><cell>AFLW</cell><cell></cell><cell>25,993</cell><cell></cell><cell>21</cell><cell></cell></row><row><cell>IBUG</cell><cell></cell><cell>135</cell><cell></cell><cell>68</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.3.1. Multi-PIE The available Multi-PIE annotations cover only the neutral expression with pose [-45 • , 45</figDesc><table><row><cell>1.</cell></row><row><cell>4: repeat</cell></row><row><cell>5: Train an AOM using Vt .</cell></row><row><cell>6: Fit the AOM to Wt .</cell></row><row><cell>7: Manually classify the fittings to Qt (Good) and W t+1 = Wt Qt (Bad).</cell></row><row><cell>8: Update V t+1 ← Vt ∪ Qt .</cell></row><row><cell>9: t → t + 1.</cell></row><row><cell>10: until|Wt| -|W t-1 | == 0</cell></row><row><cell>11: if multiple images per subject in Q.</cell></row></table><note><p><p><p><p><p><p><p>then 12: for each subject p = 1, 2, . . . , N subj do</p>13:</p>Qp ⊆ Q is the subset with the Np images of the subject. 14:</p>for each image i ∈ Qpdo 15:</p>Train a person-specific AOM using Qp {i}.</p>16:</p>Fit the person-specific AOM to the image i. 17: end for 18: end for 19: end if 20: Check and manually correct, if necessary, the generated annotations of Q. Please cite this article as: C. Sagonas, E. Antonakos, G. Tzimiropoulos, S. Zafeiriou, M. Pantic, 300 Faces In-The-Wild Challenge: database and results, Image and Vision Computing (2016), http://dx.doi.org/10.1016/j.imavis.2016.01.002 ARTICLE IN PRESS 3</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>Overview of the characteristics of the 300-W database.</figDesc><table><row><cell></cell><cell>Indoor</cell><cell>Outdoor</cell></row><row><cell># of faces</cell><cell>300</cell><cell>300</cell></row><row><cell># of images</cell><cell>222</cell><cell>177</cell></row><row><cell>Image size (range in pixels)</cell><cell></cell><cell></cell></row></table><note><p><p>[20.3k, 17</p>.3M] [27.2k, 21.0M] Face size (range in pixels) [5.0k, 0.8M] [4.7k, 2.0M] Interoccular distance (range in pixels) [42, 477] [39, 805]</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc>Median absolute deviation of the fitting results of the first conduct of 300-W challenge in 2013, reported for both 68 and 51 points.</figDesc><table><row><cell>Participant</cell><cell>68 points</cell><cell>51 points</cell></row><row><cell>Baltrusaitis et al. [41]</cell><cell>0.0486</cell><cell>0.0388</cell></row><row><cell>Hasan et al.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4</head><label>4</label><figDesc>Overview of the characteristics of the cropped images of the 300-W database.</figDesc><table><row><cell>Indoor</cell><cell>Outdoor</cell></row></table><note><p><p>Please cite this article as: C. Sagonas, E. Antonakos, G. Tzimiropoulos, S. Zafeiriou, M. Pantic, 300 Faces In-The-Wild Challenge: database and results, Image and Vision Computing (2016), http://dx.doi.org/10.1016/j.imavis.2016.01.002</p>ARTICLE IN PRESS</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5</head><label>5</label><figDesc>Second conduct of the 300-W challenge. 2nd column: Number of images for which an estimation of the landmarks was returned. 3rd and 4th columns: The mean absolute deviation of the fitting results for both 68 and 51 points. 5th column: Mean computational cost per method.</figDesc><table><row><cell>Participant</cell><cell># of images</cell><cell>mad</cell><cell></cell><cell>Timings (secs)</cell></row><row><cell></cell><cell>with detection</cell><cell>68 points</cell><cell>51 points</cell><cell></cell></row><row><cell>Čech et al. [51]</cell><cell>591 (98.5%)</cell><cell>0.1047</cell><cell>0.0998</cell><cell>4.05</cell></row><row><cell>Deng et al. [52]</cell><cell>599 (99.8%)</cell><cell>0.0226</cell><cell>0.0213</cell><cell>1.97</cell></row><row><cell>Fan et al. [53]</cell><cell>526 (87.7%)</cell><cell>0.0309</cell><cell>0.0294</cell><cell>1.29</cell></row><row><cell cols="2">Martinez et al. [54] 600 (100%)</cell><cell>0.0514</cell><cell>0.0497</cell><cell>42.5</cell></row><row><cell>Uřičář et al. [55]</cell><cell>592 (98.7%)</cell><cell>0.0970</cell><cell>0.0945</cell><cell>3.46</cell></row><row><cell>Oracle</cell><cell>-</cell><cell>0.0038</cell><cell>0.0040</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6</head><label>6</label><figDesc>Percentage of images with fitting error less than the specified values for the winners of the first (Yan et al.<ref type="bibr" target="#b46">[47]</ref>, Zhou et al.<ref type="bibr" target="#b47">[48]</ref>) and second (Deng et al.<ref type="bibr" target="#b51">[52]</ref>, Fan et al.<ref type="bibr" target="#b52">[53]</ref>) 300-W challenges, and Oracle. The error is based on 68 points using both indoor and oudoor images.</figDesc><table><row><cell>Method</cell><cell>&lt;0.02</cell><cell>&lt;0.03</cell><cell>&lt;0.04</cell><cell>&lt;0.05</cell><cell>&lt;0.06</cell></row><row><cell>Yan et al. [47]</cell><cell>0.17%</cell><cell>4.17%</cell><cell>25.8%</cell><cell>54.0%</cell><cell>71.0%</cell></row><row><cell>Zhou et al. [48]</cell><cell>0%</cell><cell>2.50%</cell><cell>20.7%</cell><cell>47.7%</cell><cell>69.2%</cell></row><row><cell>Deng et al. [52]</cell><cell>0.17%</cell><cell>4.33%</cell><cell>26.8%</cell><cell>55.5%</cell><cell>74.3%</cell></row><row><cell>Fan et al. [53]</cell><cell>0.33%</cell><cell>14.3%</cell><cell>38.2%</cell><cell>62.0%</cell><cell>75.2%</cell></row><row><cell>Oracle</cell><cell>72.8%</cell><cell>97.2%</cell><cell>99.7%</cell><cell>99.8%</cell><cell>100%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The annotations of XM2VTS, FRGC-V2, LFPW, HELEN, AFW and IBUG are publicly available from http://ibug.doc.ic.ac.uk/resources/facial-point-annotations/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The first conduct of the</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>300-W challenge (2013) is available in http://ibug.doc.ic. ac.uk/resources/300-W/.<ref type="bibr" target="#b2">3</ref> The second conduct of the 300-W challenge (2015) is available in http://ibug.doc. ic.ac.uk/resources/300-W_IMAVIS/.<ref type="bibr" target="#b3">4</ref> The 300-W database is publicly available from http://ibug.doc.ic.ac.uk/resources/ 300-W_IMAVIS/. We provide the images along with the corresponding bounding boxes.Please cite this article as: C. Sagonas, E. Antonakos, G. Tzimiropoulos, S. Zafeiriou, M. Pantic, 300 Faces In-The-Wild Challenge: database and results, Image and Vision Computing (2016), http://dx.doi.org/10.1016/j.imavis.2016.01.002</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is funded by the EPSRC project EP/J017787/1 (4D-FAB). The work of S. Zafeiriou is also partially supported by the EPSRC project EP/L026813/1 Adaptive Facial Deformable Models for Tracking (ADAManT). The work of G. Tzimiropoulos is also partially supported by EPSRC project EP/M02153X/1 Facial Deformable Models of Animals. The work of M. Pantic is further supported by the European Community Horizon 2020 [H2020/2014-2020] under grant agreement no. 645094 (SEWA).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Active appearance models revisited</title>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="164" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generic active appearance models revisited</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alabort-I-Medina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Asian Conference on Computer Vision (ACCV)</title>
		<meeting>Asian Conference on Computer Vision (ACCV)<address><addrLine>Daejeon, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="650" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimization problems for fast AAM fitting in-the-wild</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Active orientation models for face alignment in-the-wild</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alabort-I-Medina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security, Special Issue on Facial Biometrics in-the-wild</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2024" to="2034" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">HOG active appearance models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alabort-I-Medina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Image Processing (ICIP)</title>
		<meeting>IEEE International Conference on Image Processing (ICIP)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="224" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Alabort-I-Medina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<title level="m">Bayesian active appearance models, Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3438" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unifying holistic and parts-based deformable model fitting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alabort-I Medina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3679" to="3688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feature-based Lucas-Kanade and active appearance models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2617" to="2632" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gauss-Newton deformable part models for face alignment in-the-wild</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="79" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: people detection and articulated pose estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1014" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Active pictorial structures</title>
		<author>
			<persName><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alabort-I Medina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5435" to="5444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Active shape models-their training and application</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="59" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature detection and tracking with constrained local models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cristinacce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="929" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deformable model fitting by regularized landmark mean-shift</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="200" to="215" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust discriminative response map fitting with constrained local models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3444" to="3451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Face alignment by explicit shape regression</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Incremental face alignment in the wild</title>
		<author>
			<persName><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1859" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Project-out cascaded regression with an application to face alignment</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3659" to="3667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-pie</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="807" to="813" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">XM2VTSDB: the extended M2VTS database</title>
		<author>
			<persName><forename type="first">K</forename><surname>Messer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Maitre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">964</biblScope>
			<biblScope unit="page" from="965" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Overview of the face recognition grand challenge</title>
		<author>
			<persName><forename type="first">P</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scruggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Worek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="947" to="954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The AR Face Database</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<idno>24</idno>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
	<note type="report_type">CVC Technical Report</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Localizing parts of faces using a consensus of exemplars</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: a large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kostinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV-W), Workshop on Benchmarking Facial Image Analysis Technologies</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV-W), Workshop on Benchmarking Facial Image Analysis Technologies</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">300 Faces in-the-Wild Challenge: the first facial landmark localization challenge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV-W), Workshop on 300 Faces in-the-Wild Challenge (300-W)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV-W), Workshop on 300 Faces in-the-Wild Challenge (300-W)<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="397" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Menpo: a comprehensive platform for parametric image alignment and visual deformable models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alabort-I-Medina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia, Open Source Software Competition</title>
		<meeting>the ACM International Conference on Multimedia, Open Source Software Competition<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="679" to="682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Structured semi-supervised forest for facial landmarks localization with face mask reasoning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-P</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machine Vision Conference (BMVC)</title>
		<meeting>British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Using a deformation field model for localizing faces and facial points under weak supervision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3694" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semi-supervised facial landmark annotation, Comput. Vis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Underst</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="922" to="935" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A hierarchical probabilistic model for facial feature detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1781" to="1788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Subspace learning from image gradient orientations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2454" to="2466" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adaptive and constrained algorithms for inverse compositional active appearance model fitting</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maragos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generic vs. person specific active appearance models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1080" to="1093" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Constrained local neural fields for robust facial landmark detection in the wild</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV-W), Workshop on 300 Faces in-the-Wild Challenge</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV-W), Workshop on 300 Faces in-the-Wild Challenge</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">300</biblScope>
			<biblScope unit="page" from="354" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Localizing facial keypoints with global descriptor search, neighbour alignment and locally linear models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moalem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV-W), Workshop on 300 Faces in-the-Wild Challenge</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV-W), Workshop on 300 Faces in-the-Wild Challenge</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">300</biblScope>
			<biblScope unit="page" from="362" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Guided unsupervised learning of mode specific models for facial point detection in the wild</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Almaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV-W), Workshop on 300 Faces in-the-Wild Challenge</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV-W), Workshop on 300 Faces in-the-Wild Challenge</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">300</biblScope>
			<biblScope unit="page" from="370" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Local evidence aggregation for regression-based facial point detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Binefa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1149" to="1163" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multiview active shape models with SIFT descriptors for the 300-W face landmark challenge</title>
		<author>
			<persName><forename type="first">S</forename><surname>Milborrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nicolls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV-W), Workshop on 300 Faces in-the-Wild Challenge</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV-W), Workshop on 300 Faces in-the-Wild Challenge</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">300</biblScope>
			<biblScope unit="page" from="378" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.imavis.2016.01.002</idno>
		<ptr target="http://dx.doi.org/10.1016/j.imavis.2016.01.002" />
	</analytic>
	<monogr>
		<title level="m">300 Faces In-The-Wild Challenge: database and results, Image and Vision Computing</title>
		<imprint>
			<date type="published" when="1999">1999. 2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
	<note>Proceedings of IEEE International Conference on Computer Vision (ICCV)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learn to combine multiple hypotheses for accurate face alignment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV-W), Workshop on 300 Faces in-the-Wild Challenge</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV-W), Workshop on 300 Faces in-the-Wild Challenge</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="392" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Extensive facial landmark localization with coarse-to-fine convolutional network cascade</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV-W), Workshop on 300 Faces in-the-Wild Challenge</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV-W), Workshop on 300 Faces in-the-Wild Challenge</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">300</biblScope>
			<biblScope unit="page" from="386" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</title>
		<meeting>IEEE International Conference on Computer Vision &amp; Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">FDDB: a benchmark for face detection in unconstrained settings</title>
		<author>
			<persName><forename type="first">V</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<idno>UM-CS-2010-009</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-view facial landmark detection by using a 3D shape model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Čech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Franc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Uřičář</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Image Vision Comput</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">M 3 CSR: multi-view, multi-scale and multi-component cascade shape regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Approaching human level facial landmark localization by deep learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">L2,1-based regression and prediction accumulation across views for robust facial landmark detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multi-view facial landmark detector learned by the structured output SVM</title>
		<author>
			<persName><forename type="first">M</forename><surname>Uřičář</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Franc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hlaváč</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vis. Comput</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
