<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Asymmetric Contrastive Loss for Handling Imbalanced Datasets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vito</forename><surname>Valentino</surname></persName>
							<email>valentino.vito11@ui.ac.id</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science Faculty of Computer Science Universitas</orgName>
								<address>
									<postCode>16424, 16424</postCode>
									<country>Indonesia Universitas Indonesia Depok, Indonesia Depok, Indonesia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yohanes</forename><surname>Lim</surname></persName>
							<email>yohanes@cs.ui.ac.id</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science Faculty of Computer Science Universitas</orgName>
								<address>
									<postCode>16424, 16424</postCode>
									<country>Indonesia Universitas Indonesia Depok, Indonesia Depok, Indonesia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Stefanus</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer Science Faculty of Computer Science Universitas</orgName>
								<address>
									<postCode>16424, 16424</postCode>
									<country>Indonesia Universitas Indonesia Depok, Indonesia Depok, Indonesia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Asymmetric Contrastive Loss for Handling Imbalanced Datasets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Asymmetric loss</term>
					<term>class imbalance</term>
					<term>contrastive loss</term>
					<term>entropy</term>
					<term>focal loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contrastive learning is a representation learning method performed by contrasting a sample to other similar samples so that they are brought closely together, forming clusters in the feature space. The learning process is typically conducted using a two-stage training architecture, and it utilizes the contrastive loss (CL) for its feature learning. Contrastive learning has been shown to be quite successful in handling imbalanced datasets, in which some classes are overrepresented while some others are underrepresented. However, previous studies have not specifically modified CL for imbalanced datasets. In this work, we introduce an asymmetric version of CL, referred to as ACL, in order to directly address the problem of class imbalance. In addition, we propose the asymmetric focal contrastive loss (AFCL) as a further generalization of both ACL and focal contrastive loss (FCL). Results on the FMNIST and ISIC 2018 imbalanced datasets show that AFCL is capable of outperforming CL and FCL in terms of both weighted and unweighted classification accuracies. In the appendix, we provide a full axiomatic treatment on entropy, along with complete proofs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Class imbalance is a major obstacle occurring within a dataset when certain classes in the dataset are overrepresented (referred to as majority classes), while some are underrepresented (referred to as minority classes). This can be problematic for a large number of classification models. A deep learning model such as a convolutional neural network (CNN) might not be able to properly learn from the minority classes. Consequently, the model would be less likely to correctly identify minority samples as they occur. This is especially crucial in medical imaging, since a model that cannot identify rare diseases would not be effective for diagnostic purposes. For example, the ISIC 2018 dataset <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> is an imbalanced medical dataset which consists of images of skin lesions that appear in various frequencies during screening.</p><p>To produce a less imbalanced dataset, it is possible to resample the dataset by either increasing the number of minority samples <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b5">[6]</ref> or decreasing the number of majority samples <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b9">[10]</ref>. Other methods to handle class imbalance include substituting the standard cross-entropy (CE) loss for a more suitable loss, such as the focal loss (FL). Lin et al. <ref type="bibr" target="#b10">[11]</ref> modified the CE loss into FL so that minority classes can be prioritized. This is done by ensuring that the model focuses on samples that are harder to classify during model training. Recent studies also unveiled the potential of contrastive learning as a way to combat imbalanced datasets <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>.</p><p>Contrastive learning is performed by contrasting a sample (called an anchor) to other similar samples (called positive samples) so that they are mapped closely together in the feature space. As a consequence, dissimilar samples (called negative samples) are pushed away from the anchor, forming clusters in the feature space based on similarity. In this research, contrastive learning is done using a two-stage training architecture, which utilizes the contrastive loss (CL) formulated by Khosla et al. <ref type="bibr" target="#b14">[14]</ref>. This formulation of CL is supervised based, and it can contrast the anchor to multiple positive samples belonging to the same class. This is unlike self-supervised contrastive learning <ref type="bibr" target="#b15">[15]</ref>- <ref type="bibr" target="#b18">[18]</ref>, which contrasts the anchor to only one positive sample in the mini-batch.</p><p>In this work, we propose a modification of CL, referred to as the asymmetric contrastive loss (ACL). Unlike CL, the ACL is able to directly contrast the anchor to its negative samples so that they are pushed apart in the feature space. This becomes important when a rare sample has no other positive samples in the mini-batch. To our knowledge, this is the first study to modify CL directly in order to address the class imbalance problem. We also consider the asymmetric variant of the focal contrastive loss (FCL) <ref type="bibr" target="#b19">[19]</ref>, called the asymmetric focal contrastive loss (AFCL). Using FMNIST and ISIC 2018 as datasets, experiments are done to test the performance of both ACL and AFCL in binary classification tasks. It is observed that AFCL is superior to CL and FCL in multiple class-imbalance scenarios, provided that suitable hyperparameters are used. In addition, this work provides a streamlined survey on the literature related to entropy and loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND ON ENTROPY AND LOSS FUNCTIONS</head><p>In this section, we provide a literature review on basic information theory and various loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2207.07080v1 [cs.LG] 14 Jul 2022</head><p>A. Entropy, Information, and Divergence Introduced by Shannon <ref type="bibr" target="#b20">[20]</ref>, entropy provides a measure on the amount of information contained in a random variable, usually in bits. The entropy H(X) of a random variable X is given by the formula</p><formula xml:id="formula_0">H(X) = E P X [-log(P X (X))] .<label>(1)</label></formula><p>Given two random variables X and Y , their joint entropy H(X, Y ) is the entropy of the joint random variable (X, Y ):</p><formula xml:id="formula_1">H(X, Y ) = E P (X,Y ) -log(P (X,Y ) (X, Y )) .<label>(2)</label></formula><p>In addition, the conditional entropy H(Y | X) is defined as</p><formula xml:id="formula_2">H(Y | X) = E P (Y,X) -log(P Y |X (Y | X) . (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>Conditional entropy is used to measure the average amount of information contained in Y when the value of X is given.</p><p>Conditional entropy is bounded above by the original entropy; that is, H(Y | X) ? H(Y ), with equality if and only if X and Y are independent <ref type="bibr" target="#b21">[21]</ref>.</p><p>The formulas for entropy, joint entropy, and conditional entropy can be derived via an axiomatic approach <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref>. The list of axioms is provided in Appendix A, whereas the derivation of the formula of entropy is provided in Appendix B.</p><p>The mutual information I(X; Y ) is a measure of dependence between random variables X and Y <ref type="bibr" target="#b24">[24]</ref>. It provides the amount of information about one random variable provided by the other random variable, and it is defined by</p><formula xml:id="formula_4">I(X; Y ) = H(X) -H(X | Y ) = H(Y ) -H(Y | X). (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>Mutual information is symmetric. In other words, I(X; Y ) = I(Y ; X). Mutual information is also nonnegative (I(X; Y ) ? 0), and I(X; Y ) = 0 if and only if X and Y are independent <ref type="bibr" target="#b21">[21]</ref>.</p><p>The dissimilarity between random variables X and X on the same space X can be measured using the notion of KLdivergence:</p><formula xml:id="formula_6">D KL (X X ) = E P X log P X (X) P X (X) .<label>(5)</label></formula><p>Similar to mutual information, KL-divergence is nonnegative (D KL (X X ) ? 0), and D KL (X X ) = 0 if and only if X = X <ref type="bibr" target="#b21">[21]</ref>. Unlike mutual information, KL-divergence is asymmetric, so D KL (X X ) and D KL (X X) are not necessarily equal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cross-Entropy and Focal Loss</head><p>Given random variables X and X on the same space X , their cross-entropy H(X; X) is defined as <ref type="bibr" target="#b25">[25]</ref>:</p><formula xml:id="formula_7">H(X; X) = E P X -log(P X (X) .<label>(6)</label></formula><p>Cross-entropy is the average amount of bits needed to encode the true distribution X when its estimate X is provided <ref type="bibr" target="#b26">[26]</ref>.</p><p>A small value of H(X; X) implies that X is a good estimate for X. Cross-entropy is connected to KL-divergence via the following identity:</p><formula xml:id="formula_8">H(X; X) = H(X) + D KL (X X).<label>(7)</label></formula><p>When X = X, the equality H(X; X) = H(X) holds. Now, the cross-entropy loss and focal loss are provided within the context of a binary classification task consisting of two classes labeled 0 and 1. Suppose that y ? {0, 1} denotes the ground-truth class and p ? [0, 1] denotes the estimated probability for the class labeled 1. The value of 1 -p is then the estimated probability for the class labeled 0. The cross-entropy (CE) loss is given by</p><formula xml:id="formula_9">L CE = -y log(p) -(1 -y) log(1 -p) = -log(p) y = 1, -log(1 -p) y = 0.</formula><p>If y = 1, then the loss L CE is zero when p = 1. On the other hand, if y = 0, then the loss is zero when 1 -p = 1. In either case, the CE loss is minimized when the estimated probability of the true class is maximized, which is the desired property of a good classification model.</p><p>The focal loss (FL) <ref type="bibr" target="#b10">[11]</ref> is a modification of the CE loss introduced to put more focus on hard-to-classify examples. It is given by the following formula: The parameter ? in L foc is known as the focusing parameter. Choosing a larger value of ? would push the model to focus on training from the misclassified examples. For instance, suppose that ? = 4 and denote the estimated probability of the true class by p t . The graph on Figure <ref type="figure" target="#fig_0">1</ref> shows that when p t &gt; 0.5, the FL is quite small. Hence, the model would be less concerned about learning from an example when p t is already sufficiently large. FL is a useful choice when class imbalance exists as it can help the model focus on the less represented samples within the dataset.</p><formula xml:id="formula_10">L foc = -y(1 -p) ? log(p) -(1 -y)p ? log(1 -p).<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Asymmetric Loss</head><p>For multi-label classification with K labels, let y i ? {0, 1} be the ground truth for class i and p i ? [0, 1] be its estimated probability obtained by the model. The aggregate classification loss is then</p><formula xml:id="formula_11">L = K i=1 L i ,<label>(9)</label></formula><p>where</p><formula xml:id="formula_12">L i = -y i L + i -(1 -y i )L - i . (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>If FL is the chosen type of loss, L + i and L - i are set as follows:</p><formula xml:id="formula_14">L + i = (1 -p i ) ? log(p i ) and L - i = p ? i log(1 -p i ).<label>(11)</label></formula><p>In a typical multi-label dataset, the ground truth y i has value 0 for the majority of classes i. Consequently, the negative terms L - i dominate in the calculation of the aggregate loss L. Asymmetric loss (ASL) <ref type="bibr" target="#b27">[27]</ref> is a proposed solution to this problem. ASL emphasizes the contribution of the positive terms by modifying the losses of Eq. <ref type="bibr" target="#b10">(11)</ref> to</p><formula xml:id="formula_15">L + i = (1 -p i ) ? + log(p i )<label>(12)</label></formula><p>and</p><formula xml:id="formula_16">L - i = (p (m) i ) ? - log(1 -p (m) i ),<label>(13)</label></formula><p>where ? + , ? -are hyperparameters and p (m) i</p><p>is the shifted probability of p i obtained from the probability margin m ? 0 via the formula p</p><formula xml:id="formula_17">(m) i = max(p i -m, 0). (<label>14</label></formula><formula xml:id="formula_18">)</formula><p>This shift helps decrease the contribution of L - i . Indeed, if we set m = 1, then L - i = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Contrastive Loss</head><p>Contrastive learning is a learning method to learn representations from data. A supervised approach of contrastive learning was introduced by Khosla et al. <ref type="bibr" target="#b14">[14]</ref> to learn from a set of sample-label pairs {(x i , y i )} N i=1 in a mini-batch of size N . The samples x i are fed through a feature encoder Enc(?) and a projection head Proj(?) in succession to obtain features z i = Proj(Enc(x i )). The feature encoder extracts features from x i , whereas the projection head projects the features into a lower dimension and apply 2 -normalization so that z i lies in the unit hypersphere. In other words, z i 2 = 1.</p><p>A pair (z i , z j ), where i = j, is referred to as a positive pair if the features share the same class label (y i = y j ) and it is a negative pair if the features have different class labels (y i = y j ). Contrastive learning aims to maximize the similarity between z i and z j whenever they form a positive pair and minimize their similarity whenever they form a negative pair. This similarity is measured with cosine similarity <ref type="bibr" target="#b26">[26]</ref>:</p><formula xml:id="formula_19">?(z i , z j ) = z i ? z j z i 2 z j 2 = z i ? z j .<label>(15)</label></formula><p>From the above equation, we have ?(z i , z j ) ? [-1, 1]. In addition, ?(z i , z j ) = 1 when z i = z j , and ?(z i , z j ) = -1 when z i and z j form a 180 ? angle. Fixing z i as the anchor, let A i = {z k | k = i} be the set of features other than z i and let</p><formula xml:id="formula_20">P i = {z k ? A i | y k = y i } be the set of z k such that (z i , z k ) is a positive pair.</formula><p>The predicted probability p ij that z i and z j belong to the same class is obtained by applying the softmax function to the the set of similarities between z i and z k ? A i :</p><formula xml:id="formula_21">p ij = exp(z i ? z j /? ) z k ?Ai exp(z i ? z k /? ) , (<label>16</label></formula><formula xml:id="formula_22">)</formula><p>where ? is referred to as the temperature parameter. Since our goal is to maximize p ij whenever z j ? P i , the contrastive loss which is to be minimized is formulated as</p><formula xml:id="formula_23">L con = - n i=1 1 |P i | zj ?Pi log(p ij ).<label>(17)</label></formula><p>Information-theoretical properties of L con are given in <ref type="bibr" target="#b19">[19]</ref>, from which we provide a summary. Let X, Y , and Z denote random variables of the samples, labels, and features, respectively. The following theorem states that L con is positive proportional to H(Z | Y ) -H(Z) under the assumption that no class imbalance exists.</p><p>Theorem II.1 (Zhang et al. <ref type="bibr" target="#b19">[19]</ref>). Assuming that features are 2 -normalized and the dataset is balanced,</p><formula xml:id="formula_24">L con ? H(Z | Y ) -H(Z).<label>(18)</label></formula><p>Theorem II.1 implies that minimizing L con is equivalent to minimizing the conditional entropy H(Z | Y ) and maximizing the feature entropy H(Z). Since I(Z; Y ) = H(Z) -H(Z | Y ), minimizing L con is equivalent to maximizing the mutual information I(Z; Y ) between features Z and class labels Y . In other words, contrastive learning aims to extract the maximum amount of information from class labels and encode them in the form of features.</p><p>After the features are extracted, a classifier Clas(?) is assigned to convert z i into a prediction ?i = Clas(z i ) of the class label. The random variable of predicted class labels is denoted by ? .</p><p>For the next theorem, the definition of conditional cross-entropy H(Y ; ? | Z) is given as follows:</p><formula xml:id="formula_25">H(Y ; ? | Z) = E P (Y,Z) -log(P ( ? ,Z) (Y, Z) .<label>(19)</label></formula><p>Conditional CE measures the average amount of information needed to encode the true distribution Y using its estimate ? , given the value of Z. A small value of H(Y ; ? | Z) implies that ? is a good estimate for Y , given Z.</p><p>Theorem II.2 (Zhang et al. <ref type="bibr" target="#b19">[19]</ref>). Assuming that features are 2 -normalized and the dataset is balanced,</p><formula xml:id="formula_26">L con ? inf H(Y ; ? | Z) -H(Y ),<label>(20)</label></formula><p>where the infimum is taken over classifiers.</p><p>Theorem II.2 implies that minimizing L con will minimize the infimum of conditional cross-entropy H(Y ; ? | Z) taken over classifiers. As a consequence, contrastive learning is able to encode features in Z such that the best classifier can produce a good estimate of Y given the information provided by the feature encoder.</p><p>The formula for L con can be modified so as to resemble the focal loss, resulting in a loss function known as the focal contrastive loss (FCL) <ref type="bibr" target="#b19">[19]</ref>: III. METHODOLOGY In this section, our proposed modification of the contrastive loss, called the asymmetric contrastive loss, is introduced. Also, the architecture of the model in which the contrastive losses are implemented is explained.</p><formula xml:id="formula_27">L FC = - n i=1 1 |P i | zj ?Pi (1 -p ij ) log(p ij ).<label>(21)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Asymmetric Contrastive Loss</head><p>In Eq. ( <ref type="formula" target="#formula_23">17</ref>), the inside summation of the contrastive loss is evaluated over P i . Consequently, according to Eq. ( <ref type="formula" target="#formula_21">16</ref>), each anchor z i is contrasted with vectors z j that belong to the same class. This does not present a problem when the mini-batch contains plenty of examples from each class. However, the calculated loss may not give each class a fair contribution when some classes are less represented in the mini-batch.</p><p>In Figure <ref type="figure" target="#fig_1">2</ref>, a sampled mini-batch consists of 11 examples with blue-colored class label and 1 example with red-colored class label. When the anchor z i is the representation of the red-colored sample, z i does not directly contribute to the calculation of L con since P i is empty. In other words, z i cannot be contrasted to any other sample in the mini-batch. This scenario is likely to happen when the dataset is imbalanced, and it motivates us to modify CL so that each anchor z i can also be contrasted with z j not belonging to the same class.</p><p>Let N i = A i \ P i be the set of vectors z k such that (z i , z k ) is a negative pair. Motivated by the L + i and L - i of Eq. ( <ref type="formula" target="#formula_12">10</ref>), we define</p><formula xml:id="formula_28">L + i = 1 |P i | zj ?Pi log(p ij )<label>(22)</label></formula><p>and</p><formula xml:id="formula_29">L - i = 1 |N i | zj ?Ni log(1 -p ij ),<label>(23)</label></formula><p>where</p><formula xml:id="formula_30">p ij = exp(z i ? z j /? )/ z k ?Ai exp(z i ? z k /?</formula><p>). The loss function L + i contrasts z i to vectors in P i , whereas L - i contrasts z i to vectors in N i . The resulting asymmetric contrastive loss (ACL) is given by the formula</p><formula xml:id="formula_31">L AC = - n i=1 (L + i + ?L - i ),<label>(24)</label></formula><p>where ? ? 0 is a fixed hyperparameter. If ? = 0, then L AC = L con . Hence ACL is a generalization of CL.</p><p>When the batch size is set to a large number (over 100, for example), the value p ij tends to be very small. This causes L - i to be much smaller than L + i . In order to balance their contribution to the total loss L AC , a large value for ? is usually chosen (between 60 and 300 in our experiment).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Asymmetric Focal Contrastive Loss</head><p>Following the formulation of L FC in Eq. ( <ref type="formula" target="#formula_27">21</ref>), L + i can be modified to have the following formula:</p><formula xml:id="formula_32">L + i = 1 |P i | zj ?Pi (1 -p ij ) ? log(p ij ).<label>(25)</label></formula><p>Using this loss, the asymmetric focal contrastive loss (AFCL) is then given by</p><formula xml:id="formula_33">L AFC = - n i=1 (L + i + ?L - i ),<label>(26)</label></formula><p>where</p><formula xml:id="formula_34">L - i = 1 |Ni| zj ?Ni log(1 -p ij ).</formula><p>We do not modify L - i by adding the multiplicative term (p ij ) ? since p ij is usually too small and would make L - i vanish if the term is added. We have L AFC = L FC when ? = 1. Thus, AFCL generalizes FCL. Unlike FCL, we add the hyperparameter ? ? 0 to the loss function so as to provide some flexibility to the loss function. The architecture of the model is taken from <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>. The training strategy for the model, as shown in Figure <ref type="figure" target="#fig_2">3</ref>, comprises of two stages: the feature learning stage and the fine-tuning stage.</p><p>In the first stage, each mini-batch is fed through a feature encoder. We consider either ResNet-18 or ResNet-50 <ref type="bibr" target="#b28">[28]</ref> for the architecture of the feature encoder. The output of the feature encoder is projected by the projection head to generate a vector z of length 128. If ResNet-18 is used for the feature encoder, then the projection head consists of two layers of length 512 and 128. If ResNet-50 is used, then the two layers are of length 2048 and 128. Afterwards, z is 2 -normalized and the model parameters are updated using some version of the contrastive loss (either CL, FCL, ACL, or AFCL).</p><p>After the first stage is complete, the feature encoder is frozen and the projection head is removed. In its place, we have a one-layer classification head which generates the estimated probability that the training sample belongs to a certain class. The parameters of the classification head are updated using either the FL or CE loss. The final classification model is the feature encoder trained during the first stage, together with the classification head trained during the second stage. Since the classification head is a significantly smaller architecture than the feature encoder, training is mostly focused on the first stage. As a consequence, we typically need a larger number of epochs for the feature learning stage compared to the fine-tuning stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>The datasets and settings of our experiments are outlined in this section. We provide and discuss the results of the experiments on the FMNIST and ISIC 2018 datasets. The PyTorch implementation is available on GitHub<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>In our experiments, the training strategy outlined in Subsection III-C is applied to two imbalanced datasets. The first is a modified version of the Fashion-MNIST (FMNIST) dataset <ref type="bibr" target="#b29">[29]</ref>, and the second is the International Skin Imaging Collaboration (ISIC) 2018 medical dataset <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p><p>The FMNIST dataset consists of low-resolution (28 ? 28 pixels), grayscale images of ten classes of clothing. In this study, we take only two classes to form a binary classification task: the T-shirt and shirt classes. The samples are taken such that the proportion between the T-shirt and shirt images can be imbalanced depending on the scenario. On the other hand, the ISIC 2018 dataset consists of high-resolution, RGB images of seven classes of skin lesions. Following FMNIST, we use only two classes for the experiments: the melanoma and dermatofibroma classes. Illustrations of the sample images of both datasets are provided in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>FMNIST is chosen as a dataset since, although simple, it is a benchmark dataset to test deep learning models for computer vision. On the other hand, ISIC 2018 is chosen since it is a domain-appropriate imbalanced dataset for our model. We first apply the model (using AFCL as the loss function) to the more lightweight FMNIST dataset under various class-imbalance scenarios. This is conducted to check the appropriate values of the ? and ? parameters of AFCL under different imbalance conditions. Afterwards, the model is applied to the ISIC 2018 dataset using the optimal parameter values obtained during the FMNIST experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Details</head><p>The experiments are conducted using the NVIDIA Tesla P100-PCIE GPU allocated by the Google Colaboratory Pro platform. The models and loss functions are implemented using PyTorch. To process the FMNIST dataset, we use the simpler ResNet-18 architecture as the feature encoder and train it for 20 epochs. On the other hand, to process the ISIC 2018 dataset, we use the deeper ResNet-50 as the feature encoder and train it for 40 epochs. For both the FMNIST and ISIC 2018 datasets, the learning rate and batch size are set to 10 -2 and 128, respectively. In addition, the classification head is trained for 10 epochs. The encoder and the classification head are both trained using the Adam optimizer. Finally, the temperature parameter ? of the contrastive loss is set to its default value of 0.07.</p><p>The evaluation metrics utilized in the experiment are (weighted) accuracy and unweighted accuracy (UWA), both of which can be calculated from the number of true positives (TP), true negatives (TN), false negatives (FN), and false positives (FP) using the formulas Accuracy = TP + TN TP + TN + FN + FP <ref type="bibr" target="#b27">(27)</ref> and UWA = 1 2</p><formula xml:id="formula_35">TP TP + FN + TN TN + FP ,<label>(28)</label></formula><p>respectively. Unlike accuracy, UWA provides the average of the individual class accuracies regardless of the number of samples in the test set of each class. UWA is an appropriate metric when the dataset is significantly imbalanced <ref type="bibr" target="#b30">[30]</ref>.</p><p>For heavily imbalanced datasets, a high accuracy and low UWA may mean that the model is biased towards classifying samples as part of the majority class. This indicates that the model does not properly learn from the minority samples. In contrast, a lower accuracy with high UWA indicates that the model takes significant risks to classify some samples as part of the minority class. Our aim is to construct a model that maximizes both metrics simultaneously; that is, a model that can learn unbiasedly from both the majority and minority samples with minimal misclassification error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments using FMNIST</head><p>The data used in the FMNIST experiment comprise of 1000 images classified as either a T-shirt or a shirt. The dataset is split 70/30 for model training and testing. The images are augmented using random rotations and random flips. We deploy 11 class-imbalance scenarios on the dataset which control the proportion between the T-shirt class and the shirt class. For example, if the the proportion is 60:40, then 600 T-shirt images and 400 shirt images are sampled to form the experimental dataset. Our proportions range from 50:50 up to 98:2. During the first stage, the ResNet-18 encoder is trained using the AFCL. Afterwards, the classification head is trained using the CE loss during the second stage. As AFCL contains two parameters ? and ?, our goal is to tune each of these parameters independently, keeping the other parameter fixed. First, ? is tuned as we set ? = 0, followed by the tuning of ? as we set ? = 0. Each experiment is done four times in total. The average accuracy and UWA of these four runs are provided in Tables I (for the tuning of ?) and II (for the tuning of ?).</p><p>For the tuning of ?, six values of ? are experimented on, namely ? ? {0, 60, 120, 180, 240, 300}. When ? = 0, the loss function reduces to the ordinary CL. As observed in Table <ref type="table" target="#tab_0">I</ref>, the optimal value of ? tends to be larger when the dataset is moderately imbalanced. As the scenario goes from 60:40 to 90:10, the parameter ? that maximizes accuracy increases in value, from ? = 0 when the proportion is 60:40 to ? = 300 when the proportion is 90:10. In general, this indicates that the L - i term of the ACL becomes more essential to the overall loss as the dataset gets more imbalanced, confirming the reasoning contained in Subsection III-A.</p><p>As seen in Table <ref type="table" target="#tab_1">II</ref>, we experiment on ? ? {0, 1, 2, 4, 7, 10}, where choosing ? = 0 means that we are using CL. Although the overall pattern of the optimal ? is less apparent than ? of the previous experiment, some insights can still be obtained. When the scenario is between 70:30 and 90:10, the focusing parameter ? is optimally chosen when it is larger than zero. This is in direct contrast to when the proportion is perfectly balanced (50:50), where ? = 0 is the most optimal parameter. This suggests that a larger value of ? should be considered when class imbalance is significantly present within the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiments using ISIC 2018</head><p>From the ISIC 2018 dataset, a total of 1113 melanoma images and 115 dermatofibroma images are combined to create the experimental dataset. As with the previous experiment, the dataset is split 70/30 for training and testing. The images are resized to 128 ? 128 pixels. The ResNet-50 encoder is trained using one of the available contrastive losses, which include CL/FCL as baselines and ACL/AFCL as the proposed loss functions. The classification head is trained using FL as the loss function with its focusing parameter set to ? = 2.</p><p>The proportion between the melanoma class and the dermatofibroma class in the experimental dataset is close to 90:10. Using results from Tables I and II as a heuristic for determining the optimal parameter values, we set ? = 300 and ? = 2, 7. It is worth mentioning that even though ? = 2 produces the best accuracy in the FMNIST experiment, the UWA of the resulting model is quite poor. However, we decide to include this value in this experiment for completeness. The results of this experiment is given in Table <ref type="table" target="#tab_1">III</ref>. As in the previous section, each experiment is conducted four times, so the table lists the average accuracy and UWA of these four runs for each contrastive loss tested. Each run, which includes both model training and testing, is completed in roughly 80 minutes using our computational setup.</p><p>From Table <ref type="table" target="#tab_1">III</ref>, CL and ACL performs the worst in terms of UWA and accuracy, respectively. However, ACL gives the best UWA among all losses. This may indicate that ACL encourages the model to take the risky approach of classifying some samples as part of the minority class at the expense of accuracy. Overall, AFCL with ? = 300 and ? = 7 emerges as the best loss in this experiment, producing the best accuracy and the second-best UWA behind ACL. This leads us to conclude that the AFCL, with optimal hyperparameters chosen, is superior to the vanilla CL and FCL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>In this work, we introduced an asymmetric version of both contrastive loss (CL) and focal contrastive loss (FCL) referred to as ACL and AFCL, respectively. These asymmetric variants of the contrastive loss were proposed to provide more focus on the minority class. The experimental model used was a two-stage architecture consisting of a feature learning stage and a classifier fine-tuning stage. This model was applied to the FMNIST and ISIC 2018 imbalanced datasets using various contrastive losses. Our results show that AFCL was able to outperform CL and FCL in terms of both weighted and unweighted accuracies. On the ISIC 2018 binary classification task, AFCL, with ? = 300 and ? = 7 as hyperparameters, achieved an accuracy of 93.75% and an unweighted accuracy of 74.62%. This is in contrast to FCL, which achieved 93.07% and 74.34% on both metrics, respectively.</p><p>Axiom 2 (Maximality). Assuming |X | is fixed, H(X) is maximized when X is uniform.</p><p>In addition, the value of H(X) should not increase when impossible samples are added to X . Axiom 3 (Extensibility). If X ? Y with p x = q x for every x ? X (and thus q y = 0 for every y ? Y \X ), then H(Y ) = H(X).</p><p>To state the next axiom, two notions on entropy are first introduced. The joint entropy H(X, Y ) is simply the entropy of the joint random variable (X, Y ), and the conditional entropy H(Y | X) is defined as</p><formula xml:id="formula_36">H(Y | X) = x?X p x H(Y | X = x).<label>(29)</label></formula><p>Conditional entropy measures the average amount of information contained in Y given the value of X.</p><formula xml:id="formula_37">Axiom 4 (Additivity). H(X, Y ) = H(X) + H(Y | X). If X and Y are independent, then H(Y | X) = H(Y ). Therefore, H(X, Y ) = H(X) + H(Y ) in that case. In general, if X 1 , . . . , X n are independent, then H(X 1 , . . . , X n ) = n i=1 H(X i ).</formula><p>Suppose that X = {1, . . . , n}. Since H(X) only depends on the distribution of X by Axiom 1, the function H(X) can instead be seen as a function H(p 1 , . . . , p n ). The next axiom states that H(p 1 , . . . , p n ) is continuous on the space</p><formula xml:id="formula_38">S = {(p 1 , . . . , p n ) ? [0, 1] n | p 1 + ? ? ? + p n = 1}.<label>(30)</label></formula><p>Axiom 5 (Continuity). H(X) is continuous with respect to all probabilities p x .</p><p>From Axioms 0-5, the formula for H(X) is uniquely determined as shown in the following theorem. where the logarithm is to the base 2 and we set 0 ? log(0) = 0.</p><p>The proof of A.1 is provided in Appendix B. Looking back at the coin toss example, Figure <ref type="figure" target="#fig_4">5</ref> illustrates the graph of H(X) when X is either heads or tails with probabilities p and 1 -p, respectively. Entropy is maximized when the coin is fair, and it decreases in a continuous manner to zero as the coin becomes less fair.</p><p>The formula for entropy in Eq. (31) can be expressed in the form of an expectation: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. PROOF OF THEOREM A.1</head><p>The arguments used in this proof are adapted from <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref>. We first verify one direction of Theorem A.1.</p><p>Lemma B.1. The formula for H(X) given in Eq. (31) satisfies Axioms 0-5.</p><p>Proof. It is trivial to show that the normalization, invariance, extensibility, and continuity axioms hold, so we focus on proving the maximality and additivity axioms.</p><p>For maximality, we need to utilize Jensen's inequality <ref type="bibr" target="#b21">[21]</ref> applied on the concave function log. This inequality takes the form E[log Y ] ? log(E <ref type="bibr">[Y ]</ref>).</p><p>(34)</p><p>For any random variable X, H(X) = E log 1 P (X) We can obtain - Therefore, H(X, Y ) = H(X) + H(Y | X).</p><formula xml:id="formula_39">?</formula><p>To ease the notation, we can assume that X = {1, . . . , n} and write H(p 1 , . . . , p n ) in place of H(X) by the invariance axiom. For brevity, L(n) is defined as the entropy of a uniform random variable with |X | = n. In other words,</p><formula xml:id="formula_40">L(n) = H 1 n , . . . , 1 n . (<label>35</label></formula><formula xml:id="formula_41">)</formula><p>Lemma B.2. The following properties hold for the function L(n): 1) L(n) is non-decreasing.</p><p>2) L(n m ) = mL(n).</p><p>3) L(2 k ) = k. 4) L(n) = log(n).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A graph illustrating the focal loss given the predicted probability of the ground-truth class, with varying values of ?</figDesc><graphic url="image-1.png" coords="3,151.78,50.54,308.44,213.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A mini-batch consisting of 11 examples with blue-colored class label and 1 example with red-colored class label</figDesc><graphic url="image-2.png" coords="5,228.89,50.54,154.22,91.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A two-stage training strategy consisting of: (1) feature learning using contrastive loss, and (2) classifier fine-tuning using either FL or CE loss</figDesc><graphic url="image-3.png" coords="6,100.37,50.54,411.25,149.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Sample images of the FMNIST and ISIC 2018 datasets</figDesc><graphic url="image-4.png" coords="7,151.78,50.54,308.45,263.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. A graph illustrating the entropy of a coin toss with varying fairness</figDesc><graphic url="image-5.png" coords="12,151.78,50.54,308.45,208.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>HP</head><label></label><figDesc>(X) = E P X [-log(P X (X))] . (32)Likewise, joint entropy and conditional entropy can be expressed asH(X, Y ) = E P (X,Y ) -log(P (X,Y ) (X, Y ))(33)andH(Y | X) = x?X p x H(Y | X = x) Y |X (y | x) log P Y |X (y | x) x P Y |X (y | x) log P Y |X (y | x) = -(y,x) P (Y,X) (y, x) log P Y |X (y | x) = E P (Y,X) -log P Y |X (Y | X) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Since log(|X |) is the entropy of a uniform random variable on X , the entropy H(X) is maximized X is uniform.For additivity, we need to prove thatH(X, Y ) = H(X) + H(Y | X). Writing p xy = P (X,Y ) (x, y), we have H(X, Y ) = -x?X y?Y p x,y log(p x,y ) = -x?X y?Y p x,y log p x P Y |X (y | x) = -x?X y?Y p x,y log(p x ) + log P Y |X (y | x) = -x?X y?Y p x,y log(p x ) -x?X y?Y p x,y log P Y |X (y | x) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>P</head><label></label><figDesc>x?X y?Y p x,y log(p x ) = -x?X p x log(p x ) = H(X) and -x?X y?Y p x,y log P Y |X (y | x) = -x?X y?Y p x P Y |X (y | x) log P Y |X (y | x) Y |X (y | x) log P Y |X (y | x) ? ? = x?X p x H(Y | X = x) = H(Y | X).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>ACCURACY AND UWA (AVERAGED OVER FOUR INDEPENDENT RUNS) OF 11 IMBALANCE SCENARIOS USING VARIOUS VALUES OF ? FOR THE AFCL. THE PARAMETER ? IS CONSISTENTLY SET TO 0</figDesc><table><row><cell>Scenario</cell><cell>Metric</cell><cell></cell><cell>?</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>60</cell><cell>120</cell><cell>180</cell><cell>240</cell><cell>300</cell></row><row><cell>50:50</cell><cell cols="2">Accuracy 78.92 77.83 UWA 79.00 78.28</cell><cell cols="4">79.75 71.08 77.17 78.83 80.32 72.53 77.87 79.42</cell></row><row><cell>55:45</cell><cell cols="2">Accuracy 79.50 79.50 UWA 78.70 79.34</cell><cell cols="4">79.33 77.83 77.67 77.75 79.15 77.17 78.21 76.50</cell></row><row><cell>60:40</cell><cell>Accuracy 84.50 UWA 83.09</cell><cell cols="5">82.92 82.42 81.33 82.08 83.17 81.82 81.27 79.71 81.74 81.66</cell></row><row><cell>65:35</cell><cell cols="2">Accuracy 81.50 UWA 79.19 80.91 83.42</cell><cell cols="4">83.25 81.59 82.58 79.25 80.73 77.92 79.43 75.42</cell></row><row><cell>70:30</cell><cell cols="2">Accuracy 82.50 84.33 UWA 78.41 78.26</cell><cell cols="4">85.08 82.08 83.42 83.00 80.91 77.78 79.14 75.11</cell></row><row><cell>75:25</cell><cell cols="5">Accuracy 86.75 85.17 85.58 85.17 86.92 UWA 77.87 76.48 77.74 77.03 78.63</cell><cell>86.58 77.57</cell></row><row><cell>80:20</cell><cell cols="6">Accuracy 86.00 87.25 87.33 87.92 87.00 88.25 UWA 76.16 74.65 76.94 76.28 77.49 76.97</cell></row><row><cell>85:15</cell><cell cols="6">Accuracy 87.33 87.08 86.75 87.42 87.33 87.67 UWA 70.08 66.34 55.77 68.33 69.83 62.83</cell></row><row><cell>90:10</cell><cell cols="6">Accuracy 90.83 91.00 90.83 90.67 89.50 91.67 UWA 64.91 68.61 66.11 64.02 61.77 72.58</cell></row><row><cell>95:5</cell><cell cols="6">Accuracy 94.42 UWA 54.77 60.70 93.33 93.42 94.00 92.83 93.25 54.24 50.00 49.38 54.80</cell></row><row><cell>98:2</cell><cell cols="6">Accuracy 97.42 97.83 98.08 98.08 98.33 UWA 52.45 52.66 55.87 55.87 49.83 52.79 98.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II THE</head><label>II</label><figDesc>ACCURACY AND UWA (AVERAGED OVER FOUR INDEPENDENT RUNS) OF 11 IMBALANCE SCENARIOS USING VARIOUS VALUES OF ? FOR THE AFCL. THE PARAMETER ? IS CONSISTENTLY SET TO 0</figDesc><table><row><cell>Scenario</cell><cell>Metric</cell><cell></cell><cell>?</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>7</cell><cell>10</cell></row><row><cell>50:50</cell><cell>Accuracy 78.08 UWA 77.70</cell><cell cols="5">74.83 77.08 77.58 76.58 77.50 74.84 76.77 77.55 76.55 77.25</cell></row><row><cell>55:45</cell><cell cols="5">Accuracy 80.17 81.25 80.75 80.00 81.75 UWA 80.14 81.19 80.69 79.96 81.70</cell><cell>76.83 76.82</cell></row><row><cell>60:40</cell><cell cols="6">Accuracy 79.42 78.50 77.92 80.17 80.67 UWA 84.42 83.42 80.00 83.00 82.42 82.92 80.08</cell></row><row><cell>65:35</cell><cell>Accuracy 84.42 UWA 81.98</cell><cell cols="5">83.42 80.00 83.00 82.42 82.92 81.22 77.87 80.39 80.68 80.16</cell></row><row><cell>70:30</cell><cell cols="5">Accuracy 83.75 83.83 82.17 82.58 84.83 UWA 79.64 79.18 77.82 77.51 79.67</cell><cell>82.25 78.71</cell></row><row><cell>75:25</cell><cell cols="2">Accuracy 85.42 UWA 76.27 79.85 86.17</cell><cell cols="4">84.42 84.83 85.75 86.00 77.08 76.41 77.34 78.47</cell></row><row><cell>80:20</cell><cell cols="6">Accuracy 89.33 UWA 77.59 78.67 78.43 79.31 89.58 87.67 89.42 87.33 88.00 78.97 70.12</cell></row><row><cell>85:15</cell><cell cols="6">Accuracy 87.42 89.00 88.17 88.33 89.08 90.08 UWA 64.97 72.08 71.99 71.47 71.95 77.04</cell></row><row><cell>90:10</cell><cell cols="6">Accuracy 92.42 92.33 UWA 64.00 67.94 66.04 74.42 80.54 93.42 93.25 92.58 91.25 68.35</cell></row><row><cell>95:5</cell><cell cols="6">Accuracy 94.17 93.17 UWA 62.13 53.11 57.64 59.17 55.22 55.82 95.33 95.00 94.00 95.09</cell></row><row><cell>98:2</cell><cell cols="6">Accuracy 96.92 96.92 UWA 56.59 51.56 55.61 52.63 53.10 52.98 95.00 96.00 96.92 96.67</cell></row><row><cell></cell><cell cols="3">TABLE III</cell><cell></cell><cell></cell></row><row><cell cols="7">THE ACCURACY AND UWA (AVERAGED OVER FOUR INDEPENDENT RUNS) OF THE MODEL WHEN TRAINED USING VARIOUS CONTRASTIVE LOSSES</cell></row><row><cell></cell><cell>Loss function</cell><cell></cell><cell cols="2">Accuracy UWA</cell><cell></cell></row><row><cell></cell><cell>CL [14]</cell><cell></cell><cell>93.00</cell><cell>72.25</cell><cell></cell></row><row><cell></cell><cell>FCL [19]</cell><cell></cell><cell>93.07</cell><cell>74.34</cell><cell></cell></row><row><cell></cell><cell cols="2">ACL (? = 300)</cell><cell>85.94</cell><cell>75.54</cell><cell></cell></row><row><cell></cell><cell cols="2">AFCL (? = 300, ? = 2)</cell><cell>92.39</cell><cell>74.36</cell><cell></cell></row><row><cell></cell><cell cols="2">AFCL (? = 300, ? = 7)</cell><cell>93.75</cell><cell>74.62</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/valentinovito/Asymmetric-CL</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>The experiments of this research were conducted using datasets consisting of approximately 1000 total images. In the future, the experimental model may be applied to larger-scale datasets in order to test its scalability. In addition, other models based on ACL and AFCL can also be developed for specific datasets, preferably within the realm of multiclass classification.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>= L(n + 1).</p><p>Since n is arbitrary, this proves that L(n) is a non-decreasing function of n.</p><p>2. Let X be a uniform random variable on X with |X | = n. Then H(X) = L(n). Now let X 1 , . . . , X m be i.i.d. random variables with distribution identical to X. Since the joint variable (X 1 , . . . , X m ) is uniform, we obtain</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">By the normalization axiom,</head><p>Fix n and m. Now let k be the unique integer such that the inequality</p><p>holds. Applying the non-decreasing function L(?) on (36), we obtain</p><p>Applying the non-decreasing function log(?) on (36), we also obtain</p><p>Both ( <ref type="formula">37</ref>) and (38) imply that</p><p>We can obtain |L(n) -log(n)| ? 1/m by dividing both sides of (39) by m. As a consequence,</p><p>The fourth property of Lemma B.2 infers that Theorem A.1 holds for uniform random variables X. We are now ready to complete the proof of Theorem A.1 for any random variable X.</p><p>Proof of Theorem A.1. One half of Theorem A.1 is proved in Lemma B.1. It remains to prove that a function H(X) = H(p 1 , . . . , p n ) which satisfies Axioms 0-5 is necessarily equal to -n i=1 p i log(p i ). Without loss of generality, we can assume that p 1 , . . . , p n are all rational. Indeed, since the rationals are dense in the reals, the theorem would still hold for real values p 1 , . . . , p n by the continuity axiom (Axiom 5).</p><p>Let p i = g i /g for 1 ? i ? n, where each g i is a positive integer and n i=1 g i = g. Define a random variable Y dependent on X such that |Y| = g and Y is partitioned into n disjoint groups Y 1 , . . . , Y n containing g 1 , . . . , g n values, respectively. If it is given that X = i, where 1 ? i ? n, then all the values in Y i have the same probability 1/g i , and values from other groups have probability zero. It follows that</p><p>In addition, (X, Y ) and Y are identically distributed since X is completely dependent on Y . The joint variable (X, Y ) thus has a total of g possible values, and each value has the same probability 1/g of occurring. By the additivity axiom,</p><p>This proves that H(p 1 , . . . , p n ) = -n i=1 p i log(p i ) for rational p 1 , . . . , p n . The full statement of the theorem follows from continuity, as explained at the beginning of the proof.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (isic)</title>
		<author>
			<persName><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rotemberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dusza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Helba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liopyris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marchetti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03368</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Loras: An oversampling approach for imbalanced datasets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Davtyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wolfien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nassar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wolkenhauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="279" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Vos: A method for variational oversampling of imbalanced data</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Fajardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Findlay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houmanfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02596</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Gensample: A genetic algorithm for oversampling in imbalanced datasets</title>
		<author>
			<persName><forename type="first">V</forename><surname>Karia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Naeim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramezani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10806</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A novel adaptive minority oversampling technique for improved classification in data imbalanced scenarios</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Kopparapu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">657</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural network-based undersampling techniques</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Arefeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Nimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-granularity relabeled under-sampling algorithm for imbalanced data</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="page" from="109" to="083" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Radial-based undersampling for imbalanced data classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Koziarski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="107" to="262" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cusboost: Cluster-based under-sampling with boosting for imbalanced classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rayhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mahbub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shatabda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 2nd International Conference on Computational Systems and Information Technology for Sustainable Solution (CSITSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T.-Y</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fighting class imbalance with contrastive learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="466" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">SuperCon: Supervised contrastive learning for imbalanced skin lesion classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.05685</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="661" to="679" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">O</forename><surname>Henaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4182" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="776" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unleashing the power of contrastive self-supervised visual models via contrast-regularized fine-tuning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Bell system technical journal</title>
		<imprint>
			<date type="published" when="1948">1948</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lecture notes on information theory</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ajjanagadde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klusowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lab. Inf. Decis. Syst., Massachusetts Inst. Technol</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
	<note>Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Gowers</surname></persName>
		</author>
		<ptr target="https://drive.google.com/file/d/1V778zHQTx4XE8FxDgznt2jTshZzxAFot/view" />
		<title level="m">Topics in combinatorics</title>
		<imprint>
			<date type="published" when="2020">May 13, 2022. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Mathematical foundations of information theory</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Khinchin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957">1957</date>
			<publisher>Dover Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of information theory</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A unifying mutual information view of metric learning: Cross-entropy vs. pairwise losses</title>
		<author>
			<persName><forename type="first">M</forename><surname>Boudiaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Ziko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Piantanida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="548" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Machine learning: a probabilistic perspective</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14119</idno>
		<title level="m">Asymmetric loss for multi-label classification</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Fashion-MNIST: A novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A survey of speech emotion recognition in natural environment</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Fahad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deepak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="102" to="951" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">notion of entropy H(X) of a random variable X. Entropy measures the amount of information contained in X, usually in bits. For example, a fair coin toss contains one bit of information; the 0 bit can represent the heads whereas the 1 bit can represent the tails</title>
		<author>
			<persName><forename type="first">A</forename><surname>Axioms</surname></persName>
		</author>
		<author>
			<persName><surname>Entropy</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>On the other hand, an unfair coin toss whose coin always lands on heads gives no meaningful information. Hence, the trial can be conveyed using zero bits. This section aims to construct the theory of entropy via an axiomatic approach. First, a collection of axioms, known as the Shannon-Khinchin axioms [23], is employed to give desired properties of the function H(?). Then, it is shown that the usual formula for H(X) follows uniquely from these axioms. The presentation of the axioms in this section follows a set of notes provided by Gowers [22</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The first axiom is motivated using the coin toss example. Since a fair coin toss is expected to contain one bit of information, the following axiom is obtained. Axiom 0 (Normalization)</title>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Respectively</forename><surname>Let P X = P X (x) = P[x = X] And Q Y = P Y (y) = P[y = Y] For X ? X And Y ? Y</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Suppose that X and Y are discrete random variables taking values in finite spaces</title>
		<imprint/>
	</monogr>
	<note>If |X | = 2 and X has a uniform distribution, then H(X) = 1. Also, H(X) depends only on the probability distribution of X. Consequently. if Y is another random variable that has an identical distribution to X, then H(Y ) = H(X)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">H(X) depends only on the probability distribution of X, and not on any other factor. Going back to the coin toss example, we would like to ensure that a coin toss contains the most information when it is fair</title>
	</analytic>
	<monogr>
		<title level="m">Axiom 1 (Invariance)</title>
		<imprint/>
	</monogr>
	<note>In general, the following axiom is assumed</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
