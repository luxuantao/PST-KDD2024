<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SoGCN: Second-Order Graph Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-14">14 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Peihao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">SIST</orgName>
								<orgName type="institution" key="instit2">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuehao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hua</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">SIST</orgName>
								<orgName type="institution" key="instit2">ShanghaiTech University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Uisee Technology Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">AI Lab</orgName>
								<orgName type="institution">Uisee Technology Ltd</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">CIS</orgName>
								<orgName type="institution" key="instit2">University of Pennsylvania</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SoGCN: Second-Order Graph Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-14">14 Oct 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2110.07141v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Convolutional Networks (GCN) with multihop aggregation is more expressive than one-hop GCN but suffers from higher model complexity. Finding the shortest aggregation range that achieves comparable expressiveness and minimizes this side effect remains an open question. We answer this question by showing that multilayer second-order graph convolution (SoGC) is sufficient to attain the ability of expressing polynomial spectral filters with arbitrary coefficients. Compared to models with one-hop aggregation, multi-hop propagation, and jump connections, SoGC possesses filter representational completeness while being lightweight, efficient, and easy to implement. Thereby, we suggest that SoGC is a simple design capable of forming the basic building block of GCNs, playing the same role as 3 × 3 kernels in CNNs. We build our Second-Order Graph Convolutional Networks (SoGCN) with SoGC and design a synthetic dataset to verify its filter fitting capability to validate these points. For real-world tasks, we present the state-of-theart performance of SoGCN on the benchmark of node classification, graph classification, and graph regression datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph Convolutional Networks (GCNs) has gained popularity in recent years. Researchers have shown that nonlocalized multi-hop GCNs <ref type="bibr" target="#b26">(Liao et al., 2019;</ref><ref type="bibr" target="#b27">Luan et al., 2019;</ref><ref type="bibr" target="#b0">Abu-El-Haija et al., 2019)</ref> have better performance than localized one-hop GCNs <ref type="bibr" target="#b7">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b20">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b41">Wu et al., 2019)</ref>. However, in Convolutional Neural Networks (CNNs), the localized 3 × 3 kernels' expressiveness has been shown in image recognition both experimentally <ref type="bibr" target="#b38">(Simonyan &amp; Zisserman, 2014)</ref> and theoretically <ref type="bibr" target="#b44">(Zhou, 2020)</ref>. These contradictory observations motivate us to search for a maximally localized Graph Con- volution (GC) kernel with guaranteed feature expressivness.</p><p>Most existing GCN layers adopt localized graph convolution based on one-hop aggregation scheme as the basic building block <ref type="bibr" target="#b20">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b14">Hamilton et al., 2017;</ref><ref type="bibr" target="#b43">Xu et al., 2019)</ref>. The effectiveness of these one-hop models is based on the intuition that a richer class of convolutional functions can be recovered by stacking multiple one-hop layers. However, extensive works <ref type="bibr" target="#b23">(Li et al., 2018;</ref><ref type="bibr" target="#b33">Oono &amp; Suzuki, 2019;</ref><ref type="bibr" target="#b4">Cai &amp; Wang, 2020)</ref> have shown performance limitations of such design, which indicates this hypothesis may not hold. <ref type="bibr" target="#b26">Liao et al. (2019)</ref>; <ref type="bibr" target="#b27">Luan et al. (2019)</ref>; <ref type="bibr" target="#b0">Abu-El-Haija et al. (2019)</ref> observed that multi-hop aggregation run in each layer could lead to significant improvement in prediction accuracy. However, a longer-range aggregator introduces extra hyperparameters and higher computational cost. This design also contradicts the compositionality principle of deep learning that neural networks benefit from deep connections and localized kernels <ref type="bibr" target="#b22">(LeCun et al., 2015)</ref>.</p><p>Recent studies point out that one-hop GCNs suffer from filter incompleteness <ref type="bibr" target="#b16">(Hoang &amp; Maehara, 2019)</ref>. By relating low-pass filtering on the graph spectrum with oversmoothing, one can show one-hop filtering could lead to performance limitations. One natural solution of adding a more complex graph kernel, such as multi-hop connections, seems to work in practical settings <ref type="bibr" target="#b0">(Abu-El-Haija et al., 2019)</ref>. The question is: "what is the simplest graph kernel with the full expressive power of the graph convolution?"</p><p>We show that with a second-order graph kernel of "two-hop" connection, we could approximate any complicated graph relationships. Intuitively, it means we should extract a contrast between graph nodes that almost-connected (via their neighbor) vs. directly-connected.</p><p>We construct the two-hop graph kernel with second-order polynomials in an adjacency matrix and call it the Second-Order GC (SoGC). We show this Second-Order GC (SoGC) is the "sweet spot" balancing localization and fitting capability. To justify our conclusion, we introduce a Layer Spanning Space (LSS) framework to quantify the filter representation power of multi-layer GCs. Our LSS works by mapping GC filters' composition with arbitrary coefficients to polynomial multiplication (Section 3.1).</p><p>Under this LSS framework, we can show that SoGCs can approximate any linear GCNs in channel-wise filtering (Theorem 1). Vanilla GCN and GIN (first-order polynomials in adjacency matrix) cannot represent all polynomial filters in general; multi-hop GCs (higher-order polynomials in adjacency matrix) do not contribute more expressiveness (Section 3.2). In this sense, SoGC is the most localized GC kernel with the full representation power.</p><p>To validate our theory, we build our Second-Order Graph Convolutional Networks (SoGCN) by layering up SoGC layers (Section 3.3). We reproduce our theoretical results on a synthetic datasets for filtering power testing (Section 4.1).</p><p>On the public benchmark datasets <ref type="bibr" target="#b9">(Dwivedi et al., 2020)</ref>, SoGCN using simple graph topological features consistently boosts the performance of our baseline model (i.e., vanilla GCN) comparable to the state-of-the-art GNN models (with more complex attention/gating mechanisms). We also verify that our SoGCN fits extensive real-world tasks, including network node classification, super-pixel graph classification, and molecule graph regression (Section 4.3).</p><p>To our best knowledge, this work is the first study that identifies the distinctive competence of the two-hop neighborhood in the context of expressing a polynomial filter with arbitrary coefficients. Our SoGC is a special but non-trivial case of polynomial approximated graph filters <ref type="bibr" target="#b7">(Defferrard et al., 2016)</ref>. <ref type="bibr" target="#b20">Kipf &amp; Welling (2017)</ref> conducted an ablation study with GC kernels of different orders but missed the effectiveness of the second-order relationships. The work of Abu-El-Haija et al. ( <ref type="formula">2019</ref>) talked about muti-hop graph kernels; however, they did not identify the critical importance of the two-hop form. In contrast, we clarify the prominence of SoGCs in theories and experiments.</p><p>Our research on GCN using pure topologically relationship is orthogonal to those using geometric relations <ref type="bibr" target="#b31">(Monti et al., 2017;</ref><ref type="bibr" target="#b11">Fey et al., 2018;</ref><ref type="bibr" target="#b34">Pei et al., 2020)</ref>, or those with expressive edge features <ref type="bibr" target="#b25">(Li et al., 2016;</ref><ref type="bibr" target="#b12">Gilmer et al., 2017)</ref>, and hyper-edges <ref type="bibr" target="#b32">(Morris et al., 2019;</ref><ref type="bibr" target="#b28">Maron et al., 2018;</ref><ref type="bibr">2019)</ref>. It is also independent with graph sampling procedures <ref type="bibr" target="#b35">(Rong et al., 2019;</ref><ref type="bibr" target="#b14">Hamilton et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Spectral GCNs. Graph convolution is defined as elementwise multiplication on graph spectrum <ref type="bibr" target="#b15">(Hammond et al., 2011)</ref>. <ref type="bibr" target="#b3">Bruna et al. (2014)</ref> first proposed spectral GCN with respect to this definition. ChebyNet <ref type="bibr" target="#b7">(Defferrard et al., 2016)</ref> approximates graph filters using Chebyshev polynomials.</p><p>Vanilla GCN <ref type="bibr" target="#b20">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b41">Wu et al., 2019)</ref> further reduces the GC layer to a degree-one polynomial with lumping of first-order and constant terms. GIN <ref type="bibr" target="#b43">(Xu et al., 2019)</ref> disentangles the effect of self-connection and pairwise neighboring connections by adding a separate mapping for central nodes. However, these simplifications causes performance limitations <ref type="bibr" target="#b33">(Oono &amp; Suzuki, 2019;</ref><ref type="bibr" target="#b4">Cai &amp; Wang, 2020)</ref>. APPNP <ref type="bibr" target="#b21">(Klicpera et al., 2019)</ref> uses Personalized PageRank to derive a fixed polynomial filter. <ref type="bibr" target="#b1">Bianchi et al. (2019)</ref> proposes a multi-branch GCN architecture to simulate ARMA graph filters. GCNII <ref type="bibr" target="#b30">(Ming Chen et al., 2020)</ref> incorporates identity mapping and initial mapping to relieve over-smoothing problem and deepen GCNs. However, these models are not easy to implement and introduce additional hyper-parameters.</p><p>Multi-Hop GCNs. To exploit multi-hop information, <ref type="bibr" target="#b26">Liao et al. (2019)</ref> proposes to use Lanczos algorithm to construct low rank approximations of the graph Laplacian for graph convolution. <ref type="bibr" target="#b27">Luan et al. (2019)</ref> devises two architectures Snowball GCN and Truncated Krylov GCN to capture neighborhoods at various distances. To simulate neighborhood delta functions, Abu-El-Haija et al. ( <ref type="formula">2019</ref>) repeat mixing multi-hop features to identify more topological information. JKNet <ref type="bibr" target="#b42">(Xu et al., 2018)</ref> combines all feature activation of previous layers to learn adaptive and structure-aware representations of different graph substructures. These models exhibit the strength of multi-hop GCNs over one-hop GCNs while leaving the propagation length as a hyper-parameter. In the meanwhile, long-range aggregation in each layer causes higher complexity (Table <ref type="table" target="#tab_0">1</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expressiveness of GCNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Second-Order Graph Convolution</head><p>We begin by introducing our notation. We are interested in learning on a finite graph set</p><formula xml:id="formula_0">G = G 1 , • • • , G |G| .</formula><p>Assume each graph G ∈ G is simple and undirected, associated with a finite vertex set V(G), an edge set E(G) ⊆ V(G) × V(G), and a symmetric normalized adjacency matrix A(G) <ref type="bibr" target="#b6">(Chung &amp; Graham, 1997;</ref><ref type="bibr" target="#b37">Shi &amp; Malik, 2000)</ref>. Without loss of generality and for simplicity,</p><formula xml:id="formula_1">|V(G)| = N for every G ∈ G. We denote single-channel signals supported in graph G ∈ G as x ∈ R N , a vectorization of function V(G) → R.</formula><p>Graph Convolution (GC) is defined as Linear Shift-Invariant (LSI) operators to adjacency matrices <ref type="bibr" target="#b36">(Sandryhaila &amp; Moura, 2013)</ref>. This property enables GC to extract features regardless of where the local structure falls. A singlechannel GC can be written as a mapping f : G ×R N → R N . According to <ref type="bibr" target="#b7">Defferrard et al. (2016)</ref>, a GC can be approximated by a polynomial in adjacency matrix (Figure <ref type="figure" target="#fig_0">1c</ref>)<ref type="foot" target="#foot_0">1</ref> :</p><formula xml:id="formula_2">f θ (G, x) = K k=0 θ k A(G) k x,<label>(1)</label></formula><p>where</p><formula xml:id="formula_3">θ = θ 0 • • • θ K T ∈ R K+1</formula><p>represents the kernel weights. The Equation 1 indicates that graph convolution can be interpreted as a linear combination of features aggregated by A(G) k . Thereby, the hyperparameter K can reflect the localization of a GC kernel.</p><p>Previous work of <ref type="bibr" target="#b20">Kipf &amp; Welling (2017)</ref> simplified the Equation 1 to a one-hop kernel, so-called vanilla GC (Figure <ref type="figure" target="#fig_0">1a</ref>). We formulate vanilla GC as below:</p><formula xml:id="formula_4">f 1 (G, x) = θ (A(G) + I) x.<label>(2)</label></formula><p>We are interested in the overall graph convolution networks' representation power of expressing a polynomial filter (cf. Equation <ref type="formula" target="#formula_2">1</ref>) with arbitrary degrees and coefficients. At first glance, one-hop GC (cf. Equation <ref type="formula" target="#formula_4">2</ref>) can approximate any high-order GC kernels by stacking multiple layers. However, that is not the case (See formal arguments in Section 3.2). In contrast, when plugging the second-order term into vanilla GC, we will show that this approximation ability can be attained (See formal arguments in Theorem 1). We name this improved design Second-Order Graph Convolution (SoGC), as it can be written as the second-order polynomial in adjacency matrix:</p><formula xml:id="formula_5">f 2 (G, x) = θ 2 A(G) 2 + θ 1 A(G) + θ 0 I x.<label>(3)</label></formula><p>We illustrate its vertex-domain interpretation in Figure <ref type="figure" target="#fig_0">1b</ref>. The critical insight is that graph filter approximation can be viewed as a polynomial factorization problem. It is known that any univariate polynomial can be factorized into subpolynomials of degree two. Based on this fact, we show by stacking enough SoGCs (and varying their parameters) can achieve decomposition of any polynomial filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Representation Power Justification</head><p>To be more precise, we introduce our Layer Spanning Space (LSS) framework in this subsection and mathematically prove that arbitrary GC kernel can be decomposed into finite many SoGC kernels.</p><p>First, to illustrate overall graph filter space and filters expressed by a single layer, we define a graph filter space, in which every polynomial filter has degree no more than K: Definition 1. (Graph filter space) Suppose the parameter space is R. The Linear Shift-Invariant (LSI) graph filter space<ref type="foot" target="#foot_1">2</ref> of degree K &gt; 0 with respect to a finite graph set G is defined as</p><formula xml:id="formula_6">F K = f θ : G × R N → R N , ∀θ ∈ R K+1 ,</formula><p>where f θ follows the definition in Equation <ref type="formula" target="#formula_2">1</ref>.</p><p>We further provide Definition 2 and Lemma 1 to discuss the upper limit of F K 's dimension. Visualizing output activation in graph spectrum domain for vanilla GCN, SoGCN, and GRU variants. The test is conducted on a graph from the ZINC dataset. The spectrum is defined as a projection of activation functions on the graph eigenvectors. SoGCN preserved higher-order spectrum, while vanilla GCN shows over-smoothing. See Appendix F for more visualizations on the ZINC dataset.</p><p>eigenvalues of A. Spectrum capacity Γ = |S(G)| is the cardinality of all distinct graph eigenvalues. In particular, Γ = (N − 1)|G| if every graph adjacency matrix has no common eigenvalues other than 1.</p><p>Lemma 1. Filter space F K with degree K &gt; 0 has dimension min{K + 1, Γ} as a vector space.</p><p>Lemma 1 follows from Theorem 3 of <ref type="bibr" target="#b36">Sandryhaila &amp; Moura (2013)</ref>. See the complete proof in Appendix C.</p><p>According to Lemma 1, one can define an ambient filter space A of degree Γ − 1 (i.e., F Γ−1 ). Suppose a GCN consists of K-hop convolutional kernels</p><formula xml:id="formula_7">f (l) , l = 1, • • • , L,</formula><p>where the superscript indicates the layer number, and L denotes the network depth. We can consider each layer is sampled from F K . We intend to justify this GCN's filter representation power via its Layer Spanning Space (LSS):</p><formula xml:id="formula_8">F L K = F : F = f (1) • • • • • f (L) , ∀f (l) ∈ F K ,<label>(4)</label></formula><p>where the whole LSS is constructed by varying parameters of f (l) over R K+1 . When A ⊆ F L K , the LSS covers the entire ambient space, then we say the GCN composed of L GC kernels in F K has full filter representation power.</p><p>As F K is a function space, F L K can be analytically tricky. To investigate this space, we define a mapping τ :</p><formula xml:id="formula_9">F K → R K [x]</formula><p>, where R K [x] denotes a polynomial vector space of degree at most K:</p><formula xml:id="formula_10">τ : K k=0 θ k A(G) k → K k=0 θ k x k ,<label>(5)</label></formula><p>We provide Lemma 2 to reveal a good property of τ :</p><formula xml:id="formula_11">Lemma 2. τ is a ring isomorphism when K ≤ Γ − 1.</formula><p>The proof can be found in Appendix D.</p><p>This ring isomorphism signifies that the composition of filters in F K is identical to polynomial multiplication. Therefore, one can study the LSS through the polynomials that can be factorized into the corresponding sub-polynomials. For example, Equation 4 is identical to:</p><formula xml:id="formula_12">F L K p(x) : p(x) = L l=1 K k=0 θ (l) k x k , θ<label>(l)</label></formula><formula xml:id="formula_13">k ∈ R . (6)</formula><p>In the rest of this subsection, we will show that Γ−1 2 -layer SoGCs (i.e., F 2 ) can attain full representation power. That is, F L 2 covers the whole ambient filter space A when L is as large as (Γ − 1)/2 . We summarize a formal argument in the following theorem:</p><formula xml:id="formula_14">Theorem 1. For any f ∈ A, there exists f (l) 2 ∈ F 2 with coefficients θ (l) 0 , θ (l) 1 , θ (l) 2 ∈ R, l = 1, • • • , L such that f = f (L) 2 • • • • • f (1) 2 where L ≤ (Γ − 1)/2 .</formula><p>Proof. Proving Theorem 1 requires a fundamental polynomial factorization theorem, rephrased as below: Lemma 3. (Fundamental theorem of algebra) Over the field of reals, the degree of an irreducible non-trivial univariate polynomial is either one or two. <ref type="formula" target="#formula_10">5</ref>) to map kernel f to polynomial h(x). By Lemma 1, deg h(x) ≤ Γ − 1. By Lemma 3, factorize h(x) into series of polynomials with the degree at most two, and then merge first-order polynomials into second-order ones until one single or no first-order sub-polynomial remains. As a consequence, h(x) can be written as h</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For any</head><formula xml:id="formula_15">f ∈ A, apply τ : A → R Γ−1 [x] (cf. Equation</formula><formula xml:id="formula_16">(x) = L l=1 h l (x), where L = deg h(x)/2 . If deg h(x) is even, deg h l (x) = 2 for every l = 1, • • • , D.</formula><p>Otherwise, except for at most one h l (x) whose degree is one, all terms have degree two.</p><p>The last step is to apply the inverse of morphism τ −1 : R Γ−1 [x] → A formulated as below:</p><formula xml:id="formula_17">τ −1 : K k=0 θ k x k → K k=0 θ k A(G) k .</formula><p>Since τ −1 is also a ring isomorphism, we have:</p><formula xml:id="formula_18">τ −1 (h(x)) = τ −1 (h 1 (x) • • • h L (x)) = τ −1 (h 1 (x)) • • • • • τ −1 (h L (x)) = f (1) • • • • • f (L) where f (l) ∈ F 2 , l = 1, • • • , L by definition, which implies f = τ −1 (τ (f )) = f (L) • • • • • f (1) .</formula><p>Theorem 1 can be regarded as the universal approximation theorem of linear GCNs. Although nonlinear activation is not considered within our theoretical framework, we make reasonable hypothesis that achieving linear filter completeness can also boost GCNs with nonlinearity (Ming <ref type="bibr" target="#b30">Chen et al., 2020)</ref>. See our experiments in Section 4.3.</p><p>Theorem 1 implies that multi-layer SoGC can implement arbitrary filtering effects and extract features at any positions on the spectrum (Figure <ref type="figure">2</ref>). Theorem 1 also coincides with <ref type="bibr" target="#b8">Dehmamy et al. (2019)</ref> on how GCNs built on SoGC kernels could utilize depth to raise fitting accuracy (Figure <ref type="figure" target="#fig_2">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Compared with Other Graph Convolution</head><p>In this subsection, we will show that vanilla GCN (and GIN) does not attain full expressiveness in terms of filter representation. We will also contrast our SoGC to multihop GCs (i.e., higher-order GCs in our terminology) to further reveal the prominence of SoGC. A brief comparison is summarized in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Vanilla vs. second-order. Vanilla GCN <ref type="bibr" target="#b20">(Kipf &amp; Welling, 2017</ref>) is a typical one-hop GCN with lumbing of of graph node self-connection and pairwise neighboring connection. Compared with SoGC, vanilla GC is more localized and computationally cheaper. However, this design has huge performance limitations <ref type="bibr" target="#b16">(Hoang &amp; Maehara, 2019;</ref><ref type="bibr" target="#b41">Wu et al., 2019;</ref><ref type="bibr" target="#b23">Li et al., 2018;</ref><ref type="bibr" target="#b33">Oono &amp; Suzuki, 2019;</ref><ref type="bibr" target="#b4">Cai &amp; Wang, 2020)</ref>. We illustrate this issue in terms of filter approximation power based on the LSS framework. Suppose a GCN stacks L GC layers f (l) 1 (G, x) ∈ F 1 , apply mapping τ to its spanned LSS, the isomorphic polynomial space is:</p><formula xml:id="formula_19">F L 1 p : p(x) = Θ L l=0 l L x l , Θ ∈ R ,<label>(7)</label></formula><p>where 1) . According to Equation <ref type="formula" target="#formula_19">7</ref>, one can see no matter how large L is or how a optimizer tunes the parameters θ (l) , dim F L 1 = 1, which implies F L 1 degenerates to a negligible subspace inside A. GIN <ref type="bibr" target="#b43">(Xu et al., 2019)</ref> disentangles the the weights for neighborhoods and central nodes. We can write this GC layer as f</p><formula xml:id="formula_20">Θ = θ (L) • • • θ (</formula><formula xml:id="formula_21">(l) GIN (G, x) = (θ 1 A(G) + θ 0 I)x.</formula><p>The LSS of GIN is isomorphic to the polynomial space:</p><formula xml:id="formula_22">F L GIN p : p(x) = L l=1 θ (l) 1 x + θ (l) 0 , θ (l) 0 , θ (l) 1 ∈ R . (8)</formula><p>This polynomial space represents all polynomials that can split over the real domain. However, F L GIN A since not all polynomials can be factorized into first-order polynomials. The expectation number of real roots of a Kdegree polynomial with zero-mean random coefficients is E(N ) ∼ (2/π) log K <ref type="bibr" target="#b18">(Ibragimov &amp; Maslova, 1971)</ref>. When the ambient dimension goes larger, all-real-root polynomials only occupy a small proportion in the ambient space <ref type="bibr" target="#b24">(Li, 2011)</ref>, which indicates GIN does not have full expressiveness in terms of filter representation either.</p><p>Higher-order vs. second-order. Higher-order GCs refer to those polynomial filters with degree larger than three (i.e., F K , K ≥ 3). They can model multi-hop GCNs such as <ref type="bibr" target="#b27">Luan et al. (2019)</ref>; <ref type="bibr" target="#b26">Liao et al. (2019)</ref>; Abu-El-Haija et al. ( <ref type="formula">2019</ref>). Compared to SoGCs, higher-order GCs have equivalent expressive power, since they can be reduced to SoGCs. However, we point out four limitations of adopting higher-order kernels: 1) From our polynomial factorization perspective, fitting graph filters using higher-order GC requires coefficient sparsity, which brings about learning difficulty. <ref type="bibr" target="#b0">Abu-El-Haija et al. (2019)</ref> overcomes this problem by adding lasso regularization and extra training procedures. Adopting SoGC can avoid these troubles since decomposition into second-order polynomials results in at most one zero coefficient (See Section 3.1). 2) Eigenvalues of graph adjacency matrices diminish when powered. This leads to a decreasing numerical rank of A(G) k and makes aggregating larger-scale information ineffective. SoGCs can alleviate this problem by preventing higher-order powering operations. 3) Higher-order GC lacks nonlinearity. SoGCN can bring a better balance between the expressive power of low-level layers and nonlinearity among them. 4) Multi-hop aggregation consumes higher computational resources (See Table <ref type="table" target="#tab_0">1</ref>). In contrast, SoGC matches the time complexity of vanilla GCN by fixing the kernel size to two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation of Second-Order Graph Convolutional Networks</head><p>In this subsection, we introduce other building blocks to establish our Second-Order Graph Convolutional Networks (SoGCN) following the latest trends of GCN. We promote SoGC to its multi-channel version analogous to <ref type="bibr" target="#b20">Kipf &amp; Welling (2017)</ref>. Then we prepend a feature embedding layer, cascade multiple SoGC layers, and append a readout module. Suppose the network input is X ∈ R N ×D supported in graph G ∈ G, denote the output of l-th layer as X (l) ∈ R N ×E , the final node-level output as Y ∈ R N ×F , or graphlevel output as Y ∈ R E , we formulate our novel deep GCN built with SoGC (cf. Equation <ref type="formula" target="#formula_5">3</ref>) as follows:</p><formula xml:id="formula_23">X (0) = φ (X; Φ) ,<label>(9)</label></formula><formula xml:id="formula_24">X (l) = σ f (l) 2 X (l−1) ; Θ (l) ,<label>(10)</label></formula><formula xml:id="formula_25">Y = ψ X (L) ; Ψ ,<label>(11)</label></formula><p>where</p><formula xml:id="formula_26">Θ (l) = {Θ (l)</formula><p>i ∈ R E×E } i=0,1,2 are trainable weights for linear filters f (l) 2 ∈ F 2 ; φ : R N ×D → R N ×E is an equivariant embedder <ref type="bibr" target="#b28">(Maron et al., 2018)</ref> with parameters Φ; σ : R N ×E → R N ×E is an activation function. For node-level readout, ψ : R N ×E → R N ×F can be a decoder (with parameters Ψ) or an output activation (e.g., softmax) in place of the prior layer. For graph-level output, ψ : R N ×E → R E should be an invariant readout function <ref type="bibr" target="#b28">(Maron et al., 2018)</ref>, e.g., channel-wise sum, mean or max. In practice, we adopt ReLU as nonlinear activation (i.e., σ = ReLU), a multi-layer perceptron (MLP) as the embedding function φ, another MLP for node regression readout, and sum <ref type="bibr" target="#b43">(Xu et al., 2019)</ref> for graph classification readout.</p><p>We also provide a variant of SoGCN integrated with Gated Recurrent Unit (GRU) <ref type="bibr" target="#b13">(Girault et al., 2015)</ref>, termed SoGCN-GRU. According to <ref type="bibr" target="#b5">Cho et al. (2014)</ref>, GRU can utilize gate mechanism to preserve and forget information. We hypothesize that a GRU can be trained to remove redundant signals and retain lost features on the spectrum. Similar to <ref type="bibr" target="#b25">Li et al. (2016)</ref>; <ref type="bibr" target="#b12">Gilmer et al. (2017)</ref>, we append a shared GRU module after each GC layer, which takes the signal before the GC layer as the hidden state, after the GC layer as the current input. We show by our experiment that GRU can facilitate SoGCN in avoiding noises and enhancing features on the spectrum (Figure <ref type="figure">2</ref>). Our empirical study in Table <ref type="table" target="#tab_5">5</ref> also indicates the effectiveness of GRU for spectral GCNs is general. Hence, we suggest including this recurrent module as another basic building block of our SoGCNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Synthetic Graph Spectrum Dataset for Filter Fitting Power Testing</head><p>To validate the expressiveness of SoGCN, and its power to fit arbitrary graph filters, we build a Synthetic Graph Spectrum (SGS) dataset for the node signal filtering regression task. We construct SGS dataset with random graphs. The learning task is to simulate three types of hand-crafted filtering functions: high-pass, low-pass, and band-pass on the graph spectrum (defined over the graph eigenvectors). There are 1k training graphs, 1k validation graphs, and 2k testing graphs for each filtering function. Each graph is undirected and comprises 80 to 120 nodes. Appendix E covers more details of our SGS dataset. We choose Mean Absolute Error (MAE) as evaluation metric.</p><p>Experimental Setup. We compare SoGCN with vanilla GCN <ref type="bibr" target="#b20">(Kipf &amp; Welling, 2017)</ref>, GIN <ref type="bibr" target="#b43">(Xu et al., 2019)</ref>, and higher-order GCNs on the synthetic dataset. To evaluate each model's expressiveness purely on the GC kernel design, we remove ReLU activations for all tested models. We adopt the Adam optimizer <ref type="bibr" target="#b19">(Kingma &amp; Ba, 2015)</ref> in our training process, with a batch size of 128. The learning rate begins with 0.01 and decays by half once the validation loss stagnates for more than 10 training epochs.</p><p>Results and Discussion. Table <ref type="table" target="#tab_2">2</ref> summarizes the quantitative comparisons. SoGCN achieves the superior performance on all of the 3 tasks outperforming vanilla GCN and GIN, which implies that SoGC graph convolutional kernel does benefit from explicit disentangling of the second-hop neighborhoods. Our results also show that higher-order (3rdorder and 4th-order) GCNs do not improve the performance further, even though they incorporate much more parameters. SoGCN is more expressive and does a better trade-off between performance and model size.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> plots MAE results as we vary the depth of GC layers for each graph kernel type. Vanilla GCN and GIN can not benefit from depth while SoGC and higher-order GCs can leverage depth to span larger LSS, contributing to the remarkable filtering results. SoGC and higher-order GCs have very close performance after increasing the layer size, which suggests higher-order GCs do not obtain more expressiveness than SoGC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">OGB Benchmarks</head><p>We choose Open Graph Benchmark (OGB) <ref type="bibr" target="#b17">(Hu et al., 2020)</ref> to compare our SoGC with other GCNs in terms of the parameter numbers, train time per epoch, and test ROC-AUC. We only demonstrate the results for predicting presence of protein functions (multi-label graph classification). We refer interested reader to Appendix G for more results on OGB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Setup</head><p>The chosen models mainly include spectral-domain models: vanilla GCN, GIN, APPNP <ref type="bibr" target="#b21">(Klicpera et al., 2019)</ref>, GCNII (Ming Chen et al., 2020), our SoGCN, and two high-order GCNs. We also obtain the performance of GraphSage, the vertex-domain GNN baseline, for a reference. We build GIN, GCNII, GraphSage, and APPNP based on official implementations in PyTorch Geometric <ref type="bibr">(Fey &amp; Lenssen, 2019)</ref>. Every model consists of three GC layers, and the same node embedder and readout modules. We borrow the method of <ref type="bibr" target="#b9">Dwivedi et al. (2020)</ref> to compute the number of paramters. We run an exclusive training program on an Nvidia Quadro P6000 GPU to test the training time per epoch. We follow the same training and evaluation procedures on the OGB benchmarks to ensure fair comparisons. We train each model until convergence (˜1k epochs for vanilla GCN, GIN, GraphSage, and ˜3k epochs for SoGCN, higher-order GCNs, APPNP, GCNII).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion. Table 4 demonstrates the ROC-</head><p>AUC score for each model on ogb-protein dataset. Our SoGCN achieves the best performance among all presented GCNs but its parameter number and time complexity is only slightly higher than GIN (consistent with Table <ref type="table" target="#tab_0">1</ref>). SoGC is more expressive than other existing graph filters (such as APPNP and GCNII), and also outperforms message-passing GNN baseline GraphSage. Compared with higher-order (4th and 6th) GCNs, the ROC-AUC score of SoGCN surpasses all of them while reducing model complexity significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">GNN Benchmarks</head><p>We follow the benchmarks outlined in <ref type="bibr" target="#b9">Dwivedi et al. (2020)</ref> for evaluating GNNs on several datasets across a variety of artificial and real-world tasks. We choose to evaluate our SoGCN on a real-world chemistry dataset (ZINC molecules) for the graph regression task, two semi-artificial computer vision datasets (CIFAR10 and MNIST superpixels) for the graph classification task, and two artificial social network datasets (CLUSTER and PATTERN) for node classification.</p><p>Experimental Setup. We compare our proposed SoGCN and SoGCN-GRU with state-of-the-art GNNs: vanilla GCN, GIN, GraphSage, GAT <ref type="bibr" target="#b39">(Veličković et al., 2018)</ref>, MoNet <ref type="bibr" target="#b31">(Monti et al., 2017)</ref>, GatedGCN <ref type="bibr" target="#b2">(Bresson &amp; Laurent, 2017)</ref> and 3WL-GNN <ref type="bibr" target="#b29">(Maron et al., 2019)</ref>. To ensure fair comparisons, we follow the same training and evaluation pipelines (including optimizer settings) and data splits of benchmarks. Furthermore, we adjust our model's depth and width to ensure it satisfies parameter budgets as specified in the benchmark. Note that we do not use any geometrical information to encode rich graph edge relationship, as in models such as GatedGCN-E-PE. We only employ graph connectivity information for all tested models.</p><p>Results and Discussion. Table <ref type="table" target="#tab_3">3</ref> reports the benchmark results. Our model SoGCN makes small computational changes to GCN by adopting second-hop neighborhood, and it outperforms models with complicated message-passing mechanisms, such as GAT and GraphSage. With GRU module, SoGCN-GRU tops almost all state-of-the-art GNNs on the ZINC, MNIST and CIFAR10 datasets. In Figure <ref type="figure">2</ref>, we visualize a spectrum of the last layer's feature activation on ZINC dataset. One can see our SoGC can extract features on high-frequency bands and GRU can further sharpen these patterns. However, GRU does not lift accuracy on CLUSTER and PATTERN datasets for node classification task. According to <ref type="bibr" target="#b23">Li et al. (2018)</ref>, that GRU suppresses low-frequency band results in the slight performance drop on the CLUSTER and PATTERN datasets.</p><p>Ablation Study. To contrast the performance gain produced by different aggregation ranges and GRU on the benchmarks, we evaluate vanilla GCN, SoGCN, 4th-Order GCN, 6th-Order GCN as well as their GRU variants on the ZINC, MNIST and CIFAR10 datasets. Table <ref type="table" target="#tab_5">5</ref> presents the results of our ablation study, which are consistent with our observation on Section 4.1 and 4.2. As shown by our ablation study, adopting the second-hop aggregation makes huge performance gain (vanilla GCN vs. SoGCN). However, high-order GCNs are not capable of boosting the performance further over SoGCN. On the contrary, higher-order GCs can even lead to the performance drop (4th-Order GCN vs. 6th-Order GCN vs. SoGCN). We also testify GRU's effectiveness for each presented model. But the gain brought by GRU is not as large as adding second-hop aggregation. Figure <ref type="figure">2</ref> shows our SoGC can extract patterns on the spectrum alone. GRU plays a role of enhancing the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>What should be the basic convolutional blocks for GCNs? To answer this, we seek the most localized graph convolution kernel (GC) with full expressiveness. We establish our LSS framework to assess GC layers of different aggregation ranges. We show the second-order graph convolutional filter, termed SoGC, possesses the full representation power than one-hop GCs. Hence, it becomes the efficient and simplest GC building blocks that we adopt to establish our SoGCN. Both synthetic and benchmark experiments exhibit the prominence of our theoretic design. We also make an empirical study on the GRU's effects in spectral GCNs. Interesting directions for future work include analyzing twohop aggregation schemes with message-passing GNNs and proving the universality of nonlinear GCNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Remark on Definition 1</head><p>Let us rewrite the F K following Definition 1:</p><formula xml:id="formula_27">F K = f : f (G, x) = K k=0 θ k A(G) k x, ∀θ k ∈ R .</formula><p>We claim that functions f ∈ F K : G × R N → R N are all Linear Shift-Invariant (LSI) to adjacency matrix.</p><p>Proof. Given arbitrary graph G ∈ G, any filter H associated with it can be written as below:</p><formula xml:id="formula_28">H(G) = K k=0 θ k A(G) k = U K k=0 θ k Λ k U T ,</formula><p>where A(G) = U ΛU T is the eigendecomposition of A(G). Therefore, H(G) is also diagonalized by the eigenvectors of A(G). By the Lemma 4: Lemma 4. Diagonalizable matrices A 1 and A 2 are simultaneously diagonalized if and only if</p><formula xml:id="formula_29">A 1 A 2 = A 2 A 1 . we say that H(G) commutes with A(G). For any f ∈ F K , A(G)f (G, x) = f (G, A(G)x). B. Ring Isomorphism π : F K → T K</formula><p>We introduce a mathematical device that bridges the gap between the filter space F K and the polynomial space</p><formula xml:id="formula_30">R K [x].</formula><p>Since G is finite, we can construct a block diagonal matrix T ∈ R N |G|×N |G| , with adjacency matrix of every graph on the diagonal:</p><formula xml:id="formula_31">T =    A(G 1 ) . . . A(G |G| )    ∈ R N |G|×N |G| . (12)</formula><p>Remark 1. The spectrum capacity Γ in Definition 2 represents the number of eigenvalues of T without multiplicity.</p><p>Eigenvalues of adjacency matrices signify graph similarity. The spectrum capacity Γ identifies a set of graphs by enumerating the structural patterns. Even if the graph set goes extremely large (to guarantee the generalization capability), the distribution of spectrum provides the upper bound of Γ, so our theories remain their generality. Now we construct a matrix space T K by applying a ring homomorphism π : F K → T K to every element in F K :</p><formula xml:id="formula_32">π : K k=0 θ k A(G) k → K k=0 θ k T k .<label>(13)</label></formula><p>Concretely, we write the matrix space T as follows:</p><formula xml:id="formula_33">T K = H : H = K k=0 θ k T k , ∀θ k ∈ R .<label>(14)</label></formula><p>In the rest section, we prove that π is a ring isomorphism.</p><p>Proof. First, we can verify that π is a ring homomorphism because it is invariant to "summation" and "multiplication". Second, we can prove its surjectivity by the definition of T K (cf. Equation <ref type="formula" target="#formula_33">14</ref>).</p><p>Finally, we show its injectivity as follows: Consider any pair of</p><formula xml:id="formula_34">f 1 , f 2 ∈ F K , f 1 = f 2 with parameters α k , β k ∈ R, k = 0, • • • , K, there exists G j ∈ G and x ∈ R N such that f 1 (G j , x) = f 2 (G j , x).</formula><p>After applying π, we have their images</p><formula xml:id="formula_35">H 1 = π(f 1 ), H 2 = π(f 2 ). Let ξ = 0 T N (j−1) x T 0 T N (|G|−j) T</formula><p>, where 0 N denote the all-zero vector of length N , then we have:</p><formula xml:id="formula_36">H 1 ξ = 0 T N (j−1) K k=0 α k A(G j ) k x T 0 T N (|G|−j) T = 0 T N (j−1) f 1 (G j , x) T 0 T N (|G|−j) T , H 2 ξ = 0 T N (j−1) K k=0 β k A(G j ) k x T 0 T N (|G|−j) T = 0 T N (j−1) f 2 (G j , x) T 0 T N (|G|−j) T .</formula><p>Hence, H 1 = H 2 concludes the injectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proof of Lemma 1</head><p>Proof. One can show F K is a vector space by verifying the linear combination over F K is closed (or simply implied from the ring isomorphism π).</p><p>Due to isomorphism, dim F K = dim T K . Then Lemma 1 follows from Theorem 3 of <ref type="bibr" target="#b36">Sandryhaila &amp; Moura (2013)</ref>. We briefly conclude the proof as below.</p><p>Let m(x) denote the minimal polynomial of T . We have</p><formula xml:id="formula_37">Γ = deg m(x). Suppose K + 1 &lt; Γ. First, dim T K cannot be larger than K + 1, because I, T , • • • , T K is a span- ning set. If dim T K &lt; K + 1,</formula><p>then there exists some polynomial p(x) with deg p(x) &lt; K, such that p(T ) = 0. This contradicts the minimality of m(x). Therefore, dim T K can only be K + 1.</p><p>Suppose K + 1 ≥ Γ. For any H = h(T ) where polynomial h(x) has deg h(x) ≤ K. By polynomial division, there exists unique polynomials q(x) and r(x) such that</p><formula xml:id="formula_38">h(x) = q(x)m(x) + r(x),<label>(15)</label></formula><p>where deg r(x) &lt; deg m(x) = Γ. We insert T into Equation 15 as below:</p><formula xml:id="formula_39">h(T ) = q(T )m(T ) + r(T ) = q(T )0 + r(T ) = r(T ).</formula><p>Therefore, I, T , • • • , T Γ−1 form a basis of T K , i.e., dim T K = Γ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Proof of Lemma 2</head><p>Proof. Consider a mapping ϕ :</p><formula xml:id="formula_40">T K → R K [x]: ϕ : K k=0 θ k T k → K k=0 θ k x k . (<label>16</label></formula><formula xml:id="formula_41">) When K + 1 ≤ Γ, dim T K = dim R K [x] (as deg m(x) = Γ)</formula><p>, which implies ϕ is a ring isomorphism as well. Since function composition preserves isomorphism property, we can conclude the proof by showing that τ = ϕ • π.</p><p>Remark 2. The assumption that each graph has the same number of vertices is made only for the sake of simplicity. Lemma 1 and Lemma 2 still hold when the vertex numbers are varying, since the construction of T (cf. Equation <ref type="formula">12</ref>) is independent of this assumption.</p><p>Remark 3. The graph set G need to be finite, otherwise Γ might be uncountable. We leave the discussion on infinite graph sets for future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Synthetic Graph Spectrum Dataset</head><p>Our Synthetic Graph Spectrum (SGS) dataset is designed for testing the filter fitting power of spectral GCNs. </p><formula xml:id="formula_42">s t = 2 i=1 g[t/N ; a i , b i ] + 4 j=1 c j f [t; µ j , σ j ], t ∈ [N ] a i , b i ∼ Unif{0.1, 5}, µ j ∼ Unif{0, N }, σ j ∼ Unif N (j + 1) , N j 9 , c j ∼ Unif{0.5, 2} max x∈[N ] f [x; µ j , σ j ] ,<label>(17)</label></formula><p>We can retrieve the vertex-domain signals via inverse graph Fourier transformation: x = U s. Then Gaussian noise is added to the vertex-domain signals to simulate observation errors: x = x + , ∼ Norm(0, c), c ∼ Unif(0.05, 0.35).</p><p>We design three filters F * HP , F * LP , F * BP in Equation <ref type="formula" target="#formula_43">18</ref>:</p><formula xml:id="formula_43">f * HP (s) = 1 1 + ζ(s; 50, 1) , f * LP (s) = 1 − 1 1 + ζ(s; 50, 1) f * BP (s) = −1 1 + ζ(s; 100, 1.05) + 1 1 + ζ(s; 100, 0.95) , F * k = U f * k (Λ)U T , k ∈ {HP, LP, BP },<label>(18)</label></formula><p>where ζ(s; α, β) = exp{−α(s − β)}. For supervising purpose, we applying each filter to synthetic inputs to generate the groundtruth output: y = F * k x, k ∈ {HP, LP, BP }. Figure <ref type="figure">4</ref> illustrates an example of the generated spectral signals and the groundtruth responses of three filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. More Visualizations of Spectrum</head><p>For multi-channel node signals X ∈ R N ×D , where N is the number of nodes and D is the number of signal channels, the spectrum of X is computed by S = U T X. More information about the graph spectrum and graph Fourier transformation can be found in <ref type="bibr" target="#b36">Sandryhaila &amp; Moura (2013)</ref>.</p><p>Figure <ref type="figure">5</ref> shows the output spectrum of vanilla GCN, GIN and SoGCN on the synthetic Band-Pass dataset. The visualizations are consistent with the results in Table <ref type="table" target="#tab_2">2</ref> and Figure <ref type="figure" target="#fig_2">3</ref> in the main text. Vanilla GCN almost loses all the band-pass frequency, resulting in very poor performance. GIN learns to pass a part of middle-frequency band but still has a distance from the groundtruth. SoGCN's filtering response is close to the groundtruth response, showing its strong ability to represent graph signal filters.</p><p>We arbitrarily sample graph data from the ZINC dataset as input and visualize the output spectrum of vanilla GCN, SoGCN and their GRU variants in Figure <ref type="figure" target="#fig_4">6</ref>. Each curve in the visualization figure represents the spectrum of each output channel, i.e., each column of S is plotted as a curve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. More Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. Additional Experiments on SGS Dataset</head><p>We supplement two experiments to compare vanilla GCN <ref type="bibr" target="#b20">(Kipf &amp; Welling, 2017)</ref>, GIN <ref type="bibr" target="#b43">(Xu et al., 2019)</ref>, SoGCN and 4th-order GCN on synthetic High-Pass and Low-Pass datasets, respectively. With Figure <ref type="figure" target="#fig_5">7</ref>, we conclude that SoGCN and high-order GCNs perform closely on High-Pass and Low-Pass datasets and achieve remarkable filtering capability, while vanilla GCN and GIN cannot converge to considerable results by increasing the layer size. This conclusion is consistent with the previous results on Band-Pass dataset presented in the main text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. Additional Experiments on OGB Benchmark</head><p>In the main text, we have demonstrated our results on ogbprotein dataset <ref type="bibr" target="#b17">(Hu et al., 2020)</ref> for node-level tasks. In this subsection, we also show our SoGCN's effectiveness on ogbmolhiv dataset <ref type="bibr" target="#b17">(Hu et al., 2020)</ref> for graph-level tasks. As the same with experiments on ogb-protein, we evaluate different GCN models in terms of their total parameter numbers, training time per epoch, and test ROC-AUC.</p><p>Experiment Setup Again, we choose vanilla GCN, GIN, GraphSage <ref type="bibr" target="#b14">(Hamilton et al., 2017)</ref>, APPNP <ref type="bibr" target="#b21">(Klicpera et al., 2019)</ref>, <ref type="bibr">GCNII (Ming Chen et al., 2020)</ref>, ARMA <ref type="bibr" target="#b1">(Bianchi et al., 2019)</ref>, our SoGCN, and two high-order GCNs for performance comparison. We adopt the example code of vanilla GCN and GIN provided in OGB. We reimplemented GCNII, GraphSage, APPNP, ARMA based on the official code in PyTorch Geometric <ref type="bibr">(Fey &amp; Lenssen, 2019)</ref>. According to the benchmark's guideline, we add edge features to fan-out node features while propagation. Every model has the same depth and width, as well as other modules. The timing, training and evaluation procedures conform with the descriptions in our main text. We train vanilla GCN, GIN, APPNP, GraphSage for ˜100 epochs, and SoGCN, higher-order GCNs, GCNII, ARMA for ˜500 epochs.</p><p>Results and Discussion. Table <ref type="table" target="#tab_6">6</ref> demonstrates the ROC-AUC score for each model on ogb-molhiv dataset. We reach the same conclusion with our main text. On ogb-molhiv dataset, we notice that GCNII is another lightweight yet effective model. However, GCNII only allows inputs whose channel number equals to output dimension. One needs to add additional blocks (e.g., linear modules) to support varying hidden dimensions, which incorporates more parameters and higher complexity (e.g., on ogb-protein dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Implementation Details</head><p>We open source our implementation of SoGCN at https://github.com/yuehaowang/SoGCN. All of our code, datasets, hyper-parameters, and runtime configurations can be found there.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.1. Second-Order Graph Convolution</head><p>Our SoGC can be implemented using a message-passing scheme <ref type="bibr" target="#b14">(Hamilton et al., 2017)</ref> (cf. Equation <ref type="formula" target="#formula_44">19</ref>). We regard the normalized adjacency matrix A(G) as a one-hop aggregator (message propagator). When we compute the power of A(G), we invoke the propagator multiple times.</p><p>After passing the messages twice, we transform and mix up aggregated information from two hops via a linear block.</p><formula xml:id="formula_44">h (1) v = u∈N (v) 1 √ d v d u x u , h (2) v = u∈N (v) 1 √ d v d u h (1) u , y v = Θ 2 h (2) v + Θ 1 h (1) v + Θ 0 x v ,<label>(19)</label></formula><p>where x v ∈ R E is the input feature vector for node v ∈ V(G), y v ∈ R F denotes the output for node v. d v is the degree for vertex v, N (v) is the set of v's neighbor vertices. h (1) v is the feature representation of v's first-hop neighborhood. It can be computed by aggregating information once from the directly neighboring nodes. h (2)  v is the feature representation of v's second-hop neighborhood. It can be computed by feature aggregation upon neighbors' h (1)  u , u ∈ N (v). Θ i ∈ R F ×E , i = 0, 1, 2 are the weight matrices (a.k.a. layer parameters).</p><p>Our design can reduce computational time by reusing previously aggregated information and preventing power operations on A(G). In practice, our SoGC is easy to implement. Our message-passing design conforms to mainstream graph learning frameworks, such as Deep Graph Library <ref type="bibr" target="#b40">(Wang et al., 2019)</ref> and PyTorch Geometric <ref type="bibr">(Fey &amp; Lenssen, 2019)</ref>. One can simply add another group of parameters and invoke the "propagation" method of vanilla GC <ref type="bibr" target="#b20">(Kipf &amp; Welling, 2017)</ref> twice to simulate our SoGC. For the sake of clarity, we provide the pseudo-code for general K-order GCs in Algorithm 1. Our SoGC can be called by passing K = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 K-Order Graph Convolution</head><p>Input: Graph G = (V, E), node degrees {d v ∈ N, ∀v ∈ V}, input features {x v ∈ R E , ∀v ∈ V}, GC order K, weight matrices Θ i ∈ R F ×E , i = 0, • • • K.</p><p>Output: Feature representation y v ∈ R F , ∀v ∈ V.</p><formula xml:id="formula_45">h (0) v ← x v , ∀v ∈ V y v ← Θ 0 h (0) v , ∀v ∈ V for t = 1 to K do for v ∈ V do h (t) v ← u∈N (v) h (t−1) u √ d v d u end for y v ← y v + Θ t h (t)</formula><p>v , ∀v ∈ V end for Return {y v ∈ R F , ∀v ∈ V}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2. Gated Recurrent Unit</head><p>We supplement two motivations behind using Gated Recurrent Unit (GRU) <ref type="bibr" target="#b5">(Cho et al., 2014)</ref>: 1) GRU has been served as a basic building block in message-passing GNN architectures <ref type="bibr" target="#b25">(Li et al., 2016;</ref><ref type="bibr" target="#b12">Gilmer et al., 2017)</ref>. We make an explorative attempt to first introduce them into spectral GCNs. 2) By selectively maintaining information from previous layer and canceling the dominance of DC components (Figure <ref type="figure" target="#fig_4">6</ref>), GRU can also relieve the side-effect of ReLU, which is proved to be a special low-pass filter <ref type="bibr" target="#b33">(Oono &amp; Suzuki, 2019;</ref><ref type="bibr" target="#b4">Cai &amp; Wang, 2020)</ref>. Similar to <ref type="bibr" target="#b25">Li et al. (2016)</ref>; <ref type="bibr" target="#b12">Gilmer et al. (2017)</ref>, we appends a shared GRU module after each GC layer, which takes the signals before the GC layers as the hidden state, after the GC layers as the current input. We formulate its implementation by replacing Equation 10 with Equation 20 as below.</p><formula xml:id="formula_46">X (l) conv = f (l) 2</formula><p>X (l−1) ; Θ (l) l) ; Ω ,</p><formula xml:id="formula_47">X (l+1) = GRU ReLU X (l+1) conv , X<label>(</label></formula><p>where X (l+1) conv is the input, X (l) represents the hidden state, Ω denotes parameters of the GRU. Figure <ref type="figure" target="#fig_4">6</ref> illustrates the spectrum outputs of vanilla GCN + GRU and SoGCN + GRU. One can see, without filtering power of SoGCN, vanilla GCN + GRU fails to extract sharp patterns on the spectrum. Thereby, we suggest that it is SoGC that mainly contributes to the higher expressiveness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Vertex domain interpretations of vanilla GC, SoGC, and Multi-Hop GC. Denote A the first-hop aggregator, A 2 the second-hop aggregator, and A K the K-th hop aggregator. Nodes in the same colored ring share the same weights. (a) Vanilla GC only aggregates information from the first-hop neighbor nodes. (b) SoGC incorporates additional information from the second-hop (almost-connected) neighborhood. (c) Multi-hop GC simply repeats mixing information from every neighborhood within K hops.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure2. Visualizing output activation in graph spectrum domain for vanilla GCN, SoGCN, and GRU variants. The test is conducted on a graph from the ZINC dataset. The spectrum is defined as a projection of activation functions on the graph eigenvectors. SoGCN preserved higher-order spectrum, while vanilla GCN shows over-smoothing. See Appendix F for more visualizations on the ZINC dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Relations between test MAE and layer size. Experiments are conducted on synthetic Band-Pass dataset. Each model has 16 channels per hidden layer with varying layer size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. An example of graph spectrum in our SGS dataset and its corresponding high-pass, low-pass and band-pass filtered output using our hand-crafted filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FrequencyFigure 6 .</head><label>6</label><figDesc>Figure 6. More visualizations of output spectrum on the ZINC dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Relations between test MAE and layer size. Each model has 16 channels per hidden layer with varying layer size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of different GC kernels in terms of expressivness, localization, and time complexity. In this table, m represents the number of neighborhoods around a graph node, s is the dimension of input features, and K denotes the aggregation length of multi-hop GCs. We compute the time complexity with respect to the method of<ref type="bibr" target="#b0">Abu-El-Haija et al. (2019)</ref>.</figDesc><table><row><cell>Kernel</cell><cell cols="3">Expressiveness Localized Complexity</cell></row><row><cell>Vanilla GCN</cell><cell>Very Low</cell><cell></cell><cell>O(ms)</cell></row><row><cell>GIN</cell><cell>Medium</cell><cell></cell><cell>O(ms)</cell></row><row><cell>Multi-hop</cell><cell>Full</cell><cell>×</cell><cell>O(Kms)</cell></row><row><cell>SoGCN (Ours)</cell><cell>Full</cell><cell></cell><cell>O(ms)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>The performance of graph node signal regression with High-Pass, Low-Pass, and Band-Pass filters (over graph spectral space) as learning target. Each model has 16 GC layers and 16 channels of hidden layers.</figDesc><table><row><cell>Model</cell><cell>#Param</cell><cell cols="3">Test MAE High-Pass Low-Pass Band-Pass</cell></row><row><cell>Vanilla</cell><cell>4611</cell><cell>0.308</cell><cell>0.317</cell><cell>0.559</cell></row><row><cell>GIN</cell><cell>4627</cell><cell>0.344</cell><cell>0.096</cell><cell>0.274</cell></row><row><cell>SoGCN</cell><cell>12323</cell><cell>0.021</cell><cell>0.023</cell><cell>0.050</cell></row><row><cell>3rd-Order</cell><cell>16179</cell><cell>0.021</cell><cell>0.022</cell><cell>0.045</cell></row><row><cell>4th-Order</cell><cell>20035</cell><cell>0.021</cell><cell>0.022</cell><cell>0.049</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Results and comparison with other GNN models on ZINC, CIFAR10, MNIST, CLUSTER and PATTERN datasets. For ZINC dataset, the parameter budget is set to 500k. For CIFAR10, MNIST, CLUSTER and PATTERN datasets, the parameter budget is set to 100k. Red: the best model, Green: good models.</figDesc><table><row><cell>Model</cell><cell>Test MAE ± s.d. ZINC</cell><cell>MNIST</cell><cell>Test ACC ± s.d. (%) CIFAR10 CLUSTER</cell><cell>PATTERN</cell></row><row><cell>Vanilla GCN</cell><cell>0.367±0.011</cell><cell cols="3">90.705±0.218 55.710±0.381 53.445±2.029 63.880±0.074</cell></row><row><cell>Vanilla GCN + GRU</cell><cell>0.295±0.005</cell><cell cols="3">96.020±0.090 61.332±0.849 57.932±0.168 70.194±0.216</cell></row><row><cell>GAT</cell><cell>0.384±0.007</cell><cell cols="3">95.535±0.205 64.223±0.455 57.732±0.323 75.824±1.823</cell></row><row><cell>MoNet</cell><cell>0.292±0.006</cell><cell cols="3">90.805±0.032 65.911±2.515 58.064±0.131 85.482±0.037</cell></row><row><cell>GraphSage</cell><cell>0.398±0.002</cell><cell cols="3">97.312±0.097 65.767±0.308 50.454±0.145 50.516±0.001</cell></row><row><cell>GIN</cell><cell>0.387±0.015</cell><cell cols="3">96.485±0.252 55.255±1.527 58.384±0.236 85.590±0.011</cell></row><row><cell>GatedGCN</cell><cell>0.350±0.020</cell><cell cols="3">97.340±0.143 67.312±0.311 60.404±0.419 84.480±0.122</cell></row><row><cell>3WLGNN</cell><cell>0.407±0.028 3</cell><cell cols="3">95.075±0.961 59.175±1.593 57.130±6.539 85.661±0.353</cell></row><row><cell>SoGCN</cell><cell>0.238±0.017</cell><cell cols="3">96.785±0.113 66.338±0.155 68.167±1.164 85.735±0.037</cell></row><row><cell>SoGCN-GRU</cell><cell>0.201±0.006</cell><cell cols="3">97.729±0.159 68.208±0.271 67.994±2.619 85.711±0.047</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>The</figDesc><table><row><cell>Model</cell><cell>#Param</cell><cell cols="2">ogb-protein Time / Ep. ROC-AUC ± s.d.</cell></row><row><cell>Vanilla GCN</cell><cell>96880</cell><cell>3.47 ± 0.40</cell><cell>72.16 ± 0.55</cell></row><row><cell>GIN</cell><cell cols="2">128512 4.33 ± 0.27</cell><cell>76.77 ± 0.20</cell></row><row><cell>GCNII</cell><cell cols="2">227696 4.96 ± 0.29</cell><cell>74.79 ± 1.17</cell></row><row><cell>GCNII*</cell><cell cols="2">424304 5.09 ± 0.17</cell><cell>72.50 ± 2.49</cell></row><row><cell>APPNP</cell><cell>96880</cell><cell>6.56 ± 0.37</cell><cell>65.37 ± 1.15</cell></row><row><cell>GraphSage</cell><cell cols="2">193136 6.51 ± 0.13</cell><cell>77.53 ± 0.30</cell></row><row><cell>SoGCN</cell><cell cols="2">192512 4.88 ± 0.36</cell><cell>79.28 ± 0.47</cell></row><row><cell cols="3">4th-Order GCN 320512 8.89 ± 0.82</cell><cell>78.95 ± 0.57</cell></row><row><cell cols="3">6th-Order GCN 448512 9.76 ± 0.64</cell><cell>78.61 ± 0.42</cell></row><row><cell cols="4">3 This is the result of 3WLGNN with 100k parameters. The</cell></row><row><cell cols="4">test MAE of 3WLGNN with 500k parameters is increased to</cell></row><row><cell>0.427±0.011.</cell><cell></cell><cell></cell><cell></cell></row></table><note>performance of node-level multi-label classification on ogb-protein dataset. We compare each model condering the following dimensions: the number of parameters, training time (in seconds) per epoch (ep.), and final test ROC-AUC (%).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Results of ablation study on ZINC, MNIST and CIFAR10 datasets. Vanilla GCN is the comparison baseline and the number in the (↑ •) and (↓ •) represents the performance gain compared with the baseline.</figDesc><table><row><cell>Model</cell><cell>Test MAE ± s.d. ZINC</cell><cell>MNIST</cell><cell cols="2">Test ACC ± s.d. (%) CIFAR10</cell></row><row><cell>Vanilla GCN</cell><cell cols="4">0.367 ± 0.011 (Baseline) 90.705 ± 0.218 (Baseline) 55.710 ± 0.381 (Baseline)</cell></row><row><cell>SoGCN</cell><cell>0.238 ± 0.017 (↓ 0.129)</cell><cell cols="2">96.785 ± 0.113 (↑ 6.080)</cell><cell>66.338 ± 0.155 (↑ 10.628)</cell></row><row><cell>4th-Order GCN</cell><cell>0.243 ± 0.009 (↓ 0.124)</cell><cell cols="2">96.167 ± 0.198 (↑ 5.462)</cell><cell>64.230 ± 0.212 (↑ 8.520)</cell></row><row><cell>6th-Order GCN</cell><cell>0.261 ± 0.014 (↓ 0.106)</cell><cell cols="2">96.292 ± 0.134 (↑ 5.587)</cell><cell>63.687 ± 0.151 (↑ 7.977)</cell></row><row><cell>Vanilla GCN + GRU</cell><cell cols="4">0.295 ± 0.005 (↓ 0.072) 96.020 ± 0.090 (↑ 5.315) 61.332 ± 0.381 (↑ 5.622)</cell></row><row><cell>SoGCN + GRU</cell><cell>0.201 ± 0.006 (↓ 0.166)</cell><cell cols="2">97.729 ± 0.159 (↑ 7.024)</cell><cell>68.208 ± 0.271 (↑ 12.498)</cell></row><row><cell cols="5">4th-Order GCN + GRU 0.204 ± 0.004 (↓ 0.163) 97.304 ± 0.296 (↑ 6.599) 64.697 ± 0.341 (↑ 8.987)</cell></row><row><cell cols="5">6th-Order GCN + GRU 0.218 ± 0.005 (↓ 0.149) 97.325 ± 0.218 (↑ 6.620) 64.523 ± 0.251 (↑ 8.813)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>The performance of graph-level multi-label classification on ogb-molhiv dataset. We compare each model condering the following dimensions: the number of parameters, training time (in seconds) per epoch (ep.), and final test ROC-AUC (%).</figDesc><table><row><cell>Model</cell><cell>#Param</cell><cell>ogb-molhiv Time / Ep.</cell><cell>ROC-AUC ± s.d.</cell></row><row><cell>Vanilla GCN</cell><cell>527,701</cell><cell>25.57 ± 1.37</cell><cell>76.06 ± 0.97</cell></row><row><cell>GIN</cell><cell>980,706</cell><cell>29.01 ± 1.24</cell><cell>75.58 ± 1.40</cell></row><row><cell>GCNII</cell><cell>524,701</cell><cell>24.19 ± 1.26</cell><cell>77.04 ± 1.03</cell></row><row><cell>APPNP</cell><cell>327,001</cell><cell>13.56 ± 1.32</cell><cell>68.00 ± 1.36</cell></row><row><cell>GraphSage</cell><cell>976,201</cell><cell>24.43 ± 1.39</cell><cell>76.90 ± 1.36</cell></row><row><cell>ARMA</cell><cell cols="2">8,188,201 43.14 ± 0.99</cell><cell>76.91 ± 1.75</cell></row><row><cell>SoGCN</cell><cell cols="2">1,426,201 27.02 ± 1.28</cell><cell>77.26 ± 0.85</cell></row><row><cell cols="3">4th-Order GCN 2,326,201 32.24 ± 1.10</cell><cell>77.24 ± 1.21</cell></row><row><cell cols="3">6th-Order GCN 3,226,201 37.64 ± 1.15</cell><cell>77.10 ± 0.72</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We can replace the Laplacian matrix L in Defferrard et al. (2016) with the normalized adjacency matrix A since L = I − A.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">See Appendix A for a remark on how FK relates with the term "Linear Shift-Invariant".</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName><surname>Mixhop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Graph neural networks with convolutional arma filters</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Livi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alippi</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07553</idno>
		<title level="m">Residual gated graph convnets</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A note on over-smoothing for graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<title level="m">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Spectral graph theory</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>American Mathematical Soc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Understanding the representation power of graph neural networks in learning graph topology</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dehmamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabási</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00982</idno>
		<title level="m">Benchmarking graph neural networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast geometric deep learning with continuous b-spline kernels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><surname>Splinecnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Translation on graphs: An isometric shift operator</title>
		<author>
			<persName><forename type="first">B</forename><surname>Girault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gonc ¸alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Fleury</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>SPL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>ACHA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maehara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09550</idno>
		<title level="m">Revisiting graph neural networks: All we have is low-pass filters</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The mean number of real zeros of random polynomials. i. coefficients with zero mean</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Ibragimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Maslova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Probability &amp; Its Applications</title>
				<imprint>
			<date type="published" when="1971">1971</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="228" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep learning. Nature</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Probability of all real zeros for random polynomial with the exponential ensemble</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Lanczosnet: Multi-scale deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Break the ceiling: Stronger multi-scale deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Invariant and equivariant graph networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Provably powerful graph networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Zengfeng Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.-C</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dropedge: Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Discrete signal processing on graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep graph library: A graphcentric, highly-performant package for graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H D</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-I</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">How powerful are graph neural networks? In ICLR</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Universality of deep convolutional neural networks. ACHA</title>
		<author>
			<persName><forename type="first">D.-X</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
