<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bootstrapping User and Item Representations for One-Class Collaborative Filtering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-13">13 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dongha</forename><surname>Lee</surname></persName>
							<email>dongha.lee@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Seongku</forename><surname>Kang</surname></persName>
							<email>seongku@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hyunjun</forename><surname>Ju</surname></persName>
							<email>hyunjunju@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chanyoung</forename><surname>Park</surname></persName>
							<email>cy.park@kaist.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Korea Advanced Institute of Science and Technology (KAIST)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
							<email>hwanjoyu@postech.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Pohang University of Science and Technology (POSTECH)</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bootstrapping User and Item Representations for One-Class Collaborative Filtering</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-13">13 May 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3404835.3462935</idno>
					<idno type="arXiv">arXiv:2105.06323v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems → Collaborative filtering</term>
					<term>• Computing methodologies → Learning from implicit feedback</term>
					<term>Unsupervised learning One-class collaborative filtering, Bootstrapping-based representation learning, Self-supervised learning, Recommender systems</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of one-class collaborative filtering (OCCF) is to identify the user-item pairs that are positively-related but have not been interacted yet, where only a small portion of positive user-item interactions (e.g., users' implicit feedback) are observed. For discriminative modeling between positive and negative interactions, most previous work relied on negative sampling to some extent, which refers to considering unobserved user-item pairs as negative, as actual negative ones are unknown. However, the negative sampling scheme has critical limitations because it may choose "positive but unobserved" pairs as negative. This paper proposes a novel OCCF framework, named as BUIR, which does not require negative sampling. To make the representations of positively-related users and items similar to each other while avoiding a collapsed solution, BUIR adopts two distinct encoder networks that learn from each other; the first encoder is trained to predict the output of the second encoder as its target, while the second encoder provides the consistent targets by slowly approximating the first encoder. In addition, BUIR effectively alleviates the data sparsity issue of OCCF, by applying stochastic data augmentation to encoder inputs. Based on the neighborhood information of users and items, BUIR randomly generates the augmented views of each positive interaction each time it encodes, then further trains the model by this self-supervision. Our extensive experiments demonstrate that BUIR consistently and significantly outperforms all baseline methods by a large margin especially for much sparse datasets in which any assumptions about negative interactions are less valid.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Over the past decade, one-class collaborative filtering (OCCF) problems <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b24">24]</ref> have been extensively researched to accurately infer a user's preferred items, particularly for the recommender systems where only the users' implicit feedback on items are observed (e.g., click, purchase, or browsing history). This problem has remained challenging due to an extreme sparseness of such implicit feedback (i.e., most users have interacted with only a few items among numerous items), and also the non-existence of the negative labels for user-item interactions (i.e., observed feedback is expressions of positive interactions). Precisely, the goal of OCCF is to identify the most likely positive user-item interactions among a huge amount of unobserved interactions, by using only a small number of observed (positively-labeled) interactions.</p><p>The most dominant approach to the OCCF problem is discriminative modeling <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b32">32]</ref>, which explicitly aims to distinguish positive user-item interactions from the negative counterparts. They define the interaction score indicating how likely each user interacts with each item, based on the similarity (e.g., inner product) between the representation of a user and an item. From matrix factorization <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b27">27]</ref> to deep neural networks <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b32">32]</ref>, a variety of techniques have been studied to effectively model this score. Then, they optimize the scores by using the pointwise prediction loss <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b13">13]</ref> or the pairwise ranking loss <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b27">27]</ref> to discriminate between positive and negative interactions.</p><p>However, since the negative interactions are not available in the OCCF problem, previous discriminative methods assume that all unobserved interactions are negative. In other words, for each user, the items that have not been interacted yet are regarded to be less preferred to positive items. In this sense, they either use all unobserved user-item interactions as negative or adopt a negative sampling, which randomly samples unobserved user-item interactions in a stochastic manner to alleviate the computational burden. For better recommendation performance and faster convergence, advanced negative sampling strategies <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b26">26]</ref> are also proposed to sample from non-uniform distributions.</p><p>Nevertheless, the negative sampling approach has critical limitations in the following aspects. First, the underlying assumption about negative interactions becomes less valid as user-item interactions get sparser. This is because as fewer positive interactions are observed, the number of "positive but unobserved" interactions increases, which consequently makes it even harder to sample correct negative ones. Such uncertainty of supervision eventually degrades the performance for top-𝐾 recommendation. Second, the convergence speed and the final performance depend on the specific choice of distributions for negative sampling. For example, sampling negative pairs from a non-uniform distribution <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b26">26]</ref> (e.g., the multinomial distribution which models the probability of each interaction being actually negative) can improve the final performance, but inevitably incurs high computational costs, especially when a lot of users and items should be considered.</p><p>As a solution to the aforementioned limitations, this paper proposes a novel OCCF framework, named as BUIR, which does not require the negative sampling at all for training the model. The main idea is, given a positive user-item interaction (𝑢, 𝑣), to make representations for 𝑢 and 𝑣 similar to each other, in order to encode the preference information into the representations. However, a naive end-to-end learning framework that guides positive user-item pairs to be similar to each other without any negative supervision can easily converge to a collapsed solution -the encoder network outputs the same representations for all the users and items.</p><p>We argue that the above collapsed solution is incurred by the simultaneous optimization of 𝑢 and 𝑣 within the end-to-end learning framework of a single encoder. Hence, we instead adopt the studentteacher-like network <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b29">29]</ref> in which only the student's output 𝑢 (and 𝑣) is optimized to predict the target 𝑣 (and 𝑢) presented by the teacher. Specifically, BUIR directly bootstraps<ref type="foot" target="#foot_0">1</ref> the representations of users and items by employing two distinct encoder networks, referred to as online encoder and target encoder. The high-level idea is training only the online encoder for the prediction task between 𝑢 and 𝑣, where the target for its prediction is provided by the target encoder. That is, the online encoder is optimized so that its user (and item) vectors get closer to the item (and user) vectors computed by the target encoder. At the same time, the target encoder is updated based on momentum-based moving average <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b29">29]</ref> to slowly approximate the online encoder, which encourages to provide enhanced representations as the target for the online encoder. By doing so, the online encoder can capture the positive relationship between 𝑢 and 𝑣 into the representations, while preventing the model from collapsing to the trivial solution without explicitly using any negative interactions for the optimization.</p><p>Furthermore, we introduce a stochastic data augmentation technique to relieve the data sparsity problem in our framework. Motivated by the recent success of self-supervised learning in various domains <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b4">4]</ref>, we exploit augmented views of an input interaction, which are generated based on the neighborhood information of each user and item (i.e., the set of the items interacted with a user, and the users interacted with an item). The stochastic augmentation is applied to positive user-item pairs when they are passed to the encoder, so as to produce the different views of the pairs. To be precise, by making our encoder use a random subset of a user's (and item's) neighbors for the input features, it produces a similar effect to increasing the number of positive pairs from the data itself without any human intervention. In the end, BUIR is allowed to learn various views of each positive user-item pair.</p><p>Our extensive evaluation on real-world implicit feedback datasets shows that BUIR consistently performs the best for top-𝐾 recommendation among a wide range of OCCF methods. In particular, the performance improvement becomes more significant in sparser datasets, with the help of utilizing augmented views of positive interactions as well as eliminating the effect of uncertain negative interactions. In addition, comparison results on a downstream task, which classifies the items into their category, support that BUIR learns more effective representations than other OCCF baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 One-Class Collaborative Filtering</head><p>One-class collaborative filtering (OCCF) was firstly introduced to handle the real-world recommendation scenario where only positive user-item interaction can be labeled <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b24">24]</ref> as a form of users' implicit feedback on items. That is, only the set of positive user-item pairs, denoted by R, is given for training the model. The main challenge of OCCF is to find out the most likely positive interactions among a large number of unobserved user-item pairs in which both positive and negative interactions are mixed together. To handle the absence of negatively-labeled interactions, most existing methods have either treated all unobserved user-item pairs as negative, or sampled some of them <ref type="bibr" target="#b11">[11]</ref>, assuming that the items that have not been interacted yet are less preferred to positive items.</p><p>To be specific, discriminative methods <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33]</ref> train their model so that it can differentiate the scores between positive and negative interactions. Pairwise learning, which is the most popular approach to personalized ranking, explicitly utilizes the pairs of positive and negative interactions for training. Formally, the pairwise ranking loss optimizes the similarity for a positive interaction to become larger than that for a negative one as follows.</p><formula xml:id="formula_0">L = − ∑︁ (𝑢,𝑣 𝑝 ,𝑣 𝑛 ) ∈ O 𝜙 (𝑠𝑖𝑚(𝑢, 𝑣 𝑝 ) &gt; 𝑠𝑖𝑚(𝑢, 𝑣 𝑛 )),<label>(1)</label></formula><p>where O = {(𝑢, 𝑣 𝑝 , 𝑣 𝑛 )|(𝑢, 𝑣 𝑝 ) ∈ R, (𝑢, 𝑣 𝑛 ) ∉ R}, and 𝜙 is a scoring function to facilitate the optimization. For example, Bayesian personalized ranking <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b32">32]</ref> defines the similarity of a user and an item by the inner product of their representations, and collaborative metric learning <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b25">25]</ref> directly learns the latent space by modeling their similarity as the Euclidean distance. However, all these methods obtain the negative interactions from unobserved user-item pairs, thus the convergence speed and final performance largely depend on the negative sampling distribution <ref type="bibr" target="#b26">[26]</ref>.</p><p>On the other hand, generative methods <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b31">31]</ref> aim to learn the underlying latent distribution of users, usually represented by binary vectors indicating their interacted items. They employ the architecture of variational autoencoder (VAE) <ref type="bibr" target="#b18">[18]</ref> or generative adversarial networks (GAN) <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b31">31]</ref>, in order to infer the users' preference on each item based on the reconstructed (or generated) user vectors. Rather than exploiting the negative sampling, most of the generative methods implicitly assume that all unobserved useritem pairs are negative in that they learn the partially-observed binary vectors as their inputs. We remark that this assumption is not strictly valid, which eventually leads to limited performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-supervised Contrastive Learning</head><p>Recently, a self-supervised learning approach has achieved a great success in computer vision and natural language understanding <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b8">8]</ref>. Most of them basically adopt contrastive learning, which optimizes the representations of positively-related (similar) instances to be close, while those of negatively-related (dissimilar) ones far from each other. Given an unlabeled dataset D = {𝑥 1 , . . . , 𝑥 𝑁 }, positive pairs for each instance (𝑥, 𝑥 𝑝 ) is usually obtained from the data itself (i.e., data augmentation), such as geometric transformations on a target image. Note that it does not require any human annotations or additional labels, thus this approach falls into the category of self-supervised learning. The noise contrastive estimator (NCE) loss <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b23">23]</ref> mainly used for contrastive learning is defined by using all the other instances except for 𝑥 as negative:</p><formula xml:id="formula_1">L = − ∑︁ 𝑥 ∈ D log exp(𝑠𝑖𝑚(𝑥, 𝑥 𝑝 )) exp(𝑠𝑖𝑚(𝑥, 𝑥 𝑝 )) + 𝑥 𝑛 ∈ D\{𝑥 } exp(𝑠𝑖𝑚(𝑥, 𝑥 𝑛 ))</formula><p>.</p><p>(2) In case of large-scale datasets, the predefined number of negative instances can be selected (i.e., negative sampling). For contrastive learning, negative pairs must be considered for its optimization so as to prevent the representations of all instances from being similar, which is known as the problem of collapsed solutions.</p><p>Pointing out that the contrastive methods need to carefully treat the negative instances during the training for effectiveness and efficiency, the most recent work proposed a bootstrapping-based self-supervised learning framework <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b6">6]</ref>, which is capable of avoiding the collapsed solution without the help of negative instances. Inspired by bootstrapping methods in deep reinforcement learning <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22]</ref>, it directly bootstraps the representation of images by using two neural networks that iteratively learn from each other. This approach achieves the state-of-the-art performance for various downstream tasks in computer vision, and also shows better robustness to the choice of data augmentations used for self-supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BUIR: PROPOSED FRAMEWORK</head><p>In this section, we present our OCCF framework, named as BUIR, which learns the representations of users and items without any assumptions about negative interactions. We first describe the overall learning process with a simple encoder that takes the user-id and item-id as its input (Section 3.2) and how to infer the interaction score using the representations (Section 3.3). We also introduce a stochastic data augmentation technique with an extended encoder to further exploit the neighborhood information (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Let U = {𝑢 1 , . . . , 𝑢 𝑀 } and V = {𝑣 1 , . . . , 𝑣 𝑁 } be the set of 𝑀 users and 𝑁 items, respectively. Given a set of observed user-item interactions R = {(𝑢, 𝑣)|user 𝑢 is interacted with item 𝑣 }, the goal of OCCF is to obtain the interaction (or preference) score 𝑠 (𝑢, 𝑣) ∈ R indicating how likely the user 𝑢 interacts with (or prefers to) the item 𝑣. Based on the interaction scores, we can recommend 𝐾 items with the highest scores for each user, called as top-𝐾 recommendation. To define the interaction score by using the representations of users and items, we focus on training the encoder network that maps each user and item into a low-dimensional latent space where the users' preferences on the items are effectively captured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bootstrapping the Representations</head><p>Let 𝑓 be the encoder network to produce the representations of users and items. The simplest architecture of the encoder is a single embedding layer (i.e., embedding matrix); this maps each user-id (or item-id) into a 𝐷-dimensional embedding vector that represents the latent factors of the user (or item). Specifically, each encoder consists of a user encoder and an item encoder, and they take a one-hot vector indicating the user-id and item-id as their input.</p><p>BUIR makes use of two distinct encoder networks that have the same structure: online encoder 𝑓 𝜃 and target encoder 𝑓 𝜉 . They are parameterized by 𝜃 and 𝜉, respectively. The key idea of BUIR is to train the online encoder by using outputs of the target encoder as its target, while gradually improving the target encoder as well. The main difference of BUIR from existing end-to-end learning frameworks is that 𝑓 𝜃 and 𝑓 𝜉 are updated in different ways. The online encoder is trained to minimize the error between its output and the target, whereas the target network is slowly updated based on the momentum update <ref type="bibr" target="#b8">[8]</ref> so as to keep its output consistent.</p><p>To be precise, for each observed interaction (𝑢, 𝑣) ∈ R, the BUIR loss is defined based on the mean squared error of the prediction against each other (i.e., representations of 𝑢 and 𝑣) using the predictor 𝑞 𝜃 : R 𝐷 → R 𝐷 on top of the online encoder. It includes two error terms: one is for updating the online user vector 𝑓 𝜃 (𝑢) to accurately predict the target item vector 𝑓 𝜉 (𝑣), and the other is for updating the online item vector 𝑓 𝜃 (𝑣) to make its prediction as the target user vector 𝑓 𝜉 (𝑢). Finally, the loss is described as follows:</p><formula xml:id="formula_2">L 𝜃,𝜉 (𝑢, 𝑣) = 𝑙 2 𝑞 𝜃 (𝑓 𝜃 (𝑢)) , 𝑓 𝜉 (𝑣) + 𝑙 2 𝑞 𝜃 (𝑓 𝜃 (𝑣)) , 𝑓 𝜉 (𝑢) ≈ − 𝑞 𝜃 (𝑓 𝜃 (𝑢)) ⊤ 𝑓 𝜉 (𝑣) ∥𝑞 𝜃 (𝑓 𝜃 (𝑢))∥ 2 ∥𝑓 𝜉 (𝑣)∥ 2 − 𝑞 𝜃 (𝑓 𝜃 (𝑣)) ⊤ 𝑓 𝜉 (𝑢) ∥𝑞 𝜃 (𝑓 𝜃 (𝑣))∥ 2 ∥𝑓 𝜉 (𝑢) ∥ 2 ,<label>(3)</label></formula><p>where 𝑙 2 [x, y] is the 𝑙 2 distance between two normalized vectors x and y; i.e., x = x/∥x∥ 2 and y = y/∥y∥ 2 . Since the mean squared errors between two normalized vectors are equivalent to the negative value of their inner product (Equation ( <ref type="formula" target="#formula_2">3</ref>)), we simply use the inner product for the optimization. Note that BUIR updates 𝑓 𝜃 (𝑢) to be similar with 𝑓 𝜉 (𝑣) instead of 𝑓 𝜃 (𝑣) through the predictor, and vice versa. This is because directly reducing the error between 𝑓 𝜃 (𝑢) and 𝑓 𝜃 (𝑣) leads to the collapsed representations when negative interactions are not considered at all for training the encoder.</p><p>To sum up, the parameters of the online encoder and target encoder are optimized by</p><formula xml:id="formula_3">𝜃 ← 𝜃 − 𝜂 • ∇ 𝜃 L 𝜃,𝜉 𝜉 ← 𝜏 • 𝜉 + (1 − 𝜏) • 𝜃 . (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>𝜂 is the learning rate for stochastic optimization, and 𝜏 ∈ [0, 1] is a momentum coefficient (also called as target decay) for momentumbased moving average. The online encoder 𝑓 𝜃 (and the predictor 𝑞 𝜃 ) is effectively optimized by the gradients back-propagated from the loss (Equation ( <ref type="formula" target="#formula_2">3</ref>)), while the target encoder 𝑓 𝜉 is updated as the moving average of the online encoder. By taking a large value of 𝜏, the target encoder slowly approximates the online encoder. This momentum-based update makes 𝜉 evolve more slowly than 𝜃 , which enables to bootstrap the representations by providing enhanced but consistent targets to the online encoders <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8]</ref>. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the overall framework of BUIR with the simple one-hot encoders.</p><p>Bypassing the collapsed solution. It is obvious that the loss in Equation ( <ref type="formula" target="#formula_2">3</ref>) admits the collapsed solution with respect to 𝜃 and 𝜉, which means both the encoders generate the same representations for all users and items. For this reason, the conventional end-to-end learning strategy, which optimizes both 𝑓 𝜃 and 𝑓 𝜉 to minimize the loss (i.e., cross-prediction error), may easily lead to such collapsed solution. In contrast, our proposed framework updates each of the encoders in different ways. From Equation (4), the online encoder is optimized to minimize the loss, while the target encoder is updated to slowly approximate the online encoder. That is, the direction of updating the target encoder (𝜃 − 𝜉) totally differs from that of updating the online encoder (−∇ 𝜃 L 𝜃,𝜉 ), and this effectively keeps both the encoders from converging to the collapsed solution. Several recent work on bootstrapping-based representation learning <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b6">6]</ref> empirically demonstrated that this kind of dynamics (i.e., updating two networks differently) allows to avoid the collapsed solution without any explicit term to prevent it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Top-K Preferred Item Prediction</head><p>To retrieve 𝐾 most preferred items for each user (i.e., user-item interactions that are most likely to happen), we define the interaction score 𝑠 (𝑢, 𝑣) by using the representations of users and items. As we minimize the prediction error between 𝑢 and 𝑣 for each positive interaction (𝑢, 𝑣), their positive relationship is encoded into the 𝑙 2 distance between their representations (Equation ( <ref type="formula" target="#formula_2">3</ref>)). In other words, a smaller value of L 𝜃,𝜉 (𝑢, 𝑣) indicates that the user-item pair (𝑢, 𝑣) is more likely to be interacted, which means the loss becomes inversely proportional to the interaction score. To consider the symmetric relationship between 𝑢 and 𝑣, the interaction score is defined based on the cross-prediction task; the prediction of 𝑣 by (</p><formula xml:id="formula_5">)<label>5</label></formula><p>For the computation of the interaction scores, we use only the representations obtained from the online encoder, with the target encoder discarded. Since the online encoder and the target encoder finally converge to equilibrium by the slow-moving average, it is possible to effectively infer the interaction score only with the online encoder. Considering the purpose of the target network, which generates the target for training the online network, it does make sense to leave the online encoder in the end.</p><p>Existing discriminative OCCF methods <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b27">27]</ref> have tried to optimize the latent space where the user-item interactions are directly encoded into their inner product (or Euclidean distance). On the contrary, BUIR additionally uses the predictor to model their interaction, which results in the capability of encoding the high-level relationship between users and items into the representations. In conclusion, with the help of the predictor, BUIR accurately computes the user-item interaction scores as well as optimizes the representation without explicitly using negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Neighbor-based Data Augmentation</head><p>The another available source for OCCF is the neighborhood information of users and items. The neighbors of user 𝑢 and item 𝑣, denoted by V 𝑢 and U 𝑣 , refer to the set of the items interacted with 𝑢, and the users interacted with 𝑣, respectively. From the perspective that user-item interactions can be considered as a bipartite graph between user nodes and item nodes, each node's neighbors (or its local graph structure) can be a good feature to encode the similarity among the nodes. To take advantage of these neighbors as input features of users and items, we use a neighbor-based encoder <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b32">32]</ref> which additionally takes a given set of users (or items) as its input. Namely, this encoder is able to learn such set-featured inputs, represented as multi-hot vectors, capturing both the co-occurrence of users (or items) and their relationship. Adding the multi-hot inputs V 𝑢 and U 𝑣 to the one-hot inputs 𝑢 and 𝑣 within our framework, the neighbor-based user/item representations, denoted by 𝑓 𝜃 (𝑢, V 𝑢 ) and 𝑓 𝜃 (𝑣, U 𝑣 ), can be effectively optimized and utilized, instead of 𝑓 𝜃 (𝑢) and 𝑓 𝜃 (𝑣). In this case, the online encoder parameters related to user 𝑢 (or item 𝑣) are shared for computing 𝑓 𝜃 (𝑢, V 𝑢 ) and 𝑓 𝜃 (𝑣, U 𝑣 ), thus they are updated by two types of supervision (i.e., optimized not only as a target but also as one of the neighbors), which brings an effect of regularization.</p><p>For acquisition and exploitation of richer supervision, we extend our framework to consider much more user-item interactions that are augmented based on their neighborhood information in a self-supervised manner. To this end, we introduce a new augmentation technique specifically designed for positive user-item interactions; it does not statically increase the number of interactions as a pre-processing step, rather be stochastically applied to each input interaction during the training. This stochastic data augmentation allows the encoder to learn slightly perturbed interactions, referred to as augmented views of an interaction. By doing so, BUIR can effectively learn the representations even in the case that only a few positive user-item interactions are available for training (i.e., highly sparse dataset). To this end, we first represent each user and item as the pair of its identity and neighbors: (𝑢, V 𝑢 ) and (𝑣, U 𝑣 ). Then, we apply the following augmentation function 𝜓 to the user and item before passing them to the neighbor encoder.</p><formula xml:id="formula_6">𝜓 (𝑢, V 𝑢 ) = (𝑢, V 𝑢 ′ ), where V 𝑢 ′ ∼ {S|S ⊆ V 𝑢 }, 𝜓 (𝑣, U 𝑣 ) = (𝑣, U 𝑣 ′ ), where U 𝑣 ′ ∼ {S|S ⊆ U 𝑣 }.<label>(6)</label></formula><p>This augmentation function chooses one of the subsets of the user's neighbors (i.e., V 𝑢 ′ ) for an input user, and works in a similar way for an input item. For each input interaction (𝑢, 𝑣), we can make a variety of interactions containing small perturbations (𝜓 (𝑢, V 𝑢 ),𝜓 (𝑣, U 𝑣 )), and they produce a similar effect to increasing the number of positive pairs from the data itself.</p><p>Similarly to Section 3.2, the online encoder is trained by minimizing L 𝜃,𝜉 (𝜓 (𝑢, V 𝑢 ),𝜓 (𝑣, U 𝑣 )), and the target encoder is slowly updated by the momentum mechanism. After the optimization is finished, the interaction score is inferred by 𝑓 𝜃 (𝑢, V 𝑢 ) and 𝑓 𝜃 (𝑣, U 𝑣 ) (Equation ( <ref type="formula" target="#formula_5">5</ref>)). Figure <ref type="figure" target="#fig_1">2</ref> shows an example of our data augmentation which injects a certain level of perturbations to the neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we describe the experimental results that support the superiority of our proposed framework. We first present comparison results with other OCCF methods for top-𝐾 recommendation (Section 4.2), then validate the effectiveness of each component through an ablation study (Section 4.3 and 4.4). We also evaluate the quality of obtained representations for a downstream task (Section 4.5) and finally provide the hyperparameter analysis (Section 4.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets. In our experiments, we use three real-world datasets: CiteULike <ref type="bibr" target="#b30">[30]</ref>, Ciao <ref type="bibr" target="#b28">[28]</ref>, and FourSquare <ref type="bibr" target="#b20">[20]</ref>. For preprocessing the datasets, we follow previous work <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b32">32]</ref> which provide the minimum count of user-item interactions for filtering longtail users/items, considering the property of each dataset (e.g., the statistics or the domain where the implicit feedback is collected). <ref type="foot" target="#foot_2">3</ref>Table <ref type="table" target="#tab_0">1</ref> summarizes the statistics of the datasets. Baselines. We compare the performance of BUIR with that of baseline OCCF methods, including both discriminative and generative methods. They are re-categorized as either 1) the methods using only the user-id/item-id or 2) the ones additionally using the neighborhood information. Most of the methods in the first category directly optimize the embedding vectors of users and items.</p><p>• BPR <ref type="bibr" target="#b27">[27]</ref>: The Bayesian personalized ranking method for OCCF. It optimizes matrix factorization (MF) based on the pairwise ranking loss. • NeuMF <ref type="bibr" target="#b11">[11]</ref>: The neural network-based method that uses the pointwise prediction loss. It combines MF and multi-layer perceptron (MLP) to model the user-item interaction.</p><p>• CML <ref type="bibr" target="#b12">[12]</ref>: A metric learning approach to the OCCF problem.</p><p>It optimizes the Euclidean distance between a user and an item based on the pairwise hinge loss.</p><p>• SML <ref type="bibr" target="#b17">[17]</ref>: The state-of-the-art OCCF method based on metric learning. For symmetrization, it considers the Euclidean distance among items as well as between a user and an item.</p><p>Next, the neighbor-based OCCF methods exploit the neighborhood information of users and items to compute the representations.</p><p>• NGCF <ref type="bibr" target="#b32">[32]</ref>: A neighbor-based method which encodes a user's (and item's) neighbors by using graph convolutional networks (GCN). It can consider multi-hop neighbors as well based on a stack of GCN layers. • LGCN <ref type="bibr" target="#b10">[10]</ref>: The state-of-the-art method that further tailors the GCN-based user (and item) encoder for the OCCF task. It simplifies the GCN by using the light graph convolution. • M-VAE <ref type="bibr" target="#b18">[18]</ref>: The OCCF method based on a variational autoencoder that reconstructs partially-observed user vectors.</p><p>It enforces the latent distribution to approximate the prior, assumed to be the normal distribution.</p><p>• CFGAN <ref type="bibr" target="#b1">[1]</ref>: The state-of-the-art GAN-based OCCF method.</p><p>The discriminator is trained to distinguish between input (real) user vectors and generated (fake) ones, while the generator is optimized to deceive the discriminator.</p><p>Among them, NGCF and LGCN are the discriminative methods that optimize their model by using the pairwise loss based on the BPR framework. On the contrary, M-VAE and CFGAN are the generative methods that focus on learning the latent distribution of users, represented by binary vectors indicating their interacted items. We build two variants of BUIR using different encoder networks.</p><p>• BUIR id : The BUIR framework using a single embedding layer as its encoder. It simply takes the user/item vectors from the embedding matrix (Section 3.2).</p><p>• BUIR nb : The BUIR framework based on the LGCN encoder. It computes the user/item representations by using the lightweight GCN <ref type="bibr" target="#b10">[10]</ref> that adopts the proposed neighbor augmentation technique (Section 3.4).</p><p>Note that any types of user/item encoder networks, which are originally optimized in a discriminative framework (e.g., BPR), can be easily embedded into our framework.</p><p>Evaluation Protocols. For each dataset, we randomly split each user's interaction history into training/validation/test sets, with various split ratios. In detail, to verify the effectiveness of BUIR with varying levels of data sparsity, we build three training sets that include a certain proportion of interactions for each user, i.e., 𝛽 ∈ {10%, 20%, 50%}, <ref type="foot" target="#foot_3">4</ref> then equally divide the rest into the validation set and the test set. We report the average value of five independent runs, each of which uses different random seeds for the split.</p><p>As we focus on the top-𝐾 recommendation task for implicit feedback, we evaluate the performance of each method by using two widely-used ranking metrics <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b18">18]</ref>: Precision (P@𝐾) and Normalized Discounted Cumulative Gain (N@𝐾). <ref type="foot" target="#foot_4">5</ref> P@𝐾 measures how many test items are included in the list of top-𝐾 items and N@𝐾 assigns higher scores on the upper-ranked test items.</p><p>Implementation Details. We implement the proposed framework and all the baselines by using PyTorch, and use the Adam optimizer to train them. For BUIR, we fix the momentum coefficient 𝜏 to 0.995, and adopt a single linear layer for the predictor 𝑞 𝜃 . <ref type="foot" target="#foot_5">6</ref> The augmentation function 𝜓 simply uses a uniform distribution for drawing a drop probability 𝑝 ∼ U (0, 1), where each user's (item's) neighbor is independently deleted with the probability 𝑝.</p><p>For each dataset and baseline, we tune the hyperparameters using a grid search, which finds their optimal values that achieve the best performance on the validation set: the dimension size of representations 𝐷 ∈ {50, 100, 150, 200, 250}, the weight decay (i.e., coefficient for 𝐿 2 regularization) 𝜆 ∈ {10 −1 , 10 −2 , 10 −3 , 10 −4 , 10 −5 }, the initial learning rate 𝜂 ∈ {10 −1 , 10 −2 , 10 −3 10 −4 }, and the number of negative pairs for each positive pair (particularly for discriminative baselines) 𝑛 ∈ {1, 2, 5, 10, 20}. In case of baseline-specific hyperparameters, we tune them in the ranges suggested by their original papers. We set the maximum number of epochs to 500 and adopt the early stopping strategy; it terminates when P@10 on the validation set does not increase for 50 successive epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with OCCF Methods</head><p>We first measure the top-𝐾 recommendation performance of BUIR and the baseline methods. Table <ref type="table" target="#tab_1">2</ref> presents the comparison results on three different sparsity levels of datasets. In summary, BUIR achieves the best performance among all the baselines, and especially shows the significant improvements in highly sparse datasets. We analyze the results from various perspectives. 4.2.1 Effectiveness of BUIR id . For all the datasets, BUIR id shows substantially higher performance than the discriminative methods taking only user-id/item-id (i.e., BPR, NeuMF, CML, and SML). In particular, the sparser the training set becomes, the larger the performance improvement of BUIR id is achieved over the best baseline (denoted by Improv id ). It is obvious that BUIR id is more robust to the extreme sparsity compared to the other baselines that are more likely to explicitly use "positive but unobserved" interactions as negative interactions when positive user-item interactions are more rarely observed. BUIR id is not affected by such inconsistent supervision from uncertain negative interactions because it directly optimizes the representations of users and items by using only positive interactions.</p><p>Furthermore, in terms of the number of retrieved items (denoted by 𝐾), BUIR shows much larger performance improvements for P@10 and N@10 compared to P@50 and N@50, respectively. In other words, BUIR performs much better at predicting the topranked items than the other baselines, which makes it practically advantageous for real-world recommender systems that aim to accurately provide the most preferred items to their users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Effectiveness of BUIR nb .</head><p>We also observe that BUIR nb significantly outperforms all the other neighbor-based competitors, including discriminative (i.e., NGCF and LGCN) and generative methods (i.e., M-VAE and CFGAN). Similar to Section 4.2.1, there exist a consistent trend on its performance gain (denoted by Improv nb ), which becomes more significant as fewer interactions are given for training. Specifically, the neighbor-based baselines improve the recommendation performance over the methods not using the neighborhood information, as they are able to cope with the  high sparsity to some degree by leveraging the neighbors of users and items. Nevertheless, most of them, except for LGCN, perform worse than even BUIR id ; this strongly indicates that their imperfect assumption on negative interactions severely limits the capability of capturing users' preference on items even though they utilize rich information sources as well as employ advanced neural architectures. In short, for the OCCF problem where only a small number of positive interactions are given, our BUIR framework is effective regardless of the information sources used for training, in that any assumption on negative interactions is not required.</p><p>In addition, the critical drawback of the generative methods is the difficulty of stable optimization. For example, M-VAE should carefully treat the annealing technique for minimizing Kullback-Leibler (KL) divergence, and CFGAN needs to balance the adversarial updates between the discriminator and generator for their convergence to the equilibrium. In contrast, BUIR can easily train the encoder without any advanced techniques for stable optimization, which makes our framework much practical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Comparison of different negative sampling strategies.</head><p>To examine how much the choice of a negative sampling strategy affects the recommendation performance, we measure P@10 and N@10 of two discriminative methods (i.e., BPR and CML) that adopt different strategies. We vary the number of negative pairs (sampled for each positive pair) in the range of {2 0 , 2 1 , 2 2 , 2 3 , 2 4 }, and consider three different distributions for negative sampling <ref type="bibr" target="#b26">[26]</ref>: 1) uniform sampling, 2) static-and-global sampling which draws a pair based on the item popularity, and 3) adaptive-and-contextual sampling that uses the probability proportional to the interaction score.</p><p>In Figure <ref type="figure" target="#fig_2">3</ref>, we observe that the performance of the discriminative methods largely depends on the sampling strategy, whereas BUIR id consistently performs the best. To be specific, the sampling strategies show different tendencies or have different optimal hyperparameter values, depending on each dataset or each method. For instance, CML achieves marginal performance gains from the adaptive-and-contextual sampling compared to the uniform sampling, whereas BPR does not take any benefits from it. This is because CML optimizes its model by the hinge loss, which cannot produce the gradients to update the model parameters for too easily-distinguishable negative pairs. In this case, the adaptiveand-contextual sampling strategy can effectively select the hardnegative pairs for training, which accelerates the convergence and its final performance. We remark that this kind of sampling techniques can improve the performance of the discriminative methods to some extent, but the sampling operation requires a high computational cost itself as well as the process of hyperparameter tuning for each dataset (and method) takes huge efforts. On the contrary, as BUIR id does not rely on negative sampling, it always shows the greater performance (plotted as a solid black line) compared to any of the discriminative methods using various sampling techniques. This result clearly validates the superiority of BUIR in that it is not affected by the choice of the negative sampling strategy any longer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>To validate the effectiveness of each component in our framework, we measure the performance of the methods that ablate the following components: 1) modeling the interaction score based on the predictor (i.e., cross-prediction score defined in Equation ( <ref type="formula" target="#formula_5">5</ref>)), 2) the neighbor-based encoder that is able to capture the user's (item's) neighborhood information, and 3) the stochastic neighbor augmentation that produces various views of an input interaction. In Table <ref type="table" target="#tab_2">3</ref>, we report P@10 on the CiteULike dataset (𝛽=50%).</p><p>First of all, the BPR framework that optimizes the cross-prediction score, 𝑞 (𝑓 (𝑢)) ⊤ 𝑓 (𝑣) + 𝑓 (𝑢) ⊤ 𝑞 (𝑓 (𝑣)), is not as effective as ours; it is even worse compared to the conventional BPR, which optimizes the inner-product score 𝑓 (𝑢) ⊤ 𝑓 (𝑣). This implies that the performance improvement of BUIR is mainly caused by our learning framework rather than its score modeling based on the predictor. In addition, even without the stochastic augmentation, the neighborbased encoder (i.e., LGCN) based on the BUIR framework beats LGCN based on the BPR framework, which demonstrates that BUIR successfully addresses the issue of incorrect negative sampling. Lastly, our framework with the stochastic neighbor augmentation further improves the performance by taking benefits from various views of the positive user-item interactions for the optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of Neighbor Augmentation</head><p>For an in-depth analysis on the effect of our stochastic data augmentation function 𝜓 , we measure the performance of BUIR nb on the CiteULike and Ciao datasets (𝛽=20%), with various magnitudes of the perturbation added to the neighbors of users and items. We modify the augmentation function to randomly select the drop probability from a predefined interval, i.e., 𝑝 ∼ U (0, 𝑃) where 𝑃 is the maximum drop probability, then increase 𝑃 from 0.0 to 1.0.</p><p>In Figure <ref type="figure" target="#fig_3">4</ref>, our stochastic data augmentation (i.e., 𝑃 &gt; 0) brings a significant improvement compared to the case of using the fixed neighborhood information (i.e., 𝑃 = 0) as encoder inputs. This result shows that the augmented views of positive interactions encourage BUIR to effectively learn users' preference on items even in much sparse dataset. Interestingly, in case of the Ciao dataset which is less sparse than CiteULike, the benefit of our augmentation linearly increases with the maximum drop probability. This is because there is room for producing more various views (i.e., larger perturbation) based on a relatively more number of neighbors, and it eventually helps to boost the recommendation performance. To sum up, our framework that adopts the neighbor augmentation function successfully relieves the data sparsity issue of the OCCF problem, by leveraging the augmented views of few positive interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation on Representation Quality</head><p>To evaluate the quality of the obtained representations, we compare the performance for a downstream task by using the representations optimized by BUIR and the other baselines. <ref type="foot" target="#foot_6">7</ref> We consider an item classification task to evaluate how well each method encodes the items' characteristics or latent semantics into the representations. We choose two datasets that offer the side information on items, which are Ciao and FourSquare. Ciao provides the 28-category label of each item (i.e., the products), and FourSquare contains the GPS coordinates for each item (i.e., point-of-interest). In case of FourSquare, we first perform 𝑘-means clustering on the coordinates with 𝑘=100, and use the clustering results as the class labels. We train a linear and non-linear classifier (i.e., a single-layer perceptron and three-layer perceptron, respectively) to predict the class label of each item by using the fixed item representations as the input. Finally, we perform 10-fold cross-validation and report the average result and standard deviation.</p><p>In Figure <ref type="figure" target="#fig_4">5</ref>, BUIR id and BUIR nb achieve significantly higher classification accuracy than the others in each category. This shows that the latent space induced by BUIR more accurately captures the item's characteristics (or their relationship) compared to the space induced by the baseline methods. Another observation is that the rank of each method for the downstream tasks is consistent with that for top-𝐾 recommendation (in Table <ref type="table" target="#tab_1">2</ref>). It implies that the observed user-item interactions are positively-correlated with the latent semantic of the items, for this reason, effectively learning the users' implicit feedback eventually results in a good performance in the downstream tasks as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Sensitivity Analysis</head><p>For the guidance of hyperparameter selection, we provide analyses on the sensitivity of BUIR to its several hyperparameters. We investigate the performance changes of BUIR id on the FourSquare dataset (𝛽=50%) with respect to the dimension size 𝐷, the momentum coefficient 𝜏,<ref type="foot" target="#foot_7">8</ref> and the number of layers in the predictor.</p><p>Figure <ref type="figure" target="#fig_5">6</ref> clearly shows that the performance is hardly affected by 𝜏 in the range of [0.9, 1.0). In other words, any values of 𝜏 larger than 0.9 allow the target encoder to successfully provide the target representations to the online encoder, by slowly approximating the online encoder; on the contrary, BUIR cannot learn the effective representations at all in case that the target encoder is fixed (i.e., 𝜏 = 1). This observation is consistent with previous work on momentum-based moving average <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b29">29]</ref> that showed all values of 𝜏 between 0.9 and 0.999 can yield the best performance. Furthermore, BUIR performs the best with a single-layer predictor, because a multi-layer predictor makes it difficult to optimize the relationship between outputs of the two encoder networks. In conclusion, BUIR is more powerful even with fewer hyperparameters, compared to existing OCCF methods that include a variety of regularization terms or modeling components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This paper proposes a novel framework for learning the representations of users and items, termed as BUIR, to address the main challenges of the OCCF problem: the implicit assumption about negative interactions, and high sparsity of observed (positively-labeled) interactions. First, BUIR directly bootstraps the representations of users and items by minimizing their cross-prediction error. This makes BUIR use only partially-observed positive interactions for training the model, and accordingly, it can eliminate the need for negative sampling. In addition, BUIR is able to learn the augmented views of each positive interaction obtained from the neighborhood information, which further relieves the data sparsity issue of the OCCF problem. Through the extensive comparison with a wide range of OCCF methods, we demonstrate that BUIR consistently outperforms all the other baselines in terms of top-𝐾 recommendation. In particular, the effectiveness of BUIR becomes more significant for much sparse datasets in which the positively-labeled interactions are not enough to optimize the model as well as the assumption about negative interactions becomes less valid. Based on its great compatibility with existing user/item encoder networks, we expect that our BUIR framework can be a major solution for the OCCF problem, replacing the conventional BPR framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overall BUIR framework.</figDesc><graphic url="image-1.png" coords="3,317.96,83.68,240.25,140.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The stochastic data augmentation technique of BUIR based on the neighborhood information. 𝑢, and the prediction of 𝑢 by 𝑣. 2 𝑠 (𝑢, 𝑣) = 𝑞 𝜃 (𝑓 𝜃 (𝑢)) ⊤ 𝑓 𝜃 (𝑣) + 𝑓 𝜃 (𝑢) ⊤ 𝑞 𝜃 (𝑓 𝜃 (𝑣)) .(5)</figDesc><graphic url="image-2.png" coords="4,317.96,83.68,240.24,144.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison with discriminative methods (BPR and CML) using various negative sampling strategies.</figDesc><graphic url="image-3.png" coords="6,317.96,83.69,240.23,250.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance changes of BUIR nb with respect to the maximum drop probability for the augmentation.</figDesc><graphic url="image-4.png" coords="8,317.96,83.68,240.25,90.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Evaluation on the quality of representations, by using a linear/non-linear classifier.</figDesc><graphic url="image-5.png" coords="9,53.80,83.69,240.23,92.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Sensitivity analyses on the BUIR hyperparameters.</figDesc><graphic url="image-6.png" coords="9,317.96,83.69,240.23,66.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The statistics of the datasets.</figDesc><table><row><cell cols="2">Dataset CiteULike</cell><cell>Ciao</cell><cell>FourSquare</cell></row><row><cell>#Users</cell><cell>5,219</cell><cell>7,265</cell><cell>19,465</cell></row><row><cell>#Items</cell><cell>25,181</cell><cell>11,211</cell><cell>28,593</cell></row><row><cell>#Interactions</cell><cell cols="2">125,580 149,141</cell><cell>1,115,108</cell></row><row><cell>Density</cell><cell cols="2">0.096% 0.183%</cell><cell>0.200%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The recommendation performances of a wide range of OCCF methods, varying the sparsity of the datasets. Improv id and Improv nb respectively denote the improvement of BUIR over the best id/neighbor-based baseline. The superscripts *, **, and *** indicate 𝑝 ≤ 0.05, 𝑝 ≤ 0.005, and 𝑝 ≤ 0.0005 for the paired t-test of BUIR nb vs. the best baseline on P@10.</figDesc><table><row><cell></cell><cell>Setting</cell><cell></cell><cell></cell><cell cols="2">User/Item ID</cell><cell>User/Item ID + Neighbor</cell></row><row><cell>Data</cell><cell>𝛽</cell><cell>Metric</cell><cell>BPR</cell><cell>NeuMF CML</cell><cell>SML</cell><cell>BUIR id Improv id NGCF LGCN M-VAE CFGAN BUIR nb Improv nb</cell></row><row><cell></cell><cell>10% ***</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CiteULike</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performances of BUIR that ablates each component.</figDesc><table><row><cell cols="5">Method Framework Predictor Neighbor Augment</cell><cell>P@10</cell></row><row><cell>BPR</cell><cell>BPR BPR</cell><cell>✓</cell><cell></cell><cell></cell><cell>0.1229 ± 0.0035 0.0752 ± 0.0027</cell></row><row><cell>LGCN</cell><cell>BPR</cell><cell></cell><cell>✓</cell><cell></cell><cell>0.1561 ± 0.0038</cell></row><row><cell>BUIR id</cell><cell>BUIR BUIR</cell><cell>✓ ✓</cell><cell>✓</cell><cell></cell><cell>0.1555 ± 0.0029 0.1592 ± 0.0028</cell></row><row><cell>BUIR nb</cell><cell>BUIR</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>0.1624 ± 0.0032</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In this paper, the term "bootstrapping" is not used in the statistical meaning, but in the idiomatic meaning<ref type="bibr" target="#b6">[6]</ref>. Strictly speaking, it refers to using estimated values (i.e., the output of networks) for estimating its target values, which serve as supervision for the update. For instance, semi-supervised learning based on predicted pseudo-labels<ref type="bibr" target="#b29">[29]</ref> also can be thought as a bootstrapping method.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We empirically found that the normalized representations cannot take into account the popularity of users and items, thus simply use the output of the online encoder.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">We remove users having fewer than 5 (CiteULike, Ciao) &amp; 20 interactions (FourSquare), and remove items having fewer than 5 (Ciao) &amp; 10 interactions (FourSquare).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">This setting (high sparsity) is more difficult and practical than the traditional setting.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">As pointed out in<ref type="bibr" target="#b16">[16]</ref>, a sampled metric where only a smaller set of random items and the relevant items are ranked (e.g., leave-one-out evaluation protocol<ref type="bibr" target="#b11">[11]</ref>) cannot correctly indicate the true performance of recommender systems. For this reason, we instead consider the ranked list of all the items with no interaction.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">We empirically found that these hyperparameters hardly affect the final performance of BUIR, and the sensitivity analysis on the parameters is provided in Section 4.6.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">In this comparison, we exclude the generative OCCF methods as our baselines, because they do not explicitly output the item representations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">Considering that the target encoder should be slowly approximate the online encoder, we investigate 𝜏 in the range of [0.9, 1.0], as done in previous work<ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by the NRF grant funded by the MSIT (No. 2020R1A2B5B03097210, 2021R1C1C1009081), and the IITP grant funded by the MSIT (No. 2018-0-00584, 2019-0-01906).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<idno>@10 0.0679 0.0426 0.0533 0.0639 0.0786 15.83% 0.0576 0.0746 0.0481 0.0705 0.0812</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cfgan: A generic collaborative filtering framework based on generative adversarial networks</title>
		<author>
			<persName><forename type="first">Dong-Kyu</forename><surname>Chae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Soo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Tae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploring Simple Siamese Representation Learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sampler design for bayesian personalized ranking by leveraging view data</title>
		<author>
			<persName><forename type="first">Jingtao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Pierre H Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="21271" to="21284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>In AISTATS</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">VBPR: visual Bayesian Personalized Ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="144" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno>WWW. 173-182</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Collaborative metric learning</title>
		<author>
			<persName><forename type="first">Cheng-Kang</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Estrin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In WWW. 193-201</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Collaborative filtering for implicit feedback datasets</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DE-RRD: A Knowledge Distillation Framework for Recommender System</title>
		<author>
			<persName><forename type="first">Seongku</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonbin</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="605" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dual neural personalized ranking</title>
		<author>
			<persName><forename type="first">Seunghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongwuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="863" to="873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On Sampled Metrics for Item Recommendation</title>
		<author>
			<persName><forename type="first">Walid</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1748" to="1757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Symmetric Metric Learning with Adaptive Margin for Recommendation</title>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuqing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanhui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjun</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4634" to="4641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Variational autoencoders for collaborative filtering</title>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Rahul G Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="689" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep generative ranking for personalized recommendation</title>
		<author>
			<persName><forename type="first">Huafeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingxuan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liping</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="34" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An Experimental Evaluation of Point-of-Interest Recommendation in Location-Based Social Networks</title>
		<author>
			<persName><forename type="first">Yiding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuan-Anh Nguyen</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1010" to="1021" />
			<date type="published" when="2017-06">2017. jun 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adria</forename><forename type="middle">Puigdomenech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>ICML. 1928-1937</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">One-class collaborative filtering</title>
		<author>
			<persName><forename type="first">Rong</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajan</forename><surname>Lukose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="502" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Collaborative translational metric learning</title>
		<author>
			<persName><forename type="first">Chanyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="367" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving pairwise learning for item recommendation from implicit feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="273" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">mTrust: discerning multi-faceted trust in a connected world</title>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiji</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Collaborative topic regression with social regularization for tag recommendation</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wu-Jun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Irgan: A minimax game for unifying generative and discriminative information retrieval models</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dell</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="515" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural graph collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Collaborative denoising auto-encoders for top-n recommender systems</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
