<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Flexible Technique for Accurate Omnidirectional Camera Calibration and Structure from Motion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
							<email>davide.scaramuzza@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Swiss Federal Institute of Technology Lausanne (EPFL</orgName>
								<address>
									<postCode>CH-1015</postCode>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Agostino</forename><surname>Martinelli</surname></persName>
							<email>agostino.martinelli@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Swiss Federal Institute of Technology Lausanne (EPFL</orgName>
								<address>
									<postCode>CH-1015</postCode>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
							<email>roland.siegwart@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Swiss Federal Institute of Technology Lausanne (EPFL</orgName>
								<address>
									<postCode>CH-1015</postCode>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Flexible Technique for Accurate Omnidirectional Camera Calibration and Structure from Motion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">705B88C2613FF81CC72F1F8292CCC6E4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a flexible new technique for single viewpoint omnidirectional camera calibration. The proposed method only requires the camera to observe a planar pattern shown at a few different orientations. Either the camera or the planar pattern can be freely moved. No a priori knowledge of the motion is required, nor a specific model of the omnidirectional sensor. The only assumption is that the image projection function can be described by a Taylor series expansion whose coefficients are estimated by solving a two-step least-squares linear minimization problem. To test the proposed technique, we calibrated a panoramic camera having a field of view greater than 200째 in the vertical direction, and we obtained very good results. To investigate the accuracy of the calibration, we also used the estimated omni-camera model in a structure from motion experiment. We obtained a 3D metric reconstruction of a scene from two highly distorted omnidirectional images by using image correspondences only. Compared with classical techniques, which rely on a specific parametric model of the omnidirectional camera, the proposed procedure is independent of the sensor, easy to use, and flexible.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Accurate calibration of a vision system is necessary for any computer vision task requiring extracting metric information of the environment from 2D images, like in ego-motion estimation and structure from motion. While a number of methods have been developed concerning planar camera calibration <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21]</ref>, little work on omnidirectional cameras has been done, and the primary focus has been on particular sensor types.</p><p>For omnidirectional camera is usually intended a vision system providing a 360째 panoramic view of the scene. Such an enhanced field of view can be achieved by either using catadioptric systems, obtained by opportunely combining mirrors and conventional cameras, or employing purely dioptric fish-eye lenses <ref type="bibr" target="#b13">[13]</ref>. As noted in <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b11">11]</ref>, it is highly desirable that such imaging systems have a single viewpoint <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b6">6]</ref>. That is, there exists a single center of projection, so that, every pixel in the sensed images measures the irradiance of the light passing through the same viewpoint in one particular direction. The reason a single viewpoint is so desirable is that it permits the generation of geometrically correct perspective images from the pictures captured by the omnidirectional camera. Moreover, it allows applying the known theory of epipolar geometry, which easily permits to perform ego-motion estimation and structure-from-motion from image correspondences only.</p><p>Previous works on omnidirectional camera calibration can be classified into two different categories. The first one includes methods which exploit prior knowledge about the scene, such as the presence of calibration patterns <ref type="bibr" target="#b5">[5,</ref><ref type="bibr">7]</ref> or plumb lines <ref type="bibr" target="#b8">[8]</ref>. The second group covers techniques that do not use this knowledge. This includes calibration methods from pure rotation <ref type="bibr">[7]</ref> or planar motion of the camera <ref type="bibr" target="#b9">[9]</ref>, and self-calibration procedures, which are performed from point correspondences and epipolar constraint through minimizing an objective function <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11]</ref>. All mentioned techniques allow obtaining accurate calibration results, but primarily focus on particular sensor types (e.g. hyperbolic and parabolic mirrors or fish-eye lenses). Moreover, some of them require special setting of the scene and expensive equipment <ref type="bibr">[7,</ref><ref type="bibr" target="#b9">9]</ref>. For instance, in <ref type="bibr">[7]</ref>, a fish-eye lens with a 183째 field of view is used as an omnidirectional sensor.</p><p>Here, the calibration is performed by using a halfcylindrical calibration pattern perpendicular to the camera sensor, which rotates on a turntable. In <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b10">10]</ref>, the authors treat the case of a parabolic mirror. In <ref type="bibr" target="#b8">[8]</ref> it is shown that vanishing points lie on a conic section which encodes the entire calibration information. Thus, projections of two sets of parallel lines suffice for intrinsic calibration. However, this property does not apply to non-parabolic mirrors. Therefore, the proposed technique cannot be easily generalized to other kinds of sensors.</p><p>Conversely, the methods described in <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b14">14]</ref> fall in the self-calibration category. These methods require no calibration pattern, nor a priori knowledge about the scene. The only assumption is the capability to automatically find point correspondences in a set of panoramic images of the same scene. Then, calibration is directly performed by epipolar geometry by minimizing an objective function. In <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b12">12]</ref>, this is done by employing a parabolic mirror, while in <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b14">14]</ref> a fish-eye lens with a view angle greater than 180째 is used. However, besides focusing on particular sensor types, the mentioned self-calibration techniques may suffer in case of tracking difficulties and of a small number of features points <ref type="bibr" target="#b16">[16]</ref>.</p><p>All previous calibration procedures focus on particular sensor types, such as parabolic and hyperbolic mirrors or fish-eye lenses. Furthermore, they are strongly dependent on the omnidirectional sensor model they use, which is suitable only when the single effective viewpoint property is satisfied. Although several panoramic vision systems exist directly manufactured to have this property, for a catadioptric system this requires to accurately align the camera and the mirror axes. In addition, the focus point of the mirror has to coincide with the camera optical center. Since it is very difficult to avoid camera-mirror misalignments, an incorrectly aligned catadioptric sensor can lead to a quasi single-viewpoint optical system <ref type="bibr" target="#b2">[2]</ref>. As a result, the sensor model used by the mentioned techniques could be suboptimal. In the case of fish-eye lenses the discussion above is analogue. Motivated by this observation, we propose a calibration procedure which uses a generalized parametric model of the sensor, which is suitable to different kinds of omnidirectional vision systems, both catadioptric and dioptric. The proposed method requires the camera to observe a planar pattern shown at a few different locations. Either the camera or the planar pattern can be freely moved. No a-priori knowledge of the motion is required, nor a specific model of the omnidirectional sensor. The developed procedure is based on the assumption that the circular external boundary of the mirror or of the fish-eye lens (respec-tively in the catadioptric and dioptric case) is visible in the image. Moreover, we assume that the image formation function, which manages the projection of a 3D real point onto a pixel of the image plane, can be described by a Taylor series expansion. The expansion coefficients, which constitute our calibration parameters, are estimated by solving a two-step least-squares linear minimization problem. Finally, the order of the series is determined by minimizing the reprojection error of the calibration points. The proposed procedure does not require any expensive equipment. Moreover, it is very fast and completely automatic, as the user is only requested to collect a few images of the calibration pattern. The method was applied to a KAIDAN 360째 One VR single-viewpoint mirror mounted on a CCD camera. The system has a vertical view angle greater than 200째 and the image size is 900x1200 pixels. After calibration, we obtained an average reprojection error of 1 pixel. In order to test the accuracy of the method, we used the estimated model in a structure from motion problem, and we obtained a 3D metric reconstruction of a scene from two highly distorted omnidirectional images, by using image correspondences only. The structure of the paper is the following. The omnidirectional camera model and calibration are described in Sec. 2 and 3. The results of the calibration of a real system are given in Sec. 4. Finally, the 3D structure from motion experiment and its accuracy are shown and discussed in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Omnidirectional Camera Model</head><p>We want to generalize our procedure to different kinds of single-viewpoint omnidirectional vision systems, both catadioptric and dioptric. In this section we will use the notation given in <ref type="bibr" target="#b11">[11]</ref>.</p><p>In the general omnidirectional camera model, we identify two distinct references: the camera image plane . In Fig. <ref type="figure" target="#fig_2">1</ref> the two reference planes are shown in the case of a catadioptric system. In the dioptric case, the sign of u'' would be reversed because of the absence of a reflective surface. All coordinates will be expressed in the coordinate system placed in O, with the z axis aligned with the sensor axis (see Fig. <ref type="figure" target="#fig_2">1a</ref>).</p><p>Let X be a scene point. Then, assume</p><formula xml:id="formula_0">T ] ' ' , ' ' [ v u ' u'</formula><p>be the projection of X onto the sensor plane, and</p><formula xml:id="formula_1">T ] ' , ' [ v u u'</formula><p>its image in the camera plane (Fig. <ref type="figure" target="#fig_2">1b</ref> and<ref type="figure" target="#fig_2">1c</ref>). As observed in <ref type="bibr" target="#b11">[11]</ref>, the two systems are related by an affine transformation, which incorporates the digitizing process and small axes misalignments; thus</p><formula xml:id="formula_2">t A u' ' u'</formula><p>, where x . Then, let us introduce the image projection function g, which captures the relationship between a point ' u' , in the sensor plane, and the vector p emanating from the viewpoint O to a scene point X (see Fig. <ref type="figure" target="#fig_2">1a</ref>). By doing so, the complete model of an omnidirectional camera is 0</p><formula xml:id="formula_3">, P t A X u' g ' u' g p , (1)</formula><p>where 4 X is expressed in homogeneous coordinates; 3x4 P is the perspective projection matrix. By calibration of the omnidirectional camera we mean the estimation of the matrices A and t, and the non-linear function g, so that all vectors t A u' g satisfy the projection equation ( <ref type="formula">1</ref>). This means that, once the omnidirectional camera is calibrated, we are able to reconstruct, from each pixel, the direction of the corresponding scene point in the real world. We assume for g the following expression</p><formula xml:id="formula_4">T , u'',v'' f u'',v'' u'',v'' g , (<label>2</label></formula><formula xml:id="formula_5">)</formula><p>where f is rotationally symmetric with respect to the sensor axis. For instance, in the catadioptric case, this corresponds to assume that the mirror is perfectly symmetric with respect to its axis. In general, such an assumption is highly reasonable because both mirror profiles and fish-eye lenses are manufactured with micrometric precision. Function f can have various forms related to the mirror or the lens construction <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref>. As mentioned in the introduction, we want to apply a generalized parametric model of f , which is suitable to different kinds of sensors. Moreover, we want this model to compensate for any misalignment between the focus point of the mirror (or the fish-eye lens) and the camera optical center. We propose the following polynomial form for</p><formula xml:id="formula_6">f N N a a a a u'',v'' f ,, 2 ,, 2 ,, 1 0 ... ,<label>(3)</label></formula><p>where the coefficients ...N 2, 1, 0, , i a i , and the polynomial degree N are the model parameters to be determined by the calibration; 0 , , is the metric distance from the sensor axis. Thus, (1) can be rewritten as</p><formula xml:id="formula_7">0 , P ' ' , ' ' t A t A ' ' ' ' ' ' X u' u' g v u f w v u (4).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Camera Calibration</head><p>By calibration of an omnidirectional camera we mean the estimation of the parameters [A, t, N a a a a ,..., , ,</p><p>] so that all vectors t A u' g satisfy the equation (4). In order to reduce the number of parameters to be estimated, we compute the matrices A and t, up to a scale factor , by transforming the view field ellipse (see Fig. <ref type="figure" target="#fig_2">1c</ref>) into a circle centered on the ellipse center. This transformation is calculated automatically by using an ellipse detector if the circular external boundary of the sensor is visible in the image. After performing the affine transformation, an image point  </p><formula xml:id="formula_9">3 2 i 0 ij ij ij ij N ij N ij ij ij ij Y X Y X a a v u i i i 1 i i i i 1 ij t r r t r r r X p</formula><p>Therefore, in order to solve for camera calibration, the extrinsic parameters have to be determined for each pose of the calibration pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Solving for camera extrinsic parameters</head><p>Before describing how to determine the extrinsic parameters, let us eliminate the dependence from the depth scale ij . This can be done by multiplying both sides of equation ( <ref type="formula">6</ref>) vectorially by ij p 0 1 ...</p><formula xml:id="formula_10">0 1 0 ij ij N ij N ij ij ij ij ij Y X a a v u Y X i i 2 i 1 i i 2 i 1 ij ij ij t r r t r r p p p . (7)</formula><p>Now, let us focus on a particular observation of the calibration pattern. From (7), we have that each point j p on the pattern contributes three homogeneous equations </p><formula xml:id="formula_11">0 ) ( ) ( ) ( 2 22 21 3 32 31 t Y r X r f t Y r X r v j j j j j j (8.1) 0 ) ( ) ( ) ( 3 32 31 1 12 11 t Y r X r u t Y r X r f j j j j j j (8.2) 0 ) ( ) (</formula><formula xml:id="formula_12">L L L L L L L L L L u v Y u X u Y v X v u v Y u X u Y v X v M : : : : : : 1 1 1 1 1 1 1 1 1 1</formula><p>A linear estimate of H can be obtained by minimizing the least-squares criterion The solution of ( <ref type="formula">9</ref>) is known up to a scale factor, which can be determined uniquely since vectors 2 1 , r r are orthonormal. Because of the orthonormality, the unknown entries 32 31 , r r can also be computed uniquely.</p><p>To resume, the first calibration step allows finding the extrinsic parameters for each pose of the calibration pattern, except for the translation parameter 3 t . This parameter will be computed in the next step, which concerns the estimation of the image projection function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Solving for camera intrinsic parameters</head><p>In the previous step, we exploited equation (8.3) to find the camera extrinsic parameters. Now, we substitute the estimated values in the equations (8.1) and (8.2), and solve for the camera intrinsic parameters N a a a a ,..., , , 2 1 0 that describe the shape of the image projection function g. At the same time, we also compute the unknown i t 3 for each pose of the calibration pattern. As done above, we stack all the unknown entries of (8.1) and (8.2) into a vector and rewrite the equations as a system of linear equations. But now, we incorporate all K observations of the calibration rig. We obtain the following system (10)  </p><formula xml:id="formula_13">K K K N K N K K K K K K N K K K K K N N D B D B t t a a u C C C v A A A u C C C v A</formula><formula xml:id="formula_14">i i i i i i t Y r X r A 2 22 21 , ) ( 32 31 i i i i i i Y r X r v B , i i i i i i t Y r X r C 1 12 11 and ) ( 32 31 i i i i i i X r X r u D .</formula><p>Finally, the least-squares solution of the overdetermined system is obtained by using the pseudoinverse. Thus, the intrinsic parameters N a a a a ,..., , ,</p><p>, which describe the model, are now available. In order to compute the best polynomial degree N, we actually start from N=2. Then, we increase N by unitary steps and we compute the average value of the reprojection error of all calibration points. The procedure stops when a minimum error is found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>The calibration algorithm presented in the previous sections was tested on real data. The omnidirectional sensor to be calibrated is a catadioptric system composed of a KAIDAN 360째 One VR hyperbolic mirror and a SONY CCD camera having a resolution of 900x1200 pixels. The calibration rig is a checker pattern containing 9x7 squares, so there are 48 corners (calibration points) (see Fig. <ref type="figure">4</ref>). The size of the pattern is 24.3cm x 18.9 cm. Eleven images of the plane under different orientations were taken, some of which are shown in Fig. <ref type="figure" target="#fig_4">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2. Some images of the calibration pattern taken under different orientations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Performance with respect to the number of planes and the polynomial degree</head><p>This experiment investigates the performance of our technique with respect to the number of images of the planar pattern, for a given polynomial degree. We vary the number of pictures from 2 to 11, and for each set we perform the calibration. Next, according to the estimated extrinsic parameters, we reproject the 3D calibration points onto the images. Then, we compute the Root of Mean Squared Distances (RMS), in pixels, between the detected image points and the reprojected ones. The calculated RMS values versus the number of images are plotted in Fig. <ref type="figure">3</ref> for different polynomial degrees. Note that the error decreases when more images are used. Moreover, by using a 4 th order polynomial to fit the model, we obtain the minimum RMS value, that is of about 1.2 pixels. A 3 rd order polynomial also provides a similar performance if more than four images are taken. Conversely, by using a 2 nd order expansion, the RMS remains above 2 pixels. Thus, for our applications we used a 4 th order expansion. As a result, the RMS error of all reprojected calibration points is 1.2 pixels. This value is very good if we consider that the image resolution is 900x1200 pixels, and that corner detection is less precise on omnidirectional images than on conventional perspective pictures. In Fig. <ref type="figure">4</ref> you can see several corner points used to perform the calibration, and the same points reprojected onto the image according to the intrinsic and extrinsic parameters estimated by the calibration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance with respect to the noise level</head><p>In this experiment, we study the robustness of our calibration technique in case of inaccuracy in detecting the calibration points. At this end, Gaussian noise with mean 0 and standard deviation is added to the input calibration points. We vary the noise level from 0.1 pixels to 1.5 pixels. For each level, we perform the calibration and we compute the RMS error of the reprojected points. The results obtained using a 4 th order polynomial are shown in Fig. <ref type="figure">5</ref>. As it can be seen, the RMS values remain under 2 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance with respect to image rectification</head><p>In this experiment, we test the accuracy of the estimated sensor model by rectifying all calibration images. Rectification determines a transformation of the original distorted image such that the new image appears as taken by a conventional perspective camera.</p><p>In general, is impossible to rectify the whole omnidirectional image because of a view field larger than 180째. However, it is possible to perform rectification on image regions which cover a smaller field of view. As a result, linearity is preserved in the rectified image. As you can see in Fig. <ref type="figure" target="#fig_11">6</ref>, curved edges of a sample pattern in the original image (Fig. <ref type="figure" target="#fig_11">6</ref>) appear straight after rectification (Fig. <ref type="figure" target="#fig_12">7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Application to Structure from Motion</head><p>Our work on omnidirectional camera calibration is motivated by the use of panoramic vision sensors for structure from motion and 3D reconstruction. In this section, we perform a 3D metric reconstruction of a real object from two omnidirectional images, by using the sensor model estimated by our calibration procedure. In order to compare the reconstruction results with a ground truth, we exploited a trihedral object composed of three orthogonal checker patterns of known geometry (see Fig. <ref type="figure" target="#fig_9">8</ref>). Two images of the trihedron were taken by positioning our calibrated camera at two unknown different locations (see Fig. <ref type="figure" target="#fig_13">9</ref>). Then, several point matches were picked manually from both views of the object and the eight point algorithm <ref type="bibr" target="#b17">[17]</ref> was applied. In order to obtain good reconstruction results, more than eight points (actually 135) were extracted. Then, the coordinates of the correspondent 3D vectors, back-projected into the space, were normalized according to the nonuniform mirror resolution. The results of the reconstruction are shown in Fig. <ref type="figure" target="#fig_14">10</ref>, where we used checker patches to fit the reconstructed 3D points (red rounds). In order to compare the results with the ground truth, we computed the angles between the three planes fitting the reconstructed points. We found the following values: 94.6째, 86.8째 and 85.3째. Moreover, the average distances of these points from the fitted planes were respectively 0.05 cm, 0.75 cm and 0.07 cm. Finally, since we knew the size of each checker to be 6.0 cm x 6.0 cm, we also calculated the dimension of every reconstructed checker, and we found an average error of 0.29 cm.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we presented a flexible new technique for single-viewpoint omnidirectional camera calibration. The proposed method only requires the camera to observe a planar pattern shown at a few different orientations. No a-priori knowledge of the motion is required, nor a specific model of the omnidirectional sensor. The only assumption is that the image projection function can be described by a Taylor series expansion whose coefficients are estimated by solving a two-step least-squares linear minimization problem. To test the proposed technique, we calibrated a panoramic camera having a field of view greater than 200째 in the vertical direction, and we obtained very good results. To investigate the accuracy of the calibration, we also used the estimated omni-camera model in a structure from motion experiment. We obtained a 3D metric reconstruction of a real object from two omnidirectional images, by using image correspondences only. The reconstruction results were also compared with the ground truth. With respect to classical techniques, which rely on a specific parametric model of the omnidirectional camera, the proposed procedure is independent of the sensor, easy to use and flexible.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (a) Coordinate system in the catadioptric case. (b) Sensor plane, in metric coordinates. (c) Camera image plane, expressed in pixel coordinates. (b) and (c) are related by an affine transformation.</figDesc><graphic coords="3,224.47,423.40,69.96,60.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>u' is related to the corresponding point on the sensor plane ' by substituting this relation in (4) and using (3), we have the following projection equation u and ' v are the pixel coordinates of an image point with respect to the circle center, and ' is the Euclidean distance. Also, note that the factor can be directly integrated in the depth factor ; thus, only N+1 parameters ( be estimated.During the calibration procedure, a planar pattern of known geometry is shown at different unknown positions, which are related to the sensor coordinate system by a rotation matrix ] [ 3D coordinate of its points in the pattern coordinate system, and T in the image plane. Since we assumed the pattern to be planar, without loss of generality we have 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2 .</head><label>2</label><figDesc>This is accomplished by using the SVD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Proceedings of the Fourth IEEE International Conference on Computer Vision Systems (ICVS 2006) 0-7695-2506-7/06 $20.00 짤 2006 IEEE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>where</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .Figure 3 .</head><label>43</label><figDesc>Figure 4. The corner points used for calibration (red crosses) and the reprojected ones (yellow rounds) after calibration.</figDesc><graphic coords="5,360.31,523.50,141.05,105.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. The sample trihedron used for the 3D reconstruction experiment.</figDesc><graphic coords="6,360.68,261.65,132.33,102.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>5 Figure 5 .</head><label>55</label><figDesc>Figure 5. RMS error versus the noise level.</figDesc><graphic coords="6,72.67,372.20,102.88,80.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6</head><label>6</label><figDesc>Figure 6. A sample image before rectification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7</head><label>7</label><figDesc>Figure 7. The sample image of Fig. 6 after rectification. Now the edges (highlighted) appear straight.</figDesc><graphic coords="6,189.82,372.20,98.14,79.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Two pictures of the trihedron taken by the omnidirectional camera. The points used for the 3D reconstruction are indicated by red dots.</figDesc><graphic coords="7,75.73,256.85,101.05,108.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 .</head><label>10</label><figDesc>Figure10. Three rendered views of the reconstructed trihedron. Note that the object was reconstructed only from two highly distorted omnidirectional images (as in Fig.9).</figDesc><graphic coords="7,98.45,365.19,169.87,125.96" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Fourth IEEE International Conference on Computer Vision Systems (ICVS 2006) 0-7695-2506-7/06 $20.00 짤 2006 IEEE</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b1">1</ref> <p>This work was supported by the European project COGNIRON (the Cognitive Robot Companion).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theory of catadioptric image formation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV&apos;98)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV&apos;98)<address><addrLine>Bombay, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Caustics of catadioptric cameras</title>
		<author>
			<persName><forename type="first">R</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Grossberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV&apos;01)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV&apos;01)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Autocalibration &amp; 3D Reconstruction with Non-central Catadioptric Cameras</title>
		<author>
			<persName><forename type="first">B</forename><surname>Micusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision and Pattern Recognition (CVPR&apos;04)</title>
		<meeting>International Conference on Computer Vision and Pattern Recognition (CVPR&apos;04)</meeting>
		<imprint>
			<publisher>Washington US</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A theory of single-viewpoint catadioptric image formation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="196" />
			<date type="published" when="1999-11">November 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reconstruction with the calibrated syclop sensor</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cauchois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brassart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Delahoche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delhommelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Intelligent Robots and Systems (IROS&apos;00)</title>
		<meeting>the IEEE International Conference on Intelligent Robots and Systems (IROS&apos;00)<address><addrLine>Takamatsu, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1493" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Central panoramic cameras: Geometry and design</title>
		<author>
			<persName><forename type="first">T</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hlavac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research</title>
		<imprint>
			<date type="published" when="1997-12">December 1997</date>
			<pubPlace>Praha, Czech Republic</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Czech Technical University -Center for Machine Perception</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Panoramic mosaicing with a 180 field of view lens</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bakstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Workshop on Omnidirectional Vision</title>
		<meeting>the IEEE Workshop on Omnidirectional Vision</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="60" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Paracatadioptric camera calibration</title>
		<author>
			<persName><forename type="first">C</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="687" to="695" />
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ego-motion and omnidirectional cameras</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gluckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV&apos;98)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV&apos;98)<address><addrLine>Bombay, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="999" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Catadioptric self-calibration</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;00)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="201" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Estimation of omnidirectional camera model from epipolar geometry</title>
		<author>
			<persName><forename type="first">B</forename><surname>Micusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CVPR&apos;03</title>
		<meeting>of CVPR&apos;03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="485" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Para-catadioptric Camera Autocalibration from Epipolar Geometry</title>
		<author>
			<persName><forename type="first">B</forename><surname>Micusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV 2004, Korea</title>
		<imprint>
			<date type="published" when="2004-01">January 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Kumler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<title level="m">Fisheye lens designs and their relative performance</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3D Metric Reconstruction from Uncalibrated Omnidirectional Images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Micusik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Martinec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV&apos;04</title>
		<meeting><address><addrLine>Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-01">January 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Epipolar Geometry for Central Catadioptric Cameras</title>
		<author>
			<persName><forename type="first">T</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="37" />
			<date type="published" when="2002-08">August 2002</date>
			<publisher>Kluwer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From projective to Euclidean space under any practical situation, a criticism of selfcalibration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bougnoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Computer Vision</title>
		<meeting>the 6th International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1998-01">Jan. 1998</date>
			<biblScope unit="page" from="790" to="796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A computer algorithm for reconstructing a scene from two projections</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Longuet-Higgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">293</biblScope>
			<biblScope unit="page" from="133" to="135" />
			<date type="published" when="1981-09">Sept 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">In defence of the 8-point algorithm</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV&apos;95)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV&apos;95)</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An invitation to 3D vision, from images to geometric models models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<imprint>
			<publisher>Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-calibration of a moving camera from point correspondences and fundamental matrices</title>
		<author>
			<persName><forename type="first">Q.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="261" to="289" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Flexible New Technique for Camera Calibration</title>
		<author>
			<persName><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1330" to="1334" />
			<date type="published" when="2000-11">November 2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
