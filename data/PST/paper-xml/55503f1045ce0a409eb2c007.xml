<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature selection algorithm based on bare bones particle swarm optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Electrical Engineering</orgName>
								<orgName type="institution">China University of Mining and Technology</orgName>
								<address>
									<postCode>221008</postCode>
									<settlement>Xuzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dunwei</forename><surname>Gong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Electrical Engineering</orgName>
								<orgName type="institution">China University of Mining and Technology</orgName>
								<address>
									<postCode>221008</postCode>
									<settlement>Xuzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ying</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Electrical Engineering</orgName>
								<orgName type="institution">China University of Mining and Technology</orgName>
								<address>
									<postCode>221008</postCode>
									<settlement>Xuzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wanqiu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Electrical Engineering</orgName>
								<orgName type="institution">China University of Mining and Technology</orgName>
								<address>
									<postCode>221008</postCode>
									<settlement>Xuzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Feature selection algorithm based on bare bones particle swarm optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">204727D54E8B1FFB127B20D4E9DA7A12</idno>
					<idno type="DOI">10.1016/j.neucom.2012.09.049</idno>
					<note type="submission">Received 5 April 2012 Received in revised form 25 May 2012 Accepted 13 September 2012</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Feature selection Bare bones particle swarm Reinforced memory Uniform combination 1-Nearest neighbor</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature selection is a useful pre-processing technique for solving classification problems. As an almost parameter-free optimization algorithm, the bare bones particle swarm optimization (BPSO) has been applied to the topic of optimization on continuous or integer spaces, but it has not been applied to feature selection problems with binary variables. In this paper, we propose a new method to find optimal feature subset by the BPSO, called the binary BPSO. In this algorithm, a reinforced memory strategy is designed to update the local leaders of particles for avoiding the degradation of outstanding genes in the particles, and a uniform combination is proposed to balance the local exploitation and the global exploration of algorithm. Moreover, the 1-nearest neighbor method is used as a classifier to evaluate the classification accuracy of a particle. Some international standard data sets are selected to evaluate the proposed algorithm. The experiments show that the proposed algorithm is competitive in terms of both classification accuracy and computational performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The purpose of feature selection (FS) is to move the useless features without sacrificing the predictive accuracy and find a feature subset from an original set of features. Since a feature selection method can improve the predictive accuracy of algorithm and shorten computation cost required in data mining, in last few decades this kind of problems has become an emerging field.</p><p>According to search strategies of feature subsets, the existing feature selection methods can be categorized into two classes: the filter approach and the wrapper approach. The filter approach relies primarily on general characteristics of data sets to evaluate and select feature subsets without considering a special learning approach. Hence, the efficiency of this approach depends mainly on the data set itself rather than on the classifier <ref type="bibr" target="#b0">[1]</ref>. The wrapper approach employs a classification algorithm to evaluate feature subsets, and adopts a search strategy to seek for optimal subsets. Since the wrapper approach considers a classifier with the evaluation or search process, this approach often gets better results than the filter one <ref type="bibr" target="#b1">[2]</ref>.</p><p>Because success of the wrapper approach depends mainly on adopting a fruitful search strategy, various search methods have been designed to generate subsets and progress the search process, such as exact methods <ref type="bibr" target="#b2">[3]</ref> and nested partitioning methods <ref type="bibr" target="#b3">[4]</ref>. Since meta-heuristic methods are able to find solutions rapidly in the full search space by some global search strategies, recently many studies attempt the use of meta-heuristic methods for feature selection problems. For instance, Oh et al. introduced a hybrid genetic algorithm for feature selection, called the HGA. The HGA incorporates the local search operations parameterized with ripple factors to fine tune the search in FS process <ref type="bibr" target="#b4">[5]</ref>. Huang presented an ant colony optimization (ACO) based classifier model to optimize simultaneously feature subsets and the kernel parameters of support vector machine (SVM) <ref type="bibr" target="#b5">[6]</ref>. Su and Lin applied successfully the electromagnetism-like mechanism (EM) as a wrapper approach in feature selection <ref type="bibr" target="#b0">[1]</ref>. Lin et al. combined simulated annealing (SA) with SVM to deal with feature selection and classification <ref type="bibr" target="#b6">[7]</ref>.</p><p>Like genetic algorithm, particle swarm optimization (PSO) is a meta-heuristic search technique that is inspired by the behavior of bird flocks <ref type="bibr" target="#b7">[8]</ref>. Due to its advantages such as simplicity, fast convergence and population-based feature, the attention of researchers upon PSO is much higher. Recently, several PSObased methods such as the FS method based on rough sets and PSO <ref type="bibr" target="#b8">[9]</ref>, the geometric PSO based method <ref type="bibr" target="#b9">[10]</ref>, the chaotic binary PSO based method <ref type="bibr" target="#b10">[11]</ref>, the discrete PSO with adaptive feature selection <ref type="bibr" target="#b11">[12]</ref>, the method based on correlation-based feature selection (CFS) and Taguchi chaotic binary PSO <ref type="bibr" target="#b12">[13]</ref>, have been proposed to solve feature selection problems.</p><p>These methods above provide effective approaches for solving feature selection problems. However, as an extension of the PSO, they still require users to tune fine control parameters such as inertia weight, acceleration coefficients and velocity clamping in Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/neucom order to obtaining a desirable subset. Moreover, empirical and theoretical studies have shown that the convergence behavior of PSO depends strongly on the values of those control parameters <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. To overcome the above disadvantage, now many adaptive or dynamic methods have been introduced to adjust the inertia weight and/or the acceleration coefficients, such as the literature <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27]</ref>. However, since the setting of these control parameters depends on individual applications, and needs to be adjusted for different problems, how to set those parameters is still a challenging research subject.</p><p>The BPSO is first proposed by Kennedy in 2003 <ref type="bibr" target="#b14">[15]</ref>. Compared to the canonical PSO, it is simple and there is a few parameters to tune. Up to now, the concept of BPSO has been used in early literature to deal with optimization problems in continuous space or integer space <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. In this paper, we extend the continuous version of BPSO to solve the feature selection problem with binary variables. Two novel operators, adaptive disturbance and uniform combination, are proposed to improve performance of the proposed algorithm. Moreover, the 1-nearest neighbor method is used to evaluate fitness of the particles.</p><p>This paper is organized as follows. Section 2 gives a brief review of BPSO. The proposed BPSO-based methodology is described in Section 3. In Sections 4 and 5, comparison results and performance analysis with the other FS methods are given, respectively. The conclusions are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Binary PSO</head><p>The PSO is inspired by the social behavior of some biological organisms, especially the group's ability of some animal species to locate a desirable position in the given area <ref type="bibr" target="#b7">[8]</ref>. In the PSO, a swarm consists of a set of particles, and each particle represents a potential solution of the optimized problem. At every iteration each particle adjusts its velocity and position by tracking two best positions. One is the best position found by the particle itself so far, called the local leader or the personal best position (Pbest); another is the global best position found by neighbors of this particle so far, called the global leader or the global best position (Gbest). Taking the ith particle in the swarm as an example, its new position and velocity are calculated as follows: v i;j ðt þ 1Þ ¼ wnv i;j ðtÞþR 1 c 1 nðPbest i;j ðtÞÀx i;j ðtÞÞ þ R 2 c 2 nðGbest j ðtÞÀx i;j ðtÞÞ ð1Þ</p><formula xml:id="formula_0">x i;j ðt þ 1Þ ¼ x i;j ðtÞþv i;j ðt þ 1Þ:<label>ð2Þ</label></formula><p>where j is the index of position in the particle, t shows the iteration times, v i ðtÞ is the velocity of the ith particle. The acceleration coefficients c 1 and c 2 are nonnegative constants which control the influence of Pbest and Gbest on the search process; w is the inertia weight to control particle's exploration in the search space <ref type="bibr" target="#b18">[19]</ref>; R 1 and R 2 are two random numbers within [0.1].</p><p>The pioneer PSO is developed to solve the problems on continuous space. In 1997, Kennedy and Eberhart extended it to deal with the problems with binary variables, and proposed the binary PSO <ref type="bibr" target="#b19">[20]</ref>. In the binary PSO, the velocity obtained by Eq. ( <ref type="formula">1</ref>) is transformed into a vector within [0,1] through the sigmoid function sðÞ:</p><formula xml:id="formula_1">sðv i;j Þ ¼ 1=ð1 þ e À v i;j Þ ð<label>3Þ</label></formula><p>where sðv i;j Þ represents the probability that the jth bit is 1.</p><p>Hence, the new position of a particle is updated by the following equation:</p><formula xml:id="formula_2">x i;j ¼ 0 if sðv i;j Þ o R 3 ; 1 otherwise:<label>ð4Þ</label></formula><p>where R 3 is a random number within [0,1]. Up to now, the discrete or binary PSO has been widely studied and used in many real problems. The successfully application includes the flowshop scheduling problems <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, the vehicle routing problem <ref type="bibr" target="#b29">[30]</ref>, and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Bare bones PSO</head><p>The BPSO is proposed by Kennedy. This algorithm does not use the velocity, but uses a Gaussian sampling based on Pbest and Gbest to update the positions of particles, as follows:</p><formula xml:id="formula_3">x i;j ðt þ 1Þ ¼ N</formula><p>Pbest i;j ðtÞþGbest j ðtÞ 2 ; jPbest i;j ðtÞÀGbest j ðtÞj ð5Þ</p><p>In Eq. ( <ref type="formula">5</ref>), the positions of a particle are randomly selected from the Gaussian distribution with the mean ðPbest i;j ðtÞþGbest j ðtÞÞ=2 and the variance jPbest i;j ðtÞÀ Gbest j ðtÞj. Pan et al. demonstrated that the BPSO can be deduced from the traditional PSO <ref type="bibr" target="#b20">[21]</ref>. Moreover, Kennedy proposed a variation of the BPSO, called the exploiting bare bones PSO (BPSO-E), where Eq. ( <ref type="formula">5</ref>) is replaced by </p><formula xml:id="formula_4">x i;j ðt þ</formula><p>Compared to the traditional PSO, it can be seen that the BPSO is more compact and control parameter-free. Hence, it is natural to apply the BPSO to real application problems. Successful examples include the inte\ger programming <ref type="bibr" target="#b15">[16]</ref>, the modeling of vaporliquid equilibrium data <ref type="bibr" target="#b16">[17]</ref>, the multi-objective power economic dispatch <ref type="bibr" target="#b17">[18]</ref>. However, as far as we know, there has been very little discussion in the existing literature to use the BPSO to solve the problems with binary variables, in particular, the feature selection problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Description of the proposed algorithm</head><p>This section describes the proposed algorithm for feature selection problems, namely the binary BPSO. The motivation for this algorithm is to design a global search technique with a few parameters, which not only has a good performance on tackling feature selection problems, but also is easy to implement. Fig. <ref type="figure" target="#fig_0">1</ref> shows pseudo-code of the binary BPSO algorithm, where // n and n // are used to provide comments and distinguish them from the code.</p><p>In Step 1, the swarm is initialized randomly in the search space, and each particle selects the position itself as the personal best position. After that, the same iteration steps are run circularly to find optimal feature subset, until the maximum iteration times T max is satisfied. Within each iteration, each particle updates its Pbest and its Gbest by using the functions GET_PBEST and GET_GBEST. Based on the two best positions obtained, the position of this particle gets then an update by Eq. ( <ref type="formula" target="#formula_5">6</ref>). Next, the function U_COMBINATIONE showed in Step 2.2 is activated to improve the diversity of particles and seek for good solutions. A classifier based on the 1-NN is used to evaluate new particles, and the function EVALUATE returns the fitness values of these particles. Note that, whenever the position of a particle goes beyond its lower or upper bound, the particle will take the value of its corresponding lower or upper bound. Fig. <ref type="figure" target="#fig_1">2</ref> shows flowchart of the proposed algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Encoding</head><p>In general, most of the existing studies use a binary string to represent the particle, such as <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. In this binary string, if a bit is 1, the corresponding feature is chosen into a feature subset; if 0, it is not. Since the velocity generated by Eq. ( <ref type="formula">1</ref>) is a real number, these studies must adopt a function, i.e., the sigmoid function, to transform the real number into a binary number, 0 or 1.</p><p>Since not requiring the velocity item, in this paper a direct strategy of encoding is used. In this strategy, the probability of each feature being chosen in the feature subset is taken as an encoding element, and multiple elements comprise a particle which represents a candidate solution of the problem. Taking a data set with N features as an example, a particle is represented with a N-bit real string as follows:</p><formula xml:id="formula_6">X i ¼ ðx i;1 ; x i;2 ; …; x i;N Þ; i ¼ 1; 2; …; jSj ð<label>7Þ</label></formula><p>where jSj is the swarm size, and x i;j A ½0; 1 is the probability of the jth feature being chosen in the feature subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fitness evaluation</head><p>The purpose of feature selection is to find feature subsets which have stronger classification ability. The fitness is the scale for evaluating feature subsets which are denoted by particles. In this subsection, first, we introduce a method which is used to decode particles into feature subsets, then show the fitness function of particles.</p><p>Taking the particle X i as an example, we use the following equation to decide its corresponding feature subset Z i :</p><formula xml:id="formula_7">z i;j ¼ 1; x i;j Z rand 0 otherwise<label>ð8Þ</label></formula><p>where z i;j ¼ 1 represents that the jth feature is chosen into the feature subset Z i . The K-nearest neighbor (K-NN) method is one of the most popular nonparametric methods, which is first introduced by Fix and Hodges <ref type="bibr" target="#b21">[22]</ref>. Due to easy implementation and parameter-free, in this paper we adopt the leave-one-out cross-validation (LOOCV) of one nearest neighbor (1-NN) <ref type="bibr" target="#b12">[13]</ref> to evaluate feature subsets which are denoted by the particles. In this method, a single datum from the original data set is selected as a testing sample, and the  rest constitute training samples. Then the 1-NN classifier predicts the class of the testing sample by calculating and sorting the distances between the testing sample and the training ones. Note that in this paper we use the first normal form to calculate the distances among samples. This process is repeated so that each datum in the original data set is used once as the testing sample. Based on the above method, the proportion of correctly predicted samples to all the samples, i.e. the accuracy of classification, is defined as the fitness of a particle, as follows:</p><formula xml:id="formula_8">FðX i Þ ¼ No: of correctly predicted samples Size of the original data set Â 100%<label>ð9Þ</label></formula><p>In the binary BPSO, the function EVALUATE returns the fitness value of each particle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Update of Pbest and Gbest</head><p>Since the PSO-based methods use Pbest and Gbest to update the particles, the two best positions both play crucial role in our algorithm. This subsection introduces the update methods of Pbest and Gbest. Firstly, we define a phenomenon called the gene degradation.</p><p>Without loss of generality, we consider a particle X i ¼ ðx i;1 ; x i;2 ; …; x i;N Þ and suppose its second element x i;2 ¼ 0:1. According to the decoding equation ( <ref type="formula" target="#formula_7">8</ref>), obviously the survival probability of the second feature in the feature subset is 0.1. Furthermore, supposing that this feature has been chosen in the feature subset due to a smaller rand, namely Z i ¼ ðz i;1 ; 1; …; z i;N Þ, and the feature subset Z i has a good fitness. Based on the definition of Pbest, the position X i will be selected as new Pbest of this particle. According to the evolution essence of PSO, the survival probability of the second feature in the feature subset, i.e., x i;2 , should be enhanced in the next iteration of PSO. However, the value of x i;2 still keeps small with high probability after the swarm updates once by Eq. ( <ref type="formula" target="#formula_5">6</ref>), because the particles mistake the value x i;2 ¼ 0:1 for a good experience. We call this phenomena as the gene degradation.</p><p>To overcome this phenomenon, a reinforced memory strategy is proposed to update the Pbest of each particle. For a particle, supposing that its position at t þ 1 iterations is X i ðt þ 1Þ, its old Pbest is Pbest i ðtÞ, the following equation shows our proposed update strategy:</p><p>where jZðPbest i ðtÞÞj is the number of features in the feature subset which is denoted by Pbest i ðtÞ.</p><p>By the equation above, the features that can improve the particle's fitness will get high survival probability (more than 0.5) all along. For the particle X i ¼ ðx i;1 ; 0:1; …; x i;N Þ, the survival probability of its second feature in Pbest i ðt þ 1Þ becomes 0:5 Â ð0:1 þ 1Þ from 0.1 by using the equation above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Uniform combination</head><p>PSO is known to have a fast convergence speed. However, this convergence rate may decrease the population diversity rapidly and increase the probability of obtaining local optima. On the other hand, keeping a high population diversity all the time must degrade the convergence speed of an algorithm. In order to increase the diversity of swarm without compromising its convergence speed obviously, a new operator that combines the mutation and the crossover in evolutionary algorithms, called the uniform combination (UC), is incorporated into the binary BPSO.</p><p>Fig. <ref type="figure">3</ref> shows the pseudo-code of the uniform combination, where a combination probability p c set to control the combination speed. At each iteration, all the particles are checked in turn. If the value of p c is larger than the random number rand, then implement the uniform combination to the current particle. For the particle selected, we first choose a personal best position at random from the set of Pbests, note by Pbest k ; then choose U n elements from the particle at random, and replace them by the corresponding elements from Pbest k and a normally distributed random number Randn.</p><p>Analyzing the uniform combination, when Randn takes a small value, like the crossover operator in evolutionary algorithm, combining the particle and Pbest k can help the algorithm construct a good schemas rapidly; when Randn takes a big value, like the mutation operator in evolutionary algorithm, the combination operator can help the particles escape from local optima.</p><p>Moreover, an adaptive strategy is designed to modify the parameter p c based on the convergence degree of our algorithm, as follows:</p><formula xml:id="formula_9">p c ¼ 0:2 1 þ e 5 À num<label>ð11Þ</label></formula><p>where the parameter num is used to take count of the stagnation iterations of the swarm. At each iteration, if there is no improvement on the fitness of Gbest, then the parameter num increases by one; otherwise, reset num¼ 0. Fig. <ref type="figure" target="#fig_2">4</ref> shows the curve of p c with the increasing of num. By using the adaptive strategy above, the value of p c will increase from 0 to 0.2 with the increasing of num. In other words, the more the stagnation iterations of our algorithm is, the more the influence degree of the uniform combination on the particles is. This is reflected not only in the number of particles, but also the dimension of decision variables. As Fig. <ref type="figure" target="#fig_2">4</ref> shows, when num 4 13, all the particles will be reflected with the probability of 0.2. With the decreasing of num, the influence degree of the uniform combination will decrease, which is reflected by the fact that the combination probability decreases and the combination rang shrinks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental studies</head><p>This section validates the performance of the binary BPSO on several well-known real-world test problems, including Glass, Vowel, Wine, Vehicle, Segmentation, WDBC, Ionosphere and Sonar. These data sets have been the subject of many studies and cover the examples of small, medium and large dimensional data sets. The detailed description of these data sets is available in the UCI Repository <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison algorithms and parameter setting</head><p>Two GA-based algorithms and two PSO-based algorithms are selected for performance comparison. They are the simple GA algorithm (SGA) <ref type="bibr" target="#b4">[5]</ref>, the hybrid GA algorithm (including HGA (2) and HGA(3)) <ref type="bibr" target="#b4">[5]</ref>, the binary PSO algorithm (BPSO) and the CBPSO algorithm <ref type="bibr" target="#b10">[11]</ref>. In our experiments, the same conditions are</p><formula xml:id="formula_10">Pbest i ðt þ 1Þ ¼ 0:5ðX i ðt þ 1ÞþZ i ðt þ 1ÞÞ if FðPbest i ðtÞÞ o FðX i ðt þ 1ÞÞ 0:5ðX i ðt þ 1ÞþZ i ðt þ 1ÞÞ if FðPbest i ðtÞÞ ¼ FðX i ðt þ 1ÞÞ&amp;jZðPbest i ðtÞÞj Z jZ i ðt þ1ÞÞj Pbest i ðtÞ otherwise 8 &gt; &lt; &gt; :<label>ð10Þ</label></formula><p>used to compare the performance among the proposed algorithm and other meta-heuristic algorithms, i.e., the size of the population/swarm ¼20, and the 1-nearest neighbor method to evaluate feature subsets. Moreover, the maximum generation/iteration times for the SGA, the HGA, the binary PSO and the CBPSO take the same values as <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref>, and Table <ref type="table" target="#tab_1">1</ref> shows their setting in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison results</head><p>The obtained results of the binary BPSO on several benchmark data sets have been compared with that of different FS algorithms. Table <ref type="table" target="#tab_2">2</ref> shows experimental results of the binary BPSO and other five algorithms listed in Section 4.1. It can be seen in Table <ref type="table" target="#tab_2">2</ref> that, the results produced by the binary BPSO achieved the best average classification accuracies among all the six algorithms for five out of eight data sets. The average classification accuracies of Wine, Vehicle, Segmentation, WDBC and Ionosphere obtained by the binary BPSO are 99.45%, 75.54%, 98.24%, 98.36% and 96.21%, respectively. For the data set Glass, the binary BPSO, as well as the rest five FS algorithms, all obtained the maximal classification accuracy, 100%. For the data set Vowel, the binary BPSO obtained the maximal classification accuracy, 99.70%, as well as the two GAbased algorithms. For the data set Sonar, the binary BPSO obtained the second best classification accuracy, 99.08%, while the HGA (3) obtained the best solution.</p><p>In terms of the number of features d, the binary BPSO obtained the smallest d for Class, Vowel, Wine, Vehicle and WDBC. However, it is worth mentioning that, the purpose of feature selection is to move useless features without sacrificing the predictive accuracy; otherwise, the performance might be degraded though that feature subset has small size. For example, for the data set Sonor, the CBPSO got a subset consisting of the smallest features (its size¼26), but it achieved a lower classification accuracy. On the other hand, the binary BPSO selected a larger feature subset that provides better classification accuracy compared to the CBPSO for the same data set. Therefore, the results in Table <ref type="table" target="#tab_2">2</ref> indicate that the smallest or largest feature subset does no guarantee the best or worst classification accuracy.</p><p>To highlight the search progress of the binary BPSO for optimal solutions, we take the iteration times as the horizontal coordinate, and the accuracy and the number of features selected as the vertical coordinate. Fig. <ref type="figure" target="#fig_3">5</ref> illustrates the improvement process of Gbest as the iteration times increase for Vehicle and Ionosphere, Tables <ref type="table" target="#tab_3">3</ref> and<ref type="table" target="#tab_4">4</ref> show corresponding values of Gbest.</p><p>For the data set Vehicle, the binary BPSO achieved the best classification accuracy (76.5957%) within 40 iterations; the smallest feature subset (its size¼ 6) was found at the first iteration (Table <ref type="table" target="#tab_3">3</ref>), while the size of feature subset is 8 when the binary BPSO obtained the best classification accuracy. For the data set Ionosphere, a small feature subset was found at 15-16 iterations (its size¼12), but the classification accuracies within this stage are less than those accuracies within 17-30 iterations. These results indicate that reducing the number of selected features dose not always keep positive affect on classification accuracy. As long as the chosen features contain enough feature information, a higher classification accuracy can be achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Effect analysis of two new operators</head><p>The performance of the binary BPSO for feature selection problems can be seen in Table <ref type="table" target="#tab_2">2</ref>, but the effect of the two improved operators, i.e., the reinforced memory strategy and the  uniform combination, upon the feature selection process is not clear. Therefore, this experiment performs an extensive analysis about their effect. In this subsection, we evaluate the two   improved operators by removing each one at a time to examine its effects on the results of test. Table <ref type="table" target="#tab_5">5</ref> lists the statistical results of this experiment for Vehicle, WDBC, Ionosphere and Sonar over 20 independent runs. It can be seen from these results that the positive effect of the reinforced memory strategy is visible. For all the selected test data sets, the classification accuracies obtained by the full algorithm are obviously higher than that obtained by the algorithm without the reinforced memory strategy. For example, for Ionosphere, the classification accuracies of the full algorithm and the algorithm without the reinforced memory strategy are 96.21% and 95.69%, respectively. As subsection 3.3 mentioned, the degradation of outstanding genes can make the particles fly along wrong directions; while the use of the reinforced memory strategy can reduce the probability of occurring the gene degradation and enhance the search efficiency of the particles. Removing the uniform combination also decreased the classification accuracies for all the four data sets. For example, for Sonar, the classification accuracy of the algorithm without the uniform combination decreases 0.55% compared with the full algorithm. Similar improvement can also be found for the rest data sets. Overall, both two operators are beneficial to improving the performance of the proposed algorithm.</p><p>In addition, the differences among the number of selected features obtained by the three algorithms above are all small. For Ionosphere, both the algorithm without the reinforced memory strategy and that without the uniform combination keep more features into the optimal subset, but they have inferior classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Statistical analysis</head><p>To investigate the statistical robustness of the binary BPSO, three statistical tests, i.e., the Friedman test, the multiple comparison approach, and the Mann-Whitely U-test, are used to compare the average classification accuracies among the six FS algorithms. The first two methods have been used to discuss the performance of different algorithms in the literature <ref type="bibr" target="#b23">[24]</ref>.</p><p>The Friedman test is a non-parametric statistical test, and in this paper is used to test whether the accuracies of different classification algorithms are equal. The hypothesis being tested is that all the six algorithms have equal classification accuracy. To test the above hypothesis, first a rank matrix ðR i;j Þ nÂk is constructed based on the results in Table <ref type="table" target="#tab_2">2</ref>, where R i;j represents the rank (from 1 to k) assigned to the jth algorithm on the ith test data set. Herein, if an algorithm obtained the lowest classification accuracy, then its rank is assigned with 1. Taking the data set Wine as an example, its rank values for six algorithms are 2,2,2,4,5,6. Based on the matrix ðR i;j Þ nÂk , we have the value T f ¼4.92 in this paper.</p><p>The 1 À 0.05 quantile of F-distribution with k À 1 and ðk À 1Þðn À 1Þ freedom degrees is 2.49 by querying the quantile table of Fdistribution, where k ¼ 6; n ¼ 8. Since T f ¼4.92 is larger than F 0:05 ð5; 35Þ ¼ 2:49, the null hypothesis that all algorithms have the same classification accuracy is rejected at 0.95 significance level.</p><p>The multiple comparison approach is used to determine which algorithm has significantly different accuracy if the Friedman test is rejected. Like the Friedman test, the classification accuracies of six algorithms are ordered in an array, and a rank is assigned to each corresponding value according to its order. Then the rank sums of the binary BPSO, CBPSO, binary PSO, HGA(3), HGA(2) and SGA are 43, 32, 24, 26, 25 and 18, respectively. According to the definition of the multiple comparison approach, if the ranks of two algorithms are larger than a stated unit apart, they might be regarded as having unequal accuracy of prediction. In this paper, the stated units at 0.05 significance level is 10.97. Obviously, it can be concluded that the binary BPSO is statistically superior to the rest algorithms for the tested data sets.</p><p>Moreover, in this paper the Mann-Whitely U-test is used to highlight our algorithm's efficiency over the nearest competitor CBPSO. The Mann-Whitney U test is a non-parametric statistical hypothesis test for assessing whether one of the two samples tends to have larger values than the other <ref type="bibr" target="#b30">[31]</ref>. For each test data set, in this paper, we suppose that the solutions obtained by the CBPSO form sample A, and the solutions obtained by the proposed algorithm form sample B. The hypothesis being tested is that the proposed algorithm has the same average performance as the CBPSO, i.e., the two samples obtained by the two algorithms come from identical distribution. Based on the method in <ref type="bibr" target="#b30">[31]</ref>, we have the U statistical values of the eight data sets as follows: U(Class)¼ 200, U (Vowel)¼ 0, U(Wine)¼ 190, U(Vehicle)¼121.5, U(Segmentation)¼30, U(WDBC)¼0, U(Ionosphere)¼35.5, U(Sonar)¼80; while the critical value for U at 0.05 significance level with n1¼n2¼20 is 127 by consulting the Tables of Critical Values for the Mann-Whitney U-test. We can find that except the data sets Class and Wine, the U statistical values of the rest data sets all are less than the critical value. Therefore, there is evidence to reject the above null hypothesis for the data sets Vowel, Vehicle, Segmentation, WDBC, Ionosphere and Sonar. In other words, the binary BPSO is markedly superior to the CBPSO for the six data sets above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, a new PSO algorithm, called the binary BPSO algorithm, is proposed and successfully applied to solve feature selection problems. This algorithm extends the idea of BPSO to feature selection problems with binary variables. By compared with those existing results obtained by five established algorithms such as HGA and CBPSO, numerical experiences show that the proposed algorithm achieved the best average classification accuracies for seven out of eight data sets. Furthermore, the results of three statistical tests exhibit the statistical robustness of the proposed algorithm. Therefore, it seems that the proposed algorithm is a viable alternative for solving feature selection problems. In the future, we plan to apply the binary BPSO to some other feature selection problems or complicated problems in other areas. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Pseudo-code of the binary BPSO algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Flowchart of the proposed algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Curve of p c with the increasing of num.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.<ref type="bibr" target="#b4">5</ref>. Iteration times versus the accuracy and the number of features selected for Vehicle (the up one) and Ionosphere (the down one).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Setting of the generation/iteration times for the selected algorithms.</figDesc><table><row><cell>Data sets</cell><cell>SGA</cell><cell cols="5">HGA(2) HGA(3) Binary PSO CBPSO Binary BPSO</cell></row><row><cell>Glass</cell><cell cols="2">10,000 200</cell><cell>120</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Vowel</cell><cell cols="2">10,000 200</cell><cell>120</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Wine</cell><cell cols="2">10,000 200</cell><cell>120</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Vehicle</cell><cell cols="2">10,000 200</cell><cell>120</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Seg.</cell><cell cols="2">10,000 200</cell><cell>120</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>WDBC</cell><cell cols="2">20,000 200</cell><cell>120</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell cols="3">Ionosphere 25,000 200</cell><cell>120</cell><cell>100</cell><cell>100</cell><cell>100</cell></row><row><cell>Sonar</cell><cell cols="2">40,000 200</cell><cell>120</cell><cell>100</cell><cell>100</cell><cell>100</cell></row></table><note><p>Y. Zhang et al. / Neurocomputing ∎ (∎∎∎∎) ∎∎∎-∎∎∎</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Classification accuracies obtained by different algorithms for the tested data sets.</figDesc><table><row><cell>Data sets</cell><cell>d</cell><cell>SGA</cell><cell>HGA(2)</cell><cell>HGA(3)</cell><cell>Binary PSO</cell><cell></cell><cell>CBPSO</cell><cell></cell><cell>Binary BPSO</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>d</cell><cell>A(%)</cell><cell>d</cell><cell>A(%)</cell><cell>d</cell><cell>A(%)</cell></row><row><cell>Class</cell><cell>2</cell><cell>99.07</cell><cell>99.07</cell><cell>99.07</cell><cell>3</cell><cell>100</cell><cell>3</cell><cell>100</cell><cell>3</cell><cell>100</cell></row><row><cell></cell><cell>4</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vowel</cell><cell>2</cell><cell>62.02</cell><cell>62.02</cell><cell>NA</cell><cell>9</cell><cell>99.49</cell><cell>9</cell><cell>99.49</cell><cell>8</cell><cell>99.7</cell></row><row><cell></cell><cell>4</cell><cell>92.83</cell><cell>92.83</cell><cell>92.83</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>6</cell><cell>98.79</cell><cell>98.79</cell><cell>98.79</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>8</cell><cell>99.7</cell><cell>99.7</cell><cell>99.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Wine</cell><cell>3</cell><cell>93.82</cell><cell>93.82</cell><cell>93.82</cell><cell>8</cell><cell>98.88</cell><cell>8</cell><cell>99.44</cell><cell>7.5</cell><cell>99.45</cell></row><row><cell></cell><cell>5</cell><cell>95.51</cell><cell>95.51</cell><cell>95.51</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>8</cell><cell>95.51</cell><cell>95.51</cell><cell>95.51</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>92.7</cell><cell>92.7</cell><cell>92.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vehicle</cell><cell>4</cell><cell>69.5</cell><cell>69.74</cell><cell>69.62</cell><cell>11</cell><cell>74.7</cell><cell>12</cell><cell>75.06</cell><cell>6.8</cell><cell>75.54</cell></row><row><cell></cell><cell>7</cell><cell>72.97</cell><cell>73.52</cell><cell>73.52</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>11</cell><cell>71.84</cell><cell>72.46</cell><cell>72.46</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>14</cell><cell>70.8</cell><cell>70.8</cell><cell>70.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Segmentation</cell><cell>4</cell><cell>92.81</cell><cell>92.81</cell><cell>92.81</cell><cell>11</cell><cell>97.88</cell><cell>10</cell><cell>97.92</cell><cell>11.2</cell><cell>98.24</cell></row><row><cell></cell><cell>8</cell><cell>92.95</cell><cell>92.95</cell><cell>92.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>11</cell><cell>92.95</cell><cell>92.95</cell><cell>92.95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>15</cell><cell>92.57</cell><cell>92.57</cell><cell>92.57</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WDBC</cell><cell>6</cell><cell>93.67</cell><cell>94.38</cell><cell>93.99</cell><cell>13</cell><cell>97.72</cell><cell>15</cell><cell>97.54</cell><cell>12.9</cell><cell>98.36</cell></row><row><cell></cell><cell>12</cell><cell>93.95</cell><cell>94.06</cell><cell>94.06</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>18</cell><cell>93.85</cell><cell>93.99</cell><cell>94.13</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>24</cell><cell>93.85</cell><cell>93.85</cell><cell>93.85</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ionosphere</cell><cell>7</cell><cell>94.7</cell><cell>95.5</cell><cell>95.56</cell><cell>10</cell><cell>93.73</cell><cell>15</cell><cell>96.02</cell><cell>11.4</cell><cell>96.21</cell></row><row><cell></cell><cell>14</cell><cell>94.3</cell><cell>95.56</cell><cell>95.21</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20</cell><cell>93.97</cell><cell>94.19</cell><cell>93.73</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>27</cell><cell>91.17</cell><cell>91.45</cell><cell>91.45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sonar</cell><cell>12</cell><cell>92.4</cell><cell>94.71</cell><cell>94.61</cell><cell>32</cell><cell>92.79</cell><cell>26</cell><cell>95.57</cell><cell>28.2</cell><cell>96.08</cell></row><row><cell></cell><cell>24</cell><cell>95.49</cell><cell>95.96</cell><cell>96.34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>36</cell><cell>95.09</cell><cell>95.82</cell><cell>95.67</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>48</cell><cell>92.02</cell><cell>93.17</cell><cell>93.17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Evolution process of the binary BPSO for Vehicle.</figDesc><table><row><cell>Iterations</cell><cell>Best solution</cell><cell>Fitness value</cell><cell>No. of features</cell></row><row><cell></cell><cell></cell><cell></cell><cell>selected</cell></row><row><cell>1-2</cell><cell>1 5 9 10 11 14</cell><cell>67.0213</cell><cell>6</cell></row><row><cell>3</cell><cell>2 3 4 7 9 12 14 15</cell><cell>68.0851</cell><cell>9</cell></row><row><cell>4</cell><cell>2 3 5 6 7 10 14 15</cell><cell>68.0851</cell><cell>8</cell></row><row><cell>5</cell><cell>2 3 5 6 7 9 10 14 15</cell><cell>69.1489</cell><cell>9</cell></row><row><cell>6</cell><cell>1 2 3 4 5 6 7 10 14 15</cell><cell>70.2128</cell><cell>10</cell></row><row><cell>7-9</cell><cell>1 2 3 4 5 6 7 10 14 15 18</cell><cell>71.2766</cell><cell>11</cell></row><row><cell>10-11</cell><cell>1 3 4 5 6 9 11 14 16</cell><cell>73.4043</cell><cell>9</cell></row><row><cell>12-40</cell><cell>1 2 3 5 6 9 10 11 14 15 16</cell><cell>74.4681</cell><cell>11</cell></row><row><cell>41-100</cell><cell>1 2 3 5 9 10 11 14</cell><cell>76.5957</cell><cell>8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Evolution process of the binary BPSO for Ionosphere.</figDesc><table><row><cell cols="2">Iterations Best solution</cell><cell>Fitness</cell><cell>No. of features</cell></row><row><cell></cell><cell></cell><cell>value</cell><cell>selected</cell></row><row><cell>1</cell><cell>3 5 10 12 13 14 15 16 20 23 24 26 30</cell><cell>93.1624</cell><cell>14</cell></row><row><cell></cell><cell>33</cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>1 2 5 6 9 11 14 19 20 21 23 24 30 33</cell><cell>93.4473</cell><cell>15</cell></row><row><cell></cell><cell>34</cell><cell></cell><cell></cell></row><row><cell>3-5</cell><cell>1 5 6 9 11 14 19 20 21 23 24 30 33</cell><cell>93.4473</cell><cell>14</cell></row><row><cell></cell><cell>34</cell><cell></cell><cell></cell></row><row><cell>6-8</cell><cell>1 2 4 5 6 7 8 14 15 16 17 19 24 27 31</cell><cell>94.302</cell><cell>17</cell></row><row><cell></cell><cell>33 34</cell><cell></cell><cell></cell></row><row><cell>9-14</cell><cell cols="2">2 4 5 6 8 14 15 16 19 24 25 27 34 94.8718</cell><cell>14</cell></row><row><cell>15-16</cell><cell>1 4 5 6 14 16 19 23 24 25 27 34</cell><cell>95.1567</cell><cell>12</cell></row><row><cell>17-23</cell><cell cols="2">1 2 4 5 6 8 13 14 16 19 20 23 24 25 95.4416</cell><cell>14</cell></row><row><cell>24</cell><cell>1 2 4 5 6 8 13 14 15 16 19 20 23 24</cell><cell>95.7265</cell><cell>15</cell></row><row><cell></cell><cell>25</cell><cell></cell><cell></cell></row><row><cell>25-30</cell><cell cols="2">2 4 5 6 8 11 14 151 16 19 23 24 25 27 95.7265</cell><cell>14</cell></row><row><cell>31-60</cell><cell>2 4 5 6 14 15 16 19 23 24 25 33</cell><cell>96.2963</cell><cell>12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Statistical results obtained by the full algorithm and other test algorithms.</figDesc><table><row><cell>Data sets</cell><cell>No. reinforced</cell><cell>No. uniform</cell><cell>The full</cell></row><row><cell></cell><cell>memory</cell><cell>combination</cell><cell>algorithm</cell></row><row><cell cols="2">Classification accuracy(%)</cell><cell></cell><cell></cell></row><row><cell>Vehicle</cell><cell>75.53</cell><cell>75.32</cell><cell>75.54</cell></row><row><cell>WDBC</cell><cell>97.91</cell><cell>98.15</cell><cell>98.36</cell></row><row><cell cols="2">Ionosphere 95.69</cell><cell>96.21</cell><cell>96.21</cell></row><row><cell>Sonar</cell><cell>93.41</cell><cell>95.53</cell><cell>96.08</cell></row><row><cell cols="2">No. of selected features</cell><cell></cell><cell></cell></row><row><cell>Vehicle</cell><cell>7.2</cell><cell>6.8</cell><cell>6.80</cell></row><row><cell>WDBC</cell><cell>13.6</cell><cell>12.6</cell><cell>12.9</cell></row><row><cell cols="2">Ionosphere 12.1</cell><cell>12.0</cell><cell>11.4</cell></row><row><cell>Sonar</cell><cell>29.9</cell><cell>27.0</cell><cell>28.2</cell></row></table><note><p>and the China Postdoctoral Science Foundation funded project (No. 2012M521142).</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Y. Zhang et al. / Neurocomputing ∎ (∎∎∎∎) ∎∎∎-∎∎∎</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Please cite this article as: Y. Zhang, et al., Feature selection algorithm based on bare bones particle swarm optimization, Neurocomputing (2014), http://dx.doi.org/10.1016/j.neucom.2012.09.049i</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was jointly supported by the Fundamental Research Funds for the Central Universities (No. 2013QNA51), the Natural Science Foundation of Jiangsu province (No. BK2011215),</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Applying electromagnetism-like mechanism for feature selection</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="972" to="986" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive particle swarm optimizer for feature selection</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gilles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yahya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Intelligent Data Engineering and Automated Learning</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>the Eleventh International Conference on Intelligent Data Engineering and Automated Learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6283</biblScope>
			<biblScope unit="page" from="226" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast branch and bound algorithms for optimal feature selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Somol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pudil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="900" to="912" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimization-based feature selection with adaptive instance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Olafsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3088" to="3106" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hybrid genetic algorithms for feature selection</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1424" to="1437" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ACO-based hybrid classification system with feature subset selection and model parameters optimization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="438" to="448" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parameter determination of support vector machine and feature selection using simulated annealing approach</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1505" to="1512" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Neural Networks</title>
		<meeting>IEEE International Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1942" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feature selection based onrough sets and particle swarm optimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="459" to="471" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Comparison of population based metaheuristics for feature selection: application to microarray data classification</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Talbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jourdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garcia-Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AICCSA</title>
		<meeting>AICCSA</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Chaotic maps based on binary particle swarm optimization for feature selection</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="239" to="248" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A discrete particle swarm optimization method for feature selection in binary classification problems</title>
		<author>
			<persName><forename type="first">U</forename><surname>Alper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="page" from="528" to="539" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gene selection and classification using Taguchi chaotic binary particle swarm optimization</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="13367" to="13377" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-objective particle swarm optimization with time variant inertia and acceleration coefficients</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="5033" to="5049" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bare bones particle swarms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 2003 IEEE Swarm Intelligence Symposium</title>
		<meeting>eeding of the 2003 IEEE Swarm Intelligence Symposium</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="80" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Barebones particle swarm for integer programming problems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G H</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Engelbrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 2007 IEEE Swarm Intelligence Symposium</title>
		<meeting>eeding of the 2007 IEEE Swarm Intelligence Symposium</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="170" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Novel bare bones particle swarm optimization and its performance for modeling vapor-liquid equilibrium data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Rangaiah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fluid Phase Equilib</title>
		<imprint>
			<biblScope unit="volume">301</biblScope>
			<biblScope unit="page" from="33" to="45" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Barebones multi-objective particle swarm optimization for environmental/economic dispatch</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">192</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="213" to="227" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A modied particle swarm optimizer</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of IEEE Congress on Evolutionary Computation</title>
		<meeting>eeding of IEEE Congress on Evolutionary Computation</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="303" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A discrete binary version of the particle swarm algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1997 Conference Systems Man and Cybernetics</title>
		<meeting>1997 Conference Systems Man and Cybernetics</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="4104" to="4108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An analysis of bare bones particle swarm</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 2008 IEEE Swarm Intelligence Symposium</title>
		<meeting>eeding of the 2008 IEEE Swarm Intelligence Symposium</meeting>
		<imprint>
			<date type="published" when="2008-09">September 2008</date>
			<biblScope unit="page" from="21" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discriminatory Analysis-nonparametric Discrimination: Consistency Properties</title>
		<author>
			<persName><forename type="first">E</forename><surname>Fix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hodges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Air Force School of Aviation Medicine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="261" to="279" />
			<date type="published" when="1951">1951</date>
			<publisher>Randolph Field</publisher>
		</imprint>
	</monogr>
	<note>Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">UCI Repository of Machine Learning Databases</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Aha</surname></persName>
		</author>
		<ptr target="〈http://www.ics.uci.edu/$mlearn/$MLRepository.html〉" />
		<imprint>
			<pubPlace>Irvine, California</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Information and Computer Science, University of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved binary particle swarm optimization using catfish effect for feature selection</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="12699" to="12707" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The particle swarm-explosion, stability, and convergence in a multidimensional complex space</title>
		<author>
			<persName><forename type="first">M</forename><surname>Clerc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="72" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A study of particle swarm optimization particle trajectories</title>
		<author>
			<persName><forename type="first">F</forename><surname>Van Den Bergh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Engelbrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="937" to="971" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MO-TRIBES, an adaptive multiobjective particle swarm optimization algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cooren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Clerc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Siarry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Optim. Appl</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="379" to="400" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A discrete particle swarm optimization algorithm for the no-wait flowshop scheduling problem</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">K</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Tasgetiren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2807" to="2839" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">No-idle permutation flow shop scheduling based on a hybrid discrete particle swarm optimization algorithm</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">K</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Adv. Manuf. Technol</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="796" to="807" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optimizing the vehicle routing problem with time windows: a discrete particle swarm optimization approach</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man Cybern. C: Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="254" to="267" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On a test of whether one of two random variables is stochastically larger than the other</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Whitney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yong Zhang received the B.S. and Ph.D. degrees in Control theory and control Engineering from China University of Mining and Technology in 2006 and 2009, respectively. He is currently with the School of Information and Electronic Engineering</title>
		<imprint>
			<date type="published" when="1947">1947</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="50" to="60" />
		</imprint>
		<respStmt>
			<orgName>China University of Mining and Technology</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include intelligence optimization and data mining</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
