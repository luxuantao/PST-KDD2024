<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Distillation from the Last Mini-Batch for Consistency Regularization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yiqing</forename><surname>Shen</surname></persName>
							<email>shenyq@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liwu</forename><surname>Xu</surname></persName>
							<email>xuliwu@oppo.com</email>
							<affiliation key="aff1">
								<orgName type="institution">OPPO Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuzhe</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">OPPO Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yaqian</forename><surname>Li</surname></persName>
							<email>liyaqian@oppo.com</email>
							<affiliation key="aff1">
								<orgName type="institution">OPPO Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yandong</forename><surname>Guo</surname></persName>
							<email>yandong.guo@live.com</email>
							<affiliation key="aff1">
								<orgName type="institution">OPPO Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">OPPO Research Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Distillation from the Last Mini-Batch for Consistency Regularization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge distillation (KD) shows a bright promise as a powerful regularization strategy to boost generalization ability by leveraging learned sample-level soft targets. Yet, employing a complex pre-trained teacher network or an ensemble of peer students in existing KD is both timeconsuming and computationally costly. Various self KD methods have been proposed to achieve higher distillation efficiency. However, they either require extra network architecture modification or are difficult to parallelize. To cope with these challenges, we propose an efficient and reliable self-distillation framework, named Self-Distillation from Last Mini-Batch (DLB). Specifically, we rearrange the sequential sampling by constraining half of each mini-batch coinciding with the previous iteration. Meanwhile, the rest half will coincide with the upcoming iteration. Afterwards, the former half mini-batch distills on-the-fly soft targets generated in the previous iteration. Our proposed mechanism guides the training stability and consistency, resulting in robustness to label noise. Moreover, our method is easy to implement, without taking up extra run-time memory or requiring model structure modification. Experimental results on three classification benchmarks illustrate that our approach can consistently outperform state-of-the-art self-distillation approaches with different network architectures. Additionally, our method shows strong compatibility with augmentation strategies by gaining additional performance improvement. The code is available at https: //github.com/Meta-knowledge-Lab/DLB.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Knowledge Distillation (KD), first introduced by Bucilua et al. <ref type="bibr" target="#b1">[2]</ref>, was later popularized by Hinton et al. <ref type="bibr" target="#b9">[10]</ref>. Numerous previous researches have demonstrated the success of KD in various learning tasks to boost the generalization ability. For example, in the case of network compression, the two-stage offline KD is widely used to transfer dark knowledge from a cumbersome pre-trained model to a light student model that learns from teachers' intermediate feature maps <ref type="bibr" target="#b20">[21]</ref>, logits <ref type="bibr" target="#b9">[10]</ref>, attention maps <ref type="bibr" target="#b39">[40]</ref>, or auxiliary outputs <ref type="bibr" target="#b42">[43]</ref>. However, training a high-capacity teacher network heavily relies on large computational sources and run-in memory. To alleviate the time-consuming preparation of static teachers, online distillation is introduced <ref type="bibr" target="#b43">[44]</ref>, where an ensemble of peer students learns mutually from each other. Online KD achieves equivalent performance improvement, compared with offline KD, with higher computational efficiency. Thus, this line is subsequently extended by many following works to a more capable self-ensemble teacher <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref>. Other applications of KD include semisupervised learning, domain adaptation, transfer learning and etc <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref>. The main scope of this paper focuses on the KD paradigm itself.</p><p>Conventional KD approaches, both online and offline, have achieved satisfying empirical performance <ref type="bibr" target="#b23">[24]</ref>. Yet, existing KD approaches suffer from an obstacle in the low knowledge transferring efficiency <ref type="bibr" target="#b34">[35]</ref>. Additionally, high computation and run-in memory costs restrict their deployment onto the end devices, such as mobile phones, digital cameras <ref type="bibr" target="#b3">[4]</ref>. To cope with these limitations, self-knowledge distillation has received increasing popularity, which enables a student model to distill knowledge from itself. The absence of a complex pre-trained teacher and an ensemble of peer students in self-KD contributes to a marginal improvement in the training efficiency.</p><p>One popular formulation of self KD, such as Be Your Own Teacher (BYOT), requires heavy network architecture modifications, which largely increases their difficulty to generalize onto various network structures <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43]</ref>. In another line, history information, including previous training logits or model snapshots, is utilized to construct a virtual teacher for extra supervision signals as self distillation. Initially, born again networks (BAN) sequentially distill networks with identical parameters as its last generation <ref type="bibr" target="#b6">[7]</ref>. An advancement achieved by snapshot distillation is to take the secondary information from the prior mini-generation i.e. a couple of epochs, within one generation <ref type="bibr" target="#b35">[36]</ref>. This virtual teacher update frequency is further Table <ref type="table">1</ref>. A comparison with the state-of-the-arts in terms of computational cost and smoothed granularity. We compare our method with label smoothing regularization <ref type="bibr" target="#b26">[27]</ref>, teacher-free knowledge distillation (Tf-KD sel f , Tf-KD reg ) <ref type="bibr" target="#b36">[37]</ref>, class-wise self-knowledge distillation (CS-KD) <ref type="bibr" target="#b38">[39]</ref>, progressive self-knowledge distillation (PS-KD) <ref type="bibr" target="#b11">[12]</ref>, memory-replay knowledge distillation (Mr-KD) <ref type="bibr" target="#b29">[30]</ref>, data-distortion guided self-knowledge distillation (DDGSD) <ref type="bibr" target="#b34">[35]</ref>, be your own teacher (BYOT) <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Characteristic</head><p>LSR Tf-KD sel f Tf-KD reg CS-KD PS-KD Mr-KD DDGSD BYOT Ours Sample-level smoothing</p><formula xml:id="formula_0">? ? ? ? ? ? ? ? ? No pre-trained teacher ? ? ? ? ? ? ? ? ? No Architecture modification ? ? ? ? ? ? ? ? ? Forward times per batch 1 2 1 2 2 ? 2 2 1 1 Backward times per batch 1 1 1 2 1 1 2 ?2 1 Number of involved networks 1 2 1 1 2 1 1 1 1 Label update frequency - epoch - - epoch epoch - - batch</formula><p>improved to epoch-level in progressive self-knowledge distillation <ref type="bibr" target="#b11">[12]</ref> and learning with retrospection <ref type="bibr" target="#b4">[5]</ref>. Yet, existing self KD methods have the following setbacks to be tackled. Firstly, the most instant information from the last iteration is discarded. Moreover, storing a snapshot of past models consumes an extra run-in memory cost, and subsequently increases the difficulty to parallelize <ref type="bibr" target="#b35">[36]</ref>. Finally, computation of the gradient in each time backward prorogation is associated with twice the forward process on each batch of data, resulting in a computational redundancy and low computational efficiency.</p><p>To address these challenges in existing self KD methods, we propose a simple but efficient self distillation approach, named as Self-Distillation from Last Mini-Batch (DLB). Compared with existing self KD approaches, DLB is computationally efficient and saves the run-in memory by only storing the soft targets generated in the last minibatch backup, resulting in its simplicity in deployment and parallelization. Every forward process of data instances is associated with a once backpropagation process, mitigating the computational redundancy. Major differences compared with the state-of-the-arts are summarized in Table <ref type="table">1</ref>. DLB produces on-the-fly sample-level smoothed labels for selfdistillation. Leveraging the soft predictions from the last iteration, our method provides the most instant distillation for each training sample. The success of DLB is attributed to the distillation from the most immediate historically generated soft targets to impose training consistency and stability. To be more specific, the target network plays a dual role as teacher and student in each mini-batch during the training stage. As a teacher, it provides soft targets to regularize itself in the next iteration. As a student, it distills smoothed labels generated from the last iteration and minimizes the supervision learning objective e.g. the cross-entropy loss.</p><p>We empirically illustrate the comprehensive effectiveness of our methods on three benchmark datasets, namely CIFAR-10 <ref type="bibr" target="#b12">[13]</ref>, CIFAR-100 <ref type="bibr" target="#b12">[13]</ref>, TinyImageNet. Our approach is both task-agnostic and model-agnostic i.e. with no requirement of model topological modifications. We select six representative backbone CNNs for evaluations, including ResNet-18, ResNet-110 <ref type="bibr" target="#b8">[9]</ref>, VGG-16, VGG-19 <ref type="bibr" target="#b24">[25]</ref>, DenseNet <ref type="bibr" target="#b10">[11]</ref>, WideResNet <ref type="bibr" target="#b40">[41]</ref>. Experimental results demonstrate that our DLB can consistently improve the generalization ability. We also test the robustness of DLB on corrupted data. Training consistency and stability imposed by DLB on corrupted data results in higher generalization ability.</p><p>The major contributions are three-fold.</p><p>? We propose a simple but efficient consistency regularization scheme based on self-knowledge distillation, named as DLB. With no network architecture modifications, our method requires very few additional computation costs as well as the run-time memory to implement. Utilizing the latest update from the last iteration, our DLB is easy to implement for parallelization. Notably, the proposed method is also both modelagnostic and task-agnostic.</p><p>? Comprehensive experimental results on three popular classification benchmarks illustrate consistent generalization improvements on different models. We also empirically demonstrate the compatibility of DLB with various augmentation strategies.</p><p>? We systemically analyze the impact of our method on the training dynamics. Concretely, the success of its regularization effect is attributed to guidance towards training consistency, by leveraging on-the-fly samplelevel smooth labels. The consistency effect is further amplified under label corruption settings, showing strong robustness to label noise. These empirical finds may shed light on a new direction to understand the effect of knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Knowledge distillations. Knowledge distillation (KD) targets to transfer 'knowledge', such as logits, or intermediate feature maps, from a high-capability teacher model Figure <ref type="figure">1</ref>. The overall architecture of our DLB. We write B t , ? t , ? t for a mini-batch of data samples, random augmentation and the trainable parameters indexed in the t th iteration.</p><p>to a lightweight student network <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>. Despite its competitive performance improvement to generalization, pretraining a complex teacher model requires extra training time and computational cost. Another way to form an economic distillation is called mutual learning, also known as online distillation, where an ensemble of students learn mutually from each other <ref type="bibr" target="#b43">[44]</ref>. This idea was popularized by many following works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref>. But, the optimization in peer learning involves multiple numbers of networks, which requires extra memory to store all parameters.</p><p>Self Knowledge Distillation. To enhance efficiency and effectiveness in knowledge transferring, self knowledge distillation (SKD) is proposed to utilize knowledge from itself, without the involvement of an extra networks <ref type="bibr" target="#b30">[31]</ref>. There are three popular ways to construct a SKD model i.e., 1) data distortion based self-distillation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b34">35]</ref>, 2) use history information as a virtual teacher, 3) distilling across auxiliary heads <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">43]</ref>. However, the first one marginally relies on the augmentation efficiency. The second one misses the latest update from the last mini-batch. And the last kind requires heavy network architecture modifications, which increase its difficulty for deployment.</p><p>Distillations as regularization. KD is extensively used in many tasks, such as model compression, semi-supervised learning, domain adaptation and etc <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref>. However, theoretical analysis of the success of KD remains a big challenge. Recently, Yuan et al. attributed the success of KD to its regularization effect as providing sample-level soft targets from the LSR perspective <ref type="bibr" target="#b36">[37]</ref>. It reveals the large promise of applying KD to the regularization domain.</p><p>In this line, class-wise self-knowledge distillation (CS-KD) is designed by evacuating a consistency between predic-tions of two batches of samples identified with the same category <ref type="bibr" target="#b38">[39]</ref>. Progressive self-knowledge distillation (PS-KD), more similar to our work, progressively distills past knowledge from the last epoch to soften hard targets in the current epoch <ref type="bibr" target="#b11">[12]</ref>. Memory-replay knowledge distillation (Mr-KD) extends PS-KD by storing a series of abandoned network backups for distillation <ref type="bibr" target="#b29">[30]</ref>. However, implementing PS-KD or Mr-KD both requires extra GPU memory to store the historical model parameters or the whole past predictions on the disk. The previous strategy is computationally costly for large models such as deep WRN <ref type="bibr" target="#b40">[41]</ref>, while the latter one is inefficient in training large datasets such as ImageNet <ref type="bibr" target="#b22">[23]</ref>. The above-mentioned drawbacks result in low training efficiency, as well as the implementation difficulty on end devices such as mobile phones, digital cameras, etc <ref type="bibr" target="#b3">[4]</ref>, which restrict their applications for regularization. On the other hand, much recent information from the last several mini-batches is absent in these methods. To cope with these shortages, we propose a novel selfdistillation framework named DLB, which will be elaborated on in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary</head><p>In this work, we focus on a supervised classification task as case study. For a clear notation, we write a K-classes labelled dataset as D = {(x i , y i )} N i=1 , where N is the total number of training instances. In every mini-batch, a batch of n samples B = {(x i , y i )} n i=1 ? D is augmented by data distortion ? to derive the distorted images B ? = {(?(x i ), y i )} n i=1 . Afterwards, they are fed into target neural network h ? to optimize the cross-entropy loss function, defined as follows</p><formula xml:id="formula_1">L CE = 1 n n i=1 H y i , p i .<label>(1)</label></formula><p>Formally, the predictive distribution</p><formula xml:id="formula_2">p i = (p i (1), ? ? ? , p i (K)) in a softmax classifier for class k ? [K] is formulated as p i (k) = exp( f k (x i ; ?)/?) K j=1 exp( f j (x i ; ?)/?) ,<label>(2)</label></formula><p>where f k writes for the k th component of the logits from the backbone encoder parameterized by ?. Temperature ? is usually set to 1 in Eq. ( <ref type="formula" target="#formula_2">2</ref>). To improve the generalization ability, vanilla knowledge distillation <ref type="bibr" target="#b9">[10]</ref> transfers pre-trained teacher's knowledge by optimizing an additional Kullback-Leibler (KL) divergence loss between the softened outputs from teacher and student in every minibatch i.e.</p><formula xml:id="formula_3">L KD = 1 n n i=1 ? 2 ? D KL p ? i ?p ? i ,<label>(3)</label></formula><p>where p ? i , p ? i are the soften predictions, parameterized by temperature ?, from student and teacher respectively. A higher temperature results in a more uniform distribution, leading to a similar regularization effect as label smoothing <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37]</ref>. Compared with previous works where a complex network is pre-trained to generate p ? i , our work uses historic information from the last batch to efficiently generate p ? i as a more instant smoothed labels for regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Self-Distillation from Last Batch</head><p>The overall training process for the proposed selfdistillation is visualized in Figure <ref type="figure">1</ref>. Instead of adopting a complex pre-trained teacher model to provide samplewise smoothed labels, our proposed framework utilizes the backup information from the last mini-batch to generate soft targets. It results in a regularization towards training consistency. For a clear notation, we denote the original batch of data sampled in the t th iteration as B t = {(x t i , y t i )} n i=1 , and the network parameters as ? t . Formally, we substitute the p ? i in Eq. ( <ref type="formula" target="#formula_3">3</ref>) by the soften labels p ?,t-1 i generated by the identical network at t -1 th iteration i.e. f parameterized by ? t-1 . Consequently, we introduce an extra last-batch consistency regularization loss to DLB as follows:</p><formula xml:id="formula_4">L LB = 1 n n i=1 ? 2 ? D KL p ?,t-1 i ?p ?,t i . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>Rather than storing the whole ? t-1 in the t th iterations as designed in previous works <ref type="bibr" target="#b11">[12]</ref>, which is run-time memory consuming, we complete the computation of all p ?,t- </p><formula xml:id="formula_6">L = L CE + ? ? L LB , (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where ? is the coefficient to balance two terms. In short, we constrain half of each mini-batch coinciding with the previous iteration, and the rest half with the upcoming iteration. Afterwards, the former half mini-batch distills onthe-fly soft targets generated in the previous iteration. The overall training process is summarized in Algorithm 1.</p><p>Algorithm 1 Pseudo code for DLB.</p><p>Input: balancing coefficient ? Input: distillation temperature ? Require: data loader samples batches as in Figure <ref type="figure">1</ref> 1: last logits = None # initialization 2: for (x,gt labels) in data loader do Datasets. We employed three multi-class classification benchmark datasets for comprehensive performance evaluations. The CIFAR-10/CIFAR-100 contain a total number of 60,000 RGB natural images of 32 ? 32 pixels from 10/100 classes <ref type="bibr" target="#b12">[13]</ref>. Each class includes 5,000/500 training samples and 1,000/100 testing samples. We followed the widely-used pre-processing from previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">41]</ref>. More precisely, training samples were normalized by deviation and padded 4 pixels with zero-value on each side. A random crop of 32 ? 32 region was generated from the padded image or its horizontal flip. The TinyImageNet is a subset of ILSVRC-2012, made up of 200 classes. Each class includes 500 training samples and 50 testing samples, scaled at 64 ? 64. All training images were randomly cropped and resized to 32 ? 32 after the normalization. The test images were only normalized. Implementations. All experiments were performed on one NVIDIA Tesla V100 GPU with 32Gb memory. The proposed DLB and compared methods were all implemented on Pytorch 1.6.0 in Python 3.7.0 environment. The hyper-parameters for the training scheme followed a consistent setting for a fair comparison. CIFAR-100/10: We followed the settings in previous works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34]</ref>. Specifically, every backbone network was trained for 240 epochs with a batch size of 64. The initial learning rate was set to 0.05, and decayed by a factor of 10% at 150 th , 180 th and 210 th epoch. We employed a stochastic gradient descent (SGD) optimizer with 0.9 Nesterov momentum, where the weight decay rate was set to 5 ? 10 -4 . TinyImageNet: We also followed the settings in previous works <ref type="bibr" target="#b41">[42]</ref>. Concretely, we set the maximal epoch number to 200, the batch size to 128. The initial learning rate was set to 0.2 with a decayed factor of 10% at 100 th , 150 th epoch. The weight decay rate and the momentum in the SGD optimizer were set to 1 ? 10 -4 and 0.9 respectively. We fixed the temperature ? at 3 and coefficient ? in DLB to 1, where the dependence of hyper-parameters is explored in the next subsection. The temperature setting in DLB followed suggests from previous work <ref type="bibr" target="#b43">[44]</ref>, and we did not manually tune it for our approach. We used the top-1 error rate (%) on the test set as the evaluation metric. For reproducibility, we fixed the random seed at 95 in all experiments. We also measured the average and the associated standard deviation over three runs. Importantly, for a fair comparison in terms of computation cost, the proposed method (DLB) was evaluated on half of the total training interactions/epochs than the compared approaches. Although our method comes to a doubled batch size by duplicating half of the last mini-batch, it brings no extra computation cost with halving total training iterations.</p><p>Backbone architectures. We employed six representative architectures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33]</ref> for evaluation, namely Vgg-16, Vgg-19 <ref type="bibr" target="#b24">[25]</ref>, ResNet-32, ResNet-110 <ref type="bibr" target="#b8">[9]</ref>, WRN20-8 <ref type="bibr" target="#b40">[41]</ref>, DenseNet-40-12 <ref type="bibr" target="#b10">[11]</ref>.</p><p>Compared methods. Hard labels were utilized in the baseline to train the target network directly. We also compared the proposed method with label smoothing regulariza-Table <ref type="table">3</ref>. Performance compatibility with augmentation-based regularization methods including CutOut <ref type="bibr" target="#b5">[6]</ref>, CutMix <ref type="bibr" target="#b37">[38]</ref> and DDGSD <ref type="bibr" target="#b34">[35]</ref> on CIFAR-10/100. We calculated the average top-1 error rate (%), standard deviation of three runs, written in the form of 'avg ? std'. The best result in each category was highlighted in boldface.  tion (LSR) <ref type="bibr" target="#b26">[27]</ref> and self-knowledge distillation regularization approaches, including teacher-free knowledge distillation (Tf-KD sel f ,Tf-KD reg ) <ref type="bibr" target="#b36">[37]</ref>, class-wise self-knowledge distillation (CS-KD) <ref type="bibr" target="#b38">[39]</ref>, progressive self-knowledge distillation (PS-KD) <ref type="bibr" target="#b11">[12]</ref>. The above methods focus on logitlevel regularization. Data-distortion guided self-knowledge distillation (DDGSD) is a data augmentation based distillation approach <ref type="bibr" target="#b34">[35]</ref>, which was compared and tested the compatibility with DLB. We removed the feature-level supervision in DDGSD <ref type="bibr" target="#b34">[35]</ref> i.e. MMD loss, for a fair comparison. As DDGSD is an augmentation based method, we explore the performance compatibility between DLB and DDGSD. All the extra hyper-parameters involved in the compared methods were retained as their original settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Classification Results</head><p>As illustrated in Table <ref type="table" target="#tab_1">2</ref>, our method (denoted as DLB) consistently improved the performance on various backbones (baseline). To be more specific, the averaged error rate improvement achieved by DLB ranged from 0.83% to 2.50% on CIAFR-100, 0.37% to 1.01% on CIFAR-10, and 0.81% to 3.17 on TinyImageNet. It shows the effectiveness of DLB that can significantly improve the generalization ability on various classification tasks. Moreover, DLB outperformed the state-of-the-art approaches, achieving the lowest top-1 error. The best and second-best performances on each set were highlighted in red and green respectively.  We can observe that DLB on CIFAR-10 succeeded the stateof-the-arts by 0.30% with WRN20-8; whilst on CIFAR-100 by 0.47%. These improvements are attributed to the self-distillation regularization from the last mini-batch. We notice that DLB significantly outperformed Tf-KD <ref type="bibr" target="#b36">[37]</ref>, and PS-KD <ref type="bibr" target="#b11">[12]</ref>. It demonstrates the performance advantage of DLB to generalize CNN. Furthermore, the identical teacher from the last mini-batch provides dynamically updated smoothed labels that fit the training process better than a pre-trained teacher or the last-epoch backup. Conclusively, DLB can efficiently serve as a universal regularization to normally train neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Compatibility with Augmentations</head><p>Previous work claims that data augmentation and distillation provide an orthogonal improvement <ref type="bibr" target="#b21">[22]</ref>. To evaluate the compatibility of DLB with data augmentation based regularization, we combined our method with CutMix <ref type="bibr" target="#b37">[38]</ref>, CutOut <ref type="bibr" target="#b5">[6]</ref> and DDGSD <ref type="bibr" target="#b34">[35]</ref> on CIFAR-10/100.</p><p>CutOut. CutOut randomly masks a square region in the training samples <ref type="bibr" target="#b5">[6]</ref>, we set the number of holes to 1 and the hole size to 16. Combining DLB with CutOut is straightforward. As shown in Table <ref type="table">3</ref>, DLB can progressively improve CutOut by 0.45% to 1.14% on CIFAR-10 and 0.84% to 2.74% on CIFAR-100. The performance enhancement was slightly higher than itself on the baseline. It suggests that DLB works in conjunction with CutOut.</p><p>CutMix. CutMix randomly cuts and pasts patches in a mini-batch for regularization, where we follow the addi-tional hyper-parameters setting as its original work <ref type="bibr" target="#b37">[38]</ref> i.e., ? = 1 for beta distribution and augmentation probability p = 0.5. CutMix improved the baseline by approximately 0.52% on CIFAR-10 and 1.48% on CIFAR-100. We plugged CutMix into our approach by performing selfdistillation on a batch of images distorted by the same Cut-Mix operation. It resulted in an extra 0.42% improvement on CIFAR-10 and 1.02% on CIFAR-100.</p><p>DDGSD. DDGSD is a self-distillation scheme, excavating a consistency between different distorted versions of the same images <ref type="bibr" target="#b34">[35]</ref>. We plugged DLB into DDGSD by distilling the last mini-batch for both two predictions from different augmented versions. As demonstrated in Table <ref type="table">3</ref>, DLB slightly outperforms DDSGD. Moreover, combining DLB with DDSGD can obtain observable improvements.</p><p>Discussion. We empirically show that DLB and augmentation regularization work orthogonally. To be more specific, an extra performance gain can be achieved by combing DLB with augmentation based regularization. These finds indicate that using DLB as a plugged-in regularization can enhance the generation of other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Robustness to Data Corruption</head><p>In this section, we try to understand how DLB works by empirically exploring its regularization effect on a corrupted setting. DLB is expected to impose training stability and consistency. The superiority of DLB is its simplicity for implementation and parallelization. Additionally, DLB is expected to amplify the robustness of the target neural network, especially in noisy data. It results in a stronger tolerance to the corrupted data, by mitigating the over-fitting to label noise <ref type="bibr" target="#b15">[16]</ref>. To prove this statement, we kept the experimental settings aligned with previous works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42]</ref> by randomly injecting symmetric label noise at different rate p ? {0.1, 0.2, 0.3, 0.4, 0.5, 0.6} to CIFAR-100/CIFAR-10 before the training process. Whilst, the test set was kept clean. In Figure <ref type="figure" target="#fig_1">2 (a)-(b)</ref>, we can observe a stable performance improvement on different models. For example, on CIFAR-100, DLB improved baseline by 2.38% at p = 0.1, and 4.44% at p = 0.6. The generalization improvement went increasingly higher with a higher label noise rate p. These observations demonstrate that DLB can effectively mitigate a neural network to fit label noise and improve the overall performance. The training performance of Vgg-16, Vgg-19, WideResNet, DenseNet are visualized in Figure <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Impact of Hyper-parameters</head><p>To analyze the impact of two hyper-parameters in DLB, namely the temperature ? in Eq. ( <ref type="formula" target="#formula_4">4</ref>) and balancing coefficient ? in Eq. ( <ref type="formula" target="#formula_6">5</ref>), we carried out controlled experiments. Firstly, we fixed ? to 1 and assigned ? with different values ranged in {1, 2, 3, 4, 5, 10, 20} to evaluate the performance of ResNet-32, Vgg-16 on CIFAR-100. As plotted in Figure <ref type="figure" target="#fig_4">4</ref> (a), DLB achieved lowest error rate with a temperature ? = 3. The performance reliance on ? is demonstrated in Figure <ref type="figure" target="#fig_4">4</ref> (b), with a fixed ? = 3. We observe that DLB performs well when ? changes in [0.5, 2.0].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Ablation Study</head><p>We removed the last batch distillation loss and only retained the cross-entropy loss for ablation on CIFAR-100. Specifically, ResNet-32 achieved a 28.01% test top-1 error rate, which performs slightly better than the baseline (28.26%). However, it was far worse than our DLB (26.00%). These findings show the significance to include the distillation loss as well as the effectiveness of our method. Detailed ablation results on all models are reported in the Table <ref type="table" target="#tab_3">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitation and Conclusion</head><p>In this research, we introduce an efficient self-distillation mechanism for consistency regularization. Without the involvement of a complex pre-trained teacher model or an ensemble of peer students, our method (DLB) distills the on-the-fly generated smooth labels in the previous iteration after rearranging the sampling sequence. DLB regularizes network by imposing training consistency, which is further amplified under data corruption settings. Thus, it boosts the robustness to label noise. Experimental results on three benchmark datasets suggest that our method can consistently outperform the state-of-the-art self distillation mechanism. Moreover, DLB, as a universal regularization approach, works in conjunction with augmentations techniques, bringing additional performance gain. However, due to the limitation of computational resources, we did not evaluate the performance on large-scale datasets such as Im-ageNet, which is left to future work. Additionally, as our method depends on the knowledge transmission between soft labels, in this paper, we focus primarily on classification, which yields another future direction in the extension to other tasks e.g. semantic segmentation, object detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>logits = logits[n//2:].detach() # no gradient 13: end for 4. Experiments 4.1. Datasets and Settings</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Performance on training with label noise. (a) CIFAR-100. (b) CIFAR-10. The line shows the top-1 validation error rate (%) on corrupted data at different percentage (p) of the noisy label i.e., p ? {0, 10%, 20%, 30%, 40%, 50%, 60%}. The bar illustrates the improvement of our approach, compared with baseline.</figDesc><graphic url="image-2.png" coords="6,63.72,347.29,232.49,213.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The training performance of Vgg-16, Vgg-19, WRN, DenseNet on corrupted data. (a)-(d) Results on data corruption rate p ? {0.1, 0.3, 0.5}; (e)-(h) Results on data corruption rate p ? {0.2, 0.4, 0.6}.</figDesc><graphic url="image-10.png" coords="7,298.86,174.32,113.75,88.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Impact of hyper-parameters in DLB on CIFAR-100 to ResNet-32 and Vgg-16. (a) Impact of temperature ? ? {1, 2, 3, 4, 5, 10, 20}, with a fixed ? = 1. (b) Impact of balancing coefficient ? ? {0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0} with fixed temperature 3. The best performance in each controlled experiment is marked out by red.</figDesc><graphic url="image-13.png" coords="7,169.53,530.72,113.28,92.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="3,50.11,72.00,495.00,175.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>CE . Whereas predictions from B t are smoothed by temperature ? and then stored for regularization in t th iteration. Storage of a batch of smoothed labels requires very few extra memory cost, which is thus more efficient. Conclusively, the overall loss function is formulated by</figDesc><table /><note><p><p>1   </p>i in t -1 th iteration. We employ a data sampler to obtain B t and B t-1 simultaneously at the t -1 th iteration for implementation. Both the predictions from B t-1 and B t in t -1 th iteration update the L</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison with label smoothing regularization and state-of-the-art self-distillation methods on CIFAR-10, CIFAR-100 and TinyImageNet in terms of average top-1 error rate (%). We calculated the average and deviation over three runs. The best and second best performance were highlighted in Red and Green respectively.</figDesc><table><row><cell>Dataset</cell><cell>Methods</cell><cell>Vgg-16</cell><cell cols="5">Vgg-19 ResNet-32 ResNet-110 WRN20-8 DenseNet-40-12</cell></row><row><cell></cell><cell>Baseline</cell><cell>6.03?0.22</cell><cell>6.04?0.10</cell><cell>6.54?0.10</cell><cell>5.21?0.24</cell><cell>5.47?0.08</cell><cell>7.09?0.28</cell></row><row><cell></cell><cell>LSR</cell><cell>5.91?0.40</cell><cell>6.05?0.06</cell><cell>6.73?0.17</cell><cell>5.60?0.17</cell><cell>4.76?0.06</cell><cell>7.47?0.27</cell></row><row><cell></cell><cell>Tf-KD sel f</cell><cell>5.92?0.15</cell><cell>5.91?0.01</cell><cell>6.32?0.01</cell><cell>4.92?0.08</cell><cell>5.33?0.13</cell><cell>7.03?0.14</cell></row><row><cell>CIFAR-10</cell><cell>Tf-KD reg CS-KD</cell><cell>6.12?0.07 6.22?0.20</cell><cell>6.25?0.18 6.38?0.11</cell><cell>6.60?0.05 6.88?0.15</cell><cell>5.48?0.18 6.12?0.05</cell><cell>4.79?0.01 4.89?0.19</cell><cell>7.38?0.16 7.85?0.17</cell></row><row><cell></cell><cell>PS-KD</cell><cell>5.90?0.04</cell><cell>6.07?0.57</cell><cell>5.96?0.06</cell><cell>5.09?0.68</cell><cell>4.99?0.01</cell><cell>6.77?0.28</cell></row><row><cell></cell><cell>DLB</cell><cell>5.38?0.01 (0.65 ?)</cell><cell>5.58?0.08 (0.46 ?)</cell><cell>5.85?0.02 (0.70 ?)</cell><cell>4.85?0.11 (0.37 ?)</cell><cell>4.46?0.01 (1.01 ?)</cell><cell>6.57?0.02 (0.53 ?)</cell></row><row><cell></cell><cell>Baseline</cell><cell cols="2">26.37?0.19 27.16?0.42</cell><cell>28.26?0.18</cell><cell>23.64?0.25</cell><cell>22.42?0.23</cell><cell>28.31?0.35</cell></row><row><cell></cell><cell>LSR</cell><cell cols="2">25.81?0.02 26.75?0.45</cell><cell>28.21?0.13</cell><cell>23.32?0.16</cell><cell>22.17?0.01</cell><cell>29.07?0.01</cell></row><row><cell></cell><cell cols="3">Tf-KD sel f 25.94?0.11 27.46?0.82</cell><cell>26.09?0.24</cell><cell>27.02?0.01</cell><cell>21.88?0.34</cell><cell>28.40?0.09</cell></row><row><cell>CIFAR-100</cell><cell>Tf-KD reg CS-KD</cell><cell cols="2">25.85?0.30 26.82?0.59 25.81?0.43 26.65?0.49</cell><cell>28.29?0.07 29.21?0.21</cell><cell>23.54?0.04 23.41?0.28</cell><cell>22.28?0.05 21.75?0.26</cell><cell>28.92?0.04 29.65?0.51</cell></row><row><cell></cell><cell>PS-KD</cell><cell cols="2">25.95?0.74 26.36?0.76</cell><cell>27.49?0.75</cell><cell>22.85?0.65</cell><cell>21.26?0.11</cell><cell>28.48?0.53</cell></row><row><cell></cell><cell>DLB</cell><cell cols="2">23.88?0.06 24.53?0.13 (2.50 ?) (2.63 ?)</cell><cell>26.00?0.03 (2.26 ?)</cell><cell>21.82?0.19 (1.83 ?)</cell><cell>20.79?0.33 (1.63 ?)</cell><cell>27.48?0.21 (0.83 ?)</cell></row><row><cell></cell><cell>Baseline</cell><cell cols="2">48.83?0.33 50.02?0.12</cell><cell>50.38?0.38</cell><cell>43.20?0.56</cell><cell>43.72 ?0.28</cell><cell>50.94?0.56</cell></row><row><cell></cell><cell>LSR</cell><cell cols="2">47.95?0.32 48.86?0.78</cell><cell>52.75?0.69</cell><cell>42.18?0.57</cell><cell>43.51?0.13</cell><cell>51.01?0.16</cell></row><row><cell></cell><cell cols="3">Tf-KD sel f 46.76?0.10 47.25?0.06</cell><cell>49.48?0.16</cell><cell>41.31?0.01</cell><cell>41.13?0.11</cell><cell>50.78?0.01</cell></row><row><cell>TinyImageNet</cell><cell>Tf-KD reg CS-KD</cell><cell cols="2">48.31?0.06 49.54?0.16 46.95?0.15 47.89?0.13</cell><cell>50.97?0.02 53.99?0.03</cell><cell>41.88?0.91 43.06?0.22</cell><cell>42.85?0.07 42.04?0.42</cell><cell>51.47?0.10 54.93?0.25</cell></row><row><cell></cell><cell>PS-KD</cell><cell cols="2">48.39?0.23 49.77?0.23</cell><cell>50.28?0.17</cell><cell>42.22?0.72</cell><cell>43.37?0.01</cell><cell>51.57?0.05</cell></row><row><cell></cell><cell>DLB</cell><cell cols="2">45.66?0.01 46.68?0.09 (3.17 ?) (3.35 ?)</cell><cell>48.66?0.07 (1.72 ?)</cell><cell>40.39?0.01 (2.81 ?)</cell><cell>41.03?0.02 (2.69 ?)</cell><cell>50.13?0.01 (0.81 ?)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on CIFAR-100.</figDesc><table><row><cell>Networks</cell><cell cols="3">Baseline loss in (4) removed All settings</cell></row><row><cell>Vgg-16</cell><cell>26.37?0.19</cell><cell>25.45?0.13</cell><cell>23.88?0.06</cell></row><row><cell>Vgg-19</cell><cell>27.16?0.42</cell><cell>25.73?0.28</cell><cell>24.53?0.13</cell></row><row><cell>ResNet-32</cell><cell>28.26?0.18</cell><cell>28.01?0.02</cell><cell>26.00?0.03</cell></row><row><cell>ResNet-110</cell><cell>23.64?0.25</cell><cell>22.65?0.23</cell><cell>21.82?0.19</cell></row><row><cell>WRN20-8</cell><cell>22.42?0.23</cell><cell>21.18?0.01</cell><cell>20.79?0.33</cell></row><row><cell cols="2">DenseNet-40-12 28.31?0.35</cell><cell>28.30?0.24</cell><cell>27.48?0.21</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Bucilu?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Online knowledge distillation with diverse peers</title>
		<author>
			<persName><forename type="first">Defang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Ping</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Moonshine: Distilling with cheap convolutions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Elliot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.13098</idno>
		<title level="m">Learning with retrospection</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Born again neural networks</title>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno>PMLR, 2018. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Online knowledge distillation via collaborative learning</title>
		<author>
			<persName><forename type="first">Qiushan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11020" to="11029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2005">2016. 2, 4, 5</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2004">2015. 1, 3, 4</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-knowledge distillation with progressive refinement of targets</title>
		<author>
			<persName><forename type="first">Kyungyul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byeongmoon</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doyoung</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangheum</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007">2021. 2, 3, 4, 6, 7</date>
			<biblScope unit="page" from="6567" to="6576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Knowledge distillation by on-the-fly native ensemble</title>
		<author>
			<persName><forename type="first">Xiatian</forename><surname>Xu Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaogang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04606</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Selfsupervised label augmentation via input transformations</title>
		<author>
			<persName><forename type="first">Hankook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<idno>PMLR, 2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="5714" to="5724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Analyzing the noise robustness of deep neural networks</title>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Visual Analytics Science and Technology (VAST)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="60" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Msd: Multi-self-distillation learning via multi-classifiers within deep neural networks</title>
		<author>
			<persName><forename type="first">Yunteng</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafei</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09418</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised multi-target domain adaptation through knowledge distillation</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Thanh Nguyen-Meidine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atif</forename><surname>Belal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhu</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Antoine</forename><surname>Blais-Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distillation-based training for multi-exit architectures</title>
		<author>
			<persName><forename type="first">Mary</forename><surname>Phuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1355" to="1364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6596</idno>
		<title level="m">Training deep neural networks on noisy labels with bootstrapping</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Fitnets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The state of knowledge distillation for classification</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Ruffy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karanbir</forename><surname>Chahal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10850</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Knowledge distillation beyond model compression</title>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elahe</forename><surname>Arani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bahram</forename><surname>Zonooz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6136" to="6143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Patient knowledge distillation for bert model compression</title>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09355</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006">2016. 2, 4, 6</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01780</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10699</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Contrastive representation distillation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Memory-replay knowledge distillation</title>
		<author>
			<persName><forename type="first">Jiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanxiong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuk-Jin</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Harmonized dense knowledge distillation training for multi-exit architectures</title>
		<author>
			<persName><forename type="first">Xinglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="10218" to="10226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Peer collaborative learning for online knowledge distillation</title>
		<author>
			<persName><forename type="first">Guile</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2005">2021. 1, 3, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Knowledge distillation meets self-supervision</title>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="588" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Data-distortion guided self-distillation for deep neural networks</title>
		<author>
			<persName><forename type="first">Bing</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Lin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2008">2019. 1, 2, 3, 6, 7, 8</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5565" to="5572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Snapshot distillation: Teacher-student optimization in one generation</title>
		<author>
			<persName><forename type="first">Chenglin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation via label smoothing regularization</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><forename type="middle">Eh</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2020. 2, 3, 4, 6, 7</date>
			<biblScope unit="page" from="3903" to="3911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Regularizing class-wise predictions via self-knowledge distillation</title>
		<author>
			<persName><forename type="first">Sukmin</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongjin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03928</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2005">2016. 2, 3, 4, 5</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Delving deep into label smoothing</title>
		<author>
			<persName><forename type="first">Chang-Bin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng-Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Be your own teacher: Improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiebo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anni</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenglong</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisheng</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019. 1, 2, 3</date>
			<biblScope unit="page" from="3713" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
