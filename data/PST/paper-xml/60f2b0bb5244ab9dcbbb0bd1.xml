<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Early Address Prediction: Efficient Pipeline Prefetch and Reuse</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ricardo</forename><surname>Alves</surname></persName>
							<email>ricardo.alves@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Uppsala University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefanos</forename><surname>Kaxiras</surname></persName>
							<email>stefanos.kaxiras@it.uu.se.</email>
							<affiliation key="aff0">
								<orgName type="institution">Uppsala University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Uppsala University</orgName>
								<address>
									<postBox>Box 337</postBox>
									<postCode>75195</postCode>
									<settlement>Uppsala</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Black-Schaffer</surname></persName>
							<email>david.black-schaffer@it.uu.se.</email>
							<affiliation key="aff0">
								<orgName type="institution">Uppsala University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Uppsala University</orgName>
								<address>
									<postBox>Box 337</postBox>
									<postCode>75195</postCode>
									<settlement>Uppsala</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Early Address Prediction: Efficient Pipeline Prefetch and Reuse</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3458883</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Achieving low load-to-use latency with low energy and storage overheads is critical for performance. Existing techniques either prefetch into the pipeline (via address prediction and validation) or provide data reuse in the pipeline (via register sharing or L0 caches). These techniques provide a range of tradeoffs between latency, reuse, and overhead.</p><p>In this work, we present a pipeline prefetching technique that achieves state-of-the-art performance and data reuse without additional data storage, data movement, or validation overheads by adding address tags to the register file. Our addition of register file tags allows us to forward (reuse) load data from the register file with no additional data movement, keep the data alive in the register file beyond the instruction's lifetime to increase temporal reuse, and coalesce prefetch requests to achieve spatial reuse. Further, we show that we can use the existing memory order violation detection hardware to validate prefetches and data forwards without additional overhead.</p><p>Our design achieves the performance of existing pipeline prefetching while also forwarding 32% of the loads from the register file (compared to 15% in state-of-the-art register sharing), delivering a 16% reduction in L1 dynamic energy (1.6% total processor energy), with an area overhead of less than 0.5%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Satisfying loads as early as possible is critical for performance. As it is difficult to build faster L1 caches, many approaches have been proposed to reduce load latency by prefetching data directly into the pipeline <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39]</ref> just in time for consumption to achieve zero load-to-use latency. The downside of pipeline prefetching is that the prefetched loads, because they used predicted addresses, require validation. This means that pipeline prefetches need to be re-executed: re-loading their value from the memory system and verifying that the value used speculatively was correct. This is necessary to account for local and remote stores and results in a doubling of L1 cache accesses for all pipeline prefetches. This problem can be addressed by installing prefetched data in an intermediate storage between the CPU and the L1 <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27]</ref>. However, this strategy requires extra storage area and data movement to an intermediate location between L1 and the register file, which also has to be exposed to coherence traffic for correctness. Energy-wise, this approach is similar to filter-cache/L0 solutions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18]</ref>, which also pay the energy overheads of moving and installing data in extra storage despite achieving only limited locality <ref type="bibr" target="#b1">[2]</ref>.</p><p>A ubiquitous trait of these pipeline prefetching techniques is that the address predictors required to prefetch loads make the addresses of load instructions available in the front-end of the CPU. The early availability of addresses has significant implications for the way data can be tracked in the physical register file (PRF). Since the PRF is the final destination of loads, it can be seen as truly the lowest level of the cache hierarchy. However, load-to-load data reuse (typical of all other cache levels) is difficult to expose through the PRF due to the tracking of data in being done in the register-space (used by instructions) and not in the address-space (as used in the cache hierarchy). The reuse potential of the PRF has been explored before <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b31">32]</ref>, but previous work has had to rely on heuristics to track reuse, precisely because they did not have addresses early in the pipeline, which significantly limited its potential.</p><p>Our main insight is that having address predictions early in the pipeline allows us to detect loadto-load reuse before the rename stage where registers are allocated. This allows us to reuse registers at rename, instead of having to re-fetch data from memory and re-install the same data in another register. While previous pipeline prefetching techniques either increase pressure on the L1 <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39]</ref> or require extra data storage <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27]</ref>, our solution not only avoids both problems, but actually reduces L1 accesses by forwarding the data from the PRF when reuse is detected. Moreover, our solution reduces capacity pressure in the physical register file beyond traditional register-sharing techniques due to the increased reuse detection of our address-based approach, while preserving the performance benefits of existing pipeline prefetching techniques. While our solution comes with additional memory ordering problems, we show that these can be solved by re-purposing existing pipeline structures such as the load queue (to solve coherence violations) and the memory order violation detection hardware <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37]</ref> (for validating both data prefetches and load-to-load forwarding through the PRF).</p><p>In summary, our contributions are:</p><p>? We identify that having an address prediction for loads early in the pipeline enables us to effectively combine pipeline prefetching, register reuse, load coalescing, and cheap validation.</p><p>? We demonstrate that adding address tags to the PRF allows us to find reuse via address prediction at rename and keep data in the PRF after the instruction completes. As a result, we can achieve long-term temporal reuse, but without the storage and data movement overhead of an L0. (We only add metadata storage to map addresses to registers.) ? We take advantage of having address tags for the PRF to store prefetched values. This achieves the temporal reuse of dedicated prefetch storage, but with the same zero-overhead data storage of just-in-time prefetching. ? We take advantage of the cache bus returning more than one word to coalesce neighboring prefetches and install them into the PRF. This provides spatial reuse, but without the data storage and data movement overheads of an L0.</p><p>? We demonstrate that the existing memory order violation predictor can be extended to validate both pipeline prefetches and data forwarding from the PRF, eliminating the need for dedicated structures or re-issuing the loads.</p><p>Our solution is able to reuse (forward) 32% of the loads from the register file, compared to only 15% for a state-of-the-art register-sharing technique <ref type="bibr" target="#b31">[32]</ref>, thereby capturing 93% of the available load-to-load reuse potential in the instruction window. Compared to just-in-time pipeline prefetching <ref type="bibr" target="#b38">[39]</ref>, our solution avoids replaying over 99% of the instructions for validation, without the overhead of extra validation mechanisms nor the dedicated data storage of previous approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27]</ref>. As a result, we reduce the total L1 load accesses by 32%, while delivering the performance benefits of existing state-of-the-art pipeline prefetching. By using the PRF for data storage, we are able to accomplish this with only the overhead of a small tag array (0.5% of the CPU core area).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Pipeline Prefetching and Value-prediction</head><p>Value prediction (VP) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref> guesses the result of an instruction before its execution. This allows instructions to proceed with predicted operands without waiting for the instructions that generate these operands to execute. This speculatively breaks true data dependencies and thus has the potential to increase instruction level parallelism (ILP) beyond the data flow limit. VP comes with the caveat that mispredictions are costly, so predicted values are only used when the prediction has a high confidence, leading to low coverage. Furthermore, since stores change values in memory, they often interfere with predictions, further decreasing confidence (and coverage) <ref type="bibr" target="#b38">[39]</ref>.</p><p>Value prediction techniques can be divided into two classes: computation-based (such as stride predictors <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13]</ref>) and context-based (such as VTAGE <ref type="bibr" target="#b29">[30]</ref> and DVTAGE <ref type="bibr" target="#b30">[31]</ref>). Orosa et al. <ref type="bibr" target="#b26">[27]</ref> recently proposed an hybrid that merges a stride predictor with DVTAGE to improve on both.</p><p>Sheikh et al. <ref type="bibr" target="#b38">[39]</ref> made the observation that addresses are easier to predict than data, and proposed the Decoupled Load Value Prediction (DLVP). DLVP predicts load addresses instead of the load values and issues the loads as a pipeline prefetchs just-in-time to be consumed by the dependent instruction(s). Predicting addresses has two main advantages: First, addresses have simpler patterns, thereby increasing prediction coverage, reducing iterations required to trust the prediction, and generating fewer mispredictions due to stores changing the predicted value. And, second, since approximately 25% of instructions are loads, the predictor does not need to store nearly as much state <ref type="bibr" target="#b26">[27]</ref>, making the prediction mechanism smaller and less complex. Moreover, recent work on pipeline prefetchers has shown that predicting the address of loads and prefetching them from memory achieves the same performance benefit as predicting all values for all instructions <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>However, while effective, just-in-time pipeline prefetching significantly increases memory accesses, as every pipeline prefetch needs to re-access memory a second time to validate the result, in case the prediction was wrong or there was a coherence or memory ordering violation. To avoid the overhead of re-accessing memory for validation and take advantage of potential data locality, the prefetched data can be installed in an intermediate storage between the pipeline and the first level of cache <ref type="bibr" target="#b26">[27]</ref>. Gonz?lez et al. <ref type="bibr" target="#b14">[15]</ref> further mitigate the secondary access for validation by leveraging the Address Reordering Buffer (ARB) <ref type="bibr" target="#b11">[12]</ref> to filter replays. However, these solutions require installing data in a extra storage level, probing this extra storage level on every access, and exposing it to coherence. As a result, these intermediate storage approaches come with the same storage and energy overheads of filter-caches/L0s, which are generally unable to reduce total memory accesses <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Register Sharing</head><p>Register sharing was originally proposed by Jourdan et al. <ref type="bibr" target="#b15">[16]</ref> to eliminate register moves. Their mechanism detects register reuse between instructions, renames the reused registers as the source registers of the new instructions, and tracks the lifetime of the shared registers.</p><p>Several techniques have been proposed to improve the efficiency of register tracking and sharing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref>. Besides reducing register pressure, these technique also allow for zero loadto-use latency on loads that have their data forwarded by the physical register file. Most recently, Perais et al. <ref type="bibr" target="#b31">[32]</ref> proposed the Inflight Shared Register Buffer (ISRB) to both improve reuse detection and simplify register tracking, as well as enable load-to-load data forwarding through the PRF. While such load-to-load forwarding is similar to our proposal, it requires cache accesses to validate the values of data forwarded to loads, has limited coverage due to the use of an heuristic for detecting reuse (instruction distance), and can only take advantage of temporal reuse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Detecting Memory Ordering Violations</head><p>Memory ordering violations can come in two forms: internal, where a load incorrectly bypasses an older store on which the load depends, and external, due to memory consistency model violations, typically appearing as coherence invalidation requests that catch speculatively performed loads out of their correct memory order.</p><p>Internal memory violations happen because processors allow loads to bypass independent stores for performance, i.e., Speculative Memory Bypassing (SMB) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24]</ref>. However, this bypassing happens before addresses are available, and is therefore speculative, based on a prediction of the load's dependence on older stores. If the prediction is incorrect, then the load and its dependency chain (or simply all following instructions) need to be replayed. External memory ordering violations can occur due to speculatively reordered accesses that conflict with external accesses. This is handled by tracking incoming coherence events.</p><p>Ordering violations can be solved by a value-based approach, which does a brute-force reexecution of every speculative load at commit to compare the speculatively loaded value with the actual value. This comes with the cost of extra memory/cache accesses for each speculative load, which can have a significant impact on performance and energy. To address this problem, strategies have been proposed to validate most memory bypassing and coherence events while minimizing the replaying of loads <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b39">40]</ref>. For detecting internal violations, the most successful strategy uses a Store Sequence Bloom Filter (SSBF) <ref type="bibr" target="#b36">[37]</ref> to avoid over 99% of replays.</p><p>External memory violations are detected by exposing the load queue (LQ) to coherence traffic. The load queue holds the physical addresses of each load after translation and generates replays on invalidation requests hits. Virtual addresses are also stored in the load queue to facilitate storeto-load forwarding and detection of internal memory ordering violations, since they are available earlier than physical addresses, using them can reduce the penalty of mispredictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MOTIVATION</head><p>(Figure <ref type="figure" target="#fig_0">1(A)</ref>) In a baseline out-of-order processor, loads generate their addresses and then access the cache. Once the cache returns data, dependent instructions can execute. However, as loads and stores may bypass each other during execution, a final validation that the load's value is correct with regards to the internal ordering of instructions is required at commit. External ordering violations are caught by the LQ as it participates in coherence. In this work, we assume the internal ordering commit validation is done using SSBF <ref type="bibr" target="#b36">[37]</ref>, which can validate over 99% of all accesses without accessing the L1. (Figure <ref type="figure" target="#fig_0">1</ref>(B)) DLVP's <ref type="bibr" target="#b38">[39]</ref> just-in-time pipeline prefetching predicts the load addresses and accesses the L1 earlier in the pipeline. This allows the cache to deliver the data by the time the dependent instructions are ready and achieve 60% coverage (Figure <ref type="figure" target="#fig_1">2</ref>). However, DLVP requires a second L1 access once the load address is generated to validate the value, as well as the final ordering validation at commit. (Figure <ref type="figure" target="#fig_0">1</ref>(C)) Load-to-load forwarding exposes locality by adding extra storage (an L0 or an extended LQ) to keep copies of the loaded data closer to the processor <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27]</ref>. This allows later instructions to get the expected value of the load from the storage earlier in the pipeline. When the load's address is later generated, the same storage structure can be used to validate that the correct value was loaded, as the storage is searchable by the final generated address. This minimizes pressure on L1, but requires extra storage and extra data copies. The second access to validate internal ordering can be avoided <ref type="bibr" target="#b14">[15]</ref> but at least one access to the L1 or intermediate storage is needed for every load.</p><p>(Figure <ref type="figure" target="#fig_0">1</ref>(D)) Register Sharing can eliminate the need for additional data storage by using the physical register file. This also, and perhaps more importantly, avoids the need to access the cache and copy the data when a register reuse is predicted as the rename logic simply sets the appropriate register reference for the load. This achieves the same zero load-to-use latency of pipeline prefetching without extra storage or data movement. However, when the load's address is later generated, an L1 access is still required to validate internal memory ordering and coherence, since the register file is not exposed to coherence traffic. However, register-sharing is unable to provide high coverage due to its reuse heuristics <ref type="bibr" target="#b31">[32]</ref> (instruction distance in Figure <ref type="figure">3</ref>), which can only exploit less than half of the load-to-load forwarding potential that exists in the pipeline at any given time.</p><p>State of the Art summary: While DLVP's pipeline prefetching is an effective IPC improvement technique, it brings added pressure to the cache and register file, since it requires accessing both structures on prefetch and validation. Register sharing has the potential to mitigate the increased L1 pressure by satisfying prefetch requests from the register file, but previously proposed approaches have shown limited reuse detection. Our proposal, AT-RT, addresses these challenges while delivering the same zero load-to-use latency as DLVP.</p><p>(Figure <ref type="figure" target="#fig_0">1</ref>(E)) Our proposal (AT-RT) takes advantage of DLVP's early address prediction and extends it to identify reuse in the register file without the need for L1 accesses or data copies. If pipeline prefetches are found in the register file, then we do not require a memory access, resulting in reduced L1 pressure and energy. AT-RT accomplishes this by providing a table to index the register file by address (i.e., register file tags). By searching for register reuse by address, AT-RT is able to dramatically increase reuse detection over previous techniques, thereby reducing register file pressure, and take advantage of both spatial and temporal reuse. Further, as the reuse prediction Early Address Prediction: Efficient Pipeline Prefetch and Reuse 39:7 Fig. <ref type="figure">3</ref>. Ratio of loads that are forwarded using instruction distance to detect reuse, compared to total reuse potential available in the LQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1. Comparison of the Techniques Considered in this Work</head><p>is made based on the address, AT-RT can then use the predicted address to validate the load against its final generated address, avoiding the need for a second access. If the addresses match, then only a final internal ordering validation is needed. A comparison to related work is provided in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EFFICIENT TEMPORAL LOAD TO LOAD FORWARDING AND PREFETCHING</head><p>The ability to accurately predict the address of a load from its PC in the fetch stage allows us to speculatively fetch the value from the L1 without having to wait for the load address to be computed. However, to be able to take advantage of reuses of this load in the future, it must be cached close to the pipeline. Using the physical register file is a compelling choice, as it is already the final destination for the data, which means there is no additional data movement overhead.</p><p>To effectively use the PRF as a cache, we need to address two issues: (1) finding the prefetched data and (2) forwarding it without having to re-access the L1. We achieve this by providing an address-to-register mapping (essentially register file tags) that allow us to find load data in the PRF based on its address. By making the PRF work as an address-based cache, we are able to overcome the coverage limitations of previous work (since we can find all address reuses and are not limited to inter-instruction heuristics) and avoid extra cache accesses for validation (since we can use the final address to validate execution order).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Finding and Forwarding Prefetched Data via the Address Tag-Register Tag (AT-RT) Table</head><p>To detect register reuse and perform load-to-load forwarding through the physical register file, we introduce a register translation table, the Address Tag, Register Tag (AT-RT) table. The AT-RT provides address tags for the entries in the PRF that were loaded from memory. The direct-mapped AT-RT contains a register tag, the data size, reference counter for the number of load instructions that are sharing the register, a valid bit, and an address tag for each entry.</p><p>Load instructions put their destination register tag and (predicted) address tag in the appropriate AT-RT entry, increment its reference counter, and mark it as valid. The table is then indexed using the predicted address for each later load in the fetch stage. When a later load finds its predicted address in the AT-RT, it skips allocating its own destination register and instead reuses the same register tag as the one found in the AT-RT by simply incrementing its reference counter. This allows loads to treat the PRF as a cache by detecting and reusing registers based on the address they correspond to. Note that all of the above is enabled by having a predicted load address before register renaming in the pipeline.</p><p>The AT-RT reference counters are independent from existing reference counters in the PRF. Thus, freeing an entry in the AT-RT does not automatically free the corresponding physical register. However, as we will show, AT-RT entries can serve as hints to prevent register de-allocation for data that may see reuses and thereby increase reuse.</p><p>Prefetched loads are inserted in the load queue with their predicted address to enable addressbased validation later on without extra storage or tracking mechanisms. After the load reaches the execution stage, its effective address is generated and compared to its predicted address. If the addresses match, then the prediction is deemed accurate and the load does not have to be replayed for validation. (The detection of possible memory order violations may cause a replay as with any out-of-order pipeline; see Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Forwarding Validation</head><p>Validating forwarded data requires ensuring that the address prediction was correct and that there were no internal or external ordering violations between the time the forwarded data was used and when the instruction committed in program order. The address predictions can be easily validated when the load's address is finally generated by simply comparing the generated address to the predicted address in the LQ. Note that the pipeline prefetcher predicts in the virtual address space, so prefetches will still need to be translated, and we can reuse the translation if the predicted and final virtual addresses match. The baseline (Figure <ref type="figure" target="#fig_0">1(A)</ref>) requires the same translation, but it happens later, as there is no pipeline prefetching. Validating memory order is more involved and can be broken down into two separate problems, external and internal violations, discussed below.</p><p>External Violations. External violations come as memory coherence invalidation requests. Although complex, this type of ordering violations is present in all multi-core systems and the CPU already has mechanisms to detect the violating loads <ref type="bibr" target="#b36">[37]</ref>, rollback the values of the shared registers <ref type="bibr" target="#b31">[32]</ref>, and replay the infringing instruction chains <ref type="bibr" target="#b32">[33]</ref>. The only extra step introduced by our approach is that the AT-RT also needs to be accessed to invalidate any entries for the invalidated addresses to avoid forwarding stale data. While the AT-RT table is virtually tagged, the LQ, which holds both virtual and physical addresses for responding to coherence snoops, can provide translations to find the correct entry. If the LQ does not have a physical address translation yet, then it means that that particular load was not yet issued so it can not be in violation of the consistency model. Since the AT-RT forwarding detection is done in the front end of the CPU, it is done in-order, and, as such, forwarding is always performed from older to younger loads. The invariant in our design is that a snoop always catches the oldest infracting load with invalid forwarded data.</p><p>Note that simultaneous accesses to the AT-RT table (simultaneous renaming and coherence) would not stall the pipeline, as loads can correctly be renamed and dispatched without consulting the AT-RT for reuse opportunities. Recent work has also shown that delaying coherence requests in the L1 could avoid the need to invalidate entries on invalidation requests <ref type="bibr" target="#b35">[36]</ref>.</p><p>Internal Violations. Internal memory order violations occur when loads incorrectly bypass dependent stores. This is an undesired effect of speculative memory bypassing, a common technique to improve performance on out-of-order pipelines. All pipelines allowing speculative memory bypassing need to detect such violations. While our approach is independent of the mechanism chosen, we implemented the SSBF proposed by A. Roth <ref type="bibr" target="#b36">[37]</ref>, as it minimizes redundant memory accesses (&lt;1%) to the L1. SSBF works by filtering loads that are not in a Store Vulnerability Window (SVW). This relies on store sequence numbers (SseqN) to detect if a load is in the vulnerability window of a store and only loads that are in the vulnerability windows need to be replayed. Figure <ref type="figure">4</ref> shows an example of how order is validated by an SSBF as proposed in the original work. At commit, and in program order, the stores update the SSBF table and loads check the SseqN of the youngest store that wrote to the same address. If the youngest store to write to that address is older than the store the load bypassed, then the issue order is valid, otherwise, there may have been an order violation and the load needs to be replayed. The example shows an execution order that has the first load correctly bypassing an older store and a second load correctly bypassing one store (ST A ) but incorrectly bypassing a second (ST B ). At commit, the infracting load is correctly detected, since it bypassed an older store than the last one that wrote to same memory location.</p><p>While SSBF cannot be paired with register reuse or pipeline prefetching techniques as originally proposed, it can be extended to do so. For pipeline prefetched data, the validation needs to be done by comparing prefetch order (instead of execution order) with program order as in the original design (as exemplified in Figure <ref type="figure">4</ref>). This is a simple modification and similar to the one proposed by Gonz?lez et al. <ref type="bibr" target="#b14">[15]</ref>. However, this modification alone is not enough to validate load-to-load forwarding through the register file. While load-to-load forwarding does not affect the scheduling and execution order of the associated loads, by forwarding the data from older loads to younger ones there is an implicit memory bypassing by the younger load. From a memory ordering point of view, both loads "executed" at the same time. This can be an issue if there is a conflicting store between two loads with forwarding. To address this problem, we modify SSBF to have loads inherit the SseqN of the loads they had the data forwarded from, and thus inherit their vulnerability window as well. This modification enables the memory ordering validation strategy (in this case SSBF) to function properly and avoid accessing the memory for validation on both load-to-load forwarding and register-sharing techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Interaction with the Memory Bypass Predictor</head><p>Any speculative forwarding technique needs to be informed of memory dependencies by the speculative memory bypassing predictor, as misschedulings due to missed dependencies can force pipeline flushes and have a profound impact on performance <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33]</ref>. We use the same Load Store Conflict Detector <ref type="bibr" target="#b38">[39]</ref> strategy as DLVP to enable or disable forwarding for particular loads before rename. If the LSCD detects a forwarding conflict, then we simply invalidate the corresponding entry in the AT-RT table, preventing further forwarding from loads that are likely to have stale Fig. <ref type="figure">4</ref>. Example of how the Store Sequence Bloom Filter (SSBF) <ref type="bibr" target="#b36">[37]</ref> is used to detect possible memory ordering violations using Load/Store addresses (letters) and store-sequence-numbers (numbers).</p><p>data. Figure <ref type="figure">5</ref> shows an example of a typical memory ordering violation introduced by load-to-load forwarding and the correct forwarding combination.</p><p>Perais at al. <ref type="bibr" target="#b31">[32]</ref> went one step further to break memory dependencies by identifying the producer of a load and renaming the destination register of the load as the source register of the producing store. This effectively allows for load-to-store forwarding through the register file as well. In this work, we consider only load-to-load forwarding, because, unlike loads, stores already have dedicated storage in the pipeline for their data: the store-queue and the store-buffer. Data produced by stores must be installed in this storage regardless, and every load has to probe it for correctness <ref type="bibr" target="#b35">[36]</ref>, so there is no energy/storage benefit in forwarding the data from the PRF Early Address Prediction: Efficient Pipeline Prefetch and Reuse 39:11 Fig. <ref type="figure">5</ref>. Example of how forwarding can create a ordering violation even if follows program order. LSCD learns the memory violation and breaks the erroneous load-load forwarding to the third and fourth loads by forcing the third load to make a new entry in the AT-RT. This also enables the correct forwarding to the fourth load.</p><p>directly. The store-buffer can be used to filter accesses to L1 <ref type="bibr" target="#b2">[3]</ref>, but the access to the store-buffer is still required, even if data is to be forwarded from the PRF. Note that here is no overlap between load-to-load and store-to-load forwardings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Address Prediction</head><p>Our solution is independent of the chosen address predictor but needs the prediction early in the pipeline to manage register sharing and provide load-to-load forwarding through the PRF. For simplicity, we choose a stride-based predictor. The prediction table is indexed by PC and each entry holds the last accessed address, the stride, and the confidence level (2-bit saturating counter; details in Section 6.1). The prediction is only trusted when the counter is saturated and our simulation results showed an average error ratio of only 1.6% (Section 6.2).</p><p>Although highly accurate, a stride predictor is not as accurate as a hybrid DVTAGE + Stride address predictor <ref type="bibr" target="#b26">[27]</ref>. However, we found that the stride predictor was accurate enough to deliver significant benefits and provides a good baseline. More sophisticated address predictors would simply increase coverage and/or decrease errors, improving our proposed solution even further, as our reuse detection and forwarding technique is independent of the type of address predictor used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPOSING SPATIAL LOCALITY VIA THE PRF</head><p>A key function of caches (indeed, the main benefit of L0s) is that they expose spatial locality. However, for pipeline prefetching, as long as the prefetch is on-time, there is no performance benefit to providing spatial locality. Moreover, pipeline prefetching minimizes pollution, as it only fetches the data that it predicts will be used, instead of fetching and installing adjacent data.</p><p>While the performance benefit of spatial locality is likely to be covered by prefetching, the energy benefit is not. This is because CPUs have wide L1 data busses (up to 512 bits) to satisfy vector loads, meaning that L1 loads already pay the energy cost of moving adjacent words. As a result, if we can take advantage of spatial locality, then we can move the data at no additional cost. Fig. <ref type="figure">6</ref>. 32 KB cache with using different sub-array size and respective h-tree combinations <ref type="bibr" target="#b1">[2]</ref>. The smaller the sub-array, the smaller the minimum data read-out width, but the larger the overhead of the h-tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SRAM Cache Layout</head><p>SRAMs are built as matrices of smaller sub-arrays and connected with h-tree busses. Each subarray is a group of SRAM cells, and the width of the sub-arrays determines the minimum number of bits read per cache access. These sub-arrays can be resized, but smaller sub-arrays (which have smaller minimum read-out widths) require more sub-arrays for the same capacity, and thus larger h-trees. Since decreasing the sub-array size means increasing the h-tree complexity, the final design is usually a compromise (Figure <ref type="figure">6</ref>). For a first-level, 32 KB, 8-way set associative cache, a 128-bit sub-array is a typical choice <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref>, and we assume such a layout for this work.</p><p>As the sub-array determines the SRAM read-out width independently of the load instruction, all read-outs pay the energy cost of accessing and moving 128-bits. This means that energy benefit of coalescing multiple loads into 128-bits can be substantial. For example, grouping two 64-bit requests into a single 128-bit one (if they access the same sub-array line) could avoid a second 128-bit read, and thereby halve the cache access energy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Coalescing Loads Efficiently</head><p>While storing all 128-bits from a load in the PRF is appealing to minimize L1 accesses, there are two main challenges: (1) increased writes and capacity pressure on the PRF from installing extra data that may not be used and (2) finding the speculatively installed data for use by later loads. Both of these problems can be solved by leveraging predicted load addresses: (1) Since loads' predicted addresses are available in the front-end of the CPU, they can be used to detect coalescing opportunities while the data is in-flight back from the L1 and only install the data from the 128-bit block that is predicted to be used. Given the small prefetch range (128 bits), the time between rename (where possible coalescing is detected) and load write-back (when we have to decide which parts of the 128-bit block to write in the PRF) is enough to find most loads that map to the same 128-bit block. (2) Since we only prefetch data for loads at or beyond the rename stage, all loads will already have a destination register in the AT-RT table. As a result, the 128-bit block that returns from the cache can be stored across the registers already mapped by the AT-RT that match the sub-addresses of the block. The bits of the 128-bit block that do not have a valid mapping in the AT-RT by the time the load returns, are simply discarded, i.e., only the data from the 128-bit block that was predicted to be used will be installed in the PRF. This motivates our coalescing strategy: While we are waiting for a prefetch to return from the L1, we look for other prefetches that are predicted to access the remainder of the 128-bit block that the first prefetch will bring in. When the first prefetch returns, we install data into the PRF from the 128-bit block for both the original prefetch and any other prefetches that were predicted to be in the block.</p><p>To optimize for this type of coalesced access, the AT-RT table can be modified so each entry tracks the group of registers that contain parts of the 128-bit block. Each entry then tracks multiple registers depending on the load size, i.e., 8, 4, 2, or 1 registers for load sizes of 1, 4, 8, or 16 bytes, respectively. This mapping does not change the behavior of tracking individual registers, since reference counting in the register file is done independently of how many registers each individual AT-RT table entry tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Exposing more Locality than the LQ Window</head><p>Registers in the PRF are typically freed when their consumers commit. However, by freeing registers when their consumer commits, the forwarding potential of the register is limited to the processor's execution window, and, in particular, the number of loads that can be tracked at a given time. The reuse potential for loads is therefore capped by the reuse windows exposed by the load queue. This limitation also exists in previous register reuse techniques based on instruction distance, as the processor's instruction window determines the largest reuse distance that can be observed.</p><p>However, since AT-RT uses addresses to detect reuse, the lifetime of the load instruction no longer determines the time span across which we can detect reuse: Even if a load retires and leaves the load queue, it can still leave a valid address entry in the AT-RT table and data in the PRF. This means that another later load can be predicted to have the same address and reuse the data from the PRF. As a result, there is a tradeoff between freeing registers early to reduce PRF pressure and keeping them alive longer to increase PRF reuse. However, as register reuse inherently (and significantly) reduces PRF pressure, it makes sense to bias the tradeoff towards finding more reuse.</p><p>To take advantage of the increased reuse potential available due to using addresses to track register reuse, we have each entry in the AT-RT table increment the PRF reference counter as well. This way, even if all instruction referencing the register (loads or otherwise) commit, the PRF will not free the register if there is still a valid entry in the AT-RT that can allow later loads to reuse its data. Invalidating the entry (either to avoid memory ordering violations or a collision) requires decrementing the respective PRF reference counter as well.</p><p>With this strategy alone, AT-RT entries can only be invalidated if another address maps to the same entry (collision in the AT-RT table) or if a load-store collision is detected (LSCD unit). To allow the front-end of the CPU to free registers in case of increased register pressure, we need to implement register garbage collection. This system can be as simple as randomly invalidating AT-RT entries when needed to a full-blown LRU policy. Since we opted to track multiple registers per AT-RT entry to take advantage of the SRAM read-out width, each AT-RT entry has its own reference counter for how many loads in the pipeline are referencing it. As a result, the AT-RT entry is kept alive (and all its tracked registers) as long as at least one of its registers is referenced by an uncommitted load. This strategy does not maximize reuse, since we may still free registers too soon and miss out on further reuses. However, it strikes a compromise between extending a register's lifetime and implementing a trivial mechanism to recover unreferenced registers. Since the AT-RT finds significant register reuse opportunities, even increasing the lifetimes of registers in the PRF results in a lower total register file than the baseline due to the benefits of register sharing. Note that in our simulations we did not encounter a single situation where AT-RT used more registers than the baseline configuration despite using them for pipeline prefetching and keeping them alive for longer.</p><p>The complete view of the pipeline with all the new proposed structures is shown in Figure <ref type="figure" target="#fig_2">7</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION 6.1 Simulation and Modeling</head><p>We use 10 checkpoints (100 M instruction warming, 10 M instruction detailed simulation) for each SPEC2006 <ref type="bibr" target="#b8">[9]</ref> application. We use gem5 <ref type="bibr" target="#b6">[7]</ref> to simulate a large out-of-order X86_64 CPU (Intel Skylake-like 8-wide, 224 entry ROB, 72 entry LQ; full details in Table <ref type="table" target="#tab_0">2 1</ref> ). The first-level cache is dual-ported with pipelined loads and stores. For energy evaluations, we use CACTI <ref type="bibr" target="#b24">[25]</ref> with a 22 nm technology node. <ref type="foot" target="#foot_2">2</ref>We evaluate five configurations:</p><p>? DLVP: Address load predictor that prefetches the load data into the pipeline <ref type="bibr" target="#b38">[39]</ref>. <ref type="foot" target="#foot_3">3</ref>Early Address Prediction: Efficient Pipeline Prefetch and Reuse 39:15 ? IDist: Register reuse technique that uses instruction distance for load-to-load reuse <ref type="bibr" target="#b31">[32]</ref>. <ref type="foot" target="#foot_4">4</ref>? AT-RT (Temp): DLVP pipeline prefetching + Our register reuse technique that uses predicted addresses to forward data (temporal reuse) and SSBF-based prefetch and register forwarding validation. This configuration is limited by the locality exposed by the LQ. ? AT-RT (Temp+Spat): DLVP pipeline prefetching + Our register reuse technique with coalescing of loads to the same 128-bit sub-array read-out (temporal and spatial reuse) and SSBF-based prefetch and register forwarding validation. This configuration is able to expose more locality than the one available in the LQ. ? Baseline: Standard pipeline with SSBF (to filter replays at commit), but no pipelineprefetching and no register reuse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Prediction Accuracy</head><p>The forwarding potential of AT-RT is a function of the available locality and the accuracy of the address predictor, with better predictors delivering better results. For this work, we choose a simple predictor, as described in Section 4.4. More advanced address predictors will simply result in more reuse opportunities and better performance/energy. Figure <ref type="figure" target="#fig_3">8</ref> shows the accuracy of our chosen stride address predictor. The stride predictor shows high coverage, with the majority of the benchmarks <ref type="bibr" target="#b16">(17)</ref> able to accurately predict more than 60% of the addresses, and 8 benchmarks having an accuracy above 75%. Only 6 benchmarks have an accuracy below 40% with cactusadm, at 24% accuracy, being worst. On average, our simple predictor is able to accurately predict the address of 59% of the loads.</p><p>While low coverage limits the potential of AT-RT technique, incorrect address predictions can have a negative impact, since incorrectly forwarded loads must be replayed <ref type="bibr" target="#b0">[1]</ref>. We found that most benchmarks <ref type="bibr" target="#b19">(20)</ref> have misprediction ratios below 2% with an average of 1.6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Load-to-load Forwarding</head><p>Figure <ref type="figure" target="#fig_4">9</ref> compares how much load-to-load reuse the different configurations are able to take advantage of, and how much is exposed by the load queue (i.e., perfect reuse of all in-flight loads). IDist can forward 16% of the loads on average with the best benchmark being lbm, which has 41% of its loads forwarded from the physical register file. Overall, IDist is able to extract 43% of the total load-to-load locality exposed in the load queue through the PRF.</p><p>AT-RT (Temp) is able to forward more data than IDist on 26 of 29 benchmarks. Of the three benchmarks where AT-RT performs worse than IDist, milc shows the highest difference, with the AT-RT forwarding 4 percentage points fewer loads than IDist. The remainder two benchmarks (dealii and leslie3d) fall behind by less than 1 percentage point (0.7, 0.08, respectively). However, AT-RT (Temp) shows the largest gains on astar, cactusadm, and gromacs with 25, 18, and 16 percentage point increase over IDist. Overall, AT-RT (Temp) is able to forward 20% of the loads (1.2? more than IDist) and is able to extract 56% (vs. 43% for IDist) of the load-to-load locality exposed in the load queue through the PRF.</p><p>The AT-RT (Temp+Spat) configuration fetches data in 128-bit blocks and installs it in multiple PRF entries when it is predicted to be used, and is thereby able to outperform both the IDist and AT-RT (Temp) across the entire benchmark suite. Benchmarks such as h264ref, sphinx3, and xalan, show an improvement of 1.9?, 2.2?, and 1.3? compared to AT-RT (temp), and 5?, 5.2?, and 3? compared to IDist.</p><p>Since AT-RT (Temp+Spat) takes advantage of the reduced register pressure from register forwarding to extend the lifetime of some registers, it is able to forward more data than what is exposed by the load queue window. On 14 of the benchmarks, AT-RT (Temp+Spat) is able surpass the total locality available in load-queue by extending register lifetimes. Sphinx3 forwards 2.1? more loads than the best a load-queue-based forward predictor could, and lbm is able to forward 70% of its loads from the PRF. Overall AT-RT (Temp+Spat) reaches 93% of the load-to-load locality that the load queue exposes through the PRF. These results demonstrate that AT-RT is indeed able to achieve significantly improved spatial and temporal reuse over previous register-sharing techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Memory Access Reduction</head><p>Since DLVP tries to prefetch every address-predicted load instruction, the higher the accuracy of the address predictor, the more prefetches and thus the more loads that need to be replayed for validation. Figure <ref type="figure" target="#fig_5">10</ref> shows the increase in L1 accesses by loads from DLVP compared to a standard non-pipeline-prefetching CPU. On average, DLVP executes 56% more loads accessing the L1 than the baseline and more than 90% more for gemsfdtd, leslie3d, and libquantum. The verification load could potentially be filtered by using SSBF (or ARB as proposed in APDP <ref type="bibr" target="#b14">[15]</ref>), but this would only reduce the replays to match the baseline <ref type="foot" target="#foot_5">5</ref> (not shown in the graph), unlike IDist and AT-RT, which can reduce the number of L1 accesses below that of the baseline via register sharing.</p><p>Early Address Prediction: Efficient Pipeline Prefetch and Reuse 39:17  IDist (when extended with SSBF) and both AT-RT strategies can reduce the number of L1 load accesses to fewer than that of the baseline by not only avoiding DLVP's second verification load, but also avoiding sending the first prefetch to the L1 if the data is in the PRF. The number of L1 load accesses that can be filtered is directly related to the ability of the forwarding strategy to detect and take advantage of reuse. AT-RT (Temp) is able to reduce L1 load accesses by 20% on average, and up to 51% in lbm. This is an improvement over the 15% average L1 load access reduction of IDist with its highest reduction also being on lbm with 41%. Overall, AT-RT (Temp) shows a 1.3? improvement in avoided L1 accesses over IDist.</p><p>AT-RT (Temp+Spat) shows the largest improvement and eliminates 32% of the L1 load accesses compared to the baseline, a 2.1? improvement over IDist. Lbm is once again the benchmark with the largest reduction, by decreasing L1 load accesses by 71%.</p><p>The decrease in L1 accesses also reduces the L1 data cache dynamic energy, as shown in Figure <ref type="figure" target="#fig_6">11</ref>. Note that there is not a one-to-one correspondence between dynamic energy reduction and memory accesses by filtered load instructions (Figure <ref type="figure" target="#fig_5">10</ref>). This is because the dynamic energy is a function of both loads and stores, and the cache hit ratio, and the strategies evaluated in this article only affect load instructions. However, since most benchmarks are dominated by loads (70% or more on average) and have a high hit ratio, benchmarks with the highest percentage of filtered L1 accesses also show the highest reduction in dynamic energy. Libquantum and gcc, for example, stand out as exceptions due to the high number of cache misses and a low load-store instruction ratio (60%), respectively.</p><p>On average AT-RT (Temp) improves L1 dynamic energy by 10%, and up to 29% on the best benchmark (lbm). AT-RT (Temp+Spat) is able to improve on these results further by reducing L1 dynamic energy by 16% (1.6% total chip energy) compared to the baseline and up to 41% (4.1% total chip energy) on the best benchmark (zeusmp). Both strategies improve on IDist, which is only able  to reduce L1 dynamic energy by 8% on average and 26% on the best benchmark (zeusmp), 0.8% and 2.6% total processor energy, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">AT-RT Table Overhead</head><p>For our evaluation, we chose a large, multi-ported AT-RT table (512 entries, 6,208 B) to maximize reuse. This table is comparable to the other state-of-the-art register-sharing techniques and other baseline CPU components (e.g., memory order validation tables, value predictors). This table size represents about 0.5% of the core area (not including the L1 cache) and has an activation energy two orders of magnitude lower than an L1 access, as it reads very few bits and is direct-mapped. (These overheads are included in Figure <ref type="figure" target="#fig_6">11</ref>.) Figure <ref type="figure" target="#fig_1">12</ref> explores the achievable reuse as a function of the AT-RT table size and shows that the table could be made half as large with little decrease in achievable reuse.</p><p>The largest source of complexity in this design comes the need to simultaneously access the AT-RT table from multiple instructions during rename and commit. For our 8-wide rename and 6wide commit pipeline, this could be as many as 2?8 (read and install if miss) + 6 (reference counter update) simultaneous accesses to the AT-RT table. However, AT-RT only needs to be accessed by load instructions, which account for only 18% of instructions fetched each cycle (32% worst case) in our simulations. Further, a lack of available AT-RT ports at rename simply reduces reuse opportunities, as loads can still be renamed as normal without checking for reuse. A lack of ports at commit would cause a commit stall. In our designs, we modeled an AT-RT table with 4 ports and observed no reuse opportunity losses or commit stalls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Performance</head><p>Figure <ref type="figure" target="#fig_8">13</ref> compares the IPC improvement of the different strategies to the baseline configuration.</p><p>IDist is able to improve performance by 3% on average and up to 7% compared to the baseline by taking advantage of the zero latency of forwarding data from the physical register file instead of the cache for reused registers. DLVP, as expected, provides a more substantial performance improvement, since it is able to prefetch a large percentage of loads (Figure <ref type="figure" target="#fig_3">8</ref>). This means DLVP is able to provide more loads with a zero latency (through prefetching) than IDist (through reuse Early Address Prediction: Efficient Pipeline Prefetch and Reuse forwarding), and thus improve performance over the baseline by 4% on average and up to 13% on the best benchmark.</p><note type="other">39:19</note><p>Since the AT-RT is able to forward more data through the PRF than IDist (shown in Figure <ref type="figure" target="#fig_5">10</ref>), it should provide better performance benefit than IDist. As AT-RT uses the same address predictor as our implementation of DLVP, it also provides the same pipeline prefetching.</p><p>As expected, both AT-RT strategies achieve a similar performance benefit as DLVP, since they provide the same percentage of loads with zero load-to-use latency. Both AT-RT strategies do, however, significantly reduce the number of L1 accesses by loads compared to DLVP and thus also reduce L1 port pressure. This is because AT-RT can access prefetched loads directly from the PRF. The reduction in L1 pressure only translates into a marginal performance improvement: 0.4 percentage points improvement over DLVP and 1.4 percentage point on the best benchmark (bzip2). The small difference in IPC is due to the ability of the out-of-order core to deal with the extra L1 pressure of DLVP. In our simulations, the two L1 ports are able to handle most requests, and when they are not, the large issue width (8 instructions per cycle) and commit width of the core (6 instructions per cycle) is able to accommodate delayed validation loads (introduced by DLVP) with minimal impact. AT-RT (Temp+Spat) further reduces L1 accesses compared to the AT-RT (Temp) but the difference is too small for visible difference in IPC. These results demonstrate that AT-RT is able to achieve the full performance of pipeline prefetching with even better energy savings than register reuse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Security</head><p>AT-RT employs speculation (prefetching and caching data in the PRF) and as such it is important to discuss how exposed our technique is to speculative side-channel attacks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23]</ref>, more specifically the ones that exploit changes made to the memory hierarchy under speculation. Our strategy involves two cache hierarchy levels, the L1 cache and the PRF that in our design is used as an extra cache level.</p><p>When it comes to the L1, the pipeline prefecther only speculatively fetches data that hits in the L1 cache. A cache miss will not trigger a fill request, and as such, it will never modify the contents of any cache in the hierarchy. Any activity by our prefetching/caching mechanism is completely invisible between execution contexts sharing the L1 or any other level in the memory hierarchy.</p><p>The contents of the AT-RT cache itself should also be invisible between contexts, since both the AT-RT table (that holds the tags) and the PRF (that holds the data) are private to the process/thread. On a context switch, the contents of the PRF is saved by the operating system before loading the PRF state of the new context (our solution makes no change to this behavior). We just add the extra step of clearing the AT-RT table during the switch. This is done for correctness, since it is functionally incorrect to keep the tags for data that no longer resides in the PRF. <ref type="foot" target="#foot_6">6</ref> As such, a new thread/process should always find a private AT-RT table and PRF, thus making it impossible to inspect the state of other contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this work, we have demonstrated that having address predictions early in the pipeline allows us to achieve the performance benefits of pipeline prefetching together with the energy benefits of register reuse, all without the overhead of requiring more data movement, data storage in the pipeline, or extra verification hardware/accesses.</p><p>To achieve this, we added address tags to the physical register file to allow us to search it for data returned by loads. This capability, combined with the early address prediction required for pipeline prefetching, allowed us to find data forwarding opportunities through the register file at rename with only the added cost of the register file tags and install coalesced data from reads to improve reuse. Further, we take advantage of the reduced register file pressure from register sharing to intentionally keep register file entries alive for longer than their consuming instructions to increase reuse. This allows us to exceed the spatial and temporal reuse available in previous register-sharing techniques. Finally, as we have addresses for the predicted prefetch addresses and can find data in the physical register file by address, we were able to re-use the existing memory order violation predictor hardware to validate forwarding and register reuse without the need for replaying the prefetch loads or additional hardware, as in previous work.</p><p>Using early address prediction throughout the pipeline enabled us to match the performance of state-of-the-art pipeline prefetching while reducing L1 dynamic energy by 16%, with only the cost of the register file tags (less than 0.5% total core area).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Comparison of when data is available (use) and structures accessed. Register Sharing and AT-RT need only a reference to the register (not the data). Note that the pipeline stages are approximate to show the relative timings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Ratio of loads that a stride address load predictor covers and that DLVP would prefetch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Pipeline with the required structures for pipeline prefetching paired with our AT-RT register reuse structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Value prediction accuracy for all load addresses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Percentage of loads that have their data forwarded from the physical register file.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Executed load instructions (L1 accesses by loads) compared to the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. L1 Dynamic energy of the different strategies compared to the baseline configuration. The AT-RT configurations include the dynamic energy overhead of accessing and updating the AT-RT table.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig.</head><label></label><figDesc>Fig.12. Sensitivity to AT-RT table size. Percentage of loads that have their data forwarded from the physical register file, depending on the number of entries on AT-RT table. We evaluate a 512-entry AT-RT table in this work, but there is little loss in reuse at 256-entries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Performance of the different strategies compared to the baseline configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Gem5 Simulator Configuration</figDesc><table><row><cell>Frequency</cell><cell>3.6 GHz</cell></row><row><cell cols="2">IssueWidth/Ld,St Units 8/2,2</cell></row><row><cell>PRF (integer+float)</cell><cell>180+168 registers</cell></row><row><cell>SQ/LQ/IQ/ROB</cell><cell>56/72/50/224</cell></row><row><cell>iTLB/dTLB</cell><cell>512/512 fully-assoc</cell></row><row><cell>Address Predictor</cell><cell>Tagged 1K entry stride-predictor table with 2-bit confidence</cell></row><row><cell></cell><cell>counter</cell></row><row><cell>Memory Order Valid</cell><cell>SSBF: Tagged 1K entry</cell></row><row><cell>DDT (only for IDist)</cell><cell>Tagged 1K entry table</cell></row><row><cell>AT-RT</cell><cell>512 entry table</cell></row><row><cell>Temp: 3520 Bytes</cell><cell>Tag: 39 bits,</cell></row><row><cell cols="2">Temp+Spat: 6208 Bytes RegID: 8 bits,</cell></row><row><cell></cell><cell>Data Size: 2 bits</cell></row><row><cell></cell><cell>Ref-counter: 5 bits,</cell></row><row><cell></cell><cell>Valid: 1 bit</cell></row><row><cell>Caches</cell><cell>L1I/L1D/L2/L3</cell></row><row><cell>Size</cell><cell>32 KB/32 KB/256 KB/8 MB</cell></row><row><cell>Latency</cell><cell>1c/2c/12c/38c</cell></row><row><cell>Associativity</cell><cell>8w/8w/8w/16w</cell></row><row><cell>DRAM</cell><cell>DDR 3, 1600 MHz, 64 bits</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>12. Sensitivity to AT-RT table size. Percentage of loads that have their data forwarded from the physical register file, depending on the number of entries on AT-RT table. We evaluate a 512-entry AT-RT table in this work, but there is little loss in reuse at 256-entries.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>ACM Transactions on Architecture and Code Optimization, Vol. 18, No. 3, Article 39. Publication date: June 2021.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Note that cache latencies correspond only to the accesses latency. Load-to-use-latency is higher and includes issue and execution latency.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>For energy modeling, we model a 64 entry, 4-way set-associative dTLB to match the first-level TLB of the Intel Skylake architecture.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>For a fair comparison, our implementation of DLVP uses the same address predictor as the AT-RT configurations. ACM Transactions on Architecture and Code Optimization, Vol. 18, No. 3, Article 39. Publication date: June 2021.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>While the standard ISRB does not filter any load instruction, for a fair comparison, we include our modified SSBF filter in the IDist configuration as well. ACM Transactions on Architecture and Code Optimization, Vol. 18, No. 3, Article 39. Publication date: June 2021.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>There would be a slight increase due to mispredicted prefetches, but those are rare, as it can be seen in Figure8. ACM Transactions on Architecture and Code Optimization, Vol. 18, No. 3, Article 39. Publication date: June 2021.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6"><p>Note that it is safe to clear the contents of AT-RT table at any point of the execution. A miss on the AT-RT table will only translate into a reduction in energy savings at worst. There are no correctness implications. ACM Transactions on Architecture and Code Optimization, Vol. 18, No. 3, Article 39. Publication date: June 2021.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamically disabling way-prediction to reduce instruction replay</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Black-Schaffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Design (ICCD&apos;18)</title>
		<meeting>the IEEE International Conference on Computer Design (ICCD&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Addressing energy challenges in filter caches</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Nikoleris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Black-Schaffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on High-performance Computer Architecture (SBAC-PAD&apos;17)</title>
		<meeting>the IEEE International Symposium on High-performance Computer Architecture (SBAC-PAD&apos;17)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Filter caching for free: The untapped potential of the store-buffer</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Black-Schaffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th IEEE International Symposium on Computer Architecture</title>
		<meeting>the 46th IEEE International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="436" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Flexible register management using reference counting</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hempstead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on High-performance Computer Architecture</title>
		<meeting>the IEEE International Symposium on High-performance Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Correlated load-address predictors</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bekerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Jourdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Ronen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilad</forename><surname>Kirshenboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihu</forename><surname>Rappoport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adi</forename><surname>Yoaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Weiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="54" to="63" />
			<date type="published" when="1999">1999</date>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using dynamic cache management techniques to reduce energy in a high-performance processor</title>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Bellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Hajj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantine</forename><surname>Polychronopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Low Power Electronics and Design</title>
		<meeting>the International Symposium on Low Power Electronics and Design</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="64" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Binkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradford</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">R</forename><surname>Hower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somayeh</forename><surname>Sardashti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rathijit</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Korey</forename><surname>Sewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nilay</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
		<idno type="DOI">10.1145/2024716.2024718</idno>
		<ptr target="https://doi.org/10.1145/2024716.2024718" />
	</analytic>
	<monogr>
		<title level="m">The Gem5 simulator</title>
		<imprint>
			<date type="published" when="2011-08">2011. Aug. 2011</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Memory dependence prediction using store sets</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">Z</forename><surname>Chrysos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">S</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Symposium on Computer Architecture</title>
		<meeting>the 25th International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="142" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Standard Performance Evaluation Corporation</title>
		<ptr target="http://www.spec.org/cpu" />
		<imprint>
			<date type="published" when="2006">2006. 2006. 20066</date>
		</imprint>
	</monogr>
	<note>SPEC CPU</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A load-instruction unit for pipelined processors</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">J</forename><surname>Eickemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stamatis</forename><surname>Vassiliadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM J. Res. Devel</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="547" to="564" />
			<date type="published" when="1993">1993. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Continuous optimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fahs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rafacz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lumetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Symposium on Computer Architecture (ISCA&apos;05)</title>
		<meeting>the 32nd International Symposium on Computer Architecture (ISCA&apos;05)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="86" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ARB: A hardware mechanism for dynamic reordering of memory references</title>
		<author>
			<persName><forename type="first">Manoj</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gurindar</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="552" to="571" />
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Speculative Execution Based on Value Prediction. Technion-IIT</title>
		<author>
			<persName><forename type="first">Freddy</forename><surname>Gabbay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical Engineering</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reducing leakage in power-saving capable caches for embedded systems by using a filter cache</title>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Giorgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Bennati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Memory Performance: Dealing with Applications, Systems and Architecture</title>
		<meeting>the Workshop on Memory Performance: Dealing with Applications, Systems and Architecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speculative execution via address prediction and data prefetching</title>
		<author>
			<persName><forename type="first">Jos?</forename><surname>Gonz?lez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Gonz?lez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Supercomputing</title>
		<meeting>the International Conference on Supercomputing</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="196" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A novel renaming scheme to exploit value temporal locality through physical register reuse and unification</title>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Jourdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Ronen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bekerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bishara</forename><surname>Shomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adi</forename><surname>Yoaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 31st ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="216" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The alpha 21264 microprocessor</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="24" to="36" />
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The filter cache: An energy efficient memory structure</title>
		<author>
			<persName><forename type="first">Johnson</forename><surname>Kin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">H</forename><surname>Mangione-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 30th ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="184" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spectre attacks: Exploiting speculative execution</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Kocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jann</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Fogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Genkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gruss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Werner</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Hamburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Lipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Mangard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Prescher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Security and Privacy (SP&apos;19)</title>
		<meeting>the IEEE Symposium on Security and Privacy (SP&apos;19)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CACTI-P: Architecture-level modeling for SRAM-based structures with advanced leakage reduction techniques</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><surname>Ho Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><forename type="middle">B</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer-aided Design</title>
		<meeting>the International Conference on Computer-aided Design</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="694" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Value locality and load value prediction</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lipasti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 7th International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exceeding the dataflow limit via value prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mikko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lipasti</surname></persName>
		</author>
		<author>
			<persName><surname>Paul Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 29th ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="226" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Lipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gruss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Prescher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Werner</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Mangard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Kocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Genkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Yarom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Hamburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01207</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Meltdown. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamic speculation and synchronization of data dependences</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Breach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Terani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gurindar</forename><forename type="middle">S</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="181" to="193" />
			<date type="published" when="1997">1997</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<idno>HPL- 2009-85</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>HP Labs</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Load and store reuse using register file contents</title>
		<author>
			<persName><forename type="first">Soner</forename><surname>?nder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Supercomputing</title>
		<meeting>the 15th International Conference on Supercomputing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="289" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">AVPP: Address-first value-next predictor with value prefetching for improving the efficiency of load value prediction</title>
		<author>
			<persName><forename type="first">Lois</forename><surname>Orosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodolfo</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">49</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Register sharing for equality prediction</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Perais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">A</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr?</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 49th IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">EOLE: Paving the way for an effective implementation of value prediction</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Perais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr?</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE 41st International Symposium on Computer Architecture (ISCA&apos;14</title>
		<meeting>the ACM/IEEE 41st International Symposium on Computer Architecture (ISCA&apos;14</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="481" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Practical data value speculation for future high-end processors</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Perais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr?</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 20th International Symposium on High-performance Computer Architecture (HPCA&apos;14</title>
		<meeting>the IEEE 20th International Symposium on High-performance Computer Architecture (HPCA&apos;14</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="428" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">BeBoP: A cost effective predictor infrastructure for superscalar value prediction</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Perais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr?</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 21st International Symposium on High-performance Computer Architecture (HPCA&apos;15)</title>
		<meeting>the IEEE 21st International Symposium on High-performance Computer Architecture (HPCA&apos;15)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="13" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cost effective physical register sharing</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Perais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr?</forename><surname>Seznec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on High-performance Computer Architecture (HPCA&apos;16)</title>
		<meeting>the IEEE International Symposium on High-performance Computer Architecture (HPCA&apos;16)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="694" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cost-effective speculative scheduling in high performance processors</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Perais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andr?</forename><surname>Seznec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Michaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Sembrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Hagersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE 42nd International Symposium on Computer Architecture (ISCA&apos;15)</title>
		<meeting>the ACM/IEEE 42nd International Symposium on Computer Architecture (ISCA&apos;15)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="247" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Three extensions to register integration</title>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Petric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Bracy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;02)</title>
		<meeting>the 35th IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;02)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="37" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">RENO: A rename-based instruction optimizer</title>
		<author>
			<persName><forename type="first">Vlad</forename><surname>Petric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingting</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Symposium on Computer Architecture (ISCA&apos;05)</title>
		<meeting>the 32nd International Symposium on Computer Architecture (ISCA&apos;05)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="98" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The superfluous load queue</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;18)</title>
		<meeting>the 51st IEEE/ACM International Symposium on Microarchitecture (MICRO&apos;18)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="95" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Store vulnerability window (SVW): Re-execution filtering for enhanced load optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Symposium on Computer Architecture (ISCA&apos;05)</title>
		<meeting>the 32nd International Symposium on Computer Architecture (ISCA&apos;05)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="458" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Physical register reference counting</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Archit. Lett</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="9" to="12" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Load value prediction via path-based address prediction: Avoiding mispredictions due to conflicting stores</title>
		<author>
			<persName><forename type="first">Rami</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harold</forename><forename type="middle">W</forename><surname>Cain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raguram</forename><surname>Damodaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 50th IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="423" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dynamic instruction reuse</title>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Sodani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gurindar</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Symposium on Computer Architecture (ISCA&apos;97)</title>
		<meeting>the 24th International Symposium on Computer Architecture (ISCA&apos;97)</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multithreaded value prediction</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Tuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Symposium on High-performance Computer Architecture</title>
		<meeting>the 11th International Symposium on High-performance Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="5" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Highly accurate data value prediction using hybrid predictors</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manoj</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 30th ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="281" to="290" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
