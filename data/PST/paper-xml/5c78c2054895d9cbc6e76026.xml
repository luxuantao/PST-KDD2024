<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BLIND DETECTION OF PHOTOMONTAGE USING HIGHER ORDER STATISTICS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
							<email>ttng@ee.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
							<email>sfchang@ee.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qibin</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Infocomm Research</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">BLIND DETECTION OF PHOTOMONTAGE USING HIGHER ORDER STATISTICS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">274A40D663CDBD40EA8A12C02886A604</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we investigate the prospect of using bicoherence features for blind image splicing detection. Image splicing is an essential operation for digital photomontaging, which in turn is a technique for creating image forgery. We examine the properties of bicoherence features on a data set, which contains image blocks of diverse image properties. We then demonstrate the limitation of the baseline bicoherence features for image splicing detection. Our investigation has led to two suggestions for improving the performance of the bicoherence features, i.e., estimating the bicoherence features of the authentic counterpart and incorporating features that characterize the variance of the feature performance. The features derived from the suggestions are evaluated with Support Vector Machine (SVM) classification and shown to improve the image splicing detection accuracy from 62% to about 70%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Label</head><p>Feature Name Orig magnitude and phase features { fM, fP } Delta Prediction discrepancy { fM, fP } Edge Edge percentage fE</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Photomontage refers to a paste-up produced by sticking together photographic images. In olden days, creating a good composite photograph required sophisticated skills of darkroom masking or multiple exposures of a photograph negative. In today's digital age, however, the creation of photomontage is made simple by the cut-and-paste tools of the popular image processing software such as Photoshop. With such an ease of creating good digital photomontages, we could no longer take image authenticity for granted especially when it comes to legal photographic evidence <ref type="bibr" target="#b1">[1]</ref> and electronic financial documents. Therefore, we need a reliable and objective way to examine image authenticity.</p><p>Lack of internal consistency, such as inconsistencies in object perspective, in an image is sometimes a telltale sign of photomontage <ref type="bibr" target="#b1">[1]</ref>. However, unless the inconsistencies are obvious, this technique can be subjective. Furthermore, forgers can always take heed of any possible internal inconsistencies.</p><p>Although image acquisition device with digital watermarking features could be a boon for image authentication, presently there still is not a fully secured authentication watermarking algorithm, which can defy all forms of hacking, and the hardware system has to secure from unauthorized watermark embedding. Equally important are the issues such as the need for both the watermark embedder and detector to use a common algorithm and the consequence of digital watermarks degrading image quality.</p><p>On the premise that human speech signal is highly Gaussian in nature <ref type="bibr" target="#b2">[2]</ref>, a passive approach was proposed <ref type="bibr" target="#b3">[3]</ref> to detect the high level of non-gaussianity in spliced human speech using bicoherence features. Unlike human speech signal, the premise of high guassianity does not hold for image signal. It was shown <ref type="bibr" target="#b4">[4]</ref> that bispectrum and trispectrum of natural images have a concentration of high values in regions where frequency components are aligned in orientation, due to image features of zero or one intrinsic dimensionality such as uniform planes or straight edges. As images originally have high value in higher order spectrum, detecting image splicing based on the same principle of increased non-gaussianity would be a very low signal-to-noise problem, not to mention the possible complex interaction between splicing and the non-linear image features.</p><p>Recently, a new system for detecting image manipulation based on a statistical model for 'natural' images in the wavelet domain is reported <ref type="bibr" target="#b5">[5]</ref>. Image splicing is one kind of image tampering the system takes on; however, no further detail about the technical approach is provided in the article.</p><p>Image splicing is defined as a simple joining of image regions. We currently do not address the combined effects of image splicing and other post-processing operations. Creation of digital photomontage always involves image splicing although users could apply post-processing such as airbrush style edge softening, which can potentially be detected by other techniques <ref type="bibr" target="#b5">[5]</ref>. In fact, photomontages with merely image splicing, as in Figure <ref type="figure">1</ref>, can look deceivingly authentic and each of them only took a professional graphic designer 10-15 minutes to produce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Spliced images that look authentic subjectively</head><p>In this paper, we pursue the prospect of grayscale image splicing detection using bicoherence features. We first examine the properties of the proposed bicoherence features <ref type="bibr" target="#b3">[3]</ref> in relation to image splicing and demonstrate the insufficiency of the features. We then propose two new methods on improving the performance of the bicoherence features for image splicing detection. Lastly, we evaluate the methods using SVM classification experiments over a diverse data set of 1845 image blocks. More details about this work are included in <ref type="bibr" target="#b6">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BICOHERENCE</head><p>Bicoherence is a normalized bispectrum, i.e., the third order correlation of three harmonically related Fourier frequencies of a signal, X( ) <ref type="bibr" target="#b7">[7]</ref>:</p><formula xml:id="formula_0">) , ( ( 2 1 2 2 1 2 2 1 2 1 * 2 1 2 1 2 1 ) , ( ] ) ( [ ] ) ( ) ( [ )] ( ) ( ) ( [ ) , ( ω ω ω ω ω ω ω ω ω ω ω ω ω ω b j e b X E X X E X X X E b Φ = + + =</formula><p>When the harmonically related frequencies and their phase are of the same type of relation, i.e., when there exists (ω 1, φ 1 ), (ω 2, φ 2 ) and (ω 1+ ω 2, φ 1+ φ 2 ) for X( ), b(ω 1 ,ω 2 ) will have a high magnitude value and we call such phenomena quadratic phase coupling (QPC). As such, the average bicoherence magnitude would increase as the amount of QPC grows. Besides that, bicoherence is insensitive to signal gaussianity and bispectrum is often used as a measure of signal non-gaussianity <ref type="bibr" target="#b8">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Bicoherence Features</head><p>Motivated by the effectiveness of the bicoherence features used for human-speech splicing detection <ref type="bibr" target="#b3">[3]</ref>, similar features are extracted from a bicoherence with</p><formula xml:id="formula_1">• Mean of magnitude: M = | | -1 |b( 1 , 2 )| • Negative phase entropy: P= n p( n )log p( n ) where ={( 1 , 2 )| 1 =(2 m 1 )/M, 2 =(2 m 2 )/M, m 1 , m 2 = 0,…,.M-1} p( n )= | | -1 1( (b( 1 , 2 )) n ) , 1(•)=indicator function n ={ |-+(2 n)/N &lt; -+2 (n+1)/N}, n=0,…, N-1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Estimation of Bicoherence Features</head><p>With limited data sample size, instead of computing 2-D bicoherence features from an image, 1-D bicoherence features can be computed from N v vertical and N h horizontal image slices of an image and then combined as follows:</p><formula xml:id="formula_2">2 1 2 1 ) ( ) ( + = i Vertical i N i Horizontal i N M M fM v h 2 1 2 1 ) ( ) ( + = i Vertical i N i Horizontal i N P P fP v h</formula><p>In order to reduce the estimation variance, the 1-D bicoherence of an image slice is computed by averaging segment estimates:</p><formula xml:id="formula_3">+ + = k k k k k k k k k X k X X k X X X k b 2 2 1 2 2 1 2 1 * 2 1 2 1 ) ( 1 ) ( ) ( 1 ) ( ) ( ) ( 1 ) , ( ˆω ω ω ω ω ω ω ω ω ω</formula><p>We use segments of 64 samples in length with an overlap of 32 samples with adjacent segments. For lesser frequency leakage and better frequency resolution, each segment of length 64 is multiplied with a Hanning window and then zero-padded from the end before computing 128-point DFT of the segment.</p><p>In Fackrell et al. <ref type="bibr">[9]</ref>, it is suggested that N data segments should be used in the averaging procedure for estimating a Npoint DFT bispectrum of a stochastic signal. Overall, we use 768 segments to generate features for a 128x128-pixel image block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">IMAGE DATA SET</head><p>Our data set <ref type="bibr" target="#b10">[10]</ref> is collected with sample diversity in mind. It has 933 authentic and 912 spliced image blocks of size 128 x 128 pixels. The image blocks are extracted from images in CalPhotos image set <ref type="bibr" target="#b11">[11]</ref>. As the images are contributions from photographers, in our case, they can be considered as authentic i.e., not digital photomontages.</p><p>The authentic category consists of image blocks of an entirely homogenous textured or smooth region and those having an object boundary separating two textured regions, two smooth regions, or a textured regions and a smooth region. The location and the orientation of the boundaries are random.</p><p>The spliced category has the same subcategories as the authentic one. For the spliced subcategories with object boundaries, image blocks are obtained from images with spliced objects; hence, the splicing region interface coincides with an arbitrary-shape object boundary. Whereas for the spliced subcategories with an entirely homogenous texture or smooth region, image blocks are obtained from those in the corresponding authentic subcategories by copying a vertical or a horizontal strip of 20 pixels wide from one location to another location within a same image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PROPERTIES OF BICOHERENCE FEATURES</head><p>We are interested in investigating the performance of bicoherence features in detecting spliced images on the three object interface types for which such performance varies over, i.e. smooth-smooth, textured-textured, and smooth-textured. Figure <ref type="figure">2</ref> shows the scatter plot of the bicoherence magnitude feature (fM) of the authentic and spliced image blocks with a particular object interface type. The plots also show how well the edge percentage (y-axis) captures the characteristics of different interface types. The edge pixels are obtained using Canny edge detector. The edge percentage is computed by counting the edge pixels within each block. As the plots for bicoherence phase feature (fP) are qualitatively similar, they are omitted due to space constraints. We observe that the performance of the bicoherence feature in distinguishing spliced images varies for different object interface types, with textured-textured object interface type being the worst case. Figure <ref type="figure">3</ref> shows the distribution of the features for the authentic and spliced image categories. We can observe that the distributions of the two image block categories are greatly overlapped, although there are noticeable differences in the peak locations and the heavy tails. Hence, we would expect poor classification between the two categories if the features were to be used directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">METHODS FOR IMPROVING THE PERFORMANCE OF BICOHERENCE FEATURES</head><p>Our investigation on the properties of bicoherence features for images leads to two methods for augmenting the performance of the bicoherence features in detecting image splicing: 1. By estimating the bicoherence features of authentic images. 2. By incorporating image features that capture the image characteristics on which the performance of the bicoherence features varies, e.g., edge pixel percentage feature (fE) capture the characteristics of different object interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Estimating Authentic Counterpart Bicoherence Features</head><p>Assume that for every spliced image, there is a corresponding authentic counterpart, which is similar to the spliced image except that it is authentic. The rationale of the approach, formulated as below, is that if the bicoherence features of the authentic counterpart can be estimated well, the elevation in the bicoherence features due to splicing could be more detectable.</p><formula xml:id="formula_4">ε ε ε + ∆ + ≈ + Λ + Λ ≈ + Λ Λ = Splicing Authentic S I S I Bic f f s s image g image g s s image image g f ) ), , ( ( )) ( ( ) ), ,<label>( ), ( ( 2 1</label></formula><p>where I is a set of splicing-invariant features while S is a set of features induced by splicing, s is a splicing indicator and is the estimation error. In this formulation, g 1 corresponds to an estimate of the bicoherence feature of the authentic counterpart, denoted as f Authentic and g 2 corresponds to the elevation of the bicoherence feature induced by splicing, denoted as f Splicing . With f Splicing , splicing would be more detectable after the significant interference from the splicing-invariant component, g 1 , is removed. f Splicing can be estimated with f Bic -f Authentic , which we call prediction discrepancy. The f Authentic estimation performance would be determined by two factors, i.e., how much we capture the splicing-invariant features and how well we map the splicing-invariant features to the bicoherence features.</p><p>A direct way to arrive at a good estimator is through an approximation of the authentic counterpart obtained by depriving an image of the splicing effect. As a means of 'cleaning' an image of its splicing effect, we have chosen the texture decomposition method based on functional minimization <ref type="bibr" target="#b12">[12]</ref>, which has a good edge preserving property, for we have observed the sensitivity of the bicoherence features to edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Texture Decomposition with Total Variation Minimization and a Model of Oscillating Function</head><p>In functional representation, an image, f defined in R 2 , can be decomposed into two functions, u and v, within a total variation minimization framework with a formulation <ref type="bibr" target="#b12">[12]</ref>:</p><formula xml:id="formula_5">{ } v u f v u u E u + = + ∇ = Ω , ) ( inf * λ</formula><p>where the u component, a structure component of the image, is modeled as a function of bounded variation while the v component, representing the fine texture or noise component of the image, is modeled as an oscillation function. ||•|| * is the norm of the oscillating function space and is a weight parameter for trading off variation regularization and image fidelity.</p><p>The minimization problem can be reduced to a set of partial differential equations known as Euler-Lagrange equations and solved numerically with finite difference technique. As the structure component could contain arbitrarily high frequencies, conventional image decomposition by filtering could not attain such desired results. In this case, the structure component will serve as an approximation for the authentic counterpart, hence, the estimator for fM Authentic and fP Authentic are respectively  We evaluate effectiveness of the estimator, as shown in Figure <ref type="figure" target="#fig_3">5</ref> using the prediction discrepancy for the magnitude and phase features. Our objective is to show that the new features ( fM, fP) have a stronger discrimination power between authentic and spliced compared to the original features (fM, fP). This objective is partially supported by observing the difference between Figure <ref type="figure" target="#fig_3">5</ref> and Figure <ref type="figure">3</ref> (In Figure <ref type="figure" target="#fig_3">5</ref>, two distributions are more separable) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">SVM CLASSIFICATION EXPERIMENTS</head><p>We herein evaluate the effectiveness of the features, which are derived from the proposed method, i.e., prediction discrepancy and edge percentage using our data set. SVM classifications with RBF kernel are performed with parameters chosen for ensuring no overfitting as verified by 10-fold cross-validation. Three statistics obtained from 100 runs of classification are used to evaluate the performance of feature sets:</p><p>• Accuracy mean:</p><formula xml:id="formula_6">• • + + = i i A i S i A A i S S accuracy N N N N M ) ( ) ( | | | | 100 1 •</formula><p>Average precision:  all the proposed features is 71 % in M accuracy , which is 8.9 % better than the baseline method (first row).</p><formula xml:id="formula_7">• = i i S i S S</formula><p>The results are encouraging as it shows the initial promise of the authentic counterpart estimation. The third observation may be an indication that the prediction discrepancy features are less affected by image texturedness. Hence, if the estimation of the authentic counterpart bicoherence features can be further improved, it may help in the classification of the toughest case where the object interface type is textured-textured.</p><p>The block level detection results can be combined in different ways to make global decision about the authenticity of a whole image or its sub-regions. For example, Figure <ref type="figure" target="#fig_5">6</ref> illustrates the idea of localizing the suspected splicing boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>In this paper, we have shown the difficulties of image splicing detection using bicoherence features, despite the technique being effective on human speech signals. We have also empirically shown how the performances of the bicoherence features depending on the different object interface types. Two methods are proposed for improving the capability of the bicoherence features in detecting image splicing. The first exploits the dependence of the bicoherence features on the image content such as edge pixel density and the second offsets the splicinginvariant component from bicoherence and thereby obtains better discriminative features. Finally, we observe improvements in SVM classification after the derived features are incorporated. This is the first step of our effort in using bicoherence features for image splicing detection. We will next seek a model to get an insight on why bicoherence is sensitive to splicing, from which other effective features can be derived.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Bicoherence magnitude feature for different object interface types</figDesc><graphic coords="2,318.48,572.80,114.73,85.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Examples of texture decomposition For the linear prediction discrepancies between the bicoherence features of an image and those of its authentic counterpart, i.e., Authentic M f fM fM α -= ∆</figDesc><graphic coords="3,317.98,254.16,235.13,57.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Distribution of prediction discrepancy</figDesc><graphic coords="3,327.70,513.84,108.96,86.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>and A represents Spliced and Authentic respectively and N i A|B denotes the number of samples B detected as A in the ith run. The results of the experiment are shown below:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Spliced image blocks (marked with a red box)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>:</head><label></label><figDesc>Statistical t-tests for classification results using feature set {fM, fP} against all other results are performed. The null hypothesis (i.e., the mean of the two results are the same) is rejected at a 0.05 significance level for all tests.</figDesc><table><row><cell>Feature Set</cell><cell>M accuracy</cell><cell>M precision</cell><cell>M recall</cell></row><row><cell>Orig</cell><cell>0.6259</cell><cell>0.6354</cell><cell>0.5921</cell></row><row><cell>Delta</cell><cell cols="2">0.6876 (+6.2 %) 0.6685</cell><cell>0.7477</cell></row><row><cell>Orig+Delta</cell><cell cols="2">0.7028 (+7.7 %) 0.6725</cell><cell>0.7925</cell></row><row><cell>Orig+Edge</cell><cell cols="2">0.7005 (+7.5 %) 0.6780</cell><cell>0.7667</cell></row><row><cell>Delta+Edge</cell><cell cols="2">0.6885 (+6.3 %) 0.6431</cell><cell>0.8517</cell></row><row><cell>Orig+Delta+Edge</cell><cell cols="2">0.7148 (+8.9 %) 0.6814</cell><cell>0.8098</cell></row><row><cell cols="4">NoteBelow are the observations from the classification results:</cell></row><row><cell cols="4">1. Prediction discrepancy features alone obtain 6.2 %</cell></row><row><cell cols="4">improvement in M accuracy over the original bicoherence</cell></row><row><cell>features.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">2. Edge percentage improves the performance of the</cell></row><row><cell cols="3">bicoherence features by 7.5 % in M accuracy .</cell><cell></cell></row><row><cell cols="4">3. Prediction discrepancy and edge percentage are redundant</cell></row><row><cell cols="2">with respect to each other.</cell><cell></cell><cell></cell></row><row><cell cols="4">4. The best performance (last row) obtained by incorporating</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGEMENTS</head><p>Gratitude to A*STAR, Singapore for sponsoring the first author, to Lexing Xie, Shahram Ebadollahi and Anita Huang for their discussions and helps, and to Huei-Sim Tang for the professionally produced photomontages.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">When Is Seeing Believing?</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="page" from="44" to="49" />
			<date type="published" when="1994-02">February 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Detecting nonlinearities in speech sounds using the bicoherence</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W A</forename><surname>Fackrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mclaughlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Institute of Acoustics</title>
		<meeting>of the Institute of Acoustics</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="123" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting Digital Forgeries Using Bispectral Analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIM-1657, MIT AI Memo</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Higher-order statistics of natural images and their exploitation by operators selective to intrinsic dimensionality</title>
		<author>
			<persName><forename type="first">G</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zetzsche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Signal Processing Workshop on HOS</title>
		<meeting>of IEEE Signal essing Workshop on HOS</meeting>
		<imprint>
			<date type="published" when="1997-07">July 1997</date>
			<biblScope unit="page" from="21" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Picture Tells a Thousand Lies</title>
		<author>
			<persName><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Scientist</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page" from="38" to="41" />
			<date type="published" when="2003">2411. Sept. 6, 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Blind Image Splicing and Photomontage Detection Using Higher Order Statistics</title>
		<author>
			<persName><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<idno>201-2004-1</idno>
		<ptr target="http://www.ee.columbia.edu/dvmm/" />
		<imprint>
			<date type="published" when="2004-01">Jan 2004</date>
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">ADVENT Technical Report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Digital Bispectral Analysis and its Applications to Nonlinear Wave Interactions</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Powers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Plasma Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="120" to="131" />
			<date type="published" when="1979-06">June 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An estimate of the cosmological bispectrum from the MAXIMA-1 CMB map</title>
		<author>
			<persName><forename type="first">M</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">241302</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The interpretation of the bispectra of vibration signals-I. Theory</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W A</forename><surname>Fackrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Pinnington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Parsons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Mechanical Systems and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="266" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Data set of authentic and spliced image blocks</title>
		<ptr target="http://www.ee.columbia.edu/dvmm/researchProjects/AuthenticationWatermarking/BlindImageVideoForensic/" />
		<imprint/>
		<respStmt>
			<orgName>DVMM, Columbia Univ.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A database of photos of plants, animals, habitats and other natural history subjects</title>
		<author>
			<persName><forename type="first">Calphotos</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Library Project</title>
		<imprint/>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling Textures with Total Variation Minimization and Oscillating Patterns in Image Processing</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Vese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UCLA C.A.M. Report</title>
		<imprint>
			<biblScope unit="page" from="2" to="19" />
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
