<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online POI Recommendation: Learning Dynamic Geo-Human Interactions in Streams</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-19">19 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dongjie</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kunpeng</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
						</author>
						<title level="a" type="main">Online POI Recommendation: Learning Dynamic Geo-Human Interactions in Streams</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-19">19 Jan 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2201.10983v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Online POI Recommendation</term>
					<term>User Modeling</term>
					<term>Geographical Contexts Modeling</term>
					<term>Reinforcement Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point of Interest (POI) recommendation aims to model human spaio-temporal activities at various POIs in stream and dynamically recommend next POIs to users over time. Since the preferences and activity patterns of humans change over time, online recommendation is critical to learn dynamic preferences and environment states in POI visit streams. After analyzing massive mobility data, we observe that there is dynamic and mutual interactions between users and geospatial contextual environments (e.g., POIs, POI categories, functional zones, etc.) during visits. Modeling such geo-human interactions in streams can provide better online POI recommendations. While there are studies in online recommendation, the integration of geo-human interaction and online recommendation with streams is still in early stage. To fill this gap, in this paper, we focus on the problem of modeling dynamic geo-human interactions in streams for online POI recommendations. Specifically, we formulate the in-stream geo-human interaction modeling problem into a novel deep interactive reinforcement learning framework, where an agent is a recommender and an action is a next POI to visit. We uniquely model the reinforcement learning environment as a joint and connected composition of users and geospatial contexts (POIs, POI categories, functional zones). An event that a user visits a POI in stream updates the states of both users and geospatial contexts; the agent perceives the updated environment state to make online recommendations. In our preliminary work <ref type="bibr" target="#b25">[26]</ref>, we model a single-user visit stream by separately embedding a user and the static knowledge graph of geospatial contexts, and then concatenating both representations to predict recommendations. In the extended work, we model a mixed-user event stream by unifying all users, visits, and geospatial contexts as a dynamic knowledge graph stream, in order to model human-human, geo-human, geo-geo interactions. We design an exit mechanism to address the expired information challenge, devise a meta-path method to address the recommendation candidate generation challenge, and develop a new deep policy network structure to address the varying action space challenge, and, finally, propose an effective adversarial training method for optimization. Finally, we present extensive experiments to demonstrate the enhanced performance of our method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Online Point of Interest (POI) recommendation aims to model human spaiotemporal activities at various POIs in stream, in order to dynamically recommend next POIs to users over time. Online POI recommendation can help user to discover attractive places, provide personalized and usercentric human-technology interfaces (e.g., Google Maps, Foursquare, Yelp), improve user experiences and revenues in location based social network applications, and, moreover, create a better geo-social community.</p><p>Most of prior literature in POI recommendations takes a default setting: offline learning. With that being said, many offline trained POI recommendation models <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b35">[36]</ref> learn user visit preferences from historical checkin data, and recommend next visit POIs to users based on the static learned preferences However, the preferences and activity patterns of humans indeed change over time. Unlike offline recommendation, online recommendation assumes that user activity data (e.g., POI visits) are generated in stream, and human preferences and learning environments vary over time. Moreover, by analyzing large-scale POI visit data, we observe that there is dynamic and mutual influence between users and geospatial contexts (e.g., POIs,</p><p>• Dongjie Wang, Kunpeng Liu, Yanjie Fu are with the Department of Computer Science, University of Central Florida. Emails: {dongjie.wang,kunpeng.liu}@knights.ucf.edu, yanjie.fu@ucf.edu. • Hui Xiong is with Department of Management Science and Information Systems, Rutgers University. Email: hxiong@rutgers.edu.</p><p>POI categories, functional zones, etc.) in POI visit streams. For example, after visiting the New York City Metropolitan Museum, a user visits the Statue of Liberty in New York city. Each visit event will update the state of a user, as well as update the state of the connected network of NYC tourism attractions. Users and the NYC tourism attraction network will mutually influence and be influenced over time, which we call geo-human interactions. Modeling such varying and influential interactions in streams can help us to better understand how users evolve and how users influence and are influenced by a geospatial contextaul and networked environment in the scenario of POI recommendation. While there are studies in online recommendation <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b37">[38]</ref>, the integration of dynamic geo-human interaction and online recommendation with streams is still in early stage.</p><p>To fill this gap, in this paper, we focus on modeling the geo-human interactions in streams for online POI recommendation. We formulate the in-stream geo-human interaction modeling problem into a reinforcement learning task. The underlying idea is to regard a reinforcement learning agent as a recommender and regard the reinforcement learning environment as a joint composition of users and geospatial contexts (e.g., POIs, POIs categories, and functional zones), so that we can leverage the agent-environment interaction in reinforcement learning to model the interaction between users and geosaptial contexts . To that end, we propose to develop a novel deep interactive reinforcement learning framework to unify both in-stream recommendation and geo-human interaction modeling.</p><p>Our proposed learning framework includes two major modules: (1) a representation module and (2) an imitation module. Firstly, the representation module is to learn and track the state representations of users and geospatial contexts. Secondly, the imitation module is to utilize the representations of users and geospatial contexts as state and treat recommended POIs as actions, to mimic user visit behaviors. Thirdly, the mimics accuracy of user visit patterns is fed back to guide the parameter optimization of the representation and imitation modules. When the imitation module perfectly mimic user visit patterns the representation module produces the most accurate embeddings of interactive users and geospatial contexts. Finally, the trained imitation module will select user-POI pairs with the highest Q value and make updated recommendations on the fly.</p><p>Based on the overarching idea, we have conducted a preliminary study <ref type="bibr" target="#b25">[26]</ref>. In this preliminary study, we only focused on modeling a single-user POI visit stream, instead of a mixed-user POI visit stream, to simplify online recommendation. Particularly, the representation module separately learned the representations of the user and the geospatial contexts. Then, we concatenated the representations of a user and geospatial contexts together as the state of environment. The imitation module took the environment state as input and exploit its policy network to make personalized next POI recommendation to the user.</p><p>However, the modeling of dynamic interaction in streams can be significantly improved. Specifically, user visit intents exhibit multi-level interaction dynamics: 1) connected dynamics: a user is impacted by other users and POI-related geographic entities through various types of visit events. Such relationship forms a more comprehensive knowledge graph that includes not just spatial entities but also users; 2) topological dynamics: new POI visit events keep adding new entities (users) and edges (visit events) into the knowledge graph; 3) semantic dynamics: a user usually connects with POIs through unique semantic relationships via diverse meta-path schemes. How to capture these dynamic characteristics become the key point to improve the POI recommendation performance. Therefore, it is appealing to model multi-level interaction dynamics for improving online recommendation in streams.</p><p>To this end, in our journal version, we consider a new data environment setup: a mixed-user visit stream. Based on the mixed-user visit stream, we construct a new dynamic knowledge graph, in which nodes are both users and geospatial entities, edges stand for user-user, geo-user, and geo-geo relationships, and, moreover, edges and nodes are added and deleted over time. This setup is fundamentally different from the data environment setup of our preliminary study: (1) single-user visit stream; (2) nodes are geospatial entities only, excluding users; (3) nodes and edges will not be deleted. However, the new data environment setup raises three algorithmic challenges. Firstly, because there are millions of users, the new KG will grow exponentially larger over time. To solve the exploding challenge, we first design an exit mechanism for long-time-ago visit-events to remove outdated records for KG refinement. Secondly, because the new KG will be larger and more sparse, it is challenging to identify POI candidates from a sparse and huge KG. To overcome the sparsity challenge, we propose and leverage predefined meta-paths to select the most relevant POI candidates for recommendation with awareness of semantics and interests' context. Finally, because the node set of the new KG is dynamic, it creates dynamic action space and makes recommendation policy difficult to train. To address the varying challenge, we develop a new deep neural policy network architecture to support the dynamic action space.</p><p>In summary, in this paper, we propose a deep interactive reinforcement learning framework for online POI recommendation framework. Our unique perspective is to model the multilevel dynamics of geo-human interactions in data streams. Specifically, our contributions are:</p><p>(1) Formulating the problem of modeling geo-human interactions in streams for online POI recommendation. We identify that human visit activities are an interactive mutually-influenced process between users and geospatial entities. We propose a new perspective to attack online POI recommendation by unifying in-stream recommendation and dynamic geo-human interaction modeling.</p><p>(2) Proposing a representation-imitation based deep interactive reinforcement learning framework. To address the in-stream dynamic interaction modeling problem, we propose a novel deep interactive reinforcement learning framework. This framework includes a representation module and an imitation module. The former is to conduct dynamic knowledge graph learning to learn the state representations of users and geospatial entities. The latter is to perform online next POI recommendations via policy networks. The two modules form a closed-loop interactive learning system.</p><p>(3) Developing technical solutions to tackle multilevel interaction dynamics in mixed-user data streams. We identify the challenges arise in three levels of dynamics in the interactions in mixed-user data streams, including connected dynamics, topological dynamics, and semantic dynamics. To address these challenges, we propose effective technical solutions. Firstly, we propose an exit mechanism to maintain the dynamic KG on a reasonable scale and reduce computational complexity. Secondly, we develop a metapath based candidates generation method for overcoming the challenge caused by sparse KG; Thirdly, we devise a new policy network that generates Q-values for selecting recommended user-POI pairs to address the challenge of dynamic action space.</p><p>(4) Extensive experiments with real world data. We evaluate our method using two real world datasets. For comparison we implemented six algorithms. Results show that our method consistently outperformed the competing methods to justify our technical insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DEFINITIONS AND PROBLEM STATEMENT</head><p>We firstly introduce the key definitions, and then present the problem statement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Key Components of Our Framework</head><p>We aim to address the joint task of both online recommendation in streams and dynamic geo-human interaction modeling. We develop a deep interactive reinforcement learning framework that includes: </p><formula xml:id="formula_0">r = σ(λ d ×(r d −b d )+λ c ×(r c −b c )+λ p ×(r p −b p )),<label>(1)</label></formula><p>where λ d , λ c and λ p represent the weights for r d , r c and r p respectively; σ denotes the sigmoid function that smooths the distribution of the reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Problem Statement</head><p>We formulate the online POI recommendation problem into a joint task of recommendation in streams and dynamic geohuman interaciton modeling. We propose a deep interactive reinforcement learning framework. In the framework, we regard all users and geospatial contexts (POIs, POI categories, functional zones) are regarded as environment, and we train a policy network to imitate the decision-making process of the user visits based on the state of the environment. Formally, at the time step l, given the state of the environment s l , our propose is to find a mapping function f :</p><formula xml:id="formula_1">s l → a l+1 .</formula><p>Here, a l+1 is the user next visit action in the time step l + 1. Thus, the function takes s l as input, and outputs a l+1 . During the mapping process, the state of the environment s evolves over user visit events. The objective is to model user-user, user-geo, geo-geo interactions and maximize the recommendation reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework Overview</head><p>Figure <ref type="figure" target="#fig_4">1</ref> shows our framework includes two modules: representation and imitation. Firstly, the representation module is to learn the state of the environment. In particular, this module focuses on preserving the dynamic mutual interaction between user and geospatial contexts when learning the state embeding vector. Secondly, the imitation module is to predict next-visit POI and evaluate the reward value of the POI recommendation. In particular, the imitation module takes the state of the environment as input, and predict the next-visit POI (action) of a given user. The reward function is later exploited to evaluate the effectiveness of recommendation. Finally, we leverage the reward value as a feedback to guide the parameters update of the representation module. The representation module and imitation module enhance each other through iterative updates of reward values, representation parameters, and recommendation policy network parameters. When the representation module learn the most accurate representation of the dynamic knowledge graph of users and geospatial contexts, and the imitation module perfectly mimic users' next vist patterns, we will use the imitation module to perform online POI recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A Simplified Method with Static Spatial Knowledge Graph</head><p>Our preliminary work <ref type="bibr" target="#b25">[26]</ref> regards geospatial contexts (POIs, POI categories, functional zones) as a spatial KG, learn the representations of users and geospatial contexts separately, and, only model the geo-human interaction in parameter updating rules. Below, we introduce the simplified implementation in our preliminary work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Imitation Module</head><p>The right part of Figure <ref type="figure" target="#fig_0">2</ref> shows that the imitation module aims to take the state of environment as input, recommend next-visit POIs, and evaluate the recommendation effectiveness as reward. In our preliminary work, we use a classical Deep Q-Network (DQN) <ref type="bibr" target="#b18">[19]</ref> as the policy function. The DQN takes the state at the time l, denoted by s l , as input.</p><p>The − greedy method is used to integrate exploration and exploitation to search the most appropriate POI. Specifically, the imitation module chooses a random POI a l r with probability or POI a l m that owns the maximum Q value as prediction with probability 1 − , the operation can be denoted by a l m = argmax a (Q(s l , a l )). After that, the predicted POI and real POI are input into the reward function to calculate the reward value. Finally, the imitation module updates its parameters based on the Bellman Equation. The loop continues until the module imitate the user visit behavior perfectly.</p><p>We adopt two technical improvements First, to utilize the trajectories of user visit and break the strong correlation between data samples, we leverage the prioritized experience replay method <ref type="bibr" target="#b23">[24]</ref> to accelerate the learning of DQN. Specifically, we first assign a priority score for each data sample (s l , a l , r l , s l+1 ). Then, we construct a priority distribution based on the priority score for sampling batch of data from memory. Secondly, we leverage two sampling strategies from <ref type="bibr" target="#b26">[27]</ref>: (1) Reward-based, which sets the reward value of the corresponding data sample as the priority score. For a given data sample, the higher reward of the sample is, the larger possibility is to be sampled. (2) Temporal Difference (TD)-based, which sets the corresponding TD-error as the priority score. For a given data sample, the larger TD-error is, the sample contains more valuable information to be learned. The l-th step TD-error can be represented as: TD-error l = r l + γ max a l+1 Q(s l+1 , a l+1 ) − Q(s l , a l ). After assigning the priority score for each data sample, we calculate the distribution of the priority score. Specifically, a softmax function takes the priority score as input and outputs the priority distribution. Then we sample top k data based on the priority distribution as a batch to train the imitation module, where k is the size of one batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Representation Module</head><p>The left part of Figure <ref type="figure" target="#fig_0">2</ref> shows that the representation module consider two types of information <ref type="bibr" target="#b0">(1)</ref> the interaction between users and spatio-temporal context; (2) the temporal dependency of user representations; in the updating rules.</p><p>Firstly, the spatial knowledge graph (KG) indicates the geospatial contexts. We denote the spatial KG as g l =&lt; h l , rel, t l &gt;, where h l is the representation of the heads (i.e., POIs), t l is the representation of the tails (i.e., categories and functional zones), rel is the representation of the relationship between the heads and tails. Secondly, the temporal context T ∈ R M ×3 is the combination of inner traffic, in-flow traffic, and out-flow traffic in all areas of a city, M represents the number of areas, and 3 represents the three kinds of traffic flows. The interaction between user and spatial KG occurs accompanying the temporal context. The users' visit events change the representation of spatial KG, and a new spatial KG representation stimulates the users to choose next-visit places. New user representation is not just related to newest user interest changes but also related to old representative user preferences.</p><p>Updating Rules of User Representations Assuming a user u i visits the POI P j at the step l, as an example to explain the whole process, the user representation will be updated for the step l + 1. We incorporate the user representation u l i and the interaction between the POI P j and the user u i into u l+1 i such that:</p><formula xml:id="formula_2">u l+1 i = σ(α u × u l i + (1 − α u ) × (W u • (h l Pj ) • Tl )), where W u ∈ R N ×1</formula><p>is the weight; α u is a scalar that represents the proportion of old profiling information in u l+1 i , it is given by: α u = σ(W αu •u l i +b αu ), where W αu ∈ R 1×N is the weight and b αu ∈ R 1×1 is the bias term; Tl ∈ R N ×1 is the temporal context vector adaptable with state update, it can be calculated by:</p><formula xml:id="formula_3">Tl = σ(W T1 • T l • W T2 + b T ), where W T1 ∈ R N ×M , W T2 ∈ R 3×1 and b T ∈ R N ×1</formula><p>are the weights and bias respectively.</p><p>Updating Rules of Spatial KG Representations. During the learning process of the representation of the spatial KG, we only focus on directly visited POI P j and other POIs P j − that "belong to" the same category or "locate at" the same functional zones with the directly visited P j . Here, heads are POIs and tails are categories or functional zones. Formally, we need to update the spatial KG representation g l =&lt; h l , rel, t l &gt;. We update the information in h l and t l , except rel l . In addition, σ(•) denotes the sigmoid function in following formulas.</p><p>(1) Updating visited POI h l+1 Pj . Similar to update u l+1 i , we incorporate the old visited POI representation h l Pj and the interaction between the use u l i and the POI P j in a given temporal context:</p><formula xml:id="formula_4">h l+1 Pj = σ(α p × h l Pj + (1 − α p ) × (W p • (u l i ) • Tl ))</formula><p>, where W p ∈ R N ×1 is the weight; α p is a scalar that denotes the proportion of old POI information in h l+1 Pj , it is calculated by:  We update the other POIs that belong to the same POI category and locate at the same functional zones of the visited P j as</p><formula xml:id="formula_5">α p = σ(W αp • h l Pj + b αp ), where W αp ∈ R 1×N is weight and b αp ∈ R 1×1 is</formula><formula xml:id="formula_6">h h P j − = t l+1 P j − − relP j − , h l+1 P j − = σ[α h × h l P j − + (1 − α h ) × h h P j − ]<label>(2)</label></formula><p>where α h is a scalar that is the proportion of</p><formula xml:id="formula_7">h l P j − in h l+1 P j − , it is calculated by: α h = σ(Wα h •h l P j − +bα h ),</formula><p>where Wα h ∈ R 1×N is weight and bα h ∈ R 1×1 is bias. After the above updating and learning process, we concatenate u l+1 i and g l+1 as the state s l+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">An Enhanced Method with Dynamic Geo-User Knowledge Graph</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Advancing Representation Module</head><p>The right part of Figure <ref type="figure" target="#fig_0">2</ref> shows that, we propose an enhanced representation module that unifies both users and geospatial contexts (POIs, POI categories, functional zones) into a single dynamic attributed knowledge graph, in order to better model connected, topological, and semantic dynamics.</p><p>Constructing A New Dynamic KG. The dynamic KG construction includes two stages: 1) initialization stage and 2) evolving stage. Firstly, in the initialization stage, we construct a classical knowledge graph to depict the semantic connectivity among different spatial entities. There are three types of spatial entities in the stage: POI, POI category, POI location (i.e. functional zones), and two relations types: "belong to" and "locate at". We organize two triple facts based on the entities and relations: (1) &lt;POI, "belong to", POI category&gt;, which expresses the affiliation relation between POI and POI category; (2) &lt;POI, "locate at", functional zone&gt;, which demonstrates the geographical relation between POI and function zone. Secondly, in the evolving stage, we aim to add the dynamic interactions (edges) between user and spatial entities. We introduce two entities: user, RPOI, and two relations: "visit", "also visit" into the KG that is constructed in the initialization stage. Here, RPOI is the reduplicated version of POI for recording the "also visit" relation. When a user visit event occurs, we add two triple facts to the KG: (1) &lt;user, "visit", POI&gt;, which shows the visit relation between the user and POI; (2) &lt;POI, "also visit", RPOI&gt;, which indicates the visit cascade relation between different POIs. In addition, to reflect the temporal effect of visit events, we record the visit time of each "visit" relation. To show the popularity of POIs, we record the total visits of each POI. Exit Mechanism for Expired Information Since millions of users and POIs are added into the KG over time, it will lead to uncontrollable size, resulting into memory overflow and huge computation complexity. We observe that compared with previous user activities, latest user activities are more important to describe user preferences and activity patterns. Thus, we implement an exit mechanism for expired user and expired activity related information. Specifically, for each user, we create a sliding window with a fixed length to record visit events. Assuming a given user newly visits a POI, if the number of visit events in the sliding window of the user does not surpass the window size, we add the new visit event to the window, otherwise, we eliminate expired information. Specifically, we remove the oldest event at the tail of the window tand insert the new event to the head of the window. The benefits of this idea are: the window moves forward over time, while maintaining a small size of KG and retaining the latest user activities and preferences. Learning Representations of Dynamic KG. Since traditional policy networks take vectors/matrices as input, we need to transform the environment (dynamic KG) into a state representation vector. Traditional KG embedding methods are not suitable for dynamic KG, because of KG information update and expensive computational costs.</p><p>We propose to leverage the technique in <ref type="bibr" target="#b30">[31]</ref>. Figure <ref type="figure" target="#fig_1">3</ref> shows we obtain the embedding of the dynamic KG by preserving the translation relationship h * + r * = t * , where h * , r * , t * are the joint embedding of head entity, relation, and tail entity respectively. The joint embedding of object(i.e. entities and relations) in the dynamic KG is the combination of the embedding of itself (i.e. h, r, t) and its context (i.e. cx(h), cx(r), cx(t). The context of an entity includes itself and its one-hop neighbor entities. The context of a relation consists of itself and other relations connecting the same entity pairs.</p><p>Be sure to notice that, we perform the dynamic KG embedding at the same time with constructing the dynamic KG. In the initialization stage of dynamic KG construction, we simultaneously learn the KG embedding as the initial value. In particular, we aim to preserve the translation relationship h * + r * = t * among the joint embeddings of all objects (i.e entity and relation) in the KG. For example, let o be an entity in the KG The embeding initialization process is: we first learn the context embedding of o, denoted by cx(o). The context of o is a sub-graph in the KG. We input the sub-graph structure into graph convolutional network (GCN) that contains m layers. The embedding of the final layer can be represented as:</p><formula xml:id="formula_8">Z m = relu( D− 1 2 Â D− 1 2 Z m−1 W m−1 ),<label>(3)</label></formula><p>where relu is the activation function, Â = A + I, A is the adjacency matrix, I is the identity matrix, D is the degree matrix, and W m−1 is the weight matrix. The output of the final layer is Z m ∈ R n×d , where n is the number of vertex in the sub-graph, d is the dimension of each row in Z m . Then, we employ an attention layer to aggregate Z m into a graphlevel embedding. The attention weight for each vertex in the sub-graph can be calculated by:</p><formula xml:id="formula_9">α i = exp(score(Z m i , o)) n i=1 exp(score(Z m i , o))<label>(4)</label></formula><p>where Z m i is the embedding of the i-th vertex in the subgraph, o ∈ R d is the embedding of the object, score(.) measures the relevance between Z m i and o. Next, we obtain the context embedding cx(o) by calculating the weighted sum of all node embeddings as follows:</p><formula xml:id="formula_10">cx(o) = n i α i Z m i (5)</formula><p>Later, we incorporate o with cx(o) as the joint embedding o * through a gated neural cell as follows:</p><formula xml:id="formula_11">o * = σ(γ) o + (1 − σ(γ)) cx(o) (<label>6</label></formula><formula xml:id="formula_12">)</formula><p>where σ is the sigmoid function, is element-wise multiplication, γ is a trainable parameter vector. We minimize the loss function during the training process as follows:</p><formula xml:id="formula_13">L = (h,r,t)∈S (h ,r,s )∈S max(0, f (h, r, t) + ε − f (h , r, t )) (7) where f (h, r, t) = h * + r * − t *</formula><p>l1 , ε is a tolerance margin value, S is the positive triple set that contains the correct semantic meaning, S is the negative triple set that randomly replaces the head and tail entities in S. After that, we apply the average pooling on both entity and relation embeddings to obtain the representation of the whole KG. We regard the representation as the initial state of the reinforcement learning module, denoted by s 0 .</p><p>In the evolving stage of dynamic KG construction, we learn the KG embedding based on the initial value by preserving the update information of the KG. In particular, we follow a local updating manner to learn the KG embedding incrementally. Since the initialization stage has provided an initial value for the KG embedding of the evolving stage, in the evolving stage, when nodes of the KG are added or removed, the translation relation among affected objects will be ruined. But other objects whose context are not changed in the event still keep the translation relationship. Thus, we pick up all objects that are affected by the node update event, and utilize the same model structure that is used in the initialization stage to learn the embedding of these objects incrementally. In this way, we avoid retraining the whole model for the KG update event. The state evolves step by step:</p><formula xml:id="formula_14">s 0 → s 1 → • • • → s T .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Advancing Imitation Module</head><p>POI Candidates Generation by Meta-path. Intuitively, people always choose the POIs that are close to their current position as the target place. Meanwhile, different people have different mobility patterns and preferences. Thus, we do not need to consider all POIs (actions) as the recommendation candidates for a user. To reduce the action space and improve recommendation performance, we propose a metapath based POI candidate generation method for each user. Figure <ref type="figure" target="#fig_2">4</ref> show we define four meta-path schemes in the dynamic KG: (1) "user -&gt; visit -&gt; POI"; (2) "user -&gt;visit -&gt; POI -&gt; also visit -&gt; RPOI"; (3) "user -&gt; visit -&gt; POI -&gt; belong to -&gt;POI category -&gt; belong to -&gt;POI"; (4) "user -&gt; visit -&gt;POI -&gt; locate at -&gt; functional zone -&gt; locate at -&gt; POI". These paths all start from "user" entity to "POI" entity. Taking the POI recommendation at the l-th time step as an example. For a specific user, we generate POI candidates by applying the four meta-paths on the dynamic KG. Each meta-path produces a POI set according to the corresponding schema. Then, the Top-N popular POIs of each POI set are collected as the POI candidates at the time step l, denoted by cand l ∈ R 4K , where K represents the number of POI candidates of each pre-defined meta-path. In this way, we reduce the action space from all POIs to 4K. A New Policy Network Structure for Dynamic Action Space. In our preliminary study, we exploit a vanilla DQN aas a policy network However, the vanilla DQN cannot deal with a dynamically varying action space, because the vanilla DQN only learns a fixed point-wise mapping from a state to an action in a fixed action set.</p><p>We propose a new policy network structure to address the challenge of varying action space (Figure <ref type="figure" target="#fig_3">5</ref>). The underlying idea is to learn a pairwise mapping function to map a state embedding -action embeding pair to a score. Taking the l-th time step as an example. We first obtain the state embeding s l and the POI candidates cand l = {a 1 , a 2 , ..., a 4K } at the l-th time step. We then concatenate the state embedding with the embedding of each action in cand l respectively as the input of the fully connected (FC) layers. The FC layers output the Q-values of all state-action pairs in cand l based on the concatenated embedding. Finally, we feed all Qvalues into a rank module for sorting them in a descending order, and the POI with the highest Q-value is selected as the next action a l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Recommending POIs</head><p>When the model is well-trained, we regard the imitation module as the POI recommendation engine. Specifically, we input the state of the environment (i.e. users, spatial KG) into the imitation module. Then, the imitation module provides the Q values of all actions (POIs). After that, the action with the highest Q value is output as the recommended POI for the user who is searching places.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Solving the Optimization Problem</head><p>Our method is a closed-loop learning system. To train the model, we propose an adversarial training like optimization method. Firstly, we interpret our method from an adversarial learning perspective. we regard the representation module as a generator to produce the state in real-time. Then, we treat the imitation module and reward function as a discriminator. When the discriminator provides the maximum quality score continually, the representation module achieves the best situation. Different from classical adversarial learning, the scoring criteria of our method is deterministic. The reward value of each imitation behavior indicates the imitated performance. There, we regard the reward value as the feedback to update the parameters of the representation module for improving the effectiveness of learned representations. Specifically, we utilize the gradient that comes from the gap between current reward and the expected reward value to update the parameters in the representation module. The Algorithm 1 shows the training process.</p><p>Without the loss of generality, to explain Algorithm 1, we use the i-th training iteration. We first generate the state at the step i by R i . Then, the imitation module I takes the state as input and outputs the predicted action (POI). Next, we use the reward function r to calculate reward value based on predicted POI and real-visit POI. Finally, we exploit the gradient that comes from the gap between 1 (max reward value) and the current reward to update the parameters θ R .   Update θ R by descending the gradient:</p><formula xml:id="formula_15">θ R log(1 − r(I(R i ), a i )).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We presented experiments with real world data to answer the following questions: Q1. Does our proposed recommendation framework outperform the existing methods? Q2.</p><p>How about the robustness of the proposed framework? Q3. Is each part of proposed framework necessary for improving recommendation performance? Q4. How does the reward function contribute to the POI recommendation performance?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Data Description</head><p>Table <ref type="table" target="#tab_2">1</ref> shows the statistics of two check-in datasets: New York and Tokyo <ref type="bibr" target="#b32">[33]</ref>. Each dataset includes User ID, Venue ID, Venue Category ID, Venue Category Name, Latitude, Longitude, and Time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Evaluation Metrics</head><p>We evaluated recommendation effectiveness with respect to four metrics:</p><p>(1) Precision on Category (Prec Cat). POI recommendation on the POI category level can be viewed as multiclassification. We used the weighted precision, given by:</p><formula xml:id="formula_16">Prec Cat = |c k | • I k T P k |c k |(I k T P + I k F P )<label>(8)</label></formula><p>where c k is the k-th POI category, |c k | is the number of c k , I k T P is the number of true positive predictions, and I k F P is the number of false positive predictions.</p><p>(2) Recall on Category (Rec Cat). We used the weighted recall over POI categories, which is given by: where I k F N is number of false negative predictions for c k . (3) Average Similarity (Avg Sim). From the perspective of user travel semantic meaning, the predicted POI category is expected to be similar to a user's travel goal We thus evaluated the average similarity between the real and predicted POI category. We employed the pretrained Glove word embedding <ref type="bibr" target="#b20">[21]</ref> to represent POI categories, then calculated the cosine similarity between the real POI category "word l " and the predicted POI category " ŵ ord l ". Formally, the average similarity is given by:</p><formula xml:id="formula_17">Rec Cat = |c k | • I k T P k |c k |(I k T P + I k F N )<label>(9</label></formula><formula xml:id="formula_18">Avg Sim = l cosine(word l , ŵ ord l ) L<label>(10)</label></formula><p>where L denotes the total visit number. The higher value of Avg Sim is, the better the model performance is.</p><p>(4) Average Distance (Avg Dist). We evaluate the average geographic distance between the locations of predicted POIs and real visit POIs. The formula of the Avg Dist as follows:</p><formula xml:id="formula_19">Avg Dist = l Dist(P l , P l ) L .<label>(11)</label></formula><p>where Dist(P l , P l ) denotes the distance between the location of the real POI P l and the predicted POI P l for the l-th visit. The lower value of Avg Dist is, the better the model performance is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Baseline Models</head><p>We compared the performance of our enhanced POI recommendation framework (namely "DRPR", Dynamic Reinforced POI Recommendation) against the following baseline algorithms for evaluation.</p><p>(1) PMF recommends items based on the user-item interaction matrix through probabilistic matrix factorization <ref type="bibr" target="#b17">[18]</ref>.</p><p>(2) PoolNet employs a deep neural network model to model the interaction between user embedding and item embedding for recommending items <ref type="bibr" target="#b4">[5]</ref>.</p><p>(3) WaveNet is used to generate raw audio waveforms originally. It also can be used to recommend items by imitating the sequential decision-making process of users <ref type="bibr" target="#b19">[20]</ref>.</p><p>(4) LSTMNet utilizes a recurrent neural network to mimic users' behavior for recommending items <ref type="bibr" target="#b10">[11]</ref>.</p><p>(5) LightFM utilizes user-item interaction matrix and useritem meta-data for recommending items <ref type="bibr" target="#b11">[12]</ref>. In the experiment, we first selected the 15,000 continuous check-in records from New York and Tokyo datasets respectively. Then, for each city, we splited the corresponding records into two non-overlapping sets: the earliest 80% of check-ins for training and the remaining 20% of checkins for testing. During the learning process, we set the dimension of the state as 200 and the size of POI candidates as 20. For the implementation of baseline models, we evaluated PMF by adopting the implementation 2 ; we implemented PoolNet, WaveNet, and LSTMNet by adopting "spotlight" <ref type="bibr" target="#b12">[13]</ref>, where the learning rate is set as 0.1; we evaluated LightFM by adopting the implementation 3 ; we implemented IMUP by adopting the implementation 4 . Finally, to make others reproduce the experiment easily, we release the code and data in Dropbox 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Environmental Settings.</head><p>We conducted all experiments on Ubuntu 18.04.3 LTS, Intel(R) Core(TM) i9-9920X CPU@ 3.50GHz, with Titan RTX and memory size 128G. In addition, we implemented all models based on python 3.7.4, TensorFlow 2.0.0 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Performance (Q1)</head><p>We compared our method (DRPR) with baseline algorithms in terms of Precision on Category, Recall on Category, Average Similarity and Average Distance. Figure <ref type="figure">6</ref> and Figure <ref type="figure">7</ref> show that our method outperforms other baseline models under both New York and Tokyo datasets. A potential interpretation for the improvement on Precision and Recall is that we explicitly extract the users' travel preference by utilizing the sub-structure information in dynamic KG. In addition, a possible reason for the improvement on Average Similarity and Average Distance is that the evolving updating process of the dynamic KG captures the changes of user mobility preference over time. Thus, from the semantic and distance perspective, the prediction of our model is close to the benchmark visit POI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Robustness Check (Q2)</head><p>We randomly divided the dataset into 5 partitions, and evaluated our method over these subsets to examine its robustness (low variance) over partitions. As illustrated in Figure <ref type="figure">8</ref> and Figure <ref type="figure">9</ref>, we can find that compared with our conference framework RIRL, the enhanced framework DRPR is more stable in terms of Precision on Category, Recall on Category, Average Similarity, and Average Distance. Such observation indicates that with the framework captures more user visit preferences, the model performance is more stable and robust. In addition, another interesting observation is that under the same sampling strategy, the model performance of DRPR is better than RIRL. Such observation reflects the superiority and robustness of DRPR when DRPR faces different data sets. Moreover, after a careful inspection of Figure <ref type="figure">8</ref> and Figure <ref type="figure">9</ref> , we can find that no matter in DRPR or RIRL, the model performance of the TD-based sampling strategy is more superior to the reward-based sampling strategy. A possible interpretation for the observation is that the TD-based sampling strategy makes the model focus on the data samples that are difficult to learn, which improves the recommendation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">The Study of Dynamic Knowledge Graph (Q3)</head><p>Our method employed the dynamic KG to model the changes of user mobility interests . To evaluate the contribution of the dynamic KG, we developed a variant of DRPR, namely DRPR * , as the control group. DRPR * replaces the dynamic KG with a static KG, while other components remain the same. For the static KG, we first collected all   training data samples to construct a KG that contains the semantic relations among spatial entities and user visit events. Then, we utilized the TransE model <ref type="bibr" target="#b0">[1]</ref> to learn the representation of the KG as the state vector to predict the POIs of all test samples. During this process, the KG is static, which doesn't change over time. Table <ref type="table" target="#tab_5">2</ref> and Table <ref type="table" target="#tab_6">3</ref> show that DRPR outperforms DRPR * in terms of all four evaluation metrics under both New York and Tokyo datasets. This observation indicates that compared with static KG, the dynamic updating process in dynamic KG can not only capture the temporal intrinsic of the environment, but also grasp the changes of user mobility preference over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">The Study of Exit Mechanism (Q3)</head><p>In our method, we designed an exit mechanism in DRPR to eliminate the outdated user visit events to maintain the dynamic KG in a reasonable scale. To validate the effectiveness of the exit mechanism, we developed a variant of DRPR, i.e., DRPR , which removes the exit mechanism in DRPR and maintains the other components. Table <ref type="table" target="#tab_5">2</ref> and Table <ref type="table" target="#tab_6">3</ref> show that the performance of DRPR drops significantly compared with DRPR, indicating the importance of the exit mechanism. A possible interpretation for the observation is that the exit mechanism discards the outdated and trivial user mobility interests, which presents the learning process of DRPR from being disturbed by outdated user activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">The Study of POI Candidates Generation (Q3)</head><p>In our method, we generated a personalized POI candidate set for a user by leveraging pre-defined meta-paths on the dynamic knowledge graph. The generated POI candidates aim to reduce the action space and improve the performance of POI recommendations. To justify the effectiveness of POI candidates generation, we developed a variant of DRPR, namely DRPR − that removes the POI candidate generation part of DRPR. Table <ref type="table" target="#tab_5">2</ref> and Table <ref type="table" target="#tab_6">3</ref> show that, compared with DRPR − , DRPR exhibits a great improvement on both recommendation performance and model training time cost over two datasets. A potential explanation of this observation is that the personalized POI candidate set significantly reduces the action space for the agent, making the exploration on POI more efficient. Meanwhile, as the personalized POI candidate is more accurate and targeted for the user, the recommendation effectiveness can also be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">The Study of Reward Function (Q4)</head><p>Our reward function consists of three components, i.e., distance r d , category similarity r c , and prediction accuracy r p whose value is 1 if the predicted POI is identical to the real user visit event, and 0 otherwise. We combined the three components into our reward function by taking their weighted memorization, with three corresponding weights, i.e., λ d , λ c and λ p , w.r.t λ d + λ c + λ p = 1. We set the learning rate as 1e-5, and project evaluation metrics into a 3D space, with (r d , r c ,r p ) as axes. The shade of color denotes the value of the metric, and the darker the color is, the higher the performance will be. Figure <ref type="figure" target="#fig_4">10</ref> and Figure <ref type="figure" target="#fig_4">11</ref> show an interesting observation: the contribution of r d is higher than r c and r p in terms of all evaluation metrics. The reason is that we have employed the meta-path-based POI candidate generation in DRPR to discover the POI and POI category subset that a user is possibly interested in. This operation improves the importance of r d : as long as DRPR predicts the POIs that are close to real visit POIs, the model performance can be improved from all sides. A careful inspection of Figure <ref type="figure" target="#fig_4">10</ref> and Figure <ref type="figure" target="#fig_4">11</ref> find that although r d is more important than the other two factors for all metrics, the best model performance is achieved based on the balanced combination of r d , r c , and r p . Such observation reveals that the dependencies among the three factors may affect the recommendation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORKS</head><p>POI Recommendation. POI recommendation plays an important role in location-based social networks (LBSNs). Accurate POI recommendations help users explore interesting places, which makes the users' life convenient. Thus, many researchers are attracted to the POI recommendation domain <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b35">[36]</ref>. For example, Lian et al. <ref type="bibr" target="#b13">[14]</ref> explored user mobility preferences by a modified weighted matrix factorization method. Liu et al. proposed a novel geographical probabilistic factor analysis framework to study the user geographical interest <ref type="bibr" target="#b14">[15]</ref>. In <ref type="bibr" target="#b7">[8]</ref>, the researchers exploit factorization-based approaches to construct user representation by integrating spatio-temporal influence of human mobility for POI recommendation. Compared with these works, our framework DRPR incorporates multiple factors that affect POI recommendation into a dynamic knowledge graph, and the reinforced module of DRPR explores and exploits user mobility patterns and interests effectively. Knowledge Graph-based Recommendation. Knowledge graph (KG) demonstrates the semantic relations and reasoning structure among different entities. Owing to the rich semantic information in KG, many researchers adopt KG in the recommendation domain and achieve a good performance <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b34">[35]</ref>. For instance, Sun et al. utilized a recurrent neural network to learn the semantic rich embedding of entities and relations for capturing the user preference <ref type="bibr" target="#b24">[25]</ref>. Xian et al. proposed the PGPR framework that employs the meta-path in KG for explainable recommendations <ref type="bibr" target="#b31">[32]</ref>. Wang et al. generated the path embedding by incorporating the semantics of both entities and relations and inferred reasonable recommendation based on meta-paths <ref type="bibr" target="#b27">[28]</ref>. Fan et al. extracted rich meta-path structure information for user intent recommendation <ref type="bibr" target="#b5">[6]</ref>. Compared with the previous works, we implement a dynamic knowledge graph to simulate the environment where users visit. And we unify the embedding and meta-paths of the dynamic KG to explore the user mobility patterns and preferences sufficiently. Reinforcement Learning for Online Recommendation. Reinforcement learning-based recommendation systems formulate the user-item interaction into a sequential decisionmaking process for recommending items <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b36">[37]</ref>. For instance, Zheng et al. utilized reinforcement learning to capture implicit user feedback characteristics <ref type="bibr" target="#b36">[37]</ref> for news recommendation. Zhao et al. proposed "DEERS" recommendation model, which leverages reinforcement learning to automatically mine users' preferences on items. POI recommendation is different from traditional recommendation system, because its strong geographical constrains. Recently, in POI recommendation domain, Wang et al. incorporated reinforcement learning and a spatial knowledge graph to grab user mobility pattern <ref type="bibr" target="#b26">[27]</ref>. Compared with <ref type="bibr" target="#b26">[27]</ref>, we leverage a dynamic KG to incorporate the interaction and information of users and a geographical environment. Meanwhile, we recommend attractive POIs for users by exploring the substructure information in the dynamic KG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION REMARKS</head><p>In this paper, we propose a novel POI recommendation framework with integrating dynamic KG with reinforcement learning setting. Since the common practice in KGbased recommendation cannot capture the multi-level dynamics of human mobility, we introduce dynamic KG as the environment. Specifically, we regard the representation of the dynamic KG as the state in reinforcement learning. To address the uncontrollable graph-scale issue, we develop an exit mechanism to remove outdated information. To reduce the the action space, we devise a meta-path-based POI candidate generation method to select most possible POIs to recommend. To solve the problem of the dynamic action space, we propose a new policy network that takes stateaction pair as input, and output Q value for each pair. We select the pair with the highest Q value as the recommendation. From extensive experiments, we can find that our proposed method can better capture the dynamic characteristics of user mobility pattern and preference compared with other baselines. And the personalized POI candidates makes the recommendations become accurately and efficiently.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Representation Module of the conference and enhanced version.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Representation learning for dynamic KG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig.4: POI candidates generation by meta-path. Figure4show we define four meta-path schemes in the dynamic KG: (1) "user -&gt; visit -&gt; POI"; (2) "user -&gt;visit -&gt; POI -&gt; also visit -&gt; RPOI"; (3) "user -&gt; visit -&gt; POI -&gt; belong to -&gt;POI category -&gt; belong to -&gt;POI"; (4) "user -&gt; visit -&gt;POI -&gt; locate at -&gt; functional zone -&gt; locate at -&gt; POI". These paths all start from "user" entity to "POI" entity. Taking the POI recommendation at the l-th time step as an example. For a specific user, we generate POI candidates by applying the four meta-paths on the dynamic KG. Each meta-path produces a POI set according to the corresponding schema. Then, the Top-N popular POIs of each POI set are collected as the POI candidates at the time step l, denoted by cand l ∈ R 4K , where K represents the number of POI candidates of each pre-defined meta-path. In this way, we reduce the action space from all POIs to 4K. A New Policy Network Structure for Dynamic Action Space. In our preliminary study, we exploit a vanilla DQN aas a policy network However, the vanilla DQN cannot deal with a dynamically varying action space, because the vanilla DQN only learns a fixed point-wise mapping from a state to an action in a fixed action set.We propose a new policy network structure to address the challenge of varying action space (Figure5). The underlying idea is to learn a pairwise mapping function to map a state embedding -action embeding pair to a score. Taking the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: The structure of policy network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1 :</head><label>1</label><figDesc>Stochastic Gradient Descent Training for the Representation Module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>reward function; I: imitation module; R: representation module ; a: Real action (real POI). 2 for number of training iterations do 3 Assume the loop variable is i.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>4</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>)Fig. 6 :Fig. 7 :</head><label>67</label><figDesc>Fig. 6: Overall performance w.r.t. New York dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>IMUP-r is a new POI recommendation framework with incorporating spatial KG and reinforcement learning to recommend items for users. The reward-based sampling strategy is used to improve model performance<ref type="bibr" target="#b26">[27]</ref>. (7) IMUP-TD is the same as the model structure of the IMUP-r. The only difference is that the sampling strategy of the IMUP-TD is TD-based. (8) RIRL-r is the conference version of our model, which utilizes the adversarial training skill to train the whole framework and employs the reward-based sampling strategy during the training phase [26]. (9) RIRL-TD is a variant of RIRL-r, which utilizes TD-based sampling strategy during the training phase. 4.1.4 Hyperparameters, Source Code, and Reproducibility.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 :Fig. 9 :</head><label>89</label><figDesc>Fig. 8: Robustness Check w.r.t. New York dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 :Fig. 11 :</head><label>1011</label><figDesc>Fig. 10: Reward Analysis w.r.t. New York dataset.</figDesc><graphic url="image-7.png" coords="10,182.34,201.39,119.74,111.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 :</head><label>1</label><figDesc>Statistics of the checkin data.</figDesc><table><row><cell>City</cell><cell># Check-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>ins # POIs # POI Categories Time Period</head><label></label><figDesc></figDesc><table><row><cell>New York</cell><cell>227, 428</cell><cell>38, 334</cell><cell>251</cell><cell>4/2012-2/2013</cell></row><row><cell>Tokyo</cell><cell>537, 703</cell><cell>61, 858</cell><cell>385</cell><cell>4/2012-2/2013</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Prec Cat Outperform</cell><cell cols="2">Rec Cat Outperform</cell><cell cols="2">Avg Sim Outperform</cell><cell cols="2">Avg Dist Outperform</cell><cell>Time Cost</cell></row><row><cell>DRPR DRPR  *</cell><cell>0.1086 0.0313</cell><cell>− +71.2%</cell><cell>0.0913 0.0184</cell><cell>− +79.8%</cell><cell>0.3890 0.3352</cell><cell>− +13.8%</cell><cell>11.351 10.171</cell><cell>− +10.4%</cell><cell>32.760 -</cell></row><row><cell>DRPR</cell><cell>0.0359</cell><cell>+66.9%</cell><cell>0.0270</cell><cell>+70.4%</cell><cell>0.3362</cell><cell>+13.6%</cell><cell>11.855</cell><cell>+4.44%</cell><cell>-</cell></row><row><cell>DRPR −</cell><cell>0.0201</cell><cell>+81.5%</cell><cell>0.0100</cell><cell>+89.0%</cell><cell>0.3283</cell><cell>+15.6%</cell><cell>12.591</cell><cell>+10.9%</cell><cell>53.236</cell></row></table><note>Ablation Study of DRPR w.r.t. New York dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3 :</head><label>3</label><figDesc>Ablation Study of DRPR w.r.t. Tokyo dataset</figDesc><table><row><cell></cell><cell cols="2">Prec Cat Outperform</cell><cell cols="2">Rec Cat Outperform</cell><cell cols="2">Avg Sim Outperform</cell><cell cols="2">Avg Dist Outperform</cell><cell>Time Cost</cell></row><row><cell>DRPR DRPR  *</cell><cell>0.1409 0.1266</cell><cell>− +10.1%</cell><cell>0.3403 0.3234</cell><cell>− +4.97%</cell><cell>0.2202 0.2193</cell><cell>− +0.41%</cell><cell>8.4533 8.7521</cell><cell>− +3.53%</cell><cell>31.837 -</cell></row><row><cell>DRPR</cell><cell>0.1289</cell><cell>+8.52%</cell><cell>0.3243</cell><cell>+4.70%</cell><cell>0.2171</cell><cell>+1.41%</cell><cell>10.055</cell><cell>+18.9%</cell><cell>-</cell></row><row><cell>DRPR −</cell><cell>0.1204</cell><cell>+14.5%</cell><cell>0.0353</cell><cell>+89.6%</cell><cell>0.1899</cell><cell>+13.8%</cell><cell>9.5301</cell><cell>+12.7%</cell><cell>58.591</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Content-aware hierarchical point-of-interest embedding model for successive poi recommendation</title>
		<author>
			<persName><forename type="first">Buru</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonggyu</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seongsoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Jointly non-sampling learning for knowledge graph enhanced recommendation</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SI-GIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 43rd International ACM SI-GIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient heterogeneous collaborative filtering without negative sampling for recommendation</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM conference on recommender systems</title>
				<meeting>the 10th ACM conference on recommender systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Metapath-guided heterogeneous graph neural network for intent recommendation</title>
		<author>
			<persName><forename type="first">Junxiong</forename><surname>Shaohua Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linmei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongliang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2478" to="2486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hme: A hyperbolic metric embedding approach for next-poi recommendation</title>
		<author>
			<persName><forename type="first">Shanshan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Vinh Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1429" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Poi recommendation: Towards fused matrix factorization with geographical and temporal influences</title>
		<author>
			<persName><forename type="first">Jean-Benoit</forename><surname>Griesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Talel</forename><surname>Abdessalem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Naacke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM Conference on Recommender Systems</title>
				<meeting>the 9th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="301" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Realtime event embedding for poi recommendation</title>
		<author>
			<persName><forename type="first">Pei-Yi</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng-Hang</forename><surname>Cheang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Hsien</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">349</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to collaborate in multi-module recommendation via multi-agent reinforcement learning without communication</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haikai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rundong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runsheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourteenth ACM Conference on Recommender Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="210" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Session-based recommendations with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linas</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domonkos</forename><surname>Tikk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06939</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Metadata embeddings for user and item cold-start recommendations</title>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kula</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on New Trends on Content-Based Recommender Systems co-located with 9th ACM Conference on Recommender Systems</title>
				<meeting>the 2nd Workshop on New Trends on Content-Based Recommender Systems co-located with 9th ACM Conference on Recommender Systems<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-16">RecSys 2015. September 16-20, 2015. 2015</date>
			<biblScope unit="volume">1448</biblScope>
			<biblScope unit="page" from="14" to="21" />
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kula</surname></persName>
		</author>
		<ptr target="https://github.com/maciejkula/spotlight" />
		<title level="m">Spotlight</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geomf: joint geographical modeling and matrix factorization for point-of-interest recommendation</title>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangzhong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="831" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning geographical preferences for point-of-interest recommendation</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1043" to="1051" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ambulance dispatch via deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><forename type="middle">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Advances in Geographic Information Systems</title>
				<meeting>the 28th International Conference on Advances in Geographic Information Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="123" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Modeling the interaction coupling of multi-view spatiotemporal contexts for destination prediction</title>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 SIAM International Conference on Data Mining</title>
				<meeting>the 2018 SIAM International Conference on Data Mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="171" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Playing atari with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5602</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
				<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Quoc Viet Hung Nguyen, and Hongzhi Yin. Spatiotemporal representation learning for translation-based poi recommendation</title>
		<author>
			<persName><forename type="first">Tieyun</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bio-inspired collaborative and content filtering method for online recommendation assistant systems</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Sergey Rodzin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lada</forename><surname>Rodzina</surname></persName>
		</author>
		<author>
			<persName><surname>Rodzina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Science On-line Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05952</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Prioritized experience replay. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent knowledge graph embedding for effective recommendation</title>
		<author>
			<persName><forename type="first">Zhu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Bozzon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long-Kai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM Conference on Recommender Systems</title>
				<meeting>the 12th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="297" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reinforced imitative graph representation learning for mobile user profiling: An adversarial training perspective</title>
		<author>
			<persName><forename type="first">Dongjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanchun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Virtual Event</title>
				<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021">February 2-9, 2021. 2021</date>
			<biblScope unit="page" from="4410" to="4417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Incremental mobile user profiling: Reinforcement learning with spatial knowledge graph for modeling event streams</title>
		<author>
			<persName><forename type="first">Pengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="853" to="861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Explainable reasoning over knowledge graphs for recommendation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingxian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5329" to="5336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reinforced negative sampling over knowledge graph for recommendation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaokun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
				<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="99" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A reinforcement learning framework for explainable recommendation</title>
		<author>
			<persName><forename type="first">Xiting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiru</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengtao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on data mining (ICDM)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="587" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Efficiently embedding dynamic knowledge graphs</title>
		<author>
			<persName><forename type="first">Tianxing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arijit</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06708</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reinforcement knowledge graph reasoning for explainable recommendation</title>
		<author>
			<persName><forename type="first">Yikun</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuohui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval</title>
				<meeting>the 42nd international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="285" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modeling user activity preference by leveraging user spatial temporal characteristics in lbsns</title>
		<author>
			<persName><forename type="first">Dingqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="142" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploring clustering of bandits for online recommendation system</title>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourteenth ACM Conference on Recommender Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="120" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Collaborative knowledge base embedding for recommender systems</title>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Jing Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Where to go next: A spatio-temporal gated network for next poi recommendation</title>
		<author>
			<persName><forename type="first">Pengpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjing</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanchi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofang</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Drn: A deep reinforcement learning framework for news recommendation</title>
		<author>
			<persName><forename type="first">Guanjie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Jing Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
				<meeting>the 2018 World Wide Web Conference</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pseudo dyna-q: A reinforcement learning framework for interactive recommendation</title>
		<author>
			<persName><forename type="first">Lixin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
				<meeting>the 13th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="816" to="824" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
