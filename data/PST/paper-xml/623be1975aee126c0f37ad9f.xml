<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-23">23 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Umang</forename><surname>Gupta</surname></persName>
							<email>umanggup@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jwala</forename><surname>Dhamala</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Alexa</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Varun</forename><surname>Kumar</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Alexa</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Apurv</forename><surname>Verma</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Alexa</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Alexa</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Satyapriya</forename><surname>Krishna</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
							<email>gupra@amazon.com</email>
							<affiliation key="aff1">
								<orgName type="department">Amazon Alexa</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Alexa</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Greg</forename><forename type="middle">Ver</forename><surname>Steeg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Amazon Alexa</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Amazon Alexa</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-23">23 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.12574v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Language models excel at generating coherent text, and model compression techniques such as knowledge distillation have enabled their use in resource-constrained settings. However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model's biases onto the distilled model. To this end, we present a novel approach to mitigate gender disparity in text generation by learning a fair model during knowledge distillation. We propose two modifications to the base knowledge distillation based on counterfactual role reversalmodifying teacher probabilities and augmenting the training set. We evaluate gender polarity across professions in open-ended text generated from the resulting distilled and finetuned GPT-2 models and demonstrate a substantial reduction in gender disparity with only a minor compromise in utility. Finally, we observe that language models that reduce gender polarity in language generation do not improve embedding fairness or downstream classification fairness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ever-increasing size of language models (LMs) have increased their energy and compute requirements, making them impractical for many real-time resource-constrained applications such as personal assistants deployed on edge devices. To address this issue, various approaches have been proposed to compress or distill these large models (e.g., <ref type="bibr" target="#b36">Sanh et al. (2019)</ref>; <ref type="bibr" target="#b20">Jiao et al. (2020)</ref>; <ref type="bibr" target="#b18">Hinton et al. (2015)</ref>). However, distillation techniques are designed to mimic the uncompressed LM (i.e., teacher model). Thus, the societal biases encoded in the teacher He works in a hospital as a Prompt . . . doctor, treating the elderly with a variety, and by all accounts does an excellent work of medicine.</p><p>GPT-2 . . . physician and helps a lot of the patients.</p><p>Fair  She works in a hospital as a Prompt . . . nurse and was in love with her mother and her big brother, a small, shy, overweight woman. GPT-2 . . . pediatric dermatologist who gets stitches but also helps hospitals understand newborns . . . Fair  Figure <ref type="figure">1</ref>: Example texts generated by LMs under different gender contexts (identified by the words 'He' and 'She'). GPT-2 continues the prompt with the occupation word historically associated with the specific gender. Our approach aims to treat both genders equally. models <ref type="bibr" target="#b1">(Bender et al., 2021;</ref><ref type="bibr" target="#b5">Bommasani et al., 2021;</ref><ref type="bibr" target="#b39">Sheng et al., 2021)</ref> will propagate to the distilled models. In fact, our experiments show that distilled models are adjudged to be more unfair than their teacher model counterparts. In this work, we devise techniques to train models that mitigate societal biases during knowledge distillation.</p><p>One way to demonstrate this manifestation of societal biases is by looking at text generated by LMs, as illustrated in Fig. <ref type="figure">1</ref>. As such, the output text focuses on different characteristics of the person, solely based on which gender is mentioned in the context. To this end, we focus on reducing the disparity between groups during the language generation, considering the fairness definition for openended text generations as proposed in <ref type="bibr" target="#b12">Dhamala et al. (2021)</ref> and <ref type="bibr" target="#b40">Sheng et al. (2019)</ref>. We propose an approach that uses counterfactual role-reversed sentences during knowledge distillation. In other words, our approach uses counterfactual texts that are generated by substituting mentions of one demographic group with the other. We employ an automated way to generate these counterfactuals, requiring only a paired list of words from each demographic group.</p><p>Typical knowledge distillation training loss has two components: (a) the LM training loss such as crossentropy to learn information from the training data, and (b) a loss that enforces similarity between outcomes of teacher and student models<ref type="foot" target="#foot_0">1</ref> . The counterfactual knowledge is used to correct these loss components in the following ways: (a) augmenting the training set itself, which alters the training loss to learn from more equitable data; and (b) modifying the teacher's output toward more equitability so that the student learns from a more equitable output distribution.</p><p>We first demonstrate our method using English GPT2-small <ref type="bibr" target="#b34">(Radford et al., 2019)</ref> as the teacher and a 6-layer GPT-2 (called DistilGPT-2) as the student model. We focus on binary gender disparities (male vs. female) and use the gender polarity metric for profession prompts from the BOLD dataset <ref type="bibr" target="#b12">(Dhamala et al., 2021)</ref> as the primary fairness definition. We show that our approach lowers the gender disparity in the generated text. Next, we demonstrate the applicability of our approach for finetuning English GPT2-small, i.e., using the same architecture for teacher and student models in the distillation framework. Finally, we evaluated the resultant model's gender fairness on downstream tasks such as Contextual Embedding Association Tests (CEAT) <ref type="bibr" target="#b7">(Caliskan et al., 2017)</ref> and finetuning on Bios-Bias classification task <ref type="bibr" target="#b10">(De-Arteaga et al., 2019)</ref>. We find that reduced disparity in openended text generation does not necessarily lead to fairness on other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Large LMs embody societal biases that could result in harms such as misinformation, stereotype propagation, and disparate resource allocation <ref type="bibr" target="#b1">(Bender et al., 2021;</ref><ref type="bibr" target="#b39">Sheng et al., 2021)</ref>. Multiple studies have shown that LMs are biased in producing outputs with negative connotations such as toxicity <ref type="bibr" target="#b14">(Gehman et al., 2020;</ref><ref type="bibr" target="#b47">Zhou et al., 2021;</ref><ref type="bibr" target="#b44">Xu et al., 2021)</ref> and negative regard <ref type="bibr" target="#b38">(Sheng et al., 2020</ref><ref type="bibr" target="#b39">(Sheng et al., , 2021) )</ref> towards minority populations. Others have shown that LMs encode prevalent gender biases, such as one gender being more associated with a particular class of professions. Such biases can be revealed via contextual embedding tests (Guo and <ref type="bibr">Caliskan, 2021)</ref>, stereotype tests <ref type="bibr" target="#b37">(Sap et al., 2020;</ref><ref type="bibr" target="#b32">Nangia et al., 2020)</ref>, and evaluation of generated texts <ref type="bibr" target="#b12">(Dhamala et al., 2021;</ref><ref type="bibr" target="#b40">Sheng et al., 2019)</ref>. Few works have also shown that LM can be biased towards ideologies, e.g., <ref type="bibr">Islam (Brown et al., 2020)</ref>.</p><p>Approaches to mitigate bias in LMs can be broadly summarized as: (a) training or finetuning on a balanced dataset <ref type="bibr">(Solaiman and Dennison, 2021;</ref><ref type="bibr" target="#b13">Dinan et al., 2020)</ref>), (b) attaching prefix at inference or training time <ref type="bibr" target="#b38">(Sheng et al., 2020)</ref>, and (c) using a bias or attribute classifier (e.g., toxicity classifier) to control fairness in text generation <ref type="bibr" target="#b9">(Dathathri et al., 2020;</ref><ref type="bibr" target="#b24">Liang et al., 2021;</ref><ref type="bibr" target="#b25">Liu et al., 2021;</ref><ref type="bibr" target="#b21">Krause et al., 2021)</ref>. While all these debiasing approaches can be used to mitigate bias in an LM after it is distilled, no prior work aims to directly debias and distill in a single step. Furthermore, the majority of existing approaches focus on reducing toxic text generation <ref type="bibr">(Solaiman and Dennison, 2021;</ref><ref type="bibr" target="#b9">Dathathri et al., 2020;</ref><ref type="bibr" target="#b24">Liang et al., 2021;</ref><ref type="bibr" target="#b25">Liu et al., 2021;</ref><ref type="bibr" target="#b21">Krause et al., 2021)</ref>. Different from existing works, we present an approach for fair knowledge distillation that aims to mitigate gender bias in text generated from the distilled models.</p><p>Our approach is inspired by the counterfactual notion of fairness <ref type="bibr" target="#b22">(Kusner et al., 2017)</ref> and introduces two modifications to the standard distillation: (a) counterfactual data augmentation, and (b) using modified teacher probabilities. Counterfactual fairness and related notions have been previously used for bias mitigation in hate speech detection <ref type="bibr" target="#b30">(Mostafazadeh Davani et al., 2021)</ref>, word embeddings <ref type="bibr" target="#b17">(Hall Maudslay et al., 2019;</ref><ref type="bibr" target="#b27">Lu et al., 2020;</ref><ref type="bibr">Zhao et al., 2018b)</ref>, and coreference resolution <ref type="bibr">(Zhao et al., 2018a)</ref> tasks. Ours is the first work that uses counterfactual knowledge to achieve equitability in text generation during distillation. Our method is also applicable when the student model or architecture is the same as the teacher model, and we have demonstrated it via experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Notion of Language Model Fairness</head><p>We focus on mitigating gender bias in open-ended language generation from an LM. The bias is mea-sured by assessing the tendency of the LM to associate a specific set of professions to a specific gender, e.g., healthcare professions to female and engineering professions to male. As discussed in <ref type="bibr" target="#b39">Sheng et al. (2021)</ref>, such societal biases may cause a negative representational impact by propagating stereotypes, misrepresentations, or denigrations of social groups. We consider only binary gender in this paper as LMs often do not encode sufficient representation of non-binary gender context, restricting a meaningful analysis <ref type="bibr" target="#b11">(Dev et al., 2021)</ref>. We use a related counterfactual notion of fairness, commonly studied in the NLP fairness literature, to motivate our fair distillation approach in Sec. 4. The counterfactual notion of fairness <ref type="bibr" target="#b22">(Kusner et al., 2017)</ref> adjudges a model fair if it generates similar predictions before and after swapping the sensitive features in the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Fair Knowledge Distillation via</head><p>Counterfactual Role Reversal This loss consists of two terms: (a) the crossentropy (CE) between the predicted next token probability and the observed token, and (b) the KLdivergence between the output probabilities from the teacher (P teacher ) and the student (P ? ) models.</p><p>The KL-divergence term provides a stronger training signal to the student, leading to more accurate and faster learning <ref type="bibr" target="#b18">(Hinton et al., 2015)</ref>.</p><p>Knowledge distillation (Eq. ( <ref type="formula">1</ref>)) will also transfer societal biases while transferring information from the teacher model. To address this problem, we propose to infuse the bias mitigation strategy with knowledge distillation to obtain a less biased and compact model. Our bias mitigating strategy is based on the intuition that given a sequence such as 'She works as a' and its counterfactual 'He works as a', a fair LM should generate similar texts. We materialize this intuition by encouraging student LM to learn similar distribution of probabilities for a sequence of tokens and its counterfactual.</p><p>To this end, we propose two modifications to the base distillation strategy: (a) Using counterfactual role reversal to modify token probabilities of the teacher model; and (b) Using counterfactual role reversed data for model distillation. We study these two modifications independently and in various combinations<ref type="foot" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Counterfactual Role Reversal</head><p>Given a sequence of tokens referring to a particular demographic group, we want to generate a counterfactual sequence of tokens referring to another related demographic. For example, suppose the original text, referring to the female group was 'She is a mother of two kids and works as a software engineer,' we want to generate a counterfactual referring to the male group 'He is a father of two kids and works as a software engineer.' Inspired by existing works on counterfactual data augmentation for binary gender <ref type="bibr" target="#b27">(Lu et al., 2020;</ref><ref type="bibr" target="#b17">Hall Maudslay et al., 2019)</ref>, we use word-swapping operations on the sequence of tokens to generate counterfactual sequences. Specifically, we use a curated dictionary of gender words with male female mapping, for instance, father ? mother, she ?he, him?her, etc. We generate a counterfactual sequence of tokens from the original sequence by substituting the gendered word in the original sequence with a matching gendered word referring to the opposite gender from this dictionary<ref type="foot" target="#foot_2">3</ref> . See Appendix B for the curated dictionary sources and other implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Modifying Teacher Probabilities</head><p>Next, we discuss how to use counterfactual sequences to modify knowledge distillation loss. In an open-ended language generation task, the LM produces a natural continuation of text given some context or a prompt (x &lt;t ). To this end, autoregressive LMs such as GPT-2 predict the probability distribution of the next token given the context To mitigate this unchecked transference of gender disparity, we modify the teacher probability of each token by using the next token probabilities from both the original and the counterfactual context (or both genders) during student model training.</p><p>We combine them to boost the probability of more likely tokens with both genders while the probability of less likely tokens with one or both genders being suppressed or relatively unaffected (See Fig. <ref type="figure">2</ref> for a visual illustration). We experiment with different functions to combine these distributions. Let z t = log P (x t |x &lt;t ) and z s = log P (x s |x &lt;s ) are the log-probability distributions (or logits) for the original and the corresponding counterfactual context, respectively 4 . The new unnormalized logits (z t ) are obtained with max, mean, expMean, or swap operation and illustrated in Table <ref type="table">1</ref>. We normalize z t so that it is a valid log distribution.</p><p>Intuitively, the max operation would preserve the most likely tokens among either context. The mean is similar to taking the product of the two 4 Due to sub-word tokens, the index of corresponding tokens in the original and counterfactual text may be different. We use index variable s to denote the corresponding token in the counterfactual sentence, indexed at t in the original sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Function</head><p>Operation</p><formula xml:id="formula_0">max z t = max{zt, z s } mean z t = z t +z s 2 expMean z t = log e z t +e z s 2 swap z t = z s</formula><p>Table <ref type="table">1</ref>: Operations used to modify token probabilities.</p><p>distributions, thereby increasing the likelihood of words that were more likely in both cases and lowering the likelihood of any other words. One may also consider any weighted combination of z and z . Infact, the swap operation is an extreme case of a weighted combination with the weight of original logits (i.e., z t ) being 0. Finally, expMean is the average of two distributions. Our approach is reminiscent of post-processing approaches that modify the next step probabilities during inference. However, we adapt it here for gender fair-knowledge distillation and use this procedure during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Counterfactual Data Augmentation</head><p>Using modified probabilities to update the student model rectifies the probability for the tokens generated after the gendered word. However, it only provides a weak signal by changing the log probabilities, and the training data may contain biases, which the student model can learn via cross-entropy loss (See Eq. ( <ref type="formula">1</ref>)). To this end, we also augment counterfactual data to the training set. Counterfactual data augmentation has been successfully used for gender bias mitigation in various downstream tasks such as static word embedding training <ref type="bibr" target="#b17">(Hall Maudslay et al., 2019)</ref> and co-reference resolution <ref type="bibr" target="#b27">(Lu et al., 2020)</ref>. However, it has not been explored in knowledge distillation or fair</p><p>LM training for open-ended language generation. Therefore, we also experiment with counterfactual data augmentation combined with the proposed next-token logit update strategy.</p><p>We refer to our approaches as Equitable Role Alteration (ERA). Primarily, the logit modification approach reduces bias in the teacher model's predicated probabilities, thus affecting only the KL divergence component. By contrast, counterfactual data augmentation involves adding new samples to the training set, affecting both loss components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training Setup</head><p>We use GPT2-small, a 12 layer transformer-based LM comprising of ?124M parameters, as the teacher model and a six-layer version of GPT-2 as the student model. We use OpenWebText corpus, which is an open-source reproduction of WebText corpus that was used to train GPT-2 in <ref type="bibr" target="#b34">Radford et al. (2019)</ref>. Due to limitations in computational budget, we use 10% of the corpus for training. We used the knowledge distillation procedure presented in <ref type="bibr" target="#b36">Sanh et al. (2019)</ref>, but without the cosine loss between representations during knowledge transfer because adopting knowledge distillation for fair learning requires correcting the 'biased knowledge' from the teacher, but it is hard to amend biased contextual representations. This approach can also be used for fair finetuning of an LM by using the same teacher and the student model. In that case, one may initialize with the pretrained teacher's weights. For fair finetuning experiments, we use GPT2-small as both the teacher and the student. Details on training, text generation, and hyperparameters are provided in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation of Open-ended Generation</head><p>Fairness. We assess gender fairness in English text generation by evaluating the bias of an LM to associate a gender with gender-neutral professions during open-ended text generation. For this, we use the profession prompts and gender polarity metrics from BOLD <ref type="bibr" target="#b12">(Dhamala et al., 2021)</ref>. These prompts are 10,195 sentence beginnings extracted from the Wikipedia articles and refer to 18 different profession categories such as engineering, healthcare, arts &amp; entertainment, etc. Some examples of BOLD profession prompts are 'An animator is an artist who' and 'A flight nurse is a registered.'</p><p>Texts generated from the LMs with these prompts as contexts are evaluated for gender polarity.</p><p>The gender polarity score measures if the text is neutral, female-polar having words such as she, woman, etc., or male-polar having words such as he, boy, etc. It is computed by taking the maximum of the normalized projection of each word vector in the LM generated text onto she -he.</p><p>The word vectors are computed on the debiased Word2Vec embeddings <ref type="bibr" target="#b4">(Bolukbasi et al., 2016)</ref> <ref type="foot" target="#foot_3">5</ref> . We use a threshold of 0.25 on the polarity score to label the text as male or female polar. For each profession group, we compute the equitability ratio as min{ m f , f m }, where m and f are the numbers of text generations labeled as male and female polar, respectively. The equitability ratio ? [0, 1] with 1 indicating equitable treatment. We report average and min equitability scores across all professions to summarize the disparity<ref type="foot" target="#foot_4">6</ref> .</p><p>Perplexity/Fluency. For real-world applications, an LM should demonstrate high-quality generations along with fair generations. To this end, we report the perplexity of the wikitext-2 test set <ref type="bibr" target="#b28">(Merity et al., 2017)</ref> as predicted by the trained LM. Similar to <ref type="bibr" target="#b25">Liu et al. (2021)</ref>, we evaluate the fluency of the completed prompts from BOLD. The fluency is measured as the perplexity of generated text predicted by the GPT2-large model. Lower perplexity and fluency scores are better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baselines and Other Methods</head><p>First, we test the utility of our approach in knowledge distillation compared to teacher and distilled models trained without fairness constraints. We use pre-trained GPT2-small (unfair teacher model) and DistilGPT-2 from the HuggingFace (HF) model repository<ref type="foot" target="#foot_5">7</ref> . Since training hyperparameters and dataset used by DistilGPT-2 (HF) is different from ours, we also train a DistilGPT-2 using our setup.</p><p>Next, we compare our approach with two genderbias mitigation approaches by applying them to the distilled version of GPT-2 and GPT2-small from the HF repository. We finetune the distilled models with the counterfactual and original sequences using only cross-entropy loss, which is similar to CDA <ref type="bibr" target="#b27">(Lu et al., 2020)</ref> and DAPT <ref type="bibr" target="#b14">(Gururangan et al., 2020)</ref>. We also compare with the biasmitigation approach of <ref type="bibr" target="#b38">Sheng et al. (2020)</ref>, which searches for adversarial prompts that increase the likelihood of specifically curated fair texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results on Open-ended Text Generation</head><p>Table <ref type="table" target="#tab_2">2</ref> summarizes results for gender disparity mitigation in open-ended generation for DistilGPT-2 and GPT2-small. We observe that compared to the teacher GPT2-small model, which has more parameters, the distilled versions (DistilGPT-2) are more biased which is indicated by lower equitability scores. Due to using only 10% sequences for training, our implementation of DistilGPT-2 has higher perplexity than the HF's version.</p><p>Fair Knowledge Distillation with DistilGPT-2. Rows 4-7 in Table <ref type="table" target="#tab_2">2</ref> show results of using only modified teacher logits based on counterfactuals (Sec. 4.2) with various operations. Overall, these modifications improve over the baseline Distil-GPT-2 model in terms of equitability ratios with only a slight increase in perplexity. Models trained with expMean, max, and swap scored similar or higher equitability than the teacher model. The mean operation was the least effective at improving fairness. The approach that uses only counterfactual data augmentation (row 8 in Table <ref type="table" target="#tab_2">2</ref>)</p><p>showed more than 1.5? improvement in equitability while keeping perplexity almost equal to the baseline model <ref type="bibr">(40.93 vs. 40.88)</ref>. By contrast, the two-step process of creating a distilled model and then finetuning with counterfactual data (using only cross-entropy loss) resulted in a worse perplexity of 41.63 but better equitability. Our approach combining logit modification and data augmentation (rows 9-10, Table <ref type="table" target="#tab_2">2</ref>) provides better equitability among all the models. Compared to the two-step finetuning approach (i.e., distillation then bias-mitigation), it has better equitability with similar perplexity. The adversarial prompt-based approach of Sheng et al. ( <ref type="formula">2020</ref>) performs much worse in terms of fairness. One of the reasons for this could be that the adversarial prompts are created to perform well on a small curated dataset which may not generalize. We omitted the perplexity values for this approach as it is not consistent with our evaluation process.</p><p>When combining logit modification and data augmentation, we experimented with modifying logits of both counterfactual and original text, and only of the original text. We found that the results with both approaches are similar and report results of modifying both texts in Table <ref type="table" target="#tab_2">2</ref>. The models obtained by combining the counterfactual data augmentation and logit update produce text with very little disparity and achieve the best fairness. Even though the fluency metrics are low, the perplexity for these models is higher. We noticed a high variance in fluency for some of the models. Upon further investigation, we found that the fluency can be very large for one of the profession groups, resulting in a large overall variance during macro averaging. We remark that fluency is at best a noisy measure as it uses an LM to evaluate the outputs; perplexity should be considered a more reliable measure of LM quality. For further evaluations and discussion, we use models trained with the max operation, as the results with the max operation for logit modification, with and without counterfactual augmentation, were most consistent.</p><p>Fair Finetuning with GPT-2. We also experiment with finetuning GPT2-small to train genderfair models. The approach is similar to finetuning with counterfactual augmented data but employs knowledge distillation loss instead. Table <ref type="table" target="#tab_2">2</ref> (rows 13-16) summarizes the results for training fair GPT2-small models. Unlike results with distilled models, all the approaches are fairly competitive. We remark that finetuning and our best approach have similar fairness performance, but our approach has better perplexity owing to improved learning due to the additional KL-divergence term.</p><p>However, models trained using only data augmentation or logit modification resulted in less equitability. The student model has two loss componentscross-entropy and KL divergence loss. When employing only one of the techniques, the student model may receive training signals from unfair teacher logits in the former case and training data in the latter case, learning less equitable models. We also note that only logit modification with max operation led to worse results in terms of quality and fairness compared to the baseline GPT-2 model. This could be due to the cross-entropy loss being the dominant training signal, and original training sequences may have spurious gender correlations. The adversarial-prompt approach of <ref type="bibr" target="#b38">Sheng et al. (2020)</ref> has lower fluency than other models. On further inspection of generated texts, we noticed that the LM sometimes generates degenerate phrases related to the adversarial prompt instead of the actual prompt about the profession, leading to poor quality generations. Additionally, we did a human evaluation to assess the quality of generated text (See Appendix A). We find the quality of texts generated from our less biased GPT2-small (ERA)</p><p>to be similar to GPT2-small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Gender Fairness on Other Tasks</head><p>It is often expected that different fairness measures designed for different but related tasks would be correlated. However, recently Goldfarb-Tarrant et al. ( <ref type="formula">2021</ref>) found that fairness measures for static word embeddings and downstream tasks do not correlate. To this end, we study if our fair text generation models improve fairness on other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Bias in Contextual Embeddings</head><p>We  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and Limitations</head><p>Mitigating disparity across races. We conducted preliminary experiments to test if the proposed approach can be extended to different race groups. Similar to Dhamala et al. ( <ref type="formula">2021</ref>), we consider race bias manifested via people's names and race-specific tokens across four races common in the US: African, European or White, Hispanic &amp; Latino, and Asian. We construct a many-to-many mapping that maps words referring to a given race to words referring to the other races for the counterfactual generation. The rest of the method remains the same as Sec. 4. For fairness evaluation, we use race prompts from BOLD and regard classifier from <ref type="bibr" target="#b40">Sheng et al. (2019)</ref>, which evaluates whether the person in the text is portrayed as being 'highly thought of.' Results show that the LMs obtained with the proposed approach were less biased in treating different races similarly, indicating that the proposed approach can be extended to other nonbinary groups. However, the improvements were not as significant as gender bias mitigation, leaving plenty of scope for improvement left for future work. We describe the results and experiments in more detail in Appendix C.</p><p>Counterfactual data generation. Dictionarybased word-swapping is a simple and effective method for counterfactual generation <ref type="bibr" target="#b26">(Lu, 2020;</ref><ref type="bibr">Zhao et al., 2018a)</ref>. However, blind word swapping can also result in factually and/or grammatically incorrect texts. To quantify these errors, we manually evaluated 500 randomly sampled coun-terfactual texts for gender category. We found that 22 (4.4%) of these sentences were incorrect (See Appendix B.4). In this paper, we demonstrate that despite counterfactual data generation not being perfect, it can effectively reduce the gender biases in the model. We expect our bias mitigation approach to benefit from further research in counterfactual data generation, especially for reducing race disparity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We proposed techniques to use counterfactual information during knowledge distillation to mitigate gender bias in LMs. In experiments, we show that this approach improves fairness in text generation, but it does not simultaneously enhance fairness on LM embedding and downstream classification task. LMs have become the Swiss army knife of NLP because modeling next word probabilities can learn versatile models that are effective on many tasks. It was surprising that reducing gender disparity in text generation had little effect on other downstream tasks. This finding underscores the importance of evaluating LM fairness along multiple metrics and tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Broader Impact and Ethics Statement</head><p>As language models become prominent, it is imperative to understand and mitigate various harms that they may provoke <ref type="bibr" target="#b41">(Solaiman et al., 2019;</ref><ref type="bibr" target="#b5">Bommasani et al., 2021)</ref>. Moreover, to make language processing resource-efficient, more focus should be on achieving good performance with smaller models. Finally, we propose approaches to create less biased LMs. However, similar to how gifts were used as weapons in Le Guin's Gifts (Le <ref type="bibr" target="#b23">Guin, 2006)</ref>, our approach can be repurposed to cause even more disparate treatment. For example, one may remove the mention of a specific race or gender completely from the training set to create a dystopian LM that does not acknowledge that group or entity's existence or the inaccuracy of counterfactual generation may cause LM to learn from fictional and non-grammatical texts. Nevertheless, we hope that our work will inspire more good than harm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary: Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal A Human Evaluation of Generated Text</head><p>We evaluate the quality of text generated from GPT2-small, fair-GPT2-small (ERA), and Sheng et al. ( <ref type="formula">2020</ref>) (adversarial prompt method with GPT2-small). We randomly sampled 300 prompts and their corresponding text generations from all three models. We then asked annotators to annotate for two tasks. The first task was to rank the generation quality among three sentences generated with the same prompt. The labels for the ranking task were: 1 -Worst, 2 -Medium, and 3 -Best. The second task was to rate the generation quality on a scale from 1-6 -1 being very poor, 2 being poor, 3 being fair, 4 being average, 5 being good, and 6 being excellent. Unlike the ranking task, the ratings are independent of generations from other models for the same prompt. When rating the quality, we asked the annotators to focus on the following properties of the text.</p><p>? Is it gibberish and nonsensical?</p><p>? Does the generation fit the prompt?</p><p>? Is the text grammatically correct?</p><p>? Is the text consistent and coherent? Is the generation meaningful?</p><p>? Could the text have been written extracted from news, books, etc.?</p><p>? Could the text have been written by a Human? We also provided some example annotations, as shown in Table <ref type="table" target="#tab_7">4</ref>.</p><p>The four annotators participating in these tasks are volunteers proficient in English, originating from various countries but presently or in the past studied/worked in the US, and familiar with language models. The annotators were informed of the research problem. We followed our institution's review process and approval guidelines for these annotation tasks. For each sentence, we collected three annotations. We only keep the ones where at least two annotators agree out of all annotations.</p><p>The mean and standard deviation of rankings for generations from GPT2-small, fair GPT2-small, and <ref type="bibr" target="#b38">Sheng et al. (2020)</ref> were 2.55 ? 0.55, 2.34 ? 0.64, and 1.12 ? 0.41, respectively. Text generated from GPT2-small is ranked highest most of the time. However, the fairer GPT2-small obtained with our method is a close second. The average ratings for generations from GPT2-small, fair GPT2-small (ERA), and <ref type="bibr" target="#b38">Sheng et al. (2020)</ref> were respectively, 3.01 ? 1.04, 2.707 ? 1.07, and 1.12 ? 0.41. Consistent with the ranking results, GPT2-small received the highest rating, followed closely by the generations from fairer GPT2-small obtained with our method. Both ranking and rating results indicate that our approach retains most of the performance while reducing gender disparity in the generated text. We find that Sheng et al.</p><p>(2020) resulted in low-quality generations. As also discussed in the main paper, this could be because the adversarial prompts are designed to increase the likelihood of specially curated fair text and may not work for diverse prompt datasets like BOLD, which contains diverse sentences beginning from various Wikipedia articles. Moreover, we also noticed that the adversarial prompts could lead to generation unrelated to the actual prompt and generate text referring to phrases in the adversarial prompt instead. We provide some example text generations from these approaches in Table <ref type="table">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Counterfactual Role-Reversal Data Generation</head><p>Counterfactual sequences were generated for ? 78% and ? 65% of the training sequences for gender and race domain experiments, respectively. We limit sequence lengths to 1024 for training. We generate one counterfactual sequence for every sequence in the training set that has words matching with our lists and referring to the demographic groups. The word lists are described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Gender Word Lists</head><p>To generate counterfactual texts for gender disparity experiments, we create mappings between maleto-female words and vice versa using word lists from <ref type="bibr">Zhao et al. (2018a)</ref> <ref type="foot" target="#foot_7">9</ref> . We consider some additional words to mappings derived from the above lists, shown in Table <ref type="table" target="#tab_11">6</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Race Word Lists</head><p>We focus on four US-specific races: Asian-American, Hispanic &amp; Latino-American, European-American, and African-American. To create counterfactual text for mitigating racial disparity, we use word sets from different categories. Table <ref type="table" target="#tab_12">7</ref> shows the word sets we have used. We process and use these word sets as follows.</p><p>? For words in the country and race category, we append ' American' and '-American' and their equivalent lower case versions and consider these as the actual word sets. Similarly, we consider both capital and lower case variations of the country and race terms.</p><p>? For words in the color category of Table <ref type="table" target="#tab_12">7</ref>, we use both capital/lower cases and singular/plural versions.</p><p>? We use two indicators of Latin race 'latino' and 'latina' and swap them with words from Asian-, African-&amp; European-American countries word sets but not vice versa.</p><p>? We created the list of first names from <ref type="bibr" target="#b42">Tzioumis (2018)</ref>. They provide prominent first names and the percentage of times this name belonged to a particular race. We use names that are 100% of the time assigned to a particular race and that are in the top-100 names for each race. We use the capital case version of the first names.</p><p>? We collected the list of common last names from <ref type="bibr" target="#b8">Comenetz (2016)</ref> and used the capital case version. Other works have also used names as the indicator of race <ref type="bibr" target="#b29">(Mishra et al., 2020;</ref><ref type="bibr" target="#b7">Caliskan et al., 2017)</ref>.</p><p>We replace the word from a specific row and column with words from other columns in the same row randomly to create a counterfactual text. For example, the original text, 'With each new location, Vazquez and Maritza must maintain the quality their fans have come to associate with the brand.' is converted to 'With each new location, Banks and Maritza must maintain the quality their fans have come to associate with the brand.'. Similarly, in the case of gender, the text 'Your father was a drummer in a rock band?' is converted to 'Your mother was a drummer in a rock band?'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Note about Sub-word Embeddings</head><p>We use counterfactual text in two ways, as described in Sec. 4. Due to sub-word embeddings, the length of the counterfactual sequence may not be the same as the original. This is particularly problematic for modifying probability distribution as we have to know the exact location of the corresponding token in the counterfactual and origi-Generations with GPT2 ERA 1 In their study, geographers use four ices as habitats. The icy crust of Antarctica is seen as an arid backdrop for millions of years. But the same frozen crust, making up just over one third of the continent, was striking new shades of blue on Sept. 24, 2010, when a glacier erupted into Greenland's Lhotse Basin of glacial melt. Journal reference: Geophysical Research Letters, doi:10 2 Biotechnology firms can contribute to future ills and possibilities of human development, this paper suggests. Although the link between the mass production of cellulose, corn, and protein on species-to-species conversion studies and the future of farmers utilizing these crops is well-established, and has been shown to be useful for food-factory improvement, a plethora of gene-fixing (gen-catalogical) techniques could be added to the food production process as a way to understand other 3 A clinical pharmaceutical scientist is a licensed, practicing uthymologist who provides the therapeutic data for clinical trials. Because of their clinical studies and publication that are published, such experts are called "Nutronans" because they provide practical and practical information to the public about the treatment of many patients. Recently, scientists published the results of a double-blind trial. The first of the first trials to be published with the Food and Drug Administration (FDA 4 A choreographer is one who creates ides, styles a verse and dances a magic formula on the disco stage. So far, I've been unable to find someone whose chops I have known, despite my unflattering reputation for being a professional choreographer. But one thing I've been more familiar with than a career that started with the punk songs of Riot and Dead Kids has changed my career. As many of you know, in 1990, Comixan Echols, then 5 Military logistics was already practiced in ersed in Persia (some time after the first Persian Oceanic, this practice may have been described as the business of the king). That. He may have been telemarking, as we had started to accept that in Persia.</p><p>? From the mid 1800s through the late 1800s. ? In between his late forties and after his death in 1874. The chronological discrepancy between Persia and Iran continued through the 6 Recorded social dances of the late 16th vernacular style are often followed by particular theatrical content as outlined by the Scotiablogists of 1289-1500. It is the music and the whole ensemble around which the music has been written. Take for instance the carnatonic pavilion-kiln's on the page as given by Sermon Charles that Nennius prophesied, with the success of Zebes and Maelto, that when the whole orchestra Generations with GPT2 1 There are now school counselors in Shandong County Jail. These are going to help kids transition to a safer place." If you want to stay informed about our events, let us know by commenting below or by on our Facebook page or following us on Twitter @Daydreamscout Photo: iStockphoto.com/XiaoXiho 2 The public relations officer or chief communications officer said, The military had no plans to write any violence into Afghanistan's speech. The next day, the term "andoregional incitement" was deployed in the speech and incident statement which was based on statements made by Rahmatullah Khan. http://www.newstimes.co.nz/national/1984338/displayingsmoke-and-walls-in-republicans-speech 3 Many nurse educators have a clinical specialty in providing a safe home for social service providers, providing consultations and providing outpatient services for breastfeeding moms. There are many other professional services provided by nurse educators that can be critical to the success of a program such as Attic with mother and child care provider services. We cannot emphasize enough how important breastfeeding and the health of mothers and their babies is as a provider of social services. It's also important to remember that these practices and 4 Breakdancing is typically set to songs which are considered better by a female audience, the more options the audience has to choose between. Other examples are Subverted Sadness, Sadness Goes Home, if sung by The Bugles in The Turner Hour, or hope &amp; change if a male audience chooses to reject it. 5 For biologists, knowledge must be usefully with the interpretation of natural conditions and with the knowledge necessary to define their factors and forms. The objective of any study is to show some evolution of organisms that were designed and or selected, or allowed to evolve, or which have exhibited a novel ability by its natural context and the conditions of the environment. The ability to learn needs to be clearly quantified as a function of one or more physical, chemical, or biological factors and, depending Generations with <ref type="bibr" target="#b38">Sheng et al. (2020)</ref> 1 Scenography is the seamless synthesis of vernacular, visual and rhythmic characteristics," said Jon Forbes, the development manager at Widtat-MacMulling GmbH. Slating is the next stage for Widtat. Upholstered by the amount of data it can cache on its servers, Widtat launched with a working set of domains at the end of October and has expanded further over the course 2 The movement director may create, or research More Exploring concepts Explore the new direction under the lead of Takahiro Sasaki, an engineering genius. The lead teams of the past three years have worked on a range of graphical APIs that can provide a visual approach to hardware Soiling temperature maps (sometimes called -HotCatter), which reveal temperatures associated with various components Through testing of application applications to monitor Table <ref type="table">5</ref>: Examples of generations that the human annotators labeled as having a quality ? 4 (on a range 1 -6 where 6 is excellent) from different GPT2-small models.</p><p>nal sentence. To this end, we generate 'counterfactual token sequences' during training instead of 'counterfactual sentences'. We first create tokenized versions of word lists, i.e., a set of tokens representing a word (e.g., father is represented by {2988}) are mapped to another set of tokens (e.g., mother is represented by {2802}). Given a sentence such as 'Your father was a drummer in a rock band?', it is first tokenized as <ref type="bibr">{7120, 2988, 373, 257, 34269, 287, 257, 3881, 4097, 30} then converted to {7120, 2802, 373, 257, 34269, 287, 257, 3881, 4097</ref>, 30} ('Your mother was a drummer in a rock band?').</p><p>Also, depending on where and how the word occurs, it can be tokenized differently. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 On Limitations and Correctness of Counterfactual Sentences</head><p>For counterfactual data generation, we use a dictionary-based word-swapping approach. Such a naive approach has some obvious limitations as it does not guarantee the grammatical and factual correctness of the generated sentences. However, we hypothesize that while this approach can potentially generate incorrect data for some examples, overall, it is still a simple yet effective method to generate counterfactual data. In order to verify our hypothesis, we randomly sampled 500 sentences from the generated counterfactual data for gender category and analyzed these for correctness. Out of these 500 sentences, we found 22 (4.4%) incorrect sentences. Most of the errors are related to incorrect pronoun references, such as a male name being used with 'she' as a reference. One such example is 'Onelki Garcia had another interesting outing as she only allowed 1 hit, but did walk three and lasted just 2.2 innings.'</p><p>We emphasize that the main focus of the paper is not to generate better counterfactual data but to show that counterfactual data can be used to mitigate bias effectively during knowledge distillation. We expect our proposed approach to further benefit from advances in counterfactual data generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Mitigating Racial Disparity</head><p>Counterfactual Data Generation. While not the main focus of this study, we also conducted experiments to mitigate race bias, manifested towards the names of people from various races and certain race-related phrases/words. Since we consider more than two races and there is no one-toone mapping between names, we cannot use the same one-to-one substitution rule for counterfactual data generation as earlier in this case. Hence, we construct a many-to-many mapping that maps multiple words in a given race to multiple words in the remaining races. For each word in the sequence of tokens referring to one race, we substitute it with a randomly chosen word from the corresponding words-set from another race. Additional details and dictionaries used for counterfactual sentence generation are in Appendix B.</p><p>Racial Fairness Measure. We use race prompts from the BOLD Dataset to measure racial disparity and consider four races -Asian American, European American or Whites, African American or Blacks, and Hispanics &amp; Latin Americans. We use the regard classifier to measure regard for each race.</p><p>The regard classifier has three categories -positive, negative, and neutral regard. Intuitively, the regard classifier measures if sentences cause group A to be more highly thought of than group B. If this is the case, then the language model perpetuates bias towards group B <ref type="bibr" target="#b40">(Sheng et al., 2019)</ref>. To this end, we measure the ratio of positive and negatively regarded sentences for each racial group. A fair LM should have the same ratio for all the races. We report the variance across groups for each model to capture this intuition, and lower variance would imply more fair treatment. We also report the fraction of generated sentences labeled as having positive, negative, and neutral regard.</p><p>Result. Table <ref type="table" target="#tab_13">8</ref> shows the result of mitigating racial disparity in text generation with our proposed approach that exploits counterfactual data. We generated counterfactual data for this purpose by replacing mentions of one racial group with the other (see Appendix B for details). The base-line pre-trained models from Hugging-Face have consistently higher regard ratios than the baseline model we trained, indicating that they generated more positive regard than our models. However, these have more variance across groups, indicating more disparate treatment in terms of regard.</p><p>We note that our counterfactual mitigation approach using both logit modification and augmentation is promising for reducing different regard to different races, but the improvement is not substantial. This could be due to our simple counterfactual generation implementation since we randomly replace race-related words. We replace first and last names independently, which could create mismatched names. There has been some work on improving counterfactual sequence generation and studying its effects, such as <ref type="bibr" target="#b17">Maudslay et al. (2019)</ref>.</p><p>The authors show that techniques such as name pairing based on frequency can improve the effectiveness of counterfactual data. Another issue could be that we have focused on races in the American context, but the text sequences referring to another context (such as Indian or Asian contexts) can be mistakenly used to create counterfactuals. A better approach should identify and filter such texts. Finally, even though names have been used as indicators of race in our work and previous work, this may be a relatively poor indicator of race. Especially to identify races in the American context only compared to gendered words identifying gender roles leading to suboptimal results. We leave these explorations for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Training and Evaluation Details D.1 Language Model Training</head><p>We started with the knowledge distillation setup of <ref type="bibr" target="#b36">Sanh et al. (2019)</ref> <ref type="foot" target="#foot_8">10</ref> and tailored it to our requirements. We did not use the cosine loss between the representation. We assigned equal weights of 0.5 to LM loss and KL divergence term with a temperature of 2.0. We only use 10% of the OpenWebText sequences. All the models are trained using HuggingFace <ref type="bibr" target="#b43">(Wolf et al., 2020)</ref> and PyTorch <ref type="bibr" target="#b33">(Paszke et al., 2019)</ref> for three epochs with a learning rate of 10 -3 , AdamW optimizer, and a batch size of 1600. We use DeepSpeed <ref type="bibr" target="#b35">(Rasley et al., 2020)</ref> for distributed training using 8 V100 GPUs. One epoch took between 5-8 hours.</p><p>We used DistilGPT-2, which had six layers, an embedding size of 768, and 12 attention heads as the student model. We initialize student models with weights from the even-numbered layers of the teacher model, i.e., pretrained GPT2-small. When using GPT2-small as the student, we initialize with the pretrained GPT2-small.</p><p>For finetuning with counterfactual text baseline, we use the same training hyper-parameters as above but set the weight of KL divergence term to 0, and LM loss weight is set to 1. For DistilGPT-2, we initialize with DistilGPT-2 (HF) parameters instead of GPT2-small. This is because we will first distill the model and then finetune for fairness in an actual fair-finetuning setup. However, we remark that this model is slightly advantaged compared to our approach in terms of performance (perplexity). Unlike our ERA models, which only use 10% of text sequences from OpenWebText, it was distilled using all the data. For GPT2-small experiments, we initialize with the parameters of pretrained GPT2-small.</p><p>For adversarial prompts baseline of Sheng et al.</p><p>(2020) and GPT2-small, we use the adversarial prompt for man/woman condition from their paper (Appendix A, Table <ref type="table">5</ref> in their paper). We use their official implementation for DistilGPT-2 experiments to find the adversarial prompt with bias mitigation setting. We set disassociation and association loss to 1 and use "The man" and "The woman" as the demographics. The adversarial prompt found was " genomes genomes Parables Nutrition Nutrition Mathematics".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Language Model Evaluation</head><p>Text Generation. We use top-p sampling (Holtzman et al., 2020) with p = 0.9 and consider the top 10 sequences for all text generation experiments. We limit the max length of the generated sequence to 100.</p><p>Perplexity &amp; Fluency. Perplexity is measured as the exponentiated average negative log-likelihood of a sequence. Given a token sequence, X = {x 0 , x 1 , . . . , x m }, the perplexity of X, ppl(X) is,</p><formula xml:id="formula_1">ppl(X) = exp - 1 m m t=1 log P (x t |x &lt;t )</formula><p>GPT-2 is a fixed-length model with a max length of 1024. For this reason, we compute perplexity in chunks of length 1024 and stride of 512. We define fluency as the perplexity measured by GPT2-large with stride size 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Bios-Bias Training and Evaluation</head><p>We finetune language models on Bios-Bias task for 20 epochs with a batch size of 256, 10 -3 learning rate, and AdamW optimizer. Similar to De-Arteaga et al. ( <ref type="formula">2019</ref>), we use a 65-10-25 split of the dataset for training, validation, and testing. We use the validation set to pick the best model for evaluation. We do not update the pretrained language model weights during finetuning and use a weighted combination of all the embeddings. These weights are computed using attention. More specifically, we employ a learnable vector to do a dot-product with resulting embeddings (last-layer output or output before the decoder layer). The dot product result is normalized using softmax to compute the weight vector. The weighted combination of the embeddings is passed through a linear classifier to predict the label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 CEAT Details</head><p>We use CEAT Tests 6, 7, and 8. The set of target and attribute words that were considered for each test are shown in Since we are evaluating contextual embeddings, we will have multiple embeddings for each word based on the context of the word. Therefore, CEAT samples one of the embeddings of the word to compute ES and refers to it as ES i . A random-effects model is used to combine results of multiple such sampling. Eventually, the combined effect size (CES) is computed as:</p><formula xml:id="formula_2">CES = v i ES i v i ,</formula><p>Where v i is the inverse of the sum of in-sample variance and between-sample invariance.</p><p>Different contextual embeddings for a word are derived using the random occurrence of that particular word from Reddit. We use the official implementation of CEAT 11 with N=10000, which is the default in their implementation.   <ref type="bibr">miguel, mario, carmen, ana, rosa, roberto, ricardo, pedro, oscar, rafael, hector, raul, yolanda, javier, ramon, fernando, ruben, sergio, eduardo, angel, edgar, alejandro, armando, salvador, julio, arturo, alfredo, cesar, marco, alberto, guadalupe, enrique, alma, gerardo, irma, margarita, leticia, ernesto, silvia, guillermo, luz, rodolfo, felix, adriana, blanca, alfonso, gustavo, andres, omar, angelica, bertha, pablo, isabel, felipe, raquel, lorena, lourdes, juana, hilda, hugo, rogelio, ramiro, ignacio, rolando, abel, marcos, humberto, rosario, tomas, orlando, ismael, delia, gilberto, gabriela, elsa, susana, saul, josefina, israel, mercedes, lorenzo, alvaro, beatriz, reynaldo, rodrigo, maribel, leonardo, graciela, santiago, rigoberto Last Names xiong, zhang, huang, truong, yang, li, vang, huynh, vu, nguyen, ali, khan, wong, singh, chang, chung, ahmed washington, jefferson, booker, banks, joseph, mosley, jackson, charles, dorsey, rivers yoder, friednam, krueger, schwartz, schmitt, mueller, weiss, novak, o'connell, klein barajas, zavala, velazquez, avalos, orozco, vazquez, juarez, meza, huerta</ref>   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Gender</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell>Ppl (?)</cell><cell cols="2">Equitability (?)</cell><cell>Fluency (?)</cell></row><row><cell>Method</cell><cell>Mod fn.</cell><cell>Aug.</cell><cell></cell><cell>Average</cell><cell>Min</cell></row><row><cell>GPT2-small (Teacher)</cell><cell>N/A</cell><cell>N/A</cell><cell>25.17</cell><cell cols="3">0.561 ? 0.0136 0.311 ? 0.0162 54.04 ? 14.16</cell></row><row><cell>DistilGPT-2 (HF)</cell><cell>N/A</cell><cell>N/A</cell><cell>39.25</cell><cell cols="2">0.508 ? 0.0142 0.199 ? 0.0283</cell><cell>122.9 ? 1.64</cell></row><row><cell>DistilGPT-2 (Baseline)</cell><cell>N/A</cell><cell>N/A</cell><cell>40.88</cell><cell cols="2">0.492 ? 0.0107 0.237 ? 0.0256</cell><cell>80.6 ? 1.33</cell></row><row><cell>DistilGPT-2 (ERA)</cell><cell>mean</cell><cell>no</cell><cell>40.91</cell><cell cols="2">0.499 ? 0.0086 0.242 ? 0.0299</cell><cell>116.8 ? 59.5</cell></row><row><cell>DistilGPT-2 (ERA)</cell><cell>max</cell><cell>no</cell><cell>41.11</cell><cell cols="2">0.565 ? 0.0128 0.313 ? 0.0265</cell><cell>98.2 ? 1.64</cell></row><row><cell>DistilGPT-2 (ERA)</cell><cell>expMean</cell><cell>no</cell><cell>41.11</cell><cell cols="2">0.576 ? 0.0095 0.321 ? 0.0264</cell><cell>230 ? 263</cell></row><row><cell>DistilGPT-2 (ERA)</cell><cell>swap</cell><cell>no</cell><cell>41.22</cell><cell cols="2">0.587 ? 0.0144 0.303 ? 0.0402</cell><cell>89.2 ? 2.06</cell></row><row><cell>DistilGPT-2 (ERA)</cell><cell>none</cell><cell>yes</cell><cell>40.93</cell><cell cols="2">0.748 ? 0.0066 0.497 ? 0.0510</cell><cell>92.4 ? 0.65</cell></row><row><cell>DistilGPT-2 (ERA)</cell><cell>expMean</cell><cell>yes</cell><cell>41.73</cell><cell cols="2">0.892 ? 0.0052 0.693 ? 0.0260</cell><cell>85.5 ? 0.49</cell></row><row><cell>DistilGPT-2 (ERA)</cell><cell>max</cell><cell>yes</cell><cell>41.73</cell><cell cols="2">0.901 ? 0.0194 0.713 ? 0.0429</cell><cell>85.4 ? 0.24</cell></row><row><cell>DistilGPT-2 (Finetuning)</cell><cell>N/A</cell><cell>yes</cell><cell>41.63</cell><cell cols="2">0.869 ? 0.0142 0.632 ? 0.0305</cell><cell>521 ? 175.6</cell></row><row><cell>DistilGPT-2 (Sheng et al., 2020)</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">0.590 ? 0.0131 0.282 ? 0.0284</cell><cell>296 ? 337</cell></row><row><cell>GPT2-small (ERA)</cell><cell>max</cell><cell>no</cell><cell>26.97</cell><cell cols="2">0.489 ? 0.0106 0.268 ? 0.0170</cell><cell>55.89 ? 0.35</cell></row><row><cell>GPT2-small (ERA)</cell><cell>none</cell><cell>yes</cell><cell>26.60</cell><cell cols="2">0.821 ? 0.0081 0.598 ? 0.0417</cell><cell>54.97 ? 0.44</cell></row><row><cell>GPT2-small (ERA)</cell><cell>max</cell><cell>yes</cell><cell>27.61</cell><cell cols="2">0.884 ? 0.0151 0.687 ? 0.0404</cell><cell>57.19 ? 5.43</cell></row><row><cell>GPT2-small (Finetuning)</cell><cell>N/A</cell><cell>yes</cell><cell>28.56</cell><cell cols="2">0.899 ? 0.0116 0.673 ? 0.0553</cell><cell>54.59 ? 0.12</cell></row><row><cell>GPT2-small (Sheng et al., 2020)</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell cols="2">0.839 ? 0.0063 0.596 ? 0.0539</cell><cell>71.44 ? 0.87</cell></row></table><note><p>disparity in open-ended text generation as assessed by BOLD profession prompts for DistilGPT-2 and GPT2-small (result over 5 evaluation runs). Arrows indicate if higher (?) or lower (?) values are desired. Equitability measures vary from 0 to 1. We report the macro average of fluency across all 18 profession groups. ERA is our approach.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>evaluate if fairness in open-ended generation by LMs obtained via the proposed method also transfers to the LM's embeddings using the CEAT metric (Guo andCaliskan, 2021). The WEAT metric measures the effect size of social bias in a static embedding by computing the relative associations of two sets of target words (e.</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell cols="3">CEAT Tests (Effect Sizes)</cell><cell cols="2">Bios-Bias Classification</cell></row><row><cell>Method</cell><cell cols="3">Mod fn. Aug. Test 6</cell><cell>Test 7</cell><cell>Test 8</cell><cell cols="2">Accuracy (?) TPRD(?)</cell></row><row><cell>GPT2-small (Teacher)</cell><cell>N/A</cell><cell cols="4">N/A 0.326 -0.139 -0.040</cell><cell>0.818</cell><cell>0.1060</cell></row><row><cell>DistilGPT-2 (HF)</cell><cell>N/A</cell><cell cols="2">N/A 0.584</cell><cell>0.114</cell><cell>-0.078</cell><cell>0.813</cell><cell>0.0982</cell></row><row><cell>DistilGPT-2 (Baseline)</cell><cell>N/A</cell><cell cols="2">N/A 0.314</cell><cell>0.311</cell><cell>-0.065</cell><cell>0.815</cell><cell>0.1003</cell></row><row><cell>DistilGPT-2 (ERA)</cell><cell>max</cell><cell>no</cell><cell>0.245</cell><cell>0.223</cell><cell>-0.113</cell><cell>0.817</cell><cell>0.0981</cell></row><row><cell>DistilGPT-2 (ERA)</cell><cell>none</cell><cell>yes</cell><cell>0.366</cell><cell>0.274</cell><cell>0.016</cell><cell>0.816</cell><cell>0.1041</cell></row><row><cell>DistilGPT-2 (ERA)</cell><cell>max</cell><cell>yes</cell><cell>0.532</cell><cell>0.352</cell><cell>0.260</cell><cell>0.817</cell><cell>0.1020</cell></row><row><cell>GPT2-small (ERA)</cell><cell>max</cell><cell>no</cell><cell>0.212</cell><cell>0.182</cell><cell>-0.036</cell><cell>0.817</cell><cell>0.1085</cell></row><row><cell>GPT2-small (ERA)</cell><cell>none</cell><cell>yes</cell><cell>0.218</cell><cell>0.162</cell><cell>0.752</cell><cell>0.817</cell><cell>0.1031</cell></row><row><cell>GPT2-small (ERA)</cell><cell>max</cell><cell>yes</cell><cell>0.293</cell><cell>0.325</cell><cell>0.268</cell><cell>0.818</cell><cell>0.1070</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">However, the absolute effect size is often used</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">as the magnitude of bias (Goldfarb-Tarrant et al.,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">2021) 8 . As shown in Table 3, baseline models</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">have a larger effect size in tests 6 (male/female</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">names and career/family) and 7 (math/arts and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">male/female terms). In test 8 (male/female terms</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">and science/arts), there was not a strong bias in</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">the embeddings of baseline models. Overall, we</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">observe that the demonstrated fairness in LMs for</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">open-ended language generation in Sec. 5 is not</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">always reflected in the embeddings. For example,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">the model trained using modified logits based on</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">max operation has a smaller absolute effect size for</cell></row></table><note><p><p>g., career, office; and home, family) with two sets of attribute words (e.g., girl, woman; and boy, man). CEAT extends WEAT to contextual embedding by computing a distribution of effect sizes, each sample obtained by computing WEAT effect size on contextual embedding computed with a different context. CEAT summarizes the combined magnitude of bias by pooling effect sizes with a random-effects model. We use three CEAT tests that measure gender bias: 1) CEAT test 6 with attributes male/female names and targets career/family, 2) CEAT 7 with attributes male/female terms and target math/arts, and 3) CEAT 8 with attributes male/female terms and targets science/arts. See Appendix D for details.</p>Results. According to the combined effect sizes metric (known as Cohen's d), d &gt; 0.5 and d &gt; 0.8 are medium and large effect sizes, respectively.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Downstream gender fairness evaluation. See Sec. 6.1 and 6.2 for details about CEAT and Bios-Bias task, respectively. tests 6 and 7 but higher for test 8 compared to the baseline. Effect sizes on tests 7 and 8 have reduced when using the counterfactual data augmentation method, but it increased on test 6. Hence, the LM embedding fairness metric CEAT did not correlate with the fairness of LM in open-ended text generation tasks. This finding agrees with Goldfarb-Tarrant et al. (2021), but for contextual embeddings. They observed that downstream fairness measures and static embeddings are not correlated.</figDesc><table><row><cell>the pre-trained model. Overall, this suggests that</cell></row><row><cell>our method, even though effective in reducing dis-</cell></row><row><cell>parity for open-ended text generation, is not ade-</cell></row><row><cell>quate for this downstream task.</cell></row><row><cell>6.2 Fairness in Classification Task</cell></row><row><cell>We evaluate the hypothesis that an LM that is less</cell></row><row><cell>biased in text generation should be less biased on</cell></row><row><cell>downstream tasks by finetuning various baselines</cell></row><row><cell>and fairer versions of LM obtained in Sec. 5.4</cell></row><row><cell>on the Bios-Bias classification task (De-Arteaga</cell></row><row><cell>et al., 2019) and evaluating the classifier's fairness.</cell></row><row><cell>The objective is to predict one of the 28 profes-</cell></row><row><cell>sion classes from a person's biography. We use</cell></row><row><cell>a weighted combination of all token embeddings</cell></row><row><cell>with a linear layer for classification. Pre-trained</cell></row><row><cell>weights are not updated. For training details, see</cell></row><row><cell>Appendix D. Similar to De-Arteaga et al. (2019),</cell></row><row><cell>we take the average true positive rate difference</cell></row><row><cell>(TPRD) between males and females across all pro-</cell></row><row><cell>fessions as the fairness measure.</cell></row><row><cell>Results. A fair model should have a similar true</cell></row><row><cell>positive rate for both genders, i.e., TPRD ? 0.</cell></row><row><cell>However, we observe from Table 3 that TPRD</cell></row><row><cell>is around 0.1 for all the models, indicating that</cell></row><row><cell>all models lead to equally unfair outcomes. De-</cell></row><row><cell>Arteaga et al. (2019) presented a simple debiasing</cell></row><row><cell>technique of removing a set of predefined gendered</cell></row><row><cell>words (such as he, she, mrs.) from the biographies</cell></row><row><cell>before training, which resulted in an accuracy of</cell></row><row><cell>0.815 and TPRD of 0.0658 with DistilGPT-2 as</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>person worked for a high-security institution, and one day he went in to work only to find that he could not log in to his computer terminal. 2. the person was famous for her work on radioactivity and twice a winner of the Nobel Prize</figDesc><table><row><cell>Quality</cell><cell>Description</cell><cell>Examples</cell></row><row><cell>very poor</cell><cell>irrelavant to context, gib-berish</cell><cell>1. the person was known for 129$ inter got upperSDA here xxxx. ayayaya terrible</cell></row><row><cell>poor</cell><cell>not written by human, weird, illogical, repetitive</cell><cell>1. the person was known for dance, murder, dance, murder, dance 2. the person started working as a racist to get pregnant because in the sense of being equal to female</cell></row><row><cell></cell><cell>most likely not written by</cell><cell>1. the person earned money by sending spam by trading in his domain .</cell></row><row><cell>fair</cell><cell>human, partly fits the con-</cell><cell>2. the person earned money by selling his soul to companies and politicians</cell></row><row><cell></cell><cell>text</cell><cell>right from the start . -11973 , Mich . ,*</cell></row><row><cell></cell><cell></cell><cell>1. the person earned money by delivering sweets as a Valentine 's gift , The</cell></row><row><cell>average</cell><cell>partly natural sounding, partly fits the context</cell><cell>New York Times reported . 2. the person had a part-time job as a local caterer . He worked as a hair</cell></row><row><cell></cell><cell></cell><cell>stylist in an Atlanta apartment ,</cell></row><row><cell>good</cell><cell>natural sounding, fitting the context, may contain minor contradictions</cell><cell>1. the person had a job as a recruiter for recruitment agencies in the west of the country ,-</cell></row><row><cell></cell><cell></cell><cell>1. the</cell></row><row><cell>excellent</cell><cell>natural, fluent, human-written, fitting the context</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Generated texts and quality ratings that were shown as examples to annotators.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>To illustrate, consider the word 'he' in the next sentence. 'He should have arrived, but he has not arrived yet'. Clearly, the word 'he' appears in two different forms -capital-case and lowercase. Other forms are also possible. Also, GPT-2 tokenizer often has white space at the beginning of the token in its vocabulary. For this reason, we considered the word and some of the possible variations that can occur in the text. The next example best explains these variations. If the word were 'he', we use following variations -he| he| he,| he.| he'| he"|'he |"he |He |'He |"He .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>. Each test uses four set of</cell></row><row><cell>words -X, Y, A, and B. CEAT test works similar</cell></row><row><cell>to WEAT (Caliskan et al., 2017) and first evaluates</cell></row><row><cell>the difference in association of word w in set X</cell></row><row><cell>and Y to set A and B by computing difference of</cell></row><row><cell>average cosine distance as:</cell></row><row><cell>The cosine distances are computed between the</cell></row><row><cell>embeddings. It then computes the difference of</cell></row><row><cell>difference in association to measure if words in set</cell></row><row><cell>X and Y are considered differently, i.e.,</cell></row><row><cell>S(X, Y, A, B) = mean x?X s(x, A, B)</cell></row><row><cell>-mean y?Y s(y, A, B)</cell></row><row><cell>This provides an estimate of the absolute difference</cell></row><row><cell>between the association of embeddings. To eval-</cell></row><row><cell>uate if this difference is significant overall effect</cell></row><row><cell>size (ES) is computed by dividing with the standard</cell></row><row><cell>deviation the difference in the association of union</cell></row><row><cell>of set X and Y (in-sample variance). Intuitively,</cell></row><row><cell>we measure if the set X and Y have significantly</cell></row><row><cell>different associations than any other shuffling of</cell></row></table><note><p>s(w, A, B) = mean a?A cos(w, a) mean b?B cos(w, b)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>List of additional gender words.</figDesc><table><row><cell>Category</cell><cell cols="2">Asian-American</cell><cell cols="2">African-American</cell><cell cols="2">European-American</cell><cell>Hispanic &amp; Latino</cell></row><row><cell>Countries</cell><cell cols="2">korean, indian, chinese</cell><cell>nigerian,</cell><cell>ethiopian,</cell><cell cols="2">german, british, french,</cell><cell>mexican,</cell><cell>brazilian,</cell></row><row><cell></cell><cell cols="2">, japanese, indonesian,</cell><cell>egyptian,</cell><cell>congolese,</cell><cell cols="2">italian, spanish, roma-</cell><cell>salvadorian, honduran,</cell></row><row><cell></cell><cell cols="2">pakistani, bangladeshi,</cell><cell>tanzanian,</cell><cell>kenyan,</cell><cell cols="2">nian, dutch, belgian,</cell><cell>colombian,</cell><cell>cuban,</cell></row><row><cell></cell><cell cols="2">filipino, filipina, veit-</cell><cell cols="2">ugandan, moroccan</cell><cell cols="2">greek, irish, portugese,</cell><cell>peruvian, ecuadorian,</cell></row><row><cell></cell><cell cols="2">namese, turkish, turk,</cell><cell></cell><cell></cell><cell>hungarian,</cell><cell>austrian,</cell><cell>chilean, haitian, costa</cell></row><row><cell></cell><cell>iranian,</cell><cell>burmese,</cell><cell></cell><cell></cell><cell>swish,</cell><cell>bulgarian,</cell><cell>rican, costa rican, tico,</cell></row><row><cell></cell><cell cols="2">iraqi, afghan, afghani,</cell><cell></cell><cell></cell><cell cols="2">finnish, slovak, nor-</cell><cell>dominican</cell></row><row><cell></cell><cell cols="2">arab, uzbek, yemeni,</cell><cell></cell><cell></cell><cell>weigian,</cell><cell>scottish,</cell></row><row><cell></cell><cell cols="2">nepalese, sri lankan,</cell><cell></cell><cell></cell><cell cols="2">polish, swedish, lithua-</cell></row><row><cell></cell><cell cols="2">sri-lankan, srilankan,</cell><cell></cell><cell></cell><cell cols="2">nian, danish, slovenian,</cell></row><row><cell></cell><cell cols="2">israeli, laotian, lebenese,</cell><cell></cell><cell></cell><cell cols="2">latvian, estonian</cell></row><row><cell></cell><cell cols="2">lebanese, palestinian,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>kuwaiti,</cell><cell>mongol,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">armenian, thai</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">First Names young,</cell><cell cols="6">mohammed, maria, jose, juan, carlos,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>luis, manuel, antonio,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>jorge, francisco, jesus,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Word lists for generating race counterfactuals.</figDesc><table><row><cell>, ibarra</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Racial disparity in open-ended text generation as assessed by BOLD Race prompts. We report the average of over five evaluation runs. The races are abbreviated, so African is African-American, Asian is Asian-American, etc. Fluency is the macro average across all 4 races. Value in the bracket show the fraction of positively and negatively regarded generations.</figDesc><table><row><cell>Test</cell><cell>X</cell><cell></cell><cell>Y</cell><cell>A</cell><cell>B</cell></row><row><cell cols="3">Test 6 male: John, Paul, Mike,</cell><cell>female: Amy, Joan,</cell><cell>career: executive, man-</cell><cell>family: home, par-</cell></row><row><cell></cell><cell cols="2">Kevin, Steve, Greg, Jeff,</cell><cell>Lisa, Sarah, Diana,</cell><cell>agement, professional,</cell><cell>ents, children, family,</cell></row><row><cell></cell><cell>Bill</cell><cell></cell><cell>Kate, Ann, Donna</cell><cell>corporation, salary, of-</cell><cell>cousins, marriage, wed-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>fice, business, career</cell><cell>ding, relatives</cell></row><row><cell cols="3">Test 7 math: math, algebra,</cell><cell>arts: poetry, art, dance,</cell><cell>male: male, man, boy,</cell><cell>female:</cell><cell>female,</cell></row><row><cell></cell><cell>geometry,</cell><cell>calculus,</cell><cell>literature, novel, sym-</cell><cell>brother, he, him, his,</cell><cell>woman, girl, sister, she,</cell></row><row><cell></cell><cell cols="2">equations, computation,</cell><cell>phony, drama, sculpture</cell><cell>son</cell><cell>her, hers, daughter</cell></row><row><cell></cell><cell cols="2">numbers, addition</cell><cell></cell><cell></cell></row><row><cell cols="3">Test 8 science: science, tech-</cell><cell>arts: poetry, art, Shake-</cell><cell>male: brother, father,</cell><cell>female: sister, mother,</cell></row><row><cell></cell><cell cols="2">nology, physics, chem-</cell><cell>speare, dance, litera-</cell><cell>uncle, grandfather, son,</cell><cell>aunt,</cell><cell>grandmother,</cell></row><row><cell></cell><cell cols="2">istry, Einstein, NASA,</cell><cell>ture, novel, symphony,</cell><cell>he, his, him</cell><cell>daughter, she, hers, her</cell></row><row><cell></cell><cell cols="2">experiment, astronomy</cell><cell>drama</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Words sets and categories used in CEAT tests.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The teacher model refers to the original LM, and the student model refers to the LM being trained. The latter usually has fewer parameters.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Our approach may use the same student model as the teacher, as we demonstrate in Sec. 5.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We found 96% of the generated data on manual analysis to be correct (See Appendix B.4 for details).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://github.com/tolga-b/debiaswe</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>We note that this evaluation is not perfect.<ref type="bibr" target="#b16">Gonen and Goldberg (2019)</ref> show that debiased word embedding still reserves some gender information for neutral words.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>https://huggingface.co/models</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>P-values are not reported as it does not indicate the magnitude of the bias, and all models were most certainly biased.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>Specifically, we use word lists available at https: //github.com/uclanlp/corefBias/blob/ master/WinoBias/wino/extra_gendered_ words.txt, and https://github.com/uclanlp/ corefBias/blob/master/WinoBias/wino/ generalized_swaps.txt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>https://github.com/huggingface/ transformers/tree/master/examples/ research_projects/distillation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_9"><p>stephanie, jonathan, janet, cheryl, catherine, heather, judith, todd, lori, keith, jessica, bruce, craig, joshua, raymond, denise, ann, brenda, teresa, terry, katherine, alan, adam, kathryn, carolyn, nicholas, lawrence</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Solon</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
		<title level="m">Fairness and Machine Learning. fairmlbook.org</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442188.3445922</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;21</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language (technology) is power: A critical survey of &quot;bias&quot; in NLP</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Solon</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Daum?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.485</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5454" to="5476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stereotyping Norwegian salmon: An inventory of pitfalls in fairness benchmark datasets</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilsinia</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Olteanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><surname>Wallach</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.81</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1004" to="1015" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Man is to computer programmer as woman is to homemaker? debiasing word embeddings</title>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Tauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalai</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. 2016. December 5-10, 2016</date>
			<biblScope unit="page" from="4349" to="4357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Michael S Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><surname>Brunskill</surname></persName>
		</author>
		<idno>abs/2108.07258</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantics derived automatically from language corpora contain human-like biases</title>
		<author>
			<persName><forename type="first">Aylin</forename><surname>Caliskan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><forename type="middle">J</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aal4230</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="issue">6334</biblScope>
			<biblScope unit="page" from="183" to="186" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Frequently occurring surnames in the 2010 census</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Comenetz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>United States Census Bureau</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Plug and play language models: A simple approach to controlled text generation</title>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bias in Bios: A case study of semantic representation bias in a high-stakes setting</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>De-Arteaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chouldechova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahin</forename><surname>Geyik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnaram</forename><surname>Kenthapadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Tauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalai</forename></persName>
		</author>
		<idno type="DOI">10.1145/3287560.3287572</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* &apos;19</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency, FAT* &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Harms of gender exclusivity and challenges in non-binary representation in language technologies</title>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masoud</forename><surname>Monajatipoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anaelia</forename><surname>Ovalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Ar-Jun Subramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.150</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1968" to="1994" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BOLD: Dataset and metrics for measuring biases in open-ended language generation</title>
		<author>
			<persName><forename type="first">Jwala</forename><surname>Dhamala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyapriya</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442188.3445924</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;21</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="862" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Queens are powerful too: Mitigating gender bias in dialogue generation</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.656</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8173" to="8188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">RealToxic-ityPrompts: Evaluating neural toxic degeneration in language models</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Samuel Gehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.301</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3356" to="3369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Intrinsic bias metrics do not correlate with application bias</title>
		<author>
			<persName><forename type="first">Seraphina</forename><surname>Goldfarb-Tarrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Marchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><forename type="middle">Mu?oz</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mugdha</forename><surname>Pandya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.150</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1926" to="1940" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wei Guo and Aylin Caliskan. 2021. Detecting emergent intersectional biases: Contextualized word embeddings contain a distribution of human-like biases</title>
		<author>
			<persName><forename type="first">Hila</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.740</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ana</forename><surname>Marasovi?</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019. 2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
	<note>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">It&apos;s all in the name: Mitigating gender bias with name-based counterfactual data substitution</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Hall Maudslay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hila</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1530</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5267" to="5275" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">TinyBERT: Distilling BERT for natural language understanding</title>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.372</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4163" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">GeDi: Generative discriminator guided sequence generation</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mc-Cann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Fatema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajani</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.424</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana</addrLine></address></meeting>
		<imprint>
			<publisher>Dominican Republic</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4929" to="4952" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Counterfactual fairness</title>
		<author>
			<persName><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">R</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. 2017. December 4-9, 2017</date>
			<biblScope unit="page" from="4066" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ursula K Le</forename><surname>Guin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Gifts. Wadsworth Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards understanding and mitigating social biases in language models</title>
		<author>
			<persName><forename type="first">Paul Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07">2021. 18-24 July 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="6565" to="6576" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DExperts: Decoding-time controlled text generation with experts and anti-experts</title>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.522</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6691" to="6706" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Masked reasoner at SemEval-2020 task 4: Fine-tuning RoBERTa for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Daming</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.semeval-1.49</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Semantic Evaluation</title>
		<meeting>the Fourteenth Workshop on Semantic Evaluation<address><addrLine>Barcelona</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="411" to="414" />
		</imprint>
	</monogr>
	<note>International Committee for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gender bias in neural natural language processing</title>
		<author>
			<persName><forename type="first">Kaiji</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Mardziel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangjing</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-62077-6_14</idno>
	</analytic>
	<monogr>
		<title level="m">Logic, Language, and Security</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="189" to="202" />
		</imprint>
	</monogr>
	<note>Preetam Amancharla, and Anupam Datta</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Assessing demographic bias in named entity recognition</title>
		<author>
			<persName><forename type="first">Shubhanshu</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Belli</surname></persName>
		</author>
		<idno>abs/2008.03415</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving counterfactual generation for fair hate speech detection</title>
		<author>
			<persName><forename type="first">Aida</forename><surname>Mostafazadeh Davani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Omrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Atari</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.woah-1.10</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)</title>
		<meeting>the 5th Workshop on Online Abuse and Harms (WOAH 2021)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="92" to="101" />
		</imprint>
	</monogr>
	<note>Xiang Ren, and Morteza Dehghani</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">StereoSet: Measuring stereotypical bias in pretrained language models</title>
		<author>
			<persName><forename type="first">Moin</forename><surname>Nadeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Bethke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.416</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5356" to="5371" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CrowS-pairs: A challenge dataset for measuring social biases in masked language models</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasika</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.154</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1953" to="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3406703</idno>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Virtual Event, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-08-23">2020. August 23-27, 2020</date>
			<biblScope unit="page" from="3505" to="3506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno>abs/1910.01108</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Social bias frames: Reasoning about social and power implications of language</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.486</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5477" to="5490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards Controllable Biases in Language Generation</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.291</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3239" to="3254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Societal biases in language generation: Progress and challenges</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.330</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4275" to="4293" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The woman worked as a babysitter: On biases in language generation</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1339</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3407" to="3412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Irene Solaiman and Christy Dennison. 2021. Process for adapting language models to society (palms) with values-targeted datasets</title>
		<author>
			<persName><forename type="first">Irene</forename><surname>Solaiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Kreps</surname></persName>
		</author>
		<idno>abs/2106.10328</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
	<note>Release strategies and the social impacts of language models</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Data for: Demographic aspects of first names</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Tzioumis</surname></persName>
		</author>
		<idno type="DOI">10.7910/DVN/TYJKEZ</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Transformers: State-of-theart natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Detoxifying language models risks marginalizing minority voices</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eshaan</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.190</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2390" to="2397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution: Evaluation and debiasing methods</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2003</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning gender-neutral word embeddings</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1521</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4847" to="4853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Challenges in automated debiasing for toxic language detection</title>
		<author>
			<persName><forename type="first">Xuhui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.274</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3143" to="3155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Counterfactual data augmentation for mitigating gender stereotypes in languages with rich morphology</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>Zmigrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabrina</forename><forename type="middle">J</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1161</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1651" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
