<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combinatorial Bandits Revisited</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Richard</forename><surname>Combes</surname></persName>
							<email>richard.combes@supelec.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Centrale-Supelec</orgName>
								<address>
									<postCode>L2S</postCode>
									<settlement>Gif-sur-Yvette</settlement>
									<country key="FR">FRANCE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Sadegh Talebi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Automatic Control</orgName>
								<orgName type="institution">KTH</orgName>
								<address>
									<settlement>Stockholm</settlement>
									<country key="SE">SWEDEN</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>Proutiere</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Automatic Control</orgName>
								<orgName type="institution">KTH</orgName>
								<address>
									<settlement>Stockholm</settlement>
									<country key="SE">SWEDEN</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Lelarge</surname></persName>
							<email>marc.lelarge@ens.fr</email>
							<affiliation key="aff2">
								<orgName type="institution">INRIA &amp; ENS</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">FRANCE</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Combinatorial Bandits Revisited</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">41E4015C30F7089B7F66993656B653B7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper investigates stochastic and adversarial combinatorial multi-armed bandit problems. In the stochastic setting under semi-bandit feedback, we derive a problem-specific regret lower bound, and discuss its scaling with the dimension of the decision space. We propose ESCB, an algorithm that efficiently exploits the structure of the problem and provide a finite-time analysis of its regret. ESCB has better performance guarantees than existing algorithms, and significantly outperforms these algorithms in practice. In the adversarial setting under bandit feedback, we propose COMBEXP, an algorithm with the same regret scaling as state-of-the-art algorithms, but with lower computational complexity for some combinatorial problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-Armed Bandit (MAB) problems <ref type="bibr" target="#b0">[1]</ref> constitute the most fundamental sequential decision problems with an exploration vs. exploitation trade-off. In such problems, the decision maker selects an arm in each round, and observes a realization of the corresponding unknown reward distribution. Each decision is based on past decisions and observed rewards. The objective is to maximize the expected cumulative reward over some time horizon by balancing exploitation (arms with higher observed rewards should be selected often) and exploration (all arms should be explored to learn their average rewards). Equivalently, the performance of a decision rule or algorithm can be measured through its expected regret, defined as the gap between the expected reward achieved by the algorithm and that achieved by an oracle algorithm always selecting the best arm. MAB problems have found applications in many fields, including sequential clinical trials, communication systems, economics, see e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>In this paper, we investigate generic combinatorial MAB problems with linear rewards, as introduced in <ref type="bibr" target="#b3">[4]</ref>. In each round n ≥ 1, a decision maker selects an arm M from a finite set M ⊂ {0, 1} d and receives a reward M X(n) = d i=1 M i X i (n). The reward vector X(n) ∈ R d + is unknown. We focus here on the case where all arms consist of the same number m of basic actions in the sense that M 1 = m, ∀M ∈ M. After selecting an arm M in round n, the decision maker receives some feedback. We consider both (i) semi-bandit feedback under which after round n, for all i ∈ {1, . . . , d}, the component X i (n) of the reward vector is revealed if and only if M i = 1; (ii) bandit feedback under which only the reward M X(n) is revealed. Based on the feedback received up to round n -1, the decision maker selects an arm for the next round n, and her objective is to maximize her cumulative reward over a given time horizon consisting of T rounds. The challenge in these problems resides in the very large number of arms, i.e., in its combinatorial structure: the size of M could well grow as d m . Fortunately, one may hope to exploit the problem structure to speed up the exploration of sub-optimal arms. We consider two instances of combinatorial bandit problems, depending on how the sequence of reward vectors is generated. We first analyze the case of stochastic rewards, where for all</p><formula xml:id="formula_0">Algorithm LLR CUCB CUCB ESCB [9] [10] [11] (Theorem 5) Regret O m 3 d∆max ∆ 2 min log(T ) O m 2 d ∆ min log(T ) O md ∆ min log(T ) O √ md ∆ min log(T )</formula><p>Table <ref type="table">1</ref>: Regret upper bounds for stochastic combinatorial optimization under semi-bandit feedback.</p><p>i ∈ {1, . . . , d}, (X i (n)) n≥1 are i.i.d. with Bernoulli distribution of unknown mean. The reward sequences are also independent across i. We then address the problem in the adversarial setting where the sequence of vectors X(n) is arbitrary and selected by an adversary at the beginning of the experiment. In the stochastic setting, we provide sequential arm selection algorithms whose performance exceeds that of existing algorithms, whereas in the adversarial setting, we devise simple algorithms whose regret have the same scaling as that of state-of-the-art algorithms, but with lower computational complexity.</p><p>2 Contribution and Related Work , where ∆ min denotes the expected reward difference between the best and the second-best arm. ESCB assigns an index to each arm. The index of given arm can be interpreted as performing likelihood tests with vanishing risk on its average reward. Our indexes are the natural extension of KL-UCB and UCB1 indexes defined for unstructured bandits <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>. Numerical experiments for some specific combinatorial problems are presented in the supplementary material, and show that ESCB significantly outperforms existing algorithms.</p><p>Related work. Previous contributions on stochastic combinatorial bandits focused on specific combinatorial structures, e.g. m-sets <ref type="bibr" target="#b5">[6]</ref>, matroids <ref type="bibr" target="#b6">[7]</ref>, or permutations <ref type="bibr" target="#b7">[8]</ref>. Generic combinatorial problems were investigated in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. The proposed algorithms, LLR and CUCB are variants of the UCB algorithm, and their performance guarantees are presented in Table <ref type="table">1</ref>. Our algorithms improve over LLR and CUCB by a multiplicative factor of √ m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adversarial combinatorial problems under bandit feedback</head><p>Contribution. We present algorithm COMBEXP, whose regret is </p><formula xml:id="formula_1">O m 3 T (d + m 1/2 λ -1 ) log µ -1 min , where µ min = min i∈[d]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>√</head><p>dT ) <ref type="bibr" target="#b12">[13]</ref>, so the regret gap between COMBEXP and this lower bound scales at most as m 1/2 up to a logarithmic factor. Related work. Adversarial combinatorial bandits have been extensively investigated recently, see <ref type="bibr" target="#b12">[13]</ref> and references therein. Some papers consider specific instances of these problems, e.g., shortest-path routing <ref type="bibr" target="#b13">[14]</ref>, m-sets <ref type="bibr" target="#b14">[15]</ref>, and permutations <ref type="bibr" target="#b15">[16]</ref>. For generic combinatorial problems, known regret lower bounds scale as</p><formula xml:id="formula_2">Ω √ mdT and Ω m √ dT (if d ≥ 2m</formula><p>) in the case of semibandit and bandit feedback, respectively <ref type="bibr" target="#b12">[13]</ref>. In the case of semi-bandit feedback, <ref type="bibr" target="#b12">[13]</ref> proposes</p><formula xml:id="formula_3">Algorithm Regret Lower Bound [13] Ω m √ dT , if d ≥ 2m COMBAND [4] O m 3 dT log d m 1 + 2m dλ EXP2 WITH JOHN'S EXPLORATION [18] O m 3 dT log d m COMBEXP (Theorem 6) O m 3 dT 1 + m 1/2 dλ log µ -1 min</formula><p>Table <ref type="table">2</ref>: Regret of various algorithms for adversarial combinatorial bandits with bandit feedback. Note that for most combinatorial classes of interests, m(dλ</p><formula xml:id="formula_4">) -1 = O(1) and µ -1 min = O(poly(d/m)).</formula><p>OSMD, an algorithm whose regret upper bound matches the lower bound. <ref type="bibr" target="#b16">[17]</ref> presents an algorithm with O(m dL T log(d/m)) regret where L T is the total reward of the best arm after T rounds.</p><p>For problems with bandit feedback, <ref type="bibr" target="#b3">[4]</ref> proposes COMBAND and derives a regret upper bound which depends on the structure of arm set M. For most problems of interest, the regret under COMBAND is upper-bounded by O( m 3 dT log(d/m)). <ref type="bibr" target="#b17">[18]</ref> addresses generic linear optimization with bandit feedback and the proposed algorithm, referred to as EXP2 WITH JOHN'S EXPLORATION, has a regret scaling at most as O( m 3 dT log(d/m)) in the case of combinatorial structure. As we show next, for many combinatorial structures of interest (e.g. m-sets, matchings, spanning trees), COMB-EXP yields the same regret as COMBAND and EXP2 WITH JOHN'S EXPLORATION, with lower computational complexity for a large class of problems. Table <ref type="table">2</ref> summarises known regret bounds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Models and Objectives</head><p>We consider MAB problems where each arm M is a subset of m basic actions taken from [d] = {1, . . . , d}. For i ∈ [d], X i (n) denotes the reward of basic action i in round n. In the stochastic setting, for each i, the sequence of rewards (X i (n)) n≥1 is i.i.d. with Bernoulli distribution with mean θ i . Rewards are assumed to be independent across actions. We denote by θ = (θ 1 , . . . , θ d ) ∈ Θ = [0, 1] d the vector of unknown expected rewards of the various basic actions. In the adversarial setting, the reward vector</p><formula xml:id="formula_5">X(n) = (X 1 (n), . . . , X d (n)) ∈ [0, 1] d is</formula><p>arbitrary, and the sequence (X(n), n ≥ 1) is decided (but unknown) at the beginning of the experiment.</p><p>The set of arms M is an arbitrary subset of {0, 1} d , such that each of its elements M has m basic actions. Arm M is identified with a binary column vector (M 1 , . . . , M d ) , and we have M 1 = m, ∀M ∈ M. At the beginning of each round n, a policy π, selects an arm M π (n) ∈ M based on the arms chosen in previous rounds and their observed rewards. The reward of arm</p><formula xml:id="formula_6">M π (n) selected in round n is i∈[d] M π i (n)X i (n) = M π (n) X(n).</formula><p>We consider both semi-bandit and bandit feedbacks. Under semi-bandit feedback and policy π, at the end of round n, the outcome of basic actions X i (n) for all i ∈ M π (n) are revealed to the decision maker, whereas under bandit feedback, M π (n) X(n) only can be observed.</p><p>Let Π be the set of all feasible policies. The objective is to identify a policy in Π maximizing the cumulative expected reward over a finite time horizon T . The expectation is here taken with respect to possible randomness in the rewards (in the stochastic setting) and the possible randomization in the policy. Equivalently, we aim at designing a policy that minimizes regret, where the regret of policy π ∈ Π is defined by: R π (T ) = max</p><formula xml:id="formula_7">M ∈M E T n=1 M X(n) -E T n=1 M π (n) X(n) .</formula><p>Finally, for the stochastic setting, we denote by µ M (θ) = M θ the expected reward of arm M , and let M (θ) ∈ M, or M for short, be any arm with maximum expected reward:</p><formula xml:id="formula_8">M (θ) ∈ arg max M ∈M µ M (θ).</formula><p>In what follows, to simplify the presentation, we assume that the optimal M is unique. We further define:</p><formula xml:id="formula_9">µ (θ) = M θ, ∆ min = min M =M ∆ M where ∆ M = µ (θ) -µ M (θ),<label>and</label></formula><formula xml:id="formula_10">∆ max = max M (µ (θ) -µ M (θ)).</formula><p>4 Stochastic Combinatorial Bandits under Semi-bandit Feedback</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Regret Lower Bound</head><p>Given θ, define the set of parameters that cannot be distinguished from θ when selecting action M (θ), and for which arm M (θ) is suboptimal:</p><formula xml:id="formula_11">B(θ) = {λ ∈ Θ : M i (θ)(θ i -λ i ) = 0, ∀i, µ (λ) &gt; µ (θ)}.</formula><p>We define X = (R + ) |M| and kl(u, v) the Kullback-Leibler divergence between Bernoulli distributions of respective means u and v, i.e., kl(u, v)</p><formula xml:id="formula_12">= u log(u/v) + (1 -u) log((1 -u)/(1 -v)).</formula><p>Finally, for (θ, λ) ∈ Θ 2 , we define the vector kl(θ, λ) = (kl(θ i , λ i )) i∈ <ref type="bibr">[d]</ref> .</p><p>We derive a regret lower bound valid for any uniformly good algorithm. An algorithm π is uniformly good iff R π (T ) = o(T α ) for all α &gt; 0 and all parameters θ ∈ Θ. The proof of this result relies on a general result on controlled Markov chains <ref type="bibr" target="#b18">[19]</ref>.</p><p>Theorem 1 For all θ ∈ Θ, for any uniformly good policy π ∈ Π, lim inf T →∞ R π (T ) log(T ) ≥ c(θ), where c(θ) is the optimal value of the optimization problem:</p><formula xml:id="formula_13">inf x∈X M ∈M x M (M (θ) -M ) θ s.t. M ∈M x M M kl(θ, λ) ≥ 1 , ∀λ ∈ B(θ).<label>(1)</label></formula><p>Observe first that optimization problem ( <ref type="formula" target="#formula_13">1</ref>) is a semi-infinite linear program which can be solved for any fixed θ, but its optimal value is difficult to compute explicitly. Determining how c(θ) scales as a function of the problem dimensions d and m is not obvious. Also note that (1) has the following interpretation: assume that (1) has a unique solution x . Then any uniformly good algorithm must select action M at least x M log(T ) times over the T first rounds. From <ref type="bibr" target="#b18">[19]</ref>, we know that there exists an algorithm which is asymptotically optimal, so that its regret matches the lower bound of Theorem 1. However this algorithm suffers from two problems: it is computationally infeasible for large problems since it involves solving (1) T times, furthermore the algorithm has no finite time performance guarantees, and numerical experiments suggest that its finite time performance on typical problems is rather poor. Further remark that if M is the set of singletons (classical bandit), Theorem 1 reduces to the Lai-Robbins bound <ref type="bibr" target="#b19">[20]</ref> and if M is the set of m-sets (bandit with multiple plays), Theorem 1 reduces to the lower bound derived in <ref type="bibr" target="#b5">[6]</ref>. Finally, Theorem 1 can be generalized in a straightforward manner for when rewards belong to a one-parameter exponential family of distributions (e.g., Gaussian, Exponential, Gamma etc.) by replacing kl by the appropriate divergence measure.</p><p>A Simplified Lower Bound We now study how the regret c(θ) scales as a function of the problem dimensions d and m. To this aim, we present a simplified regret lower bound. Given θ, we say that a set H ⊂ M \ M has property P (θ) iff, for all (M, M ) ∈ H 2 , M = M we have M i M i (1 -M i (θ)) = 0 for all i. We may now state Theorem 2.</p><p>Theorem 2 Let H be a maximal (inclusion-wise) subset of M with property P (θ). Define</p><formula xml:id="formula_14">β(θ) = min M =M ∆ M |M \M | . Then: c(θ) ≥ M ∈H β(θ) max i∈M \M kl θ i , 1 |M \M | j∈M \M θ j . Corollary 1 Let θ ∈ [a, 1] d for</formula><p>some constant a &gt; 0 and M be such that each arm M ∈ M, M = M has at most k suboptimal basic actions. Then c(θ) = Ω(|H|/k).</p><p>Theorem 2 provides an explicit regret lower bound. Corollary 1 states that c(θ) scales at least with the size of H. For most combinatorial sets, |H| is proportional to d -m (see supplementary material for some examples), which implies that in these cases, one cannot obtain a regret smaller than O((d -m)∆ -1 min log(T )). This result is intuitive since d -m is the number of parameters not observed when selecting the optimal arm. The algorithms proposed below have a regret of O(d √ m∆ -1 min log(T )), which is acceptable since typically, √ m is much smaller than d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Algorithms</head><p>Next we present ESCB, an algorithm for stochastic combinatorial bandits that relies on arm indexes as in UCB1 <ref type="bibr" target="#b20">[21]</ref> and KL-UCB <ref type="bibr" target="#b4">[5]</ref>. We derive finite-time regret upper bounds for ESCB that hold even if we assume that M 1 ≤ m, ∀M ∈ M, instead of M 1 = m, so that arms may have different numbers of basic actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Indexes</head><p>ESCB relies on arm indexes. In general, an index of arm M in round n, say b M (n), should be defined so that b M (n) ≥ M θ with high probability. Then as for UCB1 and KL-UCB, applying the principle of optimism in face of uncertainty, a natural way to devise algorithms based on indexes is to select in each round the arm with the highest index. Under a given algorithm, at time n, we define t i (n) = n s=1 M i (s) the number of times basic action i has been sampled. The empirical mean reward of action i is then defined as θi</p><formula xml:id="formula_15">(n) = (1/t i (n)) n s=1 X i (s)M i (s) if t i (n) &gt; 0 and θi (n) = 0 otherwise. We define the corresponding vectors t(n) = (t i (n)) i∈[d] and θ(n) = ( θi (n)) i∈[d] .</formula><p>The indexes we propose are functions of the round n and of θ(n). Our first index for arm M , referred to as b</p><formula xml:id="formula_16">M (n, θ(n)) or b M (n) for short, is an extension of KL-UCB index. Let f (n) = log(n) + 4m log(log(n)). b M (n, θ(n))</formula><p>is the optimal value of the following optimization problem:</p><formula xml:id="formula_17">max q∈Θ M q s.t. (M t(n)) kl( θ(n), q) ≤ f (n),<label>(2)</label></formula><p>where we use the convention that for</p><formula xml:id="formula_18">v, u ∈ R d , vu = (v i u i ) i∈[d] .</formula><p>As we show later, b M (n) may be computed efficiently using a line search procedure similar to that used to determine KL-UCB index.</p><p>Our second index c M (n, θ(n)) or c M (n) for short is a generalization of the UCB1 and UCB-tuned indexes:</p><formula xml:id="formula_19">c M (n) = M θ(n) + f (n) 2 d i=1 M i t i (n)</formula><p>Note that, in the classical bandit problems with independent arms, i.e., when m = 1, b M (n) reduces to the KL-UCB index (which yields an asymptotically optimal algorithm) and c M (n) reduces to the UCB-tuned index. The next theorem provides generic properties of our indexes. An important consequence of these properties is that the expected number of times where b M (n, θ(n)) or c M (n, θ(n)) underestimate µ (θ) is finite, as stated in the corollary below.</p><formula xml:id="formula_20">Theorem 3 (i) For all n ≥ 1, M ∈ M and τ ∈ [0, 1] d , we have b M (n, τ ) ≤ c M (n, τ ).</formula><p>(ii) There exists C m &gt; 0 depending on m only such that, for all M ∈ M and n ≥ 2:</p><formula xml:id="formula_21">P[b M (n, θ(n)) ≤ M θ] ≤ C m n -1 (log(n)) -2 . Corollary 2 n≥1 P[b M (n, θ(n)) ≤ µ ] ≤ 1 + C m n≥2 n -1 (log(n)) -2 &lt; ∞.</formula><p>Statement (i) in the above theorem is obtained combining Pinsker and Cauchy-Schwarz inequalities. The proof of statement (ii) is based on a concentration inequality on sums of empirical KL divergences proven in <ref type="bibr" target="#b21">[22]</ref>. It enables to control the fluctuations of multivariate empirical distributions for exponential families. It should also be observed that indexes b M (n) and c M (n) can be extended in a straightforward manner to the case of continuous linear bandit problems, where the set of arms is the unit sphere and one wants to maximize the dot product between the arm and an unknown vector. b M (n) can also be extended to the case where reward distributions are not Bernoulli but lie in an exponential family (e.g. Gaussian, Exponential, Gamma, etc.), replacing kl by a suitably chosen divergence measure. A close look at c M (n) reveals that the indexes proposed in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, and <ref type="bibr" target="#b8">[9]</ref> are too conservative to be optimal in our setting: there the "confidence bonus"</p><formula xml:id="formula_22">d i=1 Mi ti(n)</formula><p>was replaced by (at least) m</p><formula xml:id="formula_23">d i=1</formula><p>Mi ti(n) . Note that <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> assume that the various basic actions are arbitrarily correlated, while we assume independence among basic actions. When independence does not hold, <ref type="bibr" target="#b10">[11]</ref> provides a problem instance where the regret is at least Ω( md ∆min log(T )). This does not contradict our regret upper bound (scaling as O( d √ m ∆min log(T ))), since we have added the independence assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Index computation</head><p>While the index c M (n) is explicit, b M (n) is defined as the solution to an optimization problem. We show that it may be computed by a simple line search. For λ ≥ 0, w ∈ [0, 1] and v ∈ N, define:</p><formula xml:id="formula_24">g(λ, w, v) = 1 -λv + (1 -λv) 2 + 4wvλ /2.</formula><p>Fix n, M , θ(n) and t(n). Define I = {i : M i = 1, θi (n) = 1}, and for λ &gt; 0, define:</p><formula xml:id="formula_25">F (λ) = i∈I t i (n)kl( θi (n), g(λ, θi (n), t i (n))). Theorem 4 If I = ∅, b M (n) = ||M || 1 . Otherwise: (i) λ → F (λ) is strictly increasing, and F (R + ) = R + . (ii) Define λ as the unique solution to F (λ) = f (n). Then b M (n) = ||M || 1 -|I| + i∈I g(λ , θi (n), t i (n)).</formula><p>Theorem 4 shows that b M (n) can be computed using a line search procedure such as bisection, as this computation amounts to solving the nonlinear equation F (λ) = f (n), where F is strictly increasing. The proof of Theorem 4 follows from KKT conditions and the convexity of the KL divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">The ESCB Algorithm</head><p>The pseudo-code of ESCB is presented in Algorithm 1. We consider two variants of the algorithm based on the choice of the index ξ</p><formula xml:id="formula_26">M (n): ESCB-1 when ξ M (n) = b M (n) and ESCB-2 if ξ M (n) = c M (n).</formula><p>In practice, ESCB-1 outperforms ESCB-2. Introducing ESCB-2 is however instrumental in the regret analysis of ESCB-1 (in view of Theorem 3 (i)). The following theorem provides a finite time analysis of our ESCB algorithms. The proof of this theorem borrows some ideas from the proof of <ref type="bibr" target="#b10">[11,</ref><ref type="bibr">Theorem 3]</ref>.  <ref type="table">2</ref>).</p><p>It might not be possible to compute the projection step exactly, and this step can be solved up to accuracy n in round n. Namely we find q n such that KL(q n , qn ) -min p∈Ξ KL(p, qn ) ≤ n . Proposition 1 shows that for n = O(n -2 log -3 (n)), the approximate projection gives the same regret as when the projection is computed exactly. Theorem 7 gives the computational complexity of COMBEXP with approximate projection. When Co(M) is described by polynomially (in d) many linear equalities/inequalities, COMBEXP is efficiently implementable and its running time scales (almost) linearly in T . Proposition 1 and Theorem 7 easily extend to other OSMD-type algorithms and thus might be of independent interest. The computational complexity of COMBEXP is determined by the structure of Co(M) and COMB-EXP has O(T log(T )) time complexity due to the efficiency of interior-point methods. In contrast, the computational complexity of COMBAND depends on the complexity of sampling from M. COMBAND may have a time complexity that is super-linear in T (see <ref type="bibr">[16, page 217]</ref>). For instance, consider the matching problem described in Section 2. We have c = 2m equality constraints and s = m 2 box constraints, so that the time complexity of COMBEXP is: O(m 5 T log(T )). It is noted that using [26, Algorithm 1], the cost of decomposition in this case is O(m 4 ). On the other hand, COMBBAND has a time complexity of O(m 10 F (T )), with F a super-linear function, as it requires to approximate a permanent, requiring O(m 10 ) operations per round. Thus, COMBEXP has much lower complexity than COMBAND and achieves the same regret.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have investigated stochastic and adversarial combinatorial bandits. For stochastic combinatorial bandits with semi-bandit feedback, we have provided a tight, problem-dependent regret lower bound that, in most cases, scales at least as O((d -m)∆ -1 min log(T )). We proposed ESCB, an algorithm with O(d √ m∆ -1 min log(T )) regret. We plan to reduce the gap between this regret guarantee and the regret lower bound, as well as investigate the performance of EPOCH-ESCB. For adversarial combinatorial bandits with bandit feedback, we proposed the COMBEXP algorithm. There is a gap between the regret of COMBEXP and the known regret lower bound in this setting, and we plan to reduce it as much as possible.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 m|M|M</head><label>1</label><figDesc>∈M M i and λ is the smallest nonzero eigenvalue of the matrix E[M M ] when M is uniformly distributed over M (Theorem 6). For most problems of interest m(dλ) -1 = O(1) [4] and µ -1 min = O(poly(d/m)), so that COMBEXP has O( m 3 dT log(d/m)) regret. A known regret lower bound is Ω(m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Example 1 :</head><label>1</label><figDesc>m-sets. M is the set of all d-dimensional binary vectors with m non-zero coordinates. We have µ min = m d and λ = m(d-m) d(d-1) (refer to the supplementary material for details). Hence when m = o(d), the regret upper bound of COMBEXP becomes O( m 3 dT log(d/m)), which is the same as that of COMBAND and EXP2 WITH JOHN'S EXPLORATION. Example 2: matchings. The set of arms M is the set of perfect matchings in K m,m . d = m 2 and |M| = m!. We have µ min = 1 m , and λ = 1 m-1 . Hence the regret upper bound of COMBEXP is O( m 5 T log(m)), the same as for COMBAND and EXP2 WITH JOHN'S EXPLORATION. Example 3: spanning trees. M is the set of spanning trees in the complete graph K N . In this case, d = N 2 , m = N -1, and by Cayley's formula M has N N -2 arms. log µ -1 min ≤ 2N for N ≥ 2 and m dλ &lt; 7 when N ≥ 6, The regret upper bound of COMBAND and EXP2 WITH JOHN'S EXPLORATION becomes O( N 5 T log(N )). As for COMBEXP, we get the same regret upper bound O( N 5 T log(N )).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 5</head><label>5</label><figDesc>The regret under algorithms π ∈ {ESCB-1, ESCB-2} satisfies for all T ≥ 1:R π (T ) ≤ 16d √ m∆ -1 min f (T ) + 4dm 3 ∆ -2 min + C m , where C m ≥ 0 does not depend on θ, d and T . As a consequence R π (T ) = O(d √ m∆ -1 min log(T )) when T → ∞.For most classes of M, we have µ -1 min = O(poly(d/m)) and m(dλ) -1 = O(1) [4]. For these classes, COMBEXP has a regret of O( m 3 dT log(d/m)), which is a factor m log(d/m) off the lower bound (see Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Proposition 1 1 Theorem 7</head><label>117</label><figDesc>If the projection step of COMBEXP is solved up to accuracy n = O(n -2 log -3 (n)), we have: R COMBEXP (T ) ≤ 2 2m 3 T d + m Assume that Co(M) is defined by c linear equalities and s linear inequalities. If the projection step is solved up to accuracy n = O(n -2 log -3 (n)), then COMBEXP has time complexity. The time complexity of COMBEXP can be reduced by exploiting the structure of M (See [24, page 545]). In particular, if inequality constraints describing Co(M) are box constraints, the time complexity of COMBEXP is O(T [c 2 √ s(c + d) log(T ) + d 4 ]).</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>A. Proutiere's research is supported by the ERC FSA grant, and the SSF ICT-Psi project.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 ESCB</head><p>for n ≥ 1 do Select arm M (n) ∈ arg maxM∈M ξM (n).</p><p>Observe the rewards, and update ti(n) and θi(n), ∀i ∈ M (n). end for Algorithm 2 COMBEXP</p><p>Sampling: Select a random arm M (n) with distribution pn-1 and incur a reward</p><p>is the pseudo-inverse of Σn-1.</p><p>Projection: Set qn to be the projection of qn onto the set P using the KL divergence. end for ESCB with time horizon T has a complexity of O(|M|T ) as neither b M nor c M can be written as M y for some vector y ∈ R d . Assuming that the offline (static) combinatorial problem is solvable in O(V (M)) time, the complexity of the CUCB algorithm in <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b10">[11]</ref> after T rounds is O(V (M)T ). Thus, if the offline problem is efficiently implementable, i.e., V (M) = O(poly(d/m)), CUCB is efficient, whereas ESCB is not since |M| may have exponentially many elements. In §2.5 of the supplement, we provide an extension of ESCB called EPOCH-ESCB, that attains almost the same regret as ESCB while enjoying much better computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Adversarial Combinatorial Bandits under Bandit Feedback</head><p>We now consider adversarial combinatorial bandits with bandit feedback. We start with the following observation:</p><p>with Co(M) the convex hull of M. We embed M in the d-dimensional simplex by dividing its elements by m. Let P be this scaled version of Co(M).</p><p>Inspired by OSMD <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>, we propose the COMBEXP algorithm, where the KL divergence is the Bregman divergence used to project onto P. Projection using the KL divergence is addressed in <ref type="bibr" target="#b22">[23]</ref>. We denote the KL divergence between distributions q and p in P by</p><p>q(i) . The projection of distribution q onto a closed convex set Ξ of distributions is p = arg min p∈Ξ KL(p, q). Let λ be the smallest nonzero eigenvalue of E[M M ], where M is uniformly distributed over M. We define the exploration-inducing distribution µ 0 ∈ P:</p><p>and let µ min = min i mµ 0 i . µ 0 is the distribution over basic actions <ref type="bibr">[d]</ref> induced by the uniform distribution over M. The pseudo-code for COMBEXP is shown in Algorithm 2. The KL projection in COMBEXP ensures that mq n-1 ∈ Co(M). There exists λ, a distribution over M such that mq n-1 = M λ(M )M . This guarantees that the system of linear equations in the decomposition step is consistent. We propose to perform the projection step (the KL projection of q onto P) using interior-point methods <ref type="bibr" target="#b23">[24]</ref>. We provide a simpler method in §3.4 of the supplement. The decomposition step can be efficiently implemented using the algorithm of <ref type="bibr" target="#b24">[25]</ref>. The following theorem provides a regret upper bound for COMBEXP.</p><p>Theorem 6 For all T ≥ 1: R COMBEXP (T ) ≤ 2 m 3 T d + m 1/2 λ log µ -1 min + m 5/2 λ log µ -1 min .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Some aspects of the sequential design of experiments</title>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Herbert Robbins Selected Papers</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="169" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Regret analysis of stochastic and nonstochastic multi-armed bandit problems</title>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolò</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="222" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Prediction, learning, and games</title>
		<author>
			<persName><forename type="first">Nicolò</forename><surname>Cesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Lugosi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Cambridge University Press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Nicolò</forename><surname>Cesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Lugosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorial bandits. Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1404" to="1422" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The KL-UCB algorithm for bounded stochastic bandits and beyond</title>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Garivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Cappé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLT</title>
		<meeting>of COLT</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Asymptotically efficient allocation rules for the multiarmed bandit problem with multiple plays-part i: iid rewards. Automatic Control</title>
		<author>
			<persName><forename type="first">Pravin</forename><surname>Venkatachalam Anantharam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Varaiya</surname></persName>
		</author>
		<author>
			<persName><surname>Walrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="968" to="976" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Matroid bandits: Fast combinatorial optimization with learning</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Branislav Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoda</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Eydgahi</surname></persName>
		</author>
		<author>
			<persName><surname>Eriksson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of UAI</title>
		<meeting>of UAI</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning multiuser channel allocations in cognitive radio networks: A combinatorial multi-armed bandit formulation</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhaskar</forename><surname>Krishnamachari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE DySpan</title>
		<meeting>of IEEE DySpan</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhaskar</forename><surname>Krishnamachari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. on Networking</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1466" to="1478" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combinatorial multi-armed bandit: General framework and applications</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yajun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tight regret bounds for stochastic combinatorial semi-bandits</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Branislav Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName><surname>Szepesvari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AISTATS</title>
		<meeting>of AISTATS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient learning in large-scale combinatorial semi-bandits</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azin</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoda</forename><surname>Eydgahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Branislav</forename><surname>Kveton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Regret in online combinatorial optimization</title>
		<author>
			<persName><forename type="first">Jean-Yves</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Lugosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="45" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The on-line shortest path problem under partial monitoring</title>
		<author>
			<persName><forename type="first">András</forename><surname>György</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamás</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gábor</forename><surname>Lugosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">György</forename><surname>Ottucsák</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Non-stochastic bandit slate problems</title>
		<author>
			<persName><forename type="first">Satyen</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lev</forename><surname>Reyzin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1054" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bandit online optimization over the permutahedron</title>
		<author>
			<persName><forename type="first">Kohei</forename><surname>Nir Ailon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eiji</forename><surname>Hatano</surname></persName>
		</author>
		<author>
			<persName><surname>Takimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="215" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">First-order regret bounds for combinatorial semi-bandits</title>
		<author>
			<persName><forename type="first">Gergely</forename><surname>Neu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLT</title>
		<meeting>of COLT</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards minimax policies for online linear optimization with bandit feedback</title>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolò</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLT</title>
		<meeting>of COLT</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Asymptotically efficient adaptive choice of control laws in controlled markov chains</title>
		<author>
			<persName><forename type="first">Todd</forename><forename type="middle">L</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tze</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lai</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="715" to="743" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Asymptotically efficient adaptive allocation rules</title>
		<author>
			<persName><forename type="first">Tze</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lai</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="22" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Finite time analysis of the multiarmed bandit problem</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolò</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lipschitz bandits: Regret lower bounds and optimal algorithms</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Magureanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Combes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Proutiere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLT</title>
		<meeting>of COLT</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Information theory and statistics: A tutorial</title>
		<author>
			<persName><forename type="first">I</forename><surname>Csiszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Shields</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lieven</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex optimization</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A constructive proof of the representation theorem for polyhedral sets based on fundamental definitions</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Sherali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Mathematical and Management Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="253" to="270" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning permutations with exponential weights</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Helmbold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manfred</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1705" to="1736" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
