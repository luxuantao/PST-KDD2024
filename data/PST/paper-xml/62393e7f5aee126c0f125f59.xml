<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compression of Generative Pre-trained Language Models via Quantization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chaofan</forename><surname>Tao</surname></persName>
							<email>cftao@connect.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lu</forename><surname>Hou</surname></persName>
							<email>houlu3@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
							<email>zhangwei379@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
							<email>shang.lifeng@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
							<email>jiang.xin@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
							<email>qun.liu@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<email>pluo@cs.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ngai</forename><surname>Wong</surname></persName>
							<email>nwong@eee.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Compression of Generative Pre-trained Language Models via Quantization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The increasing size of generative Pre-trained Language Models (PLMs) have greatly increased the demand for model compression. Despite various methods to compress BERT or its variants, there are few attempts to compress generative PLMs, and the underlying difficulty remains unclear. In this paper, we compress generative PLMs by quantization. We find that previous quantization methods fail on generative tasks due to the homogeneous word embeddings caused by reduced capacity, and varied distribution of weights. Correspondingly, we propose a token-level contrastive distillation to learn distinguishable word embeddings, and a module-wise dynamic scaling to make quantizers adaptive to different modules. Empirical results on various tasks show that our proposed method outperforms the stateof-the-art compression methods on generative PLMs by a clear margin. With comparable performance with the full-precision models, we achieve 14.4× and 13.4× compression rates on GPT-2 and BART, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer-based generative pre-trained language models (PLMs) show strong abilities of multitask and few-shot learning, and achieve remarkable performances on various tasks <ref type="bibr" target="#b28">(Radford and Narasimhan, 2018;</ref><ref type="bibr" target="#b3">Brown et al., 2020;</ref><ref type="bibr" target="#b22">Lewis et al., 2020;</ref><ref type="bibr" target="#b29">Raffel et al., 2020;</ref><ref type="bibr">Chen et al., 2021)</ref>. However, they are usually expensive in terms of both computation and memory due to a large number of parameters, and the token-by-token generation process. Many methods have been proposed to compress PLMs, but mostly focus on understanding tasks like sentence classification with BERT <ref type="bibr" target="#b21">(Lan et al., 2019;</ref><ref type="bibr" target="#b33">Sun et al., 2020b;</ref><ref type="bibr" target="#b20">Jiao et al., 2020;</ref><ref type="bibr" target="#b30">Shen et al., 2020;</ref><ref type="bibr" target="#b15">Hou et al., 2020)</ref>. Recent works try to compress GPT-2 using tensor decomposition <ref type="bibr" target="#b9">(Edalati et al., 2021)</ref>, and knowledge distillation <ref type="bibr" target="#b31">(Song et al., 2020)</ref>, but the compression ratio achieved is much smaller than that of BERT. Yet the underlying difficulty remains unclear.</p><p>In this paper, we firstly explore compressing generative PLMs by quantizing the parameters from full-precision to lower bits. We find that directly applying previous quantization methods designed for BERT or computer vision tasks to generative PLMs lead to poor performance. Figure <ref type="figure" target="#fig_0">1</ref> shows that the performance drops sharply as the weight bit-width decreases. To investigate the difficulty of quantizing generative PLMs, we find that the learned embeddings tend to be homogeneous and hard to distinguish due to the reduced capacity caused by quantization, while the weight distributions also vary significantly across different modules and different Transformer layers. These problems are further magnified due to the nature of sequential left-to-right prediction of generative PLMs, as the quantization error will accumulate across time.</p><p>To alleviate the above problems, we propose a token-level contrastive distillation to contrast on tokens and make the word embedding distinguishable. Besides, we propose a module-wise dynamic scaling for the quantizer to better adapt to different modules. Empirical results on language modeling, next utterance prediction and summarization show that compared to the full-precision baseline, our quantized GPT and BART (abbreviated as Quant-GPT and QuantBART) achieve comparable performance for 8/4-bit weight, and have only a slight arXiv:2203.10705v1 [cs.CL] 21 Mar 2022 drop for 2-bit weight, while being over 13× smaller. QuantGPT also clearly outperforms previous GPT compression methods on language modeling.</p><p>To summarize, our main contributions are: 1) We find that generative PLMs are hard to quantize due to homogeneous word embedding and varied weight distribution. 2) We then propose the tokenlevel contrastive distillation and module-wise dynamic scaling, to make the word embedding more distinguishable and make quantizers adapt to different modules, respectively. 3) Empirical results on various tasks show the efficacy of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Difficulty of Qunatizing Generative</head><p>Pre-trained Language Models</p><p>In this section, we show that it is challenging to train a low-bit generative pre-trained model with conventional quantization approaches directly. Before diving into details, we first review the necessary backgrounds of quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Network Quantization</head><p>In this paper, we apply the quantization-aware training <ref type="bibr" target="#b8">(Courbariaux et al., 2015)</ref> to generative PLMs. Specifically, denote the vectorized full-precision weight as w, each forward propagation first clips the weight by a positive clipping factor α, and then quantizes the clipped weight to b-bit as</p><formula xml:id="formula_0">w q = α • Q(clip(w, −α, α)/α),<label>(1)</label></formula><p>where Q is the quantization function that maps each entry in clip(w, −α, α)/α to its closest quantized value in the set of uniform discrete values</p><formula xml:id="formula_1">{−1,− n−1 n , • • • , − 1 n , 0, 1 n , • • • , n−1 n , 1} with n = 2 b−1 − 1.</formula><p>Then we compute the loss (w q ) with w q . During back propagation, we use the gradient with regard to the quantized weight ∇ (w q ) as the Straight-Through-Estimator <ref type="bibr" target="#b2">(Bengio et al., 2013)</ref> to update full-precision weights w due to the non-differentiability of Q(•).</p><p>A good clipping factor is expected to take the majority of full-precision weight into account via clipping, i.e., quantizing the range where data are densely distributed to reduce quantization error. To solve this problem, PACT <ref type="bibr" target="#b7">(Choi et al., 2018)</ref> learns a parameterized clipping factor and achieves better results than setting a fixed clipping factor. Instead of learning the clipping factor, LSQ <ref type="bibr" target="#b10">(Esser et al., 2020)</ref> learns the step size α/n, but requires a careful initialization and gradient update.</p><p>In practice, following previous works on BERT quantization <ref type="bibr" target="#b40">(Zhang et al., 2020;</ref><ref type="bibr" target="#b1">Bai et al., 2021)</ref>, we use layer-wise quantization (i.e., one clipping factor for elements in each weight matrix) for all weight matrices in the Transformer layers and rowwise quantization (i.e., one clipping factor for each word embedding) for the embedding layer. We use asymmetric uniform quantization for activations after self-attention and GeLU function whose elements are mostly positive, and symmetric uniform quantization for other activations. We do not quantize layer-normalization layers, skip connections, biases due to small computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Difficulty Analysis</head><p>We compare the following representative quantization methods including (i) LAQ <ref type="bibr" target="#b40">(Zhang et al., 2020)</ref> for BERT; (ii) PACT <ref type="bibr" target="#b7">(Choi et al., 2018)</ref> and LSQ <ref type="bibr" target="#b10">(Esser et al., 2020)</ref>) for computer vision tasks, to generative pre-trained model, GPT-2. Figure <ref type="figure" target="#fig_0">1</ref> shows the performance under different weight bitwidths, and the performance drops sharply as the bit-width decreases, especially for PACT and LSQ.</p><p>In the following, we study the potential reasons behind the difficulty of quantizing generative PLMs, by empirically investigating the properties of the word embedding and model parameters.</p><p>Homogeneous Word Embedding. We first study the difficulty from the learned word embeddings of different models. In Figure <ref type="figure" target="#fig_2">2</ref>, we visually compare the distributions of the word embeddings of the full-precision and quantized models under the same scale. As can be seen, the word embeddings of the full-precision model are scattered distinguishable, while those in previous quantization methods PACT, LSQ and LAQ learn homogeneous word embeddings which are clustered and less distinguishable, especially for PACT and LSQ. We speculate this is caused by the sequential computation nature of GPT. Specifically, unlike BERT which computes the representation of all tokens in parallel, GPT computes each token in left-to-right order, and the quantization error incurred in the previous tokens will pass on to future tokens, making the learning signal noisier over time, and finally less informative word embeddings.</p><p>A direct consequence of the homogeneous word embedding can be reflected in Figure <ref type="figure" target="#fig_3">3</ref>. By comparing Figure <ref type="figure" target="#fig_2">2</ref> and Figure <ref type="figure" target="#fig_3">3</ref>, we can find that the higher degree of homogeneity in the word embedding of a quantized model, the fewer dependencies   among different tokens are kept.</p><p>As will be discussed in Section 3.1, we propose a token-level contrastive learning to alleviate this problem. Compared with PACT, LSQ and LAQ, our method not only aligns the token representations between the quantized and full-precision networks (i.e., diagonal boxes), but also captures the dependencies among different tokens (nondiagonal boxes). More visualizations are available in Appendix C.3. The non-distinguishable word embeddings and poor ability to capture contextualized dependencies also make methods like PACT and LSQ more likely to generate incorrect tokens, e.g. illogical and repeated text ( Section 4.4).</p><p>Varied Distribution of Weights. Besides the learned word embeddings, we also investigate the distribution of the weights in the full-precision model. Figure <ref type="figure" target="#fig_5">4</ref> shows that the weight distributions of a 12-layer full-precision GPT-2 are highly skewed with outliers. This causes difficulty in estimating the clipping factor α of the quantizer by heuristic methods, or even by PACT which learns the α through gradient descent. Specifically, in PACT, the approximated gradient of α only relies on the weights whose absolute values are larger than α. This solution ignores the effect of weights within [−α, α] and depends heavily on the initialization of α. Figure <ref type="figure" target="#fig_5">4</ref> shows that an improper initialization together with the inaccurate gradient  estimation of the clipping factor often make the learned α of PACT too large, and can not provide fine resolution to the majority of weights within the clipping range. The quantization error accumulated over time makes this problem more severe. In this work, we re-parameterize the clipping factor to make the quantizer adaptive to each module in the Transformer layers, and consider both weights outside and inside the clipping range when estimating the gradient of the clipping factor.</p><p>As will be discussed in Section 3.2, we propose a module-wise dynamic scaling to reduce the clipping factor's sensitivity to initialization, and an improved gradient estimation that also considers the weights within <ref type="bibr">[−α, α]</ref>. Figure <ref type="figure" target="#fig_5">4</ref> shows that the clipping factor learned by our method gives finer resolutions to the majority of the weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>Based on the observations in Section 2.2, we propose a quantization method which utilizes tokenlevel contrastive distillation to make the word embedding distinguishable (Section 3.1) and a modulewise dynamic scaling adjustment to learn better clipping factors (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Token-level Contrastive Distillation</head><p>The proposed token-level contrastive distillation contrast among tokens instead of sequences sequence, to learn distinguishable representations for each token. Inspired by <ref type="bibr">Baevski et al. (2020)</ref>, which uses in-utterance representation at different positions of the same utterance as negatives for speech feature learning, for each token of the quantized network, we use the representation of the same token from the full-precision teacher network as its positive, while representations of other tokens in the same sequence as negatives (Figure <ref type="figure">5</ref>). Inspired by <ref type="bibr" target="#b12">He et al. (2020)</ref> which uses a momentum encoder for more consistent representation, we build a memory bank to store momentum token representations from the quantized network. When computing the contrastive distillation loss, we load the representations of negative samples from the memory bank with cheap indexing operations.</p><p>Specifically, we use superscripts s and t to denote the quantized student network and fullprecision teacher network, respectively. Denote the length-n input sequence of tokens as</p><formula xml:id="formula_2">(t 1 , t 2 , • • • , t n ).</formula><p>For the i-th token t i , suppose its hidden states of the last Transformer layer from the quantized and full-precision network are linearly projected to (h s i , h t i ) ∈ R d , and q s i is the smoothed representation of h s i in the memory bank. Denote S i as the indices of the sampled negatives for token i, the token-level contrastive distillation loss for the length-n sequence can be formulated as</p><formula xml:id="formula_3">L cont = − n i=1 log exp(s(q s t i , h t t i )/τ ) j∈S i exp(s(q s t i , h t t j )/τ ) ,<label>(2)</label></formula><p>where s(x, y) = x y x y computes the cosine similarity, and τ is a fixed temperature parameter.</p><p>Then we update the representation of token t i in the memory bank with the moving-average of token representations from the quantized network:</p><formula xml:id="formula_4">q s t i ← mq s t i + (1 − m)h s t i ,<label>(3)</label></formula><p>where m ∈ [0, 1) it the momentum coefficient that controls the smoothness of the token represenation. Besides, we use an additional distillation loss L dist over the logits. For the i-th token t i , suppose the logits of the quantized and full-precision network are</p><formula xml:id="formula_5">z s t i , z t t i ∈ R |V |</formula><p>, where |V | is the vocabulary size. L dist is computed with the soft crossentropy loss:</p><formula xml:id="formula_6">L dist = − n i=1 z t t i log(z s t i ).<label>(4)</label></formula><p>Thus the total training loss is</p><formula xml:id="formula_7">L = λL cont + L dist ,<label>(5)</label></formula><p>where λ is a trade-off factor set as 0.1 by default.</p><p>Intuitively, for each token in the quantized network, L dist only encourages it to mimic its corresponding token of the teacher network, while L cont not only pulls it close to its positive, but also pushes it away from its negatives. In this way, L cont helps the student to capture more information from the teacher's representation, as is also theoretically discussed in <ref type="bibr" target="#b34">Tian et al. (2019)</ref>.</p><p>The proposed token-level contrastive distillation is crucial to the performance, and outperforms the sequence-level counterpart (as will be shown empirically in Section 5.1.1). We conjecture this is because (i) token-level contrast alleviates the problem of homogeneous word embedding (Figure <ref type="figure" target="#fig_2">2</ref>) in the low-bit quantization; and (ii) similar to speech, the order of natural language is also sequential instead of spatial like images; and (iii) the self-attention mechanism allows other tokens to learn representations contextualized on the studied token, and these in-sequence negatives are harder than those from in-batch sequences, allowing more efficient representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Module-dependent Dynamic Scaling</head><p>Based on the observation of varied weight distribution in Section 2.2, we propose a simple-yeteffective dynamic scaling according to the statistics of each module weight. Specifically, instead of directly learning the original clipping factor α as PACT, we turn to learn a new scaling factor γ, which is multiplied with the average weight magnitude w 1 n to get clipping factor α: Figure <ref type="figure">5</ref>: The training workflow of the proposed method. For each token in the quantized network, we compute both (i) the token-level contrastive distillation loss where the positive tokens and negative tokens are selected from the full-precision teacher network; and (ii) the distillation loss on the logits. The embedding layer and all weights in the Transformer layers are quantized with the proposed module-dependent dynamic scaling. where • 1 denotes 1 norm. The scaling γ is initialized as 1, which not only eases the initialization but also ensures the initial clipping factor α does not deviate far from the full-precision weights, regardless of the diversity of weight distribution. Besides, we also design a more accurate gradient estimation of the scaling factor than PACT <ref type="bibr" target="#b7">(Choi et al., 2018)</ref>. Previous PACT only back propagates through weights whose absolute values are larger the clipping factor (i.e. |w| ≥ α). Instead, we also consider the weights inside the clipping range (i.e. |w| &lt; α) as:</p><formula xml:id="formula_8">α = γ • w 1 n ,<label>(6)</label></formula><formula xml:id="formula_9">Method #Bits (W-E-A) Size (MB) (↓) WikiText2 PPL (↓) PTB PPL (↓) WikiText103 PPL (↓) Persona-Chat Acc(%) (↑) - full-</formula><formula xml:id="formula_10">∂ ∂γ =        ∂ ∂wq Q(u) w 1 n , w &lt; −α ∂ ∂wq [− w α +Q(u)] w 1 n , −α ≤ w ≤ α ∂ ∂wq Q(u) w 1 n , w &gt; α ,<label>(7)</label></formula><p>where is the total training loss and u = clip(w, −α, α)/α in Eq. ( <ref type="formula" target="#formula_0">1</ref>). The detailed derivation can be found in Appendix A.</p><p>Intuitively, the update of clipping factor should be influenced by both weights outside and inside [−α, α], since α controls the quantization error of both, i.e., a large clipping factor results in small quantization error for weights outside [−α, α], while large error for weights inside. Our new estimation of the gradient of γ in Eq. ( <ref type="formula" target="#formula_10">7</ref>) considers weights both outside and inside [−α, α]. Additionally, the proposed scaling is less sensitive to the varied distribution of weight than PACT, since the gradient of scaling ∂ ∂γ is proportional to the average weight magnitude w 1 n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Tasks and Models. In this section, we evaluate the efficacy of our proposed quantization method on three kinds of generative tasks on two kinds of generative pre-training models. Specifically, we perform the proposed quantization approach on language modeling and next utterance prediction tasks on GPT-2 <ref type="bibr" target="#b28">(Radford and Narasimhan, 2018)</ref>, and abstractive summarization using BART <ref type="bibr" target="#b22">(Lewis et al., 2020)</ref>, and call the resultant models Quant-GPT and QuantBART. The token-level contrastive distillation is performed on the hidden states of the last layer of GPT-2 or the BART decoder. More details about the datasets and model architectures can be found in Appendix B.1 and B.2.</p><p>Implementation Details. For each downstream task with our proposed method, we first fine-tune a full-precision network using the pre-trained checkpoint from huggingface 1 for both GPT-2 and BART.</p><p>Then we use this fine-tuned network as the fullprecision teacher network and to initialize the quantized student network. We train each task with 8 V100 GPUs based on the Pytorch framework. The detailed hyper-parameters for each task are available in Appendix B.3.</p><p>Compared Methods. Since there are very few attempts to compress generative PLMs, we selfimplement three baseline quantization methods PACT <ref type="bibr" target="#b7">(Choi et al., 2018)</ref>, <ref type="bibr">LSQ (Esser et al., 2020)</ref> and LAQ <ref type="bibr" target="#b16">(Hou and Kwok, 2018)</ref> for comparison. Details about these methods are in Appendix B.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Language Modeling</head><p>The task of language modeling is to predict the probability distribution over a sequence of words.</p><p>For language modeling, we experiment on Wiki-Text2 <ref type="bibr" target="#b24">(Merity et al., 2016)</ref>, Penn Treebank (PTB) <ref type="bibr" target="#b25">(Mikolov and Zweig, 2012)</ref> and WikiText103 <ref type="bibr" target="#b24">(Merity et al., 2016)</ref>. We use perplexity (PPL) to evaluate the performance for language modeling.</p><p>Comparison with the Full-precision Model.</p><p>From Table <ref type="table" target="#tab_1">1</ref>, the performance of the proposed method with 8-bit weight is comparable to the fullprecision counterpart on PTB and WikiText103, while drops slightly on WikiText2. A slightly more severe performance drop is observed as the bitwidth decreases from 8 to 4, with a drop of around 1 PPL point on WikiText2 and WikiText103, and less than 0.1 PPL point on PTB. When the bit-width of weight further goes down to 2, our method has an average of 2 PPL points drop, but achieves 14.4× model size reduction.</p><p>Comparison with Other Quantization Methods.</p><p>From Table <ref type="table" target="#tab_1">1</ref>, our method outperforms PACT, LSQ 1 http://huggingface.co/models</p><p>and LAQ for all bit-widths and tasks. As the bitwidth decreases from 8 to 4, the PPL of LSQ greatly increases, with the average PPL of LSQ increasing by over 5 times. As the bit-width further decreases to 2, both LSQ and PACT fail on all datasets, despite their good performance on understanding tasks on BERT <ref type="bibr" target="#b1">(Bai et al., 2021)</ref>. We conjecture it is because though both PACT and LSQ have learnable parameters, the accumulated quantization error of generative PLMs makes the updates of these parameters by gradient descent less stable. On the other hand, the proposed module-wise dynamic scaling alleviates the problem. Comparison with Other Compression Methods.</p><p>In Table <ref type="table" target="#tab_2">2</ref>, we compare our quantization method against recent GPT-2 compression methods, including tensor decomposition method KnGPT2 <ref type="bibr" target="#b9">(Edalati et al., 2021)</ref>, as well as distillation methods Distil-GPT2 and LightPAFF <ref type="bibr" target="#b31">(Song et al., 2020)</ref>. From the comparison, our method outperforms the others in terms of model size and performance, even when weights are compressed to only 2 bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Next Utterance Prediction</head><p>The task of next utterance prediction predicts the next utterance given the dialogue context. It tests the language understanding ability of generative models. For this task, we use a large-scale dialogue dataset, Persona-Chat <ref type="bibr" target="#b39">(Zhang et al., 2018)</ref>.</p><p>From Table <ref type="table" target="#tab_1">1</ref>, all quantization methods incur a clear performance drop compared to the fullprecision baseline, even in the 8-bit setting. As the quantization becomes more aggressive, i.e., the bit-width gets smaller, the performance of PACT and LAQ decrease more significantly than ours. In particular, LSQ diverges for 2-bit weight and its accuracy is only 5%, which is no better than a random guess as there are 20 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Abstractive Summarization</head><p>Abstractive summarization aims at generating a terse summary that captures the main ideas of the source article. We experiment on XSum <ref type="bibr" target="#b26">(Narayan et al., 2018)</ref>, whose ground-truth summarizations are highly abstractive and are challenging for many extractive strategies. ROUGE 1, 2, L are used to evaluate the performance of this task.  Table <ref type="table" target="#tab_3">3</ref> shows the results of the abstractive summarization. As can be seen, our method constantly outperforms other methods again with a clear margin. Example generated summarizations of different methods in Appendix C.2 show that the summaries generated by QuantBART are logical and terse, while those from PACT have repeated texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation on Contrastive Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Choices of Negative Sampling</head><p>As shown in Figure <ref type="figure" target="#fig_8">6</ref>, we ablate on how to choose negative samples in contrastive learning. Specifically, we compare our method with variants of token-level contrastive learning, which select negative samples of each token from (a) representations of other tokens in both the full-precision and quantized networks (fp+quan.); (b) representations of other tokens in the quantized network (quan. only); and (c) the whole vocabulary randomly for each training iteration (global). Besides, we compare with (d) sequence-level contrastive learning by pulling together representations of the same sequence, and pushing away representations of differ-   ent ones from the teacher network (in-batch). Representation of a sequence is defined as the mean of representations of all tokens in the sequence.</p><p>From Table <ref type="table" target="#tab_4">4</ref>, "fp+quan." and "quan. only" performs worse than QuantGPT, which uses fullprecision representations of other tokens as negative samples. This indicates that noisy representations of tokens from the not-fully-trained quantized network may not be sufficient. "global" performs even worse, which we conjecture is because, for one token, negative tokens chosen from the same sequence are contextually related to it and more informative than random tokens. "in-batch" performs worse than all token-level variants, which may be because generative tasks make predictions in a token-wise manner and rely heavily in finergrained token-wise representations. Interestingly, contrary to in-batch negative sampling in computer vision <ref type="bibr" target="#b5">(Chen et al., 2020)</ref>, we find that reducing the number of negative samples by reducing the batch size from 32 to 16 slightly improves performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Number of Negative Samples</head><p>In Figure <ref type="figure" target="#fig_9">7</ref>, we plot the PPL of 2-bit QuantGPT on the PTB dataset, with varying number of negative   samples. We plot the mean results with standard deviations from 5 independent runs. As can be seen, the performance improves and converges gradually as the number of negative samples increases.</p><p>Figure <ref type="figure" target="#fig_9">7</ref> also shows that using the movingaverage representations (q s t i in Eq. ( <ref type="formula" target="#formula_4">3</ref>)) of negative samples in the memory bank has better performance than using the immediate representations (h s t i in Eq. ( <ref type="formula" target="#formula_4">3</ref>)), because of a smoother and more consistent representation of tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Training Cost of the Contrastive Loss</head><p>In Table <ref type="table" target="#tab_5">5</ref>, we report the training speed and memory consumption of training the GPT-2 model on the PTB dataset with and without the proposed token-level contrastive loss. Batch size is set as 4 per device, which can be increased by using GPUs with larger memory or reducing the sequence length of samples. As can be seen, with the proposed token-level contrastive loss, the performance clearly improves with only slightly slower training speed and more memory consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Representations for the Contrastive Loss</head><p>In Table <ref type="table" target="#tab_6">6</ref>, we compare the different representations to perform the contrastive loss. The "decoderlast"( resp. "decoder-first") denotes performing the proposed token-level contrastive loss on the hidden states from the last decoder layer (resp. first decoder layer) followed by a linear transformation.</p><p>From Table <ref type="table" target="#tab_6">6</ref>, "decoder-last" performs better than "decoder-first". A possible reason is that the hidden states of the last decoder blocks contain rich information from all previous layers <ref type="bibr" target="#b36">(Xiong et al., 2020)</ref>. Since the experiments of abstractive summarization are conducted on BART, which has both encoder and decoder layers, we also study the contrastive loss on the "encoder-last" and "encoderfirst". In the ablation on the encoder, the contrastive loss L cont are computed on the source input (articles), instead of target input (summaries). From Table <ref type="table" target="#tab_6">6</ref>, "decoder-last" also has better ROUGE 1, 2, L values than other counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation on Dynamic Scaling</head><p>Figure <ref type="figure" target="#fig_10">8</ref> shows the learned scaling γ of different modules in the 2-bit GPT-2 model. As can be seen, the scalings of different modules vary a lot, verifying the need for module-wise dynamic scaling.</p><p>In addition, we investigate the effect of the proposed dynamic scaling and the new estimation of the gradient in Eq. ( <ref type="formula" target="#formula_10">7</ref>) with two variants: 1) L dist only which removes the token-level contrastive learning; and 2) Ours with PACT which removes the contrastive learning, and estimates the gradient with PACT which only considers the weights whose absolute values are larger than the clipping factor α. As shown in Table <ref type="table">7</ref>, the performance gets worse without contrastive learning to learn the distinguishable representations of tokens. The performance drops significantly when using PACT to estimate the gradient of the proposed scaling, especially for the WikiText103 dataset, verifying the efficacy of the new gradient estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Compression of Generative Pre-trained Language Models. Some early explorations compress the generative pre-trained language models. KnGPT2 <ref type="bibr" target="#b9">(Edalati et al., 2021)</ref> applies the Kronecker decomposition to compress the GPT. Dis-tilGPT2<ref type="foot" target="#foot_0">2</ref> distills a 12-layer GPT-2 to a 6-layer one, which is twice as fast during inference. Light-PAFF <ref type="bibr" target="#b31">(Song et al., 2020)</ref> proposes a distillation approach that the training loss is a combination of a maximum likelihood loss of the student model, and the KL divergence between the output of teacher and student models. SpAtten <ref type="bibr" target="#b35">(Wang et al., 2021)</ref> proposes a sparse model with algorithm and architecture co-design, which removes uninformative Table <ref type="table">7</ref>: Ablation study on the learning of the clipping factor with 2-bit GPT-2 on the language modeling task.</p><p>tokens and attention heads. Compared with these methods, we not only study the difficulties of compression from the properties of generative tasks, but also study both decoder and encoder-decoder generative models.</p><p>Quantization of Pre-trained Language Models. Quantization compresses a model by representing the 32-bit floating-point parameter with a low-bit representation, and has been widely used in various domains as it does not require designing a new model architecture. There have been many attempts to quantize task-specific BERT models <ref type="bibr" target="#b38">(Zafrir et al., 2019;</ref><ref type="bibr" target="#b30">Shen et al., 2020;</ref><ref type="bibr" target="#b37">Zadeh et al., 2020)</ref> with only negligible performance drop on natural language understanding tasks. Recent works <ref type="bibr" target="#b40">(Zhang et al., 2020;</ref><ref type="bibr" target="#b1">Bai et al., 2021)</ref> even push the weight bit-width down to as low as 1-bit. Despite the success of these approaches for BERT models, attempts to quantize generative PLMs are scarce, and the underlying difficulty remains unclear.</p><p>Contrastive Learning. Contrastive learning aims at pushing the representations of similar samples together while pulling those of dissimilar ones apart. and is widely used for large-scale self-supervised learning in various domains <ref type="bibr" target="#b5">(Chen et al., 2020;</ref><ref type="bibr" target="#b32">Sun et al., 2020a;</ref><ref type="bibr">Baevski et al., 2020;</ref><ref type="bibr" target="#b18">Huang et al., 2022)</ref>, and multi-modal learning <ref type="bibr" target="#b27">(Radford et al., 2021;</ref><ref type="bibr" target="#b19">Jia et al., 2021)</ref>. SimCLR <ref type="bibr" target="#b5">(Chen et al., 2020)</ref> directly uses other in-batch samples as negatives, and sufficient large batch size is required to work well. MoCo <ref type="bibr" target="#b12">(He et al., 2020)</ref> maintains a large number of negative samples in a queue and uses a moving average key encoder to improve consistency. Contrastive learning without negative samples is also proposed in BYOL <ref type="bibr" target="#b11">(Grill et al., 2020)</ref> and SimSiam <ref type="bibr" target="#b6">(Chen and He, 2021)</ref>. Contrastive representation distillation <ref type="bibr" target="#b34">(Tian et al., 2019)</ref> distills the knowledge from the teacher network to the student network by maximizing the mutual information between them.</p><p>The closest work with our token-level contrastive distillation is Wav2vec 2.0 <ref type="bibr">(Baevski et al., 2020)</ref>, which use in-utterance representations at different positions as negatives in speech learning. Besides the difference in the modality and tasks, our method also differs from theirs in (1) Model: We quantize the model parameters and activations while they do not; (2) Representation: For each sample, we use the output of the full-precision and the quantized networks as its two views, while they use the quantized and the contextualized representation.</p><p>(3) Loss: We calculate loss over all tokens in an auto-regressive manner, while they only calculate over the masked tokens non-autoregressively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper studies low-bit quantization of generative PLMs. We find that the difficulty of quantizing generative PLMs lies in homogeneous word embedding and varied distribution of weights. To alleviate the two problems, we propose token-level contrastive learning to learn more distinguishable token emebeddings, as well as a module-dependent dynamic scaling for more accurate quantization. Extensive experiments on language modeling, next utterance prediction and abstractive summarization demonstrate the efficacy of our proposed method. We hope our work sheds a light on the compression of generative PLMs in future exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Derivation of Gradient of Dynamic Scaling</head><p>In this section, we provide the derivation of the gradient of the proposed dynamic scaling γ. The quantization in the forward process can be written as</p><formula xml:id="formula_11">α = w 1 n γ, u = clip(w, −α, +α)/α, w q = Q(u)α,</formula><p>where Q(•) is an uniform quantization function as described in Section 2.2. Based on the chain rule, the gradient of scaling γ w.r.t. the training loss function is:</p><formula xml:id="formula_12">∂ ∂γ = ∂ ∂w q [ ∂w q ∂Q(u) ∂Q(u) ∂α ∂α ∂γ + ∂w q ∂α ∂α ∂γ ] = ∂ ∂w q [α ∂Q(u) ∂α w 1 n + Q(u) w 1 n ] = ∂ ∂w q [α ∂Q(u) ∂α + Q(u)] w 1 n .<label>(8)</label></formula><p>We use straight through estimator to estimate the gradient of uniform quantizer Q(•), i.e., ∀i, ∂Q(u i ) ∂u i = 1 . Thus the gradient ∂Q(u)  ∂α can be written as:</p><formula xml:id="formula_13">∂Q(u) ∂α = ∂Q(u) ∂u ∂u ∂α =    0, w ≤ −α − w α 2 , −α &lt; w &lt; α 0, w ≥ α .<label>(9)</label></formula><p>By combining Eq. ( <ref type="formula" target="#formula_12">8</ref>) and Eq. ( <ref type="formula" target="#formula_13">9</ref>), we get</p><formula xml:id="formula_14">∂ ∂γ =        ∂ ∂wq Q(u) w 1 n , w ≤ −α ∂ ∂wq [− w α + Q(u)] w 1 n , −α &lt; w &lt; α ∂ ∂wq Q(u) w 1 n , w ≥ α</formula><p>where ∂ ∂γ considers both the weight inside and outside the clipping value, and proportional to the weight magnitude w 1 n . Table <ref type="table">9</ref>: Ablation study on larger models. We report the results on 24-layer GPT-2 and 24-layer BART.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Experimental Settings</head><p>is initialized to 0.0002 (resp. 0.001) for the BART backbone parameters (resp. clipping factor γ) and then linearly decays to 0. The number of negative samples is 32. The temperature τ and momentum coefficient m is 0.1 and 0.5, respectively. We train with the AdamW optimizer with batch size 128, for a total of 8 epochs. LAQ. LAQ <ref type="bibr" target="#b17">(Hou et al., 2017;</ref><ref type="bibr" target="#b16">Hou and Kwok, 2018</ref>) is a loss-aware quantization method that views quantization as an optimization problem and solve it via proximal Newton algorithm. We use the approximate solver in <ref type="bibr" target="#b16">(Hou and Kwok, 2018)</ref> to compute the quantized weights before each forward propagation.</p><p>For the self-implemented methods PACT, LSQ and LAQ, we adopt the commonly-used distillation loss adopted in <ref type="bibr" target="#b14">(Hinton et al., 2015;</ref><ref type="bibr" target="#b20">Jiao et al., 2020)</ref>. Note that these methods are only used for weights and embeddings, while the activations of these methods follow the same setting as our proposed method in Section 2.1. We also tried using the original language modeling loss w.r.t. the ground-truth labels, and distillation loss over the the attention as <ref type="bibr" target="#b20">(Jiao et al., 2020)</ref>. However, these two losses worsens the performance on all three methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Frameworks of Double-head GPT-2 and BART</head><p>Since we adopt double-head GPT-2 and BART for next utterance prediction and abstractive summarization, the frameworks for these tasks are slightly modified from that on language modeling due to the difference of tasks. In Figure <ref type="figure">9</ref> and 10, we illustrate the framework for double-head GPT-2 and BART, respectively. In the double-head GPT-2, we also quantize the final linear layer in the output head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Performance of Larger Models</head><p>In Table <ref type="table">9</ref>, we experiment with GPT-base and BART-large, which both have 24 Transformer layers. For all bit-widths, the training of our method converges successfully without gradient exploding/vanishing problems. QuantGPT outperforms PACT by a large margin in all tasks, especially for 2-bit weight. Our quantization method on larger models also has better performance than that on 12-layer GPT-2 and 12-layer BART in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Examples of Summarizations</head><p>In Table 10, we provide the example summarizations on the XSum dataset. By comparing the articles, references and generations, the generated summaries by our quantized model are more logical and terse than PACT, LSQ and LAQ, which face problems of homogeneous word embeddings to some extent as discussed in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 More Visualizations for the Token Representations</head><p>In Figure <ref type="figure" target="#fig_0">11</ref>, we provide the visualizations of token representations on more samples. The observations</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performance of quantized GPT-2 with varying weight bit-widths and 8-bit activation, using different methods. The right figure takes a closer look at LAQ.</figDesc><graphic url="image-1.png" coords="1,313.23,212.60,204.09,72.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: T-SNE visualization of the most frequent 500 word embeddings, of the full-precision and different 2-bit quantized models trained on PTB dataset. Embeddings of different methods show different degrees of homogeneity.</figDesc><graphic url="image-7.png" coords="3,80.84,208.13,430.86,96.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Matrices representing the cosine similarities between representations of all pairs of tokens in a sentence, between the full-precision model and 2-bit quantized models trained on PTB dataset. Token representations at the last decoder layer of GPT-2 are used. More visualizations are available in Appendix C.3.</figDesc><graphic url="image-9.png" coords="3,419.37,368.61,99.77,71.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) wo at Layer 4. (b) wg at Layer 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Distributions of output projection matrix w o in the multi-head attention module and the second linear layer w g in the feed-forward network of the 4-th layer from the 12-layer full-precision GPT-2. Other modules in other layers exhibit similar patterns. Vertical lines indicate the clipping factors learned by PACT and our method. Black curves show the estimated distribution by kernel density estimation.</figDesc><graphic url="image-8.png" coords="3,311.41,366.85,99.78,73.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Four variants of negative sampling.</figDesc><graphic url="image-12.png" coords="7,315.94,158.21,126.99,64.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Effect of the number of negative samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Scaling factors in the 2-bit QuantGPT.</figDesc><graphic url="image-15.png" coords="8,185.98,82.04,98.67,71.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>B. 4</head><label>4</label><figDesc>Description of the Compared Methods PACT. PACT<ref type="bibr" target="#b7">(Choi et al., 2018)</ref> learns a learnable clipping factor for each module by gradient descent. To make the quantization more accurate, we adopt a flexible variant of the original PACT, with different positive and negative clipping factors [−α neg , α pos ], where both α neg and α pos are initialized as 2.5.LSQ. LSQ (Esser et al., 2020) learns the stepsize of quantizer for each module by gradient descent. We use the recommended initialization strategy of the step size as<ref type="bibr" target="#b10">(Esser et al., 2020)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>prec.</cell><cell>474.9</cell><cell>14.48</cell><cell>14.72</cell><cell>14.19</cell><cell>77.01</cell></row><row><cell>PACT</cell><cell>8-8-8</cell><cell>121.4</cell><cell>17.49</cell><cell>16.11</cell><cell>16.76</cell><cell>74.73</cell></row><row><cell>LSQ</cell><cell>8-8-8</cell><cell>121.4</cell><cell>16.75</cell><cell>15.43</cell><cell>15.24</cell><cell>75.28</cell></row><row><cell>LAQ</cell><cell>8-8-8</cell><cell>121.4</cell><cell>16.91</cell><cell>15.87</cell><cell>15.88</cell><cell>76.02</cell></row><row><cell>QuantGPT</cell><cell>8-8-8</cell><cell>121.4</cell><cell>15.31</cell><cell>14.90</cell><cell>14.58</cell><cell>76.12</cell></row><row><cell>PACT</cell><cell>4-4-8</cell><cell>62.4</cell><cell>19.23</cell><cell>20.17</cell><cell>20.15</cell><cell>25.13</cell></row><row><cell>LSQ</cell><cell>4-4-8</cell><cell>62.4</cell><cell>78.99</cell><cell>79.76</cell><cell>75.12</cell><cell>45.10</cell></row><row><cell>LAQ</cell><cell>4-4-8</cell><cell>62.4</cell><cell>17.12</cell><cell>16.55</cell><cell>16.91</cell><cell>71.71</cell></row><row><cell>QuantGPT</cell><cell>4-4-8</cell><cell>62.4</cell><cell>15.55</cell><cell>14.95</cell><cell>15.31</cell><cell>76.57</cell></row><row><cell>PACT</cell><cell>2-2-8</cell><cell>33.0</cell><cell>173.02</cell><cell>189.13</cell><cell>171.03</cell><cell>5.52</cell></row><row><cell>LSQ</cell><cell>2-2-8</cell><cell>33.0</cell><cell>847.54</cell><cell>544.98</cell><cell>1470.86</cell><cell>5.54</cell></row><row><cell>LAQ</cell><cell>2-2-8</cell><cell>33.0</cell><cell>19.15</cell><cell>18.25</cell><cell>18.97</cell><cell>71.36</cell></row><row><cell>QuantGPT</cell><cell>2-2-8</cell><cell>33.0</cell><cell>17.30</cell><cell>16.12</cell><cell>16.98</cell><cell>74.78</cell></row></table><note>Results of language modeling on the test set of WikiText2, PTB and WikiText103 datasets, and next utterance prediction on the validation set of Persona-Chat dataset, with quantized GPT-2. "#Bits (W-E-A)" represents the bit-width for weights of Transformer layers, word embedding, and activations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison between our proposed quatization method and other compression methods on GPT-2.</figDesc><table><row><cell>Method</cell><cell>Size (MB)(↓)</cell><cell>WikiText2 PPL(↓)</cell><cell>PTB PPL(↓)</cell><cell>WikiText103 PPL(↓)</cell></row><row><cell cols="3">full-prec. 474.9 (1.0x) 14.4</cell><cell>14.6</cell><cell>13.9</cell></row><row><cell cols="2">KnGPT2 332.0 (1.4x)</cell><cell>-</cell><cell>-</cell><cell>20.5</cell></row><row><cell cols="2">DistilGPT2 329.6 (1.4x)</cell><cell>-</cell><cell>-</cell><cell>21.1</cell></row><row><cell cols="3">LightPAFF 268.0 (1.8x) 18.8</cell><cell>22.8</cell><cell>16.4</cell></row><row><cell cols="3">Ours(8-8-8) 121.4 (3.9x) 15.3</cell><cell>14.9</cell><cell>14.6</cell></row><row><cell cols="2">Ours(4-4-8) 62.4 (7.6x)</cell><cell>15.6</cell><cell>15.0</cell><cell>15.3</cell></row><row><cell cols="3">Ours(2-2-8) 33.0 (14.4x) 17.3</cell><cell>16.1</cell><cell>17.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results of abstractive summarization on the test set of the XSum dataset, with quantized BART.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>10 33.05</cell></row><row><cell>PACT</cell><cell>8-8-8</cell><cell cols="2">138.1 39.16 16.60 31.60</cell></row><row><cell>LSQ</cell><cell>8-8-8</cell><cell cols="2">138.1 39.09 16.72 31.56</cell></row><row><cell>LAQ</cell><cell>8-8-8</cell><cell cols="2">138.1 39.10 16.74 31.65</cell></row><row><cell cols="2">QuantBART 8-8-8</cell><cell cols="2">138.1 40.25 17.78 32.70</cell></row><row><cell>PACT</cell><cell>4-4-8</cell><cell cols="2">72.4 32.68 11.52 26.03</cell></row><row><cell>LSQ</cell><cell>4-4-8</cell><cell cols="2">72.4 38.94 16.48 31.46</cell></row><row><cell>LAQ</cell><cell>4-4-8</cell><cell cols="2">72.4 39.03 16.68 31.63</cell></row><row><cell cols="2">QuantBART 4-4-8</cell><cell cols="2">72.4 40.24 17.71 32.69</cell></row><row><cell>PACT</cell><cell>2-2-8</cell><cell>39.6</cell><cell>7.76 1.30 6.96</cell></row><row><cell>LSQ</cell><cell>2-2-8</cell><cell cols="2">39.6 37.09 14.88 29.76</cell></row><row><cell>LAQ</cell><cell>2-2-8</cell><cell cols="2">39.6 37.48 15.27 30.13</cell></row><row><cell cols="2">QuantBART 2-2-8</cell><cell cols="2">39.6 39.15 16.72 31.72</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>-</cell><cell>Sampling method</cell><cell cols="2">WikiText2 PTB WikiText103</cell></row><row><cell>-</cell><cell>QuantGPT</cell><cell>17.30 16.12</cell><cell>16.98</cell></row><row><cell></cell><cell>fp+quan.</cell><cell>17.38 16.51</cell><cell>17.13</cell></row><row><cell>Tok-level</cell><cell>quan. only</cell><cell>17.35 16.54</cell><cell>17.15</cell></row><row><cell></cell><cell>global</cell><cell>17.71 16.63</cell><cell>17.55</cell></row><row><cell>Seq-level</cell><cell cols="2">in-batch (bz=32) 17.62 19.23 in-batch (bz=16) 17.48 17.11</cell><cell>18.97 18.16</cell></row></table><note>Ablation study on negative sampling for 2bit weight, "bz" denotes for the batch size. "Tok" and "Seq" are abbreviation for token and sequence, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Efficiency study of the token-level contrastive learning. The results are reported on the PTB dataset on 2-bit GPT-2. "sec/iter" means the needed time in seconds per iteration. Memory denotes the GPU consumption per device.</figDesc><table><row><cell>Training loss</cell><cell>Training time (sec/iter) (↓)</cell><cell>Memory (MB) (↓)</cell><cell>PPL (↓)</cell></row><row><cell>L dist</cell><cell>0.61</cell><cell>14700</cell><cell>16.93</cell></row><row><cell>L dist + λL cont</cell><cell>0.67</cell><cell>14839</cell><cell>16.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Representations for the contrastive loss L cont in 2-bit setting. The "decoder-last" means the contrastive loss is computed on the hidden states from the last Transformer layer of the decoder after a linear transform.</figDesc><table><row><cell>-</cell><cell>WikiText2</cell><cell>PTB</cell><cell>WikiText103</cell><cell>Persona-Chat</cell><cell></cell><cell>XSum</cell><cell></cell></row><row><cell>Metric</cell><cell>PPL (↓)</cell><cell>PPL (↓)</cell><cell>PPL (↓)</cell><cell>Acc(%) (↑)</cell><cell>R1 (↑)</cell><cell>R2 (↑)</cell><cell>RL (↑)</cell></row><row><cell>decoder-last</cell><cell>17.30</cell><cell>16.12</cell><cell>16.98</cell><cell>74.78</cell><cell>39.15</cell><cell>16.72</cell><cell>31.72</cell></row><row><cell>decoder-first</cell><cell>18.02</cell><cell>16.61</cell><cell>17.25</cell><cell>74.75</cell><cell>39.11</cell><cell>16.70</cell><cell>31.62</cell></row><row><cell>encoder-last</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>38.91</cell><cell>16.72</cell><cell>31.67</cell></row><row><cell>encoder-first</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>38.87</cell><cell>16.70</cell><cell>31.56</cell></row><row><cell>Method</cell><cell cols="3">WikiText2 PTB WikiText103</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>QuantGPT</cell><cell>17.30</cell><cell>16.12</cell><cell>16.98</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>L dist only</cell><cell>17.85</cell><cell>16.93</cell><cell>17.78</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours with PACT</cell><cell>20.03</cell><cell>17.78</cell><cell>25.54</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Data splits of different datasets.</figDesc><table><row><cell>B.1 Datasets</cell></row><row><cell>The train/val/test splits for different datasets are</cell></row><row><cell>shown on Table 8.</cell></row><row><cell>B.2 Model Architectures</cell></row><row><cell>GPT-2. The vocabulary size of GPT-2 is 50527.</cell></row><row><cell>We use GPT-2-small with 12 decoder layers and</cell></row><row><cell>hidden state dimension as 768, for experiments</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">https://transformer.huggingface.co/ model/distil-gpt2</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported in part by the General Research Fund (GRF) project 17206020, and in part by ACCESS, AI Chip Center for Emerging Smart Systems, Hong Kong SAR.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Article: On Tuesday, a BBC Spotlight programme revealed that eight children had gone missing in Northern Ireland. Two of the girls were Somali teenagers who disappeared in 2005 and 2012. The Health and Social Care Board has said new guidelines are in place and add that no children have gone missing since 2014. Separated children are children outside their country of origin and separated from their parents or legal guardian. The term can also include unaccompanied asylum-seeking children and trafficked children. When they arrive in Northern Ireland they are taken into the care of the local health trust. Eight children have gone missing since 2005 and they remain missing. The SDLP's health spokesman Mark H Durkan said he would be raising the issue at the Northern Ireland Assembly's health committee and his party colleague Alex Attwood would be raising it at the justice committee. "The number of children who cannot be accounted for is something that needs urgent inquiry and investigation," he said. "There is a lot of very good work being done to look after the welfare of unaccompanied young people, but clearly we now have some very big questions that need to be answered." Ulster Unionist MLA Jo-Anne Dobson said it was "frankly appalling" to hear that eight children had gone missing. "I have written to Health Minister Michelle O'Neill on this issue to seek further clarification and to demand details of how the department, health trusts and the Health and Social Care Board have sought to address each of the cases involved in the investigation," she added. The Green Party leader Steven Agnew also said it was extremely worrying that children can disappear without a trace. Paula Bradshaw from the Alliance Party added that the health trusts and police "need to work closer over the handling of these cases". In a statement, the Police Ombudsman for Northern Ireland said: "Our director of investigations will be reviewing the contents of the programme to ascertain if there are any issues of police conduct which may need further investigation." The Police Service of Northern Ireland has said that in the two cases identified in the programme, investigations were robust and all information available at the time was followed. The Health and Social Care Board has said that new guidelines are in place and stress that no children have gone missing since 2014. BBC Spotlight's investigation is now available on BBC iPlayer.</p><p>Reference: An urgent inquiry is needed into separated children who have gone missing from care, the Social Democratic and Labour Party has said.</p><p>PACT: TheTheAAATheTheTheAnAnAnTheThe an an an been been been jailed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LSQ:</head><p>The SDLP has called for an urgent inquiry into the welfare of unaccompanied children in Northern Ireland.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LAQ:</head><p>The SDLP has called for "urgent inquiry and investigation" into the handling of unaccompanied children in Northern Ireland.</p><p>Ours: The SDLP is calling for an urgent inquiry and investigation into the disappearance of unaccompanied young people.</p><p>Article: The dairies operation, which processes and distributes milk, is being sold to Germany's Mueller for Â£80m. It comes as profits at the UK's largest dairy food company fell 95% to Â£900,000 in the six months to September. Dairy Crest processes and delivers around 1.3 billion litres of milk a year for retailers and homes. Dairy Crest said in a statement that the deal was in the best interests of consumers, customers and dairy farmers. The dairies business accounts for about 70% of the company's revenues, which rose 1% to Â£682.1m during the six months. After the sale, which still needs shareholder approval and could take several months, Dairy Crest will focus on its profitable cheese and spreads operations. There are about 14,000 dairy farmers in the UK, producing 3.3 million litres a day. However, with milk prices having fallen, there has been much debate about whether the economics of the industry are sustainable. Investors approved of the Dairy Crest's decision to get out of a loss-making sector, sending its shares 10% higher in morning trading on Thursday. Muller said the deal would lead to lower costs and larger exports of dairy products made in the UK. Ronald Kers, chief executive of Muller UK &amp; Ireland, said: "We are concerned that the dynamics of the UK fresh milk market are unsustainable for dairy processors in the mid to long term and this acquisition will allow us to reduce our costs, increase our efficiencies and invest in the future." Under the deal, Mueller's UK division -Muller Wiseman Dairies -will take over factories at Foston, in Derbyshire, Chadwell Heath, in Essex, and Severnside, near Gloucester. The deal also includes the Hanworth glass bottling site in Middlesex, where Dairy Crest is consulting with employees on the site's future, and 72 depots. Muller bought Robert Wiseman in 2012.</p><p>Reference: Dairy Crest, maker of Cathedral City cheese and Country Life butter, has announced a big slump in profits and the sale of its milk business.</p><p>PACT: More than than more more more than more than than than to be be be will will will be be are are are be be to the.</p><p>LSQ: Dairy Crest is to sell its Dairies business to a German company for an undisclosed sum.</p><p>LAQ: Dairy giant Dairy Crest is to sell its UK business to a German company for an undisclosed sum.</p><p>Ours: Dairy Crest, the world's largest dairy producer, is to sell its UK operations to a German firm.  are similar to those in Section 2.2. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Binarybert: Pushing the limit of bert quantization</title>
		<author>
			<persName><forename type="first">Haoli</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Litegt: Efficient and lightweight graph transformers</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaofan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
				<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pact: Parameterized clipping activation for quantized neural networks</title>
		<author>
			<persName><forename type="first">Jungwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swagath</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Jen</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayalakshmi</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailash</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><surname>Gopalakrishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06085</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Pierre</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3123" to="3131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kronecker decomposition for gpt compression</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Edalati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzieh</forename><surname>Tahaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vahid</forename><surname>Partovi Nia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Rezagholizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learned step size quantization</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Steven K Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepika</forename><surname>Mckinstry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rathinakumar</forename><surname>Bablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharmendra</forename><forename type="middle">S</forename><surname>Appuswamy</surname></persName>
		</author>
		<author>
			<persName><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Pierre H Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Gheshlaghi Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynabert: Dynamic bert with adaptive width and depth</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Loss-aware weight quantization of deep networks</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Loss-aware binarization of deep networks</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Quanming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spiral: Self-supervised perturbation-invariant representation learning for speech pre-training</title>
		<author>
			<persName><forename type="first">Wenyong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">Ting</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhsuan</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tinybert: Distilling bert for natural language understanding</title>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4163" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<title level="m">Pointer sentinel mixture models</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Context dependent recurrent neural network language model</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="234" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1797" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pretraining</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Q-bert: Hessian based ultra low precision quantization of bert</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Lightpaff: A two-stage distillation framework for pre-training and fine-tuning</title>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12817</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Contrastive distillation on intermediate representations for language model compression</title>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
			<biblScope unit="page" from="498" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mobilebert: a compact task-agnostic bert for resource-limited devices</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
			<biblScope unit="page" from="2158" to="2170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Contrastive representation distillation</title>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spatten: Efficient sparse attention architecture with cascade token and head pruning</title>
		<author>
			<persName><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="97" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On layer normalization in the transformer architecture</title>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieyan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10524" to="10533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Gobo: Quantizing attention-based nlp models for low latency and energy efficient inference</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Hadi Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isak</forename><surname>Edo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="811" to="824" />
		</imprint>
	</monogr>
	<note>Omar Mohamed Awad, and Andreas Moshovos</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Zafrir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Boudoukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Izsak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Wasserblat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06188</idno>
		<title level="m">Q8bert: Quantized 8bit bert</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Personalizing dialogue agents: I have a dog, do you have pets too?</title>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2204" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ternarybert: Distillation-aware ultra-low bit bert</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
