<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image based fruit category classification by 13-layer deep convolutional neural network and data augmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yu-Dong</forename><surname>Zhang</surname></persName>
							<email>yudongzhang@ieee.org</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Henan Polytechnic University</orgName>
								<address>
									<postCode>454000</postCode>
									<settlement>Jiaozuo</settlement>
									<region>Henan</region>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Jiangsu Key Laboratory of Advanced Manufacturing Technology</orgName>
								<address>
									<postCode>223003</postCode>
									<settlement>Huaiyin</settlement>
									<region>Jiangsu</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengchao</forename><surname>Dong</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Translational Imaging Division &amp; MRI Unit</orgName>
								<orgName type="institution">Columbia University and New York State Psychiatric Institute</orgName>
								<address>
									<postCode>10032</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xianqing</forename><surname>Chen</surname></persName>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Department of electrical engineering</orgName>
								<orgName type="department" key="dep2">College of engineering</orgName>
								<orgName type="institution">Zhejiang Normal University</orgName>
								<address>
									<postCode>321004</postCode>
									<settlement>Jinhua</settlement>
									<region>Zhejiang</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenjuan</forename><surname>Jia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sidan</forename><surname>Du</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">School of Electronic Science and Engineering</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210046</postCode>
									<settlement>Nanjing</settlement>
									<region>Jiangsu</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">&amp;</forename><surname>Khan</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">College of Software Convergence</orgName>
								<orgName type="institution">Sejong University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shui-Hua</forename><surname>Wang</surname></persName>
							<email>shuihuawang@ieee.org</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Henan Polytechnic University</orgName>
								<address>
									<postCode>454000</postCode>
									<settlement>Jiaozuo</settlement>
									<region>Henan</region>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Khan</forename><surname>Muhammad</surname></persName>
							<email>khan.muhammad@ieee.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Shui-Hua Wang</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Nanjing Normal University</orgName>
								<address>
									<postCode>210023</postCode>
									<settlement>Nanjing</settlement>
									<region>Jiangsu</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image based fruit category classification by 13-layer deep convolutional neural network and data augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A1E56F6568ED360AC504D956094644C9</idno>
					<idno type="DOI">10.1007/s11042-017-5243-3</idno>
					<note type="submission">Received: 20 June 2017 / Revised: 16 August 2017 / Accepted: 20 September 2017 Multimed Tools Appl</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Convolutional neural network</term>
					<term>Fully connected layer</term>
					<term>Softmax</term>
					<term>Fruit category identification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fruit category identification is important in factories, supermarkets, and other fields. Current computer vision systems used handcrafted features, and did not get good results. In this study, our team designed a 13-layer convolutional neural network (CNN). Three types of data augmentation method was used: image rotation, Gamma correction, and noise injection. We also compared max pooling with average pooling. The stochastic Multimed Tools Appl</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fruit classification is a challenge since it is difficult to provide a definition of a type of fruit. Nevertheless, fruit classification can help in factory automatic fruit-packing and transportation <ref type="bibr" target="#b12">[12]</ref>, supermarket price determination <ref type="bibr" target="#b8">[9]</ref>, and dietary guidance <ref type="bibr" target="#b10">[10]</ref>. At present, there are two types of fruit classification: one is to identify specific type of fruit, and the other is to classify multiple fruit categories.</p><p>In the past, scholars tend to use near-infrared imaging <ref type="bibr" target="#b28">[28]</ref>, gas sensor <ref type="bibr" target="#b27">[27]</ref>, highperformance liquid chromatography <ref type="bibr" target="#b25">[25]</ref> devices to scan the fruit. Nevertheless, those methods need expensive devices (different types of sensors) and professional operators, and their overall accuracies are commonly lower than 85% <ref type="bibr" target="#b20">[20]</ref>.</p><p>Image-based fruit classification has attracted attention of scholars since their cheap device (only a digital camera) and excellent performance. For example, Wu <ref type="bibr" target="#b37">[37]</ref> used principal component analysis (PCA) to reduce the color, texture, and morphological features. They introduced a kernel support vector machine (KSVM) as the classifier. Their overall accuracy reached 88.20%. Adak and Yumusak <ref type="bibr" target="#b1">[2]</ref> used artificial bee colony (ABC)-based neural network (NN). Ji <ref type="bibr" target="#b14">[14]</ref> replaced the KSVM with a fitness-scaled chaotic ABC (FSCABC). Tovar and Losada <ref type="bibr" target="#b35">[35]</ref> used fuzzy logic block to develop a fuzzy fruit classification system. Wei <ref type="bibr" target="#b36">[36]</ref> employed wavelet entropy (WE) and biogeography-based optimization. The overall accuracy of their method was 89.47%. Wu <ref type="bibr" target="#b38">[38]</ref> combined BBO with feedforward neural network (FNN). Garcia et al. <ref type="bibr" target="#b11">[11]</ref> extracted color chromaticity, texture and shape features. Lu <ref type="bibr" target="#b21">[21]</ref> used fractional Fourier entropy (FRFE) as the features, and they used back propagation neural network (BPNN) as the classifier. Lu and Li <ref type="bibr" target="#b22">[22]</ref> replaced BPNN with an improved hybrid genetic algorithm (IHGA). Their method received an overall accuracy nearly to 90%.</p><p>To further improve the performance of image-based fruit classifiers, we analyzed past literature, and believe the problems lies in following points: (i) Past methods used manual features designed by experts, but these authors may not be the optimal. (ii) The classifiers are of simple structure, and may fail to map the complicated features to the final identification result.</p><p>Convolutional neural network (CNN) can solve above two problems, and it is the hottest research topic. In CNN, the neuron-connectivity patterns are inspired by the visual cortex of mammals. CNN firstly showed its outstanding ability in object recognition in the ImageNet competition <ref type="bibr" target="#b39">[39]</ref>. Later, CNN has been widely applied to analyze medical images, such as choroid segmentation <ref type="bibr" target="#b29">[29]</ref>, abnormality detection <ref type="bibr" target="#b5">[6]</ref>, carcinoma nuclei grading <ref type="bibr" target="#b19">[19]</ref>, solitary cysts discrimination <ref type="bibr" target="#b17">[17]</ref>, vibrational spectroscopic data analysis <ref type="bibr" target="#b0">[1]</ref>, etc. To our knowledge, CNN has not been applied to fruit image. Therefore, we would like to investigate the performance of using CNN in fruit classification.</p><p>The structure of the remainder is organized as follows: Section 2 provides the materials, preprocessing, and data augmentation. Section 3 describes the basics of convolutional neural network (CNN). Section 4 contains the experiments and results. Section 5 gives concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset and preprocessing</head><p>The fruit dataset was obtained through three months: (i) 6 months of on-site collecting via digital camera, (ii) download from http://images.google.com; (iii) download from http://images.baidu.com. Finally, we obtain a 3600-image dataset with 200 image for each fruit type.</p><p>A four-step preprocessing technique was used. First, we move the fruit into the center of the image. Second, the image was cropped and resized to a 256 × 256 matrix. Third, split-andmerge algorithm <ref type="bibr" target="#b7">[8]</ref> was used to remove the background. Fourth, we label each image manually to one of the 18 fruit types.</p><p>Figure <ref type="figure">1</ref> shows the preprocessed sample images of our clean fruit image dataset. The 18 types from Fig. <ref type="figure">1a</ref>-r are respectively Anjou pear, blackberry, black grape, blueberry, Bosc pear, cantaloupe, golden pineapple, granny Smith apple, green grape, green plantain, Hass Fig. <ref type="figure">1</ref> Sample of our dataset of clean fruit images avocado, passion fruit, red grape, Rome apple, strawberry, tangerine, watermelon, and yellow banana.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data augmentation</head><p>Small number of training samples may lead to overfitting <ref type="bibr" target="#b23">[23]</ref>. One solution is to create fake data and add them to the training set. We divided original dataset into training set and test set randomly. The training and test set are of half size of original dataset as shown in Table <ref type="table" target="#tab_0">1</ref>. Usually less data is for training and more data for test, but considering we have a relatively large dataset, here we select 1800 for training set and the rest 1800 for test.</p><p>Afterwards, we created the fake samples based on the training set in following five means. The first data augmentation method is image rotation. The rotation angle θ is from -15°to 15°in step of 5°. Thus, we create new samples with size of 6 times of original training set. The second data augmentation method is gamma correction <ref type="bibr" target="#b31">[31]</ref>. The gamma-value r varies from 0.7 to 1.3 with step of 0.1, again leading to new samples with size of 10 times of original training set. The third data generation method is noise injection <ref type="bibr" target="#b15">[15]</ref>. We create 6 new noise-contaminated image for each image. The zero-mean Gaussian noise with variance of 0.01 was employed. The fourth is the scale transform, with scaling parameter S from 0.8 to 1.2 with increment of 0.05, generating 8 new images for each original image. Finally, the fifth is the affine transform, and we randomly generate eight different affine parameters for each original image. All the hyperparameters selected here are following the experiences, which also be used in open published literature <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b30">30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Convolutional neural network</head><p>The convolutional neural network (CNN) with deep structures have gained tremendous success in text/non-text classification <ref type="bibr" target="#b3">[4]</ref>, human detection <ref type="bibr" target="#b16">[16]</ref>, ear detection <ref type="bibr" target="#b6">[7]</ref>, etc. Compared to traditional shallow neural networks, it has three important advantages: sparse interaction, parameter sharing, and equivariance. It has shown significant gains over state-ofthe-art classifiers, such as logistic regression, extreme learning machine, support vector machine and its variants, linear regression classifier, etc. A typical CNN will include convolution layer, nonlinear activation layer, and pooling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Convolution layer</head><p>The convolution layer performs the two-dimensional convolution for three-dimensional input and three-dimensional filter. Suppose the size of the input is H I × W I × C, here H I represents the height, W I the width, and C the channels. Then, suppose the size of the filter is H F × W F × C, here HF and W F represent the height and width of the filters. The channel size of both input  Convolution + ReLU  and filter should be equivalent; hence, the 2D convolution is implemented along the height and width directions (See Fig. <ref type="figure">2</ref>). Suppose the padding size at each margin is P, the stride size is S, we can calculate the height H O and width W O of output as</p><formula xml:id="formula_0">3 × 3 120 [1 1] [1 1] 3 × 3 × 80 × 120 1 × 1 × 120 10 × 10 × 120 7 Pooling 3 × 3 [1 1] [1 1] 10 × 10 × 120 8 Convolution + ReLU 3 × 3 80 [1 1] [1 1] 3 × 3 × 120 × 80 1 × 1 ×</formula><formula xml:id="formula_1">H O ¼ H I -H F þ 2P S þ 1<label>ð1Þ</label></formula><formula xml:id="formula_2">W O ¼ W I -W F þ 2P S þ 1<label>ð2Þ</label></formula><p>The output is also three dimensional with size of H O × W O × N, where N denotes the number of filters.</p><p>The neurons in the feature map after convolution layer will pass through a nonlinear activation function, such as a rectified linear unit (ReLU) layer, which carries out a ReLU function <ref type="bibr" target="#b4">[5]</ref> as</p><formula xml:id="formula_3">ReLU x ð Þ ¼ x x ≥ 0 0 x &lt; 0<label>ð3Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pooling layer</head><p>The pooling function replaces the outputs from the ReLU layer with a summary statistic of nearby outputs <ref type="bibr" target="#b2">[3]</ref>. It has two advantages: (i) guarantee the representation become invariant to small translation of the input; (ii) help to reduce the computation burden.   Suppose the pooling region is R, the activation set A included in R is</p><formula xml:id="formula_4">A ¼ a i i ∈ R j f g<label>ð4Þ</label></formula><p>The max-pooling P M <ref type="bibr" target="#b26">[26]</ref> is the most popular pooling strategy. It is defined as</p><formula xml:id="formula_5">P M ¼ max A R ð Þ<label>ð5Þ</label></formula><p>Average-pooling P A <ref type="bibr" target="#b44">[44]</ref> is another pooling technique defined as</p><formula xml:id="formula_6">P A ¼ ∑A R A R j j<label>ð6Þ</label></formula><p>where |.| is the number of the elements in the set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network structure</head><p>The structure of our CNN is a 13-layer deep neural network. The details of each layer are shown in Table <ref type="table" target="#tab_1">2</ref>. The image input layer just inputs the preprocessed fruit image directly. The convolution layer, ReLU layer, and pooling layer are described before. We set the filter size and the number of filters by experiences.</p><p>The fully connected (FC) layer multiplies the input by a weight matrix and then adds a bias vector. The softmax layer used the softmax function, also known as the multiclass generalization of logistic regression. Suppose P(r) is the class prior probability, and P(x|r) is the conditional probability of sample given class r. Then we can conclude that the probability of sample x belonging to class r is</p><formula xml:id="formula_7">P r x j ð Þ ¼ P x r j ð ÞP r ð Þ ∑ R k¼1 P x k j ð ÞP k ð Þ<label>ð7Þ</label></formula><p>Here R is the total number of classes. If we define A r as Then we have</p><formula xml:id="formula_8">A r ¼ ln P x; r ð ÞP r ð Þ ð Þ<label>ð8Þ</label></formula><formula xml:id="formula_9">P r x j ð Þ ¼ exp A r x ð Þ ð Þ ∑ R k¼1 exp A k x ð Þ ð Þ<label>ð9Þ</label></formula><p>Finally, the output layer transforms the numerical result to categorical names with following rules: 1➔Anjou pear, 2➔blackberry, 3➔black grape, 4➔blueberry, 5➔Bosc pear, 6➔cantaloupe, 7➔golden pineapple, 8➔granny Smith apple, 9➔green grape, 10➔green plantain, 11➔Hass avocado, 12➔passion fruit, 13➔red grape, 14➔Rome apple, 15➔strawberry, 16➔tangerine, 17➔watermelon, and 18➔yellow banana. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training setting</head><p>The CNN Training was based on NVIDIA GeForce GTX 1050 with compute capability of 6.1, clock rate of 1455 MHz, and multiprocessors of 5. The training algorithm was stochastic gradient descent with momentum (SGDM). Here the Bstochastic^represents the minibatch training method, in which we set its size to 128. The initial learning rate is set to 0.01, and was decreased by factor of 10 every 10 epochs. The momentum was set to 0.9. The maximum epochs was assigned with a value of 30. Cross entropy <ref type="bibr" target="#b24">[24]</ref> was used as the loss function, since it is suitable for multiclass problem <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b40">40]</ref>. Table <ref type="table" target="#tab_3">3</ref> shows the hardware and software settings for training CNN. Here the hyperparameters are obtained by experiences, following the settings in open published literature <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">16]</ref>. Table <ref type="table" target="#tab_4">4</ref> shows the amount of augmented training set and the test set.</p><p>4 Experiments and results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Illustration of data augmentation</head><p>In this experiment, we take the Anjou pear (Fig. <ref type="figure">1a</ref>) as an example, and shows its generated images in below.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Confusion matrix</head><p>Using our data augmentation method, the confusion matrix over the test set was listed in Fig. <ref type="figure" target="#fig_7">9</ref>. The sensitivity, specificity, precision, and accuracy over the test set was presented in Table <ref type="table" target="#tab_5">5</ref>. The overall accuracy of all classes is defined as the number of correctly identified fruit images divided by the number of the whole fruit images. The result of our overall accuracy is 94.94%. Note that overall accuracy is equal to the average sensitivity at the condition of equal or similar class numbers, which is our case as shown in Table <ref type="table" target="#tab_5">5</ref>.</p><p>Table <ref type="table" target="#tab_5">5</ref> shows that the second class (blackberry) and fifteenth class (strawberry) can be identified perfectly with sensitivity of 100.0%. The fruit with worst performance is eighth class (Granny Smith apple). From the confusion matrix in Figs. 9, 9 Granny Smith apples were misclassified as Anjou pear, 1 Granny Smith apple was misclassified as Golden pineapple, four Granny Smith apples were misclassified as green grapes, and 2 Granny Smith apples were misclassified as green plantains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Pooling technique comparison</head><p>In this experiment, we compared the max-pooling and average pooling technique. The results are shown below in Table <ref type="table" target="#tab_6">6</ref>.</p><p>The comparison results between max-pooling and average-pooling approaches in Table <ref type="table" target="#tab_6">6</ref> showed max-pooling gives 0.11% better accuracy than average-pooling. Previous studies have shown that average pooling considered all elements in the region, hence, it will down-weight the strong activations. Nevertheless, the improvement of max-pooling in this study is slight. In the future, we shall test other pooling methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Optimal structure of CNN</head><p>The convolution layer and pooling layer are the most important among all layers. Our CNN contains four combined layers (convolution layer and pooling layer) as shown in Table <ref type="table" target="#tab_1">2</ref>. In this experiment, we used grid searching method to find the optimal number of combined layers. The parameter setting here are the same as in Table <ref type="table" target="#tab_3">3</ref>. Finally, the overall accuracy results are presented in Table <ref type="table" target="#tab_7">7</ref> and the corresponding bar-plot is pictured in Fig. <ref type="figure" target="#fig_8">10</ref>. We observed that CNN with 2, 3, 4, 5, 6, and 7 combined layers yielded an overall accuracy of 90.17%, 94.06%, 94.94%, 92.33%, 92.00%, and 92.89%, respectively. Hence, we select 4 combined layers in our proposed structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Comparison to state-of-the-art approaches</head><p>Finally, we compared our method with five state-of-the-art approaches: PCA + kSVM <ref type="bibr" target="#b37">[37]</ref>, PCA + FSCABC <ref type="bibr" target="#b14">[14]</ref>, WE + BBO <ref type="bibr" target="#b36">[36]</ref>, FRFE + BPNN <ref type="bibr" target="#b21">[21]</ref>, FRFE + IHGA <ref type="bibr" target="#b22">[22]</ref>. The definition of those methods can be seen in the introduction. The comparison results are shown in Table <ref type="table" target="#tab_8">8</ref>.</p><p>The overall accuracy of our CNN method achieved 94.94% as shown in Table <ref type="table" target="#tab_8">8</ref>. It provides at least 5 percentage points higher than state-of-the-art approaches. For example, the PCA + kSVM <ref type="bibr" target="#b37">[37]</ref> only obtains an accuracy of 88.20%, the PCA + FSCABC <ref type="bibr" target="#b14">[14]</ref> yielded an accuracy of 89.11%, WE + BBO <ref type="bibr" target="#b36">[36]</ref> yielded an accuracy of 89.47%, FRFE + BPNN <ref type="bibr" target="#b21">[21]</ref> yielded an  PCA + kSVM <ref type="bibr" target="#b37">[37]</ref> 88.20% PCA + FSCABC <ref type="bibr" target="#b14">[14]</ref> 89.11% WE + BBO <ref type="bibr" target="#b36">[36]</ref> 89.47% FRFE + BPNN <ref type="bibr" target="#b21">[21]</ref> 88.99% FRFE + IHGA <ref type="bibr" target="#b22">[22]</ref> 89.59% 13-layer CNN (Our) 94.94% accuracy of 88.99%, and FRFE + IHGA <ref type="bibr" target="#b22">[22]</ref> yielded an accuracy of 89.59%. This superiority performance of CNN to traditional classifiers, again demonstrate the powerfulness of CNN.</p><p>The dataset was obtained by digital camera. A potential future direction is to use magnetic resonance imaging (MRI) <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b34">34]</ref> to scan the fruit and obtain 3D volumetric image of fruits for identification. Another study is to use similarity measure <ref type="bibr" target="#b32">[32]</ref> to help identify fruit categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Result on imperfect images</head><p>In this experiment, we tested our method on the imperfect images. We collected 173 fruit images with realistic complicated background, 136 fruit images with decay, 145 fruit images with camera not well focused, 161 fruit images with partially occluded by other stuffs. Those bad samples are shown in Fig. <ref type="figure" target="#fig_10">11</ref>.</p><p>The overall accuracy using our method over these images are listed in Table <ref type="table" target="#tab_9">9</ref>. We can observe that the overall accuracy over background fruit images is 89.60%, over decay images is 94.12%, over unfocused images is 91.03%, and over occlusion image is 92.55%. We found that the overall accuracy over decay images is nearly the same as over our clean dataset. For the fruit image with complicated background, the performance deteriorates. In the future, we shall include those imperfect images to our dataset, so our trained CNN classifier can be generalized to identify them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Time analysis</head><p>In this experiment, we compared the GPU-based computation with CPU-based computation. The CPU is Intel Core i5-3470 with frequency of 3.20GHz. We run the algorithm ten times, and calculate the average value. The computation time over training and test results are listed in Table <ref type="table" target="#tab_10">10</ref>.</p><p>For the training stage, the CPU costs 1,492,377.04 s, i.e., 414.55 h, while GPU costs 8415.75 s. The GPU reaches a 177× acceleration compared to CPU. Meanwhile, for the test stage, the CPU costs 92.48 s while GPU costs 0.53 s. Hence, GPU yields a 175× acceleration compared to CPU.    <ref type="table" target="#tab_0">11</ref> and Fig. <ref type="figure">12</ref>.</p><p>From Table <ref type="table" target="#tab_0">11</ref> and Fig. <ref type="figure">12</ref>, we can observe that using data augmentation can significantly increase the classification performance in terms of overall accuracy, especially on imperfect images. The reason is the augmented image can help train the CNN to resist the attack of imperfection. Again, the fruit image with complicated background has the lowest overall accuracy of 84.39% when not using data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this study, our team developed a fruit classification method based on a 13-layer deep convolutional neural network. The experiments show that our CNN reached an overall accuracy of 94.94%, superior to five state-of-the-art approaches in terms of overall accuracy. Besides, the data augmentation expands the training data from 1800 to 63,000. The max pooling techniques performs slightly better than average pooling. We also validated the optimal structure of proposed CNN. We tested our classifier on imperfect images, and its overall accuracy decreased at most 5%. The time analysis showed GPU can achieve a 177× acceleration on training data, and a 175× acceleration on test data. We validated the effect of using data augmentation.</p><p>The shortcomings of our method are three-folds: (i) Our dataset is clean and hence it does not perform well on imperfect images. (ii) We validated the optimal combined layers of convolution layer and pooling layer, but we did not test the optimal number of fully connected layers. (iii) The computation can be accelerated if using FPGA.</p><p>In the future, we shall try to use realistic images obtained in supermarkets and factories. Besides, we shall test other advanced classification ideas, such as transfer learning, and use FGPA to accelerate the algorithm implementation. Our method may be combined with recommendation system <ref type="bibr" target="#b43">[43]</ref>, mobile computing <ref type="bibr" target="#b41">[41]</ref>, and big data <ref type="bibr" target="#b42">[42]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) θ = -15 (b) θ = -10 (c) θ = -5 (d) θ = 5 (e) θ = 10 (f) θ = 15</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 Generated images by image rotation</figDesc><graphic coords="6,46.77,538.10,345.85,54.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 Fig. 4 Fig. 5</head><label>345</label><figDesc>Fig. 4 Generated images by Gamma correction</figDesc><graphic coords="7,46.77,53.09,345.85,54.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 Fig. 6 Fig. 7</head><label>267</label><figDesc>Fig. 6 Generated images by scale transform</figDesc><graphic coords="8,46.77,53.09,345.85,175.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figures 3,<ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">and 7</ref> present the generated images by image rotation, gamma correction, noise injection, scale transform, and affine transform, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8</head><label>8</label><figDesc>Fig. 8 Training performance of CNN method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9</head><label>9</label><figDesc>Fig. 9 Confusion matrix over test set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10</head><label>10</label><figDesc>Fig. 10 Optimal number of convolution layers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11</head><label>11</label><figDesc>Fig. 11 Imperfect samples of fruit images</figDesc><graphic coords="13,46.77,53.09,345.85,76.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,46.77,400.71,346.48,202.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Original datasetSample size of all classes</figDesc><table><row><cell>Training Set</cell><cell>1800</cell></row><row><cell>Test Set</cell><cell>1800</cell></row><row><cell>Total</cell><cell>3600</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Our CNN structure    </figDesc><table><row><cell cols="2">Layer Purpose</cell><cell>Filter No. of</cell><cell>Stride Padding Weights</cell><cell>Bias</cell><cell>Activation</cell></row><row><cell></cell><cell></cell><cell>filters</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>Image Input</cell><cell></cell><cell></cell><cell></cell><cell>256 × 256 × 3</cell></row><row><cell></cell><cell>layer</cell><cell></cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>Convolution +</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>ReLU</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>CNN Training platform    </figDesc><table><row><cell>Hardware</cell><cell>NVIDIA GeForce GTX 1050</cell></row><row><cell></cell><cell>Compute capability = 6.1</cell></row><row><cell></cell><cell>Clock rate = 1455 MHz</cell></row><row><cell></cell><cell>Multiprocessors = 5</cell></row><row><cell></cell><cell>Warp size = 32</cell></row><row><cell></cell><cell>Registers per block = 65,536</cell></row><row><cell></cell><cell>Threads per block = 1024</cell></row><row><cell>Software</cell><cell>Stochastic gradient descent with momentum</cell></row><row><cell></cell><cell>minibatch size = 128</cell></row><row><cell></cell><cell>Initial learning rate = 0.01</cell></row><row><cell></cell><cell>Drop period = 10</cell></row><row><cell></cell><cell>Drop rate factor = 0.1</cell></row><row><cell></cell><cell>Momentum = 0.9</cell></row><row><cell></cell><cell>Maximum epoch = 30</cell></row><row><cell></cell><cell>Loss function = cross entropy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Data augmentation of</figDesc><table><row><cell>original dataset</cell><cell>Sample size of new images</cell><cell>Sample size of each class</cell></row><row><cell>Data Augmented Training Set</cell><cell>63,000</cell><cell>3500</cell></row><row><cell>Original Training Set</cell><cell>1800</cell><cell>100</cell></row><row><cell>Image Rotation</cell><cell>10,800</cell><cell>600</cell></row><row><cell>Gamma Correction</cell><cell>10,800</cell><cell>600</cell></row><row><cell>Noise Injection</cell><cell>10,800</cell><cell>600</cell></row><row><cell>Scale Transform</cell><cell>14,400</cell><cell>800</cell></row><row><cell>Affine Transform</cell><cell>14,400</cell><cell>800</cell></row><row><cell>Test Set</cell><cell>1800</cell><cell>100</cell></row><row><cell>Total</cell><cell>64,800</cell><cell>3600</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Performance of each class</figDesc><table><row><cell>Class</cell><cell>Sensitivity</cell><cell>Specificity</cell><cell>Precision</cell><cell>Accuracy</cell><cell>Class number</cell></row><row><cell>Anjou pear</cell><cell>91.3%</cell><cell>99.3%</cell><cell>88.7%</cell><cell>98.8%</cell><cell>103</cell></row><row><cell>Blackberry</cell><cell>100.0%</cell><cell>99.4%</cell><cell>90.3%</cell><cell>99.4%</cell><cell>102</cell></row><row><cell>Black grape</cell><cell>90.1%</cell><cell>99.5%</cell><cell>91.0%</cell><cell>98.9%</cell><cell>101</cell></row><row><cell>Blueberry</cell><cell>98.9%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>99.9%</cell><cell>93</cell></row><row><cell>Bosc pear</cell><cell>96.9%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>99.8%</cell><cell>96</cell></row><row><cell>Cantaloupe</cell><cell>91.6%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>99.6%</cell><cell>95</cell></row><row><cell>Golden pineapple</cell><cell>95.5%</cell><cell>99.0%</cell><cell>86.1%</cell><cell>98.8%</cell><cell>110</cell></row><row><cell>Granny Smith apple</cell><cell>84.6%</cell><cell>99.4%</cell><cell>89.8%</cell><cell>98.6%</cell><cell>104</cell></row><row><cell>Green grape</cell><cell>97.9%</cell><cell>99.8%</cell><cell>95.9%</cell><cell>99.7%</cell><cell>95</cell></row><row><cell>Green plantain</cell><cell>98.1%</cell><cell>99.9%</cell><cell>98.1%</cell><cell>99.8%</cell><cell>106</cell></row><row><cell>Hass avocado</cell><cell>98.2%</cell><cell>99.2%</cell><cell>88.8%</cell><cell>99.1%</cell><cell>113</cell></row><row><cell>Passion fruit</cell><cell>88.1%</cell><cell>99.8%</cell><cell>96.1%</cell><cell>99.3%</cell><cell>84</cell></row><row><cell>red grape</cell><cell>94.8%</cell><cell>99.9%</cell><cell>98.9%</cell><cell>99.7%</cell><cell>97</cell></row><row><cell>Rome apple</cell><cell>98.9%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>99.9%</cell><cell>88</cell></row><row><cell>Strawberry</cell><cell>100.0%</cell><cell>99.7%</cell><cell>95.4%</cell><cell>99.7%</cell><cell>104</cell></row><row><cell>Tangerine</cell><cell>99.0%</cell><cell>99.9%</cell><cell>98.0%</cell><cell>99.8%</cell><cell>101</cell></row><row><cell>Watermelon</cell><cell>90.4%</cell><cell>99.9%</cell><cell>99.0%</cell><cell>99.3%</cell><cell>114</cell></row><row><cell>Yellow banana</cell><cell>94.7%</cell><cell>100.0%</cell><cell>100.0%</cell><cell>99.7%</cell><cell>94</cell></row><row><cell>Average</cell><cell>94.94%</cell><cell>99.71%</cell><cell>95.34%</cell><cell>99.43%</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>Pooling technique</figDesc><table><row><cell>comparison</cell><cell>Pooling</cell><cell>Overall accuracy</cell></row><row><cell></cell><cell>Max-pooling</cell><cell>94.94%</cell></row><row><cell></cell><cell>Average-pooling</cell><cell>94.83%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>CNN structure with different number of</cell><cell>Number of combined layers</cell><cell>Overall accuracy</cell></row><row><cell>convolution layers</cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>90.17%</cell></row><row><cell></cell><cell>3</cell><cell>94.06%</cell></row><row><cell></cell><cell>4 (Proposed)</cell><cell>94.94%</cell></row><row><cell></cell><cell>5</cell><cell>92.33%</cell></row><row><cell></cell><cell>6</cell><cell>92.00%</cell></row><row><cell></cell><cell>7</cell><cell>92.89%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8</head><label>8</label><figDesc>Comparison to state-of-</figDesc><table><row><cell>the-art approaches</cell><cell>Approach</cell><cell>Overall accuracy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9</head><label>9</label><figDesc>Performance of ourIn this experiment, we checked the effect data augmentation. Suppose Bno augmentation (NA)^represents the training set contains only 1800 original image, and Bdata augmentation (DA)^represents the training set contains the 63,000 fruit images as presented in Table4. The overall accuracies on different test set are shown below in Table</figDesc><table><row><cell>method over faulty image</cell><cell>Faulty type</cell><cell>Overall accuracy</cell></row><row><cell></cell><cell>Background</cell><cell>89.60%</cell></row><row><cell></cell><cell>Decay</cell><cell>94.12%</cell></row><row><cell></cell><cell>Unfocused</cell><cell>91.03%</cell></row><row><cell></cell><cell>Occlusion</cell><cell>92.55%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 Time</head><label>10</label><figDesc></figDesc><table><row><cell>(Unit: second)</cell><cell>analysis</cell><cell>Training</cell><cell>Time (63,000 data augmented samples)</cell></row><row><cell></cell><cell></cell><cell>CPU</cell><cell>1,492,377.04</cell></row><row><cell></cell><cell></cell><cell>GPU</cell><cell>8415.75</cell></row><row><cell></cell><cell></cell><cell>Acceleration</cell><cell>177</cell></row><row><cell></cell><cell></cell><cell>Test</cell><cell>Time (1800 samples)</cell></row><row><cell></cell><cell></cell><cell>CPU</cell><cell>92.48</cell></row><row><cell></cell><cell></cell><cell>GPU</cell><cell>0.53</cell></row><row><cell></cell><cell></cell><cell>Acceleration</cell><cell>175</cell></row></table><note><p>(DA Data Augmentation, NA No Augmentation)</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This study was supported by Natural Science Foundation of China (61602250), Natural Science Foundation of Jiangsu Province (BK20150983), Open fund of Key Laboratory of Guangxi High Schools Complex System and Computational Intelligence (2016CSCI01).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compliance with ethical standards</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest</head><p>We have no conflicts of interest to disclose with regard to the subject matter of this paper.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for vibrational spectroscopic data analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Acquarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Van Laarhoven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gerretzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Anal Chim Acta</title>
		<imprint>
			<biblScope unit="volume">954</biblScope>
			<biblScope unit="page" from="22" to="31" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classification of E-Nose Aroma Data of Four Fruit Types by ABC-Based Neural Network</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Adak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yumusak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient object-based surveillance image search using spatial pooling of convolutional features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mehmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Baik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Vis Commun Image Represent</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="62" to="76" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Text/non-text image classification in the wild with convolutional neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="437" to="446" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Voxelwise detection of cerebral microbleed in CADASIL patients by leaky rectified linear unit and early stopping: a class-imbalanced susceptibility-weighted imaging data study</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11042-017-4383-9</idno>
		<ptr target="https://doi.org/10.1007/s11042-017-4383-9" />
	</analytic>
	<monogr>
		<title level="j">Multime Tools Appl</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Training and Validating a Deep Convolutional Neural Network for Computer-Aided Detection and Classification of Abnormalities on Frontal Chest Radiographs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cicero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bilbily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dowdell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Investig Radiol</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="281" to="287" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic ear detection and feature extraction using Geometric Morphometrics and convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cintas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Quinto-Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Acuna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Biometrics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="223" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An adaptive over-split and merge algorithm for page segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai-Ton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duc-Hieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn Lett</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="137" to="143" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Effectiveness of pricing strategies on french fries and fruit purchases among university students: results from an on-campus restaurant experiment</title>
		<author>
			<persName><forename type="first">T</forename><surname>Deliens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Deforche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Annemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><surname>Article</surname></persName>
		</author>
		<idno>ID: e0165298</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bioprocessing technology to exploit organic palm date (Phoenix dactylifera L. cultivar Siwi) fruit as a functional dietary supplement</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Cagno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Filannino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cavoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Funct Foods</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="9" to="19" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fruit classification by extracting color chromaticity, shape and texture features: towards an application for supermarkets</title>
		<author>
			<persName><forename type="first">F</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Lat Am Trans</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3434" to="3443" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Analysis of airflow and heat transfer inside fruit packed refrigerated shipping container: Part I -model development and validation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Getahun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ambaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Delele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Food Eng</title>
		<imprint>
			<biblScope unit="volume">203</biblScope>
			<biblScope unit="page" from="58" to="68" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Plant identification using deep neural networks via optimization of transfer learning parameters</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yanikoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Aptoula</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">235</biblScope>
			<biblScope unit="page" from="228" to="235" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fruit classification using computer vision and feedforward neural network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Food Eng</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="167" to="177" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Study of the Effect of Noise Injection on the Training of Artificial Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Zur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Pesce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting><address><addrLine>Atlanta</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2784" to="2788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Convolutional neural network-based human detection in nighttime images using visible light camera sensors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.3390/s17051065</idno>
		<ptr target="https://doi.org/10.3390/s17051065" />
	</analytic>
	<monogr>
		<title level="j">Sensors (Basel)</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discriminating solitary cysts from soft tissue lesions in mammography using a pretrained deep convolutional neural network</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Karssemeijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med Phys</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1017" to="1027" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep unfolding inference for supervised topic model</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech And Signal Processing Proceedings</title>
		<meeting><address><addrLine>Shanghai</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2279" to="2283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint multiple fully connected convolutional neural network with extreme learning machine for hepatocellular carcinoma nuclei grading</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Biol Med</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="156" to="167" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Summary on fruit identification methods: A literature review</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Snetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv Soc Sci Educ Hum Res</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1629" to="1633" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fractional Fourier entropy increases the recognition rate of fruit type detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<idno>ID: 10</idno>
	</analytic>
	<monogr>
		<title level="j">BMC Plant Biol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">S2</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A fruit sensing and classification system by fractional fourier entropy and improved hybrid genetic algorithm</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Industrial Application Engineering (IIAE)</title>
		<meeting><address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Kitakyushu, Institute of Industrial Applications Engineers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="293" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Classification of teeth in cone-beam CT using deep convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Miki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Muramatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Biol Med</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="24" to="29" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cross entropy based thresholding for magnetic resonance brain images using Crow Search Algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hinojosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cuevas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="164" to="180" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Characterization, classification and authentication of fruitbased extracts by means of HPLC-UV chromatographic fingerprints, polyphenolic profiles and chemometric methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pardo-Mates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barbosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Food Chem</title>
		<imprint>
			<biblScope unit="volume">221</biblScope>
			<biblScope unit="page" from="29" to="38" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Traffic sign recognition with convolutional neural network based on max pooling positions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Coenen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2th International Conference on Natural Computation, Fuzzy Systems And Knowledge Discovery (ICNC-FSKD)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="578" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Electronic nose based on partition column integrated with gas sensor for fruit identification and classification</title>
		<author>
			<persName><forename type="first">Ciptohadijoyo</forename><forename type="middle">S</forename><surname>Radi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Litananda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Electron Agric</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="429" to="435" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rapid classification of Chinese quince (Chaenomeles speciosa Nakai) fruit provenance by near-infrared spectroscopy and multivariate calibration</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Diao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Anal Bioanal Chem</title>
		<imprint>
			<biblScope unit="volume">409</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="120" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Choroid segmentation from Optical Coherence Tomography with graph edge weights learned from deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">D</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Z</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="page" from="332" to="341" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A snapshot of image pre-processing for convolutional neural networks: case study of MNIST</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tabik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Peralta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herrera-Poyatos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Comput Intellig Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="555" to="568" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Brain early infarct detection using gamma correction extreme-level eliminating with weighting distribution</title>
		<author>
			<persName><forename type="first">V</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scanning</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="842" to="856" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Content-based image quality metric using similarity measure of moment vectors</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Thung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Paramesran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2193" to="2204" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neurodegenerative disease diagnosis using incomplete multimodality data via matrix shrinkage and completion</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Thung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="386" to="400" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Identification of progressive mild cognitive impairment patients using incomplete longitudinal MRI scans</title>
		<author>
			<persName><surname>Kh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Yap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Struct Funct</title>
		<imprint>
			<biblScope unit="volume">221</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3979" to="3995" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fuzzy systems: case study classification of fruit Mc Stipitata Vaug (Araza)</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Tovar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Losada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Amazonia Investiga</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="45" to="56" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fruit classification by wavelet-entropy and feedforward neural network trained by fitnessscaled chaotic ABC and biogeography-based optimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="5711" to="5728" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Classification of fruits using computer vision and a multiclass support vector machine</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="12489" to="12505" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fruit classification by biogeography-based optimization and feedforward neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="253" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Comparison of regularization methods for imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Smirnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Timoshenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Andrianov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Aasri Conference on Computational Intelligence And Bioinformatics (CIB)</title>
		<meeting><address><addrLine>South Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier Science Bv</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="89" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Resource allocation in multi-class dynamic PERT networks with finite capacity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yaghoubi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Noori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Azaron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur J Oper Res</title>
		<imprint>
			<biblScope unit="volume">247</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="879" to="894" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">GroRec: a group-centric intelligent recommender system integrating social, mobile and big data technologies</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Serv Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="786" to="795" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Health-CPS: healthcare cyber-physical system assisted by cloud and big data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Syst J PP</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">iDoctor: Personalized and professionalized medical recommendations based on hybrid matrix factorization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Futur Gener Comput Syst</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="30" to="35" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visual tracking using max-average pooling and weight-selection strategy</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Appl Math</title>
		<imprint>
			<biblScope unit="page">828907</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">He worked as a postdoc from 2010 to 2012, and a research scientist from 2012 to 2013 at Columbia University. From 2013 to 2016 he worked a professor in Nanjing Normal University. He is now a professor in Henan Polytechnic University. He served as IEEE senior member and ACM senior member. He is included in BMost Cited Chinese researchers (Computer Science)^from 2015 to 2017. He won the BEmerald Citation of Excellence 2017^. He is elected as the BBentham Ambassador^</title>
		<author>
			<persName><forename type="first">Yu-Dong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimed Tools Appl Zhengchao Dong was a tenured-track Associate Professor in Division of Translational Imaging</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>Columbia University, USA and New York State Psychiatry Institute</orgName>
		</respStmt>
	</monogr>
	<note>Progress in Nuclear Magnetic Resonance Spectroscopy</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Xianqing Chen got his Ph.D. degree from Southeast University in 2013. Now he is a senior lecturer in Zhejiang Normal University. His research interest is image processing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Wen-Juan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename></persName>
		</author>
		<title level="m">Yancheng Teachers University from the department of Mathematics &amp; Statistics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>Nanjing Normal University</orgName>
		</respStmt>
	</monogr>
	<note>Now she is pursuing the M.S. in School of Computer Science &amp; Technology. Her research interest is expert system</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">She is currently a professor in the school of Electronic Science and Engineering, Nanjing University. Her research interests include in digital imaging processing and computer vision</title>
	</analytic>
	<monogr>
		<title level="m">Sidan Du received the Ph.D. degree in physics from Nanjing University</title>
		<meeting><address><addrLine>Nanjing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Khan Muhammad received his BS degree in computer science from Islamia College, Peshawar, Pakistan with research in information security. Currently, he is pursuing MS leading to PhD degree in digitals contents from College of Software Convergence</title>
		<imprint>
			<pubPlace>Seoul</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Sejong University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include image and video processing, wireless networks, information security, data hiding, image and video steganography, video summarization, diagnostic hysteroscopy. wireless capsule endoscopy, and CCTV video analysis</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m">At present, she works as an Associate Professor. She published over 100 papers in SCI-indexed journals. She served as the editor of Journal of Alzheimer&apos;s disease from 2018, and the managing guest editor of Multimedia Tools and Applications</title>
		<imprint>
			<publisher>Shui-Hua Wang received a B.S. from Southeast University</publisher>
			<date type="published" when="2005">2005-2008. 2010-2012. 2012-2014. 2014-2017. 2017-2018</date>
		</imprint>
		<respStmt>
			<orgName>M.S. from The City University of New York</orgName>
		</respStmt>
	</monogr>
	<note>She worked as a Research Assistant in Columbia University</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
