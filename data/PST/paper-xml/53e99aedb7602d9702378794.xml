<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hybridization of evolutionary Levenberg-Marquardt neural networks and data pre-processing for stock market prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2012-05-18">18 May 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shahrokh</forename><surname>Asadi</surname></persName>
							<email>s.asadi@aut.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Industrial Engineering</orgName>
								<orgName type="institution">Amirkabir University of Technology (Polytechnic of Tehran)</orgName>
								<address>
									<postBox>P.O. Box</postBox>
									<postCode>15875-4413</postCode>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Esmaeil</forename><surname>Hadavandi</surname></persName>
							<email>es.hadavandi@aut.ac.ir</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Industrial Engineering</orgName>
								<orgName type="institution">Amirkabir University of Technology (Polytechnic of Tehran)</orgName>
								<address>
									<postBox>P.O. Box</postBox>
									<postCode>15875-4413</postCode>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Farhad</forename><surname>Mehmanpazir</surname></persName>
							<email>farhad@azad.ac.ir</email>
							<affiliation key="aff1">
								<orgName type="department">School of Industrial Engineering</orgName>
								<orgName type="institution">Islamic Azad University (South Branch of Tehran)</orgName>
								<address>
									<postBox>P.O. Box 11365-9466</postBox>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><forename type="middle">Masoud</forename><surname>Nakhostin</surname></persName>
							<email>masoud.nakhostin@yahoo.com</email>
							<affiliation key="aff1">
								<orgName type="department">School of Industrial Engineering</orgName>
								<orgName type="institution">Islamic Azad University (South Branch of Tehran)</orgName>
								<address>
									<postBox>P.O. Box 11365-9466</postBox>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hybridization of evolutionary Levenberg-Marquardt neural networks and data pre-processing for stock market prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2012-05-18">18 May 2012</date>
						</imprint>
					</monogr>
					<idno type="MD5">BBAF714020102CF67BA17A40EAFC1709</idno>
					<idno type="DOI">10.1016/j.knosys.2012.05.003</idno>
					<note type="submission">Received 13 October 2010 Received in revised form 8 May 2012 Accepted 9 May 2012</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Stock price prediction Genetic algorithms Evolutionary Neural Networks Levenberg-Marquardt algorithm Data pre-processing</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Artificial Intelligence models (AI) which computerize human reasoning has found a challenging test bed for various paradigms in many areas including financial time series prediction. Extensive researches have resulted in numerous financial applications using AI models. Since stock investment is a major investment activity, Lack of accurate information and comprehensive knowledge would result in some certain loss of investment. Hence, stock market prediction has always been a subject of interest for most investors and professional analysts. Stock market prediction is a challenging problem because uncertainties are always involved in the market movements. This paper proposes a hybrid intelligent model for stock exchange index prediction. The proposed model is a combination of data preprocessing methods, genetic algorithms and Levenberg-Marquardt (LM) algorithm for learning feed forward neural networks. Actually it evolves neural network initial weights for tuning with LM algorithm by using genetic algorithm. We also use data pre-processing methods such as data transformation and input variables selection for improving the accuracy of the model. The capability of the proposed method is tested by applying it for predicting some stock exchange indices used in the literature. The results show that the proposed approach is able to cope with the fluctuations of stock market values and also yields good prediction accuracy. So it can be used to model complex relationships between inputs and outputs or to find data patterns while performing financial prediction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction and literature review</head><p>Prediction is the process of making projection about future performance based on existing historical data. Accurate prediction aids in decision-making and planning for the future. Prediction empowers people to modify current variables to make a prediction of the future to result in favorable scenario. Selection and implementation of a proper prediction methodology has always been an important planning and control issue for firms and agencies. Financial stability of an organization depends on the accuracy of the predictions been made, since such information will most likely be used to make key decisions in important areas such as human resources, purchasing, marketing, advertising and capital financing. The complexity of the prediction process used for predicting a variable depends on various factors. Most importantly, the historical pattern of variable changes and the underlying input factors which affect a variable may increase the complexity of the prediction process.</p><p>There are two hypotheses to be considered while predicting stock price. First one is, The Efficient Market Hypothesis (EMH) stating that at any time, the price of a stock fully captures all known information about the stock. Since all known information is used optimally by market participants, price variations are random, as new information occurs randomly. Thus, stock prices perform a ''random walk'', meaning that all future prices do not follow any trends or patterns and are random departure from the previous prices and it is not possible for an investor to predict the market. This hypothesis interprets fluctuations as the result of delayed or incomplete information that influence stock market prices. External phenomena affect subsequent stock market prices. Accurate predictions are difficult because it is not possible to model, quantify, or even know a priori such external phenomena <ref type="bibr" target="#b53">[54]</ref>.</p><p>There has been a lot of debate about the validity of the EMH and random walk (RW) theory. However, with the advent of computational and intelligent finance, and behavioral finance, economists have tried to establish another hypothesis which may be collectively called as the Inefficient Market Hypothesis (IMH). IMH states that financial markets are at least not always efficient, the market is not always in a RW, and inefficiencies exists <ref type="bibr" target="#b46">[47]</ref>. Many of these studies used intelligent systems such as neural networks to justify their claims. The fact that many market participants can consistently beat the market is an indication that the EMH may not be true in practice <ref type="bibr" target="#b21">[22]</ref>.</p><p>Predicting a change in market prices and making correct decisions based on that, is one of the most important requirements for anyone who has something to do with economical environments. For many years, how to make a prediction of stock market has been a prevalent research topic. Financial gain might be considered as the most basic issue in stock market prediction. If a system is able to consistently pick winners and losers in the dynamic market place, then it will be able to provide the owner of the system with more wealth. Thus, many individuals including researchers, investment professionals, and average investors are continually looking for this superior system which will yield them high returns.</p><p>There are several different approaches to time series modeling. Traditional statistical models including moving average, exponential smoothing, and ARIMA are linear in that predictions of the future values. These models use piecewise linear function as basic element of prediction model <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b45">46]</ref>.The functional form for the problem has to be specified by the user. It could take a lot of time to experiment with different possible function relations and algorithms to obtain proper models. Moreover, many researchers claim that the stock market is a chaos system. Chaos is a nonlinear deterministic system which only appears random because of its irregular fluctuations. So in finance theory, non-linear models are very important. For example, almost all of real business cycle models are highly non-linear. Superiority of non-linear models in finance is not supposed to be conflicting with the use of linear models by a practitioner. Linear models can be considered as the reasonable approximations of the non-linear phenomenon of interest and non-linear models can be applied in prediction. It is not surprising that non-linear models, from regime-switching models to neural networks and genetic algorithms are receiving a great deal of attention in the literature <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>The RW model says that not only all historical information are summarized in the current value but also those increments-positive or negative-that are uncorrelated and balanced with an expected value equal to zero. In other words, the positive fluctuations are as many as negatives in long term. The random nature of this model makes it difficult to perform accurate predictions. Hence, prediction of the stock market seems to be a daunting task. Mainly, since stock time series have a close to random-walk behavior due to the fact that stock markets have numerous underlying factors which most of them are currently not fully understood, therefore a non-linear model can be beneficent. A large set of interacting input series are often required to explain a specific stock. If we are able to predict stock market time series more accurately, we can allocate society resources to the right place so that national resources will not be wasted. The Intelligent Systems are capable of learning such non-linear chaotic systems because they make very few assumptions about the functional form of the underlying dynamic dependencies and their initial conditions. This may eventually put a question on the traditional financial theory of efficient market. Intelligent Systems are those which learn from their past experiences and use this knowledge in current and future decision making <ref type="bibr" target="#b21">[22]</ref>.</p><p>Artificial Intelligence (AI) is one of the intelligent systems that computerizes human reasoning has found a challenging test bed for various paradigms in many areas including financial time series prediction. Extensive research has resulted in numerous prediction applications using artificial neural networks (ANN), fuzzy logic and genetic algorithms (GA) and other techniques <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b24">25]</ref>. Most progress to date in AI has been made in the areas of problem solving; concepts and methods for building programs that reason about problems rather than calculate a solution.</p><p>A number of studies have compared the capability of AI techniques over conventional techniques such as ARIMA, Regression etc. in prediction problems, and they have found that AI-based systems result in more accurate outputs than conventional approaches such as ARIMA and Regression <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b27">28]</ref>).</p><p>Nowadays, increasing number of efforts have been focused on AI models for stock market prediction, while using AI models or a combining of several models has become a common practice to improve prediction accuracy. Hence, the literature on this topic has expanded dramatically <ref type="bibr" target="#b0">[1]</ref>.</p><p>Hadavandi et al. <ref type="bibr" target="#b29">[30]</ref> presented an integrated approach based on genetic fuzzy systems (GFS) and artificial neural networks for constructing a stock price prediction expert system. They used stepwise regression analysis to determine factors which have most impact on stock prices, then divided the raw data into k clusters using self-organizing map (SOM) neural networks. Finally, all clusters fed into independent GFS models with the ability of rule base extraction and data base tuning. Results showed that their approach outperforms other methods such as ANN and ARIMA.</p><p>Chang and Liu <ref type="bibr" target="#b10">[11]</ref> used a Takagi-Sugeno-Kang (TSK) type Fuzzy Rule Based System (FRBS) for stock price prediction. They used simulated annealing (SA) for training the best parameters of fuzzy systems. They found that the forecasted results from TSK fuzzy rulebased model were much better than those of back propagation network (BPNN) or multiple regressions.</p><p>Esfahanipour and Aghamiri <ref type="bibr" target="#b20">[21]</ref> used Neuro-Fuzzy Inference System adopted on a TSK type Fuzzy Rule Based System for stock price prediction. The TSK fuzzy model applies the technical index as the input variables and uses Fuzzy C-Mean clustering for identifying number of rules. They tested the proposed model on Tehran Stock Exchange Indices (TEPIX) and Taiwan stock Exchange index (TSE) data (the same data set that used by Chang and Liu <ref type="bibr" target="#b10">[11]</ref>. Results showed that the proposed model can effectively improve the prediction performance and outperforms other models.</p><p>Shen et al. <ref type="bibr" target="#b52">[53]</ref> introduced the artificial fish swarm algorithm to optimize RBF. In this paper, to improve prediction efficiency, a Kmeans clustering algorithm is optimized by artificial fish swarm algorithm during the learning process of RBF. To confirm the usefulness of their algorithm, they compared the prediction results of RBF optimized by three methods: artificial fish swarm algorithm, genetic algorithms, and particle swarm optimization as well as prediction results of ARIMA, BP, and support vector machine. Results demonstrated that the proposed model performs better than other models.</p><p>De and Araújo <ref type="bibr" target="#b17">[18]</ref> introduced a class of hybrid morphological perceptrons, called dilation-erosion perceptron to overcome the RW dilemma in the time series forecasting problem. A gradient steepest descent method was presented to design the proposed dilation-erosion perceptron, using the back propagation (BP) algorithm and a systematic approach to overcome the problem of nondifferentiability of morphological operators.</p><p>Cho <ref type="bibr" target="#b14">[15]</ref> used a new architecture for financial information systems. The developed prototype was entitled as the Multi-level and Interactive Stock Market Investment System. It is specially designed for investors to build their financial models to forecast stock price and index.</p><p>Chang Chien and Chen <ref type="bibr" target="#b13">[14]</ref> focused on stock trading data with many numerical technical indicators, and the classification problem is finding sell and buy signals from the technical indicators. This study proposed a GA-based algorithm to build an associative classifier that can discover trading rules from these numerical indicators. The experiment results revealed that the proposed approach is an effective classification technique with high prediction accuracy. It is also is highly competitive when compared with the data distribution method.</p><p>ANNs is one of the strongest AI models which can learn the complex nature of the relationship between inputs and outputs.The technique is rooted in and inspired by the biological network of neurons in the human brain that learns from external experience, handles imprecise information, stores the essential characteristics of the external input, and generalizes previous experience.</p><p>Atsalakis and Valavanis <ref type="bibr" target="#b1">[2]</ref> pointed out about 60% of the surveyed articles in recent years, used feed forward neural networks (FFNN) and recurrent networks for stock market prediction.</p><p>When developing a feed forward neural network model for prediction purposes, specifying its architecture in terms of the number of input, hidden, and output neurons and weight training are important tasks. Weight training in ANNs is usually formulated as a minimization of an error function, such as the mean square error between target and actual outputs averaged over all training data by iteratively adjusting connection weights. Most training algorithms, such as back propagation and conjugate gradient are based on gradient descent <ref type="bibr" target="#b5">[6]</ref>.</p><p>Among the literatures regarding using the ANNs as the prediction tool, most of them focus on back-propagation Neural Network. BP is characterized by very poor convergence. Several improvements for BP, such as the quick-propagation (QP) algorithm, resilient error back propagation, etc. were developed. Much better results can be obtained using second order methods such as Newton or Levenberg-Marquardt (LM). The Levenberg-Marquardt back propagation (LMBP) is a powerful optimization technique was introduced to the neural net research because it provided methods to accelerate the training and convergence of the algorithm. It utilizes the BP procedures in which derivatives are processed from the last layer of the network to the first <ref type="bibr" target="#b34">[35]</ref>.</p><p>Yet, there are two shortcomings of LMBP: firstly, despite that LMBP has shown to be successful in some areas, the algorithm often gets trapped in a local minimum of the error function and is incapable of finding a global minimum if the error function is multimodal and/or non-differentiable <ref type="bibr" target="#b58">[59]</ref>, secondly, it has been found that it does not perform well with networks with more than two or three hidden layers <ref type="bibr" target="#b4">[5]</ref>. The mentioned issues beside other problems have guided the researches toward employing evolutionary techniques to find the best set of network weights.</p><p>Evolutionary techniques have several obvious advantages over BP: genetic algorithms and other evolutionary approaches are able to find global minima in complex, multimodal spaces, they do not require a differentiable error function and they are more flexible, allowing the fitness evaluation to be changed to take into account extra factors that are not easy to incorporate in the BP algorithm <ref type="bibr" target="#b40">[41]</ref>. Many researchers have used GA for learning of neural networks and they've found that GAs has better performance compared to BPs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>In spite of the good performance of Evolutionary Neural Networks (ENNs), unfortunately a few researches can be found which verify applying of ENNs for dealing with stock market prediction problems. Wittkemper and Steiner <ref type="bibr" target="#b56">[57]</ref> discussed different methods for predicting a stock's systematic risk. They showed that the most precise forecasts are given by neural networks, whose topology has been optimized by a genetic algorithm. Versace et al. <ref type="bibr" target="#b54">[55]</ref> used a genetic algorithm to find the best mixture of neural networks, the topology of neural networks, and to determine the features set. They found the application of genetic algorithm on neural network architectures shows promise for prediction of stock market time series.</p><p>Due to nature of LMBP to converge locally, it can be demonstrated that solutions are highly dependent upon the initial random draw of weights. If these initial weights are located on a local grade, which is probable, the BP algorithm will likely become trapped in a local solution that may or may not be the global solu-tion. This local convergence could present serious problems when using NNs for real-world applications. So using global search techniques is needed for Network Training. Considering the points mentioned, in this paper the powerful combination of positive aspects of genetic algorithms and LMBP algorithm is presented to predict stock market indices. Proposed method is a combination of genetic algorithms and LMBP artificial neural networks. Global search capabilities of genetic algorithms with the ability of LMBP algorithm in the local search are combined. At the beginning, genetic algorithm is used with abroad search to find weights of artificial neural network. After searching space becomes smaller, then these weights are used as initial weights for the LMBP algorithm. LMBP algorithm around a global search with a local search acquires the best possible or weights of network.</p><p>Data quality is a key issue in prediction concepts. To increase the accuracy of the prediction, we may perform data pre-processing techniques such as input selection and data transformation. Atsalakis and Valavanis <ref type="bibr" target="#b1">[2]</ref> pointed out that input data pre-processing may impact prediction performance. The process of choosing indicators as inputs through sensitivity analysis may help eliminate redundant inputs. In many cases input data has a large range of values reducing effectiveness of training procedures. One solution to this impediment is applying data transformation techniques such as data scaling.</p><p>This paper presents a hybrid intelligent model for stock exchange index prediction. The proposed model which is called PEL-MNN (pre-processed evolutionary LM neural networks) is a combination of genetic algorithms and LM neural networks equipped with pre-processing and also post-processing concepts. The capability of the proposed model has been tested by applying it to predict seven stock exchange indices used by Chang and Liu <ref type="bibr" target="#b10">[11]</ref>, Esfahanipour and Aghamiri <ref type="bibr" target="#b20">[21]</ref>, Ferreira et al. <ref type="bibr" target="#b22">[23]</ref> as a case study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methodology</head><p>A time series is a time dependent sequence of points, generally equidistant, such as: Y t = {y t e R|t = 1, 2, 3, . . . , M}, where t is the chronological or temporal index and M is the number of observations. Therefore, y t is a sequence of temporal observations equally spaced. The main purpose of time series prediction techniques is to estimate the future values (unknown) of the series based on the past observations either from the series itself or from exogenous data. This can be accomplished by identifying certain regular patterns presented in the historical data. In this context, a crucial factor for a good forecasting performance is the correct choice of the time lags considered for representing the series <ref type="bibr" target="#b21">[22]</ref>. A time series is considered as nonlinear function of several input variable as</p><formula xml:id="formula_0">y Ã t ¼ f ðx Ã 1 ; x Ã 2 ; . . . ; x Ã n Þ,</formula><p>Where f is a nonlinear function determined by the neural network, x Ã new is normalized value of x old (input variable) and n is integer.</p><p>This paper presents four-stage architecture to develop a hybrid intelligent model for stock exchange index time series prediction. The first stage is data pre-processing. In this stage, we apply a data transformation technique to scale data then we employ stepwise regression analysis (SRA) for variable selection to choose the key variables that are to be considered in the model. In the second stage, we employ genetic algorithm as a global search method to evolve artificial neural networks' initial weights by using the proposed ENN. In the third stage, we tune the obtained weights in previous stage using LMBP algorithm. In the last stage, we post-process the outcomes and generate predicted values. General framework of PELMNN is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Each stage is detailed in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Data pre-processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Data transformation</head><p>Using transformed data is more useful in most heuristic methods especially when dealing with forecasting problems <ref type="bibr" target="#b3">[4]</ref>. A preprocessing method should contain the capability of transforming pre-processed data into its original scale (called post-processing). One of the most useful data transformation techniques is data normalization which is used in different forecasting study, for example; Azadeh et al. <ref type="bibr" target="#b2">[3]</ref>, Oliveira <ref type="bibr" target="#b44">[45]</ref>, and Niska et al. <ref type="bibr" target="#b28">[29]</ref> applied this method as a part of their approach to estimate time series functions using heuristic approaches and they obtained quite promising results.</p><p>There are different normalization algorithms, such as Min-Max normalization, Z-score normalization and sigmoid normalization. In this paper we use Min-Max normalization which is a common approach in this field. The Min-Max normalization scales the numbers in a data set to improve the accuracy of the subsequent numeric computations. If X old , X max , X min are the original, maximum and minimum values of the raw data, respectively and X Ã max ; X Ã min , are the maximum and minimum of the normalized data, respectively, then the normalization of X old called X Ã new , can be obtained by the following transformation function:</p><formula xml:id="formula_1">X Ã new ¼ X old À X min X max À X min ðX Ã max À X Ã min Þ þ X Ã min<label>ð1Þ</label></formula><formula xml:id="formula_2">2.1.2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Variable selection by stepwise regression analysis</head><p>Variable selection is the problem of selecting input variables that are most predictive of a given output. It identifies a small subset of variables so that the prediction model constructed with the selected variables minimizes the error and the selected variables also better explain the data. The variable selection gives a better generalization error. Typically, only a small number of variables of X give sufficient information for the prediction. The variable selection problem finds the small subset of variables that are relevant to the target concept. A small subset of relevant variables gives more discriminating power than using more variables. This is counter-intuitive since more variables give more information and therefore should give more discriminating power. But if a variable is irrelevant, then that variable does not affect the target concept. If a variable is redundant, then that variable does not add anything new to the target concept <ref type="bibr" target="#b41">[42]</ref>.</p><p>In recent years some researcher have used SRA for input variable selection in the field of stock market prediction and they've obtained very promising results <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30]</ref>. So, in this paper we adopt stepwise regression to analyze and select variables, and as a result improve the prediction accuracy of the system. Stepwise regression method determines a set of independent factors that most closely determine the dependent variable. This task is carried out using the repetition of a variable selection. At each of these steps, a single variable is either entered or removed from the model. For each step, simple regression is performed using the previously included independent variables and one of the excluded variables.</p><p>To illustrate the procedure <ref type="bibr" target="#b39">[40]</ref>, assume that we have K candidate variables x 1 , x 2 , . . . , x k and a single response variable y. In classification the candidate variables correspond to the polynomialexpanded elements of the feature vectors and the response variable corresponds to the class label. Note that with the intercept term b 0 we end up with K + 1 variables. In the procedure the polynomial weights (or the regression model) are iteratively found by adding or removing variables at each step. The procedure starts by building a one variable regression model using the variable that has the highest correlation with the response variable y. This variable will also generate the largest partial F-statistic. In the second step, the remaining K À 1 variables are examined. The variable that generates the maximum partial F-statistic is added to the model provided that the partial F-statistic is larger than the value of the F-random variable for adding a variable to the model, such an Frandom variable is referred to as f in . Formally the partial F-statistic for the second variable is computed by:</p><formula xml:id="formula_3">f 2 ¼ SS R ðb 2 jb 1 ; b 0 Þ MS E ðx 2 ; x 1 Þ<label>ð2Þ</label></formula><p>where MSE (x 2 , x 1 ) denotes the mean square error for the model containing both x 1 and x 2 . SS R (b 2 |b 1 , b 0 ) is the regression sum of squares due to b 2 given that b 1 , b 0 are already in the model. In general the partial F-statistic for variable j is computed by:</p><formula xml:id="formula_4">f j ¼ SS R ðb j jb 0 ; b 1 ; . . . ; b jÀ1 ; b jþ1 ; . . . ; b k Þ MS E<label>ð3Þ</label></formula><p>If variable x 2 is added to the model then the procedure determines whether the variable x 1 should be removed. This is determined by computing the F-statistic.</p><formula xml:id="formula_5">f 1 ¼ SS R ðb 2 jb 1 ; b 0 Þ MS E ðx 2 ; x 2 Þ<label>ð4Þ</label></formula><p>If f 1 is less than the value of the F-random variable for removing variables from the model, such an F-random variable is referred to as f out . The procedure examines the remaining variables and stops when no other variable can be added or removed from the model. It is also worth mentioning that one cannot arrive to the conclusion that all of the regressors that are important for predicting the response variable have been retained in the stepwise procedure. This is so because such a procedure retains regressors based on the use of sample estimates of t he true model weights. It is understood that there is a probability of making errors in retaining regressors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Artificial neural networks</head><p>ANNs are flexible computing frameworks for modeling a broad range of nonlinear problems. One significant advantage of the ANN models over other classes of nonlinear model is that ANNs are universal approximators which can approximate a large class of functions with a high degree of accuracy. Their power comes from the parallel processing of the information from the data. No prior assumption of the model form is required in the model building process. Instead, the network model is largely determined by the characteristics of the data.</p><p>ANNs consists of an inter-connection of a number of neurons. There are many varieties of connections under study; however, here we will discuss only one type of network which is called multilayer perceptron (MLP). In this network the data flows forward to the output continuously without any feedback. We have used a typical four-layer feed forward model for prediction stock exchange. The input nodes are the technical indices, while the output provides the prediction for the stock exchange index. Hidden nodes with appropriate nonlinear transfer functions are used to process the information received by the input nodes. The model can be written as Eq. ( <ref type="formula" target="#formula_6">5</ref>).</p><formula xml:id="formula_6">y t ¼ c 0 þ g X s k¼1 c k f X n j¼1 a kj f X m i¼1 b ij x i þ b 0j ! þ a 0 ! ! þ e t<label>ð5Þ</label></formula><p>where m is the number of input nodes, n is the number of hidden nodes in 1st hidden layer and s is the number of hidden nodes in 2st hidden layer, f is a sigmoid transfer function such as the logistic, f ðxÞ ¼ 1 1þexpðÀxÞ , and g is hyperbolic tangent (tanh), gðxÞ ¼ expðxÞÀexpðÀxÞ expðxÞþexpðÀxÞ ; fc k ; k ¼ 0; 1; . . . ; sg; fa j ; j ¼ 0; 1; . . . ; ng is a vector of weights from the 2nd hidden layer to output nodes and 1st hidden layer to 2nd hidden layer nodes respectively. {b ij , i = 1, 2, . . . , m; j = 0, 1, . . . , n} are weights from the input to 1st hidden layer nodes, c 0 , a 0 , b 0j , are weights of arcs leading from the bias terms that have values always equal to 1.</p><p>The attraction of MLP has been explained by the ability of the network to learn complex relationships between input and output patterns, which would be difficult to model with conventional algorithmic methods. The disadvantage of ANN is that because the network finds out how to solve the problem by itself, its operation can be unpredictable. In this paper the effort is made to identify the best-fitted network for the desired model according to the characteristics of the problem and ANN features. In Fig. <ref type="figure" target="#fig_2">2</ref>, a MLP neural network in shown.</p><p>Although many different approaches exist in order to find the optimal architecture of an ANN, these methods are usually quite complex in nature and are difficult to implement <ref type="bibr" target="#b59">[60]</ref>. Furthermore, none of these methods can guarantee the optimal solution for all real prediction problems. To date, there is no simple clearcut method for determination of these parameters and the usual procedure is to test numerous networks with varying numbers of input and hidden units (p, q), estimate generalization error for each and select the network with the lowest generalization error <ref type="bibr" target="#b35">[36]</ref>. The parameters are estimated such that the cost function of neural network is minimized. Cost function is an overall accuracy criterion such as the following mean squared error:</p><formula xml:id="formula_7">E ¼ 1 N X N n¼1 ðe i Þ 2 ¼ 1 N X N n¼1 y t À w 0 þ X Q j¼1 w jg w 0j þ X Q j¼1 w i;j y tÀj ! ! ! 2<label>ð6Þ</label></formula><p>where N is the number of error terms. This minimization is done with some efficient nonlinear optimization algorithms other than the basic backpropagation training algorithm <ref type="bibr" target="#b48">[49]</ref>, in which the parameters of the neural network, W are changed by an amount oW, according to the following formula:</p><formula xml:id="formula_8">DW ¼ Àg @E @W<label>ð7Þ</label></formula><p>where the parameter g is the learning rate and @E @W is the partial derivative of the function E with respect to the weights matrix W. This derivative is commonly computed in two passes. In the forward pass, an input vector from the training set is applied to the input units of the network and is propagated through the network, layer by layer, producing the final output. During the backward pass, the output of the network is compared with the desired output and the resulting error is then propagated backward through the network, adjusting the weights accordingly.</p><p>The Levenberg-Marquardt algorithm is the most widely used optimization algorithm. It outperforms simple gradient descent and other conjugate gradient methods in a wide variety of problems. LM algorithm is a second order algorithm that many times is overlooked by those attempting to train neural networks possibly because it is more complex to implement than EBP. However, it definitely makes up for this in superior performance. LM is similar to EBP in that it requires the calculation of the gradient vector, but in addition, LM also computes the Jacobian. The gradient vector is represented as:</p><formula xml:id="formula_9">g ¼ @E @W 1 @E @W 2 . . . @E @Wn 0 B B B B B @ 1 C C C C C A<label>ð8Þ</label></formula><p>where E is there error of the network for that pattern and W refers to the weights. The Jacobian is essentially every gradient for every training pattern and network output. The Jacobian is shown below. where N is the number of weights and P is the number of patterns. In other words the Jacobian will have as many columns as there are weights, and the number of rows will be equal to the product of M and P. Once the Jacobian is calculated, the LM algorithm can be represented by the following:</p><formula xml:id="formula_10">J ¼ @E 1 @W 1 @E 1 @W 2 Á Á Á @E 1 Wn Á Á Á Á Á Á Á Á Á Á Á Á @E 2 @W 1 @E 2 @W 2 Á Á Á @E 2 @Wn Á Á Á Á Á Á Á Á Á Á Á Á @E P @W 1 @E P @W 2 Á Á Á @E P @Wn</formula><formula xml:id="formula_11">W kþ1 ¼ W k À ðJ T k J k þ lIÞ À1 J T k E<label>ð10Þ</label></formula><p>where E is the total error for all patterns, I is the identity matrix, and l is a learning parameter. The learning parameter l is then adjusted several times each iteration and the result with the greatest reduction of error are selected. When the l value is very large the LM algorithm becomes steepest decent or BP, and when l is equal to zero it is the Newton Method. The entire process is then repeated until the error is reduced to the required value. This second order algorithm is significantly faster than BP. For small networks with few training patterns this is not a major issue, but for networks with many training patterns it is very computationally intensive. This inversion will cause each training iteration for LM to take longer than iteration for BP. the time required for training will still be far less than that of BP, because the LM will require such few iterations <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Genetic algorithm</head><p>Genetic algorithms are inspired by biological systems' improved fitness through evolution <ref type="bibr" target="#b37">[38]</ref>. Using the GAs, each individual in the population needs to be described in a chromosome representation. A chromosome is made up of a sequence of genes from a certain alphabet. An alphabet could consist of binary digits, continuous values, integers, symbols, matrices, etc. The representation method determines how the problem is structured in the GA and determines the genetic operators to be used. In this work, a chromosome is represented by a vector of continuous values, as it has been shown that natural representations are more efficient and produce better solutions. In this case, the chromosome length is the vector length of the solution which is coefficients of our model.</p><p>In GA, searching starts with an initial set of random solutions known as population. Each chromosome of population is evaluated using some measure of fitness function which represents a measure of the success of the chromosome. Based on the value of the fitness functions, a set of chromosomes is selected for breeding. In order to simulate a new generation, genetic operators such as crossover and mutation are applied. According to the fitness value, parents and offsprings are selected, while rejecting some of them so that the population size is kept constant for new generation. The cycle of evaluation-selection-reproduction is continued until an optimal or a near-optimal solution is found.</p><p>Selection attempts to apply pressure upon the population in a manner similar to that of natural selection found in biological systems. Poorer performing individuals (evaluated by a fitness function) are weeded out and better performing, or fitter, individuals have a greater than average chance of promoting the information they contain to the next generation. Crossover allows solutions to exchange information in a way similar to that used by a natural organism undergoing reproduction. This operator randomly chooses a locus and exchanges the subsequences before and after that locus between two chromosomes to create two offspring.</p><p>Mutation is used to randomly change (flip) the value of single bits within individual strings to keep the diversity of a population and help a genetic algorithm to get out of a local optimum. It is typically used sparingly <ref type="bibr" target="#b16">[17]</ref>. Principal structure of genetic algorithm is shown in Fig. <ref type="figure" target="#fig_3">3</ref>.</p><p>GAs may deal successfully with a wide range of problem areas. The main reasons for this success are: (1) GAs can solve hard problems quickly and reliably, (2) GAs are easy to interface to existing simulations and models, (3) GAs are extensible and (4) GAs are easy to hybridize. All these reasons may be summed up in only one: GAs are robust. GAs are more powerful in difficult environments where the space is usually large, discontinuous, complex and poorly understood. They are not guaranteed to find the global optimum solution to a problem, but they are generally good at finding acceptably good solutions to problems acceptably quickly. These reasons have been behind the fact that, during the last few years, GA applications have grown enormously in many fields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Evolutionary Neural Networks (ENN)</head><p>In this section, we apply GAs to evolve the weights between neurons in different layers in the neural network. The steps needed for evolving connection weights are described below:</p><p>Step 1. Encoding Each gene presents the weight between two neurons in different layers. A chromosome is constructed from a series of genes as shown in Fig. <ref type="figure" target="#fig_4">4</ref>. In this Figure, for a normal feed forward neural network that has 3 neurons in input layer, 2 neurons in hidden layer and 1 neuron in output layer, the first gene in the chromosome is the weight between neuron 1 and neuron 4, i.e. W 14 , the Second gene is the weight between neuron 1 and neuron 5, i.e. W 15 and so on. We use real number form to represent the connection weights.</p><p>Step 2. Generating the initial population The initial population (N pop ) is generated randomly. Each of Initial weights is randomly generated between À1 and 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Step 3. Calculating the fitness values</head><p>As regards the fitness function, it is based on the root mean squared error (RMSE) over a training data set, which is represented by the following expression:</p><formula xml:id="formula_12">RMSEðC j Þ ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi 1 N X N i¼1 ðY i À P i Þ 2 v u u t<label>ð11Þ</label></formula><p>where Y i is the actual value and P i is the output value of ith training data obtained from the neural network using the weights coded in jth chromosome (C j ) and N is the number of training data.</p><p>Step 4. Selection mechanism We use truncation selection scheme for selection procedure. In truncation selection individuals are sorted according to their fitness. Only the best individuals are selected for parents. The truncation threshold indicates the proportion of the population to be selected as parents. Then we use binary tournament selection scheme for selection of parents for generating new offsprings by use of genetic operators. In binary tournament selection, two members of the population are selected at random and their fitness compared and the best one according to fitness value will be chosen as one parent. Also another parent is selected with the same procedure.</p><p>Step 5. Genetic operators We use two-point crossover and one point mutation <ref type="bibr" target="#b26">[27]</ref> for genetic operators.</p><p>Step 6. Replacement</p><p>The current population is replaced by the newly generated offsprings, which forms the next generation.</p><p>Step 7. Stopping criteria If the number of generations equals to the maximum generation number, then stop; otherwise go to step 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Data post-processing and prediction</head><p>This stage is quite simpler. In this stage output dataset is postprocessed (returned to its original scale) and the predicted values are generated by the trained network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Case study</head><p>We implement PELMNN model by using Taiwan Stock Exchange index (TSE), Tehran Stock Exchange indices (TEPIX) data and other indices of Tehran Stock Exchange such as top 50 Companies index, Industry index and Financial Group index (the same dataset which were used by Chang and Liu <ref type="bibr" target="#b10">[11]</ref>, Esfahanipour and Aghamiri <ref type="bibr" target="#b20">[21]</ref>), Dow Jones Industrial Average Index (DJIA) and the Nasdaq Index.</p><p>TEPIX has evolved into an exciting and growing marketplace where individuals and institutional investors trade securities of over 420 companies. TEPIX is a weighted market value all share prices appearing on Tehran Stock Exchange price board. TEPIX calculation method is as follows: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Input variables</head><p>Technical indices are calculated through the variation of stock price, trading volumes and time following a set of formula to reflect the current tendency of the stock price fluctuations. These indexes can be applied to make decisions for evaluating the phenomena of oversold or overbought in the stock market. We use technical indices as input variables. The technical indices can be classified as movement or particular stock price variations, such as KD, RSI, MACD, MA, BIAS <ref type="bibr" target="#b20">[21]</ref>. We use these seven technical indices as input variables because they are among the most interesting and reliable technical indices in that they integrate features of trading system, are quite simple and respond to a wide variety of possible market situations adequately. These indices are defined in Table 2 <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b50">51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation metrics</head><p>To carry out the comparison we use several common evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">MAPE METRIC</head><p>The first metric is called mean absolute percentage error (MAPE). Calculation of this metric includes two steps. First, take the absolute deviation between the actual value and the predicted value. Second, compute the total ratio of deviation value by dividing it to its actual value.</p><formula xml:id="formula_13">MAPE ¼ 100 Â 1 N X N i¼1 jY i À P i j Y i<label>ð13Þ</label></formula><p>where Y i is the actual value and P i is the predicted value of ith test data obtained from the models and N is the number of test data. All of the references with which we compared our results have used MAPE metric, so the improvement metric can only be compared and evaluated for MAPE. The Ideal value for MAPE is zero, so being closer to zero for the metric indicates that prediction accuracy is higher. The lower the value of MAPE more desired results from the prediction method. Definition of improvement for MAPE metric (%) is:  </p><formula xml:id="formula_14">improvment ¼ 100 Â MAPE Metric of PELMNN À MAPE Metric of the other model MAPE Metric of the othermodel<label>ð14Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">POCID METRIC</head><p>A second interesting measure is prediction of change in direction (POCID) in Eq. ( <ref type="formula" target="#formula_14">14</ref>):</p><formula xml:id="formula_15">POCID ¼ 100 Ã P N i¼1 D i N<label>ð15Þ</label></formula><p>where</p><formula xml:id="formula_16">D i ¼ 1; ifðY i À Y iÀ1 ÞðP i À P iÀ1 Þ &gt; 0 0; otherwise:<label>ð16Þ</label></formula><p>This measure allows for an account of the number of correct decisions when predicting whether the value of the series will go up or down during the next time steps. The assigned values by POCID supposed to be between 0 and 100, the closer the values to 100 the more accurate the prediction model. This measure is more important when is applied to the stock market, because a correct prediction on the direction of the series of the stock quotation have a direct impact on financial gains and losses of the investment <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b9">10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">U of Theil Metric</head><p>A third measure is the U of Theil statistics, given by:</p><formula xml:id="formula_17">U of Theil ¼ P N i¼1 ðY i À P i Þ 2 P N i¼1 ðY i À Y iþ1 Þ 2<label>ð17Þ</label></formula><p>This measure associates the model performance with a RW model. If the U of Theil statistics is greater than 1, then the predictor has a worse performance in comparison to RW model. If the U of Theil statistics is equal to 1, then the predictor has the same performance of the RW model. If the U of Theil statistics is less than 1, then the predictor is better than a RW model. So, the predictor is usable if its U of Theil statistics tends to the perfect model if the U of Theil statistics tends to zero <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4.">ARV METRIC</head><p>A fourth evaluation measure is the Average Relative Variance (ARV), which is given by: ARV ¼ PSY is the ratio of the number of rising periods over the total number of periods. It reflects the buying power in relation to the selling power Volume Volume is a basic yet very important element of market timing strategy; volume provides clues as to the intensity of a given price move</p><formula xml:id="formula_18">P N i¼1 ðP i À Y i Þ 2 P N i¼1 ðP i À YÞ 2<label>ð18Þ</label></formula><p>If the ARV value is greater than 1, then the predictor is worse than simply taking the mean, and, if the ARV is less than 1, then the predictor is better than considering the mean as the prediction. If the ARV value is equal to 1, the predictor has the same performance as calculating the mean over the series. Hence, we benefit using the predictor if the value of ARV is less than 1, and it becomes even better when the ARV tends to zero. Therefore, the model is practical if the value of ARV is less than 1, and the closer the value to 0, it means that the predictor tends to be perfect <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b9">10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementing PELMNN for stock index prediction</head><p>In the first stage, we normalize data over range [À0.9, 0.9]. This is necessary for two reasons: First, all entries need to have the same weight. If the inputs of two neurons lie in different ranges, then the neuron with the larger absolute scale will be favored during training. Second, because of the neurons' transfer functions because either a sigmoid function or a hyperbolic tangent (tanh) is calculated and consequently these can only be performed over a limited range of values. A neuron only produces output whose absolute value is less than 1, since the transfer function has asymptotes at f(x) = 1 and À1 (the exception to this is when a linear transformation is used). If the data used with an ANN are not scaled to an appropriate range, then the network will not converge on training, or otherwise will not produce meaningful results <ref type="bibr" target="#b2">[3]</ref>.</p><p>Then we use stepwise regression <ref type="bibr" target="#b8">[9]</ref> to eliminate low impact factors and choose the most influential ones out of mentioned factors. The criterion for adding or removing is determined by F-test  In the second stage, we use GA to evolve initial weights of neural networks by using ENN. Then by using LMBP algorithm we tune obtained weights in previous stage. To meet the best network architecture with least error, different feature of parameters such as transfer function types, number of hidden layers, number of nodes for each layer and suitable features of genetic algorithm has been examined. Best obtained features of ENN and LMBP stages of PELMNN after tuning process are detailed in Tables <ref type="table" target="#tab_1">3</ref> and<ref type="table" target="#tab_2">4</ref>  respectively. At the last stage stock index prediction is done by means of test data and outcomes are post-processed (returned to original scale).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Performance analysis of PELMNN</head><p>For the purpose of evaluating prediction accuracy of the PEL-MNN, we will compare its outcomes with BPNN, PENN, PEBPNN, Esfahanipour and Aghamiri's method <ref type="bibr" target="#b20">[21]</ref>, Chang and Liu's method <ref type="bibr" target="#b10">[11]</ref> and Ferreira et al. method <ref type="bibr" target="#b22">[23]</ref> (it should be noted that all of these methods are applied to the same datasets with the same portion of training and test data).</p><p>The prediction results of PELMNN for test data sets and residuals for stock indices are shown in Figs. <ref type="figure">6</ref> and<ref type="figure">7</ref> respectively. Comparison of the performance of the proposed model with other models and percentage improvement of the PELMNN in comparison with other prediction models are shown in Table <ref type="table" target="#tab_3">5</ref> and<ref type="table" target="#tab_4">6</ref> respectively.</p><p>Regarding to Table <ref type="table" target="#tab_3">5</ref>, PELMNN has improved the prediction accuracy of indices and outperformed the other methods. Table <ref type="table" target="#tab_4">6</ref> shows the results with PELMNN model (test set) with respect to all the performance measures for each case. U of Theil metric shows that PELMNN model is not a RW model for all cases and it has been shown that PELMNN model is better than RW model for all cases. The ARV for each case is less than 1; it means that the PELMNN is rather perfect model. POCID metric for TSE and Nasdaq have the great results and on average it shows that in seventy percent of the time decisions were correct when predicting whether the value of the series will go up or down during the next time steps.</p><p>Based on the results, PELMNN has outperformed the other models and can be considered as a promising alternative for stock market prediction problems. In the PELMNN, the GA has been employed to escape the local optimal and in the following, LMBP 0 algorithm is used for finding the best local optimal. In this way the proposed model can find the appropriate weights of the network under the complex and chaotic conditions of the stock market.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>Both theoretical and empirical findings suggest that combining different methods can be an effective and efficient way to improve prediction results. This paper presented a pre-processed evolutionary Levenberg-Marquardt neural networks (PELMNN) model for stock market prediction by combining genetic algorithms and feed forward neural networks along with employing data pre-processing and post-processing concepts as reinforcement equipments. In the pre-processing stage, input data were scaled by means of data transformation technique and also stepwise regression was used for input selection to filter out the unrelated variables and keep only those variables, which have significant effects to the stock index. In the next stage genetic algorithm was adopted as a global search method to evolve neural networks' initial weights for tuning with LM algorithm. Obtained weights are used as initial weights for the Levenberg-Marquardt BP algorithm to local search, because of LMBP is capable in local search. Finally in post-processed stage, the output data were returned to its original scale and the predicted values were generated. The advantages of PEL-MNN are fast training ability, good adaptation of non-linear problem and high degree of accuracy. The results indicate that the ANN generated by the PELMNN method is not a RW model and it has been shown that PELMNN model for all cases is better than RW model. Moreover, experimental results showed that the PELMNN is able to cope with the fluctuations of stock market values and it also yields good prediction accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Framework of PELMNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. MLP neural network model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Principal structure of genetic algorithm.</figDesc><graphic coords="7,74.96,499.43,186.23,239.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Chromosome encoding.</figDesc><graphic coords="8,134.76,67.92,311.87,192.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 . 2</head><label>52</label><figDesc>Fig. 5. Stock indices time series.</figDesc><graphic coords="9,68.03,67.92,468.18,249.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>statistic value and decreasing the sum of squared error. After the entrance of first variable to the model, the variable number is increased step by step; once it is removed from this model, it will never enter the model again. Before selecting variables, the critical point, level of significant and the values of F e (F-to-enter) and F r (F-to-remove) have to be determined first. Then the partial F value of each step has to be calculated and compared to F e and F r ; If F &gt; F e , it is considered to add variables to the model; otherwise, if F &lt; F r , the variables are removed from model<ref type="bibr" target="#b8">[9]</ref>.The statistical software SPSS 17.0 was used to applying stepwise regression analysis in this research considering F e = 3.84 and F r = 2.71. The outcomes of this stage are MA6 and BIAS6 for all indices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,38.38,338.63,504.92,400.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Descriptions about Stock Indices.</figDesc><table><row><cell>Stock index name</cell><cell>From</cell><cell>To</cell><cell>Average</cell><cell>Standard deviation</cell></row><row><cell>Taiwan Stock Exchange index (TSE)</cell><cell>July 18, 2003</cell><cell>December 31, 2005</cell><cell>6070.557</cell><cell>1910.45</cell></row><row><cell>Tehran Stock Exchange Prices Index(TEPIX)</cell><cell>April 10, 2006</cell><cell>January 30, 2009</cell><cell>9991.631</cell><cell>973.63</cell></row><row><cell>Index of top 50 Companies</cell><cell>April 10, 2006</cell><cell>January 30, 2009</cell><cell>16562.49</cell><cell>3715.06</cell></row><row><cell>Industry index</cell><cell>April 10, 2006</cell><cell>January 30, 2009</cell><cell>7869.937</cell><cell>834.69</cell></row><row><cell>Index of Financial Group</cell><cell>April 3, 2006</cell><cell>January 30, 2009</cell><cell>20584.07</cell><cell>2383.53</cell></row><row><cell>Dow Jones Industrial Average Index Series</cell><cell>March 7, 2001</cell><cell>August 26, 2003</cell><cell>9345.324</cell><cell>925.15</cell></row><row><cell>Nasdaq Index Series</cell><cell>August 10, 2002</cell><cell>June 28, 2004</cell><cell>1671.681</cell><cell>278.11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>Tuned features of ENN part of PELMNN.</figDesc><table><row><cell>Optimum features</cell><cell>Parameters</cell><cell>TSE</cell><cell>TEPIX</cell><cell>Top 50 companies</cell><cell>Industry index</cell><cell>Financial group</cell><cell>DJIA</cell><cell>Nasdaq</cell></row><row><cell>Training</cell><cell>Population Size</cell><cell>80</cell><cell>80</cell><cell>100</cell><cell>90</cell><cell>90</cell><cell>90</cell><cell>90</cell></row><row><cell></cell><cell>Crossover Rate</cell><cell>0.8</cell><cell>0.75</cell><cell>0.7</cell><cell>0.85</cell><cell>0.75</cell><cell>0.85</cell><cell>0.75</cell></row><row><cell></cell><cell>Mutation Rate</cell><cell>0.08</cell><cell>0.1</cell><cell>0.1</cell><cell>0.13</cell><cell>0.15</cell><cell>0.15</cell><cell>0.15</cell></row><row><cell></cell><cell>Truncation Threshold</cell><cell>0.2</cell><cell>0.1</cell><cell>0.2</cell><cell>0.2</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell></cell><cell># Iterations</cell><cell>2000</cell><cell>2000</cell><cell>1500</cell><cell>2000</cell><cell>2000</cell><cell>2000</cell><cell>2000</cell></row><row><cell>Network architecture (input-hidden-output)</cell><cell>2-4-4-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transfer function</cell><cell>Sigmoid-Sigmoid-Tanh</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Fig. 6. Prediction results of PELMNN for test data sets.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Tuned features of LMBP part of PELMNN.</figDesc><table><row><cell>Optimum features</cell><cell>Parameters</cell><cell>TSE</cell><cell>TEPIX</cell><cell>Top 50 companies</cell><cell>Industry index</cell><cell>Financial group</cell><cell>DJIA</cell><cell>Nasdaq</cell></row><row><cell>Training</cell><cell>Learning Rate</cell><cell>0.15</cell><cell>0.1</cell><cell>0.15</cell><cell>0.15</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell></cell><cell># Iterations</cell><cell>1000</cell><cell>700</cell><cell>900</cell><cell>1000</cell><cell>800</cell><cell>800</cell><cell>800</cell></row></table><note><p>Fig. 7. Residuals of predicted values by PELMNN.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc>Comparison of the performance of the proposed model with other models.</figDesc><table><row><cell>Stock name</cell><cell>Method</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6</head><label>6</label><figDesc>Evaluation measures for all indices.</figDesc><table><row><cell>Stock name</cell><cell>MAPE</cell><cell>U of Theil</cell><cell>POCID</cell><cell>ARV</cell></row><row><cell>Taiwan Stock Exchange index (TSE)</cell><cell>0.51</cell><cell>0.48</cell><cell>85</cell><cell>0.024</cell></row><row><cell>Tehran Stock Exchange Prices Index (TEPIX)</cell><cell>0.50</cell><cell>0.72</cell><cell>60</cell><cell>0.0012</cell></row><row><cell>Index of top 50 companies</cell><cell>0.76</cell><cell>0.87</cell><cell>57.5</cell><cell>0.0036</cell></row><row><cell>Industry index</cell><cell>0.89</cell><cell>0.09</cell><cell>71.5</cell><cell>0.0108</cell></row><row><cell>Index of Financial Group</cell><cell>0.66</cell><cell>0.048</cell><cell>66.6</cell><cell>0.012</cell></row><row><cell>Dow Jones Industrial Average Index Series</cell><cell>1.41</cell><cell>0.96</cell><cell>58.3</cell><cell>0.084</cell></row><row><cell>Nasdaq Index Series</cell><cell>0.13</cell><cell>0.018</cell><cell>94.16</cell><cell>0.0024</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors wish to express their gratitude to two anonymous referees for their helpful comments, which greatly helped us to improve our paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A new hybrid for improvement of autoregressive integrated moving average models applying particle swarm optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Asadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Hejazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="5332" to="5337" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Atsalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Valavanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Surveying stock market forecasting techniques -Part II: Soft computing methods</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="5932" to="5941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An adaptive network-based fuzzy inference system for short-term natural gas demand estimation: uncertain and complex environments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Azadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Asadzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghanbari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy Policy</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1529" to="1536" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved estimation of electricity demand function by integration of fuzzy system and data mining approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Azadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saberi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghaderi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gitiforouz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ebrahimipour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy Convers. Manage</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="2165" to="2177" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Training a Neural Network with a Genetic Algorithm</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Downs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Electrical Engineering., University of Queensland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Artificial neural networks: fundamentals, computing, design, and applications</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Basheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hajmeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Microbiol. Meth</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="3" to="31" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E P</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Reinsel</surname></persName>
		</author>
		<title level="m">Time Series Analysis: Forecasting andControl</title>
		<meeting><address><addrLine>New Jersey</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>third ed.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generalized autoregressive conditional heteroscedasticity</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bollerslev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Econometrics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="307" to="327" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stepwise regression is an alternative to splines for fitting noisy data</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Burkholder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Lieber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomech</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="235" to="238" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Composite of adaptive support vector regression and nonlinear conditional heteroscedasticity tuned by quantum minimization for forecasts</title>
		<author>
			<persName><forename type="first">B.-R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-F</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="277" to="289" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A TSK type fuzzy rule based system for stock price prediction</title>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="135" to="144" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Data clustering and fuzzy neural network for sales forecasting: a case study in printed circuit board industry</title>
		<author>
			<persName><forename type="first">P-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-Y.</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="344" to="355" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evolving neural network for printed circuit board sales forecasting</title>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="83" to="92" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mining associative classification rules with stock trading data-A GA-based method</title>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Chang Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="605" to="614" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MISMIS-A comprehensive decision support system for stock market investment</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="626" to="633" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Forecasting economic and financial time-series with non-linear models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Franses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Forecast</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="169" to="183" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An Introduction to Genetic Algorithms for Scientists and Engineers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Coley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>World Scientific Publishing Co</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A class of hybrid morphological perceptrons with application in time series</title>
		<author>
			<persName><forename type="first">A</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Araújo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="513" to="529" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A robust automatic phase-adjustment method for financial forecasting</title>
		<author>
			<persName><forename type="first">A</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Araújo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="245" to="261" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autoregressive conditional heteroskedasticity with estimates of the variance of UK inflation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Engle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="987" to="1008" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adapted neuro-fuzzy inference system on indirect approach TSK fuzzy rule base for stock market analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esfahanipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Aghamiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="4742" to="4748" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient capital markets: a review of theory and empirical work</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Fama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Finance</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="383" to="417" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A E</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J L</forename><surname>Adeodato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A new intelligent system methodology for time series forecasting with artificial neural networks</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="113" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Combining forecasts -twenty years later</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W J</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Forecast</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="167" to="173" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An intelligent ACO-SA approach for short term electricity load prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghanbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hadavandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abbasian-Naghneh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNAI</title>
		<imprint>
			<biblScope unit="volume">6216</biblScope>
			<date type="published" when="2010">2010</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Combined neural networks for time series analysis</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ginzburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Horn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="224" to="231" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Goldberg</surname></persName>
		</author>
		<title level="m">Genetic Algorithms in Search, Optimization and Machine Learning</title>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Forecasting time series using a methodology based on autoregressive integrated moving average and genetic programming</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="66" to="72" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evolving the neural network model for forecasting air pollution time series</title>
		<author>
			<persName><forename type="first">H</forename><surname>Niska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hiltunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karppinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ruuskanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolehmainena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Eng Appl Artif Intel</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="159" to="167" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Integration of genetic fuzzy systems and artificial neural networks for stock price forecasting</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hadavandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shavandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghanbari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.Based Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="800" to="808" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Developing a time series model based on particle swarm optimization for gold price forecasting</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hadavandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghanbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Naghneh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Conference on Business Intelligence and Financial Engineering</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="337" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An improved sales forecasting approach by the integration of genetic fuzzy systems and data clustering: case study of printed circuit board</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hadavandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shavandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghanbari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="9392" to="9399" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Developing a hybrid artificial intelligence model for outpatient visit forecasting in hospitals</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hadavandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shavandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghanbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abbasian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="700" to="711" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Tourist arrival forecasting by evolutionary fuzzy systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hadavandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghanbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shahanaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abbasian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tourism Manage</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1196" to="1203" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Training feedforward networks with the marquardt algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Menhaj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNN</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="989" to="993" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The comparison of different feed forward neural network architectures for ECG signal diagnosis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Eng. Phys</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="372" to="378" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Chaos and nonlinear dynamics: application to financial markets</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Finance</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="1839" to="1877" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Adaptation in Natural and Artificial Systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Holland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>University of Michigan Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The adaptive neuro-fuzzy model for forecasting the domestic debt</title>
		<author>
			<persName><forename type="first">A</forename><surname>Keles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolcak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Keles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="951" to="957" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Runger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Statistics and Probability for Engineers</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Evolving neural networks for cancer radiotherapy, in: The Practical Handbook of Genetic Algorithms Applications</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Knowles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Corne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Chapman &amp; Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Toward optimal feature selection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Machine Learning (ML)</title>
		<meeting>the Thirteenth International Conference on Machine Learning (ML)<address><addrLine>Bari, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A decision support system for order selection in electronic commerce based on fuzzy neural network supported by real-coded genetic algorithm</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="141" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A neural network approach to predicting stock exchange movements using external factors</title>
		<author>
			<persName><forename type="first">N</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="371" to="378" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Detecting novelties in time series through neural networks forecasting with robust confidence intervals</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L I</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R L</forename><surname>Meira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="79" to="92" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Ozaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Time Series Models and Dynamical Systems, Handbook of Statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="1985">1985</date>
			<pubPlace>Noth-Holland, Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A joint review of technical and quantitative analysis of the financial markets towards a unified science of intelligent finance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2003 Hawaii International Conference on Statistics and Related Fields</title>
		<meeting>2003 Hawaii International Conference on Statistics and Related Fields<address><addrLine>Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Priestley</surname></persName>
		</author>
		<title level="m">Non-Linear and Non-Stationary Time Series Analysis</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Gabr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Introduction to Bispectral Analysis and Bilinear Time Series Models</title>
		<title level="s">Lecture Notes in Statistics</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1984">1984</date>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mccleland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing: Explorations in the Micro Structure of Cognition</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Schwager</surname></persName>
		</author>
		<title level="m">Schwager on Futures: Technical Analysis</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Comparative evaluation of genetic algorithm and backpropagation for training neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sexton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Sci</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="45" to="59" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Forecasting stock indices using radial basis function neural networks optimized by artificial fish swarm algorithm</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="378" to="385" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Neural networks approach to the random walk dilemma of financial time series</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sitte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sitte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Intell</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="163" to="171" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Predicting the exchange traded fund DIA with a combination of genetic algorithms and neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Versace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bhatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hinds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shiffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Appl</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="417" to="425" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Residential property price time series forecasting with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ware</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jenkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="335" to="341" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Using neural networks to forecast the systematic risk of stocks</title>
		<author>
			<persName><forename type="first">H.-G</forename><surname>Wittkemper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Operational Res</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="577" to="588" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Neural network and GA approaches for dwelling fire occurrence prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="213" to="219" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Evolving artificial neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Forecasting with artificial neural networks: the state of the art</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Patuwo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Forecast</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="35" to="62" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
