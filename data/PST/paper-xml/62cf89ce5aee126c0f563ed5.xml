<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding Unfairness in Fraud Detection through Model and Data Bias Interactions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-07-13">13 Jul 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jos?</forename><surname>Pombal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Instituto Superior T?cnico</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Instituto de Telecomunica??es</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andr?</forename><forename type="middle">F</forename><surname>Cruz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jo?o</forename><surname>Bravo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pedro</forename><surname>Saleiro</surname></persName>
						</author>
						<author>
							<persName><forename type="first">M?rio</forename><forename type="middle">A T</forename><surname>Figueiredo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Instituto Superior T?cnico</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Instituto de Telecomunica??es</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pedro</forename><surname>Bizarro</surname></persName>
						</author>
						<title level="a" type="main">Understanding Unfairness in Fraud Detection through Model and Data Bias Interactions</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-13">13 Jul 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2207.06273v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Algorithmic Fairness</term>
					<term>Data Bias</term>
					<term>Machine Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, machine learning algorithms have become ubiquitous in a multitude of high-stakes decision-making applications. The unparalleled ability of machine learning algorithms to learn patterns from data also enables them to incorporate biases embedded within. A biased model can then make decisions that disproportionately harm certain groups in society -limiting their access to financial services, for example. The awareness of this problem has given rise to the field of Fair ML, which focuses on studying, measuring, and mitigating unfairness in algorithmic prediction, with respect to a set of protected groups (e.g., race or gender). However, the underlying causes for algorithmic unfairness still remain elusive, with researchers divided between blaming either the ML algorithms or the data they are trained on. In this work, we maintain that algorithmic unfairness stems from interactions between models and biases in the data, rather than from isolated contributions of either of them. To this end, we propose a taxonomy to characterize data bias and we study a set of hypotheses regarding the fairnessaccuracy trade-offs that fairness-blind ML algorithms exhibit under different data bias settings. On our real-world account-opening fraud use case, we find that each setting entails specific trade-offs, affecting fairness in expected value and variance -the latter often going unnoticed. Moreover, we show how algorithms compare differently in terms of accuracy and fairness, depending on the biases affecting the data. Finally, we note that under specific data bias conditions, simple pre-processing interventions can successfully balance group-wise error rates, while the same techniques fail in more complex settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>for possible underlying prejudices can lead to decisions that disproportionately harm certain social groups. The goal of building systems that incorporate these concerns has given rise to the field of Fair ML, which has grown rapidly in recent years <ref type="bibr" target="#b34">[35]</ref>.</p><p>Fair ML research has focused primarily on devising ways to measure unfairness <ref type="bibr" target="#b3">[4]</ref>, and to mitigate it in algorithmic prediction tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35]</ref>. Mitigation is broadly divided in three approaches: pre-processing, in-processing, and post-processing <ref type="bibr" target="#b33">[34]</ref>, which map respectively to interventions on the training data, on the model optimization, and on the model output. Another focal point of discussion revolves around the underlying sources of algorithmic unfairness. The aforementioned mitigation methods reflect different beliefs with respect to the origins of unfair predictions. Preprocessing assumes that the cause is bias in the data, while in-and post-processing shift the onus to modeling choices and criteria.</p><p>Research seems to be divided along the same lines in what concerns uncovering the source of bias in the ML pipeline. On the one hand, there is work defending that bias in the data is at the root of downstream unfairness in predictions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref>. On the other hand, some researchers have adverted to the crucial role that model choices have in algorithmic unfairness <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>. However, the consequences of different sources of bias on unfairness produced by ML algorithms remains unclear. Little attention has been paid to the interaction between biases in the data and model choices. At best, the relationship between the two is recognized but, save a few studies, mostly left unexplored. At worst, one of them is outright disregarded.</p><p>We maintain that the two views are complementary, not mutually exclusive. In fact, we aim to add to this discussion by showing that the landscape of algorithmic bias and fairness does change dramatically with the specific bias patterns present in a dataset. Conversely, under the same data bias conditions, different models incur in distinct fairness-accuracy trade-offs. We show this empirically by devising a series of controlled experiments with fairness-blind ML algorithms that map such trade-offs to types of bias present in the data. Each experiment is motivated by a hypothesis about these trade-offs, and some are built to reflect biases that naturally arise in fraud detection, such as the selective labels problem <ref type="bibr" target="#b32">[33]</ref>, or the fact that certain agents are actively trying to evade fraud. To this end, we propose a taxonomy of different conditions under which a dataset may be considered biased with respect to a protected group.</p><p>The experiments are conducted on a large, real-world, bankaccount-opening fraud dataset, into which bias is injected through additional synthetic features. The synthetic nature of these additional features does not limit our analysis; rather, by allowing full control of the sources of bias, it lets us draw clear links between generic dataset characteristics and subsequent fairness-accuracy trade-offs. Moreover, these synthetic features have a clear grounding in real-world data bias patterns (e.g., different group-wise prevalences, under-represented minorities). This work has two overarching goals. First, to provide empirical evidence that predictive unfairness stems from the relationship between data bias and model choices, rather than from isolated contributions of either of them. Second, to steer the discussion towards relating algorithmic unfairness to concrete patterns in the data, allowing for more informed, data-driven choices of models and unfairness mitigation methods.</p><p>Summing up, we make the following contributions:</p><p>? A formal taxonomy to characterize data bias between a protected attribute, other features, and the target variable. ? Experimental results for a comprehensive suite of hypotheses regarding fairness-accuracy trade-offs ML models make under distinct types of data bias, pertinent, but not restricted to fraud detection. ? Showing how, by changing data bias settings, the picture of algorithmic fairness changes, and how comparisons among algorithms differ. ? Raising awareness to the issue of variance in fairness measurements, underlining the importance of employing robust models and metrics. ? Evaluation of the utility of simple unfairness mitigation methods under distinct data bias conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Fairness-Fairness Trade-offs</head><p>Fairness is often at conflict with itself. It has been shown that when a classifier score is calibrated and group-wise prevalences are different, it is impossible to achieve balance between false positive (FPR) and false negative (FNR) error rates <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref>. Corbett-Davies and Goel <ref type="bibr" target="#b15">[16]</ref> further discuss these metrics' trade-offs and point out their statistical limitations. Speicher et al. <ref type="bibr" target="#b40">[41]</ref> compare between-group and in-group fairness metrics, showing that solely optimizing for one may harm the other.</p><p>It is clear that no single fairness metric is ideal, and that its choice is highly dependent on assumptions made and the problem domain <ref type="bibr" target="#b38">[39]</ref>. With this in mind, as motivated in Section 4.3, we will use FPR parity to measure fairness. Decomposing this metric as per Equation 1 allows for a better understanding of the aforementioned trade-offs and of how they result from an interaction between the data and classifier. For two protected groups ? and ?, let ? ? be the prevalence of group ? ? {?, ?}, and ??? ? , ? ? ? ? be the precision and false negative rate, respectively, of a classifier on group ?. Then, as shown by Chouldechova <ref type="bibr" target="#b14">[15]</ref>,</p><formula xml:id="formula_0">? ?? ? ? ?? ? = ? ? 1-? ? 1-??? ? ??? ? (1 -? ? ? ? ) ? ? 1-? ? 1-??? ? ??? ? (1 -? ? ? ? ) .<label>(1)</label></formula><p>Notice how ? ? ? parity must be sacrificed in order to guarantee ? ?? parity, if prevalence ? ? differs between groups but ??? ? are balanced. Prevalence is only related to the data itself, while the other metrics result from an interaction between the classifier and the dataset. Indeed, for any classifier under different group-wise prevalences, we must sacrifice at least one of: FPR parity, FNR parity, or calibration<ref type="foot" target="#foot_0">1</ref> (PPV parity). Figure <ref type="figure" target="#fig_0">1</ref> illustrates this relation under different group-wise prevalences. Although different algorithms achieve different fairness-accuracy trade-offs, they all follow the same trend: the group with higher prevalence is disproportionately affected by false positives, and subsequently less affected by false negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relating Trade-offs and Data Bias</head><p>Label Bias. In the criminal justice context, Fogliato et al. <ref type="bibr" target="#b21">[22]</ref> assume that their target labels (arrest) are a noisy version of the true outcome they wish to predict (re-offense). They then propose a framework to analyze the impact of this imperfection on protected groups (e.g., race). Wang et al. <ref type="bibr" target="#b43">[44]</ref> propose a method to mimic label bias that is particularly interesting to us: they corrupt a portion of the target labels in their training data, where the amount of corrupted labels depends both on the protected group and on the target. Afterwards, they assess the impact of this on downstream unfairness mitigation methods. Most prove to be less effective under this type of bias. In the case of account opening fraud, label bias can arise in the form of the selective label problem <ref type="bibr" target="#b32">[33]</ref>, which will be explained in Section 4.2.5.</p><p>Group-size disparity, prevalence disparity, and relations between protected attribute and other dataset features. As part of a larger suite of experiments, Blanzeisky and Cunningham <ref type="bibr" target="#b6">[7]</ref> study the impact of prevalence disparities on underestimation, which is the ratio between a group's probability of being predicted positive, and the probability of being labelled positive. They test several fairness-blind algorithms on a fully synthetic dataset. The main finding is that the smaller the number of minority group observations, the stronger underestimation becomes. We build on this work by using a larger dataset, experimenting with more bias conditions and models, and evaluating them with popular metrics in the Fair ML community.</p><p>Akpinar et al. <ref type="bibr" target="#b1">[2]</ref> study the effects on observational unfairness metrics (e.g.: demographic parity, TPR parity, etc...) of training models on several types of data bias. They propose a sandbox tool to allow practitioners to inject bias in datasets, so as to run controlled experiments and evaluate the robustness of their systems. Our work is similar in the bias injection process, but its overarching goal is somewhat different. We focus on formalizing the data bias conditions, and conducting a thorough analysis of the fairnessaccuracy trade-offs different algorithms exhibit when exposed to bias.</p><p>Finally, we draw inspiration from Reddy et al. <ref type="bibr" target="#b37">[38]</ref>, who study the impact of several data bias conditions on a large suite of deep learning unfairness mitigation methods. The authors create a synthetic variant of the MNIST dataset <ref type="bibr" target="#b18">[19]</ref> (CI-MNIST), where they emulate and test the impact of group-size disparities, correlations between the target and the sensitive attribute (essentially prevalence disparity), and correlations between non-sensitive features and the target. The UCI Adult dataset, a real dataset, is also experimented on. However, the authors outline the importance of synthetic data, by stating that it is not possible to fully emulate some bias conditions on real data. While we make use of a real dataset, we augment it synthetically for this reason. The key takeaway is that the landscape of algorithmic fairness changes drastically under more extreme bias scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BIAS TAXONOMY</head><p>Throughout this work, we refer to a dataset's feature set as ? , the class label as ? and the protected attribute as ? . A generic value taken by any of these is represented as its lowercase letter. It is important to stress that the following definitions use the inequality sign (?) to mean a statistically significant difference.</p><p>Despite the multitude of definitions, there is still little consensus on how to measure data bias or its impact on the predictive performance and fairness of algorithms <ref type="bibr" target="#b34">[35]</ref>. In this paper, we propose a broad definition: there is bias in the data with respect to the protected attribute, whenever the random variables ? and ? are sufficiently statistically dependent from ? .</p><p>Bias Condition 0 (Protected attribute bias).</p><formula xml:id="formula_1">? [?, ? ] ? ? [?, ? |? ].<label>(2)</label></formula><p>For Condition 0 to be satisfied, the distribution of ? must be statistically related to either ? , ? , or both. If ? is directly dependent on ? or indirectly through ? , algorithms may use ? to predict ? .</p><p>We will study the effect of three specific bias conditions (or types). The following conditions all imply Condition 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bias Condition 1 (Prevalence disparity)</head><p>.</p><formula xml:id="formula_2">? [? ] ? ? [? |? ] ,<label>(3)</label></formula><p>i.e., the class probability depends on the protected group. For example, if we consider residence as ? and crime rate as ? , certain parts of a city have higher crime rates than others.</p><p>Bias Condition 2 (Group-wise distinct class-conditional distribution).</p><formula xml:id="formula_3">? [? |? ] ? ? [? |?, ? ].<label>(4</label></formula><p>) Note that this condition allows for ? [? ] = ? [? |? ] (no prevalence disparity). Consider an example in account opening fraud in online banking. Assume that the fraud detection algorithm receives a feature which represents how likely the submitted e-mail is to be fake (X) and the client's reported age (Z) as inputs. In account opening fraud, fraudsters tend to impersonate older people, as these have a larger line of credit to max out, but use fake e-mail addresses to create accounts. Therefore, the e-mail address feature will be better to identify fraud instances for reportedly older people, potentially generating a disparity in group-wise error-rates, even if age groups have an equal likelihood of committing fraud in general. Figure <ref type="figure" target="#fig_1">2</ref> provides a visual example using generic features. There is clear class separability for the majority group (middle), i.e., we can distinguish the fraud label using the two features. At the same time, there is virtually no separability for the minority group (right), as positive and negative samples overlap on this feature space. However, this is not discernible when looking at the marginal distribution for Y, ? 1 , and ? 2 (left).</p><p>Bias Condition 3 (Noisy Labels). The noisy labels condition is</p><formula xml:id="formula_4">? * [? |?, ? ] ? ? [? |?, ? ] ,<label>(5)</label></formula><p>where ? * is the observed distribution and ? is the true distribution. That is, some observations belonging to a protected group have been incorrectly labeled. Inaccurate labeling is a problem for supervised learning in general. It is common for one protected group to suffer more from this ailment, if the labeling process is somehow biased. For example, women and lower-income individuals tend to receive less accurate cancer diagnoses than men, due to sampling differences in medical trials <ref type="bibr" target="#b19">[20]</ref>. In account opening fraud, label bias may arise due to the selective label problem. Work on the impact of this bias on downstream prediction tasks is discussed in Section 2.</p><p>We will also study the effect of the following bias extensions. An extension is a property that does not imply Condition 0, but has consequences on algorithmic fairness.</p><p>Bias Extension 1 (Group size disparities). Let ? be a particular group from a given protected attribute ? , and ? the number of possible groups. Under group size disparities, we have</p><formula xml:id="formula_5">? [? = ?] ? 1 ? .<label>(6)</label></formula><p>Intuitively, this represents different group-wise frequencies. A typical example is religion: in many countries, there tends to be a dominant religious group and a few smaller ones.</p><p>Bias Extension 2 (Train-test disparities). Let BC ? be a set of bias conditions BC on a dataset ?. Then, under train-test disparities:</p><formula xml:id="formula_6">?? ????? ? ?? ???? .<label>(7)</label></formula><p>In supervised learning, it is assumed that the train and test data are independent and identically distributed (i.i.d.). It is crucial that the training set follows a representative distribution of the real world, so that models generalize well to unseen data. The test set is the practitioner's proxy for unseen data, and concept drift may greatly affect subsequent model performance and fairness. In fraud detection this can be particularly important, if we consider that fraudsters are constantly adapting to avoid being caught. As such, a trend of fraud learned during training can easily become obsolete when models are ran in production.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY 4.1 Dataset</head><p>Throughout this paper, we use a real-world large-scale case-study of account-opening fraud (AOF). Each row in the dataset corresponds to an application for opening a bank account, submitted via the online portal of large European bank. Data was collected over an 8-month period, containing over 500K rows. The earliest 6 months are used for training and the latest 2 months are used for testing, mimicking the procedure of a real-world production environment. As a dynamic real-world environment, some distribution drift is expected along the temporal axis, both from naturally-occurring shifts in the behavior of legitimate customers, as well as shifts in fraudsters' illicit behavior as they learn to better fool the production model.</p><p>Fraud rate (positive label prevalence) is about 1% in both sets. This means that a na?ve classifier that labels all observations as not fraud achieves a test set accuracy of almost (99%). Such large class imbalance entails certain additional challenges for learning <ref type="bibr" target="#b23">[24]</ref>, and calls for a specific evaluation framework that is presented in Section 4.3. We append a set S of synthetically generated columns to the data, in such a way that each condition BC ? ? BC is satisfied. In all cases, the protected attribute ? under analysis is part of S, allowing us to control how the data is biased with respect to ? . This way, we further our understanding of how a given bias type affects downstream fairness and performance. ? can take values A or B.</p><p>For any given set of bias conditions, we repeat the above process 10 times, yielding 10 distinct datasets, which differ in the synthetic columns. This makes our conclusions more robust to the variance in the column generation process.</p><p>Models are trained on all 10 seeds for each BC set. Results are obtained for both fairness-aware and unaware models. In the former, models have access to ? in the training process, in the latter, they do not. Section 4.3 will provide further details on this. 4.2.2 Hypothesis H1: Group size disparities alone do not threaten fairness. We would like to assess whether a protected attribute that is uncorrelated with the rest of the data can lead to downstream algorithmic unfairness. In particular, the goal is to compare the case where the two groups are of the same size, with that in which there is a majority and a minority protected group.</p><p>We append a single column to the base dataset: a protected attribute that takes value A or B (groups) for each sample. This feature is generated such that ? [? = ?] = ? ? and ? [? = ?] = ? ? = 1 -? ? , but is independent of features ? and target ? . This can be achieved by having each row of the new column take the value of a (biased, if ? ? ? 1</p><p>2 ) coin flip. Note how, according to our taxonomy in Section 3, this generative process for ? satisfies Bias Extension 1. However, since it is a simple coin flip, it does not satisfy Bias Condition 0. As such, ? remains unequivocally unbiased towards both ? and ? , on average. Our Baseline will be for ? [? = ?] = 0.5, when group sizes are equal, and so no Bias Condition or Bias Extension is satisfied -a completely unbiased scenario. Thus, for this hypothesis, we shall test cases where ? [? = ?] = 0.9, and ? [? = ?] = 0.99.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Hypothesis H2</head><p>: Groups with higher fraud prevalence have higher error rates. Contrary to the setting in H1, ? and ? are no longer independent. In particular, one of the groups in ? has higher positive label prevalence (in our case, higher fraud rate). Formally,</p><formula xml:id="formula_7">? [? = 1|? = ?] = ? ? ? [? = 1|? = ?], where ? ? R + , satisfying Bias Condition 1 if ? ? 1.</formula><p>Many real protected attributes exhibit such relationships with ? (e.g., ethnicity and crime rates, age and fraud rate). It is also interesting to study the effects of this condition with and without group size disparities (Bias Extension 1). As such, the above conditions will be tested at ? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Hypothesis H3:</head><p>Correlations between fraud, other features, and the sensitive attribute influence fairness. To test this hypothesis, we inject Bias Condition 2 -group-wise distinct class-conditional distribution (GDCCD) -into the dataset. We do so by generating not only ? but two more synthetic columns, ? 1 and ? 2 , and appending them to the dataset. The idea is to correlate ? and ? indirectly, while keeping group-wise prevalence and sizes equal.</p><p>The additional columns are created such that group B is more separable in the space {?, ? 1 , ? 2 } than group A. In particular, 4 bivariate normals (?? ? ) for the 4 permutations of label-group pairs are used. The end result is a space like the depicted in Figure <ref type="figure" target="#fig_1">2</ref>. We expect this to facilitate detecting fraud for group B, thereby generating some disparity in evaluation measures (FPR, TPR, etc. . . ). The goal is to mimic the selective label problem, where the system under study decides which observations are labelled. For example, if a model flags an observation as fraudulent and blocks the opening of an account, we will never know whether it was truly fraud. If we later use these observations to train models, we might be using inaccurate ground truth labels. There are several works that propose more complex label massaging procedures to mitigate unfairness in a dataset <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. In this context, our method may be seen as a na?ve approach to achieve balanced prevalence via label flipping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Hypothesis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head><p>4.3.1 Fairness metrics. The real-world setting in which these models would be employed -online bank account-opening fraud detection -motivates the choice of fairness and performance evaluation metrics adopted in this work.</p><p>In account-opening fraud, a malicious actor attempts to open a new bank account using a stolen or synthetic identity (or both), in order to quickly max out its line of credit <ref type="bibr" target="#b0">[1]</ref>. A false positive (FP) is a legitimate individual who was wrongly flagged as fraudulent, and wrongly blocked from opening a bank account. Conversely, a false negative (FN) is a fraudulent individual that was able to successfully open a bank account by impersonating someone else, leading to financial losses for the bank.</p><p>We must ensure that automated customer screening systems do not disproportionately affect certain protected sub-groups of the population, directly or indirectly. Fairness w.r.t. the label positives is measured as the ratio between group-wise false negative rates (FNR). Equalizing FNR is equivalent to the well-known equality of opportunity metric <ref type="bibr" target="#b22">[23]</ref>, which dictates equal true positive rates (TPR), ? ?? = 1 -? ? ?. In our setting, this ensures that a proportionately equal amount of fraud is being caught for each sub-group. On the other hand, fairness w.r.t. the label negatives is measured as the ratio between group-wise false positive rates (FPR). Within our case-study, equalizing FPR (also known as predictive equality <ref type="bibr" target="#b16">[17]</ref>) ensures no sub-group is being disproportionately denied access to banking services. 4.3.2 Performance metrics. Bank account providers are not willing to go above a certain level of FPR, because each false positive may lead to customer attrition (unhappy clients who may wish to leave the bank). At an enterprise-wide level, this may represent losses that outweigh the gains of detecting fraud. The goal is then to maximize the detection of fraudulent applicants (high global true positive rate, TPR), while maintaining low customer attrition (low global false positive rate). As such, we evaluate the model's TPR at a fixed FPR, imposed as a business requirement in our case-study. We assess the FPR ceiling of 5%. A more typical metric such as accuracy would not be informative, since it is trivial to obtain 99% accuracy by classifying all observations as not fraud (recall that fraud rate is around 1%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Algorithms and models</head><p>We test 6 different ML algorithms: XGBoost (XGB) <ref type="bibr" target="#b13">[14]</ref>, LightGBM (LGBM) <ref type="bibr" target="#b28">[29]</ref>, Logistic Regression (LR) <ref type="bibr" target="#b42">[43]</ref>, Decision Tree (DT) <ref type="bibr" target="#b9">[10]</ref>, Random Forest (RF) <ref type="bibr" target="#b8">[9]</ref>, and Feed Forward Neural Network (MLP) trained with the Adam optimizer <ref type="bibr" target="#b29">[30]</ref>. The first two are gradient boosted tree methods, which have stood out as top performers for tabular data in recent years <ref type="bibr" target="#b39">[40]</ref>. The other four are popular supervised learning algorithms, used in a variety of applications.</p><p>All the above algorithms are fairness-blind, in the sense that they do not consider fairness constraints in their optimization process. This choice is intentional: we wish to analyze fairness-accuracy tradeoffs under different kinds of bias in the data, before fairness is taken into consideration. Still, we evaluate the models' predictions when they have access to the protected attribute at training time (awareness), and when they do not (unawareness). The idea is to assess which types of data bias still lead to predictive unfairness, even when the algorithm is oblivious of the sensitive attribute.</p><p>Lastly, hyperparameter choice greatly influences performance and fairness <ref type="bibr" target="#b17">[18]</ref>. As such, for each algorithm, we randomly sample 50 hyperparameter configurations from a grid space to be used in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>We summarize our findings in the following sections. In each, we discuss the key takeaways of an hypothesis, and detail the interplay of fairness metrics. We also present a series of plots, highlighting relevant phenomena.</p><p>Figure <ref type="figure">4</ref> shows results for H1, outlining how sample variance can harm algorithmic fairness, even when models are expected to be fair. Figure <ref type="figure" target="#fig_10">6</ref> shows how different algorithms fared in terms of performance and both fairness metrics, on each hypothesis. Figure <ref type="figure" target="#fig_8">5</ref> deep dives on the LGBM algorithm, to show how Precision plays a part in error-rate disparities, depending on the bias afflicting the data.</p><p>On all Figures, the y-axis represents a ratio of group error rates ( ? ?? ? ? ?? ? or ? ? ? ? ? ? ? ? ). As such, it will be in a log 2 scale, which allows points to be laid out symmetrically <ref type="foot" target="#foot_1">2</ref> . The two red dashed lines are at the log 2 of 0.8 and 1.25, following the "80% rule", used by the US Equal Employment Opportunity Commission <ref type="bibr" target="#b35">[36]</ref>. That is, a group's error rate should be at least 80% of the other groups' rates to be considered fair.</p><p>The plots exhibit the top performing model configuration, in terms of TPR, for each of the 10 dataset seeds. This information is summarized in error-bars, whose center is the median performance of the top models, and edges correspond to the minimum and maximum achieved on each dimension (performance and fairness). The error bars may be coloured by algorithm, or by hypothesis, depending on the context. The idea is to focus on models which would be chosen for production in the 'world' of each dataset seed -that is, the top performers. Thus, their fairness, or lack thereof, is particularly relevant to the practitioner.</p><p>5.1 H1: Group size disparities do not threaten fairness.</p><p>5.1.1 Key Takeaways. Models are fair in expectation. On average, if there are no differences in each group's data distribution, models will not necessarily discriminate the minority. In fact, large group size disparities lead to high fairness variance, possibly resulting in unfair models for either group (see Figure <ref type="figure">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Fairness Metrics Interplay.</head><p>Both predictive equality and equality of opportunity are achieved on average since the target variable ? does not depend on the protected attribute ? in any way.</p><p>5.2 H2: Groups with higher fraud rate have higher error rates. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">H3:</head><p>Correlations between fraud, other features, and the sensitive attribute influence fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Key</head><p>Takeaways. Contrary to H2, FPR and FNR are skewed in the same direction: on the group that has more adept fraudsters, innocent people are systematically flagged as fraudulent more often (higher FPR), and fraudulent individuals evade detection more (higher FNR). Equalizing prevalence is no longer useful (it is equal), and unawareness actually aggravates predictive equality disparities. Random Forests, the most robust algorithm in terms of fairness on other hypotheses, was the most unfair and volatile algorithm on this scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Fairness Metrics</head><p>Interplay. Since prevalence is constant across groups, it cannot be the source of unfairness. Instead, with models better classifying observations from one group, error rate disparities stem from precision divergences. Models have higher precision on one group, leading to relatively higher error rates for the other. Innocent individuals belonging to the group that is "better" at committing fraud are flagged as fraudulent more often than the other group (higher FPR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">H4:</head><p>The selective label problem may have mixed effects on algorithmic fairness.   LGBM group-wise precision ratios. In contrast to H2.1, the median precision ratios for H3 and H4.1 are significantly skewed in favour of group B, meaning that models are better at classifying fraud in this group. In H3, this happens because group B fraud is easier to detect given ? 1 and ? 2 . In H4.1, some of group A's fraud labels are false, giving models more accurate information to classify observations that belong to group B. Furthermore, in H4.1, A is apparently more fraudulent than B (double the prevalence), contributing to a steeper FPR disparity than in H2.1 (see Figure <ref type="figure" target="#fig_10">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>The underlying causes for algorithmic unfairness in prediction tasks remain elusive. With researchers divided between blaming the data or blaming the algorithms, little attention has been heeded to interactions between the two. Our main contribution to this discussion is a comprehensive analysis on different hypotheses regarding fairness-accuracy tradeoffs exhibited by ML algorithms, when subject to different types of data bias with respect to a protected group. The use case of this work is fraud detection, but its conclusions are extensible within and outside the scope of the Financial domain.</p><p>We can confidently state that the landscape of algorithmic fairness is a puzzle where both algorithm and data are vital, intertwined pieces, essential for its completion. Our results show how an algorithm that was fair under certain biases in the data, may become unfair in other circumstances. For example, Random Forests were the fairest models when the protected group was directly linked to the target, but became quite unfair once dependencies through other features were introduced. Further exploring these interactions is a relevant avenue for future research on the causes of unfairness.</p><p>Crucially, we have brought to the fore the often overlooked dangers of variance, by experimenting on several samples of the same underlying bias settings. This showed how algorithmic fairness is subject to the idiosyncrasies of a dataset, especially when groups have significantly different sizes. A model may be fair on one sample, and drastically unfair on another, even though the generative process for both samples was the same (with differences merely stemming from sampling variance). Research is usually focused on whether a model is fair on average, which understates the importance of building systems that are robust to sample changes.</p><p>A useful side product of our study was finding that simple unfairness mitigation methods are enough to balance error rates, under certain bias conditions. We also reinforced the relevance of choosing the appropriate fairness metric by exposing the shortcomings of ratios, and showing how error rate ratios move in opposite directions under group-wise prevalence disparities -a fact that is well-grounded mathematically.</p><p>We have proposed a data bias taxonomy, and studied several biases by injecting them synthetically into real data. An interesting avenue for further research would be to develop methods to detect and characterize these bias patterns without prior knowledge.</p><p>All in all, by relating data bias to fairness-accuracy trade-offs in downstream prediction tasks, one can make more informed, datadriven decisions with regards to the unfairness mitigation methods employed, and other choices along the Fair ML pipeline. We firmly believe that this path holds the key to a better understanding of algorithmic unfairness, that generalizes well to any domain and application.</p><p>In the fraud detection domain, and the financial services industry in general, gaining a better understanding of algorithmic unfairness should be a top priority. This will lead to more effective mitigation, which is a core step towards guaranteeing that all groups in society have equal access to financial services, and thus equality in general.  Group sizes are always balanced except for H1. At a higher level, this shows how different types of bias yield distinct fairness-accuracy trade-offs. At a lower level, each algorithm exhibits particular trade-offs. For example, contrary to its counterparts, LR shows more balanced FPR rates on H2.1 than on H3. XGB is omitted because results were identical to LGBM. DT is omitted because performance was too low. Hypotheses Recap -Baseline: Unbiased setting -both group sizes are equal, no bias conditions nor extensions satisfied. H1: group size disparities alone do not threaten fairness (case shown is for group A representing 90% of the dataset). H2.x: Groups with higher fraud prevalence have higher error rates (in H2.1 both train and test sets are biased, and H2.x bars represent the case where A has double the fraud rate of B). H3: Algorithmic unfairness arises when groups leverage features unequally to avoid fraud detection.. H4.x: The selective label problem may have mixed effects on algorithmic fairness. (H4.1 studies harmful effects on fairness, and H4.2 beneficial ones).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Fairness-accuracy trade-offs attained by a variety of ML algorithms under distinct group prevalences: group A has 4x the prevalence of group B. Disparities for FPR and FNR move in opposite directions. As suggested by Equation 1, the group with highest prevalence is disproportionately affected by false positives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Group-wise class-conditional distribution relative to features ?1 and ?2. There is clear class separability for the majority group (middle), i.e., we can distinguish the fraud label using the two features. At the same time, there is virtually no separability for the minority group (right), as positive and negative samples overlap on this feature space. However, this is not discernible when looking at the marginal distribution for Y, ? 1 , and ? 2 (left).</figDesc><graphic url="image-1.png" coords="3,317.96,485.90,240.25,64.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>9 Figure 3 :</head><label>93</label><figDesc>Figure3: Illustration of the bias injection process used in our experiments: 10 instances of synthetic columns are (randomly) generated and appended to a real account-opening fraud dataset, creating 10 biased data-sets that satisfy desired bias conditions. A set of models is then trained separately on each of the biased sets of data, after which performance and fairness are measured.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Hypothesis H2. 1 :</head><label>1</label><figDesc>Algorithmic unfairness arises if both training and test sets are biased. We first generate ? such that ? [? = 1|? = ?] = 2 ? ? [? = 1|? = ?], and then ? [? = 1|? = ?] = 4 ? ? [? = 1|? = ?]. These conditions apply to both training and test sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>[? = ?] = 0.01, ? [? = ?] = 0.5, and ? [? = ?] = 0.99. Hypothesis H2.2: Only the training set needs to be biased for unfairness to arise. We set ? [? = ?] = 0.5 (no Group Size disparity) and ? [? = 1|? = ?] = 2 ? ? [? = 1|? = ?] (prevalence disparity). We also satisfy Bias Extension 2 by first injecting this bias into the training subset only, then test subet only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>H4: The selective label problem may have mixed effects on algorithmic fairness. Hypothesis H4.1: Noisy target labels can harm a protected group. We start off with ? [? = ?] = 1/? ? ? [? |? = ?] = ? [? |? = ?]. Then, we randomly flip the training labels of negative examples belonging to group A, such that ? * [? = 1|? = ?] = 2 * ? [? = 1|? = ?]. The test set remains untouched. In this case, group A is perceived as more fraudulent when in reality it is not.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Hypothesis H4. 2 :</head><label>2</label><figDesc>Noisy target labels can aid a protected group. This proposal is the inverse of H4.1. Instead of departing from an unbiased dataset, we generate ? such that ? [? |? = ?] = 2 * ? [? |? = ?]. Afterwards, we randomly flip the training labels of group A positive observations, until there are no longer disparities in prevalence: ? * [? |? = ?] = ? [? |? = ?].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Deep dive intoLGBM group-wise precision ratios. In contrast to H2.1, the median precision ratios for H3 and H4.1 are significantly skewed in favour of group B, meaning that models are better at classifying fraud in this group. In H3, this happens because group B fraud is easier to detect given ? 1 and ? 2 . In H4.1, some of group A's fraud labels are false, giving models more accurate information to classify observations that belong to group B. Furthermore, in H4.1, A is apparently more fraudulent than B (double the prevalence), contributing to a steeper FPR disparity than in H2.1 (see Figure6).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Median, minimum, and maximum performance and fairness levels for top LGBM, RF, MLP, and LR models on each dataset seed (all at 5% global FPR). Group sizes are always balanced except for H1. At a higher level, this shows how different types of bias yield distinct fairness-accuracy trade-offs. At a lower level, each algorithm exhibits particular trade-offs. For example, contrary to its counterparts, LR shows more balanced FPR rates on H2.1 than on H3. XGB is omitted because results were identical to LGBM. DT is omitted because performance was too low. Hypotheses Recap -Baseline: Unbiased setting -both group sizes are equal, no bias conditions nor extensions satisfied. H1: group size disparities alone do not threaten fairness (case shown is for group A representing 90% of the dataset). H2.x: Groups with higher fraud prevalence have higher error rates (in H2.1 both train and test sets are biased, and H2.x bars represent the case where A has double the fraud rate of B). H3: Algorithmic unfairness arises when groups leverage features unequally to avoid fraud detection.. H4.x: The selective label problem may have mixed effects on algorithmic fairness. (H4.1 studies harmful effects on fairness, and H4.2 beneficial ones).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Results for Hypothesis H1: group size disparities alone do not threaten fairness. Left plot: 50% group A, 50% group B. Middle plot: 90% group A, 10% group B. Right plot: 99% group A, 1% group B. Results obtained for a global threshold of 5% FPR. The center of the cross is the median of each metric, and each bar represents the minimum and maximum in each dimension. Slightly different samples from an unbiased data generation process may still lead to algorithmic unfairness in downstream prediction tasks.</figDesc><table><row><cell>FPR Ratio (A/B) (log2)</cell><cell>0 1 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0 1 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0 1 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Groupwise</cell><cell>2 1</cell><cell>0.5</cell><cell>0.6</cell><cell>Global TPR</cell><cell>0.7</cell><cell>XGB</cell><cell>0.8</cell><cell>1 2</cell><cell>0.5 LGBM</cell><cell>0.6 Algorithm Global TPR LR</cell><cell>0.7 DT</cell><cell>0.8 RF</cell><cell>1 2</cell><cell>0.5</cell><cell>MLP</cell><cell>0.6</cell><cell>Global TPR</cell><cell>0.7</cell><cell>0.8</cell></row><row><cell cols="3">Figure 4:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">5.4.1 Key Takeaways. Inaccurate labelling leads to harmful effects</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">if the disadvantaged group's prevalence is further increased (similar</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">to H2). Inaccurate labelling leads to beneficial effects if the disad-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">vantaged group's prevalence is decreased (label massaging [28]).</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">5.4.2 Fairness Metrics Interplay. When group A's prevalence is</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">artificially increased, together with its reduced precision due to</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">noisy labelling, predictive equality is skewed against group A. On</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">the other hand, when inaccurate labeling is used to artificially</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">equalize group-wise prevalence, models tend to fulfill fairness in</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">both predictive equality and equality of opportunity.</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A score is deemed calibrated if it reflects the likelihood of the input sample being positively labeled, regardless of the group an individual belongs to.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>For example, if A has double the FPR of B, that point in log scale will be at the same distance from the center (0) as its inverse. In a linear scale, that would not be the case -1 is farther away from 2 than from<ref type="bibr" target="#b0">1</ref> 2 .</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fraud detection system: A survey</title>
		<author>
			<persName><forename type="first">Aisha</forename><surname>Abdallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohd</forename><surname>Aizaini Maarof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anazida</forename><surname>Zainal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Network and Computer Applications</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="90" to="113" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Nil-Jana</forename><surname>Akpinar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Nagireddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Stapleton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao-Fei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoda</forename><surname>Heidari</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2204.10233</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2204.10233" />
		<title level="m">A Sandbox Tool to Bias(Stress)-Test Fairness Algorithms</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Machine Bias: There&apos;s software used across the country to predict future criminals. And it&apos;s biased against blacks</title>
		<author>
			<persName><forename type="first">Julia</forename><surname>Angwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Mattu</surname></persName>
		</author>
		<ptr target="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fairness in machine learning</title>
		<author>
			<persName><forename type="first">Solon</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS tutorial</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2017">2017. 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Big data&apos;s disparate impact</title>
		<author>
			<persName><forename type="first">Solon</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Selbst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Calif. L. Rev</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">671</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Consumer-lending discrimination in the FinTech era</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adair</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Financial Economics</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="30" to="56" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">William</forename><surname>Blanzeisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P?draig</forename><surname>Cunningham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14014</idno>
		<title level="m">Algorithmic Factors Influencing Bias in Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Artificial intelligence and machine learning in financial services: Market developments and financial stability implications</title>
	</analytic>
	<monogr>
		<title level="j">Financial Stability Board</title>
		<imprint>
			<biblScope unit="page">45</biblScope>
			<date type="published" when="2017">2017. 2017</date>
			<publisher>Financial Stability Board</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<title level="m">Classification and Regression Trees</title>
		<imprint>
			<publisher>CRC press</publisher>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Simon</forename><surname>Caton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Haas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04053</idno>
		<title level="m">Fairness in machine learning: A survey</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Joymallya</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suvodeep</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Menzies</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.12195</idno>
		<title level="m">Bias in Machine Learning Software: Why? How? What to do? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Irene</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Fredrik D Johansson</surname></persName>
		</author>
		<author>
			<persName><surname>Sontag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12002</idno>
		<title level="m">Why is my classifier discriminatory? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">XGBoost: A Scalable Tree Boosting System</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, California, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fair prediction with disparate impact: A study of bias in recidivism prediction instruments</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chouldechova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="153" to="163" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The measure and mismeasure of fairness: A critical review of fair machine learning</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Corbett-Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharad</forename><surname>Goel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00023</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Algorithmic Decision Making and the Cost of Fairness</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Corbett-Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Feller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharad</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aziz</forename><surname>Huq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 23rd ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining -KDD &apos;17</title>
		<meeting>of the 23rd ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining -KDD &apos;17<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="797" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Promoting Fairness through Hyperparameter Optimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Andr?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catarina</forename><surname>Saleiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Bel?m</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><surname>Bizarro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1036" to="1041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The mnist database of handwritten digit images for machine learning research</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="141" to="142" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The accuracy, fairness, and limits of predicting recidivism</title>
		<author>
			<persName><forename type="first">Julia</forename><surname>Dressel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hany</forename><surname>Farid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Advances</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A computer program used for bail and sentencing decisions was labeled biased against blacks. It&apos;s actually not that clear</title>
		<author>
			<persName><forename type="first">Avi</forename><surname>Feller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Corbett-Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharad</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Washington Post</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fairness Evaluation in Presence of Biased Noisy Labels</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Fogliato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chouldechova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max G'</forename><surname>Sell</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics (Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Silvia</forename><surname>Chiappa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roberto</forename><surname>Calandra</surname></persName>
		</editor>
		<meeting>the Twenty Third International Conference on Artificial Intelligence and Statistics ( Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="2325" to="2336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Equality of opportunity in supervised learning</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3315" to="3323" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edwardo</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1263" to="1284" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Moving beyond &quot;algorithmic bias is a data problem</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patter.2021.100241</idno>
		<ptr target="https://doi.org/10.1016/j.patter.2021.100241" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">100241</biblScope>
		</imprint>
	</monogr>
	<note>Patterns 2, 4 (2021</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The ugly truth about ourselves and our robot creations: the problem of bias and social inequity</title>
		<author>
			<persName><forename type="first">Ayanna</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Borenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science and engineering ethics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1521" to="1536" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Classifying without discriminating</title>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toon</forename><surname>Calders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 2nd International Conference on Computer, Control and Communication</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Data preprocessing techniques for classification without discrimination</title>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toon</forename><surname>Calders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lightgbm: A highly efficient gradient boosting decision tree</title>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3146" to="3154" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Inherent trade-offs in the fair determination of risk scores</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sendhil</forename><surname>Mullainathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Raghavan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05807</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fairness in credit scoring: Assessment, implementation and profit implications</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kozodoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lessmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">297</biblScope>
			<biblScope unit="page" from="1083" to="1094" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Selective Labels Problem: Evaluating Algorithmic Predictions in the Presence of Unobservables</title>
		<author>
			<persName><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sendhil</forename><surname>Mullainathan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3097983.3098066</idno>
		<ptr target="https://doi.org/10.1145/3097983.3098066" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Halifax, NS, Canada; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="275" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An Empirical Comparison of Bias Reduction Methods on Real-World Problems in High-Stakes Policy Settings</title>
		<author>
			<persName><forename type="first">Hemank</forename><surname>Lamba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kit</forename><forename type="middle">T</forename><surname>Rodolfa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rayid</forename><surname>Ghani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="69" to="85" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Survey on Bias and Fairness in Machine Learning</title>
		<author>
			<persName><forename type="first">Ninareh</forename><surname>Mehrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nripsuta</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3457607</idno>
		<ptr target="https://doi.org/10.1145/3457607" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1" to="115" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">What Happened in Hazelwood: Statistics, Employment Discrimination, and the 80% Rule</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Sacks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandy</forename><forename type="middle">L</forename><surname>Zabell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Bar Foundation Research Journal</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="139" to="186" />
			<date type="published" when="1984">1984. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Weapons of math destruction: How big data increases inequality and threatens democracy</title>
		<author>
			<persName><forename type="first">O'</forename><surname>Cathy</surname></persName>
		</author>
		<author>
			<persName><surname>Neil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<pubPlace>Crown</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Benchmarking Bias Mitigation Algorithms in Representation Learning through Fairness Metrics</title>
		<author>
			<persName><forename type="first">Charan</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soroush</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero-Soriano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Shabanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<ptr target="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/2723" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021</title>
		<editor>
			<persName><forename type="first">Joaquin</forename><surname>Vanschoren</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
		</editor>
		<meeting>the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021</meeting>
		<imprint>
			<date type="published" when="2021-12">2021. December 2021</date>
		</imprint>
	</monogr>
	<note>d092b63885e0d7c260cc007e8b9d-Abstract-round1.html</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dealing with Bias and Fairness in Data Science Systems: A Practical Hands-on Tutorial</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Saleiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kit</forename><forename type="middle">T</forename><surname>Rodolfa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rayid</forename><surname>Ghani</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394486.3406708</idno>
		<ptr target="https://doi.org/10.1145/3394486.3406708" />
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">Rajesh</forename><surname>Gupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">Aditya</forename><surname>Prakash</surname></persName>
		</editor>
		<meeting><address><addrLine>Virtual Event, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-08-23">2020. August 23-27, 2020</date>
			<biblScope unit="page" from="3513" to="3514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tabular data: Deep is not all you need</title>
		<author>
			<persName><forename type="first">Ravid</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Ziv</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Amitai</forename><surname>Armon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A unified approach to quantifying algorithmic unfairness: Measuring individual &amp;group unfairness via inequality indices</title>
		<author>
			<persName><forename type="first">Till</forename><surname>Speicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoda</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nina</forename><surname>Grgic-Hlaca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adish</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zafar</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2239" to="2248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Removing biased data to improve fairness and accuracy</title>
		<author>
			<persName><forename type="first">Sahil</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren?</forename><surname>Just</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03054</idno>
		<ptr target="https://arxiv.org/abs/2102.03054" />
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Estimation of the probability of an event as a function of several independent variables</title>
		<author>
			<persName><forename type="first">H</forename><surname>Strother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">B</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="167" to="179" />
			<date type="published" when="1967">1967. 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fair classification with groupdependent label noise</title>
		<author>
			<persName><forename type="first">Jialu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="526" to="536" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
