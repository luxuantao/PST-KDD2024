<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Winning the CVPR&apos;2021 Kinetics-GEBD Challenge: Contrastive Learning Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-22">22 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hyolim</forename><surname>Kang</surname></persName>
							<email>hyolimkang@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinwoo</forename><surname>Kim</surname></persName>
							<email>jinwoo-kim@yonsei.ac.kr</email>
						</author>
						<author>
							<persName><forename type="first">Kyungmin</forename><surname>Kim</surname></persName>
							<email>kyungminkim@yonsei.ac.kr</email>
						</author>
						<author>
							<persName><forename type="first">Taehyun</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hyundai Mobis</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Seon</forename><forename type="middle">Joo</forename><surname>Kim</surname></persName>
							<email>seonjookim@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Winning the CVPR&apos;2021 Kinetics-GEBD Challenge: Contrastive Learning Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-22">22 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2106.11549v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generic Event Boundary Detection (GEBD) is a newly introduced task that aims to detect "general" event boundaries that correspond to natural human perception. In this paper, we introduce a novel contrastive learning based approach to deal with the GEBD. Our intuition is that the feature similarity of the video snippet would significantly vary near the event boundaries, while remaining relatively the same in the remaining part of the video. In our model, Temporal Self-similarity Matrix (TSM) is utilized as an intermediate representation which takes on a role as an information bottleneck. With our model, we achieved significant performance boost compared to the given baselines. Our code is available at https://github.com/hello-jinwoo/LOVEU-CVPR2021.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Cognitive science suggests when human perceives a long-term video, he or she spontaneously parses the video in terms of "event" <ref type="bibr" target="#b14">[15]</ref>. To mimic this human visual perception, Generic Event Boundary Detection (GEBD) <ref type="bibr" target="#b15">[16]</ref>, which aims to detect general event boundaries that meet natural human criteria, is newly introduced. Unlike previous tasks such as Temporal Action Localization (TAL) <ref type="bibr" target="#b17">[18]</ref> or Shot Boundary Detection <ref type="bibr" target="#b2">[3]</ref>, GEBD's objective is to find out class-agnostic event boundaries, regardless of their categories. As the groundtruth boundaries in GEBD are only defined by human's unknown perception process, the task is subjective in nature, which makes it more challenging compared to other temporal detection tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>To solve GEBD, we devised a novel network that uses temporal self-similarity matrix (TSM) as its intermediate representation. The existence of an event boundary in a video implies that there is a visual content change at that point. So, our main hypothesis is that similarity among the video snippet features would significantly vary near the event boundaries while relatively consistent in the other part of the video. Accordingly, we may be able to infer where the action boundaries are by observing the TSM.</p><p>Assuming the hypothesis is true, a straightforward approach would be an neural network architecture similar to <ref type="bibr" target="#b8">[9]</ref>. However, we found that BinaryCrossEntropy loss cannot provide sufficient gradients to form distinguishable TSM, especially when we adopt Transformers-based encoder architecture. To compensate for insufficient gradient, we explicitly exploit the aforementioned hypothesis by employing the popular contrastive learning method <ref type="bibr" target="#b6">[7]</ref> that showed promising performance in self-supervised learning. With auxiliary contrastive loss, TSM can deliver more representative feature in terms of event boundary detection.</p><p>On the other hand, a direct approach without using TSM could provide a prediction with different perspective, which can improve overall model performance when combined with TSM-based prediction. To this end, our final model (Figure <ref type="figure" target="#fig_0">1</ref>) has two passes: i) encoder-TSM-decoderprediction (TSM pass) and ii) encoder-prediction (direct pass). Both passes share the encoder part, and the convex combination of predictions from two passes is considered as a final output.</p><p>With our model and some training tricks, we achieved substantial performance boost compared to the given baselines in Kinetics-GEBD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Temporal Self-Similarity Matirx (TSM)</head><p>There have been notable works that used TSM to human action recognition <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19]</ref> as it can robustly express frame similarity regardless of large view point change if paired with an appropriate feature extractor. Especially, <ref type="bibr" target="#b8">[9]</ref> proposed a novel TSM-based network architecture, RepNet, to solve action counting problem. Note that in RepNet, the frame feature extractor is solely trained with the main task loss, while ours additionally utilized auxiliary contrastive loss term to reinforce weak gradient signal from the main (BinaryCrossEntropy) loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Contrastive Learning</head><p>In contrastive learning, positive samples are attracted together while the negative ones are repulsed. In spite of its frustratingly simple idea, it has shown clear performance gain in the self-supervised learning domain, resulting in seminal works including <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5]</ref>. Among them, our model was particularly inspired by <ref type="bibr" target="#b6">[7]</ref>, which adopted a simple siamese network with additional prediction head and "StopGradient" operator to conduct contrastive learning without negative samples. Yet we borrowed the architectural concept from <ref type="bibr" target="#b6">[7]</ref>, our model still exploits negative samples in the contrastive loss term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Encoder</head><p>To capture various aspects of the given feature sequence, our encoder consists of a number of parallel modules each of which enjoys different receptive field. For instance, 1D Convs with small/intermediate kernel size are used to cap- Furthermore, it would worth noting that the output features from each module are NOT concatenated until they are respectively converted to TSM (or pairwise similarity score matrix for the contrastive loss) form. Only after the PS operation in Figure <ref type="figure" target="#fig_0">1</ref>, channelwise stacking operation of similarity matrices takes place, resulting in 12 channels in the TSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Contrastive Loss</head><p>For contrastive learning, we assume local similarities between video features, unless there is an event boundary between them. Concrete instantiation of the idea is given in Figure <ref type="figure" target="#fig_1">2</ref> (a). Similarity scores between two distant features (in Figure <ref type="figure" target="#fig_1">2</ref> (a), valid local range is set to 4), and between boundary element and the others are marked as neutral samples because of their ambiguity, while other local scores are marked as positive or negative samples.</p><p>When we calculate the contrastive loss term, we just use positive/negative samples, ignoring Let i k and j k denote a kth positive/negative sample respectively and m and n represent the number of positive/negative samples. Then, the contrastive loss term L contra is defined as follows:</p><formula xml:id="formula_0">L contra = 1 m m k=1 j k - 1 n n k=1 i k (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Other Modules</head><p>To begin with, let L stand for the feature length. As a backbone architecture for the decoder, ResNet-18 architectue is slightly modified to take (B, 12, L, L) tensor as its input and output the (B, C decoder , L, L) size tensor. From (B, C decoder , L, L) tensor, we gather diagonal elements, resulting in a tensor with shape (B, C decoder , L). The boundary predictions are made with the shallow Conv1d classification head, which takes (B, C decoder , L) tensor as its input.</p><p>On the other hand, we used Transformer encoder architecture to the classification head in the direct pass, since it should directly process relatively high-dimensional features compared to the classification head in TSM pass. The Simsiam head is just two-layer 1D Convs with kernel size = 1. A convex combination parameter in Figure <ref type="figure" target="#fig_0">1</ref>'s weighted sum operation is learnt by gradient from BCE loss. Final loss term is a linear combination of three BCE loss and L contra .</p><p>For more detailed architectural information, please refer our released code https://github.com/hello-jinwoo/LOVEU-CVPR2021.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model f1 score</head><p>Baseline PC <ref type="bibr" target="#b15">[16]</ref> 62.5</p><p>Ours Direct pass 78.0 (+15.5) TSM pass w/o CL 74.3 (+11.8) TSM pass w/ CL 81.0 (+18.5) Direct pass + TSM pass w/ CL 81.3 (+18.8) Table <ref type="table">1</ref>. Comparison of our methods with baseline PC model and ablation study for each module in our model. CL stands for the Contrastive Loss. Note that we conducted this experiment on one of the five validation splits according to Section 4.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments 4.1. Dataset and Feature</head><p>As the competition is targetting on the Kinetics-GEBD dataset, we only used the Kinetics-GEBD data for training. No additional data or annotation is exploited during training procedure, regardless of track 1.1 and track 1.2.</p><p>Following the prevailing convention in Online Action Detection (OAD) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8]</ref> and Temporal Action Localization (TAL) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b1">2]</ref>, pre-trained feature extractor is applied to extract snippet-wise features. For track 1.2, we used Kinetics pre-trained two-stream TSN <ref type="bibr" target="#b19">[20]</ref> and SlowFast <ref type="bibr" target="#b9">[10]</ref> features, while for track 1.1 we additionally used Activitynet <ref type="bibr" target="#b3">[4]</ref> pre-trained TSP <ref type="bibr" target="#b0">[1]</ref> features. For clarification, we will leave the source from which we downloaded the pre-trained weights on the footnote. 1 2 3   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Table <ref type="table">1</ref> illustrates the result of ablation study for our model. As shown in the table, each component of our model helps improve the performance when it is combined with each other. Especially, there is a huge gap between our full model and the model without TSM pass (+3.3 in f1 score). Besides, we found that TSM pass without CL (Contrastive Loss) does not work well, indicating that the sole BCE loss cannot deliver sufficient gradient to train the whole network. We achieved our best performance model by incorporating both direct pass and TSM pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Miscellaneous</head><p>In this section, we introduce several tricks to further ameliorate the final performance.</p><p>Post-processing. To prevent duplicate predictions of adjecent frames, we use peak-estimation technique after the network estimation. We predicted a frame as a boundary frame if and only if its probability score is higher than the neighboring K (we set K as 1 or 2) ones. To prune some low-probability peak from the prediction, we set the threshold so that the frame having probability score lower than the threshold is not predicted as a boundary frame.</p><p>Multiple Dataset Splits. We made new train/val splits with larger train set size for the better use of the dataset. Specifically, we arbitrarily split train+val set into 5 equal-sized, non-overlapping groups. Then, we form 5 sets of train/val split by taking one group as a validation split and the rest of them as a training split. We train a model on each split, resulting in 5 models in total, and take the mean value of their output to obtain the final prediction.</p><p>Model Ensembles. We ensembled some predictions to boost the performance of our model and prevent it from overfitting on validation sets. We used the predictions from all 5 folds, and a simple additional network consisting of two branch, one of which contains Transformer encoder and RNN decoder, and the other of which contains only CNN layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduced a novel method to solve newly-introduced GEBD task. Our model accomplished compelling performance gain compared to the given baselines. However due to lack of time, we could not conduct exhaustive ablation study. For future work, we will do extensive experiments to strengthen our idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>This work was supported by Hyundai Mobis Co., Ltd.(CTR210200164).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Overall structure of our model. Consecutive video frames are converted to video features (represented as gray bars in the figure) by a pre-trained network (omitted for brevity). As the green and red arrows show, the encoder receives gradient signals from both contrastive loss and BCE loss, allowing TSM to serve as a richer intermediate representation.</figDesc><graphic url="image-1.png" coords="2,413.72,246.51,55.53,57.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (a) explains how we define positive/negative/neutral sample for our contrastive loss (local range=4). '#' sign at the center represents an event boundary, while blue'+'/red'-'/gray'0' cell denote positive/negative/neutral samples respectively. (b) is a sample of an actual score matrix that is used calculate the contrastive loss. A ternary mask that looks like (a) is applied to (b) to compute the contrastive loss. Note that (b) is slightly different from the TSM that goes into the decoder since it is pairwise similarities between the original encoded feature and encoded feature that has gone through the additional simsiam heads.</figDesc><graphic url="image-3.png" coords="2,425.86,373.81,119.35,119.35" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>SlowFast: R50-8x8 from pytorchvideo</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>TSN: Inference code is based on anet2016-cuhk<ref type="bibr" target="#b20">[21]</ref> and the model weight is from here (Inception V3)<ref type="bibr" target="#b19">[20]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>TSP: https://github.com/HumamAlwassel/TSP</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tsp: Temporally-sensitive pretraining of video encoders for localization tasks</title>
		<author>
			<persName><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11479</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Boundary content graph neural network for temporal action proposal generation</title>
		<author>
			<persName><forename type="first">Yueran</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shot and scene detection via hierarchical clustering for reusing broadcast video</title>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Costantino</forename><surname>Grana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Analysis of Images and Patterns</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cees Snoek, and Tinne Tuytelaars</title>
		<author>
			<persName><forename type="first">Roeland</forename><surname>De Geest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Online action detection</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Counting out time: Class agnostic video repetition counting in the wild</title>
		<author>
			<persName><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Pierre H Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Gheshlaghi Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">View-independent action recognition from temporal self-similarities</title>
		<author>
			<persName><forename type="first">N</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilie</forename><surname>Junejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Dexter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="185" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Temporal selfsimilarity for appearance-based action recognition in multiview setups</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>K?rner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Analysis of Images and Patterns</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast learning of temporal action proposal via dense boundary generator</title>
		<author>
			<persName><forename type="first">Chuming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The Oxford handbook of cognitive psychology</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Reisberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Generic event boundary detection: A benchmark for event segmentation</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Zheng Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><forename type="middle">W</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.10511</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploring sparseness and self-similarity for action recognition</title>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imran</forename><forename type="middle">Nazir</forename><surname>Junejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marshall</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2488" to="2501" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cuhk &amp; ethz &amp; siat submission to activitynet challenge</title>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ActivityNet Large Scale Activity Recognition Challenge</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporal recurrent networks for online action detection</title>
		<author>
			<persName><forename type="first">Mingze</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">G-tad: Sub-graph localization for temporal action detection</title>
		<author>
			<persName><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">S</forename><surname>Rojas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end learning of action detection from frame glimpses in videos</title>
		<author>
			<persName><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
