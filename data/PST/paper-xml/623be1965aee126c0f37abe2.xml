<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unified Structure Generation for Universal Information Extraction</title>
				<funder ref="#_pCHYE9y #_kA5F3SS #_5GBycka">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_4u8h6A2">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
							<email>yaojie2017@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dai</forename><surname>Dai</surname></persName>
							<email>daidai@baidu.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
							<email>xiaoxinyan@baidu.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
							<email>hongyu@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
							<email>xianpei@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">State Key Laboratory of Computer Science Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Beijing Academy of Artificial Intelligence</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
							<email>sunle@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Chinese Information Processing Laboratory</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">State Key Laboratory of Computer Science Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
							<email>wu_hua@baidu.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Baidu Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unified Structure Generation for Universal Information Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Information extraction suffers from its varying targets, heterogeneous structures, and demandspecific schemas. In this paper, we propose a unified text-to-structure generation framework, namely UIE, which can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources. Specifically, UIE uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions via a schema-based prompt mechanism -structural schema instructor, and captures the common IE abilities via a large-scale pretrained text-to-structure model. Experiments show that UIE achieved the state-of-the-art performance on 4 IE tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings for a wide range of entity, relation, event and sentiment extraction tasks and their unification. These results verified the effectiveness, universality, and transferability of UIE 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Information extraction (IE) aims to identify and structure user-specified information from unstructured texts <ref type="bibr" target="#b1">(Andersen et al., 1992;</ref><ref type="bibr" target="#b9">Grishman, 2019)</ref>. IE tasks are highly diversified due to its varying targets (entity, relation, event, sentiment, etc.), heterogeneous structures <ref type="bibr">(spans, triplets, records, etc.)</ref>, and demand-specific schemas <ref type="bibr" target="#b10">(Grishman and Sundheim, 1996;</ref><ref type="bibr" target="#b34">Mitchell et al., 2005;</ref><ref type="bibr" target="#b12">Ji and Grishman, 2011)</ref>.</p><p>Currently, most IE approaches are taskspecialized, which leads to dedicated architectures, isolated models, and specialized knowl- edge sources for different IE task. These taskspecialized solutions greatly hinder the rapid architecture development, effective knowledge sharing, and quick cross-domain adaptation of IE systems. First, it is very complicated to develop dedicated architectures for a large amount of IE tasks/settings/scenarios. Second, learning isolated models severely restricts the knowledge sharing between related tasks and settings. Finally, it is costly and time-consuming to construct data sets and knowledge sources specialized for different IE tasks. Therefore, it will be of great benefit to develop a universal IE architecture that can uniformly model different IE tasks, adaptively predict heterogeneous structures and effectively learn from various resources, which we referred to as Universal IE.</p><p>Fundamentally, all IE tasks can be modeled as text-to-structure transformations, with different arXiv:2203.12277v1 [cs.CL] 23 Mar 2022 tasks correspond to different structures. For example, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, an entity is a named span structure, an event is a schema-defined record structure. These text-to-structure transformations in IE can be further decomposed into several atomic transformation operations: 1) Spotting, which locates the desirable spans concerning to given specific semantic types <ref type="bibr" target="#b14">(Kripke and Munitz, 1971;</ref><ref type="bibr" target="#b3">Chen and Yuille, 2004)</ref>. For example, locating span "Steve" as a Person entity and locating "excited" as a sentiment expression. 2) Associating, which connects spans by assigning them with semantic roles in pre-defined schemas <ref type="bibr" target="#b35">(Onyshkevych, 1994;</ref><ref type="bibr" target="#b33">Milward and Thomas, 2000)</ref>. For example, associating "Steve" and "Apple" by assigning them as the Arg1 and the Arg2 of a Work-for relation. In this way, different IE tasks can be decomposed into a sequence of atomic text-to-structure transformations, and all IE models share the same underlying spotting and associating abilities. For example, entity extraction can be viewed as spotting mention spans of corresponding entity types, while event detection can be reformulated as spotting triggers spans with event types. And the spotting abilities can be shared between these two tasks.</p><p>Based on the above observations, we propose UIE, a unified text-to-structure generation architecture that can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources. Specifically, to model heterogeneous IE structures, we design a structural extraction language (SEL) that can effectively encode different IE structures into a uniform representation, so that various IE tasks can be universally modeled in the same text-to-structure generation framework. To adaptively generate targeted structures for different IE tasks, we propose structural schema instructor (SSI), a schema-based prompt mechanism which controls what to spot, what to associate, and what to generate in UIE. To learn common IE abilities for UIE, we pre-train UIE on large-scale, heterogeneous datasets mined from easily accessible web sources. The large-scale pretrained UIE model provides a solid foundation for knowledge sharing and quick adaptation to new IE settings, and significantly boosts the IE performance in all supervised, low-resource, and fewshot settings.</p><p>We conduct experiments on 13 datasets of 4 main IE tasks (entity/relation/event/sentiment ex-traction and their unification), and supervised, lowresource, and few-shot settings. Experiment results show that UIE achieves significant improvements in all settings. On supervised settings, UIE achieved 1.42% F1 scores improvements over the state-of-the-art, task-specialized architectures on all datasets. On few-shot and low-resource settings, UIE exhibits strong on-demand adaptation ability: it outperforms baselines dramatically by a large margin. These results verified the effectiveness, universality, and transferability of UIE across different IE tasks, settings, and scenarios.</p><p>The main contributions of this paper are: 1) We propose UIE, a unified text-to-structure generation architecture that can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources.</p><p>2) We design a unified structure generation network, which encodes heterogeneous IE structures into a uniform representation via a structural extraction language, and controls the UIE model which to spot, which to associate, and which to generate via structural schema instructor mechanism.</p><p>3) We pre-train a large-scale text-to-structure generation model via a unified pre-training algorithm. To the best of our knowledge, this is the first text-to-structure pre-trained extraction model, which can benefit future IE studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Unified Structure Generation for Universal Information Extraction</head><p>Information extraction tasks can be formulated as text-to-structure problems, where different IE tasks correspond to different structures. This paper aims to uniformly model the text-to-structure transformations of different IE tasks via a single framework, i.e., different structure transformations will share the same underlying operations and different transformation abilities in a universal model. Formally, given a specific pre-defined schema s and texts x, a universal IE model needs to generate a structure that contains the desirable structural information in the text x indicated by the schema s.</p><p>Generally, there are two main challenges here. Firstly, due to the diversity of IE tasks, there are many different target structures to extract, e.g., entity, relation, event, etc. Secondly, IE tasks are often demand-specific which are defined using different schemas, therefore we need to adaptively control the extraction process.  In this section, we describe how to jointly formulate, learn, and conduct various IE tasks in a unified text-to-structure generation architecture, named UIE. Specifically, we first design structured extraction language (SEL) to uniformly encode heterogeneous extraction structures, i.e., encode entity, relation, event into a unified representation. Then we describe structural schema instructor (SSI), a schema-based prompt mechanism that controls the UIE model which to spot, which to associate, and which to generate for different extraction settings. The details are as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Structured Extraction Language for Uniform Structure Encoding</head><p>This section describes how to encode heterogeneous IE structures into a uniform representation. Based on the above discussions, IE structure generation can be decomposed into two atomic operations:</p><p>1. Spotting indicates locating target information pieces from the sentence, e.g., the entity and the trigger word in the event. 2. Associating indicates connecting different information pieces based on the desirable associations, e.g., the relation between entity pair or the role between event and its argument. Then different IE structures can be represented as a combination of atomic structure generation operations.</p><p>Concretely, we design a unified structured ex-traction language (SEL), which encodes different IE structures via the spotting-associating structure.</p><p>As shown in Figure <ref type="figure" target="#fig_2">2a</ref>, each SEL expression contains three types of semantic units: 1) SPOTNAME represents there is a specific information piece with the type of spot name existing in the source text; 2) ASSONAME indicates there exists a specific information piece in the source text that is with the AssoName association to its upper-level Spotted information in the structure; 3) INFOSPAN represents the text span corresponding to the specific spotting or associating information piece in the source text. Furthermore, ":" in the SEL indicates the mapping from InfoSpan to its spotting or associating names, and the two structure indicators "(" and ")" are used to form the hierarchical structure between the extracted information.</p><p>Using SEL, Figure <ref type="figure" target="#fig_2">2b</ref> shows how to represent entity, relation, and event structures. There are three entities and each entity is represented as a spotting structure such as "person:Steve", "organization:Apple", and "time:1997"; one relation which is represented as an association structure between "Steve" and "Apple" with association name work for; and one event which is represented as an association structure, where the trigger is a spotting structure "start-position:became", and its arguments are associated with the trigger: Steve as employee, Apple as employer, 1997 as time.</p><p>We can see that, SEL have the advantages that: 1) uniformly encodes varying IE structures, therefore different IE tasks can be modeled as the same text-to-structure generation process; 2) efficiently represents all extraction results of a sentence in the same structure, thus can perform joint extraction naturally; 3) the output structure of generation is very compact, which greatly reduce the complexity of decoding.</p><p>For example, the two different tasks entity recognition and event detection can be revisited using the same "(SpotName: InfoSpan)" grammar. While both relation extraction and event extraction can be formulated using the grammar "(SpotName: In-foSpan (AssoName: InfoSpan), ...)", even they are with totally different binary "entity-relation-entity" and N-ary "event-arguments" structures. Such a unified structured extraction language enables UIE to learn from and adapt to different IE tasks without designing task-specialized architectures, because these IE tasks are all universally formulated as the transformation from texts to SEL representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UIE [spot] person [asso] work for [text]</head><p>Steve became CEO of Apple in 1997.</p><p>[spot] start-position ? [asso] employee ? <ref type="bibr">[text]</ref> Steve became CEO of Apple in 1997.</p><p>[  <ref type="bibr">: 1997)</ref>)", and an event extraction system will generate "(start position: became (employee: Steve) (employer: Apple))". To this end, we propose structural schema instructor (SSI), a schemabased prompt mechanism that controls which kinds of information need to be spotted and associated.</p><p>Figure <ref type="figure">3</ref> shows the overall framework of UIE. Formally, UIE takes the given structural schema instructor (s) and the text sequence (x) as input, and generates the linearized SEL (y) which contains the extracted information from x based on schema s:</p><formula xml:id="formula_0">y = UIE(s ? x)<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">x = [x 1 , ..., x |x| ] is the text sequence, s = [s 1 , ..., s |s| ]</formula><p>is the structural schema instructor, and y = [y 1 , ..., y |y| ] is a SEL sequence that can be easily converted into the extracted information record.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Structural Schema Instructor</head><p>To describe the extraction target of a task, the structural schema instructor constructs a schema-based prompt and uses it as a prefix during generation. Specifically, corresponding to the spottingassociation structure, the structural schema instructor contains three types of token segments: 1) SPOT-NAME: the targeted spotting name in the specific information extraction task, such as "person" in the NER task; 2) ASSONAME: the targeted association name, such as "work for" in the relation extraction task; 3) Special Symbols ([spot], [asso],</p><p>[text]) which are added before each SPOTNAME, ASSONAME, and input text sequence. All tokens in SSI are concatenated and put before the original text sequences. As shown in Figure <ref type="figure">3</ref>, the entire input for UIE is in the form of:</p><formula xml:id="formula_2">s ? x = [s 1 , s 2 , ..., s |s| , x 1 , x 2 , ..., x |x| ] = [[spot], ...[spot]..., [asso], ..., [asso]..., [text], x 1 , x 2 , ..., x |x| ] (2)</formula><p>For example, the SSI "[spot] person [spot] company [asso] work for [text]" indicates extracting records of the relation schema "the person works for the company" from the sentence. Given the SSI s, UIE first encodes the text x, then generates the target record y in linearized SEL using an encoderdecoder-style architecture.</p><p>We found that the schema-based prompt can: 1) effectively guide the SEL generation of UIE, so that the general IE ability can be transferred to new IE tasks; 2) adaptively control which to spot, which to associate, and which to generate, so that semantic knowledge across different labels and tasks can be better shared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Structure Generation with UIE</head><p>Given SSI s and text x as input, UIE extracts targeted information by generating a linearized SEL. We formulate this text-to-SEL generation process using an encoder-decoder-style architecture. Given the raw text sequence x and the schema instructor s, UIE first compute the hidden representation H = [s 1 , ..., s |s| , x 1 , ..., x |x| ] of each token:</p><formula xml:id="formula_3">H = Encoder(s 1 , ..., s |s| , x 1 , ..., x |x| ) (3)</formula><p>where Encoder(?) is a Transformer encoder. Then UIE will decode the input text into a linearized SEL in an auto-regressive style. At the step i of decoding, UIE generates the i-th token y i in the SEL sequence and the decoder state h d i as following:</p><formula xml:id="formula_4">y i , h d i = Decoder([H; h d 1 , ..., h d i-1 ])<label>(4)</label></formula><p>Decoder(?) is a transformer decoder, which predicts the conditional probability p(y i |y &lt;i , x, s) of token y i . Finally, Decoder(?) finishes prediction when outputting the end symbol &lt;eos&gt;, then we convert the predicted SEL expression into the extracted information record. Compared with previous IE studies which treat labels as specific symbols, the text-to-structure generation paradigm treats labels as natural language tokens. By verbalizing and generating labels and structures, our method can effectively transfer knowledge from pre-trained language models such as BART <ref type="bibr" target="#b18">(Lewis et al., 2020)</ref>, T5 <ref type="bibr" target="#b41">(Raffel et al., 2020)</ref>, and related tasks can easily share knowledge because their labels have similar semantics (e.g., location and place) and share common label-text associations (e.g., victim for different event types).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pre-training and Fine-tuning for UIE</head><p>In this section, we describe: 1) how to pre-train a large-scale UIE model which captures common IE abilities for different IE tasks; 2) how to adapt UIE to different IE tasks in different settings via quick fine-tuning. Specifically, we first collect several large-scale datasets from the Web, including structured (e.g., knowledge bases), unstructured (e.g., raw texts), and parallel (e.g., Wikipedia-Wikidata links) data, then we uniformly pre-train our UIE model on these heterogeneous datasets. Finally, we adapt the pre-trained UIE model to the specific downstream IE tasks via on-demand fine-tuning. We found that the pre-trained UIE model provides a solid foundation for capturing, sharing, and transferring knowledge between different IE tasks, and new IE tasks can be effectively solved because UIE learns general IE ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-training Corpus Construction</head><p>UIE needs to encode the text, map text to structure, and decode valid structure. Therefore, we collect a large-scale pre-training corpus from easily accessible web data sources (more details are in the appendix):</p><p>D pair is the text-structure parallel data, where each instance is a parallel pair (token sequence x, structured record y). We collect large-scale parallel text-structure pairs by aligning Wikidata with English Wikipedia. D pair is used to pre-train the text-to-structure transformation ability of UIE.</p><p>D record is the structure dataset where each instance is structured record y. We collect structured records from ConceptNet <ref type="bibr" target="#b48">(Speer et al., 2017)</ref> and Wikidata. D record is used to pre-train the structure decoding ability of UIE.</p><p>D text is the unstructured text dataset, and we use all plain texts in English Wikipedia. D text is used to pre-train the semantic encoding ability of UIE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-training</head><p>We pre-train UIE using three sequence generation tasks with above mentioned pre-training datasets.</p><p>Text-to-Structure Pre-training using D pair . To capture the fundamental text-to-structure mapping ability, we pre-train UIE using D pair = {(x, y)}. Specifically, for each parallel pair (x, y), we extract the spot type s s+ and the associating type s a+ in the record y as the positive schema s + = s s+ ? s a+ . However, we found that if we only feed UIE with a positive schema, it will only simply remember the triplet in the pre-training data. To learn general mapping ability, we also automatically construct negative schemas for each pair, i.e., we first sample negative spots s s-and negative association set s a-, then concatenate meta-schema s meta = s + ? s s-? s a-, and construct the final extraction target. For example, person and work for is the positive schema in the record "((person: Steve (work for: Apple)))", and we sample vehicle and located in as the negative schema to construct metaschema. Finally, the objective of text-to-structure pre-training is:</p><formula xml:id="formula_5">L Pair = (x,y)?D pair -log p(y|x, s meta ; ? e , ? d ) (5)</formula><p>where ? e and ? d are the parameter of encoder and decoder, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure Generation Pre-training with D record .</head><p>To pre-train the ability of generating valid structures defined by SEL and schemas, we pre-train UIE on D record . We pre-train UIE decoder as an structured language model, where each record in D record is an expression of SEL:</p><formula xml:id="formula_6">L Record = y?D record -log p(y i |y &lt;i ; ? d ) (6)</formula><p>By pre-training for structure generation, the decoder can capture the regularity of SEL and the interactions between different labels.</p><p>Retrofitting Semantic Representation using D text . During text-to-structure pre-training, we continually pre-train UIE also with the masked language model tasks <ref type="bibr" target="#b41">(Raffel et al., 2020)</ref> on D text to retrofit semantic representations of UIE. Specifically, we add span corruption based mask language modeling objective in the pre-training stage:</p><formula xml:id="formula_7">L Text = x?Dtext -log p(x |x ; ? e , ? d ) (7)</formula><p>where x is the corrupted source text and x is corrupted target spans. We found this pre-training can effectively alleviate the catastrophic forgetting of token semantics especially on SPOTNAME and ASSONAME tokens.</p><p>Final Pre-training Criteria. We initialize UIEbase and UIE-large with T5-v1.1-base and T5-v1.1large <ref type="bibr" target="#b41">(Raffel et al., 2020)</ref>, and the model architectures are shown in Table <ref type="table">7</ref>. The final objective is the combine of the above tasks:</p><formula xml:id="formula_8">L = L Pair + L Record + L Text (8)</formula><p>For implementation, we uniformly represent all pre-training data as triplets. For text data (x) in D text , we build a triplet (None, x , x ) where x is the corrupted source text and x is corrupted spans.</p><p>For text-record data (x, y) in D pair , we construct (s, x, y) by sampling the meta-schema s for each textrecord pair. For record data (y) in D record , we take (None, None, y) as the input triplet. We randomly pack instances for different tasks in one batch, and details are shown in Algorithm 1 in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">On-Demand Fine-tuning</head><p>Given the pre-trained UIE model, we can quickly adapt it to different IE tasks and settings through model fine-tuning. Given a labeled corpus D task = {(s, x, y)}, we fine-tune the UIE model using teacher-forcing cross-entropy loss:</p><formula xml:id="formula_9">L FT = (s,x,y)?D Task -log p(y|x, s; ? e , ? d ) (9)</formula><p>To alleviate the exposure bias <ref type="bibr" target="#b42">(Ranzato et al., 2016;</ref><ref type="bibr">Zhang et al., 2020)</ref> of the auto-regressive model during decoding, we also design a Rejection Mechanism for effective fine-tuning. Specifically, given an instance (s, x, y), we first encode y using SEL language, then we randomly insert several <ref type="bibr">[NULL]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To verify the effectiveness of UIE, we conducted experiments on different IE tasks and settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets. We conduct experiments on 13 IE benchmarks across 4 well-representative IE tasks (including entity extraction, relation extraction, event extraction, structured sentiment extraction) and their combinations (e.g., joint entity-relation extraction). The used datasets includes ACE04 <ref type="bibr" target="#b34">(Mitchell et al., 2005)</ref>, ACE05 <ref type="bibr" target="#b54">(Walker et al., 2006)</ref>; CoNLL03 <ref type="bibr" target="#b52">(Tjong Kim Sang and De Meulder, 2003)</ref>, CoNLL04 <ref type="bibr" target="#b45">(Roth and Yih, 2004)</ref>, Sci-ERC <ref type="bibr" target="#b30">(Luan et al., 2018)</ref>, NYT <ref type="bibr" target="#b44">(Riedel et al., 2010)</ref>, CASIE <ref type="bibr" target="#b46">(Satyapanich et al., 2020)</ref>, SemEval-14 <ref type="bibr" target="#b39">(Pontiki et al., 2014)</ref>, SemEval-15 <ref type="bibr" target="#b38">(Pontiki et al., 2015)</ref>, SemEval-16 <ref type="bibr" target="#b37">(Pontiki et al., 2016)</ref>, see Table 8 for detail. We employ the end-to-end setting for all extraction tasks, which takes the raw text as input and directly generates the target structure.</p><p>Evaluation. We use the same evaluation metrics as all previous methods, and details of metrics are shown in the appendix. For each fine-tuning experiment, we report the average performance on 3 random seeds. Because UIE only generates text spans, we map spans to offsets by finding the first matched offsets that are not already matched in the same SEL hierarchical level (details in appendix). We found this simple heuristic rule is very effective (&lt;0.5% error offsets) and more complicated mapping approaches (such as attention-weight guided span mapping) are left as the future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments on Supervised Settings</head><p>UIE provides a universal backbone for IE tasks. This section assesses the UIE performance in supervised settings. We compare UIE with the stateof-the-art, task-specific supervised models. For a fair comparison, we only compare the state-of-theart without leveraging additional dataset-specific knowledge or larger-scale contexts. These extensions are good complementary of UIE, and can be left for further improvement. Table <ref type="table" target="#tab_2">2</ref> shows the performance of UIE on the 13 IE datasets across 4 tasks. We can observe that: 1) By modeling IE as text-to-structure generation and encoding with an effective SEL language, UIE provides an effective universal architecture for IE. The UIE model achieves state-of-the-art performance on nearly all datasets and tasks, even without pre-training (SEL). 2) The large-scale pretrained model provides a solid foundation for universal IE. Compared with baselines, the pre-trained model achieves the performance of the state-of-theart in most datasets and improves 1.42% F1 on average. 3) By universally modeling IE tasks and pre-training using large-scale datasets, UIE can effectively capture, share, and transfer IE abilities. Pre-training improves all tasks at the same time, especially events and sentiment knowledge rarely appear in the pre-train dataset. It proves that SEL is a unified and cross-task transferable structured representation for IE, which allows UIE to share learned capabilities and information across different and various information extraction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on Low-resource Settings</head><p>To verify the quick adaptation ability of UIE, we conducted low-resource experiments on six different partitions of the original training sets (1/5/10shot, 1/5/10% ratio) across 4 tasks. For the fewshot experiments, we sample 1/5/10 sentences for each entity/relation/event/sentiment type in the training set. To avoid the influence of random sampling, we repeated each experiment 10 times with different samples and reported their averaged results as previous works <ref type="bibr" target="#b11">(Huang et al., 2021)</ref>.</p><p>We compare UIE with the following pre-trained model: 1) T5-v1.1-base is an initial model of UIEbase; 2) Fine-tuned T5-base is fine-tuned with sequence generation tasks such as summarization, which have been shown effective in many lowresource NLP tasks <ref type="bibr">(Paolini et al., 2021)</ref>; 3) UIEbase w/o SSI is the distant supervised version of UIE without SSI in the pre-training stage, which is used to verify the necessity of SSI when adapting UIE in low-resource settings. Table <ref type="table" target="#tab_3">3</ref> shows the performance of 4 IE tasks under 6 low-resource settings. We observe that: 1) By guiding the generation using schema-based prompts, SSI is an effective way for adaptively controlling which to ex- To investigate the effect of different pre-training tasks, Table <ref type="table">4</ref> shows ablation experiment results of UIE-base on four downstream tasks. We can L Pair enables the model to learn the ability of extraction. After ablating L Pair , the extraction ability of UIE is significantly decreased, i.e., the performance on the relation (-0.90), event (-1.43/-1.48), and sentiment (-0.46) tasks all see large decline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effects of Rejection Noise</head><p>This section investigates the effect of the proposed rejection noise. Table <ref type="table">5</ref> shows the results of the different pre-trained models on the development set of CoNLL 03 under the 10-shot setting. The mis-generated label has a negative influence on the precision of the proposed generation method leading to a large number of error extraction results.</p><p>The proposed rejection noise is useful for the generation method, which leads to improvements of 13.16 precision (P) on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Building and pre-training universal models of NLP tasks has attracted a lot of attention in recent years, e.g., contextualized representation <ref type="bibr" target="#b7">(Devlin et al., 2019;</ref><ref type="bibr" target="#b27">Liu et al., 2019</ref>), text generation <ref type="bibr" target="#b18">(Lewis et al., 2020;</ref><ref type="bibr" target="#b41">Raffel et al., 2020)</ref>, multi-modal <ref type="bibr">(Li et al., 2021b;</ref><ref type="bibr" target="#b4">Cho et al., 2021)</ref>, and multi-lingual <ref type="bibr" target="#b5">(Conneau et al., 2020;</ref><ref type="bibr">Xue et al., 2021)</ref>. This paper proposes and pre-trains the first universal model for information extraction. IE is a long-researched area and many classical neural architectures have been proposed, such as sequence tagging <ref type="bibr" target="#b15">(Lample et al., 2016;</ref><ref type="bibr" target="#b69">Zheng et al., 2017;</ref><ref type="bibr" target="#b24">Lin et al., 2019)</ref>, span classification <ref type="bibr" target="#b47">(Sohrab and Miwa, 2018;</ref><ref type="bibr" target="#b23">Lin et al., 2018;</ref><ref type="bibr" target="#b53">Wadden et al., 2019)</ref>, and MRC <ref type="bibr" target="#b17">(Levy et al., 2017;</ref><ref type="bibr" target="#b22">Li et al., 2020;</ref><ref type="bibr" target="#b8">Du and Cardie, 2020)</ref>. And several task-specific pre-training techniques are proposed on these architectures <ref type="bibr" target="#b32">(Mengge et al., 2020;</ref><ref type="bibr">Wang et al., 2021b;</ref><ref type="bibr" target="#b40">Qin et al., 2021)</ref>. More relevant to our work are generation-based IE methods, which generate text spans via tagging <ref type="bibr" target="#b49">(Strakov? et al., 2019;</ref><ref type="bibr" target="#b31">Ma et al., 2019)</ref>, index pointer <ref type="bibr" target="#b43">(Ren et al., 2021;</ref><ref type="bibr">Yan et al., 2021b)</ref> or copy mechanism <ref type="bibr" target="#b65">(Zeng et al., 2018)</ref>, and these methods usually employ specific classifiers to represent labels. The generation can be enhanced using label templates <ref type="bibr">(Li et al., 2021a;</ref><ref type="bibr" target="#b26">Liu et al., 2021;</ref><ref type="bibr" target="#b6">Cui et al., 2021)</ref>, schema <ref type="bibr" target="#b59">(Lu et al., 2021;</ref><ref type="bibr" target="#b0">Ahmad et al., 2021)</ref>, and augmented language methods <ref type="bibr">(Paolini et al., 2021)</ref>.</p><p>Compared with previous IE studies which focus on developing more effective task-specialized mod-els, this paper aims to universally model various IE tasks in an unified text-to-structure framework, which can greatly benefit the rapid development, effective knowledge sharing, and quick adaptation of IE systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a unified text-to-structure generation framework -UIE, which can universally model different IE tasks, adaptively generate targeted structures, and unfiedly learn general IE abilities from different knowledge sources. Experimental results show that UIE achieves very competitive performance in both supervised and low-resource settings, which verified its universality, effectiveness, and transferability. A large-scale pre-trained text-to-structure model is also released, which will benefit future studies. For future work, we want to extend UIE to KB-aware IE tasks such as entity linking <ref type="bibr" target="#b2">(Cao et al., 2021)</ref>, and documentaware IE tasks such as co-reference <ref type="bibr" target="#b16">(Lee et al., 2017;</ref><ref type="bibr" target="#b28">Lu et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experiment Details</head><p>This section describes the details of experiments, including pre-training and fine-tuning on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Pre-training Details</head><p>Data Construction We use the 20210401 version of Wikipedia<ref type="foot" target="#foot_0">2</ref> and Wikidata<ref type="foot" target="#foot_1">3</ref> dump and Con-ceptNet<ref type="foot" target="#foot_2">4</ref> to construct the pre-train dataset.</p><p>For Wikidata and Wikipedia, we use them to collect the tuples T w = {&lt; T h , e h , r, e t , X &gt;}, where T h is head entity type, e h is head entity, r is relation, e t is tail entity, X is sentence, and the T w can be used to construct D pair , D record and D text . Firstly, we construct entity type dictionary L and relation dictionary P from Wikidata. Wikidata has more than 40M entity items and each item has its corresponding properties which indicate the association between entities. For type dictionary L, we regard each item as an entity, use the "instance of" and "subclass of" property values as its corresponding entity types and consider other properties as the relation of the entity with others. To learn general knowledge, all entity types will be retained except those whose instances are &lt; 5. For the type whose name is longer than 3 tokens, we use its headwords as the final type for simplicity, e.g.,"state award of the Republic of Moldova" is converted to "state award". For relation dictionary P, Wikidata has more than 9K kinds of properties<ref type="foot" target="#foot_3">5</ref> , we filter out the properties of external-id, URL, and math types. In this way, we obtain a collection of 31K types and retained 1535 properties which can serve as a solid foundation for universal IE. Secondly, we collect the mentions of each entity by using its anchor texts in Wikipedia and the top 3 frequent noun phrase occurrences of its entry page <ref type="bibr" target="#b19">(Li et al., 2010)</ref>. Then for each mention, we identify its entity types by linking it to its Wikidata item's types. For each Wikipedia page, we split the text into sentences<ref type="foot" target="#foot_4">6</ref> and filter out sentences that have no entities. Thirdly, we regard each entity as a head entity and find the associated entities according to its properties. The associated entity will set as as tail entity, and the property value will set as association type. If a head entity has no type, T h will be blank or has no associated tail entity, r and e t will be blank. To this end, given a sentence, we can construct instances based on the collected tuples T w by setting e h and e t as INFOSPAN, and assigning T h as SPOT-NAME, r as ASSONAME. Finally, from Wikipedia and Wikidata, we construct D pair , D record and D text with 65M instances, respectively. And we keep 50K as the development dataset.</p><p>To add common sense knowledge to structured extraction language (SEL), we extract the tuples T c from ConceptNet. ConceptNet contains 48 associations and has no context or entity types. So we leave the T h , T t X blank and finally construct 1M instances.</p><p>steps and data split of previous works <ref type="bibr" target="#b51">(Taill? et al., 2020;</ref><ref type="bibr" target="#b57">Yu et al., 2020;</ref><ref type="bibr" target="#b53">Wadden et al., 2019)</ref>.</p><p>Event For ACE05-Evt, we follow the same types, data splits, and pre-processing steps as <ref type="bibr" target="#b25">Lin et al. (2020)</ref>. For CASIE <ref type="bibr" target="#b46">(Satyapanich et al., 2020)</ref>, we first remove three incomplete annotated documents (999, 10001, 10002), then split the remaining documents into three sets: train/val/test=697/100/200 according to the time order of each document. We employ the state-of-the-art generation-based event extraction method TEXT2EVENT <ref type="bibr" target="#b59">(Lu et al., 2021)</ref> as the comparable state-of-the-art system.</p><p>Sentiment We conduct sentiment extraction experiments on the sentiment triplet extraction <ref type="bibr" target="#b60">(Xu et al., 2020)</ref> of SemEval 14/15/16 aspect sentiment analysis datasets. We employ the pre-processing datasets of the previous work <ref type="bibr">(Yan et al., 2021a)</ref>  12 .</p><p>Evaluation We use span-based offset Micro-F1 as the primary metric to evaluate the model: ? Entity: an entity mention is correct if its offsets and type match a reference entity. the error rate of the reported offset level evaluation is less than 0.5%. More complicated mapping approaches are left as future work.</p><p>Table <ref type="table">6</ref> shows the detailed hyper-parameters for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Comparison of UIE-base</head><p>This section introduces detailed experiment results of UIE-base.</p><p>Table <ref type="table" target="#tab_7">9</ref> shows the performance of UIE-base and the state-of-the-art systems on the four aspectbased sentiment analysis datasets. As shown in Table <ref type="table" target="#tab_7">9</ref>, the proposed SEL and SSI also have strong portability to sentiment triplets extraction, which achieves the competitive performance with the state-of-the-art with task-specific architectures. With the unified pre-training, UIE-base achieves an improvement of 3.24 on average over T5-v1.1-base across four datasets. This verifies the proposed unified pre-training algorithms can learn general IE ability even the sentiment knowledge is rarely in the pre-training stage.</p><p>Table <ref type="table" target="#tab_8">10</ref> shows the performance of SEL-SSI with the T5-v1.1-base for NYT. Due to the high overlapping of NYT and pre-trained data, we didn't conduct the experiment of UIE on NYT. Even without pre-training, SSI + SEL still achieved the stateof-the-art performance on NYT. This is because of the flexible generation architecture and the universal SEL expression, UIE can naturally handle entity overlap problems.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: From (a) Task-specialized IE: different tasks, different structures, different schemas to (b) Universal IE: unified modeling via structure generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The SEL representation of the extraction structure of "Steve became CEO of Apple in 1997.", where the relation structure is marked blue, the event structure is marked red, and the rest are entities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustrations of structured extraction language.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>unit with negative SPOTNAME and AS-SONAME: (SPOTNAME, [NULL]) and (ASSON-AME, [NULL]) into the ground-truth SEL with the SSI &lt;spot&gt; person ... &lt;spot&gt; facility &lt;asso&gt; ... &lt;text&gt; Text Steve became CEO of Apple in 1997. SEL ((person: Steve (work for: Apple)) (start-position: ... + RM ((person: Steve (work for: Apple)) (facility: [NULL]) ... An example of rejection mechanism (RM), here "(facility: [NULL])" is the injected rejection noise during learning stage, and the [NULL]-valued span will be ignored during inference stage.probability of p . For example, in Table1, f acility is the negative spot in the schema prompt, i.e., there is no facility entity in the sentence "Steve became CEO of Apple in 1997". Therefore, we randomly inject the noise of "(facility:[NULL])" into the target record during model learning. In this way, the UIE can effectively learn to reject misleading generation by generating [NULL] token.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Overall results of UIE-large on different datasets. SEL refers to UIE without pre-training by directly using T5-v1.1-large as the backbone. Because NYT overlaps with pre-training data, we didn't conduct UIE on NYT for fair comparsion. More results of UIE-base and the details of evaluation metric are shown in the appendix.</figDesc><table><row><cell></cell><cell>Domain</cell><cell>Metric</cell><cell>Comparable SOTA</cell><cell>SEL</cell><cell>UIE</cell></row><row><cell>ACE04</cell><cell>News, Speech</cell><cell>Entity F1</cell><cell>(Yan et al., 2021b)</cell><cell cols="2">86.84 86.52 86.89</cell></row><row><cell cols="2">ACE05-Ent News, Speech</cell><cell>Entity F1</cell><cell>(Yan et al., 2021b)</cell><cell cols="2">84.74 85.52 85.78</cell></row><row><cell>CoNLL03</cell><cell>News</cell><cell>Entity F1</cell><cell>(Wang et al., 2021a)</cell><cell cols="2">93.21 92.17 92.99</cell></row><row><cell cols="2">ACE05-Rel News, Speech</cell><cell>Relation Strict F1</cell><cell cols="3">(Zhong and Chen, 2021) 65.60 64.68 66.06</cell></row><row><cell>CoNLL04</cell><cell>News</cell><cell>Relation Strict F1</cell><cell>(Wang and Lu, 2020)</cell><cell cols="2">73.60 73.07 75.00</cell></row><row><cell>NYT</cell><cell>News</cell><cell>Relation Triplet F1</cell><cell>(Zheng et al., 2021)</cell><cell>92.70 93.54</cell><cell>-</cell></row><row><cell>SciERC</cell><cell>Scientific</cell><cell>Relation Strict F1</cell><cell cols="3">(Zhong and Chen, 2021) 35.60 33.36 36.53</cell></row><row><cell cols="2">ACE05-Evt News, Speech</cell><cell>Event Trigger F1 Event Argument F1</cell><cell>(Lin et al., 2020) (Lin et al., 2020)</cell><cell cols="2">72.80 72.63 73.36 54.80 54.67 54.79</cell></row><row><cell>CASIE</cell><cell>Cybersecurity</cell><cell>Event Trigger F1 Event Argument F1</cell><cell>(Lu et al., 2021) (Lu et al., 2021)</cell><cell cols="2">67.51 68.98 69.33 59.45 60.37 61.30</cell></row><row><cell>14-res</cell><cell>Reviews</cell><cell>Sentiment Triplet F1</cell><cell>(Zhang et al., 2021)</cell><cell cols="2">72.16 73.78 74.52</cell></row><row><cell>14-lap</cell><cell>Reviews</cell><cell>Sentiment Triplet F1</cell><cell>(Zhang et al., 2021)</cell><cell cols="2">60.78 63.15 63.88</cell></row><row><cell>15-res</cell><cell>Reviews</cell><cell>Sentiment Triplet F1</cell><cell>(Xu et al., 2021)</cell><cell cols="2">63.27 66.10 67.15</cell></row><row><cell>16-res</cell><cell>Reviews</cell><cell>Sentiment Triplet F1</cell><cell>(Xu et al., 2021)</cell><cell cols="2">70.26 73.87 75.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Model</cell><cell cols="3">1-Shot 5-Shot 10-Shot AVE-S 1%</cell><cell>5%</cell><cell>10% AVE-R</cell></row><row><cell cols="2">Entity (CoNLL03) Ent-F1</cell><cell cols="3">T5-v1.1-base Fine-tuned T5-base 24.93 12.73 UIE-base w/o SSI 43.52 UIE-base 46.43</cell><cell>30.17 54.85 64.76 67.09</cell><cell>58.89 65.31 72.47 73.90</cell><cell>33.93 75.74 85.71 87.70 83.05 48.36 78.51 87.67 88.91 85.03 60.25 81.91 88.41 89.84 86.72 62.47 82.84 88.34 89.63 86.94</cell></row><row><cell></cell><cell></cell><cell cols="2">T5-v1.1-base</cell><cell>2.35</cell><cell>7.99</cell><cell>25.98</cell><cell>12.11</cell><cell>6.08 32.38 41.87 26.78</cell></row><row><cell cols="2">Relation (CoNLL04) Rel-S F1</cell><cell cols="2">Fine-tuned T5-base UIE-base w/o SSI UIE-base</cell><cell>4.24 13.21 22.05</cell><cell>28.16 40.35 45.41</cell><cell>41.44 49.47 52.39</cell><cell>24.61 12.89 37.75 49.95 33.53 34.34 24.21 48.70 56.59 43.17 39.95 30.77 51.72 59.18 47.22</cell></row><row><cell cols="2">Event Trigger (ACE05-Evt) Evt Tri F1</cell><cell cols="3">T5-v1.1-base Fine-tuned T5-base 30.18 19.40 UIE-base w/o SSI 32.07 UIE-base 38.14</cell><cell>43.35 48.31 48.11 51.21</cell><cell>50.57 51.27 51.00 53.23</cell><cell>37.77 25.59 49.47 57.18 44.08 43.25 31.08 51.16 57.76 46.67 43.73 32.71 53.20 59.26 48.39 47.53 41.53 55.70 60.29 52.51</cell></row><row><cell cols="2">Event Argument (ACE05-Evt) Evt Arg F1</cell><cell cols="2">T5-v1.1-base Fine-tuned T5-base UIE-base w/o SSI UIE-base</cell><cell>2.75 6.96 9.31 11.88</cell><cell>20.21 25.07 23.99 27.44</cell><cell>27.53 30.96 30.31 33.64</cell><cell>16.83 21.00 21.20 24.32 12.80 30.43 36.28 26.50 3.59 21.53 30.90 18.67 7.39 24.97 33.90 22.09 9.57 27.25 34.18 23.67</cell></row><row><cell></cell><cell></cell><cell cols="2">T5-v1.1-base</cell><cell>0.04</cell><cell>2.11</cell><cell>12.66</cell><cell>4.94</cell><cell>3.50 27.08 45.97 25.52</cell></row><row><cell cols="2">Sentiment (16res) Rel-S F1</cell><cell cols="2">Fine-tuned T5-base UIE-base w/o SSI UIE-base</cell><cell>6.55 7.79 10.50</cell><cell>21.06 17.77 26.24</cell><cell>29.92 32.07 39.11</cell><cell>19.18 18.72 39.63 51.65 36.67 19.21 19.14 42.76 53.44 38.45 25.28 24.24 49.31 57.61 43.72</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>UIE</cell></row><row><cell cols="6">equipped with SSI achieves improvements of 4.16</cell></row><row><cell cols="6">and 3.30 on average for n-shot and n-ratio experi-</cell></row><row><cell cols="6">ments. 2) Our pre-training algorithms can learn</cell></row><row><cell cols="6">general IE ability rather than capture task-specific</cell></row><row><cell cols="6">information. Even the pre-training of UIE didn't</cell></row><row><cell cols="6">include event and sentiment knowledge, UIE still</cell></row><row><cell cols="6">achieved significantly better performance on these</cell></row><row><cell cols="6">tasks compared to the baseline with only a small</cell></row><row><cell cols="2">number of samples.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">4.4 Ablations on Pre-training Tasks</cell><cell></cell></row><row><cell>Task</cell><cell cols="2">Entity Relation</cell><cell cols="2">Event</cell><cell>Sent.</cell></row><row><cell>F1</cell><cell>Ent</cell><cell>Rel-S</cell><cell cols="3">Evt-Tri Evt-Arg Rel-S</cell></row><row><cell>UIE-base</cell><cell>95.89</cell><cell>75.97</cell><cell>72.63</cell><cell>57.27</cell><cell>74.73</cell></row><row><cell>w/o L Pair</cell><cell>95.83</cell><cell>75.07</cell><cell>71.20</cell><cell>55.79</cell><cell>74.27</cell></row><row><cell>w/o L Record</cell><cell>95.69</cell><cell>75.68</cell><cell>71.99</cell><cell>57.60</cell><cell>74.43</cell></row><row><cell>w/o L Text</cell><cell>95.66</cell><cell>75.70</cell><cell>70.89</cell><cell>54.16</cell><cell>74.28</cell></row><row><cell cols="2">T5-v1.1-base 95.29</cell><cell>72.12</cell><cell>70.50</cell><cell>54.42</cell><cell>72.03</cell></row><row><cell cols="6">Table 4: Experiment results of UIE-base with dif-</cell></row><row><cell cols="6">ferent learning tasks on the development set of four</cell></row><row><cell cols="6">downstream datasets: entity (CoNLL03), relation</cell></row><row><cell cols="6">(CoNLL04), event (ACE05-Evt) and sentiment (16res).</cell></row></table><note><p><p>Low-resource results on end-to-end IE tasks, where AVE-S(hot) and AVE-R(atio) are the averaged performance across 3 few-shot settings and 3 low-resource settings respectively.</p>tract. Compared with the UIE model w/o SSI,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Algorithm 1 The pre-training process of UIE in a Python-like style.</figDesc><table><row><cell># The training details of UIE</cell></row><row><cell>function pretraining_process</cell></row><row><cell>for step in all_steps do</cell></row><row><cell>batch = []</cell></row><row><cell># load ntext unstructured text samples</cell></row><row><cell>texts = get_data(Dtext, ntext)</cell></row><row><cell># construct corrupted source text x and</cell></row><row><cell># corrupted spans x for each text sample</cell></row><row><cell>for x in texts do</cell></row><row><cell>x , x = span_corrupt(x)</cell></row><row><cell>batch.extend((None, x , x ))</cell></row><row><cell>end for</cell></row><row><cell># load nrecord structured record samples</cell></row><row><cell>records = get_data(Drecord, nrecord)</cell></row><row><cell>for y in records do</cell></row><row><cell>batch.extend((None, None, y))</cell></row><row><cell>end for</cell></row><row><cell># load npair text-record parallel pairs</cell></row><row><cell>text_record_pairs = get_data(Dpair, npair)</cell></row><row><cell># construct meta-schema smeta</cell></row><row><cell># for each text-record pair (x, y)</cell></row><row><cell>for (x, y) in text_record_pairs do</cell></row><row><cell>s = meta_schema_sample(y)</cell></row><row><cell>batch.extend((s, x, y))</cell></row><row><cell>end for</cell></row><row><cell># compute loss and backward</cell></row><row><cell>LPair, LRecord, LText = UIE(batch)</cell></row><row><cell>loss = LPair + LRecord + LText</cell></row><row><cell>loss.backward()</cell></row><row><cell>end for</cell></row><row><cell>end function</cell></row><row><cell># The meta sample of UIE</cell></row><row><cell>function meta_schema_sample(y)</cell></row><row><cell># get positive spots and associations</cell></row><row><cell># in the record y</cell></row><row><cell>ss+, sa+ = get_schema_f rom_record(y)</cell></row><row><cell># sample negative spots</cell></row><row><cell>ss-= sample_negative_spot(s+)</cell></row><row><cell># sample negative associations</cell></row><row><cell>sa-= sample_negative_association(s+)</cell></row><row><cell>return ss+ ? ss-? sa+ ? sa-</cell></row><row><cell>end function</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>?</head><label></label><figDesc>Relation Strict: relation with strict match, a relation is correct if its relation type is correct and the offsets and entity types of the related entity mentions are correct.</figDesc><table><row><cell>Methods</cell><cell>PLM</cell><cell>14res 14lap 15res 16res</cell></row><row><cell>(Xu et al., 2020)</cell><cell cols="2">BERT-base 62.40 51.04 57.53 63.83</cell></row><row><cell>(Yan et al., 2021a)</cell><cell cols="2">BART-base 65.25 58.69 59.26 67.62</cell></row><row><cell>(Xu et al., 2021)</cell><cell cols="2">BERT-base 71.85 59.38 63.27 70.26</cell></row><row><cell>(Zhang et al., 2021)</cell><cell>T5-base</cell><cell>72.16 60.78 62.10 70.10</cell></row><row><cell></cell><cell>UIE-base</cell><cell>72.55 62.94 64.41 72.86</cell></row><row><cell>SSI + SEL</cell><cell cols="2">T5-v1.1-base 71.27 58.69 59.60 70.24</cell></row><row><cell>? Relation Triplet: relation with boundary match,</cell><cell></cell><cell></cell></row><row><cell>a relation is correct if its relation type is correct</cell><cell></cell><cell></cell></row><row><cell>and the string of the subject/object are correct.</cell><cell></cell><cell></cell></row><row><cell>? Event Trigger: an event trigger is correct if its</cell><cell></cell><cell></cell></row><row><cell>offsets and event type matches a reference trigger.</cell><cell></cell><cell></cell></row><row><cell>? Event Argument: an event argument is correct</cell><cell></cell><cell></cell></row><row><cell>if its offsets, role type, and event type match a</cell><cell></cell><cell></cell></row><row><cell>reference argument mention.</cell><cell></cell><cell></cell></row><row><cell>? Sentiment Triplet: a correct triplet requires the</cell><cell></cell><cell></cell></row><row><cell>offsets boundary of the target, the offsets bound-</cell><cell></cell><cell></cell></row><row><cell>ary of the opinion span, and the target sentiment</cell><cell></cell><cell></cell></row><row><cell>polarity to be all correct at the same time.</cell><cell></cell><cell></cell></row><row><cell>To make a fair comparison with baseline systems,</cell><cell></cell><cell></cell></row><row><cell>we mapped the generated string-level extraction</cell><cell></cell><cell></cell></row><row><cell>results to offset-level for model evaluation. In de-</cell><cell></cell><cell></cell></row><row><cell>tail, we reconstructed the offset of predicted en-</cell><cell></cell><cell></cell></row><row><cell>tity/trigger mentions by finding the matched utter-</cell><cell></cell><cell></cell></row><row><cell>ance in the input sequence one by one. For argu-</cell><cell></cell><cell></cell></row><row><cell>ment mentions in relation and event tasks, we found</cell><cell></cell><cell></cell></row><row><cell>the nearest matched utterance to the predicted en-</cell><cell></cell><cell></cell></row><row><cell>tity/trigger mention as the predicted offset. This</cell><cell></cell><cell></cell></row><row><cell>simple heuristic offset strategy achieves high ac-</cell><cell></cell><cell></cell></row><row><cell>curacy. Compared to the string level evaluation,</cell><cell></cell><cell></cell></row><row><cell>12 https://github.com/yhcc/BARTABSA</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Experiment results of UIE-base on the sentiment triplet extraction tasks.</figDesc><table><row><cell>Methods</cell><cell>PLM</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>(Wang et al., 2020)</cell><cell cols="4">BERT-base 91.40 92.60 92.00</cell></row><row><cell>(Sui et al., 2020)</cell><cell cols="4">BERT-base 92.50 92.20 92.30</cell></row><row><cell cols="5">(Zheng et al., 2021) BERT-base 93.50 91.90 92.70</cell></row><row><cell>SSI + SEL</cell><cell cols="4">T5-v1.1-base 91.94 93.28 92.60</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 :</head><label>10</label><figDesc>Experiment results of SSI and SEL on the NYT (the joint entity and relation extraction setting).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Ent &lt;spot&gt; facility &lt;spot&gt; geographical social political &lt;spot&gt; location &lt;spot&gt; organization &lt;spot&gt; person &lt;spot&gt; vehicle &lt;spot&gt; weapon organization &lt;spot&gt; other &lt;spot&gt; people &lt;asoc&gt; kill &lt;asoc&gt; live in &lt;asoc&gt; located in &lt;asoc&gt; organization in &lt;asoc&gt; work for Relation NYT &lt;spot&gt; location &lt;spot&gt; organization &lt;spot&gt; person &lt;asoc&gt; administrative divisions &lt;asoc&gt; advisors &lt;asoc&gt; capital &lt;asoc&gt; children &lt;asoc&gt; company &lt;asoc&gt; contains &lt;asoc&gt; country &lt;asoc&gt; ethnicity &lt;asoc&gt; founders &lt;asoc&gt; geographic distribution &lt;asoc&gt; industry &lt;asoc&gt; location &lt;asoc&gt; major shareholder of &lt;asoc&gt; major shareholders &lt;asoc&gt; nationality &lt;asoc&gt; neighborhood of &lt;asoc&gt; people &lt;asoc&gt; place founded &lt;asoc&gt; place lived &lt;asoc&gt; place of birth &lt;asoc&gt; place of death &lt;asoc&gt; profession &lt;asoc&gt; religion &lt;asoc&gt; teams Relation SciERC &lt;spot&gt; generic &lt;spot&gt; material &lt;spot&gt; method &lt;spot&gt; metric &lt;spot&gt; other scientific term &lt;spot&gt; task &lt;asoc&gt; compare &lt;asoc&gt; conjunction &lt;asoc&gt; evaluate for &lt;asoc&gt; feature of &lt;asoc&gt; hyponym of &lt;asoc&gt; part of &lt;asoc&gt; used for Event ACE05-Evt &lt;spot&gt; acquit &lt;spot&gt; appeal &lt;spot&gt; arrest jail &lt;spot&gt; attack &lt;spot&gt; born &lt;spot&gt; charge indict &lt;spot&gt; convict &lt;spot&gt; declare bankruptcy &lt;spot&gt; demonstrate &lt;spot&gt; die &lt;spot&gt; divorce &lt;spot&gt; elect &lt;spot&gt; end organization &lt;spot&gt; end position &lt;spot&gt; execute &lt;spot&gt; extradite &lt;spot&gt; fine &lt;spot&gt; injure &lt;spot&gt; marry &lt;spot&gt; meet &lt;spot&gt; merge organization &lt;spot&gt; nominate &lt;spot&gt; pardon &lt;spot&gt; phone write &lt;spot&gt; release parole &lt;spot&gt; sentence &lt;spot&gt; start organization &lt;spot&gt; start position &lt;spot&gt; sue &lt;spot&gt; transfer money &lt;spot&gt; transfer ownership &lt;spot&gt; transport &lt;spot&gt; trial hearing &lt;asoc&gt; adjudicator &lt;asoc&gt; agent &lt;asoc&gt; artifact &lt;asoc&gt; attacker &lt;asoc&gt; beneficiary &lt;asoc&gt; buyer &lt;asoc&gt; defendant &lt;asoc&gt; destination &lt;asoc&gt; entity &lt;asoc&gt; giver &lt;asoc&gt; instrument &lt;asoc&gt; organization &lt;asoc&gt; origin &lt;asoc&gt; person &lt;asoc&gt; place &lt;asoc&gt; plaintiff &lt;asoc&gt; prosecutor &lt;asoc&gt; recipient &lt;asoc&gt; seller &lt;asoc&gt; target &lt;asoc&gt; vehicle &lt;asoc&gt; victim Event CASIE &lt;spot&gt; capabilities &lt;spot&gt; common vulnerabilities and exposures &lt;spot&gt; data &lt;spot&gt; databreach &lt;spot&gt; device &lt;spot&gt; discover vulnerability &lt;spot&gt; file &lt;spot&gt; geopolitical entity &lt;spot&gt; malware &lt;spot&gt; money &lt;spot&gt; number &lt;spot&gt; organization &lt;spot&gt; patch &lt;spot&gt; patch vulnerability &lt;spot&gt; payment method &lt;spot&gt; person &lt;spot&gt; personally identifiable information &lt;spot&gt; phishing &lt;spot&gt; purpose &lt;spot&gt; ransom &lt;spot&gt; software &lt;spot&gt; system &lt;spot&gt; time &lt;spot&gt; version &lt;spot&gt; vulnerability &lt;spot&gt; website &lt;asoc&gt; attack pattern &lt;asoc&gt; attacker &lt;asoc&gt; capabilities &lt;asoc&gt; common vulnerabilities and exposures &lt;asoc&gt; compromised data &lt;asoc&gt; damage amount &lt;asoc&gt; discoverer &lt;asoc&gt; issues addressed &lt;asoc&gt; number of data &lt;asoc&gt; number of victim &lt;asoc&gt; patch &lt;asoc&gt; patch number &lt;asoc&gt; payment method &lt;asoc&gt; place &lt;asoc&gt; price &lt;asoc&gt; purpose &lt;asoc&gt; releaser &lt;asoc&gt; supported platform &lt;asoc&gt; time &lt;asoc&gt; tool &lt;asoc&gt; trusted entity &lt;asoc&gt; victim &lt;asoc&gt; vulnerability &lt;asoc&gt; vulnerable system &lt;asoc&gt; vulnerable system owner &lt;asoc&gt; vulnerable system version Sentiment 14/15/16-res &lt;spot&gt; aspect &lt;spot&gt; opinion &lt;asoc&gt; negative &lt;asoc&gt; neutral &lt;asoc&gt; positive Sentiment 14-lap &lt;spot&gt; aspect &lt;spot&gt; opinion &lt;asoc&gt; negative &lt;asoc&gt; neutral &lt;asoc&gt; positive</figDesc><table><row><cell>Task</cell><cell>Dataset</cell><cell>Structural Schema Instructor</cell></row><row><cell cols="2">Entity ACE04/05-Entity CoNLL03</cell><cell>&lt;spot&gt; location &lt;spot&gt; miscellaneous &lt;spot&gt; organization &lt;spot&gt; person</cell></row><row><cell>Relation</cell><cell>ACE05-Rel</cell><cell>&lt;spot&gt; facility &lt;spot&gt; geographical social political &lt;spot&gt; location &lt;spot&gt;</cell></row><row><cell></cell><cell></cell><cell>organization &lt;spot&gt; person &lt;spot&gt; vehicle &lt;spot&gt; weapon &lt;asoc&gt; agent artifact</cell></row><row><cell></cell><cell></cell><cell>&lt;asoc&gt; general affiliation &lt;asoc&gt; organization affiliation &lt;asoc&gt; part whole</cell></row><row><cell></cell><cell></cell><cell>&lt;asoc&gt; personal social &lt;asoc&gt; physical</cell></row><row><cell>Relation</cell><cell>CoNLL04</cell><cell>&lt;spot&gt; location &lt;spot&gt;</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Structured schema instructor for each dataset (we use &lt;spot&gt; and &lt;asoc&gt; rather than [spot] and[asoc]   for better visualization).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://www.wikipedia.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://www.wikidata.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://conceptnet.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://www.wikidata.org/wiki/ Wikidata:List_of_properties</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>nltk.tokenize.punkt   </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We sincerely thank the reviewers for their insightful comments and valuable suggestions. This research work is supported by the <rs type="funder">National Natural Science Foundation of China</rs> under Grants no. <rs type="grantNumber">U1936207</rs>, <rs type="grantNumber">62122077</rs> and <rs type="grantNumber">62106251</rs>, the Project of the <rs type="affiliation">Chinese Language Committee</rs> under Grant no. <rs type="grantNumber">YB2003C002</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pCHYE9y">
					<idno type="grant-number">U1936207</idno>
				</org>
				<org type="funding" xml:id="_kA5F3SS">
					<idno type="grant-number">62122077</idno>
				</org>
				<org type="funding" xml:id="_5GBycka">
					<idno type="grant-number">62106251</idno>
				</org>
				<org type="funding" xml:id="_4u8h6A2">
					<idno type="grant-number">YB2003C002</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training Details We first initialize UIE-base and UIE-large with T5-v1.1-base and T5-v1.1-large checkpoints <ref type="bibr" target="#b41">(Raffel et al., 2020)</ref>, and the model architectures are shown in Table <ref type="table">7</ref>. We employ Adam optimizer (Kingma and Ba, 2015) as the optimizer with learning rate=1e-4, and use linear scheduling with a warming up proportion 6%. For negative spots and associations in the L Pair , we randomly select negative spots and associations up to 10 for each instance, respectively. For L Text , we set the corruption rate to 15% and the average corrupting span length to 3, following <ref type="bibr" target="#b41">Raffel et al. (2020)</ref>. We truncate the concatenated overall length of schema prompt s and raw text x, as well as the length of SEL expression y, together to 128 during pre-training. We train our base model and large model for both 500K steps with batch size 512 on 8 NVIDIA A100 GPUs.</p><p>The detailed pre-training process in a pythonlike style is shown in Algorithm 1. In each batch of pre-training processes for UIE, we construct a batch of triplets (s, x, y) containing text-record pairs, text instances, and record instances. In practice, since 8 GPUs could only run the large model with an overall batch of 128 (batch=16 on each GPU), we update the model parameters after accumulating 4 gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Details of Downstream Tasks</head><p>We conduct downstream tasks on 4 IE tasks, 13 datasets, and the detailed statistic of each dataset is shown in Table <ref type="table">8</ref>. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Intent classification and slot filling for privacy policies</title>
		<author>
			<persName><forename type="first">Wasi</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Norton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.340</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4402" to="4417" />
		</imprint>
	</monogr>
	<note>Long Papers). Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic extraction of facts from press releases to generate news stories</title>
		<author>
			<persName><forename type="first">Peggy</forename><forename type="middle">M</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">J</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">P</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alison</forename><forename type="middle">K</forename><surname>Huettner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><forename type="middle">M</forename><surname>Schmandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><forename type="middle">B</forename><surname>Nirenburg</surname></persName>
		</author>
		<idno type="DOI">10.3115/974499.974531</idno>
	</analytic>
	<monogr>
		<title level="m">Third Conference on Applied Natural Language Processing</title>
		<meeting><address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="170" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Autoregressive entity retrieval</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting and reading text in natural scenes</title>
		<author>
			<persName><forename type="first">Xiangrong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2004.77</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2004), with CD-ROM</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004-06-27">2004. 2004. 27 June -2 July 2004</date>
			<biblScope unit="page" from="366" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unifying vision-and-language tasks via text generation</title>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="1931" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Template-based named entity recognition using BART</title>
		<author>
			<persName><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.161</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1835" to="1845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Event extraction by answering (almost) natural questions</title>
		<author>
			<persName><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.49</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="671" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Twenty-five years of information extraction</title>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<idno type="DOI">10.1017/S1351324919000512</idno>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="677" to="692" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Message Understanding Conference-6: A brief history</title>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beth</forename><surname>Sundheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 16th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="1996">1996. 1996</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>COLING</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fewshot named entity recognition: An empirical baseline study</title>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishan</forename><surname>Subudhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shobana</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10408" to="10423" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Knowledge base population: Successful approaches and challenges</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1148" to="1158" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Third International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Identity and necessity</title>
		<author>
			<persName><forename type="first">Saul</forename><surname>Kripke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milton</forename><forename type="middle">K</forename><surname>Munitz</surname></persName>
		</author>
		<idno type="DOI">10.1093/acprof:oso/9780199730155.001.0001/acprof-9780199730155-chapter-1</idno>
		<imprint>
			<date type="published" when="1971">1971. 1971</date>
			<biblScope unit="page" from="135" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1030</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1018</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Zero-shot relation extraction via reading comprehension</title>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K17-1034</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. CoNLL 2017</date>
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generating templates of entity summaries with an entityaspect model and pattern mining</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinglin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Uppsala, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="640" to="649" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">2021a. Documentlevel event argument extraction by conditional generation</title>
		<author>
			<persName><forename type="first">Sha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.69</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="894" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">2021b. UNIMO: Towards unified-modal understanding and generation via cross-modal contrastive learning</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guocheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2592" to="2607" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A unified MRC framework for named entity recognition</title>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5849" to="5859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Nugget proposal networks for Chinese event detection</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1145</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Australia. Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1565" to="1574" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence-to-nuggets: Nested entity mention detection via anchor-region networks</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1511</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5182" to="5192" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A joint neural model for information extraction with global features</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.713</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7999" to="8009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fine-grained entity typing via label reasoning</title>
		<author>
			<persName><forename type="first">Qing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.378</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4611" to="4622" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end neural event coreference resolution</title>
		<author>
			<persName><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2021.103632</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">303</biblScope>
			<biblScope unit="page">103632</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Text2Event: Controllable sequence-tostructure generation for end-to-end event extraction</title>
		<author>
			<persName><forename type="first">Yaojie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoyi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.217</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2795" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1360</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3219" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploring sequence-tosequence learning in aspect term extraction</title>
		<author>
			<persName><forename type="first">Dehong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangzhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1344</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3538" to="3547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Coarse-to-Fine Pre-training for Named Entity Recognition</title>
		<author>
			<persName><forename type="first">Xue</forename><surname>Mengge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.514</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6345" to="6354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">From information retrieval to information extraction</title>
		<author>
			<persName><forename type="first">David</forename><surname>Milward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Thomas</surname></persName>
		</author>
		<idno type="DOI">10.3115/1117755.1117767</idno>
	</analytic>
	<monogr>
		<title level="m">ACL-2000 Workshop on Recent Advances in Natural Language Processing and Information Retrieval</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="85" to="97" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shudong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramez</forename><surname>Zakhary</surname></persName>
		</author>
		<idno type="DOI">10.35111/8m4r-v312</idno>
		<imprint>
			<date type="published" when="2004">2005. 2004</date>
		</imprint>
	</monogr>
	<note>multilingual training corpus</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Issues and methodology for template design for information extraction</title>
		<author>
			<persName><forename type="first">Boyan</forename><surname>Onyshkevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technology: Proceedings of a Workshop</title>
		<meeting><address><addrLine>Plainsboro, New Jersey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-03-08">1994. March 8-11, 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">RISHITA ANUBHAI, Cicero Nogueira dos Santos, Bing Xiang, and Stefano Soatto. 2021. Structured prediction as translation between augmented natural languages</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SemEval-2016 task 5: Aspect based sentiment analysis</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al-Smadi</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orph?e</forename><surname>De Clercq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V?ronique</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianna</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Loukachevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniy</forename><surname>Kotelnikov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S16-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)</title>
		<meeting>the 10th International Workshop on Semantic Evaluation (SemEval-2016)<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
	<note>Nuria Bel, Salud Mar?a Jim?nez-Zafra, and G?l?en Eryigit. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SemEval-2015 task 12: Aspect based sentiment analysis</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S15-2082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Workshop on Semantic Evaluation</title>
		<meeting>the 9th International Workshop on Semantic Evaluation<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015. SemEval 2015</date>
			<biblScope unit="page" from="486" to="495" />
		</imprint>
	</monogr>
	<note>Suresh Manandhar, and Ion Androutsopoulos</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SemEval-2014 task 4: Aspect based sentiment analysis</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harris</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Manandhar</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/S14-2004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014)</title>
		<meeting>the 8th International Workshop on Semantic Evaluation (SemEval 2014)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ERICA: Improving entity and relation understanding for pre-trained language models via contrastive learning</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryuichi</forename><surname>Takanobu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.260</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3350" to="3363" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sequence level training with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Zaremba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In 4th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02">2016. 2016. May 2-4, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">HySPA: Hybrid span generation for scalable text-to-graph extraction</title>
		<author>
			<persName><forename type="first">Liliang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenkai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.356</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4066" to="4078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A linear programming formulation for global inference in natural language tasks</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Conference on Computational Natural Language Learning</title>
		<meeting>the Eighth Conference on Computational Natural Language Learning<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>HLT-NAACL 2004</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Casie: Extracting cybersecurity event information from text</title>
		<author>
			<persName><forename type="first">Taneeya</forename><surname>Satyapanich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Finin</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i05.6401</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Con-ference on Artificial Intelligence</title>
		<meeting>the AAAI Con-ference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8749" to="8757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep exhaustive model for nested named entity recognition</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Golam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sohrab</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1309</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2843" to="2849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An open multilingual graph of general knowledge</title>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="4444" to="4451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Neural architectures for nested NER through linearization</title>
		<author>
			<persName><forename type="first">Jana</forename><surname>Strakov?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajic</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1527</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5326" to="5331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Joint entity and relation extraction with set prediction networks</title>
		<author>
			<persName><forename type="first">Dianbo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengping</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/2011.01675</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Let&apos;s Stop Incorrect Comparisons in End-to-end Relation Extraction</title>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Taill?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Guigue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Scoutheeten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.301</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3689" to="3701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meulder</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2003</title>
		<meeting>CoNLL-2003<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1585</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5784" to="5789" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Medero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<idno type="DOI">10.35111/mwxc-vh88</idno>
		<title level="m">Ace 2005 multilingual training corpus</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Two are better than one: Joint entity and relation extraction with tablesequence encoders</title>
		<author>
			<persName><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.133</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1706" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">2021a. Improving named entity recognition by external context retrieving and cooperative learning</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.142</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1800" to="1812" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">TPLinker: Single-stage joint extraction of entities and relations through token pair linking</title>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.138</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1572" to="1582" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">CLEVE: Contrastive Pre-training for Event Extraction</title>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.491</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6283" to="6297" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning span-level interactions for aspect sentiment triplet extraction</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yew</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.367</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4755" to="4766" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Position-aware tagging for aspect sentiment triplet extraction</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.183</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2339" to="2349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer</title>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.41</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="483" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">2021a. A unified generative framework for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.188</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2416" to="2429" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Zheng Zhang, and Xipeng Qiu. 2021b. A unified genera-tive framework for various NER subtasks</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.451</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5808" to="5822" />
		</imprint>
	</monogr>
	<note>Long Papers). Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations based on a novel decomposition strategy</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECAI</title>
		<meeting>of ECAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Extracting relational facts by an end-to-end neural model with copy mechanism</title>
		<author>
			<persName><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1047</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Minimize exposure bias of Seq2Seq models in joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Qianying</forename><surname>Ranran Haoran Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aysa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Xuemo Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadao</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><surname>Kurohashi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.23</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="236" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Towards generative aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.64</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="504" to="510" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">PRGC: Potential relation and global correspondence based joint relational triple extraction</title>
		<author>
			<persName><forename type="first">Hengyi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.486</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6225" to="6235" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Joint extraction of entities and relations based on a novel tagging scheme</title>
		<author>
			<persName><forename type="first">Suncong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexing</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1113</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A frustratingly easy approach for entity and relation extraction</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.5</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="50" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">For nested entity extraction datasets ACE04 and ACE05-Ent, we follow the pre-processing steps and data split of previous works</title>
		<author>
			<persName><surname>Mitchell</surname></persName>
		</author>
		<idno>ACE05-Ent 8</idno>
		<ptr target="https://catalog.ldc.upenn.edu/LDC2006T06" />
	</analytic>
	<monogr>
		<title level="m">NYT 10 (Riedel et al., 2010), and SciERC 11</title>
		<editor>
			<persName><forename type="first">Tjong</forename><surname>Kim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sang</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">De</forename><surname>Meulder</surname></persName>
		</editor>
		<imprint>
			<publisher>Roth and Yih</publisher>
			<date type="published" when="2003">2005. 2006. 2003. 2020. 2006. 2004. 2018</date>
		</imprint>
	</monogr>
	<note>and CoNLL03. We follow the preprocessing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
