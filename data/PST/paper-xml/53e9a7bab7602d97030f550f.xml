<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What Makes a Good Bug Report?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Bettenburg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sascha</forename><surname>Just</surname></persName>
							<email>just@st.cs.uni-sb.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adrian</forename><surname>Schr√∂ter</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Victoria</orgName>
								<address>
									<region>BC</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cathrin</forename><surname>Weiss</surname></persName>
							<email>weiss@ifi.uzh.ch</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Zurich</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rahul</forename><surname>Premraj</surname></persName>
							<email>premraj@cs.uni-sb.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Saarland University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Zimmermann</surname></persName>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Calgary</orgName>
								<address>
									<settlement>Alberta</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">What Makes a Good Bug Report?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1B2B274E88AE94DCE027C6BDB38B986A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D.2.5 [Software Engineering]: Testing and Debugging; D.2.7 [Software Engineering]: Distribution</term>
					<term>Maintenance</term>
					<term>and Enhancement Human Factors</term>
					<term>Management</term>
					<term>Measurement</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In software development, bug reports provide crucial information to developers. However, these reports widely differ in their quality. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to find out what makes a good bug report.</p><p>The analysis of the 466 responses revealed an information mismatch between what developers need and what users supply. Most developers consider steps to reproduce, stack traces, and test cases as helpful, which are at the same time most difficult to provide for users. Such insight is helpful to design new bug tracking tools that guide users at collecting and providing more helpful information.</p><p>Our CUEZILLA prototype is such a tool and measures the quality of new bug reports; it also recommends which elements should be added to improve the quality. We trained CUEZILLA on a sample of 289 bug reports, rated by developers as part of the survey. In our experiments, CUEZILLA was able to predict the quality of 31-48% of bug reports accurately.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Bug reports are vital for any software development. They allow users to inform developers of the problems encountered while using a software. Bug reports typically contain a detailed description of a failure and occasionally hint at the location of the fault in the code (in form of patches or stack traces). However, bug reports vary in their quality of content; they often provide inadequate or incorrect information. Thus, developers sometimes have to face bugs with descriptions such as "Sem Web" (APACHE bug COCOON-1254), "wqqwqw" (ECLIPSE bug #145133), or just "GUI" with comment "The page is too clumsy" (MOZILLA bug #109242). It is no surprise that developers are slowed down by poorly written bug reports 1. Complete a survey on important information in bug reports and the problems they faced with them. We received a total of 156 responses to our survey (Section 2 and 3). 2. Rate the quality of bug reports from very poor to very good on a five-point Likert scale <ref type="bibr" target="#b21">[22]</ref>. We received a total of 1,186 votes for 289 randomly selected bug reports (Section 4).</p><p>In addition, we asked 1,354 reporters<ref type="foot" target="#foot_0">1</ref> from the same projects to complete a similar survey, out of which 310 responded. The results of both surveys suggest that there is a mismatch between what developers consider most helpful and what users provide. To enable swift fixing of bugs, this mismatch should be bridged, for example with tool support for reporters to furnish information that developers want. We developed a prototype tool called CUEZILLA (see Figure <ref type="figure" target="#fig_0">1</ref>), which gauges the quality of bug reports and suggests to reporters what should be added to make a bug report better.</p><p>1. CUEZILLA measures the quality of bug reports. We trained and evaluated CUEZILLA on the 289 bug reports rated by the developers (Section 5). 2. CUEZILLA provides incentives to reporters. We automatically mined the bug databases for encouraging facts such as "Bug reports with stack traces are fixed sooner" (Section 6).  To summarize, this paper makes the following contributions:</p><p>1. a survey on how bug reports are used among 2,226 developers and reporters, out of which 466 responded;</p><p>2. empirical evidence for a mismatch between what developers expect and what reporters provide;</p><p>3. the CUEZILLA tool that measures the quality of bug reports and suggests how reporters could enhance their reports, so that their problems get fixed sooner.</p><p>We conclude this paper with threats to validity (Section 7), related work (Section 8), and future research directions (Section 9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SURVEY DESIGN</head><p>To collect facts on how developers use the information in bug reports and what problems they face, we conducted an online survey among the developers of APACHE, ECLIPSE, and MOZILLA. In addition, we contacted bug reporters to find out what information they provide and which is most difficult to provide. For any survey, the response rate is crucial to draw generalizations from a population. Keeping a questionnaire short is one key to a high response rate. In our case, we aimed for a total completion time of five minutes, which we also advertised in the invitation email ("we would much appreciate five minutes of your time").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Selection of Participants</head><p>Each examined projects' bug database contains several hundred developers that are assigned to bug reports. Of these, we selected only experienced developers for our survey since they have a better knowledge of fixing bugs. We defined experienced developers as those assigned to at least 50 bug reports in their respective projects. Similarly, we contacted only experienced reporters, which we defined as having submitted at least 25 bug reports (=a user) while at the same time being assigned to zero bugs (=not a developer) in the respective projects. Several responders in the reporter group pointed out that they had some development experience, though mostly in other software projects.</p><p>Table <ref type="table" target="#tab_0">1</ref> presents for each project the number of developers and reporters contacted via personalized email, the number of bounces, and the number of responses and comments received. The response rate was highest for MOZILLA reporters at 32.7%. Our overall response rate of 23.2% is comparable to other Internet surveys in software engineering, which range from 14% to 20% <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Questionnaire</head><p>Keeping the five minute rule in mind, we asked developers the following questions, which we grouped into three parts (see Figure <ref type="figure">2</ref>): Contents of bug reports. Which items have developers previously used when fixing bugs? Which three items helped the most?</p><p>Such insight aids in guiding reporters to provide or even focus on information in bug reports that is most important to developers. We provided sixteen items selected on the basis of Eli Goldberg's bug writing guidelines <ref type="bibr" target="#b12">[13]</ref>; or being standard fields in the BUGZILLA database. Developers were free to check as many items for the first question (D1), but at most three for the second question (D2), thus indicating the importance of items.</p><p>Problems with bug reports. Which problems have developers encountered when fixing bugs? Which three problems caused most delay in fixing bugs?</p><p>Our motivation for this question was to find prominent obstacles that can be tackled in the future by more cautious, and perhaps even automated, reporting of bugs. Typical problems are when reporters accidentally provide incorrect information, for example, an incorrect operating system. 2 Other problems in bug reports include poor use of language (ambiguity), bug duplicates, and incomplete information. Spam recently has become a problem, especially for the TRAC issue tracking system. We decided not to include the problem of incorrect assignments to developers because bug reporters have little influence on the triaging of bugs.</p><p>In total, we provided twenty-one problems that developers could select. Again, they were free to check as many items for the first question (D3), but at most three for the second question (D4).</p><p>For the reporters of bugs, we asked the following questions (again see Figure <ref type="figure">2</ref>):</p><p>Contents of bug reports. Which items have reporters previously provided? Which three items were most difficult to provide?</p><p>We listed the same sixteen items to reporters, which we have listed to developers before. This allowed us to check whether the information provided by reporters is in line with what developers frequently use or consider to be important (by comparing the results for R1 with D1 and D2). The second question helped us to identify items, which are difficult to collect and for which better tools might support reporters in this task.</p><p>Reporters were free to check as many items for the first question (R1), but at most three for the second question (R2).</p><p>Contents considered to be relevant. Which three items do reporters consider to be most relevant for developers?</p><p>Again we listed the same items to see how much reporters agree with developers (comparing R3 with D2). For this question (R3), reporters were free to check at most three items, but could choose any item, regardless whether they selected it for question R1.</p><p>Additionally, we asked both developers and reporters about their thoughts and experiences with respect to bug reports (D5/R4). 2 Did you know? In ECLIPSE, 205 bug reports were submitted for "Windows" but later re-assigned to "Linux".</p><p>Contents of bug reports.</p><p>D1: Which of the following items have you previously used when fixing bugs? D2: Which three items helped you the most? R1: Which of the following items have you previously provided when reporting bugs? R2: Which three items were the most difficult to provide? R3: In your opinion, which three items are most relevant for developers when fixing bugs? </p><formula xml:id="formula_0">K product K hardware K observed behavior K screenshots K component K operating system K expected behavior K code examples K version K summary K steps</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Parallelism between Questions</head><p>In the first two parts of the developer survey and the first part of the reporter survey, questions share the same items but have different limitations (select as many as you wish vs. the three most important). We will briefly explain the advantages of this parallelism using D1 and D2 as examples.</p><p>1. Consistency check. When fixing bugs, all items that helped a developer the most (selected in D2) must have been used previously (selected in D1). If this is not the case, i.e., an item is selected in D2 but not in D1, the entire response is regarded inconsistent and discarded.</p><p>2. Importance of items. We can additionally infer the importance of individual items. For instance, for item i, let ND1(i) be the number of responses in which it was selected in question D1. Similarly ND1,D2(i) is the number of responses in which the item was selected in both questions D1 and D2. 3  Then the importance of item i corresponds to the conditional likelihood that item i is selected in D2, when selected in D1.</p><formula xml:id="formula_1">Importance(i) = ND1,D2(i) ND1(i)</formula><p>Other parallel questions were D3 and D4 as well as R1 and R2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SURVEY RESULTS</head><p>In this section, we discuss our findings from the survey responses.</p><p>For developers, we received a total of 156 responses, out of which 26 (or 16.7%) failed the consistency check and were removed from our analysis. For reporters we received 310 and had to remove 95 inconsistent responses (30.6%). The results of our survey are summarized in Table <ref type="table" target="#tab_2">2</ref> (for developers) and Table <ref type="table">3</ref> (for reporters). In the tables, responses for each item are annotated as bars ( ), which can be broken down into their constituents and interpreted as below (again, explained with D1 and D2 as examples): 3 When all responses are consistent, ND1,D2(i) = ND2(i) holds.</p><p>All consistent responses for the project Number of times that item was selected in D1 Number of times that item was selected in D1 and D2 Number of times that item was selected in D1 but not D2</p><p>The colored part ( + ) denotes the count of responses for an item in question D1; and the black part ( ) of the bar denotes the count of responses for the item in both question D1 and D2. The larger the black bar is in proportion to the grey bar, the higher is the corresponding item's importance in the developers' perspective. The importance of every item is listed in parentheses.</p><p>Tables <ref type="table" target="#tab_2">2</ref> and<ref type="table">3</ref> present the results for all three projects combined. For project-specific tables, we refer to our technical report <ref type="bibr" target="#b4">[5]</ref>. For the importance of items, steps to reproduce stand out clearly. Next in line are stack traces and test cases, both of which help narrowing down the search space for defects. Observed behavior, albeit weakly, mimics steps to reproduce the bug, which is why it may be rated important. Screenshots were rated as high, but often are helpful only for a subset of bugs, e.g., GUI errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Contents of Bug Reports (Developers)</head><p>Smaller surprises in the results are the relative low importance of items such as expected behavior, code examples, summary and mandatory fields such as version, operating system, product, and hardware. As pointed out by a MOZILLA developer, not all projects need the information that is provided by mandatory fields: "That's why product and usually even component information is irrelevant to me and that hardware and to some degree [OS] fields are rarely needed as most our bugs are usually found in all platforms."</p><p>In any case, we advise caution when interpreting these results: items with low importance in our survey are not totally irrelevant because they still might be needed to understand, reproduce, or triage bugs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Contents of Bug Reports (Reporters)</head><p>The items provided by most reporters are listed in the first part of Table <ref type="table">3</ref>. As expected observed and expected behavior and steps to reproduce rank highest. Only few users added stack traces, code examples, and test cases to their bug reports. An explanation might be the difficulty to provide these items, which is reported in parentheses. All three items rank among the more difficult items, with test cases being the most difficult item. Surprisingly, steps to reproduce and component are considered being difficult as well. For the latter, reporters revealed in their comments that often it is impossible for them to locate the component in which a bug occurs.</p><p>Among the items considered to be most helpful to developers, reporters ranked steps to reproduce and test cases highest. Comparing the results for test cases among all three questions reveals that most reporters consider them to be helpful, but only few provide them because they are most difficult to provide. This suggests that capture/replay tools which record test cases <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38]</ref> should be integrated into bug tracking systems. A similar but weaker observation can be made for stack traces, which are often hidden in log files and difficult to find. On the other hand, both developers and reporters consider components only as marginally important, however, as discussed above they are rather difficult to provide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Evidence for Information Mismatch</head><p>We compared the results from the developer and reporter surveys to find out whether they agree on what is important in bug reports.</p><p>First we compared which information developers use to resolve bugs (question D1) and which information reporters provide (R1). In Figure <ref type="figure" target="#fig_2">3</ref>(a), items in the left column are sorted decreasingly by the percentage of developers who have used them, while items in the right column are sorted decreasingly by the percentage of reporters who have provided them. Lines connect same items across columns and indicate the agreement (or disagreement) between developers and reporters on that particular item. Figure <ref type="figure" target="#fig_2">3(a)</ref> shows that the results match only for the top three items and the last one. In between there are many disagreements, the most notable ones for stack traces, test cases, code examples, product, and operating system. Overall, the Spearman correlation between what developers use and what reporters provide was 0.321, far from being ideal. <ref type="foot" target="#foot_1">4</ref>Next we checked whether reporters provide the information that is most important for developers. In Figure <ref type="figure" target="#fig_2">3(b)</ref>, the left column corresponds to the importance of an item for developers (measured by questions D2 and D1), and the right column to the percentage of reporters who provided an item (R1). Developers and reporters still agree on the first and last item, however, overall the disagreement increased. The Spearman correlation of -0.035 between what developers consider as important and what reporters provide shows a huge gap. In particular, it indicates that reporters do not focus on the information important for developers.</p><p>Interestingly, Figure <ref type="figure" target="#fig_2">3</ref>(c) shows that most reporters know which information developers need. In other words, ignorance of reporters is not a reason for the aforementioned information mismatch. As before the left column corresponds to the importance of items for  As a consequence, to improve bug reporting systems, one could tell users while they are reporting a bug what information is important (e.g., screenshots). At the same time one should provide better tools to collect important information, because often this information is difficult to obtain for users (see Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Problems with Bug Reports</head><p>Among the problems experienced by developers, incomplete information was, by far, most commonly encountered. Other common problems include errors in steps to reproduce and test cases; bug duplicates; and incorrect version numbers, observed and expected behavior. Another issue that developers often seemed challenged by is the fluency in language of the reporter. Most of these problems are likely to lead developers astray when fixing bugs.</p><p>The most severe problems were errors in steps to reproduce and incomplete information. In fact, in question D5 many developers commented on being plagued by bug reports with incomplete information:</p><p>"The biggest causes of delay are not wrong information, but absent information."</p><p>Other major problems included errors in test cases and observed behavior. A very interesting observation is that developers do not suffer too much from bug duplicates, although earlier research considered this to be a serious problem <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34]</ref>). Possibly, developers can easily recognize duplicates, and sometimes even benefit by a different bug description. As commented by one developer: "Duplicates are not really problems. They often add useful information. That this information were filed under a new report is not ideal thought."</p><p>The low occurrence of spam is not surprising: in BUGZILLA and JIRA, reporters have to register before they can submit this registration successfully prevents spam. Lastly, errors in stack traces are highly unlikely because they are copy-pasted into bug reports, but when an error happens, it can be a severe problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Developer Comments</head><p>We received 48 developer comments in the survey responses. Most comments stressed the importance of clear, complete, and correct bug descriptions. However, some revealed additional problems: Different knowledge levels. "In OSS, there is a big gap with the knowledge level of bug reporters. Some will include exact locations in the code to fix, while others just report a weird behavior that is difficult to reproduce."</p><p>Violating netiquette. "Another aspect is politeness and respect.</p><p>If people open rude or sarcastic bugs, it doesn't help their chances of getting their issues addressed."</p><p>Complicated steps to reproduce. This problem was pointed out by several developers: "If the repro steps are so complex that they'll require more than an hour or so (max) just to set up would have to be quite serious before they'll get attention." Another one: "This is one of the greatest reasons that I postpone investigating a bug. . . if I have to install software that I don't normally run in order to see the bug."</p><p>Misuse of bug tracking system. "Bugs are often used to debate the relative importance of various issues. This debate tends to spam the bugs with various use cases and discussions [. . . ] making it harder to locate the technical arguments often necessary for fixing the bugs. Some long-lived high-visibility bugs are especially prone to this."</p><p>Also, some developers pointed out situations where bug reports get preferred treatment:</p><p>Human component. "Well known reporters usually get more consideration than unknown reporters, assuming the reporter has a pretty good history in bug reporting. So even if a "wellknown" reporter reports a bug which is pretty vague, he will get more attention than another reporter, and the time spent trying to reproduce the problem will also be larger."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Keen bug reporters.</head><p>A developer wrote about reporters who identify offending code: "I feel that I should at least put in the amount of effort that they did; it encourages this behavior." </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RATING BUG REPORTS</head><p>After completing the questionnaire, participants were asked to continue with a voluntary part of our survey. We presented randomly selected bug reports from their projects and asked them to rate the quality of these reports. Being voluntary, we did not mention this part in the invitation email. While we asked both developers and reporters to rate bug reports, we will use only the ratings by developers in this paper, as they are more qualified to judge what is a good bug report.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Rating Infrastructure</head><p>The rating system was inspired by Internet sites such as RateMy-Face <ref type="bibr" target="#b28">[29]</ref> and HotOrNot <ref type="bibr" target="#b14">[15]</ref>. We drew a random sample of 100 bugs from the projects' bug database, which were presented one by one to the participants in a random order. They were required to read through the bug report and rate it on a five-point Likert scale ranging from very poor (1) to very good (5) (see Figure <ref type="figure" target="#fig_3">4</ref> for a screenshot). Once they rated a bug report, the screen showed the next random report and the average quality rating of the previously rated report on the left. On the right, we provided a skip button, which as the name suggests, skips the current report and navigates to the next one. This feature seemed preferable to guesswork on part of the participants, in cases where they lacked the knowledge to rate a report. Participants could stop the session at any time or choose to continue until all 100 bugs had been rated. These quality ratings by developers served two purposes:</p><p>1. They allowed us to verify the results of the questionnaire on concrete examples, i.e., whether reports with highly desired elements are rated higher for their quality and vice versa.</p><p>2. These scores were later used to evaluate our CUEZILLA tool that measures bug report quality (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Rating Results</head><p>The following number of developer votes for bug reports were received for the samples of 100 bugs from each project: 229 for APACHE, 397 for ECLIPSE, and 560 for MOZILLA. Figure <ref type="figure" target="#fig_4">5</ref> plots the distribution of the ratings, which is similar across all projects, with the most frequent ratings being 3 (average) and 4 (good). Table <ref type="table" target="#tab_3">4</ref> lists the bug reports that were rated highest and lowest by ECLIPSE developers. Some bug reports were found to be of  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Concordance between Developers</head><p>We also investigated the concordance between developers on their evaluation of the quality of bug reports. It seems reasonable to assume that developers with comparable experiences have compatible views on the quality of bug reports. However, there may be exceptions to our belief or it may simply be untrue. We statistically verified this by examining the standard deviations of quality ratings by developers (œÉrating) for the bug reports. Larger values of œÉrating indicate higher differences between developers' view of quality for a bug report. Of the 289 bug reports rated across all three projects, only 23 (which corresponds to 8%) had œÉrating &gt; 1.5. These results show that developers generally agree on the quality of bug reports. Thus, it is feasible to use their ratings to build a tool that learns from bug reports to measure the quality of new bug reports. We present a prototype of such a tool in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">MEASURING BUG REPORT QUALITY</head><p>In Section 3.3, we showed that the majority of reporters know what is important in bug reports. However, we also found evidence for an information mismatch: the importance of some items, e.g., screenshots, is not recognized by users. There are also black sheep among users who do not know yet how to write good bug reports. In general, humans can benefit from cues while undertaking tasks, which was demonstrated in software engineering by Passing and Shepperd <ref type="bibr" target="#b26">[27]</ref>. They examined how subjects revised their cost estimates of projects upon being presented checklists relevant to estimation.</p><p>Our conjecture is that bug reporters can provide better reports with similar assistance. As a first step towards assistance, we developed a prototype tool -CUEZILLA that measures the quality of bug reports; and provides suggestions to reporters on how to enhance the quality. For example, "Have you thought about adding a screenshot to your bug report?"</p><p>This section presents details on how CUEZILLA works and reports results of its evaluation at measuring quality of bug reports. To create recommendations, CUEZILLA first represents each bug report as a feature vector (Section 5.1). Then it uses supervised learning to train models (Section 5.2) that measure the quality of bug reports (Section 5.3 and 5.4). Our models can also quantify the increase in quality, when elements are added to bug reports (Section 5.5). In contrast to other quality measures for bug reports such as lifetime <ref type="bibr" target="#b13">[14]</ref>, we use the ratings that we received by developers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Input Features</head><p>Our CUEZILLA tool measures quality of bug reports on the basis of their contents. From the survey, we know the most desired features in bug reports by developers. Endowed with this knowledge, CUEZILLA first detects the features listed below. For each feature a score is awarded to the bug report, which is either binary (e.g., attachment present or not) or continuous (e.g., readability).</p><p>Itemizations. In order to recognize itemizations in bug reports, we checked whether several subsequent lines started with an itemization character (such as -, * , or +). To recognize enumerations, we searched for lines starting with numbers or single characters that were enclosed by parenthesis or brackets or followed by a single punctuation character.</p><p>Keyword completeness. We reused the data set provided by Andy Ko et al. <ref type="bibr" target="#b19">[20]</ref> to define a quality-score of bug reports based on their content. In a first step, we removed stop words, reduced the words to their stem, and selected words occurring in at least 1% of bug reports. Next we categorized the words into the following groups:</p><p>-action items (e.g., open, select, click) -expected and observed behavior (e.g., error, missing) -steps to reproduce (e.g., steps, repro) -build-related (e.g., build) -user interface elements (e.g., toolbar, menu, dialog)</p><p>In order to assess the completeness of a bug report, we computed for each group a score based on the keywords present in the bug report. The maximum score of 1 for a group is reached when a keyword is found.</p><p>In order to obtain the final score (which is between 0 and 1), we averaged the scores of the individual groups.</p><p>In addition to the description of the bug report, we analyze the attachments that were submitted by the reporter within 15 minutes after the creation of the bug report. In the initial description and attachments, we recognize the following features:</p><p>Code Samples. We identify C++ and JAVA code examples using techniques from island parsing <ref type="bibr" target="#b23">[24]</ref>. Currently, our tools can recognize declarations (for classes, methods, functions, and variables), comments, conditional statements (such as if and switch), and loops (such as for and while).</p><p>Stack Traces. We currently can recognize JAVA stack traces, GDB stack traces, and MOZILLA talkback data. Stack traces are easy to recognize with regular expressions: they consist of a start line (that sometimes also contains the top of the stack) and trace lines.</p><p>Patches. In order to identify patches in bug reports and attachments we again used regular expressions. They consist of several start lines (which file to patch) and blocks (which are the changes to make) <ref type="bibr" target="#b22">[23]</ref>.</p><p>Screenshots. We identify the type of an attachment using the file tool in UNIX. If an attachment is an image, we recognize it as a screenshot. If the file is recognized as text, we process the file and search for code examples, stack traces, and patches (see above).</p><p>For more details about extraction of structural elements from bug reports we refer to our previous work <ref type="bibr" target="#b6">[7]</ref>, in which we showed that we can identify the above features with a close to perfect precision. After cleaning the description of a bug report from source code, stack traces, and patches, we compute its readability.</p><p>Readability. To compute readability we use the style tool, which "analyses the surface characteristics of the writing style of a document" <ref type="bibr" target="#b9">[10]</ref>. It is important to not confuse readability with grammatical correctness. The readability of a text is measured by the number of syllables per word and the length of sentences. Readability measures are used by Amazon.com to inform customers about the difficulty of books and by the US Navy to ensure readability of technical documents <ref type="bibr" target="#b18">[19]</ref>.</p><p>In general, the higher a readability score the more complex a text is to read. Several readability measures return values that correspond to school grades. These grades tell how many years of education a reader should have before reading the text without difficulties. For our experiments we used the following seven readability measures: Kincaid, Automated Readability Index (ARI), Coleman-Liau, Flesh, Fog, Lix, and SMOG Grade.<ref type="foot" target="#foot_2">5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Setup</head><p>Out of the 300 bug reports in the sample, developers rated 289 bug reports at least once. These reports were used to train and evaluate CUEZILLA by building supervised learning models. We used the following three models: support vector machines (SVM), generalized linear regression (GLR), and stepwise linear regression <ref type="bibr" target="#b34">[35]</ref>.</p><p>Each model used the scores from the features described in Section 5.1 as input variables and predicted the average developer rating as output variable. We evaluated CUEZILLA using two setups: Within project. To test how well models predict within a project, we used the leave-one-out cross-validation technique. This means that for a given project, the quality of each bug report is predicted using all other bug reports to train the model. Since we have limited data, we chose this setup to maximize the training data to build the prediction models.</p><p>Across projects. We also tested if models from one project can be transferred to others. To exemplify, we built a model from all rated bug reports of project A, and applied it to predict the quality of all rated bugs in project B.</p><p>Table <ref type="table" target="#tab_4">5</ref> shows the results for ECLIPSE bug reports and stepwise linear regression using leave-one-out cross-validation. The column names in the table indicate the average rating of the bug report by developers (Observed); the row names denote the quality measured by CUEZILLA (Measured). The ranges within the square brackets next to the row names indicate the equidistant mapping of predicted values to the Likert scale. The counts in the diagonal cells, with a dark gray background, indicate the number of bug reports for which there was complete agreement between CUEZILLA and developers on their quality. In Table <ref type="table" target="#tab_4">5</ref>, this is true for 44% of the bug reports. In addition, we also look at predictions that are off by one from the developer ratings. These are the cells in the tables that are one row, either to the left or right of the diagonal. Using perfect and off-by-one agreements, the accuracy increases to 87%, in other words only 1 in 10 recommendations are off by more than one scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation within Projects</head><p>Results from predicting the quality of bug reports using other bug reports from the same project (with leave-one-out cross-validation) are presented in Table <ref type="table" target="#tab_5">6</ref>. The first number is the percentage of bug reports with perfect agreement on the quality between CUEZILLA and the developers, while the number in the parentheses indicates the percentage for off-by-one accuracy. Of the three models used, support vector machines appear to provide more number of perfect agreements than other techniques. In case of off-by-one agreements, stepwise linear regression outperforms the two other models. But on the whole, all three models seem to perform comparably across the projects. The figures also show that a higher proportion of perfect agreements were made for ECLIPSE bug reports than for APACHE and MOZILLA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation across Projects</head><p>In Table <ref type="table" target="#tab_6">7</ref>, we present the results from the across projects evaluation setup. The bars in the table can be interpreted in a similar fashion as before in Tables <ref type="table" target="#tab_2">2</ref> and<ref type="table">3</ref>. Here, the bars have the following meanings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of unique bugs rated by developers Number of perfect agreements Number of off-by-one agreements</head><p>The accuracy of CUEZILLA is represented by the black bar ( ) and the off-by-one accuracy by the overall shaded part (</p><p>). In order to facilitate comparison, Table <ref type="table" target="#tab_6">7</ref> also contains the results from the within project evaluation (for which the bars have a thinner border).</p><p>The results in Table <ref type="table" target="#tab_6">7</ref> show that models trained from one project can be transferred to other projects without much loss in predictive power. However, we can observe more variability in prediction accuracy for stepwise and generalized linear regression. It is interesting to note that models using data from APACHE and MOZILLA are both good at predicting quality of ECLIPSE bug reports. One can infer from these results that CUEZILLA's models are largely portable across projects to predict quality, but they are best applied within projects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Recommendations by CUEZILLA</head><p>The core motivation behind CUEZILLA is to help reporters file better quality bug reports. For this, its ability to detect the presence of information features can be exploited to tip reporters on what information to add. This can be achieved simply by recommending additions from the set of absent information, starting with the feature that contributes to the quality further by the largest margin. These recommendations are intended to serve as cues or reminders to reporters of the possibility to add certain types of information; likely to improve bug report quality.</p><p>The left panel of Figure <ref type="figure" target="#fig_0">1</ref> illustrates the concept. The text in the panel is determined by investigating the current contents of the report, and then determining that would be best, for instance, adding a code sample to the report. As and when new information is added to the bug report, the quality meter revises its score.</p><p>Our evaluation of CUEZILLA shows much potential for incorporating such a tool in bug tracking systems. CUEZILLA is able to measure quality of bug reports within reasonable accuracy. However, the presented version of CUEZILLA is an early prototype and we plan to further enhance the tool and conduct experiments to show its usefulness. We briefly discuss our plans in Section 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">INCENTIVE FOR REPORTERS</head><p>If CUEZILLA tips reporters on how to enhance quality of their bug reports, one question comes to mind -"What are the incentives for reporters to do so?" Of course, well-described bug reports help comprehending the problem better, consequently increasing the likelihood of the bug getting fixed. But to explicitly show evidence of the same to reporters, CUEZILLA randomly presents relevant facts that are statistically mined from bug databases. In this section, we elaborate upon how this is executed, and close with some facts found in the bug databases of the three projects.</p><p>To reduce the complexity of mining the several thousand bug reports filed in bug databases, we sampled 50,000 bugs from each project. These bugs had various resolutions, such as FIXED, DU-PLICATE, MOVED, WONTFIX, and WORKSFORME. Then, we computed the scores for all items listed in Section 5.1 for each of the 150,000 bugs. To recall, the scores for some of the items are continuous values, while others are binary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Relation to Resolution of Bug Reports</head><p>A bug being fixed is a mark of success for both, developers and reporters. But what items in bug reports increase the chances of the bug getting fixed? We investigate this on the sample of bugs described above for each project.</p><p>First, we grouped bug reports by their resolutions as: FIXED, DUPLICATE, and OTHERS. The FIXED resolution is most desired and the OTHERS resolution-that includes MOVED, WONTFIX and the likes-are largely undesired. We chose to examine DUPLICATE as a separate group because this may potentially reveal certain traits of such bug reports. Additionally, as pointed above, duplicates may provide more information about the bug to developers.</p><p>For binary valued features, we performed Chi-Square tests <ref type="bibr" target="#b30">[31]</ref> (p 0.05) on the contingency tables of the three resolution groups and the individual features for each project separately. The tests' results indicate whether the presence of the features in bug reports significantly determine the resolution category of the bug. For example, the presence of stack traces significantly increases the likelihood of a FIXED desirable resolution.</p><p>In case of features with continuous valued scores, we performed a Kruskal-Wallis test <ref type="bibr" target="#b30">[31]</ref> (p &lt; 0.05) on the distribution of scores across the three resolution groups to check whether the distribution significantly differ from one group to another. For example, bug reports with FIXED resolutions have significantly lower SMOGgrades than reports with OTHERS resolutions; indicating that reports are best written using simple language constructs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Relation to Lifetime of Bug Reports</head><p>Another motivation for reporters is to see what items in bug reports help making the bugs' lifetimes shorter. Such motivations are likely to incline reporters to furnish more helpful information. We mined for such patterns on a subset of the above 150,000 bugs with resolution FIXED only.</p><p>For items with binary scores, we grouped bug reports by their binary scores, for example, bugs containing stack traces and bugs not containing stack traces. We compared the distribution of the lifetimes of the bugs and again, performed a Kruskal-Wallis test <ref type="bibr" target="#b30">[31]</ref> (p &lt; 0.05) to check for statistically significant differences in distributions. This information would help encourage reporters to include items that can reduce lifetimes of the bugs.</p><p>In the case of items with continuous valued scores, we first dichotomized the lifetime into two categories: bugs resolved quickly vs. bugs resolved slowly. We then compared the distribution of the scores across the two categories using the Kruskal-Wallis test <ref type="bibr" target="#b30">[31]</ref> (p &lt; 0.05) to reveal statistically significant patterns. Differences in distributions could again be used to motivate users to aim at achieving scores for their reports that are likely to have lower lifetimes.</p><p>In our experiments we used one hour, one day, and one week as boundaries for dichotomization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Results</head><p>This section lists some of the key statistically significant patterns found in the sample of 150,000 bug reports. These findings can be presented in interfaces of bug tracking systems (see the right part of Figure <ref type="figure" target="#fig_0">1</ref>). A sample of our key findings are listed below:</p><p>‚Ä¢ Bug reports containing stack traces get fixed sooner.</p><p>(APACHE/ECLIPSE/MOZILLA)</p><p>‚Ä¢ Bug reports that are easier to read have lower lifetimes.</p><p>(APACHE/ECLIPSE/MOZILLA)</p><p>‚Ä¢ Including code samples in your bug report increases the chances of it getting fixed. (MOZILLA)</p><p>We are not the first to find factors that influence the lifetime of bug reports. Independently from us, Hooimeijer and Weimer <ref type="bibr" target="#b13">[14]</ref> observed for FIREFOX that bug reports with attachments get fixed later, while bug reports with many comments get fixed sooner. They also confirmed our results that easy-to-read reports are fixed faster.</p><p>Panjer observed for ECLIPSE that comment count and activity as well as severity affect the lifetime the most <ref type="bibr" target="#b25">[26]</ref>.</p><p>In contrast, our findings are for factors that can be determined while a user is reporting a bug. Each finding suggests a way to increase the likelihood of their bugs to either get fixed at all, or get fixed faster. Keen users are likely to pick up on such cues since this can lessen the amount of time they have to deal with the bug.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">THREATS TO VALIDITY</head><p>For our survey we identified the following threats to validity.</p><p>Our selection of developers was constrained to only experienced developers; in our context, developers who had at least 50 bugs assigned to them. While this skews our results towards developers who frequently fix bugs, they are also the ones who will benefit most by an improved quality of bug reports. The same discussion applies to the selection of reporters.</p><p>A related threat is that to some extent our survey operated on a self-selection principle: the participation in the survey was voluntary. As a consequence, results might be skewed towards people that are likely to answer the survey, such as developers and users with extra spare time-or who care about the quality of bug reports.</p><p>Avoiding the self-selection principle is almost impossible in an open-source context. While a sponsorship from the Foundations of APACHE, ECLIPSE, and MOZILLA might have reduced the amount of self-selection, it would not have eliminated skew. As pointed out by Singer and Vinson the decision of responders to participate "could be unduly influenced by the perception of possible benefits or reprisals ensuing from the decision" <ref type="bibr" target="#b31">[32]</ref>.</p><p>In order to take as little time as possible of participants, we constrained the selection of items in our survey. While we tried to achieve completeness, we were aware that our selection was not exhaustive of all information used and problems faced by developers. Therefore, we encouraged participants to provide us with additional comments, to which we received 175 responses. We could not include the comments into the statistical analysis; however, we studied and discussed the comments by developers in Section 3.</p><p>As with any empirical study, it is difficult to draw general con-clusions because any process depends on a large number of context variables <ref type="bibr" target="#b2">[3]</ref>. In our case, we contacted developers and users of three large open-source initiatives, namely APACHE, ECLIPSE, and MOZILLA. We are confident that our findings also apply to smaller open-source projects. However, we do not contend that they are transferable to closed-software projects (which have no patches and rarely stack traces). In future work, we will search for evidence for this hypothesis and point out the differences in quality of bug reports between open-source and closed-source development.</p><p>A common misinterpretation of empirical studies is that nothing new is learned ("I already knew this result"). Unfortunately, some readers miss the fact that this wisdom has rarely been shown to be true and is often quoted without scientific evidence. This paper provides such evidence: most common wisdom is confirmed (e.g., "steps to reproduce are important") while is challenged ("bug duplicates considered harmful").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">RELATED WORK</head><p>So far, mostly anecdotical evidence has been reported on what makes a good bug report. For instance, Joel Spolsky described how to achieve painless bug tracking <ref type="bibr" target="#b32">[33]</ref> and numerous articles and guidelines on effective bug reporting float around the Internet (e.g., <ref type="bibr" target="#b12">[13]</ref>). Still, the results from our survey suggest that bug reporting is far from being painless.</p><p>The work closest to ours is by Hooimeijer and Weimer who built a descriptive model for the lifetime of a bug report <ref type="bibr" target="#b13">[14]</ref>. They assumed that the "time until resolved" is a good indicator for the quality of a bug report. In contrast, our notion of quality is based on feedback from developers (1,186 votes). When we compared the ratings of the bug reports with lifetime, the Spearman correlation values were between 0.002 and 0.068, indicating that lifetime as measured by Hooimeijer and Weimer <ref type="bibr" target="#b13">[14]</ref> and quality are independent measures. Often a bug report that gets addressed quicker can be of poor quality, but describes an urgent problem. Also, a wellwritten bug report can be complicated to deal with and take more time to resolve. Still, knowing what contributes to the lifetime of bug reports <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26]</ref>, can encourage users to submit better reports as discussed in Section 6.</p><p>In a workshop paper, we presented preliminary results from the developer survey on the ECLIPSE project using a handcrafted prediction model <ref type="bibr" target="#b3">[4]</ref>. In other work, we quantified the amount of additional information in bug duplicates <ref type="bibr" target="#b5">[6]</ref> and gave recommendations on how to improve existing bug tracking systems <ref type="bibr" target="#b16">[17]</ref>.</p><p>Several studies used bug reports to automatically assign developers to bug reports <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref>, assign locations to bug reports <ref type="bibr" target="#b7">[8]</ref>, track features over time <ref type="bibr" target="#b11">[12]</ref>, recognize bug duplicates <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30]</ref>, and predict effort for bug reports <ref type="bibr" target="#b36">[37]</ref>. All these approaches should benefit by our quality measure for bug reports since training only with high-quality bug reports will likely improve their predictions.</p><p>In 2004, Antoniol et al. <ref type="bibr" target="#b0">[1]</ref> pointed out the lack of integration between version archives and bug databases, which make it hard to locate the most faulty methods in a system. In the meantime things have changed: the Mylyn tool by Kersten and Murphy <ref type="bibr" target="#b17">[18]</ref> allows to attach a task context to bug reports so that changes can be tracked on a very fine-grained level.</p><p>In order to inform the design of new bug reporting tools, Ko et al. <ref type="bibr" target="#b19">[20]</ref> conducted a linguistic analysis of the titles of bug reports. They observed a large degree of regularity and a substantial number of references to visible software entities, physical devices, or user actions. Their results suggest that future bug tracking systems should collect data in a more structured way.</p><p>According to the results of our survey, errors in steps to reproduce are one of the biggest problems faced by developers. This demonstrates the need for tools that can capture the execution of a program on user-side and replay it on developer-side. While there exist several capture/replay techniques (such as <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b37">38]</ref>), their user-orientation and scalability can still be improved.</p><p>Not all bug reports are generated by humans. Some bug-finding tools can report violations of safety-policies and annotate them with back-traces or counterexamples. Weimer presented an algorithm to construct such patches automatically. He also found that automatically generated "reports also accompanied by patches were three times as likely to be addressed as standard bug reports" <ref type="bibr" target="#b35">[36]</ref>.</p><p>Furthermore, users can help developers to fix bugs without filing bug reports. For example, many products ship with automated crash reporting tools that collect and send back crash information, e.g., Apple CrashReporter, Windows Error Reporting, Gnome Bug-Buddy, Mozilla Talkback. Liblit et al. introduced statistical debugging <ref type="bibr" target="#b20">[21]</ref>. They distribute specially modified versions of software, which monitor their own behavior while they run and report back how they work. This information is then used to isolate bugs using statistical techniques. Still, since it is unclear how to extract sufficient information for rarely occurring and non-crashing bugs, there will always be the need for manual bug reporting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">CONCLUSION AND CONSEQUENCES</head><p>Well-written bug reports are likely to get more attention among developers than poorly written ones. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to find out what makes a good bug report. The results suggest that, across all three projects, steps to reproduce and stack traces are most useful in bug reports. The most severe problems encountered by developers are errors in steps to reproduce, incomplete information, and wrong observed behavior. Surprisingly, bug duplicates are encountered often but not considered as harmful by developers. In addition, we found evidence for a mismatch between what information developers consider as important and what users provide. To a large extent, lacking tool support causes this mismatch.</p><p>We also asked developers to rate the quality of bug reports on a scale from one (poor quality) to five (excellent quality). Based on these ratings, we developed a tool, CUEZILLA that measures the quality of bug reports. This tool can rate up to 41% bug reports in complete agreement with developers. Additionally, it recommends what additions can be made to bug reports to make their quality better. To provide incentive for doing so, CUEZILLA automatically mines patterns that are relevant to fixing bugs and presents them to users. In the long term, an automatic measure of bug report quality in bug tracking systems can ensure that new bug reports meet a certain quality level. Our future work is as follows:</p><p>Problematic contents in reports. Currently, we award scores for the presence of desired contents, such as itemizations and stack traces. We plan to extend CUEZILLA to identify problematic contents such as errors in steps to reproduce and code samples in order to warn the reporter in these situations.</p><p>Usability studies for new bug reporting tools. We listed several comments by developers about problems with existing bug reporting tools in Section 3. To address these problems, we plan to develop prototypes for new, improved reporting tools, which we will test with usability studies.</p><p>Impact on other research. In Section 8, we discussed several approaches that rely on bug reports as input to support developers in various tasks such as bug triaging, bug localization, and effort estimation. Do these approaches improve when trained only with high-quality bug reports?</p><p>Additionally, aiding reporters in providing better bug reports can go a long way in structuring bug reports. Such structured text may also be beneficial to researchers who use them for experiments. In effect, in the short-to medium-term, data quality in bug databases would generally increase, in turn providing more reliable and consistent data to work with and feedback to practitioners.</p><p>To learn more about our work in mining software archives, visit http://www.softevo.org/</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Mockup of CUEZILLA's user interface. It recommends improvements to the report (left image). To encourage the user to follow the advice, CUEZILLA provides facts that are mined from history (right image).</figDesc><graphic coords="1,316.81,579.42,239.10,94.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Information used by developers vs. provided by reporters. (b) Most helpful for developers vs. provided by reporters.(c) Most helpful for developers vs. reporters expected to be helpful.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Mismatch between developers and reporters.</figDesc><graphic coords="5,53.80,58.78,150.63,174.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Screenshot of interface for rating bug reports</figDesc><graphic coords="6,53.80,53.80,239.10,130.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Distribution of ratings by developers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Number of invitations sent to and responses by developers and reporters of the APACHE, ECLIPSE, and MOZILLA projects.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Developers</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Reporters</cell><cell></cell></row><row><cell>Project</cell><cell cols="10">Contacted Bounces Reached Responses (Rate) Comments Contacted Bounces Reached Responses (Rate) Comments</cell></row><row><cell>APACHE</cell><cell>194</cell><cell>5</cell><cell>189</cell><cell>34 (18.0%)</cell><cell>12</cell><cell>165</cell><cell>17</cell><cell>148</cell><cell>37 (25.0%)</cell><cell>10</cell></row><row><cell>ECLIPSE</cell><cell>365</cell><cell>29</cell><cell>336</cell><cell>50 (14.9%)</cell><cell>15</cell><cell>378</cell><cell>8</cell><cell>370</cell><cell>50 (13.5%)</cell><cell>20</cell></row><row><cell>MOZILLA</cell><cell>313</cell><cell>29</cell><cell>284</cell><cell>72 (25.4%)</cell><cell>21</cell><cell>811</cell><cell>130</cell><cell>681</cell><cell>223 (32.7%)</cell><cell>97</cell></row><row><cell>Total</cell><cell>872</cell><cell>63</cell><cell>809</cell><cell>156 (19.3%)</cell><cell>48</cell><cell>1354</cell><cell>155</cell><cell>1199</cell><cell>310 (25.9%)</cell><cell>127</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 shows that the most widely used items across projects are steps to reproduce, observed and expected behavior, stack traces, and test cases. Information rarely used by developers is hardware</head><label>2</label><figDesc>and severity. ECLIPSE and MOZILLA developers favorably used screenshots, while APACHE and ECLIPSE developers more often used code examples and stack traces.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Developers rated the quality of ECLIPSE bug reports.</figDesc><table><row><cell>Bug Report</cell><cell cols="2">Votes Rating</cell></row><row><cell>Tree -Selection listener stops default expansion (#31021)</cell><cell>3</cell><cell>5.00</cell></row><row><cell>JControlModel "eats up" exceptions (#38087)</cell><cell>5</cell><cell>4.8</cell></row><row><cell>Search -Type names are lost [search] (#42481)</cell><cell>4</cell><cell>4.50</cell></row><row><cell>150M1 withincode type pattern exception (#83875)</cell><cell>5</cell><cell>4.40</cell></row><row><cell>ToolItem leaks Images (#28361)</cell><cell>6</cell><cell>4.33</cell></row><row><cell>. . .</cell><cell>. . .</cell><cell>. . .</cell></row><row><cell>Selection count not updated (#95279)</cell><cell>4</cell><cell>2.25</cell></row><row><cell cols="3">Comment out the Selection listener and now double click on</cell></row><row><cell>any tree item and notice that it expands.</cell><cell></cell><cell></cell></row><row><cell>public static void main(String[] args) {</cell><cell></cell><cell></cell></row><row><cell>Display display = new Display();</cell><cell></cell><cell></cell></row><row><cell>Shell shell = new Shell(display);</cell><cell></cell><cell></cell></row><row><cell>[. . . ] (21 lines of code removed)</cell><cell></cell><cell></cell></row><row><cell>display.dispose();</cell><cell></cell><cell></cell></row><row><cell>}</cell><cell></cell><cell></cell></row><row><cell cols="3">(ECLIPSE bug report #31021)</cell></row><row><cell cols="3">On the other hand, bug report #175222 with an average score of</cell></row><row><cell cols="3">1.57 is of fairly poor quality. Actually, this is simply not a bug</cell></row><row><cell cols="3">report and has been incorrectly filed in the bug database. Still mis-</cell></row><row><cell cols="2">filed bug reports take away valuable time from developers.</cell><cell></cell></row><row><cell cols="3">I wand to create a new plugin in Eclipse using CDT. Shall it</cell></row><row><cell cols="3">possible. I had made a R&amp;D in eclipse documentation. I had</cell></row></table><note><p><p><p><p>Outline view should [...] show all project symbols (#108759) 2 2.00 Pref Page [...] Restore Defaults button does nothing (#51558) 6 1.83 [...]&lt;Incorrect /missing screen capture&gt; (#99885) 4 1.75 Create a new plugin using CDT. (#175222) 7 1.57</p>exceptional quality, such as bug report #31021 for which all three responders awarded a score of very good (5). This report presents a code example and adequately guides the developer on its usage, and observed behavior.</p>I20030205</p>Run the following example. Double click on a tree item and notice that it does not expand. get an idea about create a plugin using Java. But i wand to create a new plugin ( user defined plugin ) using CDT. After that I wand to impliment it in my programe. If it possible?. Any one can help me please... (ECLIPSE bug report #175222)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>The results of the classification by CUEZILLA (using stepwise linear regression) compared to the developer rating.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Observed</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Measured</cell><cell cols="5">very poor poor medium good very good</cell></row><row><cell cols="2">very poor [&lt; 1.8]</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell></row><row><cell>poor</cell><cell>[1.8, 2.6]</cell><cell>0</cell><cell>2</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">medium [2.6, 3.4]</cell><cell>4</cell><cell>11</cell><cell>29</cell><cell>17</cell><cell>4</cell></row><row><cell>good</cell><cell>[3.4, 4.2]</cell><cell>0</cell><cell>1</cell><cell>6</cell><cell>12</cell><cell>5</cell></row><row><cell cols="2">very good [&gt; 4.2]</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Leave-one-out cross-validation within projects.</figDesc><table><row><cell></cell><cell>APACHE</cell><cell>ECLIPSE MOZILLA</cell></row><row><cell>Support vector machine</cell><cell cols="2">28% (82%) 48% (91%) 37% (82%)</cell></row><row><cell cols="3">Generalized linear regression 28% (82%) 40% (87%) 29% (80%)</cell></row><row><cell>Stepwise linear regression</cell><cell cols="2">31% (86%) 44% (87%) 34% (85%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Validation across projects.</figDesc><table><row><cell></cell><cell></cell><cell>Testing on</cell></row><row><cell></cell><cell>APACHE</cell><cell>ECLIPSE MOZILLA</cell></row><row><cell>Training</cell><cell>APACHE</cell><cell>SVM GLR Stepwise</cell></row><row><cell></cell><cell>ECLIPSE</cell><cell>SVM GLR Stepwise</cell></row><row><cell></cell><cell>MOZILLA</cell><cell>SVM GLR Stepwise</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Throughout this paper reporter refers to the people who create bug reports and are not assigned to any. Mostly reporters are end-users but in many cases they are also experienced developers.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>Spearman correlation computes agreement between two rankings: two rankings can be opposite (value -1), unrelated (value 0), or perfectly matched (value 1). We refer to textbooks for details<ref type="bibr" target="#b34">[35]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>This paper has a SMOG-Grade of 13, which requires the reader to have some college education. Publications with a similar SMOGgrade are often found in the New York Times.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. Many thanks to Avi Bernstein, Harald Gall, Christian Lindig, Stephan Neuhaus, Andreas Zeller, and the FSE reviewers for valuable and helpful suggestions on earlier revisions of this paper. A special thanks to everyone who responded to our survey. When this research was carried out, all authors were with Saarland University; Thomas Zimmermann was funded by the DFG Research Training Group "Performance Guarantees for Computer Systems".</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mozilla: Closing the circle</title>
		<author>
			<persName><forename type="first">G</forename><surname>Antoniol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Penta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pinzger</surname></persName>
		</author>
		<idno>TUV-1841-2004-05</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>Technical University of Vienna</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Who should fix this bug?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Anvik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hiew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSE &apos;06: Proceedings of the 28th International Conference on Software Engineering</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="361" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Building knowledge through families of experiments</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Basili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lanubile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="456" to="473" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quality of bug reports in Eclipse</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bettenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Just</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schr√∂ter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 OOPSLA Workshop on Eclipse Technology eXchange (ETX)</title>
		<meeting>the 2007 OOPSLA Workshop on Eclipse Technology eXchange (ETX)</meeting>
		<imprint>
			<date type="published" when="2007-10">October 2007</date>
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">What makes a good bug report? Version 1.1</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bettenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Just</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schr√∂ter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zimmermann</surname></persName>
		</author>
		<ptr target="http://www.st.cs.uni-sb.de/publications/details/bettenburg-tr-2008/" />
		<imprint>
			<date type="published" when="2008-03">March 2008</date>
		</imprint>
		<respStmt>
			<orgName>Saarland University, Software Engineering Chair</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>The technical report is an extended version of this paper</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Duplicate bug reports considered harmful... really?</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bettenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSM &apos;08: Proceedings of the 24th IEEE International Conference on Software Maintenance</title>
		<imprint>
			<date type="published" when="2008-09">September 2008</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extracting structural information from bug reports</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bettenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Working Conference on Mining Software Repositories</title>
		<meeting>the Fifth International Working Conference on Mining Software Repositories</meeting>
		<imprint>
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fine grained indexing of software repositories to support impact analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Canfora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cerulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MSR &apos;06: Proceedings of the International Workshop on Mining Software Repositories</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="105" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Supporting change request assignment in open source development</title>
		<author>
			<persName><forename type="first">G</forename><surname>Canfora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cerulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SAC &apos;06: Proceedings of the 2006 ACM Symposium on Applied Computing</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1767" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Writing tools -the STYLE and DICTION programs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Vesterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
		<respStmt>
			<orgName>AT&amp;T Laboratories</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic bug triage using text categorization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cubranic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SEKE 2004: Proceedings of the Sixteenth International Conference on Software Engineering &amp; Knowledge Engineering</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="92" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analyzing and relating bug report data for feature tracking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pinzger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Working Conference on Reverse Engineering (WCRE)</title>
		<meeting>the 10th Working Conference on Reverse Engineering (WCRE)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="90" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Goldberg</surname></persName>
		</author>
		<ptr target="https://bugs.eclipse.org/bugs/bugwritinghelp.html" />
		<title level="m">Bug writing guidelines</title>
		<imprint>
			<date type="published" when="2007-08-04">2007-08-04</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Modeling bug report quality</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hooimeijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Weimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASE &apos;07: Proceedings of the twenty-second IEEE/ACM International Conference on Automated Software Engineering</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="34" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<ptr target="http://www.hotornot.com/" />
		<title level="m">HOT or NOT</title>
		<imprint>
			<date type="published" when="2007-09-11">2007-09-11</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SCARPE: A Technique and Tool for Selective Record and Replay of Program Executions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Orso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd IEEE International Conference on Software Maintenance (ICSM 2007)</title>
		<meeting>the 23rd IEEE International Conference on Software Maintenance (ICSM 2007)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-10">October 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards the next generation of bug tracking systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Just</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VL/HCC &apos;08: Proceedings of the 2008 IEEE Symposium on Visual Languages and Human-Centric Computing</title>
		<imprint>
			<date type="published" when="2008-09">September 2008</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using task context to improve programmer productivity</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGSOFT International Symposium on Foundations of Software Engineering (FSE 2006)</title>
		<meeting>the 14th ACM SIGSOFT International Symposium on Foundations of Software Engineering (FSE 2006)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Fishburne</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Chissom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Naval Technical Training, U. S. Naval Air Station</publisher>
			<pubPlace>Millington, TN; Memphis, TN</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Research Branch Report 8-75</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A linguistic analysis of how people describe software problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC 2006)</title>
		<meeting>the 2006 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC 2006)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="127" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scalable statistical bug isolation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liblit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI &apos;05: Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A technique for the measurement of attitudes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Likert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Archives of Psychology</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="1" to="55" />
			<date type="published" when="1932">1932</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Comparing and Merging Files with GNU Diff and Patch</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mackenzie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eggert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Stallman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network Theory Ltd</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generating robust parsers using island grammars</title>
		<author>
			<persName><forename type="first">L</forename><surname>Moonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Working Conference on Reverse Engineering (WCRE)</title>
		<meeting>the Eighth Working Conference on Reverse Engineering (WCRE)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="13" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Isolating relevant component interactions with JINSI</title>
		<author>
			<persName><forename type="first">A</forename><surname>Orso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Fifth International Workshop on Dynamic Analysis (WODA 2007)</title>
		<meeting>of Fifth International Workshop on Dynamic Analysis (WODA 2007)</meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Predicting Eclipse bug lifetimes</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Panjer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MSR &apos;07: Proceedings of the Fourth International Workshop on Mining Software Repositories</title>
		<imprint>
			<publisher>MSR Challenge Contribution</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An experiment on software project size and effort estimation</title>
		<author>
			<persName><forename type="first">U</forename><surname>Passing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Shepperd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Symposium on Empirical Software Engineering (ISESE &apos;03)</title>
		<meeting>of International Symposium on Empirical Software Engineering (ISESE &apos;03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="120" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Conducting on-line surveys in software engineering</title>
		<author>
			<persName><forename type="first">T</forename><surname>Punter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ciolkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Freimut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Symposium on Empirical Software Engineering (ISESE &apos;03)</title>
		<meeting>of International Symposium on Empirical Software Engineering (ISESE &apos;03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="80" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ratemyface</surname></persName>
		</author>
		<author>
			<persName><surname>Com</surname></persName>
		</author>
		<ptr target="http://www.ratemyface.com/." />
		<imprint>
			<date type="published" when="2007-09-11">2007-09-11</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detection of duplicate defect reports using natural language processing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Runeson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexandersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Nyholm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSE &apos;07: Proceedings of the 29th International Conference on Software Engineering</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="499" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Nonparametric Statistics for the Behavioral Sciences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Castellan</surname><genName>Jr</genName></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ethical issues in empirical studies of software engineering</title>
		<author>
			<persName><forename type="first">J</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Vinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Software Eng</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1171" to="1180" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Spolsky</surname></persName>
		</author>
		<title level="m">Joel on Software. APress, US</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An approach to detecting duplicate bug reports using natural language and execution information</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Anvik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSE &apos;08: Proceedings of the 30th International Conference on Software Engineering</title>
		<imprint>
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">All of Statistics: A Concise Course in Statistical Inference</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wasserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Patches as better bug reports</title>
		<author>
			<persName><forename type="first">W</forename><surname>Weimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GPCE &apos;06: Proceedings of the 5th international conference on Generative programming and component engineering</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How long will it take to fix this bug?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Premraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MSR &apos;07: Proceedings of the Fourth International Workshop on Mining Software Repositories</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient checkpointing of java software using context-sensitive capture and replay</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rountev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESEC-FSE &apos;07: Proceedings of the European Software Engineering Conference and ACM SIGSOFT Symposium on Foundations of Software Engineering</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
