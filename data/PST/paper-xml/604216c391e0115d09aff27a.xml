<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extract the Knowledge of Graph Neural Networks and Go Beyond it: An Effective Knowledge Distillation Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-04">4 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
							<email>yangcheng@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Liu</surname></persName>
							<email>liu_jiawei@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
							<email>shichuan@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Beijing Key Laboratory of Intelligent Telecommunications Software and Multimedia</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Extract the Knowledge of Graph Neural Networks and Go Beyond it: An Effective Knowledge Distillation Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-04">4 Mar 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3442381.3450068</idno>
					<idno type="arXiv">arXiv:2103.02885v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Neural Networks</term>
					<term>Knowledge Distillation</term>
					<term>Label Propagation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semi-supervised learning on graphs is an important problem in the machine learning area. In recent years, state-of-the-art classification methods based on graph neural networks (GNNs) have shown their superiority over traditional ones such as label propagation. However, the sophisticated architectures of these neural models will lead to a complex prediction mechanism, which could not make full use of valuable prior knowledge lying in the data, e.g., structurally correlated nodes tend to have the same class. In this paper, we propose a framework based on knowledge distillation to address the above issues. Our framework extracts the knowledge of an arbitrary learned GNN model (teacher model), and injects it into a well-designed student model. The student model is built with two simple prediction mechanisms, i.e., label propagation and feature transformation, which naturally preserves structure-based and feature-based prior knowledge, respectively. In specific, we design the student model as a trainable combination of parameterized label propagation and feature transformation modules. As a result, the learned student can benefit from both prior knowledge and the knowledge in GNN teachers for more effective predictions. Moreover, the learned student model has a more interpretable prediction process than GNNs. We conduct experiments on five public benchmark datasets and employ seven GNN models including GCN, GAT, APPNP, SAGE, SGC, GCNII and GLP as the teacher models. Experimental results show that the learned student model can consistently outperform its corresponding teacher model by 1.4% ∼ 4.7% on average. Code and data are available at https://github.com/BUPT-GAMMA/CPF</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Computing methodologies → Machine learning; • Networks → Network algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Semi-supervised learning on graph-structured data aims at classifying every node in a network given the network structure and a subset of nodes labeled. As a fundamental task in graph analysis <ref type="bibr" target="#b2">[3]</ref>, the classification problem has a wide range of real-world applications such as user profiling <ref type="bibr" target="#b14">[15]</ref>, recommender systems <ref type="bibr" target="#b27">[28]</ref>, text classification <ref type="bibr" target="#b0">[1]</ref> and sociological studies <ref type="bibr" target="#b1">[2]</ref>. Most of these applications have the homophily phenomenon <ref type="bibr" target="#b15">[16]</ref>, which assumes two linked nodes tend to have similar labels. With the homophily assumption, many traditional methods are developed to propagate labels by random walks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b38">39]</ref> or regularize the label differences between neighbors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>With the success of deep learning, methods based on graph neural networks (GNNs) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref> have demonstrated their effectiveness in classifying node labels. Most GNN models adopt message passing strategy <ref type="bibr" target="#b5">[6]</ref>: each node aggregates features from its neighborhood and then a layer-wise projection function with a non-linear activation will be applied to the aggregated information. In this way, GNNs can utilize both graph structure and node feature information in their models.</p><p>However, the entanglement of graph topology, node features and projection matrices in GNNs leads to a complicated prediction mechanism and could not take full advantage of prior knowledge lying in the data. For example, the aforementioned homophily assumption adopted in label propagation methods represents structure-based prior, and has been shown to be underused <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30]</ref> in graph convolutional network (GCN) <ref type="bibr" target="#b10">[11]</ref>.</p><p>As an evidence, recent studies proposed to incorporate the label propagation mechanism into GCN by adding regularizations <ref type="bibr" target="#b29">[30]</ref> or manipulating graph filters <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24]</ref>. Their experimental results show that GCN can be improved by emphasizing such structure-based prior knowledge. Nevertheless, these methods have three major drawbacks: <ref type="bibr" target="#b0">(1)</ref> The main bodies of their models are still GNNs and thus hard to fully utilize the prior knowledge; (2) They are single models rather than frameworks, and thus not compatible with other advanced GNN architectures; (3) They ignored another important prior knowledge, i.e., feature-based prior, which means that a node's label is purely determined by its own features.</p><p>To address these issues, we propose an effective knowledge distillation framework to inject the knowledge of an arbitrary learned GNN (teacher model) into a well-designed student model. The student model is built with two simple prediction mechanisms, i.e., label propagation and feature transformation, which naturally preserves structure-based and feature-based prior knowledge, respectively. In specific, we design the student model as a trainable combination of parameterized label propagation and feature-based 2-layer MLP (Multi-layer Perceptron). On the other hand, it has been recognized that the knowledge of a teacher model lies in its soft predictions <ref type="bibr" target="#b7">[8]</ref>. By simulating the soft labels predicted by a teacher model, our student model is able to further make use of the knowledge in pretrained GNNs. Consequently, the learned student model has a more interpretable prediction process and can utilize both GNN and structure/feature-based priors. An overview of our framework is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>We conduct experiments on five public benchmark datasets and employ several popular GNN models including GCN <ref type="bibr" target="#b10">[11]</ref>, GAT <ref type="bibr" target="#b28">[29]</ref>, SAGE <ref type="bibr" target="#b6">[7]</ref>, APPNP <ref type="bibr" target="#b11">[12]</ref>, SGC <ref type="bibr" target="#b31">[32]</ref> and a recent deep GCN model GCNII <ref type="bibr" target="#b3">[4]</ref> as teacher models. Experimental results show that a student model is able to outperform its corresponding teacher model by 1.4% ∼ 4.7% in terms of classification accuracy. It is worth noting that we also apply our framework on GLP <ref type="bibr" target="#b13">[14]</ref> which unified GCN and label propagation by manipulating graph filters. As a result, we can still gain 1.5% ∼ 2.3% relative improvements, which demonstrates the potential compatibility of our framework. Furthermore, we investigate the interpretability of our student model by probing the learned balance parameters between parameterized label propagation and feature transformation as well as the learned confidence score of each node in label propagation. To conclude, the improvements are consistent and significant with better interpretability.</p><p>The contributions of this paper are summarized as follows:</p><p>• We propose an effective knowledge distillation framework to extract the knowledge of an arbitrary pretrained GNN model and inject it into a student model for more effective predictions.</p><p>• We design the student model as a trainable combination of parameterized label propagation and feature-based 2-layer MLP.</p><p>Hence the student model has a more interpretable prediction process and naturally preserves the structure/feature-based priors. Consequently, the learned student model can utilize both GNN and prior knowledge.</p><p>• Experimental results on five benchmark datasets with seven GNN teacher models demonstrate the effectiveness of our framework. Extensive studies by probing the learned weights in the student model also illustrate the potential interpretability of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>This work is most relevant to graph neural network models and knowledge distillation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Neural Networks</head><p>The concept of GNN was proposed <ref type="bibr" target="#b20">[21]</ref> before 2010 and has become a rising topic since the emergence of GCN <ref type="bibr" target="#b10">[11]</ref>. During the last five years, graph neural network models have achieved promising results in many research areas <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37]</ref>. Now we will briefly introduce some representative GNN methods in this section and employ them as our teacher models in the experiments.</p><p>As one of the most influential GNN models, Graph Convolutional Network (GCN) <ref type="bibr" target="#b10">[11]</ref> targeted on semi-supervised learning on graph-structured data through layer-wise propagation of node features. GCN can be interpreted as a variant of convolutional neural networks that operates on graphs. Graph Attention Network (GAT) <ref type="bibr" target="#b28">[29]</ref> further employed attention mechanism in the aggregation of neighbors' features. SAGE <ref type="bibr" target="#b6">[7]</ref> sampled and aggregated features from a node's local neighborhood and is more spaceefficient. Approximate personalized propagation of neural predictions (APPNP) <ref type="bibr" target="#b11">[12]</ref> studied the relationship between GCN and PageRank, and incorporated a propagation scheme derived from personalized PageRank into graph filters. Simple Graph Convolution (SGC) <ref type="bibr" target="#b31">[32]</ref> simplified GCN by removing non-linear activations and collapsing weight matrices between layers. Graph Convolutional Network via Initial residual and Identity mapping (GC-NII) <ref type="bibr" target="#b3">[4]</ref> was a very recent deep GCN model which alleviates the over-smoothing problem.</p><p>Recently, several works show that the performance of GNNs can be further improved by incorporating traditional prediction mechanisms, i.e., label propagation. For example, Generalized Label Propagation (GLP) <ref type="bibr" target="#b13">[14]</ref> modified graph convolutional filters to generate smooth features with graph similarity encoded. UniMP <ref type="bibr" target="#b23">[24]</ref> fused feature aggregation and label propagation by a shared messagepassing network. GCN-LPA <ref type="bibr" target="#b29">[30]</ref> employed label propagation as regularization to assist GCN for better performances. Note that the label propagation mechanism was built with simple structurebased prior knowledge. Their improvements indicate that such prior knowledge is not fully explored in GNNs. Nevertheless, these advanced models still suffer from several drawbacks as illustrated in the Introduction section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge Distillation</head><p>Knowledge distillation <ref type="bibr" target="#b7">[8]</ref> was proposed for model compression where a small light-weight student model is trained to mimic the soft predictions of a pretrained large teacher model. After the distillation, the knowledge in the teacher model will be transferred into the student model. In this way, the student model can reduce time and space complexities without losing prediction qualities. Knowledge distillation is widely used in the computer vision area, e.g., a deep convolutional neural network (CNN) will be compressed into a shallow one to accelerate the inference.</p><p>In fact, there are also a few studies combining knowledge distillation with GCN. However, their motivation and model architecture are quite different from ours. Yang et al. <ref type="bibr" target="#b33">[34]</ref> which was proposed in the computer vision area, compressed a deep GCN with large feature maps into a shallow one with fewer parameters using a local structure preserving module. Reliable Data Distillation (RDD) <ref type="bibr" target="#b34">[35]</ref> trained multiple GCN students with the same architecture and then ensembled them for better performance in a manner similar to BAN <ref type="bibr" target="#b4">[5]</ref>. Graph Markov Neural Networks (GMNN) <ref type="bibr" target="#b18">[19]</ref> can also be viewed as a knowledge distillation method where two GCNs with different reception sizes learn from each other. Note that both teacher and student models in these works are GCNs.</p><p>Compared with them, the goal of our framework is to extract the knowledge of GNNs and go beyond it. Our framework is very flexible and can be applied on an arbitrary GNN model besides GCN. We design a student model with simple prediction mechanisms and thus are able to benefit from both GNN and prior knowledge. As the output of our framework, the student model also has a more interpretable prediction process. In terms of training details, our framework is simpler and requires no ensembling or iterative distillations between teacher and student models for improving classification accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we will start by formalizing the semi-supervised node classification problem and introducing the notations. Then we will present our knowledge distillation framework to extract the knowledge of GNNs. Afterwards, we will propose the architecture of our student model, which is a trainable combination of parameterized label propagation and feature-based 2-layer MLP. Finally, we will discuss the potential interpretability of the student model and the computation complexity of our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Semi-supervised Node Classification</head><p>We begin by outlining the problem of node classification. Given a connected graph 𝐺 = (𝑉 , 𝐸) with a subset of nodes 𝑉 𝐿 ⊂ 𝑉 labeled, where 𝑉 is the vertex set and 𝐸 is the edge set, node classification targets on predicting the node labels for every node 𝑣 in unlabeled node set 𝑉 𝑈 = 𝑉 \ 𝑉 𝐿 . Each node 𝑣 ∈ 𝑉 has label 𝑦 𝑣 ∈ 𝑌 where 𝑌 is the set of all possible labels. In addition, node features 𝑋 ∈ R |𝑉 |×𝑑 are usually available in graph data and can be utilized for better classification accuracy. Each row 𝑋 𝑣 ∈ R 𝑑 of matrix 𝑋 denotes a 𝑑-dimensional feature vector of node 𝑣.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Knowledge Distillation Framework</head><p>Node classification approaches including GNNs can be summarized as a black box that outputs a classifier 𝑓 given graph structure 𝐺, labeled node set 𝑉 𝐿 and node feature 𝑋 as inputs. The classifier 𝑓 will predict the probability 𝑓 (𝑣, 𝑦) that unlabeled node 𝑣 ∈ 𝑉 𝑈 has label 𝑦 ∈ 𝑌 , where 𝑦 ′ ∈𝑌 𝑓 (𝑣, 𝑦 ′ ) = 1. For labeled node 𝑣, we set 𝑓 (𝑣, 𝑦) = 1 if 𝑣 is annotated with label 𝑦 and 𝑓 (𝑣, 𝑦 ′ ) = 0 for any other label 𝑦 ′ . We use 𝑓 (𝑣) ∈ R |𝑌 | to denote the probability distribution over all labels for brevity.</p><p>In this paper, the teacher model employed in our framework can be an arbitrary GNN model such as GCN <ref type="bibr" target="#b10">[11]</ref> or GAT <ref type="bibr" target="#b28">[29]</ref>. We denote the pretrained classifier in a teacher model as 𝑓 𝐺𝑁 𝑁 . On the other hand, we use 𝑓 𝑆𝑇𝑈 ;Θ to denote the student model parameterized by Θ and 𝑓 𝑆𝑇𝑈 ;Θ (𝑣) ∈ R |𝑌 | represents the predicted probability distribution of node 𝑣 by the student.</p><p>In knowledge distillation <ref type="bibr" target="#b7">[8]</ref>, the student model is trained to mimic the soft label predictions of a pretrained teacher model. As a result, the knowledge lying in the teacher model will be extracted and injected into the learned student. Therefore, the optimization objective which aligns the outputs between the student model and pretrained teacher model can be formulated as min</p><formula xml:id="formula_0">Θ ∑︁ 𝑣 ∈𝑉 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒 (𝑓 𝐺𝑁 𝑁 (𝑣), 𝑓 𝑆𝑇𝑈 ;Θ (𝑣)),<label>(1)</label></formula><p>where 𝑑𝑖𝑠𝑡𝑎𝑛𝑐𝑒 (•, •) measures the distance between two predicted probability distributions. Specifically, we use Euclidean distance in this work<ref type="foot" target="#foot_1">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The Architecture of Student Model</head><p>We hypothesize that a node's label prediction follows two simple mechanisms: (1) label propagation from its neighboring nodes and</p><p>(2) a transformation from its own features. Therefore, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>, we design our student model as a combination of these two mechanisms, i.e., a Parameterized Label Propagation (PLP) module and a Feature Transformation (FT) module, which can naturally preserve the structure/feature-based prior knowledge, respectively. After the distillation, the student will benefit from both GNN and prior knowledge with a more interpretable prediction mechanism.</p><p>In this subsection, we will first briefly review the conventional label propagation algorithm. Then we will introduce our PLP and FT modules as well as their trainable combinations. <ref type="bibr" target="#b37">[38]</ref> is a classical graph-based semi-supervised learning model. This model simply follows the assumption that nodes linked by an edge (or occupying the same manifold) are very likely to share the same label. Based on this hypothesis, labels will propagate from labeled nodes to unlabeled ones for predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Label Propagation. Label propagation (LP)</head><p>Formally, we use 𝑓 𝐿𝑃 to denote the final prediction of LP and 𝑓 𝑘 𝐿𝑃 to denote the prediction of LP after 𝑘 iterations. In this work, we initialize the prediction of node 𝑣 as a one-hot label vector if 𝑣 is a labeled node. Otherwise, we will set a uniform label distribution for each unlabeled node 𝑣, which indicates that the probabilities of all classes are the same at the beginning. The initialization can be formalized as:</p><formula xml:id="formula_1">𝑓 0 𝐿𝑃 (𝑣) = (0, ...1, ...0) ∈ R |𝑌 | , ∀𝑣 ∈ 𝑉 𝐿 ( 1 |𝑌 | , ... 1 |𝑌 | , ... 1 |𝑌 | ) ∈ R |𝑌 | , ∀𝑣 ∈ 𝑉 𝑈<label>(2)</label></formula><p>where 𝑓 𝑘 𝐿𝑃 (𝑣) is the predicted probability distribution of node 𝑣 at iteration 𝑘. In the 𝑘 + 1-th iteration, LP will update the label Taking the center node 𝑣 as an example, the student model starts from node 𝑣's raw features and a uniform label distribution as soft labels. Then at each layer, the soft label prediction of 𝑣 will be updated as a trainable combination of Parameterized Label Propagation (PLP) from 𝑣's neighbors and Feature Transformation (FT) of 𝑣's features. Finally, the distance between the soft label predictions of student and pretrained teacher will be minimized.</p><p>predictions of each unlabeled node 𝑣 ∈ 𝑉 𝑈 as follows:</p><formula xml:id="formula_2">𝑓 𝑘+1 𝐿𝑃 (𝑣) = (1 − 𝜆) 1 |𝑁 𝑣 | ∑︁ 𝑢 ∈𝑁 𝑣 𝑓 𝑘 𝐿𝑃 (𝑢) + 𝜆𝑓 𝑘 𝐿𝑃 (𝑣),<label>(3)</label></formula><p>where 𝑁 𝑣 is the set of node 𝑣's neighbors in the graph and 𝜆 is a hyper-parameter controlling the smoothness of node updates.</p><p>Note that LP has no parameters to be trained, and thus can not fit the output of a teacher model through end-to-end training. Therefore, we retrofit LP by introducing more parameters to increase its capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Parameterized Label Propagation</head><p>Module. Now we will introduce our Parameterized Label Propagation (PLP) module by further parameterizing edge weights in LP. As shown in Eq. 3, LP model treats all neighbors of a node equally during the propagation. However, we hypothesize that the importance of different neighbors to a node should be different, which determines the propagation intensities between nodes. To be more specific, we assume that the label predictions of some nodes are more "confident" than others: e.g., a node whose predicted label is similar to most of its neighbors. Such nodes will be more likely to propagate their labels to neighbors and keep themselves unchanged.</p><p>Formally, we will assign a confidence score 𝑐 𝑣 ∈ R to each node 𝑣. During the propagation, all node 𝑣's neighbors and 𝑣 itself will compete to propagate their labels to 𝑣. Following the intuition that a larger confidence score will have a larger edge weight, we rewrite the prediction update function in Eq. 3 for 𝑓 𝑃𝐿𝑃 as follows:</p><formula xml:id="formula_3">𝑓 𝑘+1 𝑃𝐿𝑃 (𝑣) = ∑︁ 𝑢 ∈𝑁 𝑣 ∪{𝑣 } 𝑤 𝑢𝑣 𝑓 𝑘 𝑃𝐿𝑃 (𝑢),<label>(4)</label></formula><p>where 𝑤 𝑢𝑣 is the edge weight between node 𝑢 and 𝑣 computed by the following softmax function:</p><formula xml:id="formula_4">𝑤 𝑢𝑣 = 𝑒𝑥𝑝 (𝑐 𝑢 ) 𝑢 ′ ∈𝑁 𝑣 ∪{𝑣 } 𝑒𝑥𝑝 (𝑐 𝑢 ′ ) . (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>Similar to LP, 𝑓 0 𝑃𝐿𝑃 (𝑣) is initialized as Eq. 2 and 𝑓 𝑘 𝑃𝐿𝑃 (𝑣) remains the one-hot ground truth label vector for every labeled node 𝑣 ∈ 𝑉 𝐿 during the propagation.</p><p>Note that we can further parameterize confidence score 𝑐 𝑣 for inductive setting as an optional choice:</p><formula xml:id="formula_6">𝑐 𝑣 = 𝑧 𝑇 𝑋 𝑣 ,<label>(6)</label></formula><p>where 𝑧 ∈ R 𝑑 is a learnable parameter that projects node 𝑣's feature into the confidence score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Feature Transformation</head><p>Module. Note that PLP module which propagates labels through edges emphasizes the structure-based prior knowledge. Thus we also introduce Feature Transformation (FT) module as a complementary prediction mechanism. The FT module predicts labels by only looking at the raw features of a node. Formally, denoting the prediction of FT module as 𝑓 𝐹𝑇 , we apply a 2-layer MLP<ref type="foot" target="#foot_2">2</ref> followed by a softmax function to transform the features into soft label predictions:</p><formula xml:id="formula_7">𝑓 𝐹𝑇 (𝑣) = 𝑠𝑜 𝑓 𝑡𝑚𝑎𝑥 (𝑀𝐿𝑃 (𝑋 𝑣 )).<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">A Trainable</head><p>Combination. Now we will combine the PLP and FT modules as the full model of our student. In detail, we will learn a trainable parameter 𝛼 𝑣 ∈ [0, 1] for each node 𝑣 to balance the predictions between PLP and FT. In other words, the prediction from FT module will be incorporated into that from PLP at each propagation step. We name the full student model as Combination of Parameterized label propagation and Feature transformation (CPF) and thus the prediction update function for each unlabeled node 𝑣 ∈ 𝑉 𝑈 in Eq. 4 will be rewritten as</p><formula xml:id="formula_8">𝑓 𝑘+1 𝐶𝑃 𝐹 (𝑣) = 𝛼 𝑣 ∑︁ 𝑢 ∈𝑁 𝑣 ∪{𝑣 } 𝑤 𝑢𝑣 𝑓 𝑘 𝐶𝑃 𝐹 (𝑢) + (1 − 𝛼 𝑣 )𝑓 𝐹𝑇 (𝑣),<label>(8)</label></formula><p>where edge weight 𝑤 𝑢𝑣 and initialization 𝑓 0 𝐶𝑃 𝐹 (𝑣) are the same with PLP module. Whether parameterizing confidence score 𝑐 𝑣 as Eq. 6 or not will lead to inductive/transductive variants CPF-ind/CPF-tra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Overall Algorithm and Details</head><p>Assuming that our student model has a total of 𝐾 layers, the distillation objective in Eq. 1 can be detailed as:</p><formula xml:id="formula_9">min Θ ∑︁ 𝑣 ∈𝑉 𝑈 ∥𝑓 𝐺𝑁 𝑁 (𝑣) − 𝑓 𝐾 𝐶𝑃 𝐹 ;Θ (𝑣)∥ 2 ,<label>(9)</label></formula><p>where ∥ • ∥ 2 is the L2-norm and the parameter set Θ includes the balancing parameters between PLP and FT {𝛼 𝑣 , ∀𝑣 ∈ 𝑉 }, confidence parameters in PLP module {𝑐 𝑣 , ∀𝑣 ∈ 𝑉 } (or parameter 𝑧 for inductive setting), and the parameters of MLP in FT module Θ 𝑀𝐿𝑃 . There is also an important hyper-parameter in the distillation framework: the number of propagation layers 𝐾. Alg. 1 shows the pseudo code of the training process. We implement our framework based on Deep Graph Library (DGL) <ref type="bibr" target="#b30">[31]</ref> and Pytorch <ref type="bibr" target="#b17">[18]</ref>, and employ an Adam optimizer <ref type="bibr" target="#b9">[10]</ref> for parameter training. Dropout <ref type="bibr" target="#b24">[25]</ref> is also applied to alleviate overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1</head><p>The proposed knowledge distillation framework. Update parameters by optimizing Eq. 9; 14: end while</p><formula xml:id="formula_10">Input: Graph 𝐺 = (𝑉 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discussions on Interpretability and Complexity</head><p>In this subsection, we will discuss the interpretability of the learned student model and the complexity of our algorithm.</p><p>After the knowledge distillation, our student model CPF will predict the label of a specific node 𝑣 as a weighted average between the predictions of label propagation and feature-based MLP. The balance parameter 𝛼 𝑣 indicates whether structure-based LP or feature-based MLP is more important for node 𝑣's prediction. LP mechanism is almost transparent and we can easily find out node 𝑣 is influenced by which neighbor to what extent at each iteration. On the other hand, the understanding of feature-based MLP can be derived by existing works <ref type="bibr" target="#b19">[20]</ref> or directly looking at the gradients of different features. Therefore, the learned student model has better interpretability than GNN teachers.</p><p>The time complexity of each iteration (line 3 to 13 in Alg. 1) and the space complexity of our algorithm are both 𝑂 (|𝐸| + 𝑑 |𝑉 |), which is linear to the scale of datasets. In fact, the operations can be easily implemented in matrix form and the training process can be finished in seconds on real-world benchmark datasets with a single GPU device. Therefore, our proposed knowledge distillation framework is very time/space-efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we will start by introducing the datasets and teacher models used in our experiments. Then we will detail the experimental settings of teacher models and student variants. Afterwards, we will present quantitative results on evaluating semi-supervised node classification. We also conduct experiments under different numbers of propagation layers and training ratios to illustrate the robustness of our algorithm. Finally, we will present qualitative case studies and visualizations for better understandings of the learned parameters in our student model CPF. We use five public benchmark datasets for experiments and the statistics of the datasets are shown in Table <ref type="table" target="#tab_1">1</ref>. As previous works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26</ref>] did, we only consider the largest connected component and regard the edges as undirected. The details about the datasets are as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>• Cora <ref type="bibr" target="#b21">[22]</ref> is a benchmark citation dataset composed of machine learning papers, where each node represents a document with a sparse bag-of-words feature vector. Edges represent citations between documents, and labels specify the research field of each paper.</p><p>Table <ref type="table">2</ref>: Classification accuracies with teacher models as GCN <ref type="bibr" target="#b10">[11]</ref> and GAT <ref type="bibr" target="#b28">[29]</ref>. • Citeseer <ref type="bibr" target="#b21">[22]</ref> is another benchmark citation dataset of computer science publications, holding similar configuration to Cora. Citeseer dataset has the largest number of features among all five datasets used in this paper. • Pubmed <ref type="bibr" target="#b16">[17]</ref> is also a citation dataset, consisting of articles related to diabetes in the PubMed database. The node features are TF/IDF weighted word frequency, and the label indicates the type of diabetes discussed in this article. • A-Computers and A-Photo <ref type="bibr" target="#b22">[23]</ref> are extracted from Amazon co-purchase graph, where nodes represent products, edges represent whether two products are frequently co-purchased or not, features represent product reviews encoded by bagof-words, and labels are predefined product categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>Following the experimental settings in previous work <ref type="bibr" target="#b22">[23]</ref>, we randomly sample 20 nodes from each class as labeled nodes, 30 nodes for validation and all other nodes for test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Teacher Models and Settings</head><p>For a thorough comparison, we consider seven GNN models as teacher models in our knowledge distillation framework:</p><p>• GCN <ref type="bibr" target="#b10">[11]</ref> is a classic semi-supervised model which learns node representations by defining convolution operators on graph-structured data. GCN is sensitive to the number of layers and we employ the most widely-used 2-layer setting in this work.</p><p>• GAT <ref type="bibr" target="#b28">[29]</ref> improves GCN by incorporating attention mechanism which assigns different weights to each neighbor of a node. We use a 2-layer GAT with 8 attention heads as our teacher model. • APPNP <ref type="bibr" target="#b11">[12]</ref> improves GCN by balancing the preservation of local information and the use of a wide range of neighbor information. We employ 2 layers and 10 power iteration steps for APPNP.</p><p>• SAGE <ref type="bibr" target="#b6">[7]</ref> learns node embeddings by sampling and aggregating information from a node's local neighborhood. We employ the SAGE-GCN variant as a teacher model. • SGC <ref type="bibr" target="#b31">[32]</ref> reduces the extra complexity of GCN by removing the non-linearity between GCN layers and compressing the weight matrices. Similar to GCN, we also use a 2-layer setting.</p><p>• GCNII <ref type="bibr" target="#b3">[4]</ref> is a deep model which uses initial residual and identity mapping to avoid oversmoothing of GCN model.</p><p>Here we use 16 layers GCNII as a teacher.</p><p>• GLP <ref type="bibr" target="#b13">[14]</ref> is a label-efficient model which combines label propagation with graph convolution operations by a graph filtering framework. GLP has two model variants: GLP-RNM and GLP-AR, and we use the better one for each dataset as our teacher.</p><p>The detailed training settings of teacher models are listed in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Student Variants and Experimental Settings</head><p>For each dataset and teacher model, we test the following student variants:</p><p>• PLP: The student variant with only the Parameterized Label Propagation (PLP) mechanism; • FT: The student variant with only the Feature Transformation (FT) mechanism; • CPF-ind: The full model CPF with inductive setting;</p><p>• CPF-tra: The full model CPF with transductive setting.</p><p>We randomly initialize the parameters and employ early stopping with a patience of 50, i.e., we will stop training if the classification accuracy on validation set does not increase for 50 epochs. For hyperparameter tuning, we conduct heuristic search by exploring # layers 𝐾 ∈ {5, 6, 7, 8, 9, 10}, hidden size in MLP 𝑑 𝑀𝐿𝑃 ∈ {8, 16, 32, 64}, dropout rate 𝑑𝑟 ∈ {0.2, 0.5, 0.8}, learning rate and weight decay of Adam optimizer 𝑙𝑟 ∈ {0.001, 0.005, 0.01}, 𝑤𝑑 ∈ {0.0005, 0.001, 0.01}. Table <ref type="table">4</ref>: Classification accuracies with teacher models as SGC <ref type="bibr" target="#b31">[32]</ref> and GCNII <ref type="bibr" target="#b3">[4]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of Classification Results</head><p>Experimental results on five datasets with seven GNN teachers and four student variants are presented in Table <ref type="table">2</ref>, 3, 4 and 5 <ref type="foot" target="#foot_3">3</ref> . We have the following observations:</p><p>• The proposed knowledge distillation framework accompanying with the full architecture of student model CPF-ind and CPF-tra, is able to improve the performance of the corresponding teacher model consistently and significantly. For example, the classification accuracy of GCN on Cora dataset is improved from 0.8244 to 0.8576. This is because the knowledge of GNN teachers can be extracted and injected into our student model which also benefits from structure/featurebased prior knowledge introduced by its simple prediction mechanism. This observation demonstrates our motivation and the effectiveness of our framework. • Note that the teacher model Generalized Label Propagation (GLP) <ref type="bibr" target="#b13">[14]</ref> has already incorporated the label propagation mechanism in their graph filters. As shown in Table <ref type="table" target="#tab_3">5</ref>, we can still gain 1.5% ∼ 2.3% relative improvements by applying our knowledge distillation framework, which demonstrates the potential compatibility of our algorithm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis of Different Numbers of Propagation Layers</head><p>In this subsection, we will investigate the influence of a key hyperparameter in the architecture of our student model CPF, i.e., the number of propagation layers 𝐾. In fact, popular GNN models such as GCN and GAT are very sensitive to the number of layers.</p><p>A larger number of layers will cause the over-smoothing issue and significantly harm the model performance. Hence we conduct experiments on Cora dataset for further analysis of this hyperparameter.  Fig. <ref type="figure" target="#fig_3">3</ref> shows the classification results of student CPF-ind and CPFtra with different numbers of propagation layers 𝐾 ∈ {5, 6, 7, 8, 9, 10}. We can see that the gaps among different 𝐾 are relatively small: For each teacher, we compute the gap between the best and worst performed accuracies of its corresponding student and the maximum gaps are 0.56% and 0.84% for CPF-ind and CPF-tra, respectively. Moreover, the accuracy of CPF under the worst choice of 𝐾 ∈ {5, 6, 7, 8, 9, 10} has already outperformed the corresponding teacher. Therefore, the gains from our framework are very robust when the number of propagation layers 𝐾 varies within a reasonable range. Besides changing the number of propagation layers, another model variant we test is replacing the 2-layer MLP in feature transformation module with a single-layer linear regression, which can also improve the performance with a smaller ratio (the average improvements over the seven teachers are 0.3% ∼ 2.3%). Linear regression may have better interpretability, but at the cost of weaker performance, which can be seen as a trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Analysis of Different Training Ratios</head><p>To further demonstrate the effectiveness of our framework, we conduct additional experiments under different training ratios. In specific, we take Cora dataset as an example and vary the number of labeled nodes per class from 5 to 50. Experimental results are presented in Fig. <ref type="figure" target="#fig_4">4</ref>. Note that we omit the results of PLP since its performance is poor and can not be fit into the figures.</p><p>We can see that the learned CPF-ind and CPF-tra students consistently outperform the pretrained GNN teachers under different numbers of labeled nodes per class, which illustrates the robustness of our framework. FT module, however, has enough model capacity to overfit the predictions of a teacher but gains no further improvements. Therefore, as a complementary prediction mechanism, the PLP module is also very important in our framework.</p><p>Another observation is that the students' improvements over corresponding teacher models are more significant for the fewshot setting, i.e., only 5 nodes are labeled for each class. As evidence, the relative improvements on classification accuracy are 4.9/4.5/3.2/2.1% on average for 5/10/20/50 labeled nodes per class. Thus our algorithm also has the ability to handle the few-shot setting which is an important research problem in semi-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Analysis of Interpretability</head><p>Now we will analyze the potential interpretability of the learned student model CPF. Specifically, we will probe into the learned balance parameter 𝛼 𝑣 between PLP and FT, as well as the confidence score 𝑐 𝑣 of each node. Our goal is to figure out what kind of nodes has the largest/smallest values of 𝛼 𝑣 and 𝑐 𝑣 . We use the CPF-ind student guided by GCN or GAT teachers on Cora dataset for illustration in this subsection.</p><p>Balance parameter 𝛼 𝑣 . Recall that the balance parameter 𝛼 𝑣 indicates whether structure-based LP or feature-based MLP contributes more for node 𝑣's prediction. As shown in Fig. <ref type="figure" target="#fig_5">5</ref>, we analyze the top-10 nodes with the largest/smallest 𝛼 𝑣 and select four representative nodes for case study. We plot the 1-hop neighborhood of each node and use different colors to indicate different predicted labels. We find that a node with a larger 𝛼 𝑣 will be more likely to have the same predicted neighbors. In contrast, a node with a smaller 𝛼 𝑣 will probably have more neighbors with different predicted labels. This observation matches our intuition that the prediction of a node will be confused if it has many neighbors with various predicted labels and thus can not benefit much from label propagation.</p><p>Confidence score 𝑐 𝑣 . On the other hand, a node with a larger confidence score 𝑐 𝑣 in our student architecture will have larger edge weights to propagate its labels to neighbors and keep itself unchanged. Similarly, as shown in Fig. <ref type="figure" target="#fig_6">6</ref>, we also investigate the top-10 nodes with the largest/smallest confidence score 𝑐 𝑣 and select four representative nodes for case study. We can see that nodes with high confidences will also have a relatively small degree and the same predicted neighbors. In contrast, nodes with low confidences 𝑐 𝑣 will have an even more diverse neighborhood than nodes with small 𝛼 𝑣 . Intuitively, a diverse neighborhood of a node will lead to  lower confidence to propagate its labels. This finding validates our motivation for modeling node confidences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose an effective knowledge distillation framework which can extract the knowledge of an arbitrary pretrained GNN (teacher model) and inject it into a well-designed student model. The student model CPF is built as a trainable combination of two simple prediction mechanisms: label propagation feature transformation which emphasize structure-based and feature-based prior knowledge, respectively. After the distillation, the learned student is able to take advantage of both prior and GNN knowledge and thus go beyond the GNN teacher. Experimental results on five benchmark datasets show that our framework can improve the classification accuracies of all seven GNN teacher models consistently and significantly with a more interpretable prediction process. Additional experiments on different numbers of training ratios and propagation layers demonstrate the robustness of our algorithm. We also present case studies to understand the learned balance parameters and confidence scores in our student architecture.</p><p>For future work, we will explore the adoption of our framework for other graph-based applications besides semi-supervised node classification. For example, the unsupervised node clustering task would be interesting since the label propagation scheme can not be applied without labels. Another direction is to refine our framework by encouraging the teacher and student models to learn from each other for better performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGMENT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of our knowledge distillation framework. The two simple prediction mechanisms of our student model ensure the full use of structure/feature-based prior knowledge. The knowledge in GNN teachers will be extracted and injected into the student during knowledge distillation. Thus the student can go beyond its corresponding teacher with more effective predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: An illustration of the architecture of our proposed student model. Taking the center node 𝑣 as an example, the student model starts from node 𝑣's raw features and a uniform label distribution as soft labels. Then at each layer, the soft label prediction of 𝑣 will be updated as a trainable combination of Parameterized Label Propagation (PLP) from 𝑣's neighbors and Feature Transformation (FT) of 𝑣's features. Finally, the distance between the soft label predictions of student and pretrained teacher will be minimized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) The CPF-ind student.(b) The CPF-tra student.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Classification accuracies of CPF-ind and CPFtra with different numbers of propagation layers on Cora dataset. The legends indicate the teacher model by which a student is guided.</figDesc><graphic url="image-36.png" coords="7,318.12,433.33,100.90,75.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Classification accuracies under different numbers of labeled nodes on Cora dataset. The subcaptions indicate the corresponding teacher models.</figDesc><graphic url="image-43.png" coords="8,116.85,195.20,126.10,94.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Case studies of balance parameter 𝛼 𝑣 for interpretability analysis. Here the subcaption indicates the node is selected by large/small 𝛼 𝑣 value with GCN/GAT as teachers.</figDesc><graphic url="image-50.png" coords="9,53.80,240.03,126.10,94.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Case studies of confidence score 𝑐 𝑣 for interpretability analysis. Here the subcaption indicates the node is selected by large/small 𝑐 𝑣 value with GCN/GAT as teachers.</figDesc><graphic url="image-51.png" coords="9,179.90,240.03,126.10,94.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>𝐸), labeled node set 𝑉 𝐿 ⊂ 𝑉 , unlabeled node set 𝑉 𝑈 ⊂ 𝑉 , node features 𝑋 and pretrained GNN classifier 𝑓 𝐺𝑁 𝑁 . Output: The learned student model 𝑓 𝐶𝑃 𝐹 .Compute confidence score 𝑐 𝑣 for each node 𝑣 ∈ 𝑉 by Eq. 6;Compute edge weight 𝑤 𝑢𝑣 for each edge (𝑢, 𝑣) ∈ 𝐸 by Eq. 5;</figDesc><table><row><cell>3:</cell><cell>if inductive setting then</cell></row><row><cell>4:</cell><cell></cell></row><row><cell>5:</cell><cell>end if</cell></row><row><cell>6:</cell><cell></cell></row><row><cell>7:</cell><cell>for all node 𝑣 ∈ 𝑉 𝑈 do</cell></row><row><cell>8:</cell><cell>Compute the prediction of FT module 𝑓 𝐹𝑇 (𝑣) by Eq. 7;</cell></row><row><cell>9:</cell><cell>for k=1,2. . . K do</cell></row><row><cell>10:</cell><cell>Update the prediction after 𝑘 layers 𝑓 𝑘 𝐶𝑃 𝐹 (𝑣) by Eq. 8;</cell></row><row><cell>11:</cell><cell>end for</cell></row><row><cell>12:</cell><cell>end for</cell></row><row><cell>13:</cell><cell></cell></row></table><note>1: Initialize the label prediction 𝑓 0 𝐶𝑃 𝐹 (𝑣) for each node 𝑣 by Eq. 2; 2: while not converge do</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Dataset statistics.</figDesc><table><row><cell>Dataset</cell><cell cols="4"># Nodes # Edges # Features # Classes</cell></row><row><cell>Cora</cell><cell>2,485</cell><cell>5,069</cell><cell>1,433</cell><cell>7</cell></row><row><cell>Citeseer</cell><cell>2,110</cell><cell>3,668</cell><cell>3,703</cell><cell>6</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>44,324</cell><cell>500</cell><cell>3</cell></row><row><cell>A-Computers</cell><cell>13,381</cell><cell>245,778</cell><cell>767</cell><cell>10</cell></row><row><cell>A-Photo</cell><cell>7,487</cell><cell>119,043</cell><cell>745</cell><cell>8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Classification accuracies with teacher models as APPNP<ref type="bibr" target="#b11">[12]</ref> and SAGE<ref type="bibr" target="#b6">[7]</ref>.</figDesc><table><row><cell></cell><cell>Teacher GCN</cell><cell>PLP</cell><cell cols="3">Student variants FT CPF-ind CPF-tra</cell><cell>+Impv.</cell><cell>Teacher GAT</cell><cell>PLP</cell><cell>Student variants FT CPF-ind CPF-tra</cell><cell>+Impv.</cell></row><row><cell>Cora</cell><cell cols="4">0.8244 0.7522 0.8253 0.8576</cell><cell>0.8567</cell><cell>4.0%</cell><cell cols="3">0.8389 0.7578 0.8426</cell><cell>0.8576</cell><cell>0.8590</cell><cell>2.4%</cell></row><row><cell>Citeseer</cell><cell cols="3">0.7110 0.6602 0.7055</cell><cell>0.7619</cell><cell>0.7652</cell><cell>7.6%</cell><cell cols="3">0.7276 0.6624 0.7591</cell><cell>0.7657</cell><cell>0.7691</cell><cell>5.7%</cell></row><row><cell>Pubmed</cell><cell cols="3">0.7804 0.6471 0.7964</cell><cell>0.8080</cell><cell>0.8104</cell><cell>3.8%</cell><cell cols="3">0.7702 0.6848 0.7896</cell><cell>0.8011</cell><cell>0.8040</cell><cell>4.4%</cell></row><row><cell cols="5">A-Computers 0.8318 0.7584 0.8356 0.8443</cell><cell>0.8443</cell><cell>1.5%</cell><cell cols="3">0.8107 0.7605 0.8135 0.8190</cell><cell>0.8148</cell><cell>1.0%</cell></row><row><cell>A-Photo</cell><cell cols="4">0.9072 0.8499 0.9265 0.9317</cell><cell>0.9248</cell><cell>2.7%</cell><cell cols="3">0.8987 0.8496 0.9190 0.9221</cell><cell>0.9199</cell><cell>2.6%</cell></row><row><cell>Datasets</cell><cell>Teacher APPNP</cell><cell>PLP</cell><cell cols="3">Student variants FT CPF-ind CPF-tra</cell><cell>+Impv.</cell><cell>Teacher SAGE</cell><cell>PLP</cell><cell>Student variants FT CPF-ind CPF-tra</cell><cell>+Impv.</cell></row><row><cell>Cora</cell><cell cols="3">0.8398 0.7251 0.8379</cell><cell>0.8581</cell><cell>0.8562</cell><cell>2.2%</cell><cell cols="3">0.8178 0.7663 0.8201 0.8473</cell><cell>0.8454</cell><cell>3.6%</cell></row><row><cell>Citeseer</cell><cell cols="3">0.7547 0.6812 0.7580</cell><cell>0.7646</cell><cell>0.7635</cell><cell>1.3%</cell><cell cols="3">0.7171 0.6641 0.7425</cell><cell>0.7497</cell><cell>0.7575</cell><cell>5.6%</cell></row><row><cell>Pubmed</cell><cell cols="3">0.7950 0.6866 0.8102</cell><cell>0.8058</cell><cell>0.8081</cell><cell>1.6%</cell><cell cols="3">0.7736 0.6829 0.7717</cell><cell>0.7948</cell><cell>0.8062</cell><cell>4.2%</cell></row><row><cell cols="4">A-Computers 0.8236 0.7516 0.8176</cell><cell>0.8279</cell><cell>0.8211</cell><cell>0.5%</cell><cell cols="3">0.7760 0.7590 0.7912</cell><cell>0.7971</cell><cell>0.8199</cell><cell>5.7%</cell></row><row><cell>A-Photo</cell><cell cols="3">0.9148 0.8469 0.9241</cell><cell>0.9273</cell><cell>0.9272</cell><cell>1.4%</cell><cell cols="3">0.8863 0.8366 0.9153 0.9268</cell><cell>0.9248</cell><cell>4.6%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Classification accuracies with teacher model as GLP<ref type="bibr" target="#b13">[14]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Teacher SGC</cell><cell>PLP</cell><cell cols="3">Student variants FT CPF-ind CPF-tra</cell><cell>+Impv.</cell><cell>Teacher GCNII</cell><cell>PLP</cell><cell>Student variants FT CPF-ind CPF-tra</cell><cell>+Impv.</cell></row><row><cell>Cora</cell><cell></cell><cell cols="4">0.8052 0.7513 0.8173</cell><cell>0.8454</cell><cell>0.8487</cell><cell>5.4%</cell><cell>0.8384 0.7382 0.8431</cell><cell>0.8581</cell><cell>0.8590</cell><cell>2.5%</cell></row><row><cell>Citeseer</cell><cell></cell><cell cols="4">0.7133 0.6735 0.7331</cell><cell>0.7470</cell><cell>0.7530</cell><cell>5.6%</cell><cell>0.7376 0.6724 0.7564 0.7635</cell><cell>0.7569</cell><cell>3.5%</cell></row><row><cell>Pubmed</cell><cell></cell><cell cols="4">0.7892 0.6018 0.8098</cell><cell>0.7972</cell><cell>0.8204</cell><cell>4.0%</cell><cell>0.7971 0.6913 0.7984</cell><cell>0.7928</cell><cell>0.8024</cell><cell>0.7%</cell></row><row><cell cols="6">A-Computers 0.8248 0.7579 0.8391</cell><cell>0.8367</cell><cell>0.8407</cell><cell>1.9%</cell><cell>0.8325 0.7628 0.8411 0.8467</cell><cell>0.8447</cell><cell>1.7%</cell></row><row><cell cols="2">A-Photo</cell><cell cols="5">0.9063 0.8318 0.9303 0.9397</cell><cell>0.9347</cell><cell>3.7%</cell><cell>0.9230 0.8401 0.9263 0.9352</cell><cell>0.9300</cell><cell>1.3%</cell></row><row><cell>Datasets</cell><cell cols="2">Teacher GLP</cell><cell>PLP</cell><cell cols="3">Student variants FT CPF-ind CPF-tra</cell><cell>+Impv.</cell><cell></cell></row><row><cell>Cora</cell><cell></cell><cell cols="4">0.8365 0.7616 0.8314 0.8557</cell><cell>0.8539</cell><cell>2.3%</cell><cell></cell></row><row><cell>Citeseer</cell><cell></cell><cell cols="4">0.7536 0.6630 0.7597 0.7696</cell><cell>0.7696</cell><cell>2.1%</cell><cell></cell></row><row><cell>Pubmed</cell><cell></cell><cell cols="3">0.8088 0.6215 0.7842</cell><cell>0.8133</cell><cell>0.8210</cell><cell>1.5%</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">April 19-23, 2021 ,Ljubljana, Slovenia. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3442381.3450068</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">We also tried to minimize KL-divergence or maximize cross entropy as alternatives. But we find that Euclidean distance performs best and is more numerically stable.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">We find that 2-layer MLP is necessary for increasing the model capacity of our student, though a single layer logistic regression is more interpretable.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">We omit the results of GLP on A-Computer/A-Photo because GLP performs much worse than other GNN models on these two datasets in our experiments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is supported by the National Natural Science Foundation of China (No. U20B2045, 62002029, 61772082, 61702296), the Fundamental Research Funds for the Central Universities 2020RC23, and the National Key Research and Development Program of China (2018YFB1402600).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DETAILS FOR REPRODUCIBILITY</head><p>In the appendix, we provide more details of experimental settings of teacher models for reproducibility.</p><p>The training settings of 5 classical GNNs come from the paper <ref type="bibr" target="#b22">[23]</ref>. For the two recent ones (GCNII and GLP), we follow the settings in their original papers. The details are as follows:</p><p>• GCN <ref type="bibr" target="#b10">[11]</ref>: we use 64 as hidden-layer size, 0.01 as learning rate, 0.8 as dropout probability and 0.001 as learning rate decay.</p><p>• GAT <ref type="bibr" target="#b28">[29]</ref>: we use 64 as hidden-layer size, 0.01 as learning rate, 0.6 as dropout probability, 0.3 as attention dropout probability, and 0.01 as learning rate decay.</p><p>• APPNP <ref type="bibr" target="#b11">[12]</ref>: we use 64 as hidden-layer size, 0.01 as learning rate, 0.5 as dropout probability and 0.01 as learning rate decay.</p><p>• SAGE <ref type="bibr" target="#b6">[7]</ref>: we use 128 as hidden-layer size, 0.01 as learning rate, 5 as sample number, 256 as batch size and 0.0005 as learning rate decay.</p><p>• SGC <ref type="bibr" target="#b31">[32]</ref>: we use 0.1 as learning rate and 0.001 as learning rate decay.</p><p>• GCNII <ref type="bibr" target="#b3">[4]</ref>: we use 16 as layer number, 64 as hidden-layer size, 0.01 as learning rate, 0.6 as dropout probability, 256 as batch size and 0.1/0.0005 as learning rate decays.</p><p>• GLP <ref type="bibr" target="#b13">[14]</ref>: we use 16 as hidden-layer size, 0.01 as learning rate, 0.5 as dropout probability, 0.0005 as learning rate decay, k=2 for rnm setting and alpha=10 for ar setting.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of text classification algorithms</title>
		<author>
			<persName><forename type="first">C</forename><surname>Charu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining text data</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="163" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Bias and variance in the social structure of gender</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kristen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Altenburger</surname></persName>
		</author>
		<author>
			<persName><surname>Ugander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04774</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Node classification in social networks</title>
		<author>
			<persName><forename type="first">Smriti</forename><surname>Bhagat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Social network data analytics</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="115" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02133</idno>
		<title level="m">Simple and deep graph convolutional networks</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Born Again Neural Networks</title>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural message passing for Quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transductive learning via spectral graph partitioning</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on International Conference on Machine Learning</title>
				<meeting>the Twentieth International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="290" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Weißenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13354" to="13366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Label efficient semi-supervised learning via graph filtering</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9582" to="9591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">User profiling in an ego network: co-profiling attributes and relationships</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Chuan</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on World wide web</title>
				<meeting>the 23rd international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="819" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Birds of a feather: Homophily in social networks</title>
		<author>
			<persName><forename type="first">Lynn</forename><surname>Miller Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of sociology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Query-driven active surveying for collective classification</title>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umd</forename><surname>Edu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Workshop on Mining and Learning with Graphs</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GMNN: Graph Markov Neural Networks</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5241" to="5250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
	<note>Why should I trust you?</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<title level="m">Pitfalls of graph neural network evaluation</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Yunsheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.03509</idno>
		<title level="m">Masked Label Prediction: Unified Massage Passing Model for Semi-Supervised Classification</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-Stage Self-Supervised Learning for Graph Convolutional Networks on Graphs with Few Labeled Nodes</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Partially labeled classification with Markov random walks</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="945" to="952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scalable learning of collective behavior based on sparse social dimensions</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM conference on Information and knowledge management</title>
				<meeting>the 18th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unifying graph convolutional neural networks and label propagation</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06755</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<title level="m">Deep graph library: Towards efficient and scalable deep learning on graphs</title>
				<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distilling Knowledge From Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Yiding</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7074" to="7083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reliable Data Distillation on Graph Convolutional Network</title>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xupeng</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Ruas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data</title>
				<meeting>the 2020 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1399" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning with local and global consistency</title>
		<author>
			<persName><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Thomas N Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data with label propagation</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>CMU-CALD-02-107</idno>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using Gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on International Conference on Machine Learning</title>
				<meeting>the Twentieth International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
