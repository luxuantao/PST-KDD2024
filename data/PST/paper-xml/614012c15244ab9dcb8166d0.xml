<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-09-13">13 Sep 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Runxin</forename><surname>Xu</surname></persName>
							<email>runxinxu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="laboratory" key="lab2">MOE</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Equal Contribution. Joint work between Alibaba</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fuli</forename><surname>Luo</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Equal Contribution. Joint work between Alibaba</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="laboratory" key="lab2">MOE</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
							<email>chuanqi.tcq@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Key Laboratory of Computational Linguistics</orgName>
								<orgName type="laboratory" key="lab2">MOE</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
							<email>songfang.hsf@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
							<email>f.huang@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-09-13">13 Sep 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2109.05687v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent pretrained language models extend from millions to billions of parameters. Thus the need to fine-tune an extremely large pretrained model with a limited training corpus arises in various downstream tasks. In this paper, we propose a straightforward yet effective fine-tuning technique, CHILD-TUNING, which updates a subset of parameters (called child network) of large pretrained models via strategically masking out the gradients of the non-child network during the backward process. Experiments on various downstream tasks in GLUE benchmark show that CHILD-TUNING consistently outperforms the vanilla fine-tuning by 1.5 ‚àº 8.6 average score among four different pretrained models, and surpasses the prior fine-tuning techniques by 0.6 ‚àº 1.3 points. Furthermore, empirical results on domain transfer and task transfer show that CHILD-TUNING can obtain better generalization performance by large margins.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained Language Models (PLMs) have had a remarkable effect on the natural language processing (NLP) landscape recently <ref type="bibr" target="#b6">(Devlin et al., 2019;</ref><ref type="bibr" target="#b23">Liu et al., 2019;</ref><ref type="bibr" target="#b4">Clark et al., 2020)</ref>. Pretraining and fine-tuning have become a new paradigm of NLP, dominating a large variety of tasks.</p><p>Despite its great success, how to adapt such large-scale pretrained language models with millions to billions of parameters to various scenarios, especially when the training data is limited, is still challenging. Due to the extremely large capacity and limited labeled data, conventional transfer learning tends to aggressive fine-tuning <ref type="bibr" target="#b13">(Jiang et al., 2020)</ref>, resulting in: 1) degenerated results on the test data due to overfitting <ref type="bibr" target="#b6">(Devlin et al., 2019;</ref><ref type="bibr" target="#b29">Phang et al., 2018;</ref><ref type="bibr" target="#b19">Lee et al., 2020)</ref>, and 2)</p><formula xml:id="formula_0">+ = Forward Vanilla Backward CHILD-TUNING Backward ùë§ ! ‚àÜùë§ ! ùë§ " + =</formula><p>Task-Free or Task-Driven Gradients Mask</p><p>Pretrained Weights</p><p>Weights at 1-th Iteration ùë§ ! ‚àÜùë§ ! ùë§ "</p><p>Figure <ref type="figure" target="#fig_6">1</ref>: The illustration of CHILD-TUNING. Left: It forwards on the whole network while backwarding on a subset of network (i.e., child network). Right:</p><p>To achieve this, a task-free or task-driven mask is performed on the gradients of the non-child network, resetting them to zero (grey diagonal grids).</p><p>poor generalization ability in transferring to out-ofdomain data or other related tasks <ref type="bibr">(Mahabadi et al., 2021;</ref><ref type="bibr" target="#b0">Aghajanyan et al., 2021)</ref>.</p><p>Preventing the fine-tuned models to deviate too much from the pretrained weights (i.e., with less knowledge forgetting), is proved to be effective to mitigate the above challenges <ref type="bibr" target="#b10">(Gouk et al., 2020)</ref>. For instance, RecAdam <ref type="bibr" target="#b3">(Chen et al., 2020)</ref> introduces L 2 distance penalty between the fine-tuned weights and their pretrained weights. In addition, Mixout <ref type="bibr" target="#b19">(Lee et al., 2020)</ref> randomly replaces part of the model parameters with their pretrained weights during fine-tuning. The core idea behind them is to utilize the pretrained weights to regularize the fine-tuned model.</p><p>In this paper, we propose to mitigate the aggressive fine-tuning problem from a new perspective. Based on the observation that it is unnecessary to update all the parameters within the large-scale model during fine-tuning, we propose an effective fine-tuning technique, CHILD-TUNING, which straightforwardly updates a subset of parameters (called child network) via strategically masking out the gradients of non-child network in the backward process, as illustrated in Figure <ref type="figure" target="#fig_6">1</ref>. Note that it is different from model pruning, since it still forwards on the whole network, thus making the full use of knowledge hidden in the pretrained weights.</p><p>In detail, we propose two variants, CHILD-TUNING F and CHILD-TUNING D , which respectively detect the child network in a task-free and a task-driven way. CHILD-TUNING F chooses out the child network in the absence of task data via a Bernoulli distribution. It introduces noise to the full gradients, playing a role of regularization, hence preventing overfitting to small datasets and leading to better generalization. Furthermore, CHILD-TUNING D utilizes the downstream task data to detect the most task-related parameters as the child network and freezes the parameters in non-child network to their pretrained weights. It decreases the hypothesis space of the model via a task-specific mask applied to the full gradients, helping to effectively adapt the large-scale pretrained model to various tasks and meanwhile greatly maintain its original generalization ability.</p><p>Our extensive experiments on the GLUE benchmark show that CHILD-TUNING can be more excellent at fine-tuning different PLMs, with up to 8.60 average score improvement on CoLA/RTE/MRPC/STS-B tasks compared to vanilla fine-tuning (Section. 3.3). Moreover, it achieves better generalization ability in transferring to out-of-domain data and other related tasks <ref type="bibr">(Section. 3.4)</ref>. Experimental results also demonstrate that CHILD-TUNING yields consistently greater improvements than state-of-the-art fine-tuning methods. More importantly, since CHILD-TUNING is orthogonal to these prior methods, integrating CHILD-TUNING with them can even lead to further improvements (Section. 4.1).</p><p>In summary, our contributions are three-fold:</p><p>‚Ä¢ We propose CHILD-TUNING, a straightforward yet effective fine-tuning technique that only updates the parameters in the child network. We explore to detect the child network in both task-free and task-driven ways.</p><p>‚Ä¢ CHILD-TUNING can effectively adapt the large-scale pretrained model to various downstream scenarios, from in-domain to out-ofdomain, and cross-task transfer learning.</p><p>‚Ä¢ Since CHILD-TUNING is orthogonal to prior fine-tuning methods, integrating CHILD-TUNING with them can further boost the finetuning performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>To better adapt large-scale pretrained language model to various downstream tasks, we propose a simple yet effective fine-tuning technique, CHILD-TUNING. We firstly introduce a gradient mask in the backward process to achieve the aim of updating a subset of parameters (i.e., child network), while still utilizing the knowledge of the whole large model in the forward process (Section 2.1). Then, we explore two ways to detect the child network (i.e., generate different gradient masks): CHILD-TUNING F that are in a task-free way (Section 2.2), and CHILD-TUNING D that are in a taskdriven way ( Section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of CHILD-TUNING</head><p>We start the introduction of CHILD-TUNING by giving a general formulation of the back propagation during the vanilla fine-tuning. We denote the parameters of the model at the t-th iteration as w t (w 0 refers to the pretrained weights). The vanilla fine-tuning computes the gradient of the loss L(w t ) and then applies gradient descent to all parameters, which can be formulated as:</p><formula xml:id="formula_1">w t+1 = w t ‚àí Œ∑ ‚àÇL(w t ) ‚àÇw t<label>(1)</label></formula><p>where ‚àÇL(wt)  ‚àÇwt are the gradients corresponding to the model parameters w t , Œ∑ is the learning rate.</p><p>CHILD-TUNING also backwardly computes the gradients of all trainable parameters like standard fine-tuning. However, the key difference is that CHILD-TUNING determines a child network C t at the t-th iteration, and only updates this part of parameters. To achieve this, we firstly define a 0-1 mask that is the same-sized as w as follows:</p><formula xml:id="formula_2">M (i) t = 1, w (i) t ‚àà C t 0, w (i) t / ‚àà C t (2)</formula><p>where</p><formula xml:id="formula_3">M (i)</formula><p>t and w</p><formula xml:id="formula_4">(i)</formula><p>t denote the i-th element of the mask M t and parameters w t at the t-th training iteration, respectively.</p><p>Then, we formally define CHILD-TUNING technique by simply replacing Eq. 1 with the following equation:</p><formula xml:id="formula_5">w t+1 = w t ‚àí Œ∑ ‚àÇL(w t ) ‚àÇw t M t (3)</formula><p>Algorithm 1 provides the pseudo-code of CHILD-TUNING when applied to widely used Adam <ref type="bibr" target="#b17">(Kingma and Ba, 2015)</ref> optimizer. The main difference is the insertion of line 5-7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 CHILD-TUNING for Adam Optimizer</head><p>Require: w 0 : initial pretrained weights; L(w):</p><p>stochastic objective function with parameters w; Œ∑: learning rate; Œ≤ 1 , Œ≤ 2 ‚àà [0, 1): exponential decay rates for the moment estimates; 1: initialize timestep t ‚Üê 0, first moment vector m 0 ‚Üê 0, second moment vector v 0 ‚Üê 0 2: while not converged do 3: </p><formula xml:id="formula_6">t ‚Üê t + 1 // Get</formula><formula xml:id="formula_7">m t ‚Üê Œ≤ 1 ‚Ä¢ m t‚àí1 + (1 ‚àí Œ≤ 1 ) ‚Ä¢ g t 9: v t ‚Üê Œ≤ 2 ‚Ä¢ v t‚àí1 + (1 ‚àí Œ≤ 2 ) ‚Ä¢ g 2 t // Bias correction 10: mt ‚Üê m t /(1 ‚àí Œ≤ t 1 ) 11: vt ‚Üê v t /(1 ‚àí Œ≤ t 2 ) // Update weights 12: w t ‚Üê w t‚àí1 ‚àí Œ∑ ‚Ä¢ mt /( ‚àö<label>vt</label></formula><p>+ ) 13: end while 14: return w t 2.2 Task-Free Variant: CHILD-TUNING F In this section, we firstly explore the choice of the child network that does not require any downstream task data, i.e., a task-free technique called CHILD-TUNING F . Specifically, CHILD-TUNING F generates a 0-1 mask M t at the t-th iteration drawn from a Bernoulli distribution with a probability p F :</p><formula xml:id="formula_8">M t ‚àº Bernoulli(p F )<label>(4)</label></formula><p>The higher the p F is, the larger the child network is, and hence more parameters are updated. When p F = 1, CHILD-TUNING F degenerates into the vanilla fine-tuning method. Note that we also enlarge the reserved gradients by 1 p F to maintain the expectation of the gradients.</p><p>We theoretically justify the effectiveness of CHILD-TUNING F . We denote ‚àÜw as the update at each iteration:</p><formula xml:id="formula_9">‚àÜw = Œ∑ ‚àÇL(w) ‚àÇw M<label>(5)</label></formula><p>Intuitively, Theorem 1 shows the variance of gradients is a strictly decreasing function of p F . Thus, CHILD-TUNING F improves the variance of the gradients, and the trade-off between exploration and exploitation can be controlled by adjusting p F . As illustrated in Theorem 2, with higher variance, the model can converge to more flat local minima (smaller œÅ in Theorem 2). Inspired by studies that show flat minima tends to generalize better <ref type="bibr" target="#b15">(Keskar et al., 2017;</ref><ref type="bibr" target="#b31">Sun et al., 2020;</ref><ref type="bibr" target="#b8">Foret et al., 2021)</ref>, we can further prove CHILD-TUNING F decreases the generalization error bound.</p><p>Theorem 1. Suppose L denotes the loss function on the parameter w, the gradients obey a Gaussian distribution N ( ‚àÇL ‚àÇw , œÉ 2 g I k ), and SGD with learning rate Œ∑ is used. For a randomly sampled batch B, if GradMask reserves gradients with probability p F , the mean and covariance of the update ‚àÜw are,</p><formula xml:id="formula_10">E[‚àÜw] = ‚àíŒ∑ ‚àÇL ‚àÇw (6) Œ£[‚àÜw] = Œ∑ 2 œÉ 2 g I k p F |B| + (1 ‚àí p F )Œ∑ 2 diag{ ‚àÇL ‚àÇw } 2 p F (7) Specially, when w is a local minima, E[‚àÜw] = 0 k , Œ£[‚àÜw] = œÉ 2 I k and œÉ 2 = Œ∑ 2 œÉ 2 g p F |B| is a strictly decreasing function of p F .</formula><p>Theorem 2. Suppose w 0 denotes the pretrained parameter; k is the number of parameters; w denotes the local minima the algorithm converges to; œÅ is the greatest eigenvalue of the Hessian matrix on w, which indicates the sharpness. If ‚àÜw ‚àº N (0 k , œÉ 2 I k ), when the following bound holds, the algorithm can converge to the local minima w with high probability,</p><formula xml:id="formula_11">œÅ ‚â§ O 1 œÉ 2 (8)</formula><p>Suppose the prior over parameters after training is P = N (w 0 , œÉ 2 0 I k ), the following generalization error bound holds with high probability, bound(w</p><formula xml:id="formula_12">) ‚â§ O kœÉ 2 0 ‚àí w‚àíw 0 2 œÉ 2 + R (9)</formula><p>where R is a term not determined by œÉ.</p><p>Thus, CHILD-TUNING F can be viewed as a strong regularization for the optimization process. It enables the model to skip the saddle point in the loss landscape and encourages the model to converge to a more flat local minima. Please refer to Appendix E for more details about stated theorems and proofs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Task-Driven Variant: CHILD-TUNING D</head><p>Taking the downstream labeled data into consideration, we propose CHILD-TUNING D , which detects the most important child network for the target task. Specifically, we adopt the Fisher information estimation to find the highly relevant subset of the parameters for a specific downstream task. Fisher information serves as a good way to provide an estimation of how much information a random variable carries about a parameter of the distribution <ref type="bibr">(Tu et al., 2016a,b)</ref>. For a pretrained model, Fisher information can be used to measure the relative importance of the parameters in the network towards the downstream tasks.</p><p>Formally, the Fisher Information Matrix (FIM) for the model parameters w is defined as follows:</p><formula xml:id="formula_13">F (w) = E ( ‚àÇ log p(y|x;w) ‚àÇw )( ‚àÇ log p(y|x;w) ‚àÇw )</formula><p>where x and y denote the input and the output respectively. It can be also viewed as the covariance of the gradient of the log likelihood with respect to the parameters w. Following <ref type="bibr" target="#b18">Kirkpatrick et al. (2016)</ref>, given the task-specific training data data D, we use the diagonal elements of the empirical FIM to point-estimate the task-related importance of the parameters. Formally, we derive the Fisher information for the i-th parameter as follows:</p><formula xml:id="formula_14">F (i) (w) = 1 |D| |D| j=1 ‚àÇ log p (y j |x j ; w) ‚àÇw (i) 2 (10)</formula><p>We assume that the more important the parameter towards the target task, the higher Fisher information it conveys. Hence the child network C is comprised of the parameters with the highest information. The child network ratio is p D = |C| |C|+|C| ‚àà (0, 1], where C denotes the non-child network. As p D rises, the scale of the child network also increases, and when p D = 1 it degenerates into the vanilla fine-tuning strategy.</p><p>Since the overhead of obtaining the task-driven child network is heavier than that of the taskfree one, we simply derive the child network for CHILD-TUNING D at the beginning of fine-tuning, and keep it unchanged during the fine-tuning, i.e.,</p><formula xml:id="formula_15">C 0 = C 1 = ‚Ä¢ ‚Ä¢ ‚Ä¢ = C T .</formula><p>In this way, CHILD-TUNING D dramatically decreases the hypothesis space of the large-scale models, thus alleviating overfitting. Meanwhile, keeping the non-child network freezed to their pretrained weights can substantially maintain the generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>3.1 Datasets GLUE benchmark Following previous studies <ref type="bibr" target="#b19">(Lee et al., 2020;</ref><ref type="bibr" target="#b7">Dodge et al., 2020)</ref>, we conduct experiments on various datasets from GLUE leaderboard <ref type="bibr" target="#b34">(Wang et al., 2019)</ref>, including linguistic acceptability (CoLA), natural language inference (RTE, QNLI, MNLI), paraphrase and similarity (MRPC, STS-B, QQP), and sentiment classification (SST-2). CoLA and SST-2 are single-sentence classification tasks and the others are involved with a pair of sentences. The detailed statistics and metrics are provided in Appendix A. Following most previous works <ref type="bibr" target="#b29">(Phang et al., 2018;</ref><ref type="bibr" target="#b19">Lee et al., 2020;</ref><ref type="bibr" target="#b7">Dodge et al., 2020)</ref>, we fine-tune the pretrained model on the training set and directly report results on the dev set using the last checkpoint, since the test results are only accessible by the leaderboard with a limitation of the number of submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NLI datasets</head><p>In this paper, we also conduct experiments to explore the generalization ability of the fine-tuned model based on several Natural Language Inference (NLI) tasks. Specifically, we additionally introduce three NLI datasets, i.e., SICK <ref type="bibr" target="#b26">(Marelli et al., 2014)</ref>, <ref type="bibr">SNLI (Bowman et al., 2015)</ref> and SciTail <ref type="bibr" target="#b16">(Khot et al., 2018)</ref>. We also report results on the dev set consistent with GLUE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experiments Setup</head><p>We use the pretrained models and codes provided by HuggingFace<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b35">(Wolf et al., 2020)</ref>, and follow their default hyperparameter settings unless noted otherwise. Appendix B provides detailed experimental setups (e.g., batch size, training steps, and etc.) for BERT LARGE <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref>, XLNet LARGE <ref type="bibr" target="#b36">(Yang et al., 2019)</ref>, RoBERTa LARGE <ref type="bibr" target="#b23">(Liu et al., 2019)</ref>, and ELECTRA LARGE <ref type="bibr" target="#b4">(Clark et al., 2020)</ref>. We report the averaged results over 10 random seeds.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results on GLUE Benchmark</head><p>In this section, we show the results of four widely used large PLMs on four GLUE tasks: CoLA, RTE, MRPC, and STS-B, following <ref type="bibr" target="#b19">Lee et al. (2020)</ref>.</p><p>Besides vanilla fine-tuning, we also report the results of two variants of CHILD-TUNING, including both CHILD-TUNING F (p F = 0.2, 0.3, 0.4) and CHILD-TUNING D (p D = 0.1, 0.2, 0.3).</p><p>As Table <ref type="table" target="#tab_1">1</ref> illustrates, CHILD-TUNING outperforms vanilla fine-tuning by a large gain across all the tasks on different PLMs. For instance, CHILD-TUNING yields an improvement of up to 2.08 average score on XLNet, and 8.60 average score on ELECTRA. Besides, the straightforward task-free variant, CHILD-TUNING F , can still provide an improvement of 0.87 average score on BERT and 6.27 on ELECTRA. CHILD-TUNING D , which detects child network in a task-driven way, is more aware of the unique characteristics of the downstream task, and therefore achieves the best performance, with up to 1.50 and 8.60 average score improvement on BERT and ELECTRA. In summary, we can come to a conclusion that CHILD-TUNING is model-agnostic and can consistently outperform vanilla fine-tuning on different PLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Probing Generalization Ability of the Fine-tuned Model</head><p>To measure the generalization properties of various fine-tuning methods, in this section, we conduct probing experiments from two aspects, that is, domain generalization and task generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Domain Generalization</head><p>Besides boosting performance on the target downstream task, we also expect CHILD-TUNING can help the fine-tuned model achieve better generalization ability towards out-of-domain data.</p><p>We evaluate how well the fine-tuned model generalizes to out-of-domain data based on several Natural Language Inference (NLI) tasks. In detail, The results suggest that CHILD-TUNING encourages the model to learn more general semantic features during fine-tuning, rather than some superficial features unique to the training data. Hence, the fine-tuned model can well generalize to different datasets, even though their domains are quite different from the dataset the model is trained on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Task Generalization</head><p>To justify the generalization ability of the model from another perspective, we follow the probing experiments from <ref type="bibr" target="#b0">Aghajanyan et al. (2021)</ref>, which first freezes the representations from the model trained on one task and then only trains a linear classifier on top of the model for another task.</p><p>In particular, we fine-tune BERT LARGE on MRPC task, and transfer to four other GLUE tasks, i.e., CoLA, STS-B, QNLI, and QQP. As In summary, fine-tuning with CHILD-TUNING gains better performance when the fine-tuned model is transferred to another task, demonstrating that CHILD-TUNING can maintain more generalizable representations produced by the model than vanilla fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Analysis and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with Prior Methods</head><p>In this section, we review and compare prior studies towards effective fine-tuning: 1) Weight Decay <ref type="bibr" target="#b5">(Daum√© III, 2007)</ref>, which adds the Œª w‚àíw 0 2 penalty to the loss function, where w 0 denotes the pretrained weights; 2) Top-K Tuning, which only fine-tune the top-K layers of the model with other layers freezed. <ref type="bibr" target="#b12">Houlsby et al. (2019)</ref> uses it as a strong baseline; 3) Mixout <ref type="bibr" target="#b19">(Lee et al., 2020)</ref>, which randomly replaces the parameters with their pretrained weights; 4) RecAdam <ref type="bibr" target="#b3">(Chen et al., 2020)</ref>, which is similar to Weight Decay while its loss weights Œª keeps changing during finetuning; 5) Robust Representations through Regularized Finetuning (R3F) <ref type="bibr" target="#b0">(Aghajanyan et al., 2021)</ref>, which is rooted in trust region theory. Appendix C shows detailed hyperparameter settings.</p><p>We compare CHILD-TUNING with these methods based on BERT LARGE , and report the mean (max) score results in Table <ref type="table" target="#tab_4">3</ref>, following <ref type="bibr" target="#b19">Lee et al. (2020)</ref>. While all the fine-tuning methods can bring improvements across four different tasks compared with vanilla fine-tuning, CHILD-TUNING achieves the best performance. In detail, among prior finetuning methods, Mixout and R3F yield the highest improvement with 0.84 and 0.88 average score re- spectively. CHILD-TUNING F has performance on par with Mixout and R3F, while CHILD-TUNING D achieves 1.50 average score improvement in total. More importantly, CHILD-TUNING is flexible and orthogonal to most fine-tuning methods. Thus, integrating CHILD-TUNING with other methods can boost the performance. For instance, combining CHILD-TUNING D with R3F leads to a 1.84 average score improvement in total.</p><p>In short, compared with prior fine-tuning methods, we find that 1) CHILD-TUNING is more effective in adapting PLMs to various tasks, especially for the task-driven variant CHILD-TUNING D , and 2) CHILD-TUNING has the advantage that it is flexible enough to integrate with other methods to potentially achieve further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results in Low-resource Scenarios</head><p>Fine-tuning a large pretrained model on extremely small datasets can be very challenging since the risk of overfitting rises <ref type="bibr" target="#b7">(Dodge et al., 2020)</ref>. Thus, in this section, we explore the effect of CHILD-TUNING with only a few training examples. To this end, we downsample all datasets in GLUE to 1k training examples and fine-tune BERT LARGE on them.</p><p>As Table <ref type="table" target="#tab_5">4</ref> demonstrates, compared with vanilla fine-tuning, CHILD-TUNING F improves the average score by 1.42, and the improvement is even larger for CHILD-TUNING D , which is up to 2.24. It suggests that although overfitting is quite severe when the training data is in extreme lowresource scenarios, CHILD-TUNING can still effectively improve the model performance, especially for CHILD-TUNING D since it decreases the hypothesis space of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">What is the Difference Between</head><p>CHILD-TUNING and Model Pruning?</p><p>CHILD-TUNING D detects the most important child network in a task-driven way, and only updates this parameters within the child network during the fine-tuning with other parameters freezed. It is very likely to be confused with model pruning <ref type="bibr" target="#b21">(Li et al., 2017;</ref><ref type="bibr" target="#b38">Zhu and Gupta, 2018;</ref><ref type="bibr" target="#b22">Lin et al., 2020)</ref>, which also detects a subnetwork within the model (but then removes the other parameters). Actually, CHILD-TUNING and model pruning are different in both the objectives and methods. Regarding objectives, model pruning aims at improving the inference efficiency and maintaining the performance at the same time, while CHILD-TUNING is proposed to address the overfitting problem and improve the generalization ability for largescale language models during fine-tuning. Regrading model pruning abandons the unimportant parameters during inference, while the parameters that do not belong to the child network are still reserved for CHILD-TUNING during training and inference. In this way, the knowledge of the non-child network hidden in the pretrained weights will be fully utilized.</p><p>To better illustrate the effectiveness of CHILD-TUNING D compared to model pruning, we set all the parameters not belonging to the child network to zero, which is referred to as Prune in Table <ref type="table" target="#tab_6">5</ref>. It shows that, once we abandon parameters out of the child network, the score dramatically decreases by 33.89 points averaged on four tasks (CoLA/RTE/MRPC/STS-B), and the model even collapses on CoLA task. It also suggests that besides parameters in child network, those in the nonchild network are also necessary since they can provide general knowledge learned in pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Is the Task-Driven Child Network Really</head><p>that Important to the Target Task?</p><p>CHILD-TUNING D detects the task-specific child network by means of choosing parameters with the highest Fisher information towards the downstream task data. In this section, we exlore whether the detected task-driven child network is really that important to the task.</p><p>To this end, we introduce two ablation studies for CHILD-TUNING D : 1) Random: We randomly choose a child network and keep it unchanged during fine-tuning; 2) Lowest Info.: We choose those parameters with lowest Fisher information as the child network, contrasted to the highest Fisher in- formation adopted in CHILD-TUNING D .</p><formula xml:id="formula_16">C o L A R T E M R P C S T S -B M N L I Q N L I Q Q P RTE</formula><p>As shown in Table <ref type="table" target="#tab_6">5</ref>, choosing the child network randomly can even outperform vanilla fine-tuning, with 0.18 average score improvement. It supports our claim that there is no need to update all parameters of the large PLMs, and decreasing the hypothesis space can reduce the risk of overfitting. However, it is still worth finding a proper child network to further boost the performance. If we choose parameters with the lowest Fisher information (Lowest Fisher), the average score is dramatically decreased by 6.65 compared with choosing with the highest Fisher information adopted in CHILD-TUNING D . Hence, we can conclude that the child network detected by CHILD-TUNING D is indeed important to the downstream task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">What is the Relationship among Child</head><p>Networks for Different Tasks?</p><p>As the task-driven child networks are correlated with the tasks, we further explore the relationship among child networks for different tasks. To this end, we visualize the overlapping rate among different task-driven child networks, where we use the Jaccard similarity coefficient,</p><formula xml:id="formula_17">|C i ‚à©C j | |C i ‚à™C j |</formula><p>, to calculate the overlapping rate between task i and j.</p><p>Figure <ref type="figure" target="#fig_1">3</ref> shows the overlap among GLUE tasks. As we expected, similar tasks tend to have higher overlapping ratios of child network. For example, the overlapping ratio among NLI tasks is remarkably higher than others, such as RTE and QNLI, QNLI and MNLI. For different kinds of tasks, their overlapping ratio is relatively lower, such as CoLA and MRPC. It is also interesting to find that the task-driven child network for SST2 overlaps less with other tasks except CoLA, even though SST2 and CoLA is not so similar. The reason may be that both SST2 and CoLA belongs to a single sentence classification task, while others are in a different format of sentence-pair classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Explosion of PLMs. There has been an explosion of studies on Pretrained Language Models (PLMs). <ref type="bibr" target="#b6">Devlin et al. (2019)</ref> propose BERT that is pretrained on large quantities of unannotated corpus with self-supervised tasks. Many PLMs also emerged such as GPT-2 <ref type="bibr" target="#b30">(Radford et al., 2018)</ref>, <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>, ELECTRA <ref type="bibr" target="#b4">(Clark et al., 2020)</ref>, XLNet <ref type="bibr" target="#b36">(Yang et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b23">(Liu et al., 2019)</ref>, and BART <ref type="bibr" target="#b20">(Lewis et al., 2020)</ref>. The number of parameters of PLMs also explodes. BERT LARGE has 340 millions of parameters, and the number for GPT-3 is even up to 175 billions.</p><p>Effective and generalizable fine-tuning. With a mass of parameters, fine-tuning large PLMs tend to achieve degenerated performance due to overfitting and have poor generalization ability, especially on small datasets <ref type="bibr" target="#b6">(Devlin et al., 2019;</ref><ref type="bibr" target="#b29">Phang et al., 2018;</ref><ref type="bibr" target="#b19">Lee et al., 2020)</ref>. Therefore, different finetuning techniques have been proposed. Some of them utilize the pretrained weights to regularize the deviation of the fine-tuned model <ref type="bibr" target="#b19">(Lee et al., 2020;</ref><ref type="bibr" target="#b5">Daum√© III, 2007;</ref><ref type="bibr" target="#b3">Chen et al., 2020)</ref>, while others compress the output information <ref type="bibr">(Mahabadi et al., 2021)</ref> or injects noise into the input <ref type="bibr" target="#b13">(Jiang et al., 2020;</ref><ref type="bibr" target="#b0">Aghajanyan et al., 2021)</ref>. Moreover, <ref type="bibr" target="#b37">Zhang et al. (2021)</ref> and <ref type="bibr" target="#b27">Mosbach et al. (2021)</ref> point out that the omission of bias correction in the Adam optimizer used in <ref type="bibr" target="#b6">Devlin et al. (2019)</ref> is also responsible for the degenerated results. Orthogonal to these methods, CHILD-TUNING address the problems by detecting the child network within the model in a task-free or task-driven way. It only updates parameters within the child network via a gradient mask, which is proved to be effective in adapting large PLMs to various tasks, along with better generalization ability.</p><p>Parameter-efficient Fine-tuning. There are also studies focusing on parameter-efficient fine-tuning, for example, the adapter-based methods <ref type="bibr" target="#b12">(Houlsby et al., 2019;</ref><ref type="bibr" target="#b28">Pfeiffer et al., 2020;</ref><ref type="bibr">Karimi Mahabadi et al., 2021)</ref>, and the Diff-Pruning method <ref type="bibr" target="#b11">(Guo et al., 2021)</ref>. However, our CHILD-TUNING is different from this line of works. Firstly, they aim at fine-tuning as few as possible parameters to maintain performance, while we target effective and generalizable fine-tuning. Secondly, Diff-Pruning sparsifies diff-vector with gradient estimators, and adapterbased methods fine-tune new added module during training, while we detect the child network inside the model without extra parameters and only need to calculate the FIM before training for CHILD-TUNING D . Finally, we consistently outperform vanilla fine-tuning by a large margin, while they achieve competitive performance with full model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>To mitigate the overfitting problem and improve generalization for fine-tuning large-scale PLMs, we propose a straightforward yet effective fine-tuning technique, CHILD-TUNING, which only updates the child network during fine-tuning via strategically masking out the gradients of the non-child network. Two variants are introduced, CHILD-TUNING F and CHILD-TUNING D , which detect the child network in a task-free and task-driven way, respectively. Extensive experiments on various downstream tasks show that both of them can outperform vanilla fine-tuning and prior works by large gains among four different pretrained language models, and meanwhile largely enhance the generalization ability of the fine-tuned models. Since CHILD-TUNING is orthogonal to most prior fine-tuning techniques, integrating CHILD-TUNING with them can further boost the performance.  and ELECTRA LARGE<ref type="foot" target="#foot_8">9</ref> . The training epochs/steps, batch size, and warmup steps are listed in Table 7. We use AdamW <ref type="bibr" target="#b24">(Loshchilov and Hutter, 2019)</ref> optimizer, and set Œ≤ 1 = 0.9, Œ≤ 2 = 0.999, = 1e-6. We clip the gradients with a maximum norm of 1, and the maximum sequence length is set as 128. For CHILD-TUNING F , we uses p F = {0.2, 0.3, 0.4} and re-scale the gradients to ensure the gradients after CHILD-TUNING F are unbiased. For CHILD-TUNING D , we use p D = {0.1, 0.2, 0.3}. We use grid search for learning rate from {1e-5, 2e-5, . . . , 1e-4}. We conduct all the experiments on a single GTX-3090 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset #Train</head><p>These pretrained models are all Transformerbased. XLNet <ref type="bibr" target="#b36">(Yang et al., 2019)</ref> is an autoregressive pretrained language model with token permutations. It generates tokens in an autoregressive way while can still capture bidirectional context information. RoBERTa <ref type="bibr" target="#b23">(Liu et al., 2019</ref>) is a robustly optimized version of BERT. It uses a dynamic masking mechanism, larger batch size, and longer training times, and it also abandons the next sentence prediction task. ELECTRA <ref type="bibr" target="#b4">(Clark et al., 2020)</ref> pretrains the model with a generator and a discriminator. The discriminator is trained to distinguish whether the token is generated by the generator or the original token.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Theoretical Details</head><p>We theoretically justify the effectiveness of CHILD-TUNING F . Assume CHILD-TUNING F reserves gradients with probability p F ‚àà (0, 1], and we simply use p to denote p F in the following content.</p><p>Theorem 1 shows the variance of gradients is a strictly decreasing function of p. When p = 1, it degenerates into normal fine-tuning methods. Therefore, CHILD-TUNING F can improve the variance of the gradients of the model. Next, Theorem 2 shows that with higher variance, the model can converge to more flat local minima (smaller œÅ in Theorem 2). Inspired by studies that show flat minima tends to generalize better <ref type="bibr" target="#b15">(Keskar et al., 2017;</ref><ref type="bibr" target="#b31">Sun et al., 2020;</ref><ref type="bibr" target="#b8">Foret et al., 2021)</ref>, we can further prove CHILD-TUNING F decreases the generalization error bound.</p><p>Theorem 1. Suppose L denotes the loss function on the parameter w, for multiple data instances in the training set x ‚àº S, the gradients obey a Gaussian distribution N ( ‚àÇL ‚àÇw , œÉ 2 g I k ). For a randomly sampled batch B ‚àº S, when the learning algorithm is SGD with learning rate Œ∑, the reserving probability of the CHILD-TUNING F is p, then the mean and covariance of the update ‚àÜw are,</p><formula xml:id="formula_18">E[‚àÜw] = ‚àíŒ∑ ‚àÇL ‚àÇw (11) Œ£[‚àÜw] = Œ∑ 2 œÉ 2 g I k p|B| + (1 ‚àí p)Œ∑ 2 diag{ ‚àÇL ‚àÇw } 2 p (<label>12</label></formula><formula xml:id="formula_19">)</formula><p>where Œ£ is the covariance matrix and diag(x) is the diagonal matrix of the vector x. Specially, when w is a local minima,</p><formula xml:id="formula_20">E[‚àÜw] = 0 k , Œ£[‚àÜw] = œÉ 2 I k and œÉ 2 = Œ∑ 2 œÉ 2 g</formula><p>p|B| is a strictly decreasing function of p.</p><p>Theorem 2. Suppose L denotes the expected error rate loss function; w 0 denotes the pretrained parameter; k is the number of parameters; w denotes the local minima the algorithm converges to; H is the Hessian matrix on w and œÅ is its greatest eigenvalue; F k is the cumulative distribution function of the œá 2 (k) distribution.</p><p>If the next update of the algorithm ‚àÜw ‚àº N (0 k , œÉ 2 I k ) and the training loss increases more than with probability Œ¥, we assume the algorithm will escape the local minima w. When the following bound holds, the algorithm can converge to the local minima w, with higher order infinity omitted, , with higher order infinity omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Proof of Theorem 1</head><p>Proof. Suppose g (i) is the gradient of data instance x (i) , (1 ‚â§ i ‚â§ |B|), then g (i) ‚àº N ( ‚àÇL ‚àÇw , œÉ 2 g I k ).</p><p>Then, define g = Suppose ƒùi , g i are the i-th dimension of ƒù, g, we have</p><formula xml:id="formula_21">D[ƒù i ] = E[ƒù 2 i ] ‚àí (E[ƒù i ]) 2 (18) = pE[( g i p ) 2 ] ‚àí (E[ƒù i ]) 2 (19) = E[g 2 i ] p ‚àí (E[ƒù i ]) 2 (20) = (E[g i ]) 2 + D[g i ] p ‚àí (E[ƒù i ]) 2 (21) = D[g i ] p + (1 ‚àí p)(E[ƒù i ]) 2 p (22)</formula><p>Therefore, </p><formula xml:id="formula_22">Œ£[ƒù] = œÉ 2 g I k p|B| + (1 ‚àí p)diag{E[g]} 2 p (<label>23</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Proof of Theorem 2</head><p>Proof. We first prove Eq. 13. Apply a Taylor expansion on training loss L, notice that ‚àá w L(w) = 0 k since w is a local minima. When the algorithm can escape the local minima w, with higher order infinity omitted, we have,</p><formula xml:id="formula_23">‚â§ L(w + v) ‚àí L(w) (26) =v T ‚àá w L(w) + 1 2 v T Hv + o( v 2 2 ) (27) ‚â§ œÅ v 2 2 2 + o( v 2 2 ) = œÅ v 2 2 2 (28)</formula><p>If the probability of escaping, P esc , we have when Eq. 13 holds,</p><formula xml:id="formula_24">P ( ‚àÜw œÉ 2 2 ‚â§ 2 œÅœÉ 2 ) = F k ( 2 œÅœÉ 2 ) (33) ‚â• F k (F ‚àí1 k (1 ‚àí Œ¥)) = 1 ‚àí Œ¥ (34)</formula><p>Therefore, P esc ‚â§ 1 ‚àí P ( ‚àÜw œÉ 2 2 ‚â§ 2 œÅœÉ 2 ) ‚â§ Œ¥. The algorithm will not escape the local minima w and can converge to the local minima w.</p><p>To prove Eq. 14, we introduce Lemma 1 in paper <ref type="bibr" target="#b8">Foret et al. (2021)</ref>, which is Theorem 2 in the paper. .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2 shows, CHILD-TUNING consistently outperforms vanilla fine-tuning on different transferred tasks. Compared with vanilla fine-tuning, CHILD-TUNING F improves 4.58 average score (58.95 ‚Üí 63.53), while CHILD-TUNING D even gains up to 7.06 average score improvement (58.95 ‚Üí 66.01).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The overlapping ratio among task-driven child networks among GLUE tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Suppose the prior over parameters after training is P = N (w 0 , œÉ 2 0 I k ), the following generalization error bound holds with probability 1-Œ¥ over the choice of training set S ‚àº D,bound(w) ‚â§ (kœÉ 2 0 ‚àí w ‚àí w 0 2 ) kF ‚àí1 k (1 ‚àí Œ¥)œÉ 2 + R (14)where bound(w) = L S (w) ‚àí L D (w), R =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>w is a local minima, ‚àÇL ‚àÇw = 0 k . Therefore, E[‚àÜw] = 0 k , Œ£[‚àÜw] = œÉ 2 I k and œÉ 2 = Œ∑ 2 œÉ 2 gp|B| is a strictly decreasing function of p.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>P</head><label></label><figDesc>esc = P (L(w + ‚àÜw) ‚àí L(w) ‚â• ) œÅœÉ 2 ) ‚â§ 1 ‚àí P esc . Since ‚àÜw œÉ ‚àº N (0 k , I k ),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Lemma 1 .</head><label>1</label><figDesc>Suppose d &gt; 0, the prior over parameters is P = N (w P , œÉ 2 P I k ) and œÉ 2 P = d 2 + w‚àíw P 2 k , the following bound holds with probability 1-Œ¥ over the choice of training set S ‚àº D, L D (w) ‚â§ max ‚àÜw 2 ‚â§d L S (w + ‚àÜw) + R (35)where k denotes the number of parameters and infinity omitted.In Lemma 1, when we set w P = w 0 and œÉ P = œÉ 0 , we have d 2 = œÉ 2 infinity omitted, we have max ‚àÜw 2 ‚â§d L S (w + ‚àÜw) = L S (w) + œÅd 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>gradients</figDesc><table><row><cell>4:</cell><cell>g t ‚Üê ‚àÇL(wt) ‚àÇwt</cell></row><row><cell></cell><cell>// Get task-free/task-driven child network</cell></row><row><cell>5:</cell><cell>C t ‚Üê GetChildNetwork()</cell></row><row><cell></cell><cell>// Generate a corresponding gradient mask</cell></row><row><cell>6:</cell><cell>M t ‚Üê GenerateMask(C t )</cell></row><row><cell></cell><cell>// Employ mask for gradients</cell></row><row><cell>7:</cell><cell>g t ‚Üê g t M t</cell></row><row><cell>8:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>2  Comparison between CHILD-TUNING and vanilla fine-tuning applied to four widely used large-scale Pretrained Language Models (PLMs). Average scores on all tasks are underlined. The best results are bold. It shows that CHILD-TUNING yields consistent improvements across all tasks among different PLMs, especially for CHILD-TUNING D that detects the child network in a task-driven way.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell>BERT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">XLNet</cell></row><row><cell></cell><cell></cell><cell cols="9">CoLA RTE MRPC STS-B Avg CoLA RTE MRPC STS-B Avg</cell></row><row><cell cols="5">Vanilla Fine-tuning 63.13 70.18 90.77</cell><cell cols="6">89.61 78.42 47.14 77.62 91.90</cell><cell>91.77 77.11</cell></row><row><cell cols="2">CHILD-TUNING F</cell><cell cols="3">63.71 72.06 91.22</cell><cell cols="6">90.18 79.29 52.07 78.05 92.29</cell><cell>91.81 78.56</cell></row><row><cell cols="2">CHILD-TUNING D</cell><cell cols="3">64.92 73.14 91.42</cell><cell cols="6">90.18 79.92 51.54 80.94 92.46</cell><cell>91.82 79.19</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell>RoBERTa</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ELECTRA</cell></row><row><cell></cell><cell></cell><cell cols="9">CoLA RTE MRPC STS-B Avg CoLA RTE MRPC STS-B Avg</cell></row><row><cell cols="5">Vanilla Fine-tuning 66.10 85.20 92.62</cell><cell cols="6">92.04 83.99 47.42 88.23 92.95</cell><cell>81.86 77.62</cell></row><row><cell cols="2">CHILD-TUNING F</cell><cell cols="3">65.99 84.80 92.66</cell><cell cols="6">92.15 83.90 62.31 88.41 93.09</cell><cell>91.73 83.89</cell></row><row><cell cols="2">CHILD-TUNING D</cell><cell cols="3">66.71 86.14 92.78</cell><cell cols="6">92.36 84.50 70.62 88.90 93.32</cell><cell>92.02 86.22</cell></row><row><cell>Datasets</cell><cell></cell><cell></cell><cell>MNLI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SNLI</cell></row><row><cell></cell><cell cols="2">Vanilla C.TUNING F</cell><cell>‚àÜ F</cell><cell cols="2">C.TUNING D</cell><cell>‚àÜ D</cell><cell cols="2">Vanilla C.TUNING F</cell><cell>‚àÜ F</cell><cell>C.TUNING D</cell><cell>‚àÜ D</cell></row><row><cell>MNLI</cell><cell>75.30</cell><cell>75.95</cell><cell>+0.65</cell><cell>76.61</cell><cell></cell><cell cols="2">+1.31 65.80</cell><cell>66.01</cell><cell>+0.21</cell><cell>66.82</cell><cell>+1.02</cell></row><row><cell cols="2">MNLI-m 76.50</cell><cell>77.79</cell><cell>+1.29</cell><cell>77.98</cell><cell></cell><cell cols="2">+1.48 67.71</cell><cell>67.27</cell><cell>-0.44</cell><cell>68.48</cell><cell>+0.77</cell></row><row><cell>SNLI</cell><cell>69.61</cell><cell>70.35</cell><cell>+0.74</cell><cell>71.17</cell><cell></cell><cell cols="2">+1.56 82.90</cell><cell>83.17</cell><cell>+0.27</cell><cell>83.66</cell><cell>+0.76</cell></row><row><cell>SICK</cell><cell>48.25</cell><cell>49.13</cell><cell>+0.88</cell><cell>50.15</cell><cell></cell><cell cols="2">+1.90 51.50</cell><cell>51.16</cell><cell>-0.34</cell><cell>51.42</cell><cell>-0.08</cell></row><row><cell>SciTail</cell><cell>73.65</cell><cell>75.42</cell><cell>+1.77</cell><cell>75.08</cell><cell></cell><cell cols="2">+1.43 69.35</cell><cell>70.74</cell><cell>+1.39</cell><cell>71.10</cell><cell>+1.75</cell></row><row><cell>QQP</cell><cell>71.37</cell><cell>72.24</cell><cell>+0.87</cell><cell>72.67</cell><cell></cell><cell cols="2">+1.30 70.60</cell><cell>71.52</cell><cell>+0.92</cell><cell>71.19</cell><cell>+0.59</cell></row><row><cell>Avg  *</cell><cell>67.88</cell><cell>68.99</cell><cell>+1.11</cell><cell>69.41</cell><cell></cell><cell cols="2">+1.53 64.99</cell><cell>65.34</cell><cell>+0.35</cell><cell>65.80</cell><cell>+0.81</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Probing</figDesc><table /><note>domain generalization. The models are trained on MNLI/SNLI and tested on out-of-domain data. ‚àÜ F and ‚àÜ D denotes the improvement of C.TUNING F and C.TUNING D compared with vanilla fine-tuning. Average scores (marked with * ) is computed excluding in-domain results (underlined). Positive transfer results are highlighted in blue. CHILD-TUNING can better maintain the out-of-domain generalization ability of the model.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Probing task generalization. The model is fine-tuned on MRPC task and transferred to four different tasks. CHILD-TUNING can maintain more generalizable representations compared with vanilla fine-tuning.we fine-tune BERT LARGE with different strategies on 5k subsampled MNLI and SNLI datasets respectively, and directly test the accuracy of the finetuned models on other NLI datasets in different domains, including MNLI, MNLI-mismatch 3 , SNLI, SICK, SciTail, and QQP 4 . As Table2illustrates, CHILD-TUNING outperforms vanilla fine-tuning across different out-of-domain datasets. Specifically, CHILD-TUNING</figDesc><table><row><cell>39.90</cell><cell>78.74</cell><cell>77.60</cell><cell>67.99</cell><cell>67.80</cell></row><row><cell>38.68</cell><cell></cell><cell>75.26</cell><cell></cell><cell></cell></row><row><cell></cell><cell>72.19</cell><cell></cell><cell></cell><cell></cell></row><row><cell>35.17</cell><cell>70.57</cell><cell>69.45</cell><cell>60.61</cell><cell></cell></row><row><cell>(a) CoLA (Matthews Corr)</cell><cell>(b) STS-B (Spearman Corr)</cell><cell>(c) QNLI (Accuracy)</cell><cell>(d) QQP (F1)</cell><cell></cell></row><row><cell>Figure 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>F improves 1.11/0.35 average score for models trained on MNLI/SNLI, while CHILD-TUNING D improves up to 1.53/0.81 average score. In particular, CHILD-TUNING D achieves 1.90 score improvement on SICK task and 1.56 on SNLI task for models trained on MNLI.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison between CHILD-TUNING with other fine-tuning methods. We report the mean (max) results of 10 random seeds. Results with ‚Ä† are taken from<ref type="bibr" target="#b36">Yang et al. (2019)</ref>, and others are from our implementation. The task-driven variant, CHILD-TUNING D , achieves the best performance compared with other methods. Integrating CHILD-TUNING D with other fine-tuning methods like R3F can yield further improvements. Note that since R3F is not applicable to regression task, the result on STS-B (marked with * ) is the same as CHILD-TUNING D .</figDesc><table><row><cell>Methods</cell><cell></cell><cell></cell><cell>CoLA</cell><cell>RTE</cell><cell>MRPC</cell><cell>STS-B</cell><cell>Avg</cell><cell>‚àÜ</cell></row><row><cell cols="2">Vanilla Fine-tuning  ‚Ä†</cell><cell></cell><cell cols="3">60.60 ( -) 70.40 ( -) 88.00 ( -)</cell><cell cols="2">90.00 ( -) 77.25</cell><cell>-</cell></row><row><cell cols="2">Vanilla Fine-tuning</cell><cell></cell><cell cols="5">63.13 (64.31) 70.18 (72.56) 90.77 (91.42) 89.61 (90.12) 78.42 0.00</cell></row><row><cell cols="3">Weight Decay (Daum√© III, 2007)</cell><cell cols="5">63.63 (64.56) 71.99 (74.37) 90.93 (91.70) 89.82 (90.29) 79.09 +0.67</cell></row><row><cell cols="8">Top-K Tuning (Houlsby et al., 2019) 62.63 (64.06) 70.90 (74.73) 91.09 (92.20) 89.97 (90.15) 78.65 +0.23</cell></row><row><cell cols="2">Mixout (Lee et al., 2020)</cell><cell></cell><cell cols="5">63.60 (64.82) 72.15 (75.45) 91.29 (91.85) 89.99 (90.13) 79.26 +0.84</cell></row><row><cell cols="3">RecAdam (Chen et al., 2020)</cell><cell cols="5">64.33 (65.33) 71.63 (73.29) 90.85 (92.01) 89.86 (90.42) 79.17 +0.75</cell></row><row><cell cols="3">R3F (Aghajanyan et al., 2021)</cell><cell cols="5">64.13 (66.32) 72.28 (74.73) 91.18 (91.57) 89.61 (90.12) 79.30 +0.88</cell></row><row><cell cols="2">CHILD-TUNING F</cell><cell></cell><cell cols="5">63.71 (66.06) 72.06 (74.73) 91.22 (91.85) 90.18 (90.92) 79.29 +0.87</cell></row><row><cell cols="2">CHILD-TUNING D</cell><cell></cell><cell cols="5">64.92 (66.03) 73.14 (76.17) 91.42 (92.17) 90.18 (90.64) 79.92 +1.50</cell></row><row><cell cols="2">CHILD-TUNING D + R3F</cell><cell></cell><cell cols="5">65.18 (66.03) 73.43 (76.17) 92.23 (92.65) 90.18 (90.64)  *  80.26 +1.84</cell></row><row><cell cols="4">Dataset Vanilla C.TUNING F C.TUNING D</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CoLA</cell><cell>47.48</cell><cell>48.44</cell><cell>50.37</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RTE</cell><cell>65.09</cell><cell>65.52</cell><cell>68.09</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MRPC</cell><cell>84.91</cell><cell>85.44</cell><cell>86.49</cell><cell></cell><cell></cell><cell></cell></row><row><cell>STS-B</cell><cell>81.86</cell><cell>82.25</cell><cell>82.76</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SST2</cell><cell>90.25</cell><cell>90.34</cell><cell>90.39</cell><cell></cell><cell></cell><cell></cell></row><row><cell>QNLI</cell><cell>81.68</cell><cell>83.09</cell><cell>83.42</cell><cell></cell><cell></cell><cell></cell></row><row><cell>QQP</cell><cell>71.30</cell><cell>72.15</cell><cell>71.79</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MNLI</cell><cell>55.72</cell><cell>62.47</cell><cell>62.93</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Avg</cell><cell>72.29</cell><cell>73.71</cell><cell>74.53</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results</figDesc><table /><note>in low-resource scenarios. CHILD-TUNING is better than vanilla fine-tuning in alleviating overfitting problems.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study of CHILD-TUNING D . Prune: Abandon parameters out of the child network. Random: Randomly choose a child network and keep it unchanged during fine-tuning. Lowest Info.: Detect a child network with lowest Fisher information instead.</figDesc><table><row><cell>Methods</cell><cell cols="3">CoLA RTE MRPC STS-B</cell></row><row><cell>Vanilla</cell><cell cols="2">63.13 70.18 90.77</cell><cell>89.61</cell></row><row><cell>Prune</cell><cell>0.00</cell><cell>51.12 81.40</cell><cell>45.63</cell></row><row><cell>Random</cell><cell cols="2">63.23 70.69 90.83</cell><cell>89.67</cell></row><row><cell cols="3">Lowest Info. 60.33 59.86 83.82</cell><cell>88.52</cell></row><row><cell cols="3">C.TUNING D 64.92 72.78 91.26</cell><cell>90.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Statistics and metrics of eight datasets used in this paper form GLUE benchmark.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Hyperparameters settings for different pretrained models on variant tasks. These settings are reported in the their official repository for best practice.</figDesc><table><row><cell>Robust Representations through Regularized</cell></row><row><cell>Fine-tuning (R3F) Aghajanyan et al. (2021)</cell></row><row><cell>propose R3F for fine-tuning based on trust region</cell></row><row><cell>theory, which adds noise into the sequence input</cell></row><row><cell>embedding and tries to minimize the symmetrical</cell></row><row><cell>KL divergence between probability distributions</cell></row><row><cell>given original input and noisy input. The loss func-</cell></row><row><cell>tion of R3F is as follows:</cell></row><row><cell>KL(y||x). We use both normal and unform</cell></row><row><cell>distribution, Œª R3F = 1, and grid search the œÉ</cell></row><row><cell>from {0.1, 0.5, 1.0, 5.0}. We use the implemen-</cell></row><row><cell>tation in https://github.com/pytorch/</cell></row><row><cell>fairseq/tree/master/examples/rxf.</cell></row><row><cell>D Label Mapping in Task Generalization</cell></row><row><cell>MNLI and SNLI datasets contain three labels, i.e.,</cell></row><row><cell>entailment, neutral, and contradiction. For SciTail,</cell></row><row><cell>it only has two labels, entailment and neutral, and</cell></row></table><note>L(w) = L CE (w) + Œª R3F KL S (f (x)||f (x + z)) s.t. z ‚àº N (0, œÉ 2 I) or z ‚àº U(‚àíœÉ, œÉ)where f (‚Ä¢) denotes the model and z denotes the noise sampled from either normal distribution or uniform distribution controlled by hyperparameter œÉ, and KL S (x||y) = KL(x||y) + therefore we map both neutral and contradiction in source label space to neutral in target label space followingMahabadi et al. (2021). For QQP, it has two labels, duplicate and not duplicate, and<ref type="bibr" target="#b9">Gong et al. (2018)</ref> interpret them as entailment and neutral respectively. We follow<ref type="bibr" target="#b9">Gong et al. (2018)</ref> and use the same mapping strategy as SciTail.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/huggingface/ transformers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Our code is available at https://github.com/ alibaba/AliceMind/tree/main/ChildTuning and https://github.com/PKUnlp-icler/ ChildTuning.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">MNLI-m has different domain from MNLI training data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">The target tasks may have different label spaces and we introduce the label mapping in Appendix D.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://gluebenchmark.com/faq</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://huggingface.co/ bert-large-cased/tree/main</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://huggingface.co/ xlnet-large-cased/tree/main</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">https://huggingface.co/roberta-large/ tree/main</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8">https://huggingface.co/google/ electra-large-discriminator/tree/main</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This paper is supported by the National Key Research and Development Program of China under Grant No. 2020AAA0106700, the National Science Foundation of China under Grant No.61936012 and 61876004.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A GLUE Benchmark Introduction</head><p>In this paper, we conduct experiments on 8 datasets in GLUE benchmark <ref type="bibr" target="#b34">(Wang et al., 2019)</ref> as shown in Table <ref type="table">6</ref>, including single-sentence tasks, inference tasks, and similarity and paraphrase tasks. Note that the original GLUE benchmark includes 9 different datasets in total. However, there are some issues with the construction of the WNLI dataset 5 . Therefore most studies exclude this dataset <ref type="bibr" target="#b6">(Devlin et al., 2019;</ref><ref type="bibr" target="#b7">Dodge et al., 2020)</ref> and we follow them. The metrics we report for each dataset are also illustrated in Table <ref type="table">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Settings for Different Pretrained Language Models</head><p>In this paper, we fine-tune different large pretrained language models with CHILD-TUNING, including BERT LARGE 6 , XLNet LARGE 7 , RoBERTa LARGE 8 ,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Settings for Other Fine-tuning Methods</head><p>We compare Child-tuning with several other regularization approaches in our paper. In this section, we simply introduce these approaches and their hyperparameters settings.</p><p>Weight Decay <ref type="bibr" target="#b5">Daum√© III (2007)</ref> proposes to adds a penalty item to the loss function to regulate the L 2 distance between fine-tuned models and the pretrained models. Therefore, the loss function is as follows:</p><p>We grid search the optimal Œª WD from 10, 1, 10 ‚àí1 , 10 ‚àí2 , 10 ‚àí3 , 10 ‚àí4 .</p><p>Top-K Fine-tuning Top-K Fine-tuning is a common method and Houlsby et al. ( <ref type="formula">2019</ref>) uses it as a strong baseline. Top-K Fine-tuning only updatess the top K layers along with the classification layer, while freezing all the other bottom layers.</p><p>We grid search the optimal K from {0, 3, 6, 12} in our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixout</head><p>Lee et al. ( <ref type="formula">2020</ref>) randomly replace the parameters with its pretrained weights with a certainly probability p during fine-tuning, which aims to minimize the deviation of the finetuned model towards the pretrained weights. In our paper, we grid search the optimal p from {0.1, 0.2, . . . , 0.8}. We use the implementation in https://github.com/bloodwass/ mixout. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RecAdam</head><p>We grid search the k from {0.05, 0.1, 0.2, 0.5, 1.0}, and t 0 from {50, 100, 250, 500}. We use the implementation in https://github.com/ Sanyuan-Chen/RecAdam.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Better fine-tuning by reducing representational collapse</title>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshat</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1075</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<editor>
			<persName><forename type="first">Sam</forename><surname>Mc-Candlish</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</editor>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recall and learn: Fine-tuning deep pretrained language models with less forgetting</title>
		<author>
			<persName><forename type="first">Sanyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutai</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangzhan</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.634</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Electra: Pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daum√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL)</title>
				<meeting>the 45th Annual Meeting of the Association of Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
				<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>NAACL-HLT</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06305</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sharpness-aware minimization for efficiently improving generalization</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Foret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Kleiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Natural language inference over interaction space</title>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Distance-based regularisation of deep networks for fine-tuning</title>
		<author>
			<persName><forename type="first">Henry</forename><surname>Gouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08253</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning with diff pruning</title>
		<author>
			<persName><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.378</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<meeting>the 59th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parameter-efficient transfer learning for NLP</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (ICML)</title>
				<meeting>the 36th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SMART: Robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization</title>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.197</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Parameterefficient multi-task fine-tuning for transformers via shared hypernetworks</title>
		<author>
			<persName><forename type="first">Rabeeh</forename><surname>Karimi Mahabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.47</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<meeting>the 59th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Mudigere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scitail: A textual entailment dataset from science question answering</title>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second Conference on Artificial Intelligence (AAAI)</title>
				<meeting>the Thirty-Second Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell</title>
				<imprint>
			<publisher>PNAS</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Agnieszka Grabska-Barwinska</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mixout: Effective regularization to finetune large-scale pretrained language models</title>
		<author>
			<persName><forename type="first">Cheolhyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanmo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanan</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic model pruning with feedback</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniil</forename><surname>Dmitriev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized BERT pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Variational information bottleneck for effective low-resource fine-tuning</title>
		<author>
			<persName><forename type="first">Rabeeh</forename><surname>Karimi Mahabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A SICK cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC)</title>
				<meeting>the Ninth International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the stability of fine-tuning BERT: Misconceptions, explanations, and strong baselines</title>
		<author>
			<persName><forename type="first">Marius</forename><surname>Mosbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksym</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietrich</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuliƒá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.617</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>F√©vry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01088</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Exploring the vulnerability of deep neural networks: A study of parameter corruption</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuancheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruixuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangyou</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05620</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reducing the model order of deep neural networks using information theory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Berisha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISVLSI.2016.117</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)</title>
				<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ranking the parameters of deep neural networks using the fisher information</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Berisha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Woolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2016.7472157</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><surname>Drame</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Quentin Lhoest, and Alexander Rush</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Revisiting fewsample BERT fine-tuning</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">To prune, or not to prune: Exploring the efficacy of pruning for model compression</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyog</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
