<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spot-adaptive Knowledge Distillation</title>
				<funder ref="#_pemDjQq">
					<orgName type="full">Science and technology project of SGCC (State Grid Corporation of China</orgName>
				</funder>
				<funder ref="#_7WSuBQd">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-05">5 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Jie</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ying</forename><surname>Chen</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jingwen</forename><surname>Ye</surname></persName>
							<email>yejing-wen@zju.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Mingli</forename><surname>Song</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gumbel</forename><surname>Softmax</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">the Zhejiang University</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<addrLine>Block 1 Block 2 Block 3 Block 4 Student logits Multi-spot distillation logits Block 1 Block 3 Block 1 Block 2 Block 4 logits</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>logits FC Block 1 Block 2 Block 3 Block 4</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<addrLine>Block 1 Block 2 Block 3 Block 4 FC logits</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spot-adaptive Knowledge Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-05">5 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2205.02399v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Knowledge distillation</term>
					<term>deep neural networks</term>
					<term>distillation spots</term>
					<term>spot-adaptive distillation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge distillation (KD) has become a well established paradigm for compressing deep neural networks. The typical way of conducting knowledge distillation is to train the student network under the supervision of the teacher network to harness the knowledge at one or multiple spots (i.e., layers) in the teacher network. The distillation spots, once specified, will not change for all the training samples, throughout the whole distillation process. In this work, we argue that distillation spots should be adaptive to training samples and distillation epochs. We thus propose a new distillation strategy, termed spotadaptive KD (SAKD), to adaptively determine the distillation spots in the teacher network per sample, at every training iteration during the whole distillation period. As SAKD actually focuses on "where to distill" instead of "what to distill" that is widely investigated by most existing works, it can be seamlessly integrated into existing distillation methods to further improve their performance. Extensive experiments with 10 state-of-theart distillers are conducted to demonstrate the effectiveness of SAKD for improving their distillation performance, under both homogeneous and heterogeneous distillation settings. Code is available at https://github.com/zju-vipa/spot-adaptive-pytorch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>With the rapid development of deep learning in the last decade, deep neural networks (DNNs) have become the predominant models in various fields, including computer vision, natural language processing (NLP), etc. However, for a long time till today, DNNs have always been criticized as being excessively large to be deployed to resource-limited edge devices. To make DNNs more applicable in these realworld scenarios, knowledge distillation (KD) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> has been proposed for crafting the lightweight substitutes for these expensive DNNs. The main idea is adopting a teacherstudent learning scheme, where the competitive lightweight substitutes, called students, are produced by mimicking some behaviors from well-behaved yet cumbersome DNNs which play the role of teachers. By harnessing the dark knowledge learned by teacher models, the lightweight student models are expected to achieve comparable performance, yet with much fewer parameters.</p><p>After recent years of development, KD has made remarkable progress and nowadays become a well established paradigm for compressing DNNs. Based on the number of distillation spots, existing distillation methods can be roughly categorized into two schools: one-spot distillation <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>, and multispot distillation <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b10">[11]</ref>, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. In one-spot distillation, KD happens at only one layer, typically the logit layer, in the teacher model. For example, Hinton et al. <ref type="bibr" target="#b0">[1]</ref> proposed to minimize the KL divergence between the probabilistic outputs of the teacher and the student networks. Contrastive representation distillation (CRD) <ref type="bibr" target="#b1">[2]</ref> adopts a contrastive learning approach to distill structural knowledge, i.e., inter-dependencies between different output dimensions of the representation layer. Relational knowledge distillation (RKD) <ref type="bibr" target="#b2">[3]</ref> transfers mutual relations of data examples from teacher models to student models, where the mutual relations are produced within a single representation layer. As DNNs are typically composed of dozens of layers, researchers have also investigated how to leverage more layers to boost the performance of KD. Multi-spot distillation provides student models with more supervision signals by mining knowledge from multiple layers in the teacher network. For example, Fit-Nets <ref type="bibr" target="#b5">[6]</ref> adopts not only the outputs but also the intermediate representations learned by the teacher as hints for training the student. Attention transfer <ref type="bibr" target="#b6">[7]</ref> improves the performance of a student CNN network by forcing it to mimic the attention maps at different layers of a powerful teacher network. Activation boundary (AB) <ref type="bibr" target="#b9">[10]</ref> transfers the knowledge in teacher models via distilling activation boundaries formed by hidden neurons at different layers. As multi-spot distillation methods utilize more information from teacher models than one-spot counterparts, they are usually deemed to exhibit superior performance for training student models. Existing distillation methods, whether they are one-spot or multi-spot, share one common characteristic: the distillation spot is usually a manual design choice which is inefficient to optimize for, especially for networks with hundreds or thousands of layers. On the one hand, if the distillation spots are set too sparsely, the student model is not sufficiently supervised by the teacher. On the other hand, if the distillation spots are set too densely, e.g., every possible layer or neuron, the student model learning can be over-regularized, which also deteriorates distillation performance. Furthermore, current methods employ a global distillation strategy, i.e., the distillation spots are fixed for all the samples once specified. The underlying assumption is that such distillation spots are optimal for the entire data distribution, which is not true in many cases. Ideally, we would like the distillation spots to be determined automatically for each possible spot, per sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spot-adaptive distillation</head><p>In this work, we propose a new distillation strategy, termed spot-adaptive KD (SAKD), to make the distillation spots adaptive to training samples and distillation stages. To this end, we first merge the student model and the teacher model into a multi-path routing network, as shown in Fig. <ref type="figure">2</ref>. When the data flow through the network, there are many feasible paths for the data to reach the output layer. A lightweight policy network is devised to make decisions per sample on the optimal propagation path when data reach the branch spots in the network. If the data are routed to layers of the teacher model by the policy network, it indicates that the counterpart layers in the student model (abbreviated to student layers) can not yet replace the layers in the teacher model (abbreviated to teacher layers). The knowledge in these teacher layers thus should be distilled into corresponding student layers. Otherwise, if the data are routed to some student layers by the policy network, it indicates that these student layers are good substitutes for corresponding teacher layers, yielding superior or at least comparable performance. In this case, the distillation is not allowed in these layers. As the policy network is devised on top of the routing network and is optimized simultaneously with the routing network, it can automatically determine the optimal distillation spots per sample, at different training iterations of the student model.</p><p>The proposed method focuses on "where to distill", which is vastly different from and orthogonal to current literature where "what to distill" is mainly investigated. It thus can be seamlessly combined with existing methods to further enhance their distillation performance. Specifically, the proposed method is naturally compatible with homogeneous distillation where the student model is in the same-style architecture as the teacher model. However, experiments demonstrate that the proposed method also works surprisingly well under heterogeneous distillation settings where the student model differs largely from that of the teacher model. Moreover, although the proposed method is designed primarily for multi-spot distillation, it can also boost the performance of one-spot distillation by dynamically determining distillation or not for each training sample.</p><p>In a nutshell, we made following three main contributions in this work:</p><p>? To our best knowledge, we are the first to introduce the adaptive distillation problem where the distillation spots should be adaptive to different training samples and varying distillation stages.</p><p>? We propose a novel spot-adaptive distillation strategy to automatically determine the distillation spots, making the distillation spots adaptive to the training samples and distillation stages.</p><p>? Extensive experiments under various experimental settings are conducted to showcase the effectiveness of the proposed method for improving distillation performance of existing state-of-the-art distillers. The reminder of this work is organized as follows. Section II presents an overview of related works on knowledge distillation and routing networks. Section III describes the proposed spot-adaptive distillation in detail. The experimental setups and results are provided in Section IV.The conclusions are drawn in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Knowledge Distillation</head><p>Knowledge distillation has attracted widespread attention due to its importance in deploying DNNs to low-capacity edge devices. The main idea is leveraging the dark knowledge in a bulky teacher to craft a compact student model, of which the performance is expected to be on par with the teacher. Over the last several years, most works devote themselves to the exploration of "what to distill", i.e., forms of knowledge for distillation. Representative forms include soft targets <ref type="bibr" target="#b0">[1]</ref>, features <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>, attention <ref type="bibr" target="#b6">[7]</ref>, factors <ref type="bibr" target="#b8">[9]</ref>, activation boundary <ref type="bibr" target="#b9">[10]</ref>, sample relationship <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> and so on. By imitating the teacher to behave in a similar way, the student model achieves comparable performance to the teacher model even with much fewer parameters. In this work, we focus on "where to distill" instead of "what to distill", which is largely overlooked by existing works. Moreover, as "where to distill" is orthogonal to "what to distill", the proposed method can be seamlessly integrated into existing distillation methods to further improve their performance.</p><p>Existing KD methods can be divided into different categories according to different taxonomies. Based on the number of spots where distillation happens, we roughly divide existing KD methods into one-spot <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref> and multi-spot <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b10">[11]</ref> distillation. The proposed distiller is a spot-adaptive approach where the number of spots for distillation is automatically determined rather than pre-defined. Based on the similarity between the teacher and the student in architectures, KD can also be categorized into homogeneous distillation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b7">[8]</ref> and heterogeneous distillation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, where the heterogeneous distillation is deemed more challenging due to the large architecture difference <ref type="bibr" target="#b1">[2]</ref>. The proposed method in this work makes few assumptions about the architectures of the teacher and the student, and thus it can be applied to both homogeneous and heterogeneous distillation scenarios. KD has derived a variety of different problem settings except the canonical teacher-student distillation, including mutual distillation <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, self-distillation <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>, knowledge amalgamation <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> and data-free distillation <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b26">[27]</ref>. In mutual distillation, an ensemble of student models learn collaboratively and teach each other throughout the training process, enabling the teacher and the student models progress Fig. <ref type="figure">2</ref>. An illustrative diagram of the proposed method. The upper is the policy network and the bottom is the routing network. In this example, there are six possible distillation spots. The input of the policy network is the features from the teacher and the student models. The output of the policy network determines the spots for distillation. The teacher is fixed during distillation. together. Self distillation assumes no teacher network available and distills knowledge within the student network itself by using outputs of deeper layers to supervise shallower layers. Knowledge amalgamation aims to craft a single multi-task student model by fusing the knowledge from multiple teacher models. Data-free distillation relaxes the assumption that the training data of the teacher model is available for training the student, i.e., the knowledge should be distilled to the student without any original training data. In this work, we follow the canonical distillation settings where both the teacher network and the original training data are available for training the student network. However, we believe the general idea can be also applied to various KD settings, which is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Routing Networks</head><p>Routing networks <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b29">[30]</ref> are a type of neural networks with high modularity, which is a key property required for encouraging task decomposition, reducing model complexity, and improving model generalization. A routing network usually consists of two trainable components: a set of function modules and a policy agent. In neural network settings, the function modules are implemented by sub-networks and used as candidate modules for processing the input data. For each sample, the policy agent selects a subset of function modules from these candidates, assembles them into a complete model, and applies the assembled model to the input data for task predictions. Several algorithms have been proposed for optimizing the policy module, including genetic algorithm <ref type="bibr" target="#b30">[31]</ref>, multi-agent reinforcement learning <ref type="bibr" target="#b27">[28]</ref>, reparameterization strategies <ref type="bibr" target="#b28">[29]</ref>, etc.</p><p>Routing networks are closely related to conditional computation <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, mixture of experts <ref type="bibr" target="#b33">[34]</ref>, as well as their modern attention based <ref type="bibr" target="#b34">[35]</ref> and sparse <ref type="bibr" target="#b35">[36]</ref> variants. They have been successfully applied to several fields like multitask learning <ref type="bibr" target="#b27">[28]</ref>, transfer learning <ref type="bibr" target="#b36">[37]</ref> and language modeling <ref type="bibr" target="#b29">[30]</ref>. In this work, with the aid of a routing network, we propose a novel distillation strategy to automatically determine the spots of distillation in the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SPOT-ADAPTIVE KNOWLEDGE DISTILLATION</head><p>In this section, we introduce the proposed spot-adaptive KD. The overview of the proposed method is shown in Fig. <ref type="figure">2</ref>. The whole model consists of two main components: a multi-path routing network and a lightweight policy network. The multipath routing network is composed by the teacher model and the student model, with the adaption layers to adapt their features to each other if necessary. The policy network is used to make routing decisions per sample on the data flow path when the data reach the branch spots in the routing network.</p><p>The general idea of the proposed distillation method is to automatically determine whether or not to conduct the distillation at the candidate distillation spots, as shown in Fig. <ref type="figure">2</ref>. If the samples are routed to certain teacher layers by the policy network, it indicates that the counterpart student layers can not yet replace these teacher layers. The knowledge in these teacher layers thus should be distilled into corresponding student layers. Otherwise, if the data are transmitted to some student layers by the policy network, it indicates that these student layers are good substitutes for corresponding teacher layers, yielding superior or at least comparable performance. No distillation is needed any more in this case. The ultimate goal of distillation is to make the policy network gradually choose the student layers for routing the data, which implies the student model is a good substitute for the teacher network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Multi-path Routing Network</head><p>Without loss of generality, assume a canonical Convolution Neural Network (CNN) for visual classification is composed of several convolution blocks for representation learning, a fully connected layer for vectorizing the feature maps, and a softmax layer for making probability predictions. Each convolution block consists of several convolution layers, each followed by a non-linear activation layer and a batch normalization <ref type="bibr" target="#b37">[38]</ref> layer 1 . Generally speaking, after each block, the feature maps are downsized by factor of 2 or more with a pooling layer or a convolution layer. Formally, we denote the function underlying a teacher CNN by</p><formula xml:id="formula_0">T = S ? F t ? B t N ? ? ? ? ? B t 1 ,<label>(1)</label></formula><p>and the function underlying a student CNN by</p><formula xml:id="formula_1">S = S ? F s ? B s N ? ? ? ? ? B s 1 ,<label>(2)</label></formula><p>where S denotes the softmax function, and F the linear function. B i denotes the function underlying the i-th block.</p><p>The superscript s and t denote the student and the teacher models, respectively. The symbol ? denotes the function composition operation. The multi-path routing network is composed by the teacher and the student networks, with their intermediate layers interconnected with each other. However, there may exist dimension mismatch between the teacher and the student features. We adopt adaption layers <ref type="bibr" target="#b5">[6]</ref>, which are implemented by 1 ? 1 convolution layers, to align their features. The underlying function of the multi-path routing network is denoted by</p><formula xml:id="formula_2">M = S ? F ? BN ? ? ? ? ? B1 ,<label>(3)</label></formula><formula xml:id="formula_3">F = wF t + (1 -w)F s , Bi = w i B t i + (1 -w i )B s i , 1 ? i ? N,<label>(4)</label></formula><p>where w and w i are the feature fusing weights produced by the policy network, bounded by [0, 1]. When the feature fusing weights take discrete values from {0, 1}, the network actually becomes a combinatorial network whose layers are composed of intertwined teacher and student layers. Note that in Eqn. ( <ref type="formula" target="#formula_3">4</ref>), for simplicity we omit the adaption layers that are used for feature alignment. With the routing network M, our ultimate goal is to obtain an isolated student model S which performs on the task in interest as better as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Policy Network</head><p>We adopt a policy network to make decisions per sample on the data flow path through the routing network. Here we simply adopt a lightweight fully connected layer to implement the policy network. The input of the policy network is the concatenated features from the teacher and the student models. The outputs of the policy network are N + 1 twodimensional routing vectors, where N + 1 denotes the number of branch points, i.e., the number of candidate distillation spots. Each routing vector is a probability distribution, from which we stochastically draw a categorical value to make the 1 Different architectures may have different network configurations.</p><p>decision on the data flow path for one branch point in the routing network. The sampling operation is not differentiable due to the discretization. To enable the differentiability for the sampling operation, we utilize Gumbel-Softmax <ref type="bibr" target="#b38">[39]</ref> to implement the policy network. Formally, assume for the ith branch point in the routing network, the corresponding routing vector is a i j = {a i 1 , a i 2 }, where element a i 1 stores the probability value that denotes how likely the teacher layers in the i-th block would be used to process the incoming data. During the forward propagation, the policy makes a discrete decision drawn from the categorical distribution based on the distribution:</p><formula xml:id="formula_4">w i = one hot{arg max k (log a i k ) + k }.<label>(5)</label></formula><p>Here w i is a two-dimensional one-hot vector. "one hot" is the function returning a one-hot vector where only the specified element is 1 and all the others are 0. ? R 2 is a vector in which the elements are i.i.d samples drawn from the Gumbel distribution (0, 1) to add a small amount of noise to avoid the argmax operation always selecting the element with the highest probability value.</p><p>To enable differentiability of the discrete sampling function, we use the Gumbel-Softmax trick to relax w i during backward propagation as</p><formula xml:id="formula_5">w i = exp((log a i + )/? ) k exp((log a i k + k )/? ) ,<label>(6)</label></formula><p>where ? is the temperature that controls how sharp the distribution is after the approximation. Note that for each vector w i , as w i 1 + w i 2 = 1, we simply use w i and 1 -w i to denote the routing decision on the teacher and the student layers, as shown in Eqn. (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Spot-adaptive Distillation</head><p>The proposed spot-adaptive distillation is conducted by training the routing network and the policy network simultaneously. Training the proposed network is non-stationary from the perspective of both the policy network, and the routing network, because the optimal routing strategy depends on the module parameters and vice versa. In this work, the multi-path routing network and the policy network are trained simultaneously in an end-to-end manner. The overall objective is</p><formula xml:id="formula_6">L = L student + ? 1 L KL + ? 2 L KD + ? 3 L routing , (7)</formula><p>where L student is the widely-used cross-entropy loss <ref type="bibr" target="#b39">[40]</ref> between the targets and the predictions from only the student model. L KL is the Kullback-Leibler divergence <ref type="bibr" target="#b40">[41]</ref> between the teacher and the student predictions, which is also the vanilla distillation loss proposed by Hinton et al. <ref type="bibr" target="#b0">[1]</ref>. L KD is existing knowledge distillation loss imposed on the intermediate layers, such as the losses proposed by FitNets <ref type="bibr" target="#b5">[6]</ref>, Attention Transfer <ref type="bibr" target="#b6">[7]</ref>, etc. As the proposed method focuses on "where to distill" instead of "what to distill" that is mainly investigated by current literature, it can be combined with most existing distillation methods. L student , L KL and L KD are used to make the student model perform similarly with the teacher model. L routing is the cross entropy loss between the targets and the predictions from routing network. ? 1 , ? 2 and ? 3 are three hyper-parameters for trading off these loss terms.</p><p>Note that during the whole training phase, the pre-trained parameters of the teacher model are kept fixed. The trainable parameters include only the parameters of the student models, the adaption layers and the policy netowrk. As the policy network and the adaption layers are involved in computing only L routing , their parameters are trained under the supervision of only L routing . The student network and the policy network forms a loop where the output of the student model goes into the policy network, and the output of the policy network goes into the student network again. To stabilize the training of the student network, we do not back-propagate the gradients from policy network to the student network anymore. At the early stage of the training process, as the teacher model is well pre-trained, samples are more likely to be passed to the teacher layers by the policy network. In this case, knowledge distillation happens at all candidate distillation spots. As the training proceeds, the student model gradually master the knowledge of the teacher to different degrees at different layers. In this situation, the policy network may plan a path per sample where both teacher layers and student layers are intertwined. Knowledge distillation is thus conducted adaptively at certain layers to push the optimal policy to involve only student layers. In the following section, we provide detailed optimization algorithm to more clearly describe the proposed distillation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Optimization algorithm</head><p>To make the proposed method clearer to readers, the pseudocode of SAKD is provided in Algorithm 1. Given two deep neural networks, a student S and a teacher T . Let x be the network input; we denote the lists of the intermediate representations from teacher and student models as f eat T and f eat S , the final predictions as logit T and logit S , respectively. The input of the policy network P is the concatenated features from the teacher and the student models, as seen in Line #20. The outputs of the policy model P are N + 1 two-dimensional routing vectors, donates as w (Line #21 ? 22), which are discrete decisions during the forward propagation and will be relaxed with Gumbel-softmax <ref type="bibr" target="#b38">[39]</ref> during backward propagation. An apparent difficulty here is that the distillation loss L s for the student depend on the routing decision w, so it is problematic to optimize the student model together with the policy network. We circumvent this difficulty by stop-gradient (Line #23) operation. This means that d is treated as a constant in the loss terms. The complete objective for the student model is shown in Line #26 ? 28, which includes the cross-entropy loss, Kullback-Leibler divergence, and knowledge distillation loss.</p><p>After that, we begin the forward propagation of the multipath routing network, which is composed of the teacher and the student networks, with their intermediate layers interconnected with each other. We set the student to eval mode in advance in order to make the multi-path routing network work more steadily, and set the student back to train mode  To align features between the teacher and the student, We adopt adaption layers (i.e., H st and H ts ), as shown in Line #37 ? 38. The cross-entropy loss is used for optimizing the parameters of the policy module and adaption layers.</p><formula xml:id="formula_7">Ls += ? 1 ? L KL (logit S , logit T ) ? d[:, -1] 28: Ls += ? 2 ? N i L KD (f eat S [i], f eat T [i]) ? d[:, i]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>Here we first briefly describe the main experimental settings which are necessary to understand our experiments in this work. Then the benchmark comparisons are provided to demonstrate the superiority of the proposed method. Finally we conduct extensive ablation study to illustrate the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Settings</head><p>We compare the distillation performance with the top-1 and top-5 accuracy at the last epoch. Experiments are randomly repeated three times and the average results are provided.</p><p>1) Datasets: Experiments are conducted on CIFAR-100 <ref type="bibr" target="#b41">[42]</ref>, tiny-ImageNet <ref type="bibr" target="#b42">[43]</ref> and ImageNet <ref type="bibr" target="#b43">[44]</ref>. CIFAR-100 consists of 60,000 32?32 colour images in 100 classes, with 600 images per class. There are 50,000 training images and 10,000 test images in the official split. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. Tiny-ImageNet is a subset of ImageNet with 200 classes, where each image is down-sized to 64 ? 64 pixels. Each class has 500 training images, 50 validation images, and 50 test images. For data augmentation, all images are normalized to zero mean and unit variance at each channel, we normalize each channel of all images to zero mean and unit variance for pre-processing. Images in CIFAR-100 are zero-padded with 4 pixels on every side, randomly cropped to 32?32 pixels, then randomly horizontal flipped; and for tiny-ImageNet, we just employ random horizontal flipping. ImageNet consists of 1.2M training images and 50k validation images with 1,000 classes. Following the common practice, images in ImageNet are augmented with random cropping of size 224?224 and horizontal flipping.</p><p>2) Model Architectures: To validate the effectiveness of the proposed method, we adopt a variety of backbone architectures to implement the proposed model, including ResNet <ref type="bibr" target="#b44">[45]</ref>, WideResNet (WRN) <ref type="bibr" target="#b45">[46]</ref>, Vgg <ref type="bibr" target="#b46">[47]</ref>, ShuffleNet V1 <ref type="bibr" target="#b47">[48]</ref>, MobieNet V2 <ref type="bibr" target="#b48">[49]</ref>. In this work, we adopt C-ResNet to denote the cifar-style ResNet with 3 groups of basic blocks, each with 16, 32, and 64 channels, respectively. and I-ResNet represents ImageNet-style ResNet with bottleneck blocks and more channels. More detailed configurations about the two types of networks can be found in <ref type="bibr" target="#b1">[2]</ref>. WRN-d-w represents a WideResNet with depth d and width factor w.</p><p>3) Distillation Methods: The proposed adaptive distillation method can be combined with most existing distillation methods. We combine the proposed method with ten state-ofthe-art distillation methods to demonstrate the superiority of the proposed method, including FitNets <ref type="bibr" target="#b5">[6]</ref>, AT <ref type="bibr" target="#b6">[7]</ref>, SP <ref type="bibr" target="#b13">[14]</ref>, CC <ref type="bibr" target="#b49">[50]</ref>, VID <ref type="bibr" target="#b50">[51]</ref>, RKD <ref type="bibr" target="#b2">[3]</ref>, PKT <ref type="bibr" target="#b51">[52]</ref>, FT <ref type="bibr" target="#b8">[9]</ref>, NST <ref type="bibr" target="#b52">[53]</ref>, CRD <ref type="bibr" target="#b1">[2]</ref>. For all methods, we combine their objectives with the vanilla KD <ref type="bibr" target="#b0">[1]</ref> objective, the KL divergence between the softened predictions from teacher and the student models, to boost their performance. Thus all the methods involve at least two distillation spots, turning into a multi-spot version no matter they are originally one-spot or multi-spot distillation methods.</p><p>4) Distillation Spots: In this work, we combine the proposed spot-adaptive KD (SAKD) with ten state-of-the-art knowledge distillation methods, including FitNets <ref type="bibr" target="#b5">[6]</ref>, AT <ref type="bibr" target="#b6">[7]</ref>, SP <ref type="bibr" target="#b13">[14]</ref>, CC <ref type="bibr" target="#b49">[50]</ref>, VID <ref type="bibr" target="#b50">[51]</ref>, RKD <ref type="bibr" target="#b2">[3]</ref>, PKT <ref type="bibr" target="#b51">[52]</ref>, FT <ref type="bibr" target="#b8">[9]</ref>, NST <ref type="bibr" target="#b52">[53]</ref>, CRD <ref type="bibr" target="#b1">[2]</ref>. Additionally, we bond these objectives  with the vanilla KD <ref type="bibr" target="#b0">[1]</ref> objective. The above distillation methods determine different distillation spots before training and remain the same throughout the whole distillation process. We assume that the network consists of N blocks and one linear function. If one method adopts the output from Block i (1 ? i ? N + 1) learned by the teacher as knowledge for training the student, its distillation spot is referred to as i. The detailed experimental setups about distillation spots are shown in Table <ref type="table">I</ref>.</p><p>5) Implementation Details: For fair comparisons, we keep the implementation settings of all the experiments the same. Specifically, following CRD <ref type="bibr" target="#b1">[2]</ref>, we set the batch size to 64, the number of total training epochs to 240. Most experiments employ SGD as the optimizer for training the student model. The initial learning rate is set to 0.05 for most backbone architectures, except MobineNet V2 and ShuffleNet V1 where the initial learning rate is set to 0.01. Weight decay is set to be 0.0005. The learning rate decays by a factor of 0.1 at the 150-th, the 180-th, and the 210-th epochs. The temperature value for softening the predicted distributions is set to 4. ? in Gumbel-Softmax is initially set to 5 and decays gradually during training so the network can explore freely in the early stage and exploit the converged distillation policy in the later stage. For simplicity, the hyper-parameters ? 1 and ? 3 in Eqn. 7 are set to 1. ? 2 is set based on the distillation methods. We adopt corresponding hyper-parameters in CRD <ref type="bibr" target="#b1">[2]</ref> to set ? 2 for most distillation methods, except FitNets where ? 2 is set to 1 instead of 1, 000 for more stable training. Detailed settings of ? 2 are shown in Table <ref type="table">II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Benchmark Comparisons 1) Homogeneous Distillation:</head><p>In this section, we evaluate the proposed method under homogeneous distillation. To this end, we adopt various teacher-student combinations, where the teacher models and the student models are in the samestyle architectures, including C-   layer for L KL , and some intermediate layers for L KD . Note that all competitors are combined with KD <ref type="bibr" target="#b0">[1]</ref> to enhance their performance in the standard distillation scheme, and thus the softmax layer is always a candidate distillation spot in the proposed adaptive scheme. However, for intermediate layers, different distillers conduct knowledge distillation at different number of layers (one-spot distillation or multi-spot distillation). The proposed adaptive distillation strategy only determines where or not to conduct distillation at these distillation spots. It does not add any other candidate distillation spots to the standard distillation methods.</p><formula xml:id="formula_8">ResNet56 ? C-ResNet20, C- ResNet110 ? C-ResNet32, C-ResNet32x4 ? C-ResNet8x4, Vgg13 ? Vgg8,</formula><p>Experimental results on CIFAR-100 are shown in Table <ref type="table" target="#tab_3">III</ref>, and results on tiny-ImageNet are shown in Table <ref type="table" target="#tab_6">V</ref>. From these results, it can be seen that the proposed adaptive distillation strategy consistently outperforms the standard distillation strategy with nearly all distillers, under all teacher-student architectures, and on both CIFAR-100 and tiny-ImageNet datasets. For example, when combined with PKT <ref type="bibr" target="#b51">[52]</ref> for distilling knowledge from C-ResNet56 to C-ResNet20, the proposed adaptive distillation strategy improves the accuracy by 0.84% (from 70.72% to 71.56%) and 0.21% (from 54.29% to 54.50%), respectively. The standard distillation strategy imposes distillation constraints on all distillation spots and throughout the whole distillation process, which leads to overregularization on the student and thus the performance degradation. Similar results are also observed on other distillation methods and student-teacher combinations. The highly consistent results demonstrate the effectiveness of the proposed method for improving performance of existing distillation methods.</p><p>2) Heterogeneous Distillation: The proposed adaptive distillation strategy is naturally compatible with homogeneous distillation. However, it seems not so compatible with heterogeneous distillation. In this section, we evaluate the proposed method under heterogeneous distillation settings. On CIFAR-100, the adopted teacher-student combinations include Vgg13 ? MobileNetV2, I-ResNet50 ? MobileNetV2, I-ResNet50 ? Vgg8, C-ResNet32?4 ? C-ResNet32 and WRN 40 2 ? ShuffleNetV1. On tiny-ImageNet, we provide results of only Vgg13 ? MobileNetV2 for space consideration, as other teacher-student combinations produce similar results. Similar to homogeneous distillation, the candidate distillation spots include the softmax layer for L KL , and intermediate layers for L KD . The softmax layer is always a candidate distillation spot in the proposed adaptive scheme.</p><p>Experimental results on CIFAR-100 are provided in Ta-   ble IV, and the results on tiny-ImageNet are provided in Table V (Vgg13 ? MobileNetV2). The results show that the proposed adaptive distillation strategy also consistently improves performance of existing distillation methods under the heterogeneous distillation settings. For example, when combined with VID <ref type="bibr" target="#b50">[51]</ref> for distilling knowledge from Vgg13 to MobileNetV2, the proposed adaptive distillation strategy improves the accuracy by 0.60% (from 72.10% to 72.70%) and 0.39% (from 60.84% to 61.23%), respectively. The highly consistent results again demonstrate the effectiveness and universality of the proposed spot-adaptive distillation strategy across various experimental settings.</p><formula xml:id="formula_9">C-ResNet56?C-ResNet20 C-ResNet110?C-ResNet20 Vgg13 ? Vgg8 WRN<label>40</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Results on ImageNet:</head><p>To validate the scalability of the proposed method on large-scale datasets, here we evaluate the proposed method on ImageNet. For a fair comparison with existing methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b6">[7]</ref> that have been tested on ImageNet, here we follow their experimental settings, adpoting ResNet-34 as the teacher and ResNet-18 as the student. Experimental results are provided in Table VI. It can be seen that the proposed adaptive distillation strategy also consistently improves both top@1 and top@5 accuracy of existing distillers on ImageNet. For examples, with the proposed method, CC <ref type="bibr" target="#b49">[50]</ref> improves its top@1 accuracy from 69.96% to 70.72%, and top@5 from 89.17% to 90.28%. These results verify the scalability of the proposed method to larger datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>We perform ablation study in this section to demonstrate the effectiveness of the proposed method. Specifically, we answer the following questions with carefully designed experiments. 1) Does the policy network make useful decisions?</p><p>Here we validate usefulness of the decision made by the policy network. To this end, we introduce four baseline distillation strategies: always-distillation, rand-distillation, antidistillation and no-distillation. Always-distillation is actually the standard distillation strategy where distillation is always conducted at every distillation spot. Rand-distillation, as its name implies, randomly decides on whether or not to make the distillation at candidate distillation spots. Anti-distillation adopts an opposite distillation strategy to the proposed adaptive distillation: if adaptive distillation distills at a certain spot, anti-distillation does not distill; otherwise, it distills at this spot. No-distillation means that the student is trivially trained without any distillation. Experiments are conducted on CIFAR-100 and tiny-ImageNet with different KD methods and network pairs under both homogeneous and heterogeneous distillation. The results are provided in Table <ref type="table" target="#tab_6">VII</ref>. It can be seen that the proposed adaptive distillation consistently outperforms other baselines, including the competitive always-distillation. Although the improvement is sometimes marginal for some distillation methods, the consistent improvement with almost all the distillation verifies that the proposed policy network indeed makes useful routing decisions for distillation. Furthermore, anti-distillation usually produces much worse performance than adaptive-, always-and randdistillation, sometimes even worse than no-distillation (e.g., WRN 40 2?ShuffleNetV1 on CIFAR100). These results indicate that conducting distillation at inappropriate spots can be harmful for training the student.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) How does the decision change at different spots?</head><p>Here we study how the distillation decisions made by the policy network change at different distillation spots and distillation stages (i.e., training epochs). At each candidate distillation spot, the probability of distillation is the ratio of the number of samples that are distilled at this spot to the total number of training samples. The probability curves at  different spots along the training epochs are depicted in Fig. <ref type="figure">3</ref>. At early stages, as the teacher network is well trained, the optimal routing decision should be choosing the teacher layers at all branch points in the routing network. Thus the distillation probability should be nearly 100% at all distillation spots. However, as the policy network is randomly initialized and has not been well trained yet, it makes nearly random decisions on routing and thus the distillation probability is low. As the training proceeds, the policy network gradually learns how to make the right decisions and finds that the teacher layers tend to be better, thus the distillation probability increases rapidly. After a period of distillation, the student model master the knowledge of the teacher. Some samples become less useful  for training the student model, and thus the distillation probability decreases (e.g., KD 1). Generally speaking, shallow layers are more sensitive to adaptive distillation. Deep layers, on the other hand, requires distillation nearly all the time and for almost all the samples, as shown by curves of KD 4 and KL. The reasons for this phenomenon may be that the features from shallow layers are relatively noisy for distillation. As the capacity of the student model is much smaller than the teacher, learning from these noisy features degrades its performance on the final target task.</p><formula xml:id="formula_10">PKT [52] FitNet [6] Teacher C-ResNet56 Vgg13 C-ResNet56 Vgg13 Student C-ResNet20 MobileNetV2 C-ResNet20</formula><p>3) Trainable vs frozen teacher networks, which is better?</p><p>The teacher network is frozen all the time in the proposed method. Here we relax this constraint and introduce two alternative settings: (1) the teacher network is randomly initialized and co-trained with the student network; (2) the teacher network is initialized with the pre-trained parameters and cotrained with the student network. The trainable teacher network improves the capacity of the multi-path routing network, but may impair the training of the student model which will be deployed in isolation. Experimental results are provided in Table <ref type="table" target="#tab_10">VIII</ref>. It can be seen that training the teacher network, whether from scratch or from pre-trained parameters, degrades the distillation performance, which verifies our assumption. What is worse is that training the teacher network slows down the distillation process, as updating teacher parameters needs more computation. 4) Sensitivity analysis of ? 3 and ? .</p><p>The proposed method involves several hyperparameters. However, most of them are introduced in previous works. We follow the common settings in the literature for these hyperparameters. This work introduces two new hyperparameters, including ? in Eqn. 6 and ? 3 in Eqn. 7. Here we make sensitivity analysis to discuss their impact on the final performance. The results are listed in Fig. <ref type="figure">4</ref>. It can be seen that both ? 3 and ? affect the final performance to some degree. However, they make the proposed spot-adaptive method achieve superior results in a wide range of values to standard distillation methods. This feature makes the proposed method very attractive as we do not have make great efforts to tune the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Visualization of the distillation decision.</head><p>To better understand the distillation decisions made by the policy network, we provide visualization of distillation decisions on ten categories of tiny-ImageNet <ref type="bibr" target="#b42">[43]</ref> in Fig. <ref type="figure" target="#fig_4">5</ref>. It can be seen that most images that will be distilled ( ) are usually of better quality than those not (?). We summarize the samples without knowledge distillation into four categories: missing content, ambiguous subjects, group of objects and unusual morphology, which are represented by red, yellow, purple and green frames in the figure, respectively.</p><p>Missing content (red). Some images of this type, due to their extreme close-ups or uncharacteristic views, capture only parts of the object, such as the tail of a goldfish, the body of a penguin and the body of a hog. In other images with missing content, the objects are indistinguishable from the background, e.g., bullfrog, jellyfish, penguin, chihuahua and ox.</p><p>Ambiguous subjects (yellow). These images contain multiple objects without identifying which object is the focus of the image, e.g., bullfrog and human, koala and human leg, lobster and human, penguin and human, chihuahua and human, chihuahua and bag, guinea pig and human, etc. With these input images, it is easy for a model to learn features that do not fall into the target category, and eventually leads to errors.</p><p>The group of objects (purple). The close-up of an individual object reveals its characteristics in detail, whereas the group of samples only provide overall features, e.g., the group of goldfish and the group of penguins.</p><p>Unusual morphology (green). Some of the images are different from most of those in the dataset, which will not be distilled because of their extraordinary. The rarity of these images makes the features they provide incompatible with the general features. For example, we can see the lobster in blue, the furry penguin and the pink-haired hog in this figure, which provide conflicting features with red lobsters, molted penguins and pigs without head hair that are common in the dataset.</p><p>These low-quality features can produce noisy features or predictions, which may hurt the learning of the student model due to its limited capacity. We admit that these non-distilled images can provide information for the model from another perspective, but the noise introduced by them is also worth considering. Generally, the distillation decision shown in this figure is reasonable as discriminating images offer informative features, and thus the knowledge from these images will guide the student well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this work, we argue knowledge distillation strategy should not be fixed throughout the training process and for all training samples. We thus propose a spot-adaptive distillation strategy to automatically decide on the distillation spots per sample at different distillation stages. The proposed distillation strategy first merges the teacher network and the student network into a multi-path routing network. Then a policy network is introduced to make decisions per sample on the data flow path through the network. The distillation decisions are made based on the routing decisions from the policy network. We evaluate the proposed method under both homogeneous and heterogeneous distillation settings. And extensive ablation study is also conducted to demonstrate the effectiveness of the proposed method. The experimental results showcase that the proposed adaptive distillation strategy consistently improves the performance of various existing distillation methods.</p><p>For possible future work, the proposed method can be studied to applied to broader scenarios, e.g., data-free distillation where original training data is not available, or transformerbased distillation where the model architectures are replaced with the trending transformers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A schematic illustration of one-spot distillation, multi-spot distillation and the proposed spot-adaptive distillation. The dotted green arrows denote that these distillation spots are adaptively excluded dependent on samples and training iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>SAKD Pseudocode, PyTorch-like 1: # S i : i-th block in the student model 2: # T i : i-th block in the teacher model 3: # P : policy module 4: # Hst, Hts: adaption layer for adopting feature from student to teacher and from teacher to student, respectively 5: 6: # L CE : cross-entropy loss 7: # L KL : Kullback-Leibler divergence 8: # L KD : knowledge distillation loss 9: # ? 1 ,? 2 ,? 3 : hyper-parameters for trading off loss terms 10: 11: S.train() 12: T .eval() 13: for x, target in loader: 14: # load a mini-batch (x, target) with B samples 15: 16: with torch.no_grad(): 17: logit T , f eat T = T (x) 18: logit S , f eat S = S(x) 19: 20: ft = torch.cat((f eat T [-1], f eat S [-1]), 1) 21: w = P (ft) # shape: [B, N + 1, 2] 22: w = Gumbel-Softmax(w)[:, :, 0] # routing decisions 23: d = w.detach() # stop gradient 24: 25: # loss for student 26: Ls = L CE (logit S , target) 27:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>29 : 30 : 51 :</head><label>293051</label><figDesc>input_s = x 31: input_t = x 32: S.eval() 33: # Multi-path Routing Network 34: for i in range(1, N + 1): 35: f_t = T i (input_t) 36: f_s = S i (input_s) 37: input_s = f_s ? (1 -w[:, i]) + Hts(f_t) ? w[:, i] 38: input_t = Hst(f_s) ? (1 -w[:, i]) + f_t ? w[:, i] 39: f_t = T N +1 (input_t) 40: f_s = S N +1 (input_s) 41: 42: # output from routing network 43: out = f_s ? (1 -w[:, -1]) + f_t ? w[:, -1] 44: S.train() 45: 46: # loss for policy and adaption layers 47: L routing = ? 3 ? L CE (out, target) 48: L routing .backward() # back-propagate 49: update(P , Hst, Hts) # SGD update 50: Ls.backward() # back-propagate 52: update(S) # SGD update after gaining the final prediction of the routing network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Distillation probability curves at different distillation spots. Left: WRN 40 2 ? WRN 16 2, VID [51] on CIFAR-100. Right: Vgg13 ? MobileNetV2, AT [7] on tiny-ImageNet. KD 1, KD 2, KD 3, KD 4, KL are the candidate distillation spots ordered by their position in the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Visualization of the distillation decisions from the proposed SAKD on tiny-ImageNet<ref type="bibr" target="#b42">[43]</ref> with VID<ref type="bibr" target="#b50">[51]</ref> under C-ResNet56 ? C-ResNet20 distillation. " " denotes distilling knowledge from the teacher to the student, "?" means the student will not accept knowledge from the teacher.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>WRN  40 2 ? WRN 16 2. In our experiments, the candidate distillation spots include the softmax</figDesc><table><row><cell></cell><cell cols="9">C-ResNet56?C-ResNet20 C-ResNet110?C-ResNet32 C-ResNet32?4?C-ResNet8?4</cell><cell cols="2">Vgg13?Vgg8</cell><cell></cell><cell cols="3">WRN 40 2?WRN 16 2</cell></row><row><cell></cell><cell cols="2">Stan. Adap.</cell><cell>?</cell><cell cols="2">Stan. Adap.</cell><cell>?</cell><cell cols="2">Stan. Adap.</cell><cell>?</cell><cell cols="2">Stan. Adap.</cell><cell>?</cell><cell cols="2">Stan. Adap.</cell><cell>?</cell></row><row><cell>T</cell><cell>72.34</cell><cell>-</cell><cell>-</cell><cell>74.31</cell><cell>-</cell><cell>-</cell><cell>79.42</cell><cell>-</cell><cell>-</cell><cell>74.64</cell><cell>-</cell><cell>-</cell><cell>75.61</cell><cell>-</cell><cell>-</cell></row><row><cell>S</cell><cell>69.06</cell><cell>-</cell><cell>-</cell><cell>71.14</cell><cell>-</cell><cell>-</cell><cell>72.50</cell><cell>-</cell><cell>-</cell><cell>70.36</cell><cell>-</cell><cell>-</cell><cell>73.26</cell><cell>-</cell><cell>-</cell></row><row><cell>KD</cell><cell>70.66</cell><cell>-</cell><cell>-</cell><cell>73.08</cell><cell>-</cell><cell>-</cell><cell>73.33</cell><cell>-</cell><cell>-</cell><cell>72.98</cell><cell>-</cell><cell>-</cell><cell>74.92</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Fitnets 71.05 71.40</cell><cell>+0.35</cell><cell cols="2">73.19 73.58</cell><cell>+0.39</cell><cell cols="2">74.66 74.95</cell><cell>+0.29</cell><cell cols="5">73.22 73.54 +0.32 75.12 75.29</cell><cell>+0.17</cell></row><row><cell>AT</cell><cell cols="2">70.99 71.41</cell><cell>+0.42</cell><cell cols="2">73.16 73.47</cell><cell>+0.31</cell><cell cols="2">74.53 75.53</cell><cell>+1.00</cell><cell cols="5">73.48 73.81 +0.33 75.32 75.15</cell><cell>-0.17</cell></row><row><cell>SP</cell><cell cols="2">70.65 71.22</cell><cell>+0.57</cell><cell cols="2">73.03 73.26</cell><cell>+0.23</cell><cell cols="2">74.02 74.64</cell><cell>+0.60</cell><cell cols="5">73.49 73.61 +0.12 74.98 75.06</cell><cell>+0.08</cell></row><row><cell>CC</cell><cell cols="2">71.03 71.48</cell><cell>+0.45</cell><cell cols="2">73.07 73.42</cell><cell>+0.35</cell><cell cols="2">74.21 74.64</cell><cell>+0.43</cell><cell cols="5">73.04 73.26 +0.22 75.09 75.41</cell><cell>+0.32</cell></row><row><cell>VID</cell><cell cols="2">71.06 71.34</cell><cell>+0.28</cell><cell cols="2">73.31 73.71</cell><cell>+0.40</cell><cell cols="2">74.56 75.26</cell><cell>+0.70</cell><cell cols="5">73.19 74.12 +0.93 75.14 75.27</cell><cell>+0.13</cell></row><row><cell>RKD</cell><cell cols="2">71.07 71.33</cell><cell>+0.26</cell><cell cols="2">72.87 73.17</cell><cell>+0.30</cell><cell cols="2">73.79 74.44</cell><cell>+0.65</cell><cell cols="5">72.97 73.44 +0.47 74.89 75.21</cell><cell>+0.32</cell></row><row><cell>PKT</cell><cell cols="2">70.72 71.56</cell><cell>+0.84</cell><cell cols="2">73.61 73.92</cell><cell>+0.31</cell><cell cols="2">74.23 74.49</cell><cell>+0.26</cell><cell cols="5">73.25 73.53 +0.28 75.33 75.49</cell><cell>+0.16</cell></row><row><cell>FT</cell><cell cols="2">71.15 71.37</cell><cell>+0.22</cell><cell cols="2">73.44 73.65</cell><cell>+0.21</cell><cell cols="2">74.62 75.19</cell><cell>+0.57</cell><cell cols="5">73.44 73.46 +0.02 75.15 75.30</cell><cell>+0.15</cell></row><row><cell>NST</cell><cell cols="2">70.68 71.07</cell><cell>+0.39</cell><cell cols="2">72.91 73.50</cell><cell>+0.59</cell><cell cols="2">74.28 75.16</cell><cell>+0.88</cell><cell cols="5">73.33 75.44 +0.31 74.67 75.27</cell><cell>+0.60</cell></row><row><cell>CRD</cell><cell cols="2">71.46 71.78</cell><cell>+0.32</cell><cell cols="2">73.58 74.25</cell><cell>+0.67</cell><cell cols="2">75.46 75.81</cell><cell>+0.25</cell><cell cols="5">74.29 74.53 +0.24 75.64 76.03</cell><cell>+0.39</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III TOP</head><label>III</label><figDesc>-1 ACCURACY OF HOMOGENEOUS DISTILLATION ON CIFAR-100 (IN %). EXPERIMENTS ARE REPEATED FOR THREE TIMES AND THE AVERAGE RESULTS ARE PROVIDED. "STAN." DENOTES THE STANDARD DISTILLATION STRATEGY IN EXISTING METHODS. "ADAP." DENOTES THE ADAPTIVE DISTILLATION STRATEGY PROPOSED IN THIS WORK. ?: ACCURACY DIFFERENCE BETWEEN "ADAP." AND "STAN.". NUMBERS IN GREEN (RED) DENOTE THAT "ADAP." OUTPERFORMS (UNDERPERFORMS) "STAN.". T AND S REPRESENT THE TEACHER AND THE STUDENT MODELS, RESPECTIVELY.</figDesc><table><row><cell></cell><cell cols="6">Vgg13?MobileNetV2 I-ResNet50?MobileNetV2</cell><cell cols="3">I-ResNet50?Vgg8</cell><cell cols="6">C-ResNet32?4?C-ResNet32 WRN 40 2?ShuffleNetV1</cell></row><row><cell></cell><cell cols="2">Stan. Adap.</cell><cell>?</cell><cell cols="2">Stan. Adap.</cell><cell>?</cell><cell cols="2">Stan. Adap.</cell><cell>?</cell><cell cols="2">Stan. Adap.</cell><cell>?</cell><cell cols="2">Stan. Adap.</cell><cell>?</cell></row><row><cell>T</cell><cell>74.64</cell><cell>-</cell><cell>-</cell><cell>79.34</cell><cell>-</cell><cell>-</cell><cell>79.34</cell><cell>-</cell><cell>-</cell><cell>74.92</cell><cell>-</cell><cell>-</cell><cell>75.61</cell><cell>-</cell><cell>-</cell></row><row><cell>S</cell><cell>64.60</cell><cell>-</cell><cell>-</cell><cell>64.60</cell><cell>-</cell><cell>-</cell><cell>70.36</cell><cell>-</cell><cell>-</cell><cell>71.14</cell><cell>-</cell><cell>-</cell><cell>70.50</cell><cell>-</cell><cell>-</cell></row><row><cell>KD</cell><cell>67.37</cell><cell>-</cell><cell>-</cell><cell>67.35</cell><cell>-</cell><cell>-</cell><cell>73.81</cell><cell>-</cell><cell>-</cell><cell>72.98</cell><cell>-</cell><cell>-</cell><cell>74.83</cell><cell>-</cell><cell>-</cell></row><row><cell cols="6">Fitnets 72.47 73.11 +0.64 72.69 73.03</cell><cell>+0.34</cell><cell cols="5">73.24 73.75 +0.51 72.35 72.63</cell><cell>+0.28</cell><cell cols="2">75.67 75.93</cell><cell>+0.26</cell></row><row><cell>AT</cell><cell cols="5">72.00 72.60 +0.60 71.54 72.11</cell><cell>+0.57</cell><cell cols="2">74.01 73.82</cell><cell cols="3">-0.19 72.67 72.86</cell><cell>+0.19</cell><cell cols="2">76.24 76.55</cell><cell>+0.29</cell></row><row><cell>SP</cell><cell cols="5">73.04 73.31 +0.27 73.17 73.42</cell><cell>+0.25</cell><cell cols="5">73.52 74.23 +0.71 71.79 72.65</cell><cell>+0.86</cell><cell cols="2">76.29 76.54</cell><cell>+0.25</cell></row><row><cell>CC</cell><cell cols="5">72.41 72.93 +0.52 72.61 72.73</cell><cell>+0.32</cell><cell cols="5">73.48 73.54 +0.06 72.37 72.71</cell><cell>+0.34</cell><cell cols="2">75.24 75.77</cell><cell>+0.53</cell></row><row><cell>VID</cell><cell cols="5">72.10 72.70 +0.60 72.58 73.08</cell><cell>+0.50</cell><cell cols="5">73.46 73.92 +0.46 72.29 72.85</cell><cell>+0.56</cell><cell cols="2">75.88 76.21</cell><cell>+0.33</cell></row><row><cell>RKD</cell><cell cols="5">72.58 72.87 +0.29 72.90 73.86</cell><cell>+0.96</cell><cell cols="5">73.51 73.85 +0.34 71.35 72.17</cell><cell>+0.82</cell><cell cols="2">75.66 75.88</cell><cell>+0.22</cell></row><row><cell>PKT</cell><cell cols="5">72.76 73.23 +0.47 73.01 73.41</cell><cell>+0.40</cell><cell cols="5">73.61 73.90 +0.29 72.04 72.75</cell><cell>+0.71</cell><cell cols="2">75.66 76.05</cell><cell>+0.39</cell></row><row><cell>FT</cell><cell cols="5">72.02 72.30 +0.28 72.85 73.11</cell><cell>+0.26</cell><cell cols="5">72.98 73.54 +0.56 72.42 73.46</cell><cell>+1.02</cell><cell cols="2">72.96 73.60</cell><cell>+0.64</cell></row><row><cell>NST</cell><cell cols="5">72.31 72.66 +0.35 73.01 73.41</cell><cell>+0.40</cell><cell cols="5">71.74 72.00 +0.26 72.34 72.69</cell><cell>+0.35</cell><cell cols="2">76.45 75.96</cell><cell>-0.49</cell></row><row><cell>CRD</cell><cell cols="5">73.56 73.81 +0.25 73.76 74.29</cell><cell>+0.53</cell><cell cols="5">74.58 74.81 +0.23 72.99 73.21</cell><cell>+0.22</cell><cell cols="2">76.03 76.26</cell><cell>+0.23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV TOP</head><label>IV</label><figDesc>-1 ACCURACY OF HETEROGENEOUS DISTILLATION ON CIFAR-100 (IN %). EXPERIMENTS ARE REPEATED FOR THREE TIMES AND THE AVERAGE RESULTS ARE PROVIDED.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>2 ? WRN 16 2 Vgg13 ? MobileNetV2</figDesc><table><row><cell></cell><cell cols="2">Stan. Adap.</cell><cell>?</cell><cell cols="2">Stan. Adap.</cell><cell>?</cell><cell cols="2">Stan. Adap.</cell><cell>?</cell><cell cols="2">Stan. Adap.</cell><cell>?</cell><cell cols="2">Stan. Adap.</cell><cell>?</cell></row><row><cell>T</cell><cell>58.34</cell><cell>-</cell><cell>-</cell><cell>58.46</cell><cell>-</cell><cell>-</cell><cell>60.09</cell><cell>-</cell><cell>-</cell><cell>61.26</cell><cell>-</cell><cell>-</cell><cell>60.09</cell><cell>-</cell><cell>-</cell></row><row><cell>S</cell><cell>52.66</cell><cell>-</cell><cell>-</cell><cell>51.89</cell><cell>-</cell><cell>-</cell><cell>56.03</cell><cell>-</cell><cell>-</cell><cell>57.17</cell><cell>-</cell><cell>-</cell><cell>57.73</cell><cell>-</cell><cell>-</cell></row><row><cell>KD</cell><cell>53.04</cell><cell>-</cell><cell>-</cell><cell>53.40</cell><cell>-</cell><cell>-</cell><cell>57.33</cell><cell>-</cell><cell>-</cell><cell>59.16</cell><cell>-</cell><cell>-</cell><cell>60.02</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Fitnets 54.43 54.53</cell><cell>+0.10</cell><cell cols="2">54.04 54.25</cell><cell>+0.21</cell><cell cols="5">58.33 59.10 +0.77 58.88 59.33</cell><cell>+0.45</cell><cell cols="2">61.37 61.83</cell><cell>+0.46</cell></row><row><cell>AT</cell><cell cols="2">54.39 54.88</cell><cell>+0.49</cell><cell cols="2">54.57 54.71</cell><cell>+0.14</cell><cell cols="5">58.85 59.31 +0.46 59.39 59.65</cell><cell>+0.26</cell><cell cols="2">60.84 61.34</cell><cell>+0.50</cell></row><row><cell>FT</cell><cell cols="2">53.90 54.32</cell><cell>+0.42</cell><cell cols="2">54.46 55.10</cell><cell>+0.64</cell><cell cols="5">58.87 59.21 +0.34 58.85 58.90</cell><cell>+0.05</cell><cell cols="2">61.78 61.96</cell><cell>+0.18</cell></row><row><cell>PKT</cell><cell cols="2">54.29 54.50</cell><cell>+0.21</cell><cell cols="2">54.70 55.01</cell><cell>+0.31</cell><cell cols="5">58.87 59.13 +0.26 59.19 59.59</cell><cell>+0.40</cell><cell cols="2">61.90 62.14</cell><cell>+0.24</cell></row><row><cell>SP</cell><cell cols="2">54.23 54.39</cell><cell>+0.16</cell><cell cols="2">54.38 54.52</cell><cell>+0.14</cell><cell cols="5">58.78 59.26 +0.48 57.63 58.26</cell><cell>+0.13</cell><cell cols="2">61.90 62.29</cell><cell>+0.39</cell></row><row><cell>VID</cell><cell cols="2">53.89 53.95</cell><cell>+0.06</cell><cell cols="2">53.94 54.28</cell><cell>+0.34</cell><cell cols="5">58.55 58.80 +0.25 58.78 58.99</cell><cell>+0.21</cell><cell cols="2">60.84 61.23</cell><cell>+0.39</cell></row><row><cell>CC</cell><cell cols="2">54.22 54.83</cell><cell>+0.63</cell><cell cols="2">54.26 54.35</cell><cell>+0.09</cell><cell cols="5">58.18 58.67 +0.49 58.83 59.08</cell><cell>+0.25</cell><cell cols="2">61.32 61.82</cell><cell>+0.50</cell></row><row><cell>RKD</cell><cell cols="2">53.95 54.05</cell><cell>+0.10</cell><cell cols="2">53.88 54.09</cell><cell>+0.21</cell><cell cols="5">58.58 58.62 +0.04 59.31 59.32</cell><cell>+0.01</cell><cell cols="2">61.19 61.58</cell><cell>+0.39</cell></row><row><cell>NST</cell><cell cols="2">53.66 54.27</cell><cell>+0.61</cell><cell cols="2">53.82 54.01</cell><cell>+0.19</cell><cell cols="5">58.85 59.53 +0.68 59.07 59.20</cell><cell>+0.13</cell><cell cols="2">60.59 60.64</cell><cell>+0.05</cell></row><row><cell>CRD</cell><cell cols="2">55.04 55.06</cell><cell>+0.02</cell><cell cols="2">54.69 55.28</cell><cell>+0.59</cell><cell cols="5">58.88 59.38 +0.50 59.42 59.87</cell><cell>+0.45</cell><cell cols="2">61.63 61.89</cell><cell>+0.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V</head><label>V</label><figDesc>TOP-1 ACCURACY OF ADAPTIVE DISTILLATION ON TINY-IMAGENET (IN %). BOTH HOMOGENEOUS AND HETEROGENEOUS DISTILLATION ARE</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>INCLUDED HERE.</cell></row><row><cell></cell><cell>Method</cell><cell>AT</cell><cell>SP</cell><cell>CC</cell><cell>CRD</cell></row><row><cell></cell><cell>Standard</cell><cell>70.70</cell><cell>70.62</cell><cell>69.96</cell><cell>71.38</cell></row><row><cell>top@1</cell><cell>Adaptive</cell><cell>70.94</cell><cell>71.17</cell><cell>70.72</cell><cell>71.63</cell></row><row><cell></cell><cell>?</cell><cell>+0.24</cell><cell>+0.55</cell><cell cols="2">+0.76 +0.25</cell></row><row><cell></cell><cell>Standard</cell><cell>90.00</cell><cell>89.80</cell><cell>89.17</cell><cell>90.49</cell></row><row><cell>top@5</cell><cell>Adaptive</cell><cell>90.33</cell><cell>90.22</cell><cell>90.28</cell><cell>90.85</cell></row><row><cell></cell><cell>?</cell><cell>+0.33</cell><cell>+0.42</cell><cell cols="2">+1.11 +0.36</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE VIII TRAINABLE</head><label>VIII</label><figDesc>vs FROZEN TEACHER NETWORK. ? DENOTES THE TEACHER NETWORK IS CO-TRAINED WITH THE STUDENT NETWORK.</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div><p>This work is funded by the <rs type="funder">National Key R&amp;D Program of China</rs> (Grant No: <rs type="grantNumber">2018AAA0101503</rs>) and the <rs type="funder">Science and technology project of SGCC (State Grid Corporation of China</rs>): fundamental theory of human-in-the-loop hybrid-augmented intelligence for power grid dispatch and control. <rs type="person">J. Song</rs>, <rs type="person">Y. Chen</rs>, <rs type="person">J. Ye</rs> and <rs type="person">M. Song</rs> are with logits <rs type="programName">Block 1 Block 2 Block 3 Block 4 Teacher Block 1 Block 2 Block 3 Block 4 Student Distillation spot logits One-spot distillation logits Block 1 Block 2 Block 3 Block 4</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7WSuBQd">
					<idno type="grant-number">2018AAA0101503</idno>
				</org>
				<org type="funding" xml:id="_pemDjQq">
					<orgName type="program" subtype="full">Block 1 Block 2 Block 3 Block 4 Teacher Block 1 Block 2 Block 3 Block 4 Student Distillation spot logits One-spot distillation logits Block 1 Block 2 Block 3 Block 4</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1503.02531</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contrastive representation distillation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="3962" to="3971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the efficacy of knowledge distillation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="4793" to="4801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation via label smoothing regularization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1412.6550</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7130" to="7138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Paraphrasing complex network: Network compression via factor transfer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Knowledge transfer via distillation of activation boundaries formed by hidden neurons</title>
		<author>
			<persName><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge transfer with Jacobian matching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07-15">10-15 Jul 2018</date>
			<biblScope unit="page" from="4723" to="4731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tree-like decision distillation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">497</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Knowledge distillation via instance relationship graph</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="7089" to="7097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Knowledge distillation via route constrained optimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1345" to="1354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">357</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="4320" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge transfer via dense cross-layer mutualdistillation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Be your own teacher: Improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3712" to="3721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation via label smoothing regularization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3902" to="3910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-distillation as instance-specific label smoothing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2184" to="2195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Student becoming the master: Knowledge amalgamation for joint scene parsing, depth estimation, and more</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="2824" to="2833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Amalgamating knowledge towards comprehensive classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Data-free learning of student networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3514" to="3522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Data-free adversarial distillation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11006</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Elastic knowledge distillation by learning from recollection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mosaicking to distill: Knowledge distillation from out-of-domain data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Routing networks: Adaptive selection of non-linear functions for multi-task learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riemer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Routing networks and the challenges of modular and compositional computation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cases</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Klinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="1904">1904.12774, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modular networks: Learning to decompose neural computation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kunze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pathnet: Evolution channels gradient descent in super neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno>abs/1701.08734</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Conditional computation in neural networks for faster models</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<idno>abs/1511.06297</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Accme : Actively compressed conditional mean embeddings for model-based reinforcement learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Stafford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName><forename type="first">R</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Correcting forecasts with multifactor neural attention</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vempaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Calmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Khabiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Outrageously large neural networks: The sparselygated mixture-of-experts layer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spottune: Transfer learning through adaptive fine-tuning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Simunic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="4800" to="4809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v37/ioffe15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ser. Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning, ser. Machine Learning Research<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015-07">Jul 2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rkE3y85ee" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A tutorial on the cross-entropy method</title>
		<author>
			<persName><forename type="first">P.-T. De</forename><surname>Boer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kroese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of operations research</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="67" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Approximating the kullback leibler divergence between gaussian mixture models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Olsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP&apos;07</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">317</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Tiny imagenet visual recognition challenge</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Ieee</publisher>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.30.87</idno>
		<ptr target="https://dx.doi.org/10.5244/C.30.87" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">R H</forename><surname>Richard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Wilson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">A P</forename><surname>Smith</surname></persName>
		</editor>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2016-09">September 2016</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="87" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1704.04861</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Correlation congruence for knowledge distillation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5006" to="5015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Variational information distillation for knowledge transfer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="9155" to="9163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning deep representations with probabilistic knowledge transfer</title>
		<author>
			<persName><forename type="first">N</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tefas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">She is currently a Postdoctoral Fellow with National University of Singapore. Her research interests include transfer learning, human parsing, scene parsing, depth estimation, and image processing in various applications. Mingli Song (Senior Member</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1707.01219</idno>
	</analytic>
	<monogr>
		<title level="m">Jingwen Ye received the B.Sc. degree from the Dalian University of Technology in 2016 and the Ph.D. degree from Zhejiang University in 2021</title>
		<meeting><address><addrLine>China; China; Zhejiang University; China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2017. 2020. 2006</date>
		</imprint>
		<respStmt>
			<orgName>Zhejiang University. He received his B.Sc. degree in Computer Science and Technology from Sichuan University ; Computer Science and Technology from College of Computer Science, Zhejiang University ; Microsoft Visual Perception Laboratory, Zhejiang University. He received the Ph.D. degree in Computer Science from Zhejiang University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests mainly include knowledge distillation, transfer learning, few-and zeroshot learning, and interpretable machine learning. Ying Chen received the B.E. degree with the College of Software. He was awarded Microsoft Research Fellowship in 2004. His research interests include pattern classification, weakly supervised clustering, color and texture analysis, object recognition, and reconstruction. He is a senior member of the IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
