<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interactions with Big Data Analytics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Danyel</forename><surname>Fisher</surname></persName>
							<email>danyelf@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research |</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rob</forename><surname>Deline</surname></persName>
							<email>rob.deline@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research |</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mary</forename><surname>Czerwinski</surname></persName>
							<email>marycz@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research |</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Steven</forename><surname>Drucker</surname></persName>
							<email>sdrucker@microsoft.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research |</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Interactions with Big Data Analytics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D0CB515444C9C349A822283338BA19C7</idno>
					<idno type="DOI">10.1145/2168931.2168943</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When times are mysterious Serious numbers will speak to us always. That is why a man with numbers Can put your mind at ease. We've got numbers by the trillions Here and overseas."</p><p>-Paul Simon, "When Numbers Get Serious," 1983    Increasingly in the 21st century, our daily lives leave behind a detailed digital record: our shifting thoughts and opinions shared on Twitter, our social relationships, our purchasing habits, our information seeking, our photos and videos-even the movements of our bodies and cars. Naturally, for those interested in human behavior, this bounty of personal data is irresistible. Decision makers of all kinds, from company executives to government agencies to researchers and scientists, would like to base their decisions and actions on this data. In response, a new discipline of big data analytics is forming. Fundamentally, big data analytics is a workflow that distills terabytes of low-value data (e.g., every tweet) down to, in some cases, a single bit of high-value data (Should Company X acquire Company Y? Can we reject the null hypothesis?). The goal is to see the big picture from the minutia of our digital lives.</p><p>It is no surprise today that big data is useful for HCI researchers and user interface design. As one example, A/B testing is a standard practice in the usability community to help determine relative differences in user performance using different interfaces. For many years, we have used strict laboratory conditions to evaluate interfaces, but more recently we have seen the ability to implement those tests quickly and on a large population by running controlled</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>experiments on the Web <ref type="bibr" target="#b0">[1]</ref>. These experiments allow practitioners to identify causal relationships between changes in design and changes in user-observable behavior on a potentially massive scale. Even more recently, practitioners are starting to identify usability issues by mining query logs for commonly asked questions by users of certain applications <ref type="bibr" target="#b1">[2]</ref>. This can help product teams discover large, real-world usability issues while supplementing laboratory techniques that tend to focus on smaller, more isolated problems.</p><p>Other companies use the data more directly to modify their offerings. The online game company Zynga creates games and studies data on how its audience plays them in order to update the games immediately. "We're an analytics company masquerading as a games company," said Ken Rudin, a Zynga vice president in charge of its data-analysis team. He continued, "We are totally disrupting the traditional video games industry; a huge portion of that disruption is the ability to use data" <ref type="bibr" target="#b3">[3]</ref>.</p><p>Of course, big data analytics, like any research method, has its limits and pitfalls. Just because analysts have big data to work with doesn't guarantee the sample they need is sufficiently representative of their entire user population (bigger is not better); nor does it mean they have the ground truth around their users' motivations or needs from their behavior logs. For instance, boyd and Crawford argue that working with big data is still subjective and that automated data collection is not self-explanatoryit requires selection and interpretation <ref type="bibr" target="#b4">[4]</ref>. They point out that the data sampling and cleaning processes in particular are prone to potential error and bias. So, the challenge for HCI researchers is to leverage the big data that will be increasingly available, but to do so judiciously.</p><p>Here we report on the state of the practice of big data analytics, based on a series of interviews we conducted with 16 analysts. While the problems uncovered are pain points for big data analysts (including HCI practitioners), the opportunity for better user experience around each of these areas is vast. It is our hope that HCI researchers will not only turn their attention toward designs that improve the big data research experience, but that they will also cautiously embrace the big data available to them as a converging line of evidence in their iterative design work. The big data user experience challenge will affect every one of us. As Pat Hanrahan, a professor at Stanford, recently said: "The reason big data is impacting every one of us is the data oozing out of everything… It's like electricity flowing throughout an organization-everyone can tap into it on command to answer the individual questions their jobs demand" <ref type="bibr" target="#b5">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Nature of Analytics Work</head><p>The term analytics (including its big data form) is often used broadly to cover any data-driven decision making. Here, we use the term for two groups: corporate analytics teams and academic research scientists. In the corporate world, an analytics team uses their expertise in statistics, data mining, machine learning, and visualization to answer questions that corporate leaders pose. They draw on data from corporate sources (e.g., customer, sales, or product-usage data) called business information, sometimes in combination with data from public sources i nte ra c ti o n s M ay + j u n e 2 012</p><p>In the academic world, research scientists analyze data to test hypotheses and form theories. Though there are undeniable differences with corporate analytics (e.g., scientists typically choose their own research questions, exercise more control over the source data, and report results to knowledgeable peers), the overall analysis workflow is often similar.</p><p>This article is an alert and a call to action. It is an alert that big data computing is coming to us all: As we collect data from sensors and stream it from logs, traditional usability testing is being complemented by big data analytics. Increasingly, the usability professional must learn how to do large-scale analytics to provide valuable insights into how millions of users are working on large-scale applications.</p><p>It is a call to action in that we highlight important UX challenges in handling large datasets. There is a great opportunity to make the analysis of big data easier to do and faster. While systems researchers are building ever-larger designs, there is a great need to improve the experience of doing analysis with these systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Defining Big Data</head><p>When does analytics become big data analytics? The size that constitutes "big" data has grown according to Moore's Law. In 1975 attendees of the first VLDB (Very Large Databases) conference worried about handling the millions of data points found in U.S census information <ref type="bibr" target="#b6">[6]</ref>. In the context of information visualization, Shneiderman describes a dataset as big when it's too big to fit on a screen-at one item per pixel, most desktops would stop at a few million data points today <ref type="bibr" target="#b7">[7]</ref>.</p><p>(e.g., tweets or demographics). For example, the CFO of a games company might turn to the analytics department when considering several schemes to sell "extras" to improve a game's revenue. To help this decision making, the analyst's role is to choose informative metrics that can be computed from available data, to perform the necessary computations, and to report the results in a way the CFO can comprehend and act upon. This role is marked by several characteristics:</p><p>• The work is exploratory and demand-driven. While some analytics work is routine, analytics teams need flexibility to deal with new and changing data sources, new types of questions, and new techniques and technology.</p><p>• The ultimate goal of the work is clear communication, often to an audience with little background in analysis techniques like statistics.</p><p>• The work must produce highconfidence results, often under pressure. Reporting the limits of the analysis is often as important as reporting the results. These limits arise both from a poor fit between the available source data and the "ideal" data for answering the question, and from the "lossy" nature of the data transformations. To reduce these limits, the analysts sometimes improve the raw data, for example, by requesting that the engineering team log specific product usage.</p><p>• The work creates a strong need to preserve institutional memory, both by tracking the origins of past decisions and by allowing repeatability across analyses. Larger analytics departments often share "tribal knowledge," including the meaning of data values (e.g, how missing values are represented), domain-specific analysis techniques (e.g., how to filter spam bots out of Web requests logs), and general analysis techniques (e.g., when to apply a correction in a statistical test).</p><p>More often, big data means data that cannot be handled and processed in a straightforward manner. A spreadsheet fits in memory; it is reasonably quick to determine if the data is clean, the values are reasonable, and the results can be computed rapidly. In contrast, a big dataset won't fit in memory, so it will be hard to check whether it is clean. Computations will take a long time. New data may well be constantly streaming in, so that the processing system needs to make decisions about which part of the stream to capture. The dataset may consist of images, natural language text, or heterogeneous data, so it will be hard to predict where the database join keys are.</p><p>Finally, a big dataset will probably be so large as to not fit on a single hard drive; as a result, it will be stored on several different disks, and will be processed on a number of cores. Queries will have to be distributed and written to work across a network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An Old "New" Way of Analyzing Data</head><p>In many ways, today's big data analytics is a throwback to an earlier age of mainframe computing. To illustrate this, let's contrast the familiar interactive approach to analyzing data in spreadsheets with the brave new world of big data. If you had a dataset at the turn of the 21st century, you tended to copy your data to your disk, which typically took seconds or less. You would load the dataset into memory and then interactively perform one or more analyses on the data. There was no need to examine preliminary results and iterate, unless you were considering performing different tests or transforming the data. The whole process was very fast, and results would be ready in seconds. You knew right away if you were satisfied with your findings.</p><p>In today's world of big data, the luxuries of interactivity, direct manipulation, and fast system response are gone. You can't copy your data to your personal computer in whole; it's more than can be processed in real time, as well as more than can be visualized all at once, and it requires much work to get the right sample and amount of data. In some sense, with existing tools a data scientist may not really know if she got the most appropriate answer or not. Because of these issues, we feel the workflow of computing has taken a giant step backward-back to the punch cards of the 1960s! Most of the cloud systems used for big data analysis feel more like batch jobs, in which you submit a job and go get some coffee, with little insight into what's really going on behind the scenes, how long it will take, or how much it's going to cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Challenges Involved in Big Data</head><p>To understand the challenges of conducting big data analytics in more detail, we interviewed 16 data analysts at Microsoft. Each of them was working with large datasets and using a variety of cloud and distributed services to process their overwhelming quantity of data. As an example, one of our analysts was a social psychologist who works intimately with Twitter data. He receives the raw Twitter "fire hose" dump of data, often years' worth of it. He then analyzes the feed to study trends such as changing sentiment over time, or how information spreads through the Twitter feed. Several of the other analysts we interviewed do machine learning over very large datasets, for example, over all of the search queries coming out of the Bing search engine.</p><p>Although each analyst's workflow varies in specific ways, analyst activities are generally clustered into five steps, shown in Figure <ref type="figure">1:</ref> 1) acquiring data, 2) choosing an architecture, 3) shaping the data to the architecture, 4) writing and editing code, 5) reflecting and iterating on the results. This sort of workflow is in line with other documented workflows, such as the softwaredevelopment waterfall model.</p><p>These steps differ jarringly from the way data is handled in Excel: Cloud computation is fundamentally different from local computing. Whether Microsoft's Azure, Amazon's EC2, or a Hadoop cluster, parallel computation is a different way of building code. In these highly parallel systems, identical code is run on multiple virtual machines (VMs). Users typically first get their code running on a single instance that runs locally, and then deploy the code to a series of VMs that run remotely on a network. The data is stored across multiple servers to ensure that it can be processed as rapidly as possible; the code itself contains rules that help decide which machines execute the code, and in what sequence. Whereas computing against local data is fast and efficient, computing against data that is stored on a separate machine can be a slow operation.</p><p>In a spreadsheet, the user can choose the type of analysis on the fly or can export the dataset into the appropriate tool. Computation doesn't cost much in Excel, so cost planning is not very important. On a VM, each step costs a measurable amount of money and time, and different designs can have substantially different financial or temporal implications.</p><p>We describe the challenges of each of these steps in turn, using examples from our interviewees to illustrate them. Each of these steps provides the HCI practitioner with ample room to improve the user experience.</p><p>Acquiring data. The first challenge our analysts identified was determining where the data in their big-data systems came from. How do they discover sources of data? Increasingly, data is available in a wide variety of sources and formats: Online databases of public statistics are provided by the U.S. government (http:// data.gov) and the United Nations (http://unstats.org); private companies sell data from data marketplaces, such as Microsoft's Azure Marketplace and Infochimps. Experts ran into many problems with data available online, however. They struggled to figure out what data was available; even when it was available in machinereadable formats, it would often be stored in a schema that made it hard to use. In many of these systems, however, the data is available only after running an aggregation query-or worse, only in PDF files filled with text. Once this data was ready to go online, our analysts would combine it with information they collected themselves from sensor systems. This, in turn, caused new challenges: For example, it could take a third dataset to link the zip codes in a crime database to the area codes in a phone directory.</p><p>There are new opportunities for improving standards for announcing data, helping people find data, and formatting data so it can be more easily entered.</p><p>Choosing architecture based on cost and performance. Whatever platform the big data analysis is A completely new part of designing a data analysis for the cloud is planning for the economic impact of the design choices. With cloud computing, nearly every choice about computation, uploading/ downloading data, and storage has a direct dollar cost. In addition, each choice has an effect on how long a job will take to execute. Planning and monitoring these costs is unfamiliar and poorly supported for end users, and making mistakes can be quite expensive. Many of these decisions need to be made before the first byte is uploaded to the cloud and before the first line of code is written.</p><p>Using cloud computing can support a broad selection of VMs; for more computation, you simply pay more, by buying either more machines or larger ones. Doubling the memory or computation speed of a machine, however, does not double its speed; it can impose non-linear costs as communication overhead, storage, and other aspects change. For example, in certain systems, a developer who takes a larger-scale VM gets access to lower-level systems of the machine and better guarantees of performance; smaller VMs do not get this access.</p><p>There is no support for estimating the cost or the duration of a computation before performing it. Programmers end up iteratively re-running their application to adjust the number of VMs, the size of queues, and so on, incurring larger bills while empirically finding the right time-cost balance point. When working with VMs in a shared cloud, their performance characteristics may even vary over repeated experiments.</p><p>Current cloud-computing platforms provide little to no support for partitioning an application across VMs. Users must do their own empirical measurements, gauging not only how long a job takes to compute, but also how much overhead is involved in starting a job on a new VM. In our users' projects, this overhead could be substantial, and so the cost of a job was often needed to factor in this long startup time. The time a job takes is affected by both the startup time and the computation time; adding more cores reduces the latter but doesn't affect the former.</p><p>Our analysts found it difficult to estimate the effects of these different potential configurations on task and computation time.</p><p>Shaping the data to the architecture. Once the analyst has found a dataset and a computing platform, he or she must upload the data into the platform. The analyst must ensure that the data is uploaded in a way that is compatible with how the computation will be structured, and distributed and partitioned appropriately.</p><p>Cloud-computing systems use data storage differently than desktop machines. Cloud systems are still evolving models for storage: They offer cloud-based databases (such as Amazon's RDS and Microsoft's SQL Azure), distributed file systems (as in Hadoop), and novel data structures (as in Azure's queues and blobs). These structures are meant to be adapt-able to users' coding needs, but they represent a way of thinking about storage and communication that is distinct from what users have previously experienced. These structures hide underlying structure: A queue, for example, is a distributed data structure across a number of machines, and a partitioned table is silently split across multiple machines.</p><p>Several of our interviewees commented that moving files back and forth between the cloud and a local machine was extremely common but a huge pain point. When the files are large, efficiency is critical, so it's not simply an issue of finding some way to get files uploaded, but an issue of doing it right. The files need to be organized, partitioned, and prepared before they are uploaded to the cloud. Different techniques for importing can take very different amounts of time; for example, converting files to a binary representation rather than a comma-delimited text file could radically change the ingestion process on the far side.</p><p>Currently, tooling is limited: Analysts are used to a rich set of tools for handling local files, including moving and renaming them, looking at their content, and referring to them from applications. Each of these steps is more complex in the cloud. Our experts had difficulty inspecting and working with files once they had been moved up into the cloud.</p><p>Once data has been uploaded, it must be cleaned. Cleaning can be a difficult process, requiring multiple people's expertise; as a result, some of our analysts had collaboratively cleaned their dataset. Unfortunately, it was then often difficult to understand who had touched which part of the data. In particular, analysts found them-selves jury-rigging mechanisms to capture descriptions of what the data cleaner was trying to accomplish and what features were used.</p><p>Fairly often, cleaning was a process that occurred only after code had been written and errors hit: They would go back and look for aberrations after a crash, or when a model looked odd. As a result, the analysts also wanted to capture the justifications for cutting aberrant data points: How had they detected the error? What model had found the issue?</p><p>Most data tables have the built-in notion that data should be stored and edited in place. Interviewees stressed the expectation that data storage is comparatively cheap: Rather than mutating it in place and losing history, they would prefer to create additional clean versions of columns and new datasets.</p><p>Write code. With an architecture selected and the data in place, the analyst begins to select their analysis. In the examples we studied, the analyses were articulated through code, written in C# and Microsoft's SCOPE; outside these environments, analysts might work in languages such as R, Python, or PIG (a database-like language), usually over Hadoop. High-level languages that make it easy for the compiler to support parallelismsuch as DryadLINQ or Matlab's matrix-based language-will ultimately help users write cloudbased jobs.</p><p>Users must design their code and systems around the idea of separating their work into parallelizable jobs. Algorithms need to be written in new ways in order to do this, and data must be stored differently. For example, some resources might need to be duplicated, one per node. In order to reduce costly communication, an analyst might store a copy of a reference lookup table on each VM.</p><p>Abstracting away the cloud. These high-level languages are designed to allow analysts to "abstract away" the cloud. Analysts should spend less effort considering where their data is being processed, and more effort considering the nature of computation. Unfortunately, the abstraction can be leaky. Several of our users complained of times when one process or another was blocked based on transient network issues. The symptoms of these issues can be masked by other failures, as redundant systems and parallel code attempt to recover. As a result, from the analysts' perspective, sometimes the work simply stops, without a clear message; later, the system spins back up and the lag time is not accounted for. So a fine balance between transparency and abstraction seems to be required.</p><p>Directly manipulating data versus scripting. Our analysts are accustomed to working in Excel for their smaller data jobs. As a result, they often compared their procedures to working in an Excel-like tool. Needless to say, the cloud was found wanting: Cloud analysis is far more complex than when using desktop tools, even when the type of analysis is similar. They hoped for directmanipulation environments.</p><p>On the other hand, several analysts emphasized the importance of scripting an analysis rather than carrying it out "by hand" through a direct manipulation interface. (R is an example of a scripting interface; SPSS is a direct-manipulation environment that generates a script in the background.) Scripting leaves a log of analysis, which makes it easy to repeat later or to share with teammates. Logs also make it easier to repeat experiments, to try variants when new questions or new data arise, and to fix mistakes in later runs. Finally, scripts document the derivation of high-level information and charts.</p><p>Of course, neither of these is available now; instead, analysts write compiled code.</p><p>Debugging and iteration. After the execution run is complete, the analyst wants to know if their test worked. This leads to a process of debugging and looking for errors, iteration and changing code to work, and visualization, in order to interpret results.</p><p>Code rarely works the first time. While modern programming environments will usually break into a debugger on a crash, a cloud-based computing solution will often be far more difficult to debug. Our users reported that they often found themselves poring through trace files, which reminded some respondents of debugging in the 1980s. While it is simple to write out trace files, a job distributed across a group of VMs means that a single crash might be distributed across multiple VMs, with trace files stored on a variety of machines. This is compounded by the temporary nature of VMs. If a VM fails or stops responding, the infrastructure cleanly recovers from the fault, moving jobs to a different machine. Unfortunately, the transparency of this process can have the effect of hiding errors when they occur, making it a challenge to diagnose and eliminate them.</p><p>Data analytics is inherently exploratory, which makes rapid iteration highly desirable. After a job completes, for example, an analyst may want to tweak i nte ra c ti o n s M ay + j u n e 2 012</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>56</head><p>CoVer STorY a parameter and try again. In machine learning, for example, a user may want to check different combinations of parameters and feature sets to run after running one iteration suggests different features to try out. As one interviewed analyst put it, "Fast iteration is key but incompatible with the way jobs are submitted and processed in the cloud." It's frustrating to wait for hours only to realize you need a slight tweak to your feature set.</p><p>Analysts moved back and forth from local machines to cloudbased systems. For an iterative machine-learning process, some of the feature generation could be done locally, but the raw data then needed to be uploaded to the cloud. If the feature set was not correct or of high enough quality, they needed to go back to the raw data. They would sample data to a local machine, which they would use to both explore features and test models they had generated from a full run-through. This back and forth from local to cloud was poorly supported.</p><p>Most, if not all, interviewees stressed the critical role of visualization ("Visualization is huge" was a frequent comment). The common need was that of inspecting data at multiple scales. With large datasets, one must frequently start wide and zoom into very small details. Visual interfaces are extremely well suited to this scenario and can be used in conjunction with statistical analyses (e.g., suggesting to the user a number of clusters to specify in a cluster analysis). A related scenario is the need to see context. Especially in large datasets, getting lost in the data is easy. Visualization provides a way to maintain context by showing data as a subset of a larger part of the data, showing correlated variables, and so on. Visualization is also relevant to data streams that are now common, in that they can help identify patterns over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hope for the Future</head><p>This list of complaints and pain points can be exhausting. This new paradigm of computation has placed stumbling blocks before analysts who try to take advantage of the opportunities. Yet it also means new opportunities for better user experiences and improved tools for analysis and investigation. As one frustrated analyst asked, "Can we take a typical Excel user and empower them to become a data scientist?"</p><p>When we picture a system that addresses these issues, we believe this leads to a new way of writing queries and analysis scripts. Here, we imagine a hypothetical system, called an analytics cloud environment (ACE), which helps bring together these many ideas under one roof. Constructing this system requires critical contributions from HCI researchers, who can help understand the user experience of interacting with big data and can help figure out what aspects to bring to the fore-in the end, benefitting the process that HCI researchers will eventually need to participate in themselves!</p><p>To support rapid iteration on data in the cloud, the environment must be interactive, one in which users iteratively pose queries and see rapid responses. We already know about the power of interactive environments: Tools like R, Python, and Matlab are popular for data analysis. Projects like CONTROL <ref type="bibr" target="#b8">[8]</ref> have begun to point the way toward online computation, in which large jobs are broken into small portions whose results are reported incrementally. Fisher et al. <ref type="bibr" target="#b9">[9]</ref> point to ways to integrate visualization with online systems, allowing users to get visual feedback on their queries. This rapid iteration over partial results would allow analysts to catch bugs and explore alternatives more quickly.</p><p>The ACE would retain the team's institutional memory by allowing the user to save, share, browse, and reuse previous interactions with a data source. This would be richer than a standard R or Python shell-a tool that can introspect on the code it runs. This, in turn, may allow partial computations, letting users drill down into the workflow so they can use only part of a previous iteration.</p><p>Part of storing history is remembering where the data came from. The ACE would have an intrinsic notion of provenance. Inspired by tools like VisTrails <ref type="bibr" target="#b10">[10]</ref>, which today can remember both where a given data item came from and how users have changed their and CPU utilization, computation rate and queue lengths, and errors for a user. This feedback should be brought in as an integral part of the ACE, so that when a user writes a line of code they can see how many rows per second it can compute-and how much it will cost if it continues to run.</p><p>Additionally, just as the backend can be abstracted, so might the data itself. As stated, analysts we interviewed frequently worked with samples on their local machines before carrying out a full analysis in the cloud. This allowed them both to regain the interactive, exploratory experience of working with small data and insulated them from the complexities and expenses of computing in the cloud. Given the vast differences between the two computing environments, we feel that working locally with samples will continue to have productivity and economic benefits and deserves better support.</p><p>The ACE should be able to sample data from the cloud for local experiments and equally be able to push those experiments back up into the cloud. Sampling should also be a first-class activity, allowing users to create samples with desirable properties like statistical distributions or the ability to test Contemporary high-level languages are increasingly being adapted for distributed computation, allowing analysts to express their queries as parallelizable entities. The ACE should similarly divide queries transparently among multiple machines. It draws on existing systems, such as Distributed-Computing Matlab, Hadoop's PIG, and DryadLinq, highlevel languages that allow parallel computation. The ACE should come with a robust library of algorithms that are known to be efficient in the cloud.</p><p>Abstracting away the back-end does not mean hiding all execution details from users, particularly with respect to resources and failures. The ACE should allow users to feel fully in control of the resources and tools that the system is using. It is a challenge to visualization-oriented HCI researchers to figure out what makes for the best combination of tools for tracking and monitoring the states of storage, job execution corner cases of the scripts. This will steer users away from typical samples of convenience, such as the first 10,000 rows. In addition to sampling, users should also have robust tools for cleaning, previewing, and exploring their data both locally and in the cloud; groups of users working together should be able to distribute and make comments about their cleaning work.</p><p>One of the most valuable aspects of the language R is the substantial community that has grown up around it. Our analysts, too, did not work alone, but rather worked with teams. While permission models for cloud computing might still be primitive, the CSCW community has a rich history of supporting complex collaboration. The ACE should encourage collaboration around data, including allowing users to share not just scripts, but also data sources, versions of data sources that have already had a round of computation done on them, and even data runs that are in progress.</p><p>The ACE, then, is an umbrella vision for a system that smoothly lives both locally and in the cloud. Users enter commands, but rather than waiting for arduous computation to complete, they get incremental results as soon as they are available. It keeps a rich history of computation and provenance, allowing communities of users to understand how their data is being manipulated and how they are interacting with it. It provides high-level constructs for efficient coding against parallel systems and also allows users to introspect on costs and uses in detail. The challenge for HCI designers is to expose many of these capabilities to users in ways that empower sophisticated users without overwhelming them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>58</head><p>CoVer STorY Supporting non-specialists. Universities are quickly creating advanced degree programs in analytics and minting as many new statisticians as they can; nonetheless, in the near term, demand will exceed supply. This shortfall creates the opportunity for tools to allow data-savvy end users to do their own analyses, without expert supervision. On the one hand, this creates the opportunity for rapid discovery, akin to citizen science; on the other hand, this exposes end users to the pitfalls that scientists are trained to avoid. Some pitfalls, like being careless with missing data values, misapplying statistical tests, or overfitting models, are inherent to the workflow, creating the opportunity to develop tools to recognize and mediate them. Other problems are more philosophical, such as confusing correlation with causation or ascribing too much importance to statistical significance. These larger pitfalls are typically avoided through apprenticeship with experts, which is difficult to apply to large numbers of people. If emerging tools allow end users to analyze large datasets without the rigor of scientific practice, the tools could lend the aura of science to otherwise poor conclusions. Societal issues with ample source data, like healthcare and climate change, could be acted on more on the basis of "truthiness" than truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Big Data Analytics Is the Future of Interaction Testing and Research</head><p>Here we have described big data analytics as an emerging type of knowledge work, with plenty of opportunities for study and productivity improvements. However, even for those who are not interested in this form of knowledge work, big data analytics cannot be ignored: It's an important new avenue to learn about how people interact with computing.</p><p>Product teams are already taking advantage of product usage data from tens of thousands to millions of customers. As one example, Bungie Studios recorded the deaths of all players of Halo 3 and produced heat maps to spot problems in game play <ref type="bibr" target="#b11">[11]</ref> (see Figure <ref type="figure">2</ref>). If too many players were dying in a particular game location, they could adjust the game, for example, by moving ammunition to a more obvious location. Another example is the creation of the Ribbon UI in Microsoft Office 2007 <ref type="bibr" target="#b12">[12]</ref>. By analyzing the customer experience data from previous releases of the product (about 1.3 billion sessions), the Office team could make informed choices about the most commonly used features. This kind of analysis, which is based on simple tallies of operations, just scratches the surface of what is possible.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>BIG DATA a map of the frequency with which people in different places reply to each other on Twitter. The brightness of each arc is proportional to the log of the number of tweets from one place addressing someone in another place, with locations chunked to 20-mile squares. Communication is shown moving clockwise from the person sending the tweet to the person being addressed. Data from Twitter streaming aPI, may 15october 10, 2011. See something or say something: los angeles. Red dots are locations of Flickr pictures. Blue dots are locations of Twitter tweets. White dots are locations that have been posted to both. This graph charts the frequency of mentions in the New York Times of the five u.S. presidents between 1984 and 2009. It also depicts story weights-the darkest lines shows front-page stories, the lighter lines indicate stories buried deeper in the paper. Image by Eric Fischer Image by Eric Fischer Image by jer Thorp i nte ra c ti o n s M ay + j u n e 2 012 performed on, the platform organizes the computation around a set of programming abstractions substantially different from those of the normal desktop environment. Analysts trained on the desktop environment have to learn these new abstractions and plan their computation around them, often facing a new set of engineering trade-offs and failure modes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>zoom into very small details. i nte ra c ti o n s M ay + j u n e 2 012</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>scripts, the ACE would store computation histories. A user could click on a result and see where it came from: what operations have been called on it, what queries it has passed through, and perhaps even what rows in the source data caused it to exist.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>•</head><label></label><figDesc>Figure 2. Heat maps of problem areas in Halo 3 (image courtesy of Microsoft Studios). i nte ra c ti o n s M ay + j u n e 2 012</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>i nte ra c ti o n s M ay + j u n e 2 012</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>About thE Authors</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Controlled experiments on the web: Survey and practical guide</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Longbotham</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">D</forename><surname>Sommerfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Henne</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-008-0114-1</idno>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="140" to="181" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Characterizing the usability of interactive applications through query log analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fourney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2011 Annual Conference on Human Factors in Computing Systems. aCM, new york</title>
		<meeting>of the 2011 Annual Conference on Human Factors in Computing Systems. aCM, new york</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1817" to="1826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<idno type="DOI">=10.1145/1978942.1979205</idno>
		<ptr target="http://doi.acm.org/10.1145/1978942.1979205" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Virtual product, real profits: Players spend on Zynga&apos;s games, but quality turns some off</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wingfield</surname></persName>
		</author>
		<ptr target="http://online.wsj.com/article/SB10001424053111904823804576502442835413446.html?KEyWORDS=zynga" />
	</analytic>
	<monogr>
		<title level="j">Wall Street Journal</title>
		<imprint>
			<date type="published" when="2011-09">Sep. 9, 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Six provocations for big data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crawford</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.1926431</idno>
		<ptr target="http://dx.doi.org/10.2139/ssrn.1926431" />
	</analytic>
	<monogr>
		<title level="m">Decade in Internet Time: Symposium on the Dynamics of the Internet and Society</title>
		<imprint>
			<date type="published" when="2011-09">Sept. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tableau Software&apos;s Pat Hanrahan on &apos;What Is a Data Scientist?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Woods</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Forbes</title>
		<imprint>
			<date type="published" when="2011-11-30">nov. 30, 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Simonson</surname></persName>
		</author>
		<author>
			<persName><surname>Alsbrooks</surname></persName>
		</author>
		<title level="m">DBMS for the U. S. Bureau of the Census. Proc. of the 1st International Conference on Very Large Data Bases. aCM, new york</title>
		<imprint>
			<date type="published" when="1975">1975</date>
			<biblScope unit="page" from="496" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extreme visualization: Squeezing a billion datapoints into a million pixels</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shneiderman</surname></persName>
		</author>
		<idno type="DOI">10.1145/1376616.1376618;Key:citeu-like:3009411</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM SIGMOD International Conference on Management of Data. aCM, new york</title>
		<meeting>of the ACM SIGMOD International Conference on Management of Data. aCM, new york</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interactive data analysis: The Control Project</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hidber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="1999-08">aug. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Trust me, I&apos;m partially right: Incremental visualization lets analysts explore large datsets faster</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName><surname>Mc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>To appear in Proc. of CHI</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">VisTrails: Visualization meets data management</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Callahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Vo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGMOD</title>
		<meeting>of ACM SIGMOD</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Halo 3: How Microsoft Labs invented a new science of play</title>
		<author>
			<persName><forename type="first">C</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wired</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<date type="published" when="2007-08">aug. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Inside Deep Thought (why the UI</title>
		<author>
			<persName><forename type="first">J</forename><surname>Harris</surname></persName>
		</author>
		<ptr target="http://blogs.msdn.com/b/jensenh/archive/2006/04/05/568947.aspx" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
