<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Edge-Featured Graph Attention Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-01-19">19 Jan 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software Shanghai</orgName>
								<orgName type="institution">Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Haopeng</forename><surname>Chen</surname></persName>
							<email>chen-hp@sjtu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Software Shanghai</orgName>
								<orgName type="institution">Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Edge-Featured Graph Attention Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-19">19 Jan 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2101.07671v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lots of neural network architectures have been proposed to deal with learning tasks on graph-structured data. However, most of these models concentrate on only node features during the learning process. The edge features, which usually play a similarly important role as the nodes, are often ignored or simplified by these models. In this paper, we present edge-featured graph attention networks, namely EGATs, to extend the use of graph neural networks to those tasks learning on graphs with both node and edge features. These models can be regarded as extensions of graph attention networks (GATs). By reforming the model structure and the learning process, the new models can accept node and edge features as inputs, incorporate the edge information into feature representations, and iterate both node and edge features in a parallel but mutual way. The results demonstrate that our work is highly competitive against other node classification approaches, and can be well applied in edge-featured graph learning tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In many real-world applications, data are best constructed as graphs to analyze and display. The graph is such a natural structure whose nodes and edges can be used to characterize the entities and their inner-relationships among data. Recently, several works have defined neural networks on graphs <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b25">26]</ref>. Kipf et al. <ref type="bibr" target="#b9">[10]</ref> proposed graph convolutional networks, namely GCNs, based on spectral graph theory. Veličković et al. <ref type="bibr" target="#b20">[21]</ref> presented graph attention networks (GATs), which aggregate features following a self-attention strategy. Though such graph neural networks have been proven successful in some of node classification tasks, they still have obvious shortcomings. One of these networks' major problems is that the edge features are not incorporated into the models. In fact, most of the current state-of-the-art graph neural networks have not consider the edge features.</p><p>However, edges with their features play an essential role in many real-world node classification tasks. For example, in a trading network, the node labels may be highly relevant to the transactions. In such a case, the information contained in edges may have a more significant contribution to the classification accuracy compared with node features. Actually, different graphs have different preferences for features of nodes and edges, whereas all existing GNNs ignore or shun this fact.</p><p>In this paper, we proposed edge-featured graph attention networks (EGATs) to address the above challenges. This work can be regarded as an extension of GATs. To exploit the edge features effectively, we enhance the original attention mechanism; thus, the edge information can be an important factor in attention-weight computing. Further, the structures and learning processes of traditional attention models are also redesigned in our work, so the models can accept both node and edge features and iterate them individually. The updating of edge features is necessary and should be node-equivalent, because an iterative consistency between nodes and edges should be kept during the learning. Besides, a multi-scale merge strategy, which concatenates features from different iterations, is also adopted in our work. All node and edge features will be gathered in the final layer so that the model can learn the necessary characteristics which benefit the classification from various scales.</p><p>To our best knowledge, we are the first to incorporate edges into GATs as equivalent entities like nodes, point out graphs' different different preferences for features, and handle them spontaneously within the models. Our models can be applied to graphs with discrete and continuous features for both nodes and edges, which can satisfy the demands of many real-world node classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Graph neural networks (GNNs) <ref type="bibr" target="#b16">[17]</ref> first extended neural network architectures to graphs. As a result, many related works were sprung up ensuingly. Spectral approaches, established on spectral graph theory, are among the most critical parts of these works. Bruna et al. <ref type="bibr" target="#b1">[2]</ref> first defined convolution operations on Fourier domain. However, the filters this model computed are non-spatially localized, so Henaff et al. <ref type="bibr" target="#b7">[8]</ref> improved it by generating spatially localized filters. Kipf et al. <ref type="bibr" target="#b9">[10]</ref> proposed graph convolutional networks (GCNs), which further simplified above methods by using a firstorder approximation on the Chebyshev polynomials. Unlike GCNs, Veličković et al. <ref type="bibr" target="#b20">[21]</ref> proposed graph attention networks (GATs) to dynamically aggregate node features. Numerous variants have been derived from such design. Wang et al. <ref type="bibr" target="#b21">[22]</ref> introduced heterogeneous graph attention networks (HANs) to process various heterogeneous graphs. Ma et al. <ref type="bibr" target="#b12">[13]</ref> put forward disentangled graph convolutional networks using a routing algorithm. Several other works also made efforts to learn graph representations. GraphSAGE <ref type="bibr" target="#b6">[7]</ref> generates node embeddings by aggregating node features using several pre-defined aggregate operations. Inspired by RNNs like LSTM <ref type="bibr" target="#b8">[9]</ref> and GRU <ref type="bibr" target="#b3">[4]</ref>, gated graph neural networks <ref type="bibr" target="#b10">[11]</ref>, namely GGNNs, were proposed. Furthermore, Xu et al. <ref type="bibr" target="#b23">[24]</ref> explored jumping knowledge networks, in which layer aggregation is adopted to acquire multi-scale features.</p><p>However, all these graph neural networks have a common characteristic: focus on node rather than edge features. Only very few works have tried to integrate edge features into GNN architecture. Schlichtkrull et al. <ref type="bibr" target="#b17">[18]</ref> proposed an extension architecture of GCNs named R-GCNs. Gong et al. <ref type="bibr" target="#b5">[6]</ref> presented a framework that augments GCNs and GATs with edges. However, such approaches are somewhat not as reasonable as they are. We will highlight their limitations in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Motivation</head><p>The demands of processing edge-featured graphs are quite common in real-world tasks. For example, if there is a need to find users who may have illegal behaviors in a trading network, it is better to use some node classification approaches to pick them up. Evidently, one user is suspicious or not is highly relevant to the amount he paid or received some time. In other words, the edge features are likely to have a more significant impact on classification than node features under such a situation. However, traditional GNNs cannot handle these graphs in a direct, elegant, and reasonable way. There may be some doubts that if it is possible to convert graphs to which current models can readily accept. Obviously, ignoring edge features is unacceptable. Using pre-defined aggregate functions to integrate edge features into nodes may be a better solution. We do not deny it may perform well on certain graphs, whereas it is not a panacea suitable for every condition, for the selection of function is highly dependent on graphs' traits. It is more like feature engineering, rather than a universal approach.</p><p>Only a few works exploit edge features in graph neural networks, and all of them have obvious limitations. Schlichtkrull et al. <ref type="bibr" target="#b17">[18]</ref> proposed R-GCNs to process modeling relational data. However, the models can accept graphs only when their edges are labeled, which indicates that the edges cannot include continuous attributes. Gong et al. <ref type="bibr" target="#b5">[6]</ref> presented a framework enhances GCNs <ref type="bibr" target="#b9">[10]</ref> and GATs <ref type="bibr" target="#b20">[21]</ref>. This framework can accept continuous attributes of edges, whereas it merely regards them as weights between different node pairs. In most cases, it is somewhat unreasonable. For example, a special graph can be constructed, with the same features for nodes and different features for edges. If we consider edge features as weights and all weights of each node sum as one, it is interesting to find that no matter how the node features update, it will remain unchanged during the learning process.</p><p>The above phenomenon shows a fact, which has never been discussed in recent researches, that different graphs may have different preferences for node and edge features. To those graphs that  It accepts H and E as inputs, and produces two sets of new features. A H and A E are adjacency matrices, while M H and M E are mapping matrices for nodes and edges, respectively. The H m is edge-integrated node features generated by node attention block, which will only be used in the merge layer; (b) The architecture of EGATs. The model is constructed by several EGAT layers and a merge layer. The node and edge features generated from each iteration will be concatenated in the merge layer to achieve a multi-scale feature fusion. For convenience, the adjacency and mapping matrices are not shown in this figure, as well as the multi-head attention.</p><p>edges possess a great impact, it is infelicitous to treat edge features as weights or labels. However, all existing works have ignored such a key fact. Our work's motivation is not only to integrate edge features into GATs but also to propose general models that spontaneously handle such preferences. To our best knowledge, we are the first to try to solve such problems. It should be emphasized that we do not want to present disparate models against state-of-the-art approaches. Since the attention mechanism has proved itself in lots of tasks, it is unnecessary to propose a completely new one. Our work is an extension of GATs <ref type="bibr" target="#b20">[21]</ref>, and all the improvements we made are served for our motivation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The proposed model 4.1 EGAT layer overview</head><p>A single EGAT layer contains two different blocks: node attention block and edge attention block. Each EGAT layer is designed in a symmetrical scheme; thus, the node and edge features can update themselves in a parallel and equivalent way. Figure <ref type="figure" target="#fig_1">1</ref> (a) gives an illustration of the EGAT layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Each EGAT layer accepts a set of node features, H</head><formula xml:id="formula_0">= { h 1 , h 2 . . . , h N }, h i ∈ R FH ,</formula><p>as well as a set of edge features, E = { e 1 , e 2 . . . , e M }, e p ∈ R FE , as inputs. N and M represent the number of nodes and edges, while F H and F E symbolize the number of their respective features. After processing, the layer will produce high-level outputs, which include a new set of node features,</p><formula xml:id="formula_1">H ′ = { h ′ 1 , h ′ 2 . . . , h ′ N }, h ′ i ∈ R F ′ H , and a new set of edge features, E ′ = { e ′ 1 , e ′ 2 . . . , e ′ M }, e ′ p ∈ R F ′ E .</formula><p>The cardinality of F and F ′ may be different (whether the F is F H or F E ), since the linear transformations performed on the node and edge features are not same. We use two learnable matrices,</p><formula xml:id="formula_2">W H ∈ R FH ×F ′ H , and W E ∈ R FE ×F ′ E</formula><p>, to achieve such transformations. For each node i and edge p, their transformed features can be computed by h * i = W H h i , and e * p = W E e p , respectively. Then, both of them will be fed into the node attention block and the edge attention block, which individually producing the new sets of node and edge features. Moreover, the adjacency and mapping matrices of nodes and edges will also be injected into the two blocks for ancillary computation. For simplicity, we will re-use some symbols, which include H, E, h i , and e p . These symbols will have new meanings that characterize the features transformed by linear transformations in the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Node attention block</head><p>The node attention block accepts H, a set of node features, and E, a set of edge features, and produces H ′ , a new set of node features. In E, the edge features are ranked in a preset order, so it is hard to find the relations between edges and their adjacent nodes. Thus, a mapping transformation will be first applied to E in the block, to re-organize it into another common form E * . Every element in E * can be represented as e ij , while i and j denote the nodes on each end of an edge. The transformation from E to E * can be realized by matrix multiplication using an edge mapping matrix M E , which is an N × N × M tensor. Compared with the adjacency matrix, it expands the third dimension to indicate where each edge should be placed. Figure <ref type="figure" target="#fig_2">2</ref> (b) gives a simple example about the mapping process.</p><p>Before the multiplication, the edge mapping matrix should first be reshaped into N 2 ×M so that M E and E could have the same dimension of 2. Eventually, the multiplication result needs to reshape back with a size of N × N × F ′ E , transforming the edge set E into the adjacency form. The edge mapping matrix is unique for a particular graph structure with determining orders of nodes and edges so that it can be constructed in a pre-processing step before the learning process.</p><p>Thanks to the adjacent form, the model can quickly seek out the edge between two specified nodes. Based on that, an edge-integrated attention mechanism can be performed on each node, generating the attention weights of its neighbors includes not only the features of the two nodes but also the edge connecting them. For each node i, the weight w ij will be computed for every j ∈ N i , where the N i is the set including the first-order neighbors of node i as well as the node i itself. During the process, features will be concatenated, parameterized by a weight vector a, and applied LeakyReLU as the activation function. Normalization will also be performed on these weights across all choices of node j, where j ∈ N i , by using a softmax function. The whole process can be formulated as follows:</p><formula xml:id="formula_3">α ij = exp(LeakyReLU( a T [ h i h j e ij ])) k∈Ni exp(LeakyReLU( a T [ h i h k e ik ]))<label>(1)</label></formula><p>It is interesting to note that, for each node, the aggregated features include not only the neighbors' but also the ones of itself. Without edge features, the problem can be solved by adding an identity matrix to the adjacency matrix. However, the introduction of edge features makes it more difficult.</p><p>In our work, we use a tricky method by adding virtual featured self-loops to the graph. If a node does not have an edge that connected itself, a virtual self-loop will be attached to it. In particular, for every virtual self-loop, its features will be computed as an average of all its adjacent edges' features in each dimension as a compromise. All these operations should be done before being fed to the model.</p><p>After acquiring the normalized attention weights for each neighborhood, we can perform a weighted sum on these neighbor node features. In addition, a non-linearity σ will be applied to these summation results. The final results, which is also the outputs of this node attention block, can be expressed as:</p><formula xml:id="formula_4">h ′ i = σ( j∈Ni α ij h j )<label>(2)</label></formula><p>It should be noticed that we only aggregate the node features to generate the new set of node features. The edge features only play a part in weight computing but not a part of the new node features. It is for the clarity and symmetry of the model that we design such a strategy. If we merge edge features into nodes in each iteration, all the features may tangle up together and make the network more complicated and confusing. In fact, we also produce the set of edge-integrated node features H m in the node attention block. For each node i, we generate its new edge-integrated features as follows:</p><formula xml:id="formula_5">m i = σ( j∈Ni α ij ( h j e ij ))<label>(3)</label></formula><p>However, these features will only be used in the last-level merge layer to achieve a multi-scale concatenation. They will never be passed to the next EGAT layer as the inputs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Edge Attention Block</head><p>The node features can update themselves periodically in node attention blocks to acquire high-level features, whereas it is unreasonable to reuse the original low-level edge features during the weight computation. Besides, we also need high-level edge features to keep a balance of importance between nodes and edges. Thus, we proposed edge attention blocks, each of which accepts a set of node features, H, and a set of edge features, E, and produces E ′ , a new set of edge features.</p><p>A natural idea to realize such blocks is to update each edge's features by aggregating adjacent edges' features. In undirected graphs, we consider two edges are adjacent only if two edges have at least one common vertex. To achieve the aggregation, we adopt a tricky approach in our work by first switching the roles of nodes and edges in the graph. A similar concept on directed graphs has been proposed by Chen et al. <ref type="bibr" target="#b2">[3]</ref> for community detection. To achieve this, we create a new graph based on the original graph, whose nodes and edges are the edges and nodes of the original one, respectively. The transformation of the graph and the matrices structured by us are illustrated in Figure <ref type="figure" target="#fig_2">2</ref> (right).</p><p>The inputs of node and edge features are organized in the same sequential form. Thanks to the symmetric design, we can easily perform the attention mechanism on the new graph because the node feature set can be converted into the adjacency form by using M H , the node mapping matrix, with no difficulty. For each edge p, the normalized attention weight of edge q can be expressed as:</p><formula xml:id="formula_6">β pq = exp(LeakyReLU( b T [ e p e q h pq ])) k∈Np exp(LeakyReLU( b T [ e p e k h pk ]))<label>(4)</label></formula><p>where N p is the first-order neighbor set of edge p (including p), and b is a weight vector with a size of R 2F ′ E +F ′ H . One noteworthy point is that, when we compute the attention weight of an arbitrary edge and the edge itself, it is no middle node between the two edges. In our experiments, we logically create an empty node between the two edges, by padding all the features of this virtual node as zeros. Like node features, the computing of new set of edge features can be represented as:</p><formula xml:id="formula_7">e ′ p = σ( q∈Np β pq e q )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">EGAT Architecture</head><p>In this section, we present EGATs, which are constructed by stacking several EGAT layers and appending a merge layer at the tail. The architecture of EGATs is illustrated by Figure <ref type="figure" target="#fig_1">1</ref> (b).</p><p>Multi-scale strategies have been widely used to aggregate hierarchical feature maps in CNN models. Xu et al. <ref type="bibr" target="#b23">[24]</ref> first introduced such strategies into GNNs and proposed jumping knowledge networks, which further improve the accuracy. Inspired by such works, we adopt a multi-scale merge strategy by adding a merge layer in EGATs. Unlike jump knowledge networks, we collect not only node features but also edge features. Edge features will be integrated into nodes in each EGAT layer, result in H m , which we mentioned in 4.2. All H m generated from different iterations will aggregate together using a concatenation operation. Besides, we adopt the multi-head attention in the merge layer to further stabilize the attention mechanism. Unlike GATs <ref type="bibr" target="#b20">[21]</ref>, our multi-head attention is performed on the unity of all EGAT layers rather than a single layer. K independent multi-scale edge-integrated features would be computed and merged, resulting in the feature representation as follows:</p><formula xml:id="formula_8">h * i = K k=1 ( L l=1 m l,k i )<label>(6)</label></formula><p>where L indicates the number of the EGAT layers, and m l,k i represents the edge-integrated node features of node i produced in iteration l of the group k. To obtain a more refined representation, we apply a one-dimensional convolution to the results as a linear transformation and a non-linearity. For node classification tasks, a softmax function will be applied in the end to generate predicted labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We experientially assess the efficiency of EGAT by performing comparative evaluation against stateof-the-art approaches on several node classification datasets, which include both node-sensitive and edge-sensitive graphs. Besides, some additional analyses are also included in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We conduct our experiments on five node classification tasks containing both node-sensitive and edge-sensitive datasets. The former are graphs whose node features highly correlate with node labels, while the latter are those whose edges possess a dominant position. Such a division is somewhat relative, and it does not mean the features in a weak status have no contributions to the final results.</p><p>Node-Sensitive Graphs. Three real-world node classification datasets, which include Cora, Citeseer, and Pubmed <ref type="bibr" target="#b18">[19]</ref>, are utilized in our experiments for node-sensitive graph learning. Such datasets are citation networks and have been widely used in graph learning research works as standard benchmarks. Notably, these datasets are undirected and do not have edge features within the graphs. For a fair comparison, in our work, we adopt the same dataset splits used in papers of GCNs <ref type="bibr" target="#b9">[10]</ref> and GATs <ref type="bibr" target="#b20">[21]</ref>.</p><p>Edge-Sensitive Graphs. We derive two trading networks, Trade-B and Trade-M, to test the effectiveness of our models on edge-sensitive graphs. The two datasets are financial-collaborative and refer to real-world trading records. For confidentiality, we cleaned and extracted some distinct patterns of abnormal behaviors from the original data provided by a bank and regenerated them as new datasets. In these datasets, each node represents a customer, with an attribute indicating the risk level of it. The edges, however, represent the relations among customers, whose features contain the number and total amount of recent transactions. Trade-B is a binary classification dataset, which possesses 3907 nodes (97 of them are labeled) and 4394 edges. Trade-M, however, is ternary classified, with 4431 nodes (139 of them are labeled) and 4900 edges. For both datasets, we separated the labeled nodes into three parts, for training, validation, and test, with a ratio of 3:1:1. The two datasets are directed initially; however, to make it suitable for EGATs, we converted them into an undirected form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>For all the experiments, we implement EGATs based on the Pytorch framework <ref type="bibr" target="#b14">[15]</ref>. Because of the large memory usage for both adjacency and mapping matrices, we convert them into sparse forms to reduce the memory requirement and computational complexity during the learning process. The experimental setup for node-sensitive and edge-sensitive graphs are described as follows.</p><p>Node-Sensitive Graphs. Because all three citation networks do not possess even an edge feature, we generate one weak topological feature for each edge, by enumerating the numbers of its adjacent edges. In those experiments, we adopt an EGAT model with L = 2 and K = 8, where L and K represent the number of the EGAT layers and the attention heads. For simplicity, we use the same numbers of the output features for every EGAT layer, where F ′ H = 8 and F ′ E = 4, for nodes and edges separately. A one-dimensional convolution operation are performed in the merge layer to produce C features (where C is the number of classes), followed by a softmax function. To improve accuracy, some techniques like dropout <ref type="bibr" target="#b19">[20]</ref> and L 2 regularization are also used in EGATs. All these experiments, but Pubmed, were run on a machine with two GPUs of Geforce RTX 1080 Ti. Because of a larger requirement on video memory, Pubmed was run on Tesla V100 instead.</p><p>Edge-Sensitive Graphs. Trade-B and Trade-M are the two edge-sensitive benchmarks used in our experiments. As we mentioned above, some virtual self-loops are added to the graphs before the training. For those datasets, we apply an EGAT model whose L = 2 and K = 8, with the same output feature dimension for each EGAT layer. Differing from the node experiments, we use three kinds of combinations of F ′ H and F ′ E here with different ratios, which can be listed as 8:4, 6:6, 4:8, respectively. Besides, all other details of this model are similar to those used for node-sensitive graph learning. All these experiments were run on a machine with two GPUs of Geforce RTX 1080 Ti.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>For the node-sensitive tasks, we report the classification accuracy on the test nodes after 10 runs, which are listed in Table <ref type="table" target="#tab_0">1</ref>. We compare our results against several strong baselines and state-ofthe-art approaches proposed in previous works. In particular, we re-implement a two-layer GAT model <ref type="bibr" target="#b20">[21]</ref> by PyTorch, namely SP-GAT*, with F ′ , the number of hidden units, equals to 8. For a fair comparison, SP-GAT* accepts the same sparse representations of matrices used in our model.</p><p>The results show that EGATs are highly competitive against the state-of-the-art models on such nodesensitive graphs. We notice that there is a slight decrease for both Cora and Citeseer compared with SP-GAT*, which may be caused by the introduction of edge features. Since we generate the feature for each edge with the number of its adjacent edges, some interference may occur if these features are kind of useless. However, those negative effects are quite insignificant. Thanks to the symmetrical design, EGATs can adjust themselves during the learning and put more concentration on these useful features and produce acceptable results. In a word, EGATs can achieve high accuracy in nodesensitive classification tasks, surpassing the performance of most state-of-the-art approaches.</p><p>For the edge-sensitive tasks, we report the mean classification accuracy on test nodes after 10 runs, and apply SP-GAT* and its variants to the benchmarks as comparisons. For SP-GAT*, we only feed the original node features as inputs. To ensure fairness, we further create three variants of SP-GAT*, by aggregating edge features into nodes as node features in advance using different functions, including sum, average, and max pooling. Besides, we evaluate the accuracy by comparing three EGATs with different ratios of F ′ H and F ′ E . The comparative results are listed in Table <ref type="table" target="#tab_1">2</ref>. EGATs show an incredible performance from the table, which is streets ahead of other approaches on both two datasets. For Trade-B and Trade-M, the best classification accuracy of EGATs can reach 92.0% and 85.4%. It is also interesting to observe that different datasets may possess different characteristics. For example, the edge features within Trade-B can be better expressed by summing up together. On the contrary, the average operation may be more applicable to representing the edge features in Trade-M. Despite their traits, EGATs can achieve high accuracy against these baselines on all these datasets, which means that EGATs can learn these characteristics of graphs spontaneously. To our best knowledge, there are no existing approaches that can process these kinds of graphs effectively.</p><p>We also investigate the effects of the ratio of F ′ H and F ′ E on accuracy. According to the results, if the edge features may play a more important role than node ones, we recommend choosing a small or balance value of F ′ H : F ′ E so that edges will have a higher chance to show themselves. However,  <ref type="bibr" target="#b0">[1]</ref> 59.5% 60.1% 70.7% SemeiEmb <ref type="bibr" target="#b22">[23]</ref> 59.0% 59.6% 71.7% LP <ref type="bibr" target="#b27">[28]</ref> 68.0% 45.3% 63.0% DeepWalk <ref type="bibr" target="#b15">[16]</ref> 67.2% 43.2% 65.3% ICA <ref type="bibr" target="#b11">[12]</ref> 75.1% 69.1% 73.9% Planetoid <ref type="bibr" target="#b24">[25]</ref> 75.7% 64.7% 77.2% Chebyshev <ref type="bibr" target="#b4">[5]</ref> 81.2% 69.8% 74.4% GCN <ref type="bibr" target="#b9">[10]</ref> 81.5% 70.3% 79.0% Monet <ref type="bibr" target="#b13">[14]</ref> 81 there may be exceptions in some cases. For example, the accuracy decreased to 78.2% when we select a high F ′ E in Trade-M. Due to the mutual effect of the features of nodes and edges, the model becomes complex, and it is hard to consider both features separately. So, if better performance is demanded, it is better to adjust these hyper-parameters several times to choose the most suitable ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Complexity Analysis</head><p>The complexity analysis of EGATs is given in this subsection. Since the constructions of adjacency and mapping matrices occur in a pre-processing step rather than the critical path, we merely ignore them and concentrate on the learning process. In EGATs, the matrix multiplication is the most time-consuming operation, which can be regarded as the entry point. Assume that now we have a graph with N nodes and E edges. In each node attention block, we introduce the edge features by applying a mapping transformation, which is actually matrix multiplication. It can be easily proved that the computation complexity of multiplying an m × n sparse matrix A and an n × p dense matrix B can be reduced to O(cp), where c denotes to the number of non-zero elements in A. Thus, the complexity of such a transformation is O(E), for the reason that p, the number of features, can be seen as a constant. Because the complexity of GATs is no less than O(E) in each iteration, so the introduction of edges in node attention blocks will not significantly increase the complexity.</p><p>Things may be a little different occurring in edge attention blocks. Consider a graph with one central node and K neighbors. Because every two edges are neighbors, when we switch the roles of nodes and edges, the number of edges in the new graph, E * , is on the order of O(K 2 ). When we extend this conclusion to a regular graph with N nodes and E edges, the number of non-zero elements in the converted mapping matrix can be represented as O( N i=1 d 2 i ), where d i indicate the degree of each node in the graph. Thus, the complexity of edge attention block is in the same order. However, based on our experimental results, the delay of EGATs is quite acceptable on the benchmarks and those graphs with a similar scale. Besides, all the test datasets except Pubmed can run on Geforce RTX 1080 Ti without exceeding the memory limit. If someone has higher performance requirements, some modifications could be made in edge attention block. For example, each edge can regard the two adjacent nodes as virtual edges and only aggregate the edge part of H m during the learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We proposed edge-featured graph attention networks (EGATs), novel edge-integrated graph neural networks that performed on graphs with node and edge features. We incorporate edge features into GNNs and present a symmetrical approach to exploit them. To our best knowledge, we are the first to incorporate edges as node-equivalent entities, point out graphs' different preferences for features, and handle them spontaneously. The results demonstrate that EGATs have successfully achieved state-of-the-art performance on node classification tasks, especially for edge-sensitive datasets.</p><p>There are some potential improvements to EGATs that could be addressed as future work. In EGATs, we update each edge's features by aggregating its neighbors' information. However, the number of neighbors of each edge may be huge. Despite transforming matrices into sparse forms, the models still need a large memory usage when it performed on large-scale graphs. Thus, it is better to find an improved way to reduce the models' memory requirement. Besides, EGATs do not naturally support directed graphs as well as multi-graphs. We intend to achieve these extensions further on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>In this work, edge-featured graph attention networks (EGATs), novel edge-integrated graph neural networks, were proposed to perform node classification on those graphs with node and edge features. This work has the following potential positive impact on society. First, the models proposed in this paper are kind of versatile and have a broad application foreground in many fields. For example, the models can be applied in the financial sector, as an aid to finding those people who may have a suspicion in financial fraud, money laundering, etc. Second, given the absence of edge features in traditional GNN approaches, our work may attract the attention of other researchers, spawning a series of related research, to enhance further the basic framework of graph neural networks from a theoretical level. At the same time, this works may have some negative consequences with a small probability. Because there are very few works trying to exploit edge features in graph neural networks, this field is still kind of immature and receives little attention. Therefore, the negative impact of our models on society are quite unclear and need further exploration. Besides, we should be cautious about the result of the failure of the system. It should be noticed that the prediction results of our models should only be regarded as an auxiliary reference rather than a definite truth. Users of the models should perform a second manual verification by their own to ensure the authenticity of the results. We will not be responsible for the negative effects caused by the wrong prediction of EGATs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: (a) An illustration of one EGAT layer. It accepts H and E as inputs, and produces two sets of new features. A H and A E are adjacency matrices, while M H and M E are mapping matrices for nodes and edges, respectively. The H m is edge-integrated node features generated by node attention block, which will only be used in the merge layer; (b) The architecture of EGATs. The model is constructed by several EGAT layers and a merge layer. The node and edge features generated from each iteration will be concatenated in the merge layer to achieve a multi-scale feature fusion. For convenience, the adjacency and mapping matrices are not shown in this figure, as well as the multi-head attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: (a) An illustration of a regular graph; (b) An example of the mapping transformation performed on (a). M E is the edge mapping matrix in size of N × N × M , with its last dimension encoded in a one-hot scheme. For simplicity, we draw the matrix by replacing one-hot encoding vectors with non-zero indices. E is the edge feature set with a size of M × F ′ E . M E should first be reshaped to N 2 × M , and recover to N × N × F ′ E after multiplication. Right: (c) An example of graph transformation. The nodes and edges' roles are inversed in the new graph; (d) The adjacency matrix A E of the new graph, namely edge adjacency matrix. An identity matrix has been added to the matrix to pre-build the self-adjacent relations; (e) The mapping matrix M H of the new graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of the results on node classification accuracy, for Cora, Citeseer and Pubmed. SP-GAT* corresponds to the best result of GAT implemented by us with a sparse form.</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>MLP</cell><cell>55.1%</cell><cell>46.5%</cell><cell>71.4%</cell></row><row><cell>ManiReg</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Summary of the results on node classification accuracy, for Trade-B and Trade-M. The hyper-parameters h and e represent F ′ H and F ′ E used in our EGAT model, respectively.</figDesc><table><row><cell></cell><cell>.7%</cell><cell>-</cell><cell>78.8%</cell></row><row><cell>SP-GAT*</cell><cell>82.5±0.4%</cell><cell>70.8±0.5%</cell><cell>78.1±0.4%</cell></row><row><cell>EGAT (ours)</cell><cell>82.1±0.7%</cell><cell>70.3±0.5%</cell><cell>78.1±0.4%</cell></row><row><cell>Method</cell><cell>Trade-B</cell><cell></cell><cell>Trade-M</cell></row><row><cell>SP-GAT*</cell><cell>65.0%</cell><cell></cell><cell>46.4%</cell></row><row><cell>SP-GAT-sum*</cell><cell>85.0%</cell><cell></cell><cell>51.1%</cell></row><row><cell>SP-GAT-avg*</cell><cell>78.0%</cell><cell></cell><cell>71.4%</cell></row><row><cell>SP-GAT-max*</cell><cell>81.5%</cell><cell></cell><cell>65.7%</cell></row><row><cell>EGAT (h = 8, e = 4)</cell><cell>87.5%</cell><cell></cell><cell>84.3%</cell></row><row><cell>EGAT (h = 6, e = 6)</cell><cell>88.0%</cell><cell></cell><cell>85.4%</cell></row><row><cell>EGAT (h = 4, e = 8)</cell><cell>92.0%</cell><cell></cell><cell>78.2%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Preprint. Under review.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Supervised community detection with line graph neural networks</title>
		<author>
			<persName><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08415</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoderdecoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting edge features for graph neural networks</title>
		<author>
			<persName><forename type="first">Liyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9211" to="9219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graphstructured data</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName><forename type="first">Qing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning (ICML-03)</title>
				<meeting>the 20th International Conference on Machine Learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Disentangled graph convolutional networks</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4212" to="4221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><surname>Michael M Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning via semisupervised embedding</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08861</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep learning on graphs: A survey</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04202</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International conference on Machine learning (ICML-03)</title>
				<meeting>the 20th International conference on Machine learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
