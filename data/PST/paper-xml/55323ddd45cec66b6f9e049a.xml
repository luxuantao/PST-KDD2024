<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Mengyang</roleName><forename type="first">Li</forename><surname>Liu</surname></persName>
							<email>li2.liu@northumbria.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Multiview Alignment Hashing for Efficient Image Search</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Multiview Alignment Hashing for Efficient Image Search</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<email>ling.shao@northumbria.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Multiview Alignment Hashing for Efficient Image Search</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Digital Technologies</orgName>
								<orgName type="institution">Northumbria University</orgName>
								<address>
									<postCode>NE1 8ST</postCode>
									<settlement>Newcastle upon Tyne</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Digital Technologies</orgName>
								<orgName type="institution">Northumbria University</orgName>
								<address>
									<postCode>NE1 8ST</postCode>
									<settlement>Newcastle upon Tyne</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science and Digital Technologies</orgName>
								<orgName type="institution">Northumbria University</orgName>
								<address>
									<postCode>NE1 8ST</postCode>
									<settlement>Newcastle upon Tyne</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E8E4EBF92A68EB26352795BD1B4A8664</idno>
					<idno type="DOI">10.1109/TIP.2015.2390975</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2390975, IEEE Transactions on Image Processing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2390975, IEEE Transactions on Image Processing IEEE TRANSACTIONS ON IMAGE PROCESSING 2</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hashing</term>
					<term>Multiview</term>
					<term>NMF</term>
					<term>Alternate optimization</term>
					<term>Logistic regression</term>
					<term>Image similarity search</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hashing is a popular and efficient method for nearest neighbor search in large-scale data spaces, by embedding high-dimensional feature descriptors into a similarity-preserving Hamming space with a low dimension. For most hashing methods, the performance of retrieval heavily depends on the choice of the high-dimensional feature descriptor. Furthermore, a single type of feature cannot be descriptive enough for different images when it is used for hashing. Thus, how to combine multiple representations for learning effective hashing functions is an imminent task. In this paper, we present a novel unsupervised Multiview Alignment Hashing (MAH) approach based on Regularized Kernel Nonnegative Matrix Factorization (RKNMF), which can find a compact representation uncovering the hidden semantics and simultaneously respecting the joint probability distribution of data. Specifically, we aim to seek a matrix factorization to effectively fuse the multiple information sources meanwhile discarding the feature redundancy. Since the raised problem is regarded as nonconvex and discrete, our objective function is then optimized via an alternate way with relaxation and converges to a locally optimal solution. After finding the low-dimensional representation, the hashing functions are finally obtained through multivariable logistic regression. The proposed method is systematically evaluated on three datasets: Caltech-256, CIFAR-10 and CIFAR-20, and the results show that our method significantly outperforms the state-of-the-art multiview hashing techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>some tree-based search schemes are proposed to partition the data space via various tree structures. Among them, KD-tree and R-tree <ref type="bibr" target="#b5">[6]</ref> are successfully applied to index the data for fast query responses. However, these methods cannot operate with high-dimensional data and do not guarantee faster search compared to the linear scan. In fact, most of the vision-based tasks suffer from the curse of dimensionality problems <ref type="foot" target="#foot_0">1</ref> , because visual descriptors usually have hundreds or even thousands of dimensions. Thus, some hashing schemes are proposed to effectively embed data from a high-dimensional feature space into a similarity-preserving low-dimensional Hamming space where an approximate nearest neighbor of a given query can be found with sub-linear time complexity.</p><p>One of the most well-known hashing techniques that preserve similarity information is Locality-Sensitive Hashing (LSH) <ref type="bibr" target="#b6">[7]</ref>. LSH simply employs random linear projections (followed by random thresholding) to map data points close in a Euclidean space to similar codes. Spectral Hashing (SpH) <ref type="bibr" target="#b7">[8]</ref> is a representative unsupervised hashing method, in which the Laplace-Beltrami eigenfunctions of manifolds are used to determine binary codes. Moreover, principled linear projections like PCA Hashing (PCAH) <ref type="bibr" target="#b8">[9]</ref> has been suggested for better quantization rather than random projection hashing. Besides, another popular hashing approach, Anchor Graphs Hashing (AGH) <ref type="bibr" target="#b9">[10]</ref>, is proposed to learn compact binary codes via tractable low-rank adjacency matrices. AGH allows constant time hashing of a new data point by extrapolating graph Laplacian eigenvectors to eigenfunctions. More relevant hashing methods can be seen in <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>.</p><p>However, single-view hashing is the main topic on which the previous exploration of hashing methods focuses. In their architectures, only one type of feature descriptor is used for learning hashing functions. In practice, to make a more comprehensive description, objects/images are always represented via several different kinds of features and each of them has its own characteristics. Thus, it is desirable to incorporate these heterogenous feature descriptors into learning hashing functions, leading to multi-view hashing approaches. Multiview learning techniques <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> have been well explored in the past few years and widely applied to visual information fusion <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Recently, a number of multiview hashing methods have been proposed for efficient similarity search, such as Multi-View Anchor Graph Hashing (MVAGH) <ref type="bibr" target="#b20">[21]</ref>, Sequential Update for Multi-View Spectral Hashing (SU-MVSH) <ref type="bibr" target="#b21">[22]</ref>, Multi-View Hashing (MVH-CS) <ref type="bibr" target="#b22">[23]</ref>, Composite Hashing with Multiple Information Sources (CHMIS) <ref type="bibr" target="#b23">[24]</ref> and Deep Multi-view Hashing (DMVH) <ref type="bibr" target="#b24">[25]</ref>. These methods mainly depend on spectral, graph or deep learning techniques to achieve data structure preserving encoding. Nevertheless, the hashing purely with the above schemes are usually sensitive to data noise and suffering from the high computational complexity.</p><p>The above drawbacks of prior work motivate us to propose a novel unsupervised mulitiview hashing approach, termed Multiview Alignment Hashing (MAH), which can effectively fuse multiple information sources and exploit the discriminative low-dimensional embedding via Nonnegative Matrix Factorization (NMF) <ref type="bibr" target="#b25">[26]</ref>. NMF is a popular method in data mining tasks including clustering, collaborative filtering, outlier detection, etc. Unlike other embedding methods with positive and negative values, NMF seeks to learn a nonnegative partsbased representation that gives better visual interpretation of factoring matrices for high-dimensional data. Therefore, in many cases, NMF may be more suitable for subspace learning tasks, because it provides a non-global basis set which intuitively contains the localized parts of objects <ref type="bibr" target="#b25">[26]</ref>. In addition, since the flexibility of matrix factorization can handle widely varying data distributions, NMF enables more robust subspace learning. More importantly, NMF decomposes an original matrix into a part-based representation that gives better interpretation of factoring matrices for non-negative data. When applying NMF to multiview fusion tasks, a partbased representation can reduce the corruption between any two views and gain more discriminative codes.</p><p>To the best of our knowledge, this is the first work using NMF to combine multiple views for image hashing. It is worthwhile to highlight several contributions of the proposed method:</p><p>• MAH can find a compact representation uncovering the hidden semantics from different view aspects and simultaneously respecting the joint probability distribution of data. • To solve our nonconvex objective function, a new alternate optimization has been proposed to get the final solution.</p><p>• We utilize multivariable logistic regression to generate the hashing function and achieve the out-of-sample extension.</p><p>The rest of this paper is organized as follows. In Section 2, we give a brief review of NMF. The details of our method are described in Section 3. Section 4 reports the experimental results. Finally, we conclude this paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. A BRIEF REVIEW OF NMF</head><p>In this section, we mainly review some related algorithms, focusing on Nonnegative Matrix Factorization (NMF) and its variants. NMF is proposed to learn the nonnegative parts of objects. Given a nonnegative data matrix</p><formula xml:id="formula_0">X = [x 1 , • • • , x N ] ∈ R D×N ≥0 , each column of X is a sample data. NMF aims to find two nonnegative matrices U ∈ R D×d ≥0 and V ∈ R d×N ≥0</formula><p>with full rank whose product can approximately represent the original matrix X, i.e., X ≈ U V . In practice, we always have d &lt; min(D, N ). Thus, we minimize the following objective function</p><formula xml:id="formula_1">L N M F = X -U V 2 , s.t. U, V ≥ 0,<label>(1)</label></formula><p>where • is Frobenius norm. To optimize the above objective function, an iterative updating procedure was developed in <ref type="bibr" target="#b25">[26]</ref> as follows:</p><formula xml:id="formula_2">V ij ← (U T X) ij (U T U V ) ij V ij , U ij ← (XV T ) ij (U V V T ) ij U ij ,<label>(2)</label></formula><p>and normalization</p><formula xml:id="formula_3">U ij ← U ij i U ij .<label>(3)</label></formula><p>It has been proved that the above updating procedure can find the local minimum of L N M F . The matrix V obtained in NMF is always regarded as the low-dimensional representation while the matrix U denotes the basis matrix. Furthermore, there also exists some variants of NMF. Local NMF (LNMF) <ref type="bibr" target="#b26">[27]</ref> imposes a spatial localized constraint on the bases. In <ref type="bibr" target="#b27">[28]</ref>, sparse NMF was proposed and later, NMF constrained with neighborhood preserving regularization (NPNMF) <ref type="bibr" target="#b28">[29]</ref> was developed. Besides, researchers also proposed graph regularized NMF (GNMF) <ref type="bibr" target="#b29">[30]</ref>, which effectively preserves the locality structure of data. Beyond these methods, <ref type="bibr" target="#b30">[31]</ref> extends the original NMF with the kernel trick as kernelized NMF (KNMF), which could extract more useful features hidden in the original data through some kernel-induced nonlinear mappings. Moreover, it can deal with data where only relationships (similarities or dissimilarities) between objects are known. Specifically, in their work, they addressed the matrix factorization by K ≈ U V , where K is the kernel matrix instead of the data matrix X, and (U, V ) are similar with standard NMF. More related to our work, a multiple kernels NMF (MKNMF) <ref type="bibr" target="#b31">[32]</ref> approach was proposed, where linear programming is applied to determine the combination of different kernels.</p><p>In this paper, we present a Regularized Kernel Nonnegative Matrix Factorization (RKNMF) framework for hashing, which can effectively preserve the data intrinsic probability distribution and simultaneously reduce the redundancy of lowdimensional representations. Rather than locality-based graph regularization, we measure the joint probability of pairwise data by the Gaussian function, which is defined over all the potential neighbors and has been proved to effectively resist data noise <ref type="bibr" target="#b32">[33]</ref>. This kind of measurement is capable to capture the local structure of the high-dimensional data while also revealing global structure such as the presence of clusters at several scales. To the best of our knowledge, this is the first time that NMF with multiview hashing has been successfully applied to feature embedding for large-scale similarity search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MULTIVIEW ALIGNMENT HASHING</head><p>In the section, we introduce our new Multiview Alignment Hashing approach, referred as MAH. Our goal is to learn a hash embedding function, which fuses the various alignment representations from multiple sources while preserving the high-dimensional joint distribution and obtaining the orthogonal bases simultaneously during the RKNMF. Originally, we need to find the binary solution which, however, is first relaxed to a real-valued range so that a more suitable solution can be gained. After applying the alternate optimization, we convert the real-valued solutions into binary codes. Fig. <ref type="figure" target="#fig_0">1</ref> shows the outline of the proposed MAH approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Objective Function</head><p>Given the i-th view training data</p><formula xml:id="formula_4">X (i) = [x (i) 1 , • • • , x (i) N ] ∈ R Di×N ,</formula><p>we construct the corresponding N × N kernel matrix K i using the Heat Kernel formulation:</p><formula xml:id="formula_5">K i (x (i) p , x (i) q ) = exp -||x (i) p -x (i) q || 2 2τ 2 i , ∀p, q,</formula><p>where τ i is the related scalable parameter. Actually, our approach can work with any legitimate kernel. Without loss of generality, one of the most popular kernels, Heat Kernel, is used here in the purposed method. Our discussion in further section will only focus on the Heat Kernel.</p><p>Then multiple kernel matrices from each view data</p><formula xml:id="formula_6">{K 1 , • • • , K n } are computed and K i ∈ R N ×N ≥0</formula><p>, ∀i. We further define the fusion matrix</p><formula xml:id="formula_7">K = n i=1 α i K i , subject to n i=1 α i = 1, α i ≥ 0, ∀i.</formula><p>To obtain a meaningful low-dimensional matrix factorization, we then set a constraint for the binary representation</p><formula xml:id="formula_8">V = [v 1 , • • • , v N ]</formula><p>as the similarity probability regularization, which is utilized to preserve the data distribution in the intrinsic objective space. The optimization is expressed as:</p><formula xml:id="formula_9">arg min V ∈{0,1} d×N 1 2 p,q W (i) pq v p -v q 2 ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_10">W (i) pq = Pr(x (i) p , x<label>(i)</label></formula><p>q ) is the symmetric joint probability between x (i) p and x (i) q in the i-th view feature space. In this paper, we use a Gaussian function<ref type="foot" target="#foot_1">2</ref>  <ref type="bibr" target="#b32">[33]</ref> to measure it:</p><formula xml:id="formula_11">Pr(x (i) p , x (i) q ) =    exp -x (i) p -x (i) q 2 /2σ 2 i k =l exp -x (i) k -x (i) l 2 /2σ 2 i , if p = q 0, if p = q (5) where σ i is the Gaussian smooth parameter. x (i) p -x (i) q</formula><p>is always measured by the Euclidean distance. To refer the framework in <ref type="bibr" target="#b29">[30]</ref>, the similarity probability regularization for the i-th view can be reduced to</p><formula xml:id="formula_12">arg min V ∈{0,1} d×N Tr(V L i V T ),<label>(6)</label></formula><p>where</p><formula xml:id="formula_13">L i = D (i) -W (i) is the Laplacian matrix, W (i) = W (i) pq</formula><p>∈ R N ×N is the symmetric similarity matrix and D (i) is a diagonal matrix with its entries</p><formula xml:id="formula_14">D (i) pp = p W (i)</formula><p>pq . In addition, to reduce the redundancy of subspaces and obtain the compact bases in NMF simultaneously, we hope the basis matrix U of NMF should be as orthogonal as possible to minimize the redundancy, i.e., U T U -I = 0. Here we also minimize U T U -I 2 and make basis U near-orthogonal since this objective function can be fused into the computation of the L 2 -norm loss function of NMF. Finally, we combine the above-mentioned two constraints for U and V and construct our optimization problem as follows:</p><formula xml:id="formula_15">arg min U,V,αi n i=1 α i K i -U V 2 + γ n i=1 α i Tr(V L i V T )<label>(7)</label></formula><formula xml:id="formula_16">+ η U T U -I 2 , s.t. V ∈ {0, 1} d×N , U, V ≥ 0, n i=1 α i = 1, α i ≥ 0, ∀i,</formula><p>where γ and η are two positive coefficients balancing the tradeoff between the NMF approximation error and additional constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Alternate Optimization via Relaxation</head><p>In this section, we first relax the discreteness condition to the continuous condition, i.e., we relax the binary domain V ∈ {0, 1} d×N in Eq. ( <ref type="formula" target="#formula_15">7</ref>) to the real number domain V ∈ R d×N but keep the NMF requirements (nonnegative conditions). To the best of our knowledge, there is no direct way to obtain its global solution for a linearly constrained nonconvex optimization problem. Thus, in this paper the optimization procedure is implemented as an iterative procedure and divided into two steps by optimizing (U, V ) and α = (α 1 , • • • , α n ) alternately <ref type="bibr" target="#b33">[34]</ref>. For each step, one of (U, V ) and α is optimized while the other is fixed and at the next step, we switch (U, V ) and α. The iteration procedure stops until it converges. a) Optimizing (U, V ): By first fixing α, we substitute K = n i=1 α i K i and L = n i=1 α i L i . A Lagrangian multiplier function is introduced for our problem as:</p><formula xml:id="formula_17">L 1 (U, V, Φ, Ψ) = K -U V 2 + γ Tr(V LV T ) + η U T U -I 2 + Tr(ΦU T ) + Tr(ΨV T ),<label>(8)</label></formula><p>where Φ and Ψ are two matrices in which all their entries are the Lagrangian multipliers to constrain U, V ≥ 0 respectively. Then we let the partial derivatives of L 1 with respect to U and V be zeroes, i.e., ∇ U,V L 1 = 0, we obtain</p><formula xml:id="formula_18">∇ U L 1 = 2 -KV T + U V V T + 2ηU U T U -2ηU + Φ = 0, (9) ∇ V L 1 = 2 -U T K + U T U V + γV L + Ψ = 0. (10)</formula><p>Using the Karush-Kuhn-Tucker (KKT) conditions <ref type="bibr" target="#b34">[35]</ref> since our objective function satisfies the constraint qualification condition, we have the complementary slackness conditions:</p><formula xml:id="formula_19">Φ ij U ij = 0 and Ψ ij V ij = 0, ∀i, j.</formula><p>Multiplying U ij and V ij on the corresponding entries of Eq. ( <ref type="formula">9</ref>) and ( <ref type="formula">10</ref>), we have the following equations for U ij and V ij Hence, similar to the standard procedure of NMF <ref type="bibr" target="#b25">[26]</ref>, we get our updating rules</p><formula xml:id="formula_20">-KV T + U V V T + 2ηU U T U -2ηU ij U ij = 0, -U T K + U T U V + γV L ij V ij = 0.</formula><formula xml:id="formula_21">U ij ← KV T + 2ηU ij (U V V T + 2ηU U T U ) ij U ij ,<label>(11)</label></formula><formula xml:id="formula_22">V ij ← U T K + γV W ij (U T U V + γV D) ij V ij . (<label>12</label></formula><formula xml:id="formula_23">)</formula><p>where</p><formula xml:id="formula_24">D = n i=1 α i D (i) and W = n i=1 α i W (i)</formula><p>. This allocation is to guarantee that all the entries of U and V are nonnegative. U is also needed to be normalized as Eq. ( <ref type="formula" target="#formula_3">3</ref>). The convergence of U is based on <ref type="bibr" target="#b35">[36]</ref> and the convergence of V is based on <ref type="bibr" target="#b29">[30]</ref>. Similar to <ref type="bibr" target="#b36">[37]</ref>, they have proven that after each update of U or V , the objective function is monotonically nonincreasing. b) Optimizing α: Due to the quadratic norm in our objective function, the procedure for optimizing α is different from the general linear programming. For fixed U and V , omitting the irrelevant terms, we define the Lagrangian function </p><formula xml:id="formula_25">L 2 (α, λ, β) = n i=1 α i K i -U V 2 + γ n i=1 α i Tr(V L i V T ) + λ( n i=1 α i -1) + βα T ,<label>(13)</label></formula><formula xml:id="formula_26">α i K i -U V )K j +γ Tr(V L j V T ) + λ + β j 1≤j≤n = 0, (<label>14</label></formula><formula xml:id="formula_27">)</formula><formula xml:id="formula_28">∇ λ L 2 = n i=1 α i -1 = 0,<label>(15)</label></formula><formula xml:id="formula_29">∇ β L 2 = α ≥ 0.<label>(16)</label></formula><p>And we also have complementary slackness conditions</p><formula xml:id="formula_30">β j α j = 0, j = 1, • • • , n.<label>(17)</label></formula><p>Suppose α j = 0 for some j, specifically denote J = {j|α j = 0}, then the optimal solution would include some zeroes. In this case, there is no difference with the optimization procedure for minimizing j ∈J α j K j -U V 2 . Without loss of generality, we suppose α j &gt; 0, ∀j. Then β = 0. From Eq. ( <ref type="formula" target="#formula_26">14</ref>), we have</p><formula xml:id="formula_31">n i=1 α i Tr(K i K j ) = Tr(U V K j ) -γ Tr(V L j V T )/2 -λ/2, (<label>18</label></formula><formula xml:id="formula_32">) for j = 1, • • • , n.</formula><p>If we transform the above equations to the matrix form and denote</p><formula xml:id="formula_33">T j = Tr(U V K j ) -γ Tr(V L j V T )/2, we obtain      Tr(K 2 1 ) Tr(K 2 K 1 ) • • • Tr(K n K 1 ) Tr(K 1 K 2 ) Tr(K 2 2 ) • • • Tr(K n K 2 ) . . . . . . . . . . . . Tr(K 1 K n ) Tr(K 2 K n ) • • • Tr(K 2 n )           α 1 α 2 . . . α n      =      T 1 -λ/2 T 2 -λ/2 . . . T n -λ/2      . (<label>19</label></formula><formula xml:id="formula_34">)</formula><p>Let us denote Eq. ( <ref type="formula" target="#formula_33">19</ref>) by Aα T = B. Note that matrix A is actually the Gram matrix of K i based on Frobenius inner product</p><formula xml:id="formula_35">K i , K j = Tr(K i K T j ) = Tr(K i K j ). Let M = (vec(K 1 ), • • • , vec(K n )), where vec(K i ) is the vectorization of K i , then A = M T M . And rank(A) = rank(M T M ) = rank(M ) = n under the assumption that the kernel matrices K 1 , • • • , K n from n different views are linearly independent.</formula><p>Combined with Eq. ( <ref type="formula" target="#formula_29">16</ref>) and eliminated λ by subtracting other rows from the first row in Eq. ( <ref type="formula" target="#formula_33">19</ref>), we can obtain the linear equations demonstrated in Eq. ( <ref type="formula" target="#formula_46">20</ref>) in a matrix form.</p><p>We denote Eq. ( <ref type="formula" target="#formula_46">20</ref>) by Aα T = B. According to the variety of different views, 1 = (1, • • • , 1) and all of the rows in A are linearly independent. Then we have rank( A) = rank(A) -1 + 1 = n. Thus, the inverse of A exists and we have</p><formula xml:id="formula_36">α T = A -1 B.</formula><p>c) Global Convergence: Let us denote our original objective function in Eq. ( <ref type="formula" target="#formula_15">7</ref>) by L(U, V, α). Then for any m-th step, the alternate iteration procedure will be:</p><formula xml:id="formula_37">(U (m) , V (m) ) ← arg min U,V L(U, V, α (m-1) ) and α (m) ← arg min α L(U (m) , V (m) , α).</formula><p>Thus we have the following inequality:</p><formula xml:id="formula_38">L(U (m-1) , V (m-1) , α (m-1) ) ≥ L(U (m) , V (m) , α (m-1) ) ≥ L(U (m) , V (m) , α (m) ) ≥ L(U (m+1) , V (m+1) , α (m) ) ≥ L(U (m+1) , V (m+1) , α (m+1) ) ≥ • • • .</formula><p>In other words, L(U (m) , V (m) , α (m) ) are monotonic nonincreasing as m → ∞. Note that we have L(U, V, α) ≥ 0. Then the alternate iteration converges.</p><p>Following the above optimization procedure on (U, V ) and α, we compute them alternately by fixing another until the object function converges. In practice, we terminate the iteration process when the difference |L(U (m) , V (m) , α (m) ) -L(U (m-1) , V (m-1) , α (m-1) )| is less than a small threshold or the number of iterations reaches a maximum value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Hashing Function Generation</head><p>From the above section, we can easily compute the weight vector α = (α 1 , • • • , α n ) and further get the fused kernel matrix K and the combined joint probability Laplacian matrix L. Thus, from Eq. ( <ref type="formula" target="#formula_21">11</ref>) and Eq. ( <ref type="formula" target="#formula_22">12</ref>) we can obtain the multiview RKNMF bases U ∈ R N ×d and the lowdimensional representation V ∈ R d×N , where d D i , i = 1, • • • , n. We now convert the above low-dimensional real-valued representation from V = [v 1 , • • • , v N ] into binary codes via thresholding: if the l-th element of v p is larger than the specified threshold, the mapped bit v(l) p = 1; otherwise v(l) p = 0, where p = 1,</p><formula xml:id="formula_39">• • • , N and l = 1, • • • , d.</formula><p>According to <ref type="bibr" target="#b37">[38]</ref>, a well-designed semantic hashing should also be entropy-maximized to ensure efficiency. Moreover, from the information theory <ref type="bibr" target="#b38">[39]</ref>, the maximal entropy of a source alphabet is attained by having a uniform probability distribution. If the entropy of codes over the bit-string is small, it means that documents are mapped to only a small part of codes (hash bins), thereby rendering the hash table inefficient. In our method, we set the threshold for each element in v(l) p as the median value of v</p><formula xml:id="formula_40">(l)</formula><p>p , which can meet the entropy maximizing criterion mentioned above. Therefore, the p-th bitstring will be 1 for half of it and 0 for the other half. The above scheme gives each distinct binary code roughly equal probability of occurring in the data collection, hence achieves the best utilization of the hash table. The binary code can be represented as:</p><formula xml:id="formula_41">V = [v 1 , • • • , vN ],</formula><p>where vp ∈ {0, 1} d and p = 1, • • • , N . A similar scheme has been used in <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>.</p><p>Up to now, this would only tell us how to compute the hash code of items in the training set, while for a new coming sample we cannot explicitly find the corresponding hashing function. Inspired by <ref type="bibr" target="#b41">[42]</ref>, we determine our out-of-sample extension using a regression technique with multiple variables. In this paper, instead of applying linear regression as mentioned in <ref type="bibr" target="#b41">[42]</ref>, the binary coding environment forces us to implement our tasks via the logistic regression <ref type="bibr" target="#b42">[43]</ref>, which can be treated as a type of probabilistic statistical classification model. The corresponding probabilities describe the possibilities of binary responses. In n distributions</p><formula xml:id="formula_42">Y i |X i ∼ Bernoulli(p i ), i = 1, • • • , n, for the function Pr(Y i = 1|X i = x) := h θ (x)</formula><p>with the parameter θ, the likelihood function is</p><formula xml:id="formula_43">n i=1 Pr(Y i = y i |X i = x i ) = n i=1 h θ (x i ) yi (1 -h θ (x i )) 1-yi .</formula><p>According to the maximum log-likelihood criterion, we define our logistic regression cost function:</p><formula xml:id="formula_44">J(Θ) = - 1 N N p=1 vp , log(h Θ (v p )) + (1 -vp ), log(1 -h Θ (v p )) + ξ||Θ|| 2 ,<label>(21)</label></formula><p>where</p><formula xml:id="formula_45">h Θ (v p ) = 1 1 + e -(Θ T vp)i T 1≤i≤d is the regression function for each component in v p ; log(x) = (log(x 1 ), • • • , log(x n )) T for x = (x 1 , • • • , x n ) T ∈ R n ; •,</formula><p>• represents the inner product; Θ is the corresponding regression matrix with the size d × d; 1 is denoted as N × 1 all ones matrix and ξ is the parameter for regularization term ξ||Θ|| 2 in logistic regression, which is to avoid overfitting.</p><formula xml:id="formula_46">     Tr(K 2 1 ) -Tr(K 1 K n ) • • • Tr(K n K 1 ) -Tr(K 2 n ) . . . . . . . . . Tr(K 1 K n-1 ) -Tr(K 1 K n ) • • • Tr(K n K n-1 ) -Tr(K 2 n ) 1 • • • 1         α 1 . . . α n    =      T 1 -T n . . . T n-1 -T n 1      . (<label>20</label></formula><formula xml:id="formula_47">)</formula><p>Further aiming to minimize J(Θ), a standard gradient descent algorithm has been employed. According to <ref type="bibr" target="#b42">[43]</ref>, the updated equation with the learning rate r can be written as:</p><formula xml:id="formula_48">Θ t+1 = Θ t -r 1 N N p=1 (h Θ (v p ) -vp )v T p + ξ N Θ t . (<label>22</label></formula><formula xml:id="formula_49">)</formula><p>We stop the update iteration when the norm of difference between Θ t+1 and Θ t , ||Θ t+1 -Θ t || 2 , is less than an empirical small value (i.e., reach the convergence) and then output the regression matrix Θ.</p><p>In this way, given a sample, the related kernel matrices</p><formula xml:id="formula_50">{K new 1 , • • • , K new n</formula><p>} for each view are first computed via Heat Kernel, where K new i is a N ×1 matrix, ∀i. We then fuse these kernels with optimized weights α:</p><formula xml:id="formula_51">K new = n i=1 α i K new i</formula><p>and obtain the low-dimensional real-value representation by a linear projection matrix:</p><formula xml:id="formula_52">P = (U T U ) -1 U T</formula><p>via RKNMF, which is the pseudoinverse of U . Since h Θ is a Sigmoid function, finally the hash code for the new sample can be calculated as:</p><formula xml:id="formula_53">vnew = h Θ (P • K new ) , (<label>23</label></formula><formula xml:id="formula_54">)</formula><p>where function • is the nearest integer function for each entry of h Θ . In fact, it follows the property of h Θ ∈ (0, 1) to binarize vnew by a threshold 0.5. If any bit of h Θ (P •K new )'s output is larger than 0.5, we assign "1" to this bit, otherwise "0". In this way, we can obtain our final Multiview Alignment Hashing (MAH) codes for any data points (see in Fig. <ref type="figure" target="#fig_3">2</ref>). It is noteworthy that our approach can be regarded as an embedding method. In practice, we map all the training and test data in a same way to ensure that they are in the same subspace by Eq. ( <ref type="formula" target="#formula_53">23</ref>) with {P, α, Θ} which are obtained via multiview RKNMF optimization and logistic regression. This procedure is similar as a handcrafted embedding scheme, without re-training. The corresponding integrated MAH algorithm is depicted in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Complexity Analysis</head><p>The cost of MAH learning mainly contains two parts. The first part is for the constructions of Heat Kernels and similarity probability regularization items for different views, i.e., K i and L i . From Section III-A, the time complexity of this part is O(2( and regression matrix Θ. 1: Calculate matrix W (i) for each view through Eq. ( <ref type="formula">5</ref>);</p><formula xml:id="formula_55">n i=1 D i )N 2 ).</formula><formula xml:id="formula_56">2: Initialize α = (1/n, 1/n, • • • , 1/n); 3: repeat 4:</formula><p>Compute the basis matrix U and the low-dimensional representation matrix V via Eq. ( <ref type="formula" target="#formula_21">11</ref>) and Eq. ( <ref type="formula" target="#formula_22">12</ref>); 5:</p><p>Obtain kernel weights α T = A -1 B with Eq. ( <ref type="formula" target="#formula_46">20</ref>); 6: until convergence 7: Calculate the regression matrix Θ by Eq. ( <ref type="formula" target="#formula_48">22</ref>) and the final MAH encoding for a sample is defined in Eq. ( <ref type="formula" target="#formula_53">23</ref>). <ref type="bibr" target="#b29">[30]</ref>. And, the updating of α has the complexity of O(n 2 N 2 ) in MAH. In total, the time complexity of MAH learning is</p><formula xml:id="formula_57">O(2( n i=1 D i )N 2 + T × (N 2 d + n 2 N 2 ))</formula><p>, where T is the number of iterations for alternate optimization. Empirically, T is always less than 10, i.e., MAH converges within 10 rounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head><p>In this section, the MAH algorithm is evaluated for the high dimensional nearest neighbor search problem. Three different datasets are used in our experiments, i.e., Caltech-256 <ref type="bibr" target="#b43">[44]</ref>, CIFAR-10 <ref type="bibr" target="#b44">[45]</ref> and CIFAR-20 <ref type="bibr" target="#b44">[45]</ref>. Caltech-256 consists of 30607 images associated with 256 object categories. CIFAR-10 and CIFAR-20 are both 60, 000-image subsets collected from the 80-million tiny images dataset <ref type="bibr" target="#b45">[46]</ref> with 10 class labels and 20 class labels, respectively. Following the experimental setting in <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b20">[21]</ref>, for each dataset, we randomly select 1000 images as the query set and the rest of dataset is used as the training set. Given an image, we would like to describe it with multiview features extracted from it. The descriptors are expected to capture the orientation, intensity, texture and color information, which are the main cues of an image. Therefore, 512-dim Gist<ref type="foot" target="#foot_2">3</ref>  <ref type="bibr" target="#b46">[47]</ref>, 1152-dim histogram of oriented gradients (HOG) <ref type="foot" target="#foot_3">4</ref> [48], 256-dim local binary pattern (LBP) 5 [49] and 192-dim color histogram (ColorHist) 6 are respectively employed for image representation. In the test phase, a returned point is regarded as a true neighbor if it lies in the top 100, 500 and 500 points closest to a query for Caltech-256, CIFAR-10 and CIFAR-20, respectively. For each query, all the data points in the database are ranked according to their Hamming distances to the query, since it is fast enough with short hash codes in practice. We then evaluate the retrieval results by the Mean Average Precision (MAP) and the precision-recall curve. Additionally, we also report the training time and the test time (the average searching time used for each query) for all the methods. All experiments are performed using Matlab 2013a on a server configured with a 12-core processor and 128G of RAM running the Linux OS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Compared Methods and Settings</head><p>We compare our method against six popular unsupervised multiview hashing algorithms, i.e., Multi-View Anchor Graph Hashing (MVAGH) <ref type="bibr" target="#b20">[21]</ref>, Sequential Update for Multi-View Spectral Hashing (SU-MVSH) <ref type="bibr" target="#b21">[22]</ref>, Multi-View Hashing (MVH-CS) <ref type="bibr" target="#b22">[23]</ref>, Composite Hashing with Multiple Information Sources (CHMIS) <ref type="bibr" target="#b23">[24]</ref>, Deep Multi-view Hashing (DMVH) <ref type="bibr" target="#b24">[25]</ref> with a 4-layers deep-net and a derived version of MVH-CS, termed MAV-CCA, which is a special case of MAV-CS when the averaged similarity matrix is fixed as the identity matrix <ref type="bibr" target="#b22">[23]</ref>. Besides, we also compare our method with two state-of-the-art single-view hashing methods, i.e., Spectral Hashing (SpH) <ref type="bibr" target="#b7">[8]</ref> and Anchor Graphs Hashing (AGH) <ref type="bibr" target="#b9">[10]</ref>. For single-view hashing methods, data from multiviews are concatenated into a single representation for hashing learning. It is noteworthy that we have normalized all the features into a same scale before concatenating the multiple features into a single representation for single-view hashing algorithms. All of the above methods are then evaluated on six different lengths of codes <ref type="bibr" target="#b15">(16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr">64,</ref><ref type="bibr">80,</ref><ref type="bibr">96)</ref>. Following the same experimental setting, all the parameters used in the compared methods have been strictly chosen according to their original papers.</p><p>For our MAH, we apply Heat Kernel:</p><formula xml:id="formula_58">K i (x (i) p , x (i) q ) = exp -||x (i) p -x (i) q || 2 2τ 2 i , ∀p, q,</formula><p>to construct the original kernel matrices, where τ i is set as the median of pairwise distances of data points. The selection of the smooth parameter σ i used in Eq. ( <ref type="formula">5</ref>) also follows with the same scheme of τ i . The optimal learning rate r for each dataset is selected from one of {0.01, 0.02, • • • , 0.10} with the step of 0.01 which yields the best performance by 10fold cross-validation on the training data. The choice of three regularization parameters {γ, η, ξ} is also done via crossvalidation on the training set and we finally fix γ = 0.15, η = 0.325 and ξ = 0.05 for all three datasets. To further speed up the convergence of the proposed alternate optimization procedure, in our experiments, we apply a small trick with the following steps:</p><p>1) For the first time to calculate U and V in the step of optimizing (U, V ) in Section III-B, we utilize Eq. <ref type="bibr" target="#b10">(11)</ref> and Eq. ( <ref type="formula" target="#formula_22">12</ref>) with random initialization, following the original NMF procedure <ref type="bibr" target="#b25">[26]</ref>. We then store the obtained U and V . 2) From the second time, we optimize (U, V ) by using the stored U and V from the last time to initialize the NMF algorithm, instead of using random values.</p><p>This small improvement can effectively reduce the time of convergence in the training phase and make the proposed method more efficient for large-scale tasks. Fig. <ref type="figure" target="#fig_4">3</ref> shows the retrieval performance of MAH when four descriptors (i.e., Gist, HOG, LBP, ColorHist) are used together via different kernel combinations, or when single descriptors are used. Specifically, MAH denotes that the kernels are combined by the proposed method:</p><formula xml:id="formula_59">K = n i=1 α i K i ,</formula><p>where the weight α i is obtained via alternate optimization. MAH (Average) means that the kernels are combined by arithmetic mean</p><formula xml:id="formula_60">K = 1 n n i=1 K i ,</formula><p>and MAH (Product) indicates the combination of kernels    through geometric mean</p><formula xml:id="formula_61">K = ( n i=1 K i ) 1 n .</formula><p>The corresponding combinations of similarity probability regularization items L i also follow the above similar schemes. The results on three datasets demonstrate that integrating multiple features achieves better performance than using single features and the proposed weighted combination improves the performance compared with average and product schemes. Fig. <ref type="figure">4</ref> illustrates the MAP curves of all compared algorithms on three datasets. In its entirety, firstly the retrieval accuracies on the CIFAR-10 dataset are obviously higher than that on the more complicated CIFAR-20 and Caltech-256 datesets. Secondly, the multiview methods always achieve better results than single-view schemes. Particularly, in most cases, MVAGH achieves higher performance than SU-MVSH, MVH-CS, MVH-CCA, DMVH-4layers and CHMIS. The results of MVH-CS always climb up then go down when the length of codes increases. The same tendency also appears with CHMIS. In general, our MAH outperforms all other compared methods (also see Table <ref type="table" target="#tab_1">I</ref>). In additional, we have also compared our proposed method with other various baselines (with/without the probability regularization or the orthogonal constraint) in Table <ref type="table" target="#tab_0">II</ref>). It is obviously observed that the proposed method has significantly improved the effectiveness of NMF and its variants in terms of accuracies.</p><p>Furthermore, we present the precision-recall curves of all the algorithms on three datasets with the code length of 96 bits in Fig. <ref type="figure">5</ref>. From this figure, we can further discover that MAH consistently achieves the better results again by comparing the Area Under the Curve (AUC). Some examples of retrieval results on Caltech-256 are also shown in Fig. <ref type="figure" target="#fig_6">6</ref> and Fig. <ref type="figure" target="#fig_7">7</ref>.</p><p>The proposed algorithm aims to generate a non-linear binary code, which can better fit the intrinsic data structure than linear ones. The only existing non-linear multiview hashing algorithm is MVAGH. However, the proposed algorithm can successfully find the optimal aggregation of the kernel matrix, whereas MVAGH cannot. Therefore, it is expected that the proposed algorithm performs better than MVAGH and the existing linear multiview hashing algorithms.</p><p>Finally, we list the training time and the test time for different algorithms on three datasets in Table <ref type="table" target="#tab_1">I</ref>. Considering the training time, since DMVH involves learning of a 4-layer deep-net, it spends the most time to train hashing functions. MVH-CCA spends the second longest time for training. Our MAH only costs slightly more time than MVAGH and CHMIS but significantly less than other methods for training. For the test phase, MVH-CS and MVH-CCA are the most efficient methods, while MAH has competitive searching time as SU-MVSH. DMVH needs the most time for testing as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have presented a novel unsupervised hashing method called Multiview Alignment Hashing (MAH), where hashing functions are effectively learnt via kernelized Nonnegative Matrix Factorization with preserving data joint probability distribution. We incorporate multiple visual features from different views together and an alternate way is introduced to optimize the weights for different views and simultaneously produce the low-dimensional representation. We address this as a nonconvex optimization problem and its alternate procedure will finally converge at the locally optimal solution. For the out-of-sample extension, multivariable logistic regression has been successfully applied to obtain the regression matrix for fast hash encoding. Numerical experiments have been systematically evaluated on Caltech-256, CIFAR-10 and CIFAR-20 datasets. The results manifest that our MAH significantly outperforms the state-of-the-art multiview hashing techniques in terms of searching accuracies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the working flow of MAH. During the training phase, the proposed optimization method is derived by an alternate optimization procedure which optimizes the kernel's weight α and the factorization matrices (U, V ) alternately. Using the multivariable logistic regression, the algorithm then outputs the projection matrix P and regression matrix Θ to generate the hash function, which are directly used in the test phase.</figDesc><graphic coords="4,61.81,56.72,488.35,266.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where λ and β = (β 1 , • • • , β n ) are the Lagrangian multipliers for the constraints and α = (α 1 , • • • , α n ). Considering the partial derivatives of L 2 with respect to α, λ and β, we have the equality ∇ α,λ L 2 = 0 and the inequality ∇ β L 2 ≥ 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The second part is for the alternate optimization. The time complexity of the matrix factorization in updating (U, V ) step is O(N 2 d) according to Algorithm 1 Multiview Alignment Hashing (MAH) Input: A set of training kernel matrices from n different views: {K 1 , • • • , K n } computed via Heat Kernel; the objective dimension of hash code d; learning rate r for logistic regression and regularization parameters {γ, η, ξ}. Output: Kernel weights α = (α 1 , • • • , α n ), basis matrix U</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The illustration of the embedding via Eq. (23), i.e., the nearest integer function.</figDesc><graphic coords="7,74.67,56.73,462.65,100.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Performance (MAP) of MAH when four descriptors are used together via different kernel combinations in comparison to that of single descriptors.</figDesc><graphic coords="8,48.96,77.60,514.07,156.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Performance (MAP) comparison with different numbers of bits.</figDesc><graphic coords="8,48.96,308.07,514.07,160.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Some retrieval results on the Caltech-256 dataset. The top-left image in each block is the query and the rest are the 8-nearest images. The incorrect results are marked by red boxes.</figDesc><graphic coords="9,74.67,56.72,462.63,151.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Comparison of some retrieval results via compared hashing methods on the Caltech-256 dataset. The top-left image in each block is the query and the rest are the 8-nearest images. The incorrect results are marked by red boxes.</figDesc><graphic coords="9,74.67,250.39,462.67,307.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF DIFFERENT VARIANTS OF MAH TO PROVE THE EFFECTIVENESS OF THE IMPROVEMENT.</figDesc><table><row><cell>````````M ethods</cell><cell>Datasets</cell><cell cols="3">Caltech-256 CIFAR-10 CIFAR-20</cell></row><row><cell cols="2">MAH-1</cell><cell>0.034</cell><cell>0.091</cell><cell>0.047</cell></row><row><cell cols="2">MAH-2</cell><cell>0.057</cell><cell>0.113</cell><cell>0.062</cell></row><row><cell cols="2">MAH-3</cell><cell>0.090</cell><cell>0.360</cell><cell>0.106</cell></row><row><cell>MAH</cell><cell></cell><cell>0.103</cell><cell>0.381</cell><cell>0.123</cell></row><row><cell cols="5">(MAH-1 is the original MAH with neither the probability regularization nor the</cell></row><row><cell cols="5">orthogonal constraint; MAH-2 is the original MAH without the probability regularization;</cell></row><row><cell cols="5">MAH-3 represents the original MAH without the orthogonal constraint.)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I MEAN</head><label>I</label><figDesc>AVERAGE PRECISION (MAP) OF 32 BITS WITH TRAINING AND TEST TIME ON THREE DATASETS. All the time calculated does not include the cost of feature extraction.)</figDesc><table><row><cell>Methods</cell><cell>MAP</cell><cell cols="2">Caltech-256 Train</cell><cell>Test</cell><cell>MAP</cell><cell>CIFAR-10 Train</cell><cell>Test</cell><cell>MAP</cell><cell>CIFAR-20 Train</cell><cell>Test</cell></row><row><cell></cell><cell cols="2">(100-NN)</cell><cell>time</cell><cell>time</cell><cell>(500-NN)</cell><cell>time</cell><cell>time</cell><cell>(500-NN)</cell><cell>time</cell><cell>time</cell></row><row><cell>SU-MVSH</cell><cell>0.082</cell><cell></cell><cell cols="2">314.2s 48.4µs</cell><cell>0.241</cell><cell>387.3s</cell><cell>63.8µs</cell><cell>0.109</cell><cell>402.3s</cell><cell>72.7µs</cell></row><row><cell>MVH-CS</cell><cell>0.063</cell><cell></cell><cell cols="2">371.1s 32.4µs</cell><cell>0.237</cell><cell>498.2s</cell><cell>48.8µs</cell><cell>0.081</cell><cell>562.0s</cell><cell>53.1µs</cell></row><row><cell>MVH-CCA</cell><cell>0.052</cell><cell></cell><cell cols="2">477.0s 28.3µs</cell><cell>0.270</cell><cell>544.5s</cell><cell>45.0µs</cell><cell>0.080</cell><cell>570.6s</cell><cell>56.1µs</cell></row><row><cell>CHMIS</cell><cell>0.074</cell><cell></cell><cell cols="2">299.9s 90.1µs</cell><cell>0.273</cell><cell cols="2">333.6s 104.8µs</cell><cell>0.102</cell><cell>363.7s</cell><cell>113µs</cell></row><row><cell>MVAGH</cell><cell>0.092</cell><cell></cell><cell cols="2">253.9s 57.0µs</cell><cell>0.359</cell><cell>276.2s</cell><cell>64.5µs</cell><cell>0.118</cell><cell>297.3s</cell><cell>74.8µs</cell></row><row><cell>DMVH-4layers</cell><cell>0.080</cell><cell></cell><cell cols="2">22104s 97.4µs</cell><cell>0.278</cell><cell cols="2">48829s 105.1µs</cell><cell>0.111</cell><cell cols="2">52350s 117.2µs</cell></row><row><cell>MAH</cell><cell>0.103</cell><cell></cell><cell cols="2">274.7s 49.1µs</cell><cell>0.381</cell><cell>361.5s</cell><cell>62.9µs</cell><cell>0.123</cell><cell>397.5s</cell><cell>76.5µs</cell></row><row><cell></cell><cell>(</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The effectiveness and efficiency of these methods drop exponentially as the dimensionality increases, which is commonly referred to as the curse of dimensionality.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Our approach can work with any legitimate similarity probability measure, though we focus on Gaussian similarity probability in this paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Gabor filters are applied on images with 8 different orientations and</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>scales. Each filtered image is then averaged over 4 × 4 grid leading to a</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>512-dimensional vector (8 × 4 × 16 = 512).<ref type="bibr" target="#b3">4</ref> 36 4 × 4 non-overlapping windows yield a 1152-dimensional vector.<ref type="bibr" target="#b4">5</ref> The LBP labels the pixels of an image by thresholding a 3 × 3 neighborhood, and responses are mapped to a 256-dimensional vector.<ref type="bibr" target="#b5">6</ref> For each R,G,B channel, a 64-bin histogram is computed and the total length is 3 × 64 = 192.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Feature learning for image classification via multiobjective genetic programming</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1359" to="1371" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly-supervised cross-domain dictionary learning for visual recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="42" to="59" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Representing and retrieving video shots in human-centric brain imaging space</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2723" to="2736" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A memory learning framework for effective image retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="511" to="524" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An objectoriented visual saliency detection framework based on sparse coding representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2009" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multidimensional access methods</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gaede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Günther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="170" to="231" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Very Large Data Bases</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spectral hashing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised hashing for large scale search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2393" to="2406" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hashing with graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning binary hash codes for large-scale image search</title>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Computer Vision</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="49" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised multi-graph hashing for scalable similarity search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="12" to="21" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spectral hashing with semantically consistent graph for image indexing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="152" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-margin multi-view information bottleneck</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1559" to="1572" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiview spectral embedding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part B</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1438" to="1446" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A survey on multi-view learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno>abs/1304.5634</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">m-sne: Multiview stochastic neighbor embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1088" to="1096" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Part B: Cybernetics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unified video annotation via multigraph learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="733" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multimodal graphbased reranking for web image search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4649" to="4661" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semisupervised multiview distance metric learning for cartoon synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4636" to="4648" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-view anchor graph hashing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequential spectral learning to hash with multiple representations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning hash functions for cross-view similarity search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Udupa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Composite hashing with multiple information sources</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning to hash with multiple representations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="issue">6755</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning spatially localized, parts-based representation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization with sparseness constraints</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1457" to="1469" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neighborhood preserving nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph regularized nonnegative matrix factorization for data representation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1548" to="1560" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization on kernels</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Rim International Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multiple kernel nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Some notes on alternating optimization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Bezdek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hathaway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Soft Computing-AFSS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="288" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex optimization</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dimensionality reduction with category information fusion and non-negative matrix factorization for text categorization</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Computational Intelligence</title>
		<imprint>
			<biblScope unit="page" from="505" to="512" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Algorithms for non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to hash: forgiving hash functions and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Covell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="402" to="430" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOBILE Mobile Computing and Communications Review</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="55" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-taught hashing for fast similarity search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Compressed hashing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Spectral regression for efficient regularized subspace learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Hosmer</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lemeshow</surname></persName>
		</author>
		<title level="m">Applied logistic regression</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="473" to="482" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Department University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1958" to="1970" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Currently, he is a research fellow in the Department of Computer Science and Digital Technologies at Northumbria University. His research interests include computer vision, machine learning and data mining</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ahonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mengyang Yu (S&apos;14) received the B.S. and M.S. degrees from the School of Mathematical Sciences</title>
		<meeting><address><addrLine>Xi&apos;an, China; Sheffield, U.K.; Beijing, China; Newcastle upon Tyne, U.K</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2014. 2010. 2013</date>
		</imprint>
		<respStmt>
			<orgName>University of Sheffield ; Peking University ; Department of Computer Science and Digital Technologies at Northumbria University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include computer vision, machine learning and data mining</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
