<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Evaluation of Local Improvement Operators for Genetic Algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Walter</forename><forename type="middle">D</forename><surname>Potter</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ravi</forename><forename type="middle">V</forename><surname>Gandham</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chito</forename><forename type="middle">N</forename><surname>Lapena</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Compute1 Science and the Artificial Intelligence Programs</orgName>
								<orgName type="institution">University of Georgia</orgName>
								<address>
									<postCode>30602</postCode>
									<settlement>Athens</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Corporation</orgName>
								<address>
									<postCode>98052</postCode>
									<settlement>Redmond</settlement>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Evaluation of Local Improvement Operators for Genetic Algorithms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">05ACD0E9828EC40C19944D6A570162CA</idno>
					<note type="submission">Manuscript received June 8, 1992; revised February 3, 1993.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Genetic algorithms have demonstrated considerable success in providing good solutions to many NP-hard optimization problems. For such problems, exact algorithms that always find an optimal solution are only useful for small toy problems, so heuristic algorithms such as the genetic algorithm must be used in practice. In this paper, we apply the genetic algorithm to the NP-hard problem of multiple fault diagnosis (MFD). We compare a pure genetic algorithm with several variants that include local improvement operators. These operators, which are often domain-specific, are used to accelerate the genetic algorithm in converging on optimal solutions. Our empirical results indicate that by using the appropriate local improvement operator, the genetic algorithm is able to find an optimal solution in all hut a tiny fraction of the cases and at a speed orders of magnitude faster than exact algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>ANY traditional optimization algorithms suffer from M myopia for highly complex search spaces, leading them to less than desirable performance (both in terms of execution speed and fraction of time they need to find an optimal solution). When search spaces are particularly irregular, algorithms need to be highly robust in order to avoid getting stuck at local optima. For example, adding a randomization step to a gradient search algorithm can improve its performance. Still, for many large and complex search spaces this may not be enough. Indeed, for the multiple fault diagnosis (MFD) problem (which we examine in this paper), our experiments show that such randomization has little effect. The problem is that in many regions of the search space there is little information to direct the search (e.g., in a flat valley). Consequently, local search algorithms may exhibit less than desirable performance.</p><p>A more global search strategy is what is needed. One alternative is to use an exhaustive search algorithm. This will guarantee 100 percent reliability, i.e., an optimal solution will be found in 100 percent of the cases. Unfortunately, most interesting discrete optimization problems (e.g., MFD) are NP-hard, making such algorithms of value for only small toy problems. Highly reliable heu-</p><formula xml:id="formula_0">IEEE Log Number 920969 I .</formula><p>Georgia, Athens, GA 30602. ristics are needed for real-world problems. To handle irregular search spaces, such heuristics should adopt a global strategy and rely heavily on intelligent randomization. Genetic algorithms follow just such a strategy. Following the model of evolution, they establish a population of individuals, where each individual corresponds to a point in the search space. An objective function is applied to each individual to rate their fitness. Using wellconceived operators, a next generation is formed based upon the survival of the fittest. Therefore, the evolution of individuals from generation to generation tends to result in fitter individuals, solutions, in the search space. Empirical studies have shown that genetic algorithms do converge on global optima for a large class of NP-hard problems <ref type="bibr" target="#b6">[7]</ref>.</p><p>Unfortunately, for the problem we are studying, MFD, the genetic algorithm exhibits low-er reliability than our local search algorithm. To achieve higher reliability we simply combine the two algorithms to form a hybrid. This is straightforward to do with genetic algorithms via the introduction of local improvement operators <ref type="bibr">[ 141.</ref> The nature of our local improvement operators is such that they can be easily adapted to a wide variety of domains (i.e., they are not highly domain specific).</p><p>In the following sections, we introduce several local search-genetic algorithm hybrids. Experiments show how these hybrids perform, and how the local search schemes perform independently of the genetic algorithm [ 1 11. First, we present the reader with a brief but self-contained background section that introduces the major cornerstones of our research, namely, the problem of multiple fault diagnosis, the relative likelihood function, and the genetic algorithm. Of particular importance in using the genetic algorithm (or any search technique) is the method used to evaluate each diagnosis. We adapt the relative likelihood of the probabilistic causal model (PCM) developed by Peng and Reggia <ref type="bibr" target="#b18">[23]</ref>, <ref type="bibr" target="#b19">[24]</ref> to the genetic algorithm <ref type="bibr" target="#b21">[26]</ref>, <ref type="bibr" target="#b22">[27]</ref>. A brief description of the PCM is included here; however, the reader is referred to the work of Peng and Reggia for detailed discussions of the motivation and justification for the PCM. Following these sections, we present a detailed discussion of our local search algorithms, which are variants of our engineered conditioning operator <ref type="bibr" target="#b23">[28]</ref>, and then discuss the hybrid genetic algorithm approaches. Finally, we discuss our experiments, including types, setup, results/conclusions, and our future directions for this work. 0018-9472/93$03.00 0 1993 IEEE 11. THE MULTIPLE FAULT DIAGNOSIS PROBLEM Diagnosis is the process of determining the correct problem from a collection of problems given a set of symptoms that indicate a problem exists. Common experiences with this process include visits to the physician in order to determine our illness (disease) and visits to our local mechanic to determine the cause (fault) of a poorly operating car. In either case, we report the symptoms of the problem to the diagnostician (physician or mechanic), who determines the most likely cause that best explains these symptoms. In terms of the complexity of determining the correct diagnosis, the diagnostician must find a diagnosis from a set of possible diagnoses. That is, if a total of 10 diseases or faults are being considered where only one of these is the correct one, then at most 10 diagnoses will need to be evaluated.</p><p>However, in the more typical case where several problems (diseases/faults) may occur simultaneously, the complexity of finding a proper diagnosis increases exponentially with the number of problems. For example, us- ing the 10 problems considered above, the situation changes such that any of the 1024 (i.e., 2") possible combinations of problems may turn out to be the correct diagnosis.</p><p>In medicine as well as electronics and other domains, multiple problem diagnosis, henceforth called multiple fault diagnosis, is the identification of a set of problems (diseases or faulty components) that best corresponds to or explains some observed abnormal behavior (symptoms) [8], <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b18">[23]</ref>, <ref type="bibr" target="#b26">[31]</ref>. This type of problem solving is commonly referred to as an abductive inference <ref type="bibr" target="#b4">[5]</ref> and automating this approach to diagnosis' has been the focus of extensive research efforts <ref type="bibr" target="#b5">[6]</ref>, [8], [13], [18], <ref type="bibr" target="#b18">[23]</ref>, 1241, [261, <ref type="bibr">[301-[341, 1281.</ref> A closer look at multiple fault diagnosis reveals three major hurdles to overcome in order to solve a diagnostic problem via a "reasonable" automated solution: 1) the exponentially large number of possible diagnoses, 2) measuring the relative "goodness" of a particular diagnosis, and 3) the search strategy used to find optimal or near-optimal diagnoses. In this discussion, the ''most reasonable" solution corresponds to the diagnosis or diagnoses that best explain the observed symptoms. This best explanation is determined and wholly dependent on the calculation of the goodness of a diagnosis. Consider, for example, the small hypothetical situation where we have 20 components in a microwave communications system. In this system, each component may exhibit faulty behavior via a set of, say, 10 alarms or symptoms. Assuming that we have some mechanism for determining the goodness of each of the 220 (that is, 1 048 576) possible di-'Diagnostic bystems following this approach to diagnosis apply the "reasoning from first principles" paradigm where a description of some physical system's structure and behavior is maintained and compared to abnormal behavior. This is in sharp contrast to the "experiential" paradigm (e.g.. used by the MYCIN expert system [3]). which is driven by the problemsolving rules of thumb or heuristics acquired from a human expert diagnostician [I], [4], <ref type="bibr" target="#b26">[31]</ref>.</p><p>agnoses, we are now faced with searching this large set of possible solutions to determine which diagnosis is best. The search strategy and the goodness measurement work together to find the best diagnosis. (An intuitive representation for a diagnosis in such a scenario is a 20-bit binary string where each of the 20 components is associated with one of the bit positions, i.e., component i corresponds with bit position i . In a diagnosis, a 0 in a particular bit position means that the corresponding component is not considered to be at fault, while a 1 means that this component helps explain some or all of the symptoms.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . The Relative Likelihood Objective Function</head><p>One of the leading theories of diagnosis is based on the notion of parsimoniously covering a set of observable symptoms [30], <ref type="bibr" target="#b29">[34]</ref>, that is, finding a minimal set covering (an NP-hard problem [lo], [ 121 ), or a set of diseases in a medical domain that explains a given set of symptoms. The fact that a cover is necessary in order to explain the symptoms is intuitively clear but minimality is another matter and, in some typical cases, inappropriate. In order to overcome this major shortcoming, Peng and Reggia introduced the probabilistic causal model (PCM) <ref type="bibr" target="#b18">[23]</ref>, <ref type="bibr" target="#b19">[24]</ref>. The PCM integrates "symbolic causeeffect inference with numeric probabilistic inference" to solve multiple fault diagnosis problems.</p><p>In their approach, a multiple fault diagnosis problem is characterized as a four-tuple: that is, we may expect the frequency with which dJ causes m, to remain stable. An additional assumption stipulates that no manifestation may exist in M + unless it is actually caused by some disorder in D. See Fig. <ref type="figure">1</ref> for an example tendency matrix. Now, we have ID( prior probabilities and ID1 IMI causal strengths. Using these values, Peng and Reggia derive a formula for calculating the "relative likelihood," denoted L(DZ, M + ) , of a diagnosis DZ given observable manifestations M f . The likelihood is the product of three factors <ref type="bibr" target="#b18">[23]</ref>: L(DZ, M') = L l L 2 L 3 . The first factor, 1</p><formula xml:id="formula_1">(D, M , C, M f &gt; where D is M is C is M + is a finite</formula><formula xml:id="formula_2">Ll = I t 1 -rI (1 -e,,) m, E M + ( d, E Dl</formula><p>is the likelihood that disorders in DZ cause the manifestations in M ' . For diagnoses that do not cover M + , L , evaluates to 0 thus forcing L to 0. Unfortunately, this denies any analysis of noncover diagnoses. The second fac- tor, &amp; = rI rI</p><formula xml:id="formula_3">(1 -CIJ) d , ~D l miEeffect\(d,) ~ M +</formula><p>is the likelihood that disorders in DI do not cause manifestations outside of M + (e.g., in M -M ' ) . According to Peng and Reggia, L2 is "a weight based on manifestations expected with DI but which are actually absent." Ideally, we prefer L2 values that are close to 1. Unfortunately, this factor denies any analysis of supercover diagnoses. Finally, the third factor, is the likelihood that a highly probable (very common) disorder dj contributes significantly in the overall likelihood of a diagnosis D I containing d,.</p><p>By combining the three factors, the relative likelihood gives us the capability to compare diagnoses and select the best one. This capability is the foundation for any automated approach to finding the best diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Solution Approaches</head><p>One possible (highly inefficient) approach for finding the best diagnosis is to simply generate each of the 220 diagnoses, calculate the relative likelihood or goodness of each, and report the best one. However, for systems with more than about 25 components (that is, over 33.5 million possible diagnoses), this approach becomes infeasible, especially if the system is in an aircraft or spacestation where quick diagnosis followed by corrective action would be crucial. In practice, it is not unusual for a medium-sized system to have upwards of SO to 75 components, similar to medical domains that have large numbers of diseases or causes to consider <ref type="bibr">[30]</ref>. Specialized heuristic search strategies have been proposed as alternatives to the exhaustive search strategy; see for example (81, 1181, One general heuristic search method used to attack very hard problems that is receiving increased interest from the AI-community is the genetic algorithm (GA) <ref type="bibr" target="#b6">[7]</ref>, [14], <ref type="bibr">[16]</ref>. This strategy incorporates the determination of a goodness or likelihood that a particular diagnosis explains the observable symptoms. Our approach is to use the genetic algorithm to attack the problem of multiple fault diagnosis (to our knowledge, 1261 was the first attempt to use the genetic algorithm for multiple fault diagnosis). In order to improve the reliability of the genetic algorithm, we introduced the engineered conditioning operator <ref type="bibr" target="#b23">[28]</ref>. In this paper, we introduce several variations of the engineered conditioning operator and show how these variations perform within the genetic search setting, as well as independent of the genetic algorithm.</p><p>~4 1 , w i , 1311.</p><p>111. GENETIC ALGORITHMS Genetic algorithms have been applied to a wide variety of problems: multiple fault diagnosis (MFD) <ref type="bibr">[2 11, 1261, 1271, [28]</ref>, set covering (SC) and traveling salesman problems (TSP) <ref type="bibr" target="#b15">[20]</ref>, communication network configuration 1291, and control of natural gas pipelines and game playing [ 141. Genetic algorithms are a type of stochastic search method and are applied to NP-hard problems in many areas ranging from scheduling optimization to designing optimal configurations and layouts. They are modeled from and mimic the theory of natural evolution, and were developed by Holland in 1975 [ 161.</p><p>In genetic algorithms, a population is nothing but a collection of "chromosomes" representing possible solutions. These chromosomes are altered or modified using genetic operators through which a new generation is created. This process is repeated a predetermined number of times or until no improvement in the solution to the problem is found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . Encoding Schemes</head><p>Originally, the chromosomes (or individuals) in the population were represented as strings of binary digits. Later, other types of representations were experimented with and found to be, depending on the problem to be solved, more convenient. However, bit string representations are still the most commonly used encoding techniques and have been used in many real-world applications of genetic algorithms. Such representations have several advantages: they are simple to create and manipulate, many types of information can be easily encoded, and the genetic operators are easy to apply <ref type="bibr" target="#b6">[7]</ref>. Other types of encodings include that of real number representations (e.g., applied to parametric design of aircraft <ref type="bibr" target="#b1">[2]</ref>, and network design <ref type="bibr" target="#b24">[29]</ref>), and ordered list representations (e.g., applied to scheduling optimization <ref type="bibr" target="#b28">[33]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation</head><p>The evaluation of a chromosome is done to test its "fitness" as a solution and is achieved by making use of a mathematical formula known as an objective function. The objective function plays the role of the environment in natural evolution by rating individuals in terms of their fitness. Choosing and formulating an appropriate objective function is crucial to the efficient solution of any given genetic algorithm problem. (When designing an evaluation function for an optimization problem with constraints, penalty functions can be introduced and applied to individuals that violate the imposed constraints.) A more detailed discussion on the specific objective function we use is presented shortly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Genetic Operators</head><p>Genetic operators are used to alter the composition of chromosomes. The fundamental genetic operators such as mate selection, crossover, and mutation are used to create children (or individuals in the next generation) that differ from their parents (or individuals in the previous generation). Additional advanced genetic operators have been inspired by knowledge derived from the field of genetics (e.g., inversion dominance, diploidy, and abeyance) [14]. In the following discussion the major operators such as mate selection, crossover, and mutation are discussed.</p><p>Using the mate selection operator, individual chromosomes are selected according to their fitness, which is evaluated using an objective function. This means that a chromosome with a higher fitness value will have a higher probability of contributing one or more chromosomes in the next generation. There are many ways this operator can be implemented. A basic method calls for using a weighted roulette wheel with slots sized according to fitness <ref type="bibr">[14]</ref>. Thus, on the roulette wheel the individual with the highest fitness will have a larger slot than the other individuals in the population. Consequently, when the wheel is spun, the best individual will have a higher ents, yet retain some of their parents characteristics. There are two important crossover techniques called one-point crossover and two-point crossover. In one-point crossover, two parent chromosomes are interchanged at a randomly selected point thus creating two children. In twopoint crossover, two crossover points are selected instead of just one crossover point. The part of the chromosome string between these two points is then swapped to generate two children. Empirical studies have shown that twopoint crossover usually provides better randomization than one-point crossover <ref type="bibr">[ 141.</ref> Other crossover techniques such as uniform crossover are considered whenever it is found that both one-point and two-point crossover techniques are not combining useful characteristics of chromosomes from the parents. In uniform crossover, for each bit position within the new child chromosome, it is decided randomly which parent the child will inherit the bit from. As discussed in <ref type="bibr" target="#b6">[7]</ref>, it has been found that uniform crossover is inferior to two-point crossover in certain instances.</p><p>Some of the individuals in the new generation produced by mate selection and crossover are mutated using the mutation operator. The most common form of mutation is to take a bit from a chromosome and alter (i.e., flip) it with some predetermined probability. As mutation rates are very small in natural evolution, the probability with which the mutation operator is applied is set to a very low value and is generally experimented with before this value is fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Genetic Algorithm Formulation for MFD</head><p>The genetic algorithm adapted by Potter et al. <ref type="bibr" target="#b21">[26]</ref>, <ref type="bibr" target="#b22">[27]</ref> to solve the MFD problem is modeled after Goldberg's simple genetic algorithms <ref type="bibr">[14]</ref>. Starting with an initial population po and the corresponding fitness values fo, the algorithm iterates through several generations, as shown in Fig. <ref type="figure">2</ref>.</p><p>This algorithm uses two-point crossover, an elitist policy (the fittest individual in the previous generation conditionally replaces the weakest individual in the current generation), and the initial population is randomly generated, except for one seeded individual representing the diagnosis where all relevant disorders are present. Finally, the parameters for the algorithm are set to the following values: a global maximum using diagnoses that are not covers (e.g., do not completely explain the manifestations). The reason for this is to allow progress whenever the search space terrain resembles Monument Valley with a generally large flat surface except for occasional very thin high peaks or needle-like structures. A search strategy may become "lost" in the flat area and never "see" a nearby peak without some broad convergence mechanism. This corresponds to a restricted diagnostic problem where very few covers for the manifestation set exist. With the relative likelihood, all noncover diagnoses would have a zero likelihood and would provide no search improvement information (from a genetic algorithm prospective). This situation is avoided by insuring that factor L, is never zero. That is, factor Ll is forced to zero in the relative likelihood whenever the causal associations between some disorder in the diagnosis and symptoms in the manifestation set are equal to zero, but these associations are set to a value very close to zero for use with the genetic algorithm. Therefore, the differentiating factors become the disorder prior probabilities and the expected but absent manifestations associated with a diagnosis. Also, L, changes as more of the manifestations are explained. Another aspect that aids convergence is associated with factor L2. In certain cases using the relative likelihood, L2 is forced to zero in the event of a redundant or irrelevant cover diagnosis. This occurs primarily when some disorder in a diagnosis has a unit causal association with a manifestation that is not present in the observed manifestation set. Substituting a value very close to one for these situations allows diagnoses to be evaluated and compared in order for the search to converge to a global maximum. This is because factors LI and L2 cause likelihood values to be less than optimal (but not zero) when evaluating noncovers and supercovers, respectively.</p><formula xml:id="formula_4">Pop-Size = SO to 140 / /</formula><p>As an example, consider the situation where we have a tendency matrix with 15 disorders and 10 manifestations IV. LOCAL SEARCH ALGORITHMS Another category of heuristics that has met with some success in providing good solutions to combinatorial optimization problems is local search algorithms. Local search algorithms work by moving to the next best neighbor in the local solution search space. Their main advantage is that they are not combinatorially explosive like exhaustive search algorithms are. There are many possible improvements suggested in the literature to enhance this basic strategy, many of which are problem dependent. Since local search algorithms do not guarantee the optimality of the solution, care should be taken when applying these techniques to any specific problem. The main disadvantage is that they can end up in a locally optimum location in the search space where there can be no further improvement to the trial solution. This can happen if a point in the (local) search space is better than all of its neighbors, but a globally optimum solution is at some other point farther away from this current point. Other times when the trial solution falls onto a flat plateau, where all the neighboring points have the same value, the search loses its direction. The advantage of local search algorithms is that they are not typically affected by combinatorial explosion, as is the case with exhaustive search algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A . The Engineered Conditioning Operator ( E C )</head><p>Engineered conditioning ( E C ) <ref type="bibr" target="#b23">[28]</ref> is a local search scheme that provides the basis for our local search variants, and hybrid genetic algorithms. It can be used as an additional genetic algorithm operator that works together with the fundamental genetic algorithm operators: mate selection, crossover, and mutation. It functions as a local improvement operator, hopefully enhancing convergence toward an optimal MFD diagnosis. The EC operator is somewhat similar to the hill-climbing mutation scheme of <ref type="bibr" target="#b1">[2]</ref> and the steepest ascent scheme of [22] except that we condition only the dominant individual in the population at each generation, and we continue to use the other GA operators (selection, crossover, and mutation) without any modifications.</p><p>From another viewpoint, the EC operator resembles a more traditional optimization strategy, namely, ''move to the best adjacent point in the search space." EC uses the diagnoses in the immediate search space vicinity of the fittest individual in the current population and compares their strength. The dominant individual acquires the strength and characteristics of any stronger neighbor found during conditioning. Note, the conditioning process may or may not improve the fitness of the dominant individual. Although developed as a local improvement operator for the genetic algorithm, the EC operator can also be used by itself to form a local search algorithm via iterative application.</p><p>Three local search or "conditioning" tests are performed by the EC operator on a trial solution. These are, in order of application, the superset, substitute, and subset evaluation tests. These tests look for better diagnoses of greater cardinality, equal cardinality, and lesser cardinality, respectively.</p><p>1) Superset Test: In the superset test, a single scan of the candidate solution (diagnosis) bit vector is made. If a bit position is off, a check is made to determine whether turning it on will improve the fitness value. If so, the bit is turned on, and the scan continues until the end of the vector is reached. Note that this implies that several bit positions could be turned on in one application of the superset test. Still, the complexity of this test is relatively small, as the objective function will be evaluated only a linear number of times, i.e., the complexity is O ( n ) , where n is the number of bits or disorders in the diagnosis. This test works as follows.</p><p>Start with a trial diagnosis and its fitness value. For all disorders, if a disorder is not identified, and there exists an association between this disorder and an observed symptom or if the disorder has a prior probability greater than 0.5, then add the disorder to the trial diagnosis; test whether this modified diagnosis explains the given symptom set better than the trial diagnosis (i.e., has a greater fitness); if it is better, make the modified diagnosis the trial diagnosis, move to the next disorder, and repeat.</p><p>2) Substitution Test: The substitute test tries to find equal cardinality solutions with improved fitness. This test makes a major scan of the bit vector looking for a bit to turn off. Upon finding a bit i that could be turned off, the test begins a minor scan to find a replacement bit j ( j # i ) to turn on. If this combination of turning bit i off and bit j on results in a higher fitness, the candidate solution is so changed. After the change is made, the minor scan is continued to see whether a better replacement bit can be found. If so the previous change is overwritten. Once the minor scan completes, the major scan is resumed by incrementing i. Because of the nature of the major scan, several replacements can occur with one application of the substitute test. Since there is a minor scanthat occurs within a major scan, this test is of quadratic complexity, i.e., O(n2). This test works as follows.</p><p>0 Start with a trial diagnosis and its fitness value. For all disorders, if there is an identified disorder then remove it, introduce another disorder, and test whether this diagnosis explains the given symptom set better than the trial diagnosis; if it is better, make the modified diagnosis the trial diagnosis, move to the next disorder, and repeat.</p><p>3) Subset Test: Finally, the subset test is just the flip side of the superset test. A single scan is made looking for bits to turn off. A bit that is currently on is turned off if doing so improves the fitness value of the candidate solution. Since a complete scan is made, several bits could possibly be turned off, but as with the superset test the complexity is linear. This test works as follows.</p><p>Start with a trial diagnosis and its fitness value. For all disorders, if a disorder is identified, then eliminate it from the trial diagnosis, and test whether this modified diagnosis explains the given symptom set better than the trial diagnosis; if it is better, make the modified diagnosis the trial diagnosis, move to the next disorder, and repeat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Engineered Conditioning Local Search Algorithm (EC *)</head><p>As mentioned before, the EC operator can be used as the basic move generation mechanism within a complete local search algorithm. Within this algorithm, the EC operator is iteratively applied to improve the current best diagnosis until there is no further improvement. If the search space is unimodal with no flat portions, then this algorithm will produce the globally optimal solution. However, the surface to be searched for MFD problems may not be nearly so regular (in fact, the search space surfaces from our experiments are multimodal with sharp spires of various heights scattered throughout a desertlike terrain). Consequently, one would expect EC* to get stuck in local optima and to get lost in flat areas. Because of the strength of the combination of the three tests making up EC, we believe that it has some immunity to these pitfalls. Indeed, its performance is much better than a simple greedy algorithm that would begin with an empty diagnosis and add disorders until there is no further improvement (see <ref type="bibr" target="#b22">[27]</ref>, [35]). Such a greedy algorithm would be tantamount to basing the EC operator solely on the superset test. The EC* local search algorithm works as follows.</p><p>0 Start with an initial diagnosis where there are no disorders identified and make this the first trial solution. Apply the EC operator to the trial solution to form a new solution. If the new solution is better than the trial solution, replace the trial solution with the new solution and repeat the application; otherwise terminate (since no further improvement is possible).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Engineered2 Conditioning Local Search Algorithm (E2 C *)</head><p>Encouraged by the success of the EC operator, we decided to construct a test that would increase the "vision" of the EC operator. We felt that increasing what the operator considered its neighborhood to be would enhance the reliability of the operator. In this way, the operator would be able to jump off of local optima onto a neighboring slope some distance away. The farther the operator can jump, the greater reliability it will exhibit. Unfortunately, if we do not limit the global vision, we will be left with the equivalent of an exhaustive search with its unacceptably slow execution time. Therefore, to maintain some of the efficiency of the EC operator, we chose to simply replace the substitution test with a double substitution test to form a new operator called E' C. Because of this change, the E 2 C operator looks more deeply into the search space and evaluates more points than the EC operator evaluates. This should enable the E' C operator to be more successful in the sense that it should encounter more optimal solutions more consistently than EC. The increase in the number of evaluations causes this algorithm to be computationally more expensive than EC. In particular, since scans are made for two outgoing bits to be replaced by two incoming bits, the double substitution test is of quartic complexity, i.e., O(n4). The double substitution test works as follows.</p><p>For all disorders, remove two identified disorders, add two other disorders, and test whether this diagnosis explains the given symptom set better than the trial diagnosis; if it is better, make the modified diagnosis the trial diagnosis, move to the next disorder and repeat. The other two tests making up the EC operator, the supertest and subset tests, are retained in E' C.</p><p>As was done before with EC, the new operator can be applied iteratively to form a complete local search algorithm, which we call E 2 C*. Within this algorithm, E' C is applied repeatedly until the diagnosis converges on a value (i.e., no further improvement is found). The E'C* algorithm works just like E C * , except that the ordinary substitution test is replaced with the double substitution test. As previously mentioned, this algorithm is still prone to the problems of local search, namely local optima and plateaus, but these problems are somewhat alleviated by the improved vision of double substitution (at a cost of increased computation time).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V . HYBRID ALGORITHMS</head><p>Hybrid algorithms are combinations of two or more different algorithms. In this section, we discuss a variety of ways in which we have coupled genetic algorithms with local search algorithms to form hybrids. Alternatively, one can view this as simply adding local improvement operators, based on the local search algorithms, to the genetic algorithm.</p><p>The motivation and incentive for designing and imple-menting hybrid algorithms is to improve the solutions obtained by using either of the heuristic algorithms alone. The reason for combining the genetic algorithm (GA) with a local search algorithm is that they compliment each other. The local search algorithm will try to optimize locally, while the genetic algorithm will try to optimize globally. In other words, the resulting hybrid scheme produces improved reliability by exploiting the "global" nature of the genetic algorithm as well as the ''local'' improvement capabilities of the engineered conditioningbased operators. The local search operators used in this study to create hybrid algorithms are engineered conditioning ( E C ) , and engineered2 conditioning (E' C ) .</p><p>Readers interested in other schemes to improve GA search are directed to case studies using greedy crossover, post-evaluation, edge recombination, and other forms of local improvement. The greedy crossover operator has been shown to improve genetic algorithm results when applied to job shop scheduling, set covering, and traveling salesman problems [ 191, <ref type="bibr" target="#b15">[20]</ref>. In addition, edge recombination <ref type="bibr" target="#b31">[36]</ref> and local improvement [ 171 have been proposed to improve traveling salesman problem results. The post-evaluation scheme has been shown to be effective at improving results of set covering type problems such as multiple fault diagnosis [2 I , 26, 271. Post-evaluation (the predecessor of our engineered conditioning operator) is a process applied to the final GA produced solution, and has no direct relationship to the intemal workings of the GA. In addition, the post-evaluation scheme has been shown to produce reliability results that are well below results achieved with the EC operator (but, still better than the standard GA alone).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Engineered Condirioning-GA Hybrid (EC-GA)</head><p>The EC-GA hybrid algorithm was developed by Potter et al. <ref type="bibr" target="#b23">[28]</ref> to achieve higher reliability in solving MFD problems. The local improvement offered by the EC operator is used to improve the diagnosis obtained from the global searching of the genetic algorithm. This hybrid algorithm works as follows: run the genetic algorithm as shown in Fig. <ref type="figure">2</ref>, but after each generation remove the fittest individual, apply the EC operator to this individual, and introduce the hopefully improved individual back into the population. Similar types of hybridization have been used in the past when designing new genetic operators <ref type="bibr" target="#b6">[7]</ref>, when adding local improvement methods [ 171, and when incorporating problem specific knowledge [ 151. Typically, these hybridization techniques are very much problem specific and care should be taken when enhancing genetic algorithms so as not to harm the benefits offered by genetic algorithms [ 141. The EC operator is generic enough to be applicable in a variety of domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Engineered Conditioning Twice-GA Hybrid (EC '-GA)</head><p>In this variation of the EC-GA hybrid, the motivation stems from the somewhat successful operation of the EC* 'Reasonable estimates were used here because of excessive runtimes algorithm. The main idea behind the EC2-GA hybrid is that if the EC operator is applied twice after applying the standard GA operators, then hopefully additional local improvement can be obtained after each generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C, Phased-in Engineered Conditioning-GA Hybrid (pEC-GA)</head><p>One of the problems with the overuse of local improvement operators is that in the early stages of GA evolution, these operators have the potential to destroy the randomness of the population. This early reduction in diversity can possibly lead to premature convergence and result in overall poor performance. Therefore, the phased-in variant, pEC-GA, reduces the chances of applying the EC operator in early generations. In particular, the EC operator is applied only when the following condition is satisfied:</p><formula xml:id="formula_5">Random () &lt; (Generations /Threshold)2</formula><p>The value of Random ( ) is uniformly distributed between 0 and 1, and the parameter Threshold is adjustable (e.g., if Threshold = 4, then EC is guaranteed to be applied during the fourth generation and each generation thereafter, while the probabilities of application in the first 3 generations are 1 /16, 1 /4, 9/16, respectively). This variant may be more computationally efficient since in many cases the application of the EC operator is skipped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Engineered * Conditioning-GA Hybrid (E C-GA)</head><p>Because of the success of the E* C* local search algorithm (which uses double substitution) in generating reliable diagnoses consistently in our test problems (see Table <ref type="table" target="#tab_2">I</ref>), it was decided that the E* C operator should be used as a local improvement operator for the genetic algorithm. As with the other hybrids, when a new generation is created the fittest individual is removed, modified, and then re-introduced into the evolutionary process. With the EC operator, one is able to produce a better diagnosis by going to a neighbor in the search space, while with the EC2 operator (applying EC twice), one is able to move to a better neighbor and then move to an even better neighbor. A problem occurs when there is a valley around the current solution point; there are no immediate neighbors of higher quality, yet the neighbors of the neighbors are of higher quality. Unfortunately, even though the EC2 operator can take two steps, it cannot get to these higher quality neighbors because in order to do so it would have to first step down. The E 2 C scheme remedies this situation since it can step over immediate neighbors. Consequently, it should be less prone to getting stuck at local optima and thus obtain superior solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EMPIRICAL COMPARISONS</head><p>Many experiments were conducted to test the reliabilities of the various algorithms presented in this paper. Given a problem consisting of a tendency matrix of symptom-disorder associations, a vector of prior probabilities for disorders, and a current symptom set, an exhaustive algorithm is run (see <ref type="bibr" target="#b22">[27]</ref> ). Then the genetic algorithm of Fig. <ref type="figure">2</ref>, the local search algorithms of Section IV, and the hybrid algorithms of Section V are all run. The best solution found by each of the heuristic algorithms is compared to a globally optimal solution found with the exhaustive search algorithm to determine a percentage reliability for each heuristic (i.e., the percentage of the cases in which they find a globally optimal solution out of either 1023 or 4095 trials, depending on the experiment setup).</p><p>The complexity of a diagnosis problem is determined by the size of the tendency matrix. A tendency matrix of size m X n associates m symptoms and n disorders, and if a globally optimal solution is required then the best known algorithm (e.g., exhaustive search) is O(2"). (All the tendency matrices used in these experiments were generated randomly, but can be replaced by exact values when used for real-world problems.)</p><p>To test how the reliabilities of the heuristic algorithms are affected by the size of the problem, the following tendency matrix sizes are used in the experiments: 10 x 10, 10 x 12, 10 x 15, 10 x 20 12 x 10, 12 x 12, 12 x 15, 12 x 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Size:</head><p>A second characteristic of the tendency matrix was also varied in these experiments, namely the density of the matrix. The density indicates what fraction of the entries in the matrix are nonzero (i.e., have positive causal associations in the original matrix). The following densities are used for each problem size. Density: one-ThiRD (TRD), one-HaLF (HLF), Three-QuarTers (TQT) A total of 24 experiments (see Table <ref type="table" target="#tab_2">I</ref>) were conducted using the algorithms. In each experiment, the best diagnoses are found for all possible nonempty symptom subsets (a total of 2" -1 diagnostic trials per experiment). For a given experiment (one of the 24), when the genetic algorithm or one of the hybrid GA algorithms is run, it is actually run 10 times with varying population sizes (ranging from 50 to 140 in increments of 10). This is done to analyze the effects of the size of the population on the reliabilities of these algorithms. In all, approximately 3.2 million diagnostic trials support the results presented in Table <ref type="table" target="#tab_2">I</ref>.</p><p>Table <ref type="table" target="#tab_2">I</ref> contains (some of) the reliability results obtained in this study; it shows the reliability of each algorithm for each combination of matrix size and density. In addition, four bar charts (Fig. <ref type="figure">3(a)-(d</ref>)) comparing the reliabilities are shown. These charts summarize the reliabilities of the m x 10, m x 12, m X 15, and m X 20 data from Table <ref type="table" target="#tab_2">I</ref> by simply averaging the table values. Note, for a 10 x n matrix problem, a reliability of 99 percent indicates that the heuristic algorithm actually found a globally optimal solution in more than 1013 cases out of 1023. For a 12 X n matrix problem, a reliability of 99 percent indicates that the heuristic algorithm actually found the globally optimal solution in more than 4055 cases out of 4095.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>Although the tested search techniques are not applied to real-world MFD data (we follow <ref type="bibr" target="#b20">[25]</ref> by generating tendency matrix data at random but with certain constraints such as the density), the reliabilities obtained do shed light on the behavior of these methods. In summary, the following conclusions can be made about the different heuristics and hybrids used, namely, GA, EC*, EC-GA, pEC-GA, <ref type="figure">EC2-GA, E' C * , E' C-GA</ref>.</p><p>1) The pure algorithms GA and EC* exhibited the least reliability, while the GA-hybrids were substantially more reliable than the pure GA. The GA was particularly poor at low population levels, but became competitive with EC* at higher population levels (this is expected as the population size becomes a larger percentage of the search space). 2) In all the cases considered, the EC-GA hybrid turned out to be significantly better than the GA.</p><p>Clearly, the local improvement nature of the EC operator is certainly playing an important role in improving the dominant individual of the GA populations.</p><p>3) In almost all the cases examined, the EC-GA performed better than EC* (note that the averages in the charts are all better). This difference can be explained due to the greedy nature of the EC* algorithm, which leads to its premature convergence. This conclusion also supports the theory that, GA's are globally optimizing techniques, and are robust in nature. 4) The reliabilities offered by the variations tried on the EC-GA (i.e., pEC-GA and EC2-GA) do not show any substantial improvement over the reliabilities of the EC-GA. However in some cases, pEC-GA exhibited marginally better performance than EC-GA, so that in at least these cases it evidently provides better convergence. Also, there may be some saving in the cost of computation when pEC-GA is used rather than EC-GA. On the other hand, the reliabilities of EC'-GA indicate that taking a second relatively small step does not help very much. What is needed is one bigger step. 6) The E2C-GA hybrid is found to be the best of all methods tested and in several cases the reliabilities are 100 percent! Furthermore, in all but one of the cases, the reliabilities are greater than 99 percent. Unfortunately, E2C-GA (and to a lesser extent E* C * ) suffer from relatively slow execution times, since the E2C operator is of quartic complexity. Still, their execution times are orders of magnitude less than exhaustive algorithms, that is, measured in minutes to hours rather than months to years. 7) Although no actual statistics are reported here, when ranking execution speeds of the algorithms we obtain the following ordering (fastest to slowest): EC*, EC-GA, GA, pEC-GA, EC2-GA, E 2 C * , and E2C-GA. EC-GA is faster than GA because, although it contains additional computation due to the EC operator, it converges sooner (i.e., the total number of individuals processed is much less). However, the differences in execution speed for EC-GA, GA, and pEC-GA are small compared to the others, where the differences can be characterized as substantial but less than an order of magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8)</head><p>Finally, combining high reliability and fast execution speed, the EC operator offers a good compromise when used within the genetic algorithm.</p><p>In the future, we plan to generalize our local improvement operators into a systematic family of parameterizable operators. The implementation of the E2 C operator originated from an idea to develop a general k-hop operator for MFD problems. This proposed k-hop operator will be able to hop (or move) to a point that is k-hops away from the given point in an n-dimensional search space (hypercube). The advantages of the k-hop operator are expected to result from improved global vision as k gets larger.</p><p>If k is equal to I , then the hyper-conditioning (HC) operator, as we call it, can move from the current point to any of n immediately adjacent neighbors. This corresponds to changing 1 bit in the gray code, i.e., the two points must be at a Hamming distance of I . Consequently, the complexity of the HC operator is linear. For example, in a four-dimensional hypercube the point (or node) 1 0 0 1 has the following four neighbors: 1 1 0 1,</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>[</head><label></label><figDesc>see Fig. 11. Given the observed manifestations Mi = m,, m4, m5, m7, m8, m9, mlo&gt; and the diagnosis DI = 0011oooO1001100. We find that the factors and the adapted relative likelihood Ladapted of this (optimal) diagnosis are L , = 1.99315527715718e -01 &amp; = 2.11844640547108e -01 L3 = 1.82012538561086e + 00 Laddpted(DI, M ' ) = 7.68528401831913e -02.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>5 )Fig. 3 .</head><label>53</label><figDesc>The E 2 C * algorithm does just this; it can take a larger step in the search space by flipping more bits simultaneously. E ' C * is better than the other methods (except the hybrid E 2 C-GA), and provides highly reliable solutions for all the test cases. This improvement can be attributed to the more vigorous hopping in the search space (in other words, to the improved global vision) of the E' C operator. Bar charts comparing reliabilities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>di d i di da d l da d l ds dg d x dll d n d n dlr dis pi</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="2">0.12 0.14 0.39</cell><cell cols="2">0.M 0.01 021</cell><cell>026 019</cell><cell>059</cell><cell cols="2">029 006</cell><cell>047</cell><cell>056</cell><cell>041</cell><cell>0.06</cell></row><row><cell>ml</cell><cell cols="5">058 OW 0.W OW OW 025 OW 096</cell><cell cols="3">OW OW 085</cell><cell>OW</cell><cell>074</cell><cell>OW 038</cell></row><row><cell>m i</cell><cell cols="9">0.W 0.W 0.W OW OW 015 0.81 000 032 OW 036 077 0.30 061 OW</cell></row><row><cell>ml</cell><cell>OW</cell><cell cols="6">0.03 044 OW OW 0.00 OW 011 OW 097</cell><cell cols="2">064 OW 0.61 085 OW</cell></row><row><cell>mr</cell><cell>043</cell><cell>0.67 079</cell><cell cols="7">0.W 026 072 007 0.W OW OW 084 OW 0.W 042 OM</cell></row><row><cell>m ,</cell><cell cols="6">0.46 0.10 0.00 058 000 OW 046 OW 057</cell><cell cols="2">040 OW</cell><cell>051</cell><cell>OW 097</cell><cell>OW</cell></row><row><cell>me</cell><cell>091</cell><cell cols="2">0.W 0.00 OW OW</cell><cell cols="6">I W 028 OW OW OW OW OW 0.00 OW 012</cell></row><row><cell>m,</cell><cell cols="9">0.W 0.94 007 028 OW 0.00 OW OW 097 OW 0.00 091 048 0.23 0.72</cell></row><row><cell>ms</cell><cell>O.W</cell><cell>0.00 014</cell><cell>0.17 024</cell><cell cols="6">0.W 010 0.00 026 OW OW 0.05 OW 0.W O.W</cell></row><row><cell>me</cell><cell>0.W</cell><cell cols="8">OW 0.13 012 017 OM OW OW 0.97 OW OW O.W 0.43 0.08 OW</cell></row><row><cell>mm</cell><cell cols="9">OW 063 007 075 012 OW OW 045 OW 088 021 0.00 045 0.86 019</cell></row><row><cell cols="10">Fig. 1. Prior probability and tendency matrix: 10 x 15 one-half dense.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>nonempty set of disorders (i.e., dis-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>eases or faulty components).</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>a finite nonempty set of manifestations (i.e.,</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>symptoms).</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>a relation that is a subset of D x M . This re-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>lation pairs diseases with associated symptoms</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>such that (d, m) E C means that disease d may</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>cause symptom m.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>a subset of M that identifies the observed man-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ifestations. Note that manifestations not iden-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tified in M + are assumed to be absent.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>A diagnosis DI (a subset of D) identifies the disorders that</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>are possibly responsible for the symptoms in M + . Diag-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>nosis DI covers M + if each of the individual manifesta-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tions in M + is associated with at least one of the disorders</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>in DI as determined using C . As with M + , disorders not</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>identified in DI are assumed to be absent.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Associated with each disorder dJ in D is a prior proba-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>bility pJ where 0 &lt; pJ &lt; 1. Values are assumed to exist</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>and disorders in D are assumed to be independent [23].</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Associated with each "causal association" in C is a causal</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>strength c,, such that 0 I cIJ I 1, which represents how</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>frequently (i.e., the tendency which) a disorder d, causes</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>manifestation m,. The causal strength represents the con-ditional probability P(dJ causes m, I d,), which has the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>advantage of being unaffected by coincident disorders,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Individuals thus selected are further operated on Function: Because of the disadvantages mentioned in with other genetic operators such as crossover and muta-Section 11-A, the relative likelihood is not well suited for tion.direct use as an objective function for the genetic algo-The purpose of the crossover operator is to produce new rithm or our local search algorithms; however, it is easily chromosomes that are distinctly different from their par-adapted [26J. Doing so allows the search to converge on Max-Generations do // initial population for j = 1 to Pop-Size by 2 do ( P i U l &gt; P i [i+ll) = Mate-Selection (p-1); if Random () &lt; Crossover-Prob then</figDesc><table><row><cell>( p i U l , ~, U+ll) =Crossover (p, u ] , p , u+1]);</cell><cell></cell></row><row><cell>Crossover-Prob Mutate-Prob Max-Generations Term-Generations chance of being selected to contribute to the next gener-= 0.6 = 0.0333 = SO = S endfor; for j = 1 to Pop-Size do P i VI = Mutate (P, u]); // flip each bit with Mutate-Prob f i u ] = Objective-Fun (pi U]); end for; if Max cf,-l) &gt; Min v i ) then //elitist policy endif; //if there is no change in the fittest individual // in Term-Generations, then terminate if No-Change () then endif; p i [min I = p i -l [ m a r ] ; exit; endfor; ation. endif; Fig. 2. Genetic algorithm used for MFD</cell><cell>Population Size i i Probability of Crossover / / Probability of Mutation / / Maximum number of Generations / / Terminate if no change to fittest i / individual in Tern-Generations 1 ) The ModiJied Relative Likelihood Objective</cell></row></table><note><p><p>(Po, fo) = Generate-Pop 0;</p>for i = 1 to</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I SLMMARY</head><label>I</label><figDesc>OF RELlABlLlTlES'</figDesc><table><row><cell></cell><cell></cell><cell>GA</cell><cell>EC*</cell><cell>EC-GA</cell><cell>pEC-GA</cell><cell>EC'-GA</cell><cell>E'C*</cell><cell>E' C-GA</cell></row><row><cell>I O x 10</cell><cell>TRD</cell><cell>61.58</cell><cell>96.87</cell><cell>88.27</cell><cell>99.12</cell><cell>98.92</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell></cell><cell>HLF</cell><cell>79.86</cell><cell>91.89</cell><cell>95.60</cell><cell>95.70</cell><cell>95.11</cell><cell>99.02</cell><cell>99.80</cell></row><row><cell></cell><cell>TQT</cell><cell>81.23</cell><cell>93.84</cell><cell>96.87</cell><cell>97.65</cell><cell>97.65</cell><cell>98.90</cell><cell>100.00</cell></row><row><cell>IO x 12</cell><cell>TRD</cell><cell>74.19</cell><cell>93.84</cell><cell>100.00</cell><cell>94.72</cell><cell>94.62</cell><cell>96.87</cell><cell>96.97</cell></row><row><cell></cell><cell>HLF</cell><cell>71.36</cell><cell>90.62</cell><cell>94.43</cell><cell>94.43</cell><cell>94.92</cell><cell>99.32</cell><cell>99.51</cell></row><row><cell></cell><cell>TQT</cell><cell>64.81</cell><cell>93.16</cell><cell>93.35</cell><cell>84.13</cell><cell>93.35</cell><cell>97.46</cell><cell>99.90</cell></row><row><cell>10 x 15</cell><cell>TRD</cell><cell>74.10</cell><cell>98.53</cell><cell>99.90</cell><cell>99.71</cell><cell>99.71</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell></cell><cell>HLF</cell><cell>69.50</cell><cell>92.38</cell><cell>94.53</cell><cell>94.44</cell><cell>94.23</cell><cell>99.32</cell><cell>99.90</cell></row><row><cell></cell><cell>TQT</cell><cell>61.68</cell><cell>93.16</cell><cell>93.74</cell><cell>94.23</cell><cell>93.83</cell><cell>96.58</cell><cell>100.00</cell></row><row><cell>10 x 20</cell><cell>TRD</cell><cell>65.10</cell><cell>93.06</cell><cell>99.22</cell><cell>95.41</cell><cell>95.21</cell><cell>99.41</cell><cell>99.61</cell></row><row><cell></cell><cell>HLF</cell><cell>63.73</cell><cell>87.49</cell><cell>90.42</cell><cell>90.13</cell><cell>90.52</cell><cell>98.73</cell><cell>99.51</cell></row><row><cell></cell><cell>TQT</cell><cell>61.39</cell><cell>93.65</cell><cell>93.65</cell><cell>94.43</cell><cell>93.45</cell><cell>96.58</cell><cell>99.90</cell></row><row><cell>12 x 10</cell><cell>TRD</cell><cell>88.08</cell><cell>98.83</cell><cell>98.61</cell><cell>99.58</cell><cell>99.61</cell><cell>99.93</cell><cell>100.00</cell></row><row><cell></cell><cell>HLF</cell><cell>76.36</cell><cell>87.01</cell><cell>94.21</cell><cell>94.38</cell><cell>95.63</cell><cell>99.12</cell><cell>99.78</cell></row><row><cell></cell><cell>TQT</cell><cell>72.01</cell><cell>87.84</cell><cell>92.50</cell><cell>94.19</cell><cell>94.58</cell><cell>98.44</cell><cell>99.85</cell></row><row><cell>12 x 12</cell><cell>TRD</cell><cell>81.20</cell><cell>96.73</cell><cell>96.61</cell><cell>98.56</cell><cell>98 41</cell><cell>99.95</cell><cell>99.95</cell></row><row><cell></cell><cell>HLF</cell><cell>69.40</cell><cell>89.62</cell><cell>92.11</cell><cell>93.92</cell><cell>93.02</cell><cell>98.51</cell><cell>99.44</cell></row><row><cell></cell><cell>TQT</cell><cell>67.81</cell><cell>94.14</cell><cell>94.77</cell><cell>95.95</cell><cell>95.97</cell><cell>96.90</cell><cell>99.95</cell></row><row><cell>12 x 15</cell><cell>TRD</cell><cell>88.06</cell><cell>97.51</cell><cell>99.51</cell><cell>99.37</cell><cell>99.19</cell><cell>100.00</cell><cell>100.00</cell></row><row><cell></cell><cell>HLF</cell><cell>64.00</cell><cell>89.21</cell><cell>93.72</cell><cell>94.02</cell><cell>93.31</cell><cell>99.24</cell><cell>99.73</cell></row><row><cell></cell><cell>TQT</cell><cell>60.90</cell><cell>88.23</cell><cell>91.70</cell><cell>92.50</cell><cell>92.55</cell><cell>96.75</cell><cell>99.93</cell></row><row><cell>12 x 20</cell><cell>TRD</cell><cell>80.68</cell><cell>95.43</cell><cell>99.27</cell><cell>97.17</cell><cell>97.09</cell><cell>99.98</cell><cell>99.95'</cell></row><row><cell></cell><cell>HLF</cell><cell>60.49</cell><cell>88.27</cell><cell>90.26</cell><cell>91.43</cell><cell>90.43</cell><cell>98.97</cell><cell>99.95h</cell></row><row><cell></cell><cell>TQT</cell><cell>60.76</cell><cell>91.64</cell><cell>94.43</cell><cell>93.87</cell><cell>94. I6</cell><cell>96.9</cell><cell>99.95h</cell></row></table><note><p><p>~</p>"All genetic algorithm experiment results reported in this table were run with a population size of 50.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the faculty, staff, and students of the Artificial Intelligence Programs and the Department of Computer Science, University of Georgia, for their help and cooperation with this research project. In addition, we wish to express our thanks to the referees for their helpful comments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 0 1 0, 0 0 0 1, and 1 0 0 0. Note that the first two [I41 D. E. Goldberg, Generic Algorithms in Search, Optimization, and Machine Learning. points be found by the test' the last [I51 J . J . Grefenstette, "Incorporating problem specific knowledge into genetic algorithms," in Genetic Algorithms and Simulated Anneal- two could be found by the subset test. Thus when k = 2 the complexity is quadratic, when k = 3 it is cubic, and so on. If one designs an algorithm that allows k to range from n down to 1, the reliability will be guaranteed to be 100 percent, i.e., the algorithm will be exact (not heuristic). By the binomial theorem, the complexity of this algorithm will be O(2"). Although such an algorithm would be of little practical use, the idea of selecting a set of values fork, e.g., [4, 2, l l , is quite useful.</p><p>Making such a selection will be dependent on the size of the problem, the level of reliability desired, and on any applicable time constraints. A trade-off between execution time and reliability is provided; the higher the value of k the greater the reliability at the cost of increased execution time. Currently, he works part time as a programmer and consultant for University Computing and Networking Services. the primary computing resource service organization at the University of Georgia. In addition, he is involved with the Electronic Design and Maintenance Department, where he designs and programs microcontroller pert database systems, hyper-semantic data modeling, and advanced information system design. His work with genetic algorithms began while he applications for research personnel. He is especially interested in code optimization.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Organizing and understanding beliefs in advice-giving diagnostic systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bourne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Orogo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Uckun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Brodersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowledge, Data Eng</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="269" to="280" />
			<date type="published" when="1991-09">Sept. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Genetic Algorithms in Parametric Design of Aircraft</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Bramlette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Genetic Algorithms</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Van Nostrand Reinhold</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rule-Based Expert Systems</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Buchanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Shortliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Models versus rules, deep versus compiled, content versus form</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chandrasekaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Expert</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="1991-04">Apr. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Introduction to Artificial Inrelligence</title>
		<author>
			<persName><forename type="first">E</forename><surname>Chamiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcdermott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diagnostic reasoning based on structure and behavior</title>
		<author>
			<persName><forename type="first">R</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Arrijical Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="347" to="410" />
			<date type="published" when="1984-12">Dec. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Handbook of Genetic Algorithm</title>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Van Nostrand Reinhold</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Diagnosing Multiple Faults</title>
		<author>
			<persName><forename type="first">Si</forename><forename type="middle">J</forename><surname>De Kleer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Art$cial Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="130" />
			<date type="published" when="1987-04">Apr. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Characterizing diagnoses</title>
		<author>
			<persName><forename type="first">J</forename><surname>De Kleer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mackworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Nut. Conf. Artificial Intell</title>
		<meeting>8th Nut. Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="1990-08">Aug. 1990</date>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="324" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Covers and packings in a family of sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edmonds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Amer. Mathemat. Soc</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="494" to="499" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On improving diagnostic decision making</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Gandham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991-09">Sept. 1991</date>
		</imprint>
		<respStmt>
			<orgName>Univ. Georgia</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M.S. thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Computers and Intractability; A Guide to the Theory of NP-Completeness</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<publisher>Freeman</publisher>
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The effects of population size, heuristic crossover and local improvement on a genetic algorithm for the traveling salesman problem</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Genesereth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">P</forename><surname>Jog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Gucht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings o f t h e 3rd International Conference on Genetic Algorithms</title>
		<meeting>o f t h e 3rd International Conference on Genetic Algorithms<address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Kaufmann</publisher>
			<date type="published" when="1984-12">Dec. 1984. 1989</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="411" to="436" />
		</imprint>
	</monogr>
	<note>The use of design descriptions in automated diagnosis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A mechanism for forming composite explanatory hypotheses</title>
		<author>
			<persName><forename type="first">J</forename><surname>Josephson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Tanner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cyber</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="445" to="454" />
			<date type="published" when="1987-06">May/ June 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Greedy genetics</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Liepins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Hilliard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Genetic Algorithms and Their Applications: Proceedings of the 2nd International Conference on Genetic. Algorithms</title>
		<meeting><address><addrLine>Hillsdale, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Erlbaum</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Genetic algorithm applications to set covering and traveling sales problems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Liepins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Hilliard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ORIAI: The Integration of Problem Solving Strategies</title>
		<meeting><address><addrLine>Brown</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A genetic algorithm approach to multiple fault diagnosis</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Liepins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Potter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Genetic Algorithm Handbook</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Evolution in time and space-The parallel genetic algorithm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Muhlenbein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Genetic Algorithms, G. Rowlins</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A probabilistic causal model for diagnostic problem solving, Part 1: Integrating symbolic causal inference with numeric probabilistic inference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Reggia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="146" to="162" />
			<date type="published" when="1989-04">Mar./Apr. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A probabilistic causal model for diagnostic problem solving, Part 11: Diagnostic strategy</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Sysr. Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="395" to="406" />
			<date type="published" when="1987">1987</date>
			<pubPlace>MayiJune</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A connectionist model for diagnostic problem solving</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="285" to="298" />
			<date type="published" when="1989-04">Mar./ Apr. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Diagnosis, parsimony, and genetic algorithms</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E M R</forename><surname>Tonn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hilliard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Liepins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Purucker</surname></persName>
		</author>
		<author>
			<persName><surname>Goeltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rdInt. Conf. Industrial, Eng. Applicat</title>
		<meeting>3rdInt. Conf. Industrial, Eng. Applicat</meeting>
		<imprint>
			<date type="published" when="1990-07">July, 1990</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A comparison of methods for diagnostic decision making</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">R</forename><surname>Weyrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst. Applicat. Int. J</title>
		<imprint>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="425" to="436" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving the reliability of heuristic multiple fault diagnosis via the EC-based genetic algorithm</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Tonn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Gandham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Lapena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks, Complex Problem-Solving Technol</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="5" to="23" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note>A. Intell.: Int. J . Artificial Intell.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">IDA-NET: An intelligent decision aid for battlefield communications network configuration</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Pitts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><surname>Caramadre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th IEEE Conf. Artijicial Intell. Applications (CAIA&apos;92</title>
		<meeting>8th IEEE Conf. Artijicial Intell. Applications (CAIA&apos;92</meeting>
		<imprint>
			<date type="published" when="1992-06">Mar. 6 , 1992</date>
			<biblScope unit="page" from="247" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Diagnostic expert systems based on a set covering model</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Reggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J . Man-Machine Studies</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="437" to="460" />
			<date type="published" when="1983-11">November, 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A theory of diagnosis from first principles</title>
		<author>
			<persName><forename type="first">R</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="95" />
			<date type="published" when="1987-04">Apr. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Abductive and default reasoning: A computational core</title>
		<author>
			<persName><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Levesque</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Nut. Conf. Artificial Intell</title>
		<meeting>8th Nut. Conf. Artificial Intell</meeting>
		<imprint>
			<date type="published" when="1990-08">Aug. 1990</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="343" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Schedule Optimization Using Genetic Algorithms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Syswerda</surname></persName>
		</author>
		<editor>L. Davis</editor>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Van Nostrand Reinhold</publisher>
			<biblScope unit="page" from="332" to="349" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Handbook of Genetic Algorithms</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An experimental study of criteria for hypothesis plausibility</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tuhrim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reggia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goodall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Experiment., Theoret. Arr$cial Intell</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="129" to="144" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A Simulation Study of Heuristic Techniques for Diagnostic Decision Making</title>
		<author>
			<persName><forename type="first">R</forename><surname>Weyrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AI. Simulat. Con$ 1990SCS Eastern Mulficonf</title>
		<meeting>AI. Simulat. Con$ 1990SCS Eastern Mulficonf</meeting>
		<imprint>
			<date type="published" when="1990-04">Apr. 1990</date>
			<biblScope unit="page" from="217" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Scheduling problems and traveling salesmen: The genetic edge recombination operator</title>
		<author>
			<persName><forename type="first">D</forename><surname>Whitley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Starkweather</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fuquay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Van Nostrand Reinhold</publisher>
			<biblScope unit="volume">17</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
