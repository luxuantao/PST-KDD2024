<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Body Movements for Affective Expression: A Survey of Automatic Recognition and Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michelle</forename><surname>Karg</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ali-Akbar</forename><surname>Samadani</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rob</forename><surname>Gorbet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kolja</forename><surname>K€ Uhnlenz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jesse</forename><surname>Hoey</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dana</forename><surname>Kuli</surname></persName>
						</author>
						<author>
							<persName><forename type="first">R</forename><surname>Gorbert</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<addrLine>200 University Avenue West</addrLine>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country>Canada, Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Centre for Knowledge Integration</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<addrLine>200 University Avenue West</addrLine>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Dept. of EE and CS</orgName>
								<orgName type="institution">Coburg University of Applied</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Sciences and Arts</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">David R. Cheriton School of Computer Science</orgName>
								<orgName type="institution">Univer-sity of Waterloo</orgName>
								<address>
									<addrLine>200 University Avenue West</addrLine>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Body Movements for Affective Expression: A Survey of Automatic Recognition and Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1BD1CD533CE41B77291CD8AD045ED37B</idno>
					<idno type="DOI">10.1109/T-AFFC.2013.29</idno>
					<note type="submission">received 21 June 2013; revised 23 Oct. 2013; accepted 28 Oct. 2013.; date of publication 11 Nov. 2013; date of current version 13 Mar. 2014.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Body movements communicate affective expressions and, in recent years, computational models have been developed to recognize affective expressions from body movements or to generate movements for virtual agents or robots which convey affective expressions. This survey summarizes the state of the art on automatic recognition and generation of such movements. For both automatic recognition and generation, important aspects such as the movements analyzed, the affective state representation used, and the use of notation systems is discussed. The survey concludes with an outline of open problems and directions for future work.</p><p>Index Terms-Movement analysis, recognition of affective expressions, generation of affective expressions Ç</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A FFECTIVE computing aims to enhance human-computer interaction (HCI) and human-robot interaction (HRI) through affective communication to create a more intuitive, engaging, and entertaining interaction. During the interaction, affective states can be expressed and recognized through facial expressions, speech, body movements, and physiological parameters. Automatic recognition of human affective expressions and generation of expressive behavior for virtual avatars and robots are key challenges in this research area. Several surveys address detection of affective states in general [1], [2], [3], [4], [5], through facial or/and audio expressions [6], [7] and generation of affective expressions [4], [8]. As facial expressions and speech dominate during face-to-face interaction, these are the modalities that have been predominantly studied in communication of nonverbal behavior, psychology, and computer science to date [2], [9], [10], [11]. Yet, there exists evidence from communication of nonverbal behavior and psychology research that body movements also convey affective expressions, e.g., [12], [13], [14], [15]. Considering body movement as a modality for affective computing is particularly suitable in situations where the affective state is estimated from a distance [16], to retrieve expressions which are less susceptible to social editing [17], and to communicate affective states which are easier conveyed through movement [18].</p><p>A recent survey <ref type="bibr" target="#b18">[19]</ref> reviews the literature on affect recognition from body posture and movement, and discusses the main challenges in affect recognition from body posture and movement, including inter-individual differences, impact of culture and multi-modal recognition, and the challenges in collecting appropriate data sets and ground truth labeling. Computational models have been developed for both automatic recognition and generation of affectexpressive movements. 1 A large body of work has emerged in recent years developing these computational models; this survey is intended to synthesize the findings of these studies, and to identify key contributions and open research questions. Two significant characteristics of these computational models are 1) the representation of movements in physical space and time, and 2) the representation of affect.</p><p>To provide a comprehensive overview of affectexpressive movements studied to date in HCI/HRI, we introduce a suitable movement categorization and summarize works studying similar movements. We discuss the use of movement notation systems in automatic recognition and generation of affect-expressive movements. Movement notation systems, commonly used in the dance community, can provide a systematic approach for the choice of movement descriptors and facilitate knowledge transfer between communication, psychology, and computer science. This elaboration on the movements studied to date and the use of movement notation systems for both automatic recognition and generation provides complementary information to the previous survey discussing the importance of postural and dynamic features for automatic recognition <ref type="bibr" target="#b18">[19]</ref>.</p><p>A categorical or dimensional approach can be used for representing affective states. For both automatic recognition and generation studies, we analyze the set of considered affective states and their representation. We report on the common results regarding the expressiveness of affective M. Karg, A.-A. Samadani, and D. Kuli c are with the Department of Electrical and Computer Engineering,</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. In this work, we introduce the term affect-expressive movement to mean that subset of expressive movements whose purpose is to convey affect.</p><p>states and whether these results can be linked to psychological studies. This detailed analysis facilitates the selection of a suitable representation for affective states <ref type="bibr" target="#b18">[19]</ref>.</p><p>The outline of the paper is as follows: Section 2 briefly summarizes relevant background information on the definition, expression, and representation of affective states in HCI/HRI. Section 3 introduces a movement categorization for summarizing the state of the art in recognition and generation of affect-expressive movements, and discusses movement notation systems and their utility for affectexpressive movement analysis. Selected studies on human perception of affect-expressive movements are discussed in Section 4. Studies on automatic recognition of affect-expressive movements are summarized in Section 5. Section 6 provides an overview of generative models to synthesize affect-expressive movements for virtual agents or robots. Current achievements are summarized in <ref type="bibr">Section 7</ref> and open questions are discussed. Concluding remarks are drawn in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">AFFECT: DEFINITION, EXPRESSION, AND REPRESENTATION IN HCI/HRI</head><p>Affective phenomena (broadly termed affect <ref type="bibr" target="#b19">[20]</ref>) include emotions, feelings, moods, attitudes, temperament, affective dispositions, and interpersonal stances <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. A categorization of these terms based on event focus, intrinsic and transactional appraisal, synchronization, rapidity of change, behavioral impact, intensity, and duration is provided in <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, e.g., emotions change rapidly, are short-term and intense, whereas mood covers a longer time-span and changes more slowly. Among the affective phenomena, emotions are those most widely studied in HCI and HRI <ref type="bibr">[21, p. 4</ref>]. Scherer defines emotion as "an episode of interrelated, synchronized changes in the states of all or most of the five organismic subsystems in response to the evaluation of an external or internal stimulus event as relevant to major concerns of the organism" <ref type="bibr" target="#b21">[22]</ref>. The five subsystems are: cognitive (for evaluation), physiological (for regulation), motivational (for preparation of an action), subjective feeling, and motor expression (for communication) <ref type="bibr" target="#b21">[22]</ref>. Theories of emotion expression are often based on facial expressions; fewer studies have been conducted that investigate the extent to which existing theories predict and explain bodily expressions of emotion <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Effective emotional communication requires both the ability to send or encode one's own emotion in an appropriate and comprehensible manner and the ability to receive or decode the emotions of others <ref type="bibr" target="#b20">[21]</ref>. For virtual agents and robots, these two skills refer to automatic generation and recognition of emotional expressions, respectively. These two modules can be integrated into higher-level computational models which cover the generation of appropriate emotions, and relations to cognitive and motivational subsystems <ref type="bibr" target="#b28">[29]</ref>.</p><p>The terms affect and emotion have been used inconsistently, often interchangeably, in the field of HCI/HRI, and both terms can be found in studies on automatic recognition and generation of affect-expressive movements. Here, we are concerned with body movements that can convey both short-term emotions and long-term moods, e.g., depression influences the kinematics of walking <ref type="bibr" target="#b29">[30]</ref>. When covering both long-term and short-term affective phenomena, we use the broader term affective states in this survey. Within this survey, we use the term emotion when studies explicitly address emotions.</p><p>Automatic recognition of affective states is based on observing expressions. Humans can control their expressions to a certain extent, e.g., using display rules to achieve a social goal <ref type="bibr">[11, p. 72]</ref>. This gives rise to the possibility of a difference between the internal experience and external expression. This possible discrepancy is most widely studied for the communication of emotions. The communication of emotions can be either spontaneous or strategic <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Spontaneous communication is involuntary and the content is non-propositional, 2  whereas strategic or symbolic communication is goal-oriented and the content is propositional <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Considering automatic recognition, it is important to note that an observed expression may not necessarily be the observable manifestation of an internal state, but rather, displayed to achieve a social goal. Within the scope of this survey, which focuses on HCI and HRI studies, this difference is not further explored as most studies on automatic recognition and generation assume a correspondence between the expressed and internally felt state.</p><p>Evidence from psychology indicates that affective states are expressed in body movements <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>. During daily human-human interaction, humans pay attention to expressive body and activity cues almost as often as to expressive facial, indirect verbal, and context cues <ref type="bibr" target="#b32">[33]</ref>. A recent study highlights the utility of bodily cues for the discrimination between intense positive and negative emotions <ref type="bibr" target="#b17">[18]</ref>. Gelder et al. suggest that bodily cues are particularly suitable for communication over larger distances, whereas facial expressions are more suitable for a finegrained analysis of affective expressions <ref type="bibr" target="#b34">[35]</ref>. Studies comparing bodily expression versus facial expressions are summarized in <ref type="bibr" target="#b18">[19]</ref>, and Kleinsmith and Bianchi-Berthouze conclude that bodily expressions are an important modality for non-verbal communication in HCI/HRI. Even though the expression and recognition of affective states from gestures and body motion is relatively unexplored in comparison to studies on facial expression and physiology <ref type="bibr" target="#b36">[37]</ref>, these modalities are advantageous 1) for perception from a distance, because bodily expressions are more easily visible from a distance than subtle changes in the face <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b37">[38]</ref>, 2) to analyze types of expressions which are less susceptible to social editing, because people are often less aware of their bodily than their facial expressions <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b38">[39]</ref>, and 3) to convey affective states which are more easily expressed through movement, e.g., intense positive or negative emotions <ref type="bibr" target="#b17">[18]</ref>.</p><p>Affective states can be represented using a set of distinct categorical labels, or a dimensional model. Categorical labels describe affective states based on their linguistic use in daily life. Different sets of categorical labels can be chosen depending on the study. Most frequently, happiness, 2. One can apply an analysis "true or false?" to a proposition. <ref type="bibr">[31, p. 7</ref> referring to <ref type="bibr" target="#b31">[32]</ref>]. sadness, fear, and anger are included, a subset of the basic emotions <ref type="bibr" target="#b39">[40]</ref>. A basic emotion is defined by a set of neural correlates in the brain, a specific set of bodily expressions, and a motivational component for action tendencies <ref type="bibr" target="#b39">[40]</ref>. A popular set of basic emotions contains anger, happiness, sadness, surprise, disgust, and fear <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>.</p><p>An affective state is represented in a dimensional model as a point on a continuum spanned by a set of independent dimensions. A popular example is a circumplex model, where similar affective states are arranged to lie adjacent to each other on a circle, and dissimilar affective states are arranged to lie opposite each other <ref type="bibr" target="#b42">[43]</ref>. A common model applied in affective computing is the PAD-model, with the dimensions pleasure (or valence), arousal, and dominance <ref type="bibr" target="#b43">[44]</ref>. Arousal corresponds to the level of activation, mental alertness, and physical activity. Dominance represents the amount of control over others and the surroundings versus feeling controlled by external circumstances. Categorical labels can be mapped to the continuous PAD space, e.g., happiness, amusement, and contentment are related to high pleasure, whereas anger, fear, and sadness are related to low pleasure <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, as qualitatively illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. 3 A dimensional representation may relate more to the underlying physiological changes <ref type="bibr" target="#b25">[26]</ref> and Barrett suggests that categorical labels are a result of subjective categorization of emotions using conceptual knowledge, e.g., similar to color perception <ref type="bibr" target="#b48">[49]</ref>.</p><p>Most studies on affect recognition to date use categorical labels, while fewer studies have applied a dimensional representation <ref type="bibr" target="#b1">[2]</ref>. Challenges for dimensional affect recognition include unbalanced data sets, differences in the inter-observer agreement on the dimensions, and handling of categories which are not covered by the dimensions or overlap with other categories in a dimensional approach <ref type="bibr" target="#b1">[2]</ref>. Recent works in HCI/HRI have investigated the use of a dimensional representation and further exploration of the utility of dimensional representations in HCI/HRI is advised <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">AFFECT-EXPRESSIVE MOVEMENTS</head><p>Affective states can be expressed through body movements in various ways, e.g., by whole-body gestures, arm gestures, or modulation of functional movements. To refer to this set of movements and given the discussion in Section 2, we introduce the term affect-expressive movements, because expressions and not internal experiences are analyzed and among these only expressions that are related to affect. In this section, we introduce a categorization of movements for summarizing the current state of the art in automatic recognition and generation of affect-expressive movements. Next, a representative review of movement notation systems is presented. We focus on movement notation systems that have been applied to study affective expressiveness and discuss a set of criteria to evaluate the suitability of the notation systems for computational analysis of affectexpressive movements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Categorization of Affect-Expressive Movements</head><p>The current state of the art on automatic generation and recognition of affect-expressive movements analyzes a large variety of body movements. An affective state can be expressed via modulation of a single movement or by selecting from a library of different movements. For instance, anger can be communicated via modulation of a movement, e.g., increasing the walking speed, or through selecting a specific movement type, e.g., making a fist. Studies discussed in this survey analyze affective expressiveness either through modulation of a single movement, use of different movement types, or a combination of both. Considering the definition of a gesture as "a movement of part of the body, especially a hand or the head, to express an idea or meaning" <ref type="bibr" target="#b49">[50]</ref>, gestures generally combine type selection with movement modulation. We consider that postures are snapshots of movements, and not separate entities.</p><p>We introduce the following categorization for the large variety of movements studied to date:</p><p>Communicative movements cover a broad range of movements, which are performed in daily life and may convey affective states, e.g., gestures <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>. A communicative movement can select from a library of movement types to express an affective state and may be accompanied by a modulation level. The objective of functional movements is to perform a task unrelated to the expression of affect, e.g., walking or knocking <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>. In contrast to the former category, affective states can only be expressed through modulation of a functional movement and expressiveness is secondary to function. Artistic movements, such as choreographed <ref type="bibr" target="#b56">[57]</ref> and non-choreographed dancing <ref type="bibr" target="#b57">[58]</ref>, can display exaggerated expressions and can consist of movement types which do not occur during daily life. Artistic movements vary in terms of movement type to express an affective state and may be accompanied by a modulation level. For choreographed artistic movements, different affective states are expressed only by modulation, because the movement type is 3. Categorical terms for each octant in the PAD space in Fig. <ref type="figure" target="#fig_0">1</ref> are selected based on pleasure, arousal, and dominance ratings in <ref type="bibr" target="#b47">[48]</ref>.</p><p>specified by the choreography, e.g., choreographed dancing <ref type="bibr" target="#b56">[57]</ref>. Abstract movements are used neither to explicitly accomplish a task nor to communicate a meaning (e.g., lifting the arms <ref type="bibr" target="#b58">[59]</ref>). Expressivity of a movement can be analyzed independently of a possible symbolic meaning of the movement itself and movement types can be selected which do not necessarily occur during daily life. Affective states can also be conveyed by a lack of movement (e.g., freezing in terror). However, this type of response is rarely addressed in HCI/HRI studies to date, and is therefore omitted from the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Movement Notation Systems</head><p>In computational human movement analysis, movements are commonly represented in terms of joint angle trajectories (e.g., <ref type="bibr" target="#b59">[60]</ref>), or derived discrete or time-series features (e.g., maximum velocity <ref type="bibr" target="#b58">[59]</ref>). Movement analysis based on these kinematic features is usually computationally expensive due to the high-dimensional nature of the movement trajectories and more importantly, does not necessarily capture expressive qualities which are critical for affect-expressive movement analysis. Movement notation systems provide an efficient tool for systematic and compact representation of movements that capture both structural characteristics and expressive qualities of the movements. As FACS 4 pushed forward the research on computational analysis of facial expressions, movement notation systems could also advance the computational analysis of affect-expressive movements by providing an objective and systematic movement representation.</p><p>A good movement notation system achieves the following criteria proposed by Guest <ref type="bibr" target="#b61">[62]</ref>: universality (capability of coding all forms of movement), comprehensiveness (covering every aspect of a movement), movement analysis (anatomically and physiologically sound movement coding), versatility in movement description (truthful representation of intention and expressivity in addition to structure), flexibility in application, logicality (consistent presentation of similar actions using logically related symbols and codes), visuality (readable visual presentation), legibility (distinctive and discrete coding symbols or categories), and practicability (ease of use and integrability with modern technologies). Expanding on the criteria proposed by Guest, in order to limit coders' bias, the notation systems should not need any contextual assumption or perceptual inference for notating an observed movement. A comparison between notation systems can be found in <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>. However, a more recent categorization by Burgoon et al. enables a more systematic comparison between the notation systems and their utilities <ref type="bibr" target="#b13">[14]</ref>. Burgoon et al. divide movement notation systems into functional 5 and structural approaches <ref type="bibr" target="#b13">[14]</ref>. We focus on notation systems that are primarily designed for coding bodily movements and do not discuss notation systems that use bodily movements only as a peripheral indicators of affective expressions (e.g., SPAFF <ref type="bibr" target="#b64">[65]</ref>).</p><p>Functional approaches describe the communicative function of a displayed movement using verbal labels. The Ekman and Friesen formulation of kinesic behaviours into five categories (emblems, illustrators, affective displays, regulators, and manipulators) is an example of a functional notation system <ref type="bibr" target="#b8">[9]</ref>.</p><p>Structural approaches are primarily concerned with the question of what bodily movements look like and provide detailed notation of posture and movement dynamics <ref type="bibr" target="#b13">[14]</ref>.As a result, they provide sufficient structural and expressive details for movement replication, and are more appropriate for computational affect-expressive movement analysis.</p><p>Inspired by linguistic notation systems, Birdwhistell proposed a structural movement notation system that parallels phonemic transcription in linguistics <ref type="bibr" target="#b65">[66]</ref>. Birdwhistell referred to non-verbal communicative body movements as kinesics <ref type="bibr" target="#b65">[66]</ref>, and introduced kine (the smallest perceivable body motion, e.g., raising eye brows), kineme (a group of movements with a same social meaning, e.g., one nod, two nods, three nods), and kinemorphs (a combination of kinemes forming a gesture) followed by kinemorphic classes and complex kinemorphic constructs, which are analogous to sentences and paragraphs in linguistics. Birdwhistell used motion qualifiers and action modifiers that define: 1) the degree of muscular tension involved in executing a movement, 2) duration of the movement, and 3) the range of the movement. The kinegraph is introduced as a tool for notating individual kines and their direction at different body sections. The Birdwhistell system is capable of micro analysis of body movements as its kines capture barely perceivable body motion ranging from 1/50 to 3 seconds in duration <ref type="bibr" target="#b65">[66]</ref>. Birdwhistell emphasizes the importance of context for inferring the meaning of an observed movement. A Birdwhistell-inspired annotation was used to extract semantic areas in emoticons 6 for automatic recognition of their expressed emotions <ref type="bibr" target="#b66">[67]</ref>.</p><p>The Laban system is another prominent example of a structural movement notation system, which was developed for writing and analyzing both the structure and expressivity of movements in dance choreography <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>. The Laban notation system has four major components: Body, Space, Effort, and Shape. Body indicates the active body parts, and the sequence of their involvement in a movement. Space defines where in space a movement is happening, and the directions of the body and body parts. Laban Effort and Shape components provide a comprehensive set of descriptors for a qualitative characterization of a movement <ref type="bibr" target="#b69">[70]</ref>. Effort describes the inner attitude toward the use of energy along four bipolar components: Space, Weight, Time, and Flow, with their extremes being Indirect/Direct, Light/ Strong, Sustained/Sudden, and Free/Bound, respectively. Shape consists of Shape Flow, Directional, and Shaping/ Carving, all of which describe dynamic changes in the 4. Face action coding system (FACS) provides a comprehensive set of action units that can be used to objectively describe any type of facial movement <ref type="bibr" target="#b60">[61]</ref>.</p><p>5. In Section 3.1, functional movements are discussed, which should not be confused with functional movement notation systems discussed here.</p><p>6. An emoticon is a string of symbols used in text communication to express users' emotions <ref type="bibr" target="#b66">[67]</ref>.</p><p>movement form <ref type="bibr" target="#b69">[70]</ref>. Labanotation is the Laban notation tool that makes use of a set of abstract geometric symbols to notate an observed movement. Computational Laban analysis has been carried out for movement recognition (e.g., <ref type="bibr" target="#b70">[71]</ref>), and generation (e.g., <ref type="bibr" target="#b71">[72]</ref>), and to relate Laban components to low-level movement features, e.g., velocity and acceleration <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b74">[75]</ref> and different affective expressions <ref type="bibr" target="#b75">[76]</ref>.</p><p>Delsarte <ref type="bibr" target="#b76">[77]</ref> classifies emotion as a form of expression in gestures and divides the body into zones within which mental, moral, and vital components are defined. He identifies nine laws that contribute to the meaning of a movement: altitude, force, motion (expansion, contraction), sequence, direction, form, velocity, reaction, and extension. The Delsarte system has been used for automatic generation of affect-expressive full-body <ref type="bibr" target="#b77">[78]</ref> and hand and arm <ref type="bibr" target="#b78">[79]</ref> movements. In <ref type="bibr" target="#b78">[79]</ref>, participants' perception of a set of Delsarte-generated hand and arm movements displayed on an animated agent was shown to be consistent with the Delsarte model prediction.</p><p>Recently, Dael et al. proposed BAP (body action and posture), a structural notation system for a systematic description of temporal and spatial characteristics of bodily expression of emotions that, analogous to FACS, introduces 141 behavioural categories for coding action, posture, and function of an observed body movement <ref type="bibr" target="#b24">[25]</ref>. BAP segments body movements into localized units in time and describes them at three levels: anatomical (articulation of different body parts), form (direction and orientation of the movements), and functional (behavioural classes categorized in kinesics emblems, illustrators, manipulators). BAP anatomical and form variables are Boolean (0 for absence and 1 for presence), while functional variables are ordinal (1 for very subtle and 5 for very pronounced). BAP was developed using the GEMEP corpus of emotion portrayals. Since the movements are captured from the knees upwards in GEMEP, the current version of BAP does not code whole body postures and leg movements. BAP also does not code dynamic movement characteristics such as velocity, acceleration, and energy. BAP reliability has been demonstrated by assessing intercoder agreement (two coders) on occurrence, temporal precision, and segmentation of posture and action units <ref type="bibr" target="#b24">[25]</ref>. To the best of our knowledge, there is a single report on the application of BAP for computational analysis of affect-expressive movements, in which BAP behavioural categories are employed for recognition of 12 affective states encoded in 120 movements demonstrated by 10 actors <ref type="bibr" target="#b36">[37]</ref>. Recently, AutoBAP has been proposed for automatic annotation of posture and action units based on BAP anatomical and form (and not functional) coding guidelines <ref type="bibr" target="#b79">[80]</ref>.</p><p>For automated affect-expressive movement recognition and generation, there is a need for consistent and quantitative description of movements, leading to a preference for structural notation systems that provide a fixed number of distinct movement descriptors (legibility criterion). These descriptors should be mappable to measurable movement characteristics, e.g., joint position, velocity, or they should be readily quantifiable. Furthermore, such a mapping (or quantification) should be reversible for generation studies in which low-level movement trajectories are needed to produce a desired affect-expressive movement. The reversible map enables modifying low-level motion trajectories for a given set of specified quantitative descriptors characterizing a desired affective expression <ref type="bibr" target="#b71">[72]</ref>.</p><p>Despite their proven suitability for movement coding, except for BAP, the structural notation systems do not explicitly provide quantitative measures, which is perhaps the main barrier to their application in computational movement analysis. In addition, the extensive attention to microanalysis (e.g., Birdwhistell system <ref type="bibr" target="#b65">[66]</ref>), and the need for special training (e.g., Laban system) hamper their adoption in affective computing. Furthermore, some notation systems require the coder to infer the meaning or function of an observed movement, e.g., Delsarte <ref type="bibr" target="#b76">[77]</ref>. However, the correspondence between movements and affective expressions is not transcultural and transcontextual and there may be idiosyncratic, gender-specific, or age-specific differences in affect-expressive movements <ref type="bibr" target="#b80">[81]</ref>. Such movement/affective expression discrepancies result in a drawback for the notation systems that code the meaning or function of an observed movement. In addition, the amount and intensity of an affect-expressive movement is important for computational analysis; hence, the preference for structural notation systems that code such information. A table summarizing the characteristics of the prominent structural notation systems is provided in the supplementary material, which can be found on the Computer Society Digital Library at http:// doi.ieeecomputersociety.org/10.1109/T-AFFC.2013.29.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">HUMAN PERCEPTION OF AFFECT-EXPRESSIVE MOVEMENTS</head><p>Various psychological studies indicate that humans are not only capable of recognizing the intended action <ref type="bibr" target="#b81">[82]</ref>, but also gender <ref type="bibr" target="#b82">[83]</ref>, identity <ref type="bibr" target="#b83">[84]</ref>, and affective state <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b84">[85]</ref> from body movements. In general, the perception of affective state is multimodal, e.g., sound and body movements both influence the affective interpretation of music <ref type="bibr" target="#b85">[86]</ref>. Many researchers have tested the human perception of affective state through dance and body movements, using a variety of stimuli including both full-light (FL) and pointlight (PL) videos <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b88">[89]</ref>, and reported above chance recognition rates. Research on the capacity of humans to recognize affective states helps to develop a better understanding of human perception of affect-expressive movements, and can offer insight into whether movements of full-body or isolated body parts are capable of communicating affective expressions. Furthermore, perceptual studies can inform computational analysis by identifying the movement features most salient to affective perception in human observers.</p><p>In this section, an overview of the affect-expressive movement perception literature is presented to motivate and support the computational analysis of affect-expressive movements covered in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Full Body Movements and Gait</head><p>Boone and Cunningham <ref type="bibr" target="#b86">[87]</ref> report the following six movement cues used by participants for the perception of affect-expressive dance movements: changes in tempo (anger), directional changes in face and torso (anger), frequency of arms up (happiness), duration of arms away from torso (happiness), muscle tension (fear), the duration of time leaning forward (sadness). Camurri et al. suggest that the duration of the movement, quantity of the movement (the amount of observed movement relative to the velocity and movement energy represented), and contraction index (measured as the amount of body contraction/ expansion) play key roles in the perception of affect from dance movement <ref type="bibr" target="#b87">[88]</ref>.</p><p>Studies from psychology indicate that affective states can be expressed during walking and recognized above chance level by human observers (e.g., videos <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b90">[91]</ref>, animation <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b91">[92]</ref>). These studies show that affective states modulate gait parameters and kinematics such as walking speed, shoulder and elbow range of motion, and head orientation <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b90">[91]</ref>, <ref type="bibr" target="#b91">[92]</ref>. For instance, sadness and depression are characterized by reduced walking speed, arm swing, and vertical head movements as well as slumped postures and larger lateral body sway <ref type="bibr" target="#b29">[30]</ref>. In gait-based analysis using the PAD model, human observers recognize differences in arousal better than differences in pleasure <ref type="bibr" target="#b53">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Upper Body Movements</head><p>There are fewer works on affect perception from individual body parts in isolation from the rest of the body. There are situations where only individual body parts are observable due to occlusion of the rest of the body (e.g., the head and hands movements in a video call). Furthermore, there is interest to display affect-expressive movements on embodiments that, due to kinematic constraints, are incapable of full-body movements and rather display only isolated limb movements (e.g., <ref type="bibr" target="#b92">[93]</ref>, <ref type="bibr" target="#b93">[94]</ref>). Therefore, it is important to explore expression and perception of affect through individual body parts.</p><p>Ekman and Friesen suggest that head orientation is an indicator of gross affective state (i.e., positive versus negative) as well as intensity of emotion <ref type="bibr" target="#b9">[10]</ref>. Busso et al. conducted a user study using the PAD model to evaluate the perception of affect from head motion during affective speech <ref type="bibr" target="#b94">[95]</ref>. They report that head motion corresponding to different affective states is characterized by distinct motion activation, range, and velocity.</p><p>There is evidence that hand and arm movements are most significant for distinguishing between affective states <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b95">[96]</ref>. Different affective states conveyed with hand and arm movements are recognized above chance level (FL videos of hand and arm movements <ref type="bibr" target="#b96">[97]</ref>, <ref type="bibr" target="#b97">[98]</ref>, <ref type="bibr" target="#b98">[99]</ref>, <ref type="bibr" target="#b99">[100]</ref>, PL animation of affective drinking and knocking movements <ref type="bibr" target="#b27">[28]</ref>, animated anthropomorphic and non-anthropomorphic hand models displaying abstract movements <ref type="bibr" target="#b100">[101]</ref>, <ref type="bibr" target="#b101">[102]</ref>). Velocity, acceleration, and finger motion range are frequently reported as important hand and arm movement features for distinguishing different affective states <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b99">[100]</ref>. For instance, in <ref type="bibr" target="#b99">[100]</ref>, happy movements were characterized by indirect arm trajectories, angry movements were forceful and fast, whereas sad movements were slow and weak. Perceived arousal was found to be correlated with velocity, acceleration, and jerk (rate of change of acceleration) of the arm movement <ref type="bibr" target="#b27">[28]</ref>. Affective state is also recognized above chance level during sign language communication, even for observers who do not understand the language being signed <ref type="bibr" target="#b97">[98]</ref>, <ref type="bibr" target="#b98">[99]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Gender Differences in Perception</head><p>The effect of gender on the perception of bodily expression of affect is largely unexplored. Differences in the perception of affect-expressive movements may exist due to the gender of the demonstrator and/or observer. In general, reports on gender differences in the perception of affective state mainly focus on facial expressions. There are various and sometimes contradicting findings on the abilities of men and women in decoding facial expressions <ref type="bibr" target="#b102">[103]</ref>. Women perceive affective states through facial expressions more accurately than men <ref type="bibr" target="#b103">[104]</ref>, <ref type="bibr" target="#b104">[105]</ref>, <ref type="bibr" target="#b105">[106]</ref>. Men are found to be superior in recognizing angry facial expressions, e.g., <ref type="bibr" target="#b106">[107]</ref>, <ref type="bibr" target="#b107">[108]</ref>, <ref type="bibr" target="#b108">[109]</ref>, whereas women are found to be better at perceiving happy facial expressions, e.g., <ref type="bibr" target="#b109">[110]</ref>, <ref type="bibr" target="#b110">[111]</ref> and sad facial expressions, e.g., <ref type="bibr" target="#b108">[109]</ref>. However, in a recent study on decoding affect-expressive movements, male observers outperformed female observers in recognizing happy movements, whereas the female observers were better at recognizing angry and neutral knocking movements <ref type="bibr" target="#b111">[112]</ref>. In another study, no significant gender differences in the perception of affective hand and arm movements were observed <ref type="bibr" target="#b96">[97]</ref>.</p><p>Other studies investigate the role of the demonstrator's gender in the perception of affect-expressive movements. In a user study, participants tended to apply social stereotypes to infer the gender of a point-light display throwing a ball with different emotions. Angry movements were judged to be demonstrated by men and sad movements were more likely to be attributed to women <ref type="bibr" target="#b112">[113]</ref>. Similarly, the perception of fearful gait is facilitated if the walker is female <ref type="bibr" target="#b113">[114]</ref>, due to kinematic similarities between fearful gait and natural female gait.</p><p>These sometimes conflicting findings illustrate the important role that gender might play in the perception and demonstration of affect-expressive movements, and emphasize the importance of considering gender in studying affect-expressive movements. For computational affectexpressive movement analysis, to remove (or control) the potential role of gender, databases should contain a balanced number of male and female demonstrators, and the reliability of the databases should be evaluated with both male and female observers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Embodiment</head><p>The embodiment of an artificial agent expressing an affective state can be physical (robotic agent) or virtual (animated agent). Physical and virtual embodiments can be further subdivided into anthropomorphic (human-like kinematics and appearance, e.g., physical anthropomorphic <ref type="bibr" target="#b114">[115]</ref>, virtual anthropomorphic <ref type="bibr" target="#b115">[116]</ref>) and non-anthropomorphic (non-human-like kinematics and appearance, e.g., physical non-anthropomorphic <ref type="bibr" target="#b116">[117]</ref>, virtual non-anthropomorphic <ref type="bibr" target="#b101">[102]</ref>).</p><p>It is well known that humans can perceive affective states from non-anthropomorphic demonstrators. For example, humans can perceive life-like affective states from the movement of abstract geometrical shapes <ref type="bibr" target="#b117">[118]</ref>. However, there are conflicting reports on the role of embodiment in perception. In some studies, the perception of affect-expressive movements was not influenced by non-anthropomorphic appearance <ref type="bibr" target="#b100">[101]</ref> and kinematics <ref type="bibr" target="#b116">[117]</ref> of demonstrators, whereas other studies have shown that non-anthropomorphic kinematics and appearance of display embodiment may influence the affect-expressive movement perception <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b118">[119]</ref>, <ref type="bibr" target="#b119">[120]</ref>. For instance, in a recent study <ref type="bibr" target="#b101">[102]</ref>, the non-anthropomorphic appearance was found to significantly affect participants' ratings of affect-expressive movements demonstrated on animated human-like and frondlike embodiments.</p><p>Movement features salient to the perception of affect from animated geometric agents include absolute velocity, relative angle, relative velocity, relative heading, relative vorticity, absolute vorticity and relative distance <ref type="bibr" target="#b120">[121]</ref>. The arousal component of perceived affect-expressive movements displayed on embodiments with non-anthropomorphic kinematics <ref type="bibr" target="#b116">[117]</ref>, <ref type="bibr" target="#b121">[122]</ref> and appearance <ref type="bibr" target="#b101">[102]</ref> is found to be correlated with the velocity and acceleration of the movements. In <ref type="bibr" target="#b121">[122]</ref>, the valence component of perceived affect-expressive movements, displayed by an interactive device with non-anthropomorphic kinematics, is related to the smoothness of the movements.</p><p>In a recent study, the impact of the embodiment on the participants' perception of affect-expressive movements varied between male and female participants <ref type="bibr" target="#b122">[123]</ref>. For instance, male participants correctly recognized arousal and valence characteristics of angry movements regardless of the embodiment, whereas female participants associated less arousal and less-negative valence to the non-anthropomorphic embodiment displaying the angry movements.</p><p>There are also conflicting reports on differences between perceiving affective states from physical embodiments (e.g., robot) and virtual embodiments depicted in video or animation. Some studies report no significant differences <ref type="bibr" target="#b123">[124]</ref>, <ref type="bibr" target="#b124">[125]</ref>, whereas significant differences between the perception of affect-expressive movements from videos of an actor and his animated replicates are reported in <ref type="bibr" target="#b115">[116]</ref>. Therefore, the embodiment of affective displays may influence their interaction with human users and merits further exploration to identify the role of display embodiment in the perception of demonstrated social, behavioural, and affective cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">AUTOMATIC RECOGNITION OF AFFECT-EXPRESSIVE MOVEMENTS</head><p>Although affective phenomena have been studied for over a century and a half by psychologists <ref type="bibr" target="#b125">[126]</ref>, <ref type="bibr" target="#b126">[127]</ref>, it was only in the 1990s that research into systems capable of automatic recognition of affective states attracted the interest of engineers and computer scientists <ref type="bibr" target="#b127">[128]</ref>. Early research was conducted on facial expressions and identified as challenges, among others, the subconscious nature of the perception of affective expressions as well as contextual, interpersonal, and intercultural differences in expressing and perceiving affect <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Automatic recognition of facial expressions has been investigated for both person-dependent and person-independent recognition 7 of acted facial expressions of basic emotions. Most recent works move towards the recognition of spontaneous expressions, representing affective states as dimensions rather than distinct categories, and multi-modal recognition based on facial expressions, linguistic and non-linguistic components of speech, and body movements <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. While recognition from body movements is not as mature as facial expression recognition, most of the challenges described above for automatic recognition of facial expressions have been investigated in individual case studies for body movements.</p><p>In the following, we summarize the type of sensors recording the movements, the consideration of acted, elicited, or natural expressions, and the components of an automatic recognition system for affect-expressive movements. Then, we provide an overview on the current achievements on analyzing upper-body and whole-body movements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sensors for Recording Affect-Expressive Movements</head><p>The following approaches have been applied to record expressive motions:</p><p>Computer vision <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b128">[129]</ref>, <ref type="bibr" target="#b129">[130]</ref>, <ref type="bibr" target="#b130">[131]</ref>, <ref type="bibr" target="#b131">[132]</ref>, <ref type="bibr" target="#b132">[133]</ref>, <ref type="bibr" target="#b133">[134]</ref>.</p><p>Motion capture <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b134">[135]</ref>, <ref type="bibr" target="#b135">[136]</ref>, <ref type="bibr" target="#b136">[137]</ref>, <ref type="bibr" target="#b137">[138]</ref>.</p><p>Pressure sensors <ref type="bibr" target="#b138">[139]</ref>, <ref type="bibr" target="#b139">[140]</ref>. Optical motion capture data provides high resolution data which facilitates recognition. However, optical motion tracking is limited to methodological studies, while wireless inertial motion tracking <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b137">[138]</ref>, data gloves, bend sensors, pressure sensors, or computer vision algorithms are used to record movements in a naturalistic environment. Most studies that approach real-world scenarios are based on computer vision and track regions of the hands, the head, the shoulders, the neck, the upper body, or the complete body <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b128">[129]</ref>, <ref type="bibr" target="#b140">[141]</ref>. A framework for extracting a minimum representation of affective states from the position and dynamics of the head and hands from video data is provided in <ref type="bibr" target="#b131">[132]</ref>. To circumvent the difficulties that accompany computer vision algorithms (e.g., different lighting, background conditions, viewing angle, and occlusions), other approaches suitable for HCI include inertial motion tracking <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b137">[138]</ref> or, for seated postures, instrumenting the seat and the back of the chair with pressure sensors <ref type="bibr" target="#b138">[139]</ref>, <ref type="bibr" target="#b139">[140]</ref>. These sensors are especially suitable to insure privacy by avoiding the use of video data from which the user's identity might be easily retrieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Acted, Elicited, or Natural Expressions</head><p>Automatic recognition of affective states from facial expression is moving towards the use of databases that consist of natural expressions <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. A similar trend is observed for affective body movements. Early studies are based on acted or elicited affective expressions. Recent work has considered the recording of natural expressions during HCI <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b138">[139]</ref>, <ref type="bibr" target="#b141">[142]</ref>. Still, the number of such studies is small in comparison to works on facial expression.</p><p>Most studies rely on data from acted or elicited affective expressions. Dancers demonstrate the movements in <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b99">[100]</ref> and non-professional actors in <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b130">[131]</ref>. Affective expressions are either elicited by a human-computer conversation <ref type="bibr" target="#b128">[129]</ref>, or by imagining a scenario. The scenario is either a short story <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b142">[143]</ref> or one of a person's own memories <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>. A small number of studies investigate natural expressions during HCI. The scenarios are playing a computer game <ref type="bibr" target="#b52">[53]</ref>, playing a video game <ref type="bibr" target="#b141">[142]</ref>, and using an automatic tutoring system <ref type="bibr" target="#b138">[139]</ref>. Kleinsmith et al. discuss advantages and disadvantages of recording acted and elicited expressions and conclude that works on acted and elicited expressions provide useful insights, but that more studies are necessary which consider natural expressions for real-world HCI/HRI applications <ref type="bibr" target="#b18">[19]</ref>.</p><p>Furthermore, the data sets differ regarding annotation. Ground truth is considered to be the affective expression which actors are asked to act, the intended elicited expression, e.g., <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b55">[56]</ref>, or the affective state which is most frequently labeled by observers, e.g., <ref type="bibr" target="#b141">[142]</ref>, <ref type="bibr" target="#b143">[144]</ref>. This aspect is discussed extensively in <ref type="bibr" target="#b18">[19]</ref>, and Kleinsmith et al. introduce a novel approach for reliably estimating the affective expression for a small number of annotators <ref type="bibr" target="#b141">[142]</ref>. When moving towards natural expressions, a reliable estimate for the ground truth is necessary and recognition algorithms need to be developed which can handle label uncertainty and multiple labels for an expression.</p><p>A table listing the available affect-expressive movement databases is provided in the supplementary material, available online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Components of an Automatic Recognition</head><p>System for Affect-Expressive Movements</p><p>Developing an automatic recognition system for affectexpressive movements consists of the following four steps (e.g., <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b128">[129]</ref>, <ref type="bibr" target="#b131">[132]</ref>):</p><p>1. Estimation of motion trajectories from sensor data, 2. temporal segmentation (based on time windows or movement primitives), 3. construction of a feature set and dimensionality reduction (when necessary), 4. detection of affective expression. Steps 1 and 2 address human movement analysis in general.</p><p>Step 1 reconstructs a description of the movements, e.g., joint angle trajectories, from sensor data. For this purpose, various sensors can be used, see Section 5.1; the motion reconstruction approach used depends on the sensor data. Step 2 partitions the continuous time series data into short-duration segments for subsequent analysis. Many studies investigating affect-expressive movements use presegmented data, which are either manually segmented or only individual movement segments are recorded. Only a small number of studies include automatic segmentation, e.g., based on the quality of movement <ref type="bibr" target="#b56">[57]</ref> or motion energy <ref type="bibr" target="#b55">[56]</ref>. For training and evaluation of the classifier, these segments need to be labeled.</p><p>A large variety of features can be constructed from movement data. Most approaches transform the time series data describing the motions to a set of time-independent variables using descriptive measures. In <ref type="bibr" target="#b59">[60]</ref>, functional dimensionality reduction is used to extract relevant features directly from time-series data. In general, the following three approaches are commonly used for constructing the feature space: 1) A set of hand-selected features is created describing human movement and when necessary this set is reduced to relevant features by dimensionality reduction, e.g., <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b128">[129]</ref>. This approach is not grounded in psychological theories about affective expressions and is particularly suitable when the sensor data cannot easily be related to a kinematic or shape-based model of human motion, e.g., for pressure sensors integrated in a seat <ref type="bibr" target="#b138">[139]</ref>. 2) In the second approach, features are selected based on findings from perceptual studies in psychology <ref type="bibr" target="#b53">[54]</ref>. 3) The third approach utilizes a notation system to create a set of high-level descriptors as features <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b137">[138]</ref>, <ref type="bibr" target="#b140">[141]</ref>. These three approaches are not mutually exclusive and can be combined to find a comprehensive feature set, e.g., in <ref type="bibr" target="#b56">[57]</ref>. Across the three approaches, movement speed is commonly selected as a feature in most studies.</p><p>Step 4 maps low-level motion features or high-level descriptors to affective states. Classifiers such as Support Vector Machine, Naive Bayes, Nearest Neighbor, or Multi-Layer Perceptron are used for categorical labels. When a dimensional representation is used, each dimension is either categorized into low, neutral, and high and a classifier is trained, or regression techniques are applied, e.g., support vector regression. Each technique can be applied for either person-independent or person-dependent recognition. Reported recognition rates from several studies are summarized in the supplementary material, available online.</p><p>To date, only a small number of studies have made use of movement notation systems <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b130">[131]</ref>, <ref type="bibr" target="#b137">[138]</ref>, <ref type="bibr" target="#b140">[141]</ref>. The lack of use of the notation systems can be explained by 1) a lack of quantification of the notation systems, 2) a lack of awareness of the notation systems, which are most commonly used in the dance community and psychology, in engineering and computer science, and 3) a lack of systematic and quantitative mapping between the components of the notation system and affective states. The majority of the approaches map kinematic motion features directly to affective states or dimensions. This approach provides reliable recognition rates when only movement data is available, comparable recognition rates to facial expressions or speech in multi-modal scenarios, and improves overall accuracy in multi-modal systems when combined with other modalities <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b128">[129]</ref>, <ref type="bibr" target="#b140">[141]</ref>, <ref type="bibr" target="#b144">[145]</ref>.</p><p>An additional challenge when developing systems that automatically analyze affective expressions is that expressions differ between individuals in both intensity and quality. Factors contributing to these differences are, among others, differences in early child development, personality, life history of a person, gender, and culture <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b145">[146]</ref>, <ref type="bibr" target="#b146">[147]</ref>. Inter-individual differences in the expressiveness and movement performance lead to noticeable decreases in accuracy for person-independent recognition <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b58">[59]</ref>. This difference is particularly large for functional movements where expressiveness is secondary to function <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b55">[56]</ref>. For a group of known users during HCI/HRI, the recognition accuracy can be improved when the algorithm is adapted and trained for each user. An approach to circumvent person-specific training would be to cluster affectexpressive movements in sets of expression styles. Persons who express affective states in a similar style are grouped into a cluster and a form of style-specific recognition is developed which is between person-dependent and personindependent recognition. Kleinsmith and Bianchi-Berthouze discuss the dependence of affective expressions on the culture <ref type="bibr" target="#b18">[19]</ref>. As nonverbal behavior differs between cultures <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, this aspect needs to be considered when recording databases, annotating databases, developing algorithms, and transferring a system to a different cultural area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Upper Body Movements</head><p>Abstract and functional movements have been investigated in studies using optical motion capture. Pre-defined abstract movements are studied to analyze recognition of affective states based only on modulation of the specified movement, e.g., arm-lowering motions <ref type="bibr" target="#b99">[100]</ref>, both raising and lowering the arm <ref type="bibr" target="#b58">[59]</ref>, and closing and opening the hand <ref type="bibr" target="#b101">[102]</ref>. The results of these studies <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b101">[102]</ref> support the hypothesis that information about affective states can be retrieved from the way a motion is performed and recognition is not only limited to the detection of expressive gestures. This result is relevant for functional movements, where expressiveness is only possible through trajectory modulation, e.g., knocking <ref type="bibr" target="#b55">[56]</ref>.</p><p>Studies that approach naturalistic scenarios are mostly based on computer vision. Estimation of head pose and gesture types using computer vision is covered in the surveys <ref type="bibr" target="#b147">[148]</ref> and <ref type="bibr" target="#b148">[149]</ref>. To generalize to a wide range of movements, classifiers for recognition of affective states are trained on a variety of communicative movements, which include gestures, head motions, torso movements, or combinations of them. These movements can vary in type selection and style. Considering a set of four affective states, Glowinski et al. suggest that the discriminative features may be features describing movement style rather than specific gesture types <ref type="bibr" target="#b131">[132]</ref>. This facilitates recognition, because learning a complete set of possible gesture types that might occur during an interaction can be avoided. It has not yet been investigated whether this finding holds also for discriminating between a larger set of affective states.</p><p>Recognition of affective states from observing only head movements is investigated in <ref type="bibr" target="#b149">[150]</ref>, <ref type="bibr" target="#b150">[151]</ref>, <ref type="bibr" target="#b151">[152]</ref>, <ref type="bibr" target="#b152">[153]</ref>. Using only a small set of possible movement types (e.g., nodding, shaking, tilting, no movement, leaning forward and backward) and the way they are performed (modulation), results in person-dependent recognition rates above chance level. Person-independent recognition rates above chance level are reported when combining head movements with arm gestures or observing only arm gestures and torso movements, e.g., <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b52">[53]</ref>. By observing only torso movement during HCI, e.g., by instrumenting the seat and the back of the chair in front of the computer with pressure sensors, it is possible to classify the two-class case whether a specific affective state is expressed or not, but the recognition rate for distinguishing between several affective states is low <ref type="bibr" target="#b138">[139]</ref>. Hence, the most promising approach to achieve robust person-independent recognition is to create a feature set that captures head movements, torso motion, and arm gestures.</p><p>Several studies investigate multi-modal systems which combine the recognition of affective states from body movements, facial expressions, and speech <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b128">[129]</ref>, <ref type="bibr" target="#b130">[131]</ref>, <ref type="bibr" target="#b140">[141]</ref>, <ref type="bibr" target="#b152">[153]</ref>. For example, <ref type="bibr" target="#b37">[38]</ref> uses both body movements and facial expression to recognize affect, and finds that recognition accuracy is improved over either modality alone. The results of these studies indicate that gestures and upper body movements are suitable modalities for multi-modal systems to recognize affective states and improve recognition accuracy of the overall system. Furthermore, it might be easier for a vision-based system to model and recognize affective states from a combination of gestures and head motions than from atomic movements in the face <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Gait, Dance and Whole-Body Gestures</head><p>Many of the studies on recognition of affective states from whole-body movements use optical motion capture providing high accuracy of the reconstructed joint angles <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b134">[135]</ref>, <ref type="bibr" target="#b135">[136]</ref>, <ref type="bibr" target="#b136">[137]</ref>. Naturalistic scenarios are approached in <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b130">[131]</ref>, <ref type="bibr" target="#b141">[142]</ref> using computervision or body-worn sensors for motion recording. The state of the art can be subdivided into works considering functional movements such as walking, artistic movements such as choreographed and non-choreographed dancing, and those analyzing communicative movements such as whole-body gestures.</p><p>For walking, the primary task is locomotion; affective expression through variations in the walking style is secondary. Gait databases are characterized by highly dimensional, temporally dependent, and highly variable data vectors. Efficient dimensionality reduction is essential, achieved, e.g., by an adapted blind source separation algorithm which considers joint-specific time delays <ref type="bibr" target="#b136">[137]</ref>. Recognition rates for person-dependent training are higher than person-independent training <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b135">[136]</ref>. Therefore, a two-stage approach is proposed in <ref type="bibr" target="#b53">[54]</ref>, which consists of first identifying the person and then applying person-dependent recognition of affective expressions. Conversely, identifying a person by his/her walking style is affected by expressiveness; accuracy for identification decreases by about 10 percent when the affective state differs between the training and test set <ref type="bibr" target="#b53">[54]</ref>.</p><p>The recognition of emotions from dance movements is studied in <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>. Basic emotions are recognized in both studies without the need to relate a set of specific expressive gestures to each emotion. This is necessary to allow the dancers to perform any motion and not restrict the dancers to a set of motions.</p><p>Gestures communicate information about affective states and/or emphasize words during verbal interaction (i.e., kinesic regulators). This makes them especially suitable for recognizing affective states. A set of acted, expressive wholebody gestures are considered in <ref type="bibr" target="#b50">[51]</ref> and <ref type="bibr" target="#b143">[144]</ref>. Even though it can be expected that the expression of affective states by gestures has common characteristics among subjects, inter-individual variations are sufficiently large to influence recognition accuracy <ref type="bibr" target="#b50">[51]</ref>. Kessous et al. <ref type="bibr" target="#b130">[131]</ref> use the Eye-sWeb software <ref type="bibr" target="#b153">[154]</ref> to extract features from whole-body gestures and accuracy is larger than for features extracted from facial expressions or speech in their study. The highest recognition rate is achieved by feature-level fusion of the three modalities (gesture, facial expression, and speech) <ref type="bibr" target="#b130">[131]</ref>. Dimensionality reduction of high-dimensional wholebody gestures is directly applied to the time series data using both supervised and unsupervised techniques in <ref type="bibr" target="#b59">[60]</ref>.</p><p>Work on static postures considers snapshots of movements as the input signal and excludes dynamic information. The discriminability of affective postures is studied in, e.g., <ref type="bibr" target="#b141">[142]</ref>, <ref type="bibr" target="#b143">[144]</ref>. Feature extraction and classification do not process temporal information. These studies show that, in addition to the dynamic characteristics of movement, postural configurations contain information about the affective state. In most studies, postural cues are not explicitly separated from dynamic cues. Hence, recognition of affective expressions from body movements is usually based on a feature set combining postural and dynamic movement information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">GENERATION OF AFFECT-EXPRESSIVE MOVEMENTS</head><p>This section reviews the research on the generation of affect-expressive movements for virtual agents for HCI and robots for HRI. It is believed that the character of the virtual agent or robot becomes more believable if emotional body language stylizes the movements <ref type="bibr" target="#b115">[116]</ref>. Both facial and bodily expressions have been investigated for human-like, zoomorphic, or cartoon-like virtual and physical embodiments <ref type="bibr" target="#b151">[152]</ref>, <ref type="bibr" target="#b154">[155]</ref>, <ref type="bibr" target="#b155">[156]</ref>, <ref type="bibr" target="#b156">[157]</ref>, <ref type="bibr" target="#b157">[158]</ref>. Affectexpressive movements are particularly suitable for simplified humanoids without sophisticated hardware for displaying facial expressions. For virtual agents, bodily and facial expressions have been combined in a few studies, providing an enhancement of the character's expressiveness <ref type="bibr" target="#b154">[155]</ref>. The generation of affective expressions for robotic heads and virtual faces can utilize the FACS <ref type="bibr" target="#b154">[155]</ref>, <ref type="bibr" target="#b155">[156]</ref>, <ref type="bibr" target="#b156">[157]</ref>, <ref type="bibr" target="#b158">[159]</ref>. Such a standardization facilitates the development of automatic generation of affect-expressive movements as it provides engineers and computer scientists with a common psychologically-accepted framework and enables a systematic comparison between results of different studies. Unfortunately, such a coding system has not been developed and evaluated for the generation of expressive body movements with respect to generalization across a variety of different movements and embodiments. Automatic generation of affect-expressive movements includes the following steps:</p><p>1. Selection of desired affective state. 2. Movement type selection. 3. Movement modulation. 4. Trajectory generation (when necessary). 5. Robotics: Motor commands. The movement type is either selected based on the desired affective state (step 1) or to accomplish a functional task. Modulation of the movement type (step 3) adds affective expressiveness to a functional or abstract movement and enhances expressiveness for communicative or artistic movements. When step 2 and 3 provide key poses, step 4 generates smooth trajectories for each joint, which link the joint angle configurations from the start, the intermediate, to the end frames. These trajectories are used directly to control the joints of a virtual agent. An additional step 5 is required for real robots to translate these trajectories into motor commands. The desired movement type and its modulation (step 2 and 3) can be either designed by artists, dancers, photographers, and other experts, e.g., <ref type="bibr" target="#b159">[160]</ref>, <ref type="bibr" target="#b160">[161]</ref>,</p><p>or automatically generated by a computational model. The latter approach is preferable in affective computing because it facilitates replication and generalization for different situations and different embodiments. This section focuses on summarizing the approaches which provide explicit computational models for relating desired affective states to movement features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">State of the Art</head><p>The animation industry has extensive experience developing believable character movement. Animators at Walt Disney studios have proposed a set of 12 design principles to create believable characters <ref type="bibr" target="#b163">[164]</ref>, <ref type="bibr" target="#b164">[165]</ref>, <ref type="bibr" target="#b165">[166]</ref>. Out of these 12 design principles, four are associated with the expression of affective states and can be relevant for enhancing the believability of a virtual agent or robot <ref type="bibr" target="#b163">[164]</ref>, <ref type="bibr" target="#b164">[165]</ref>, <ref type="bibr" target="#b165">[166]</ref>, <ref type="bibr" target="#b166">[167]</ref>, <ref type="bibr" target="#b167">[168]</ref>. Experienced animators recommend that only one clearly defined affective state is expressed at a time. Expressing an affective state can be a secondary action, when it is started before or after the primary action, e.g., displaying happiness while drinking. These two principles consider that the attention of the observer may be drawn to only one action at a time and a simultaneous secondary action may be easily missed by the observer. Third, appropriate timing, such as fast or slow movement, influences the expression. And fourth, exaggeration can be utilized to enhance the visibility of an expression. The former two principles influence the selection of the affect-expressive movement; timing and exaggeration influence the generation of the specific motion path for the selected expression. In the following, different approaches are summarized for a computational model which generates affect-expressive movements. Models for generation have been applied to a wide variety of embodiments, see Fig. <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Virtual Agents</head><p>Studying bodily expressiveness with animations has the advantage that the full range of possible joint angles can be investigated, whereas hardware limitations can decrease the range of possible movements for physical embodiments such as robots. Using animations also facilitates the evaluation of a computational model. A model can be easily tested regarding its generalizability to different embodiments and to different movements. Perceptual user studies are commonly used to evaluate the computational model <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b168">[169]</ref>, <ref type="bibr" target="#b169">[170]</ref>. An alternative is to compare the generated motion trajectories to recorded affect-expressive movement Fig. <ref type="figure">2</ref>. Generation of affect-expressive movements is studied for a wide variety of embodiments, e.g., for an artificial hand or a frond-like structure <ref type="bibr" target="#b101">[102]</ref>, a real or animated hexapod <ref type="bibr" target="#b161">[162]</ref>, and a humanoid <ref type="bibr" target="#b162">[163]</ref>.</p><p>trajectories <ref type="bibr" target="#b170">[171]</ref>. The computational model relies on a relationship between movement features and affective states. This relationship can be designed manually from motion capture templates <ref type="bibr" target="#b170">[171]</ref>, <ref type="bibr" target="#b171">[172]</ref>, or extracted by machine learning <ref type="bibr" target="#b101">[102]</ref>.</p><p>For example, the motion of an animated figure can be described by a Fourier series for each joint angle over time. <ref type="bibr">Unuma et al.</ref> propose a model which continuously transitions between two behaviors, e.g., neutral and sadness <ref type="bibr" target="#b171">[172]</ref>. The transition is controlled by linear interpolation between the models of the two behaviors. A template for each affective state and each movement needs to be recorded with optical motion tracking to obtain the Fourier series. Amaya et al. also rely on optical motion capture for affect-expressive movements <ref type="bibr" target="#b170">[171]</ref>. They derive an abstract description for expressiveness based on the speed and spatial amplitude of the movement. Other approaches that rely on recording sample movements with motion capture and interpolating between these templates describe a movement using radial basis functions <ref type="bibr" target="#b172">[173]</ref>, a Hidden Markov Model <ref type="bibr" target="#b173">[174]</ref>, or nonlinear inverse optimization <ref type="bibr" target="#b174">[175]</ref>. Chi et al. extend the number of movement descriptors by including the Effort and Shape components of Laban <ref type="bibr" target="#b71">[72]</ref>. The Effort and Shape components can be varied to achieve expressiveness for virtual characters. The intention of <ref type="bibr" target="#b71">[72]</ref> is to provide animators with a tool to design affective expressions by high-level descriptors. However, a computational model between the descriptors and affective states is not integrated. A behavior markup language is proposed in <ref type="bibr" target="#b175">[176]</ref>, <ref type="bibr" target="#b176">[177]</ref>, which formulates an XML format for behavior generation for virtual agents. Each behavior is encoded by the developer and can have attributes such as duration and amount.</p><p>Deriving high-level descriptors for affective expressions facilitates the transfer to other movements, but requires that two relations are defined: 1) the relationship between the movement features and the descriptors and 2) the relationship between the high-level descriptors and affective states. The studies <ref type="bibr" target="#b168">[169]</ref>, <ref type="bibr" target="#b169">[170]</ref> circumvent the need for the second relationship by mirroring high-level descriptors of human affective expressions during HCI. They observe the human's gestures with video analysis and use contraction index, velocity, acceleration, and fluidity as descriptors for human movements. These descriptors are mapped to spatial extension, temporal extension, power, and fluidity of the virtual agent. This approach is evaluated in a perceptual user study for a repetitive hand motion (called beating in the original text) <ref type="bibr" target="#b168">[169]</ref>, <ref type="bibr" target="#b177">[178]</ref>, and choosing different gestures depending on the expressed affective state in <ref type="bibr" target="#b169">[170]</ref>.</p><p>Machine learning can be utilized to automatically derive a set of descriptors for affect-expressive movements. One possible approach is proposed in <ref type="bibr" target="#b101">[102]</ref>, where descriptors represent the main modes of variation in a collection of affect-expressive movements, obtained using functional PCA. These descriptors were then used to generate affectexpressive movements, which were perceived as intended by observers in a user study <ref type="bibr" target="#b101">[102]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Robotics</head><p>The concept of proximity zones in HRI follows the theory of proxemics for human-human interaction in psychology <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b35">[36]</ref>. The distance between human and robot is divided into intimate (0-0:46 m), personal (0:46-1:22 m), and social (1:22-3:66 m) proximity zones. Body movements are especially suitable for conveying affective states during HRI in the personal and social proximity zone <ref type="bibr" target="#b7">[8]</ref>. Several approaches have been undertaken to automatically generate expressive motions for robots. They can be subdivided into studies which utilize Laban to describe affect-expressive movements and studies which model kinematic parameters directly.</p><p>A relationship between motion parameters and affective state is directly derived in <ref type="bibr" target="#b161">[162]</ref>, <ref type="bibr" target="#b162">[163]</ref>, <ref type="bibr" target="#b178">[179]</ref>. Beck et al. obtain a key pose for each affective state, arrange these in the valence and arousal space, and generate new expressions by blending between the joint angles of the key poses <ref type="bibr" target="#b162">[163]</ref>. In <ref type="bibr" target="#b161">[162]</ref>, the gait of a hexapod robot is adapted to display affective states by variations of the gait parameters step length, height, and time. Values for each parameter corresponding to low or high pleasure, arousal, and dominance are retrieved from a human gait database and are mapped automatically to the kinematics of the hexapod. To find a general approach which modifies an arbitrary motion so that affective nuances are expressed, Nakagawa et al. propose to divide an arbitrary motion into the velocity and expansiveness of the motion and a basic posture <ref type="bibr" target="#b178">[179]</ref>. Velocity and expansiveness correlate with arousal and the basic posture relates to the expressed level of valence, with a contracted posture for low valence and an open posture for high valence. Evaluation for the movements pointing and waving shows that the type of motion has an effect on the expressiveness, and that the observers recognized the intended affective nuances except for the combination of high valence and low arousal. The approaches <ref type="bibr" target="#b161">[162]</ref>, <ref type="bibr" target="#b162">[163]</ref>, <ref type="bibr" target="#b178">[179]</ref> derive a direct relation between motion parameters and affective states.</p><p>Laban provides a scheme to describe motions, but is not in itself a computational model for relating affective states to bodily expression. Several research groups have proposed the use of Laban to derive a computational model for the generation of affect-expressive movements.</p><p>To derive a computational model, first the relationship between an affective expression and its Laban-based movement descriptors needs to be established. This can be achieved by manually designing a set of possible relations <ref type="bibr" target="#b93">[94]</ref>, taking a relation between affective state and movement from psychological studies, e.g., <ref type="bibr" target="#b179">[180]</ref> who refer to <ref type="bibr" target="#b87">[88]</ref>, or evaluating the relation for a specific robot in a user study <ref type="bibr" target="#b157">[158]</ref>, <ref type="bibr" target="#b180">[181]</ref>, <ref type="bibr" target="#b181">[182]</ref>. In these studies, affectexpressive movements are modeled either using only the Effort component <ref type="bibr" target="#b179">[180]</ref> or both Effort and Shape components <ref type="bibr" target="#b93">[94]</ref>, <ref type="bibr" target="#b157">[158]</ref>, <ref type="bibr" target="#b180">[181]</ref>. Second, the mapping between the Laban components and the joint angles and velocities needs to be specified for the kinematics of the robot. Reporting this mapping, e.g., in <ref type="bibr" target="#b157">[158]</ref>, <ref type="bibr" target="#b180">[181]</ref>, facilitates the transfer of a published approach for a specific robot to new studies with similar kinematics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Influence of Embodiment on Expressiveness</head><p>Virtual agents and robots can differ in their embodiment. Perceptual user studies have shown that differences in the embodiment can lead to differences in the perception of affective states, and that the embodiment-specific impact on perception limits the range of affective states that can be conveyed through movement (see Section 4.4). The results from perceptual user studies indicate that a general computational model for generation of affective expressions needs to consider the influence of embodiment on expressiveness. Individual case studies have addressed this influence for a virtual versus a physical robot <ref type="bibr" target="#b161">[162]</ref>, different appearances <ref type="bibr" target="#b93">[94]</ref>, and anthropomorphic versus non-human like appearance <ref type="bibr" target="#b101">[102]</ref>. Each study evaluated a single aspect in a user study and the interplay between aspects has not been investigated.</p><p>Matsumaru investigates the perception of affective states for a teddy bear robot and a black-suited teddy bear robot performing the same expressive motion <ref type="bibr" target="#b181">[182]</ref>. Their results show that the perception of anger and disgust depend on the appearance (regular versus black-suited) of the robot whereas the perception of joy, sadness, surprise, and fear did not depend on the appearance. Differences in human perception are also reported for generated affect-expressive movements displayed on a virtual hand with human-like or frond-like appearances (Fig. <ref type="figure">2</ref>), e.g., sad movements displayed on the frond-like hand were perceived as happy movements <ref type="bibr" target="#b101">[102]</ref>. However, Saerbeck and Bartneck find no difference in expressiveness between an iCat and a Roomba robot when the motion characteristics acceleration and curvature are altered to convey affective expressions <ref type="bibr" target="#b116">[117]</ref>. These studies show that differences in the kinematics and the appearance can lead to differences in the perception of affect-expressive movements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>In the following, we summarize the key issues for automatic recognition and generation of affect-expressive movements.</p><p>We elaborate what movements have been investigated, whether affective states are represented as categorical labels or dimensions, and what movement features and computational models are used. We conclude by outlining future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">What Movements Have Been Investigated?</head><p>The perception of affective states from bodily motions has been studied in psychology for a variety of affect-expressive movements: functional movements, e.g., <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b89">[90]</ref>, <ref type="bibr" target="#b90">[91]</ref>, <ref type="bibr" target="#b91">[92]</ref>, <ref type="bibr" target="#b182">[183]</ref>, communicative movements, e.g., <ref type="bibr" target="#b97">[98]</ref>, <ref type="bibr" target="#b98">[99]</ref>, artistic movements, e.g., <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b86">[87]</ref>, <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b183">[184]</ref>, and abstract movements, e.g., <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b101">[102]</ref>. Perceptual studies mostly focus on analyzing the full body and less attention has been directed to affective expression through individual body parts in isolation from the rest of the body <ref type="bibr" target="#b94">[95]</ref>, <ref type="bibr" target="#b101">[102]</ref>.</p><p>Motivated by these results from psychological studies, several machine learning algorithms have been developed to recognize affect-expressive movements. Algorithms learn either to detect an affective state for one specific movement or for various movements. In the latter case, the training data contains several movements for the same affective state and the approach attempts to generalize across different movements. Several studies analyze communicative movements, e.g., upper-body gestures <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>, and demonstrate that affective states can be recognized from gestures. A small number of studies analyze the recognition of affective states from artistic movements <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, and abstract movements <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b99">[100]</ref>. The results of <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b99">[100]</ref> indicate that affective states can be retrieved not only from different movement types for each affective state but also from the way a movement is performed. Functional movements such as walking and knocking have been investigated in <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref>. As the primary task is walking or knocking, affective expression is secondary and it is displayed mainly by modulation of the functional movement. These variations can be retrieved from high-resolution motion-capture data.</p><p>The generation of affect-expressive movements is often confined to a small set of movement types as exemplars for each affective state. Abstract movements, e.g., hand movements <ref type="bibr" target="#b101">[102]</ref>, and functional movements, e.g., drinking, knocking, kicking <ref type="bibr" target="#b170">[171]</ref>, clapping <ref type="bibr" target="#b168">[169]</ref>, have been studied for computer animations. Implementations on robots mainly focus on upper-body movements, e.g., <ref type="bibr" target="#b93">[94]</ref>, <ref type="bibr" target="#b157">[158]</ref>, <ref type="bibr" target="#b178">[179]</ref>. Exceptions are studies on the functional movements gait <ref type="bibr" target="#b161">[162]</ref> and planar locomotion <ref type="bibr" target="#b179">[180]</ref>. Evaluation of the generated affect-expressive movements show a strong relationship between movement type and style for expressiveness. Modulation of the same movement can lead to different affective expressions <ref type="bibr" target="#b101">[102]</ref>, <ref type="bibr" target="#b161">[162]</ref>. Some affective states may be more easily conveyed by a set of specific movements, e.g., happiness by waving. This is supported by the results of <ref type="bibr" target="#b157">[158]</ref>, <ref type="bibr" target="#b168">[169]</ref>, <ref type="bibr" target="#b177">[178]</ref>, <ref type="bibr" target="#b178">[179]</ref>, <ref type="bibr" target="#b180">[181]</ref> for arm movements. On the other hand, some movements may be unsuitable for conveying certain affective states, e.g., anger in a study-specific conversational gesture <ref type="bibr" target="#b168">[169]</ref>. <ref type="bibr">Mancini et al.</ref> hypothesize that the movement type can signal a particular affective state so strongly that a different affective state can be difficult to communicate <ref type="bibr" target="#b168">[169]</ref>. Possible explanations for this interaction are that some movements are only performed during certain affective states in everyday life, the meaning of the movement type dominates for some movements, and some movements cannot be modified to express certain affective states. More detailed research on the role of movement type and style on the expressiveness would facilitate the generalization of algorithms for automatic recognition and generation across different movements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Affective Dimensions or Categorical Labels ?</head><p>Most perceptual studies on affect-expressive movements use a set of categories representing the affective states. Fewer studies investigate perception of affective dimensions through body movements. Similarly, the majority of the proposed approaches for automatic recognition and generation of affect-expressive movements refer to a set of categorical labels and only a few utilize a dimensional approach.</p><p>When using categorical labels, a subset of the basic emotions is predominantly studied, particularly happiness, sadness, and anger for both automatic recognition and generation. This is in accordance with results from perceptual user studies in psychology, which show that the basic emotions anger, sadness, and happiness are conveyed through body movements <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b89">[90]</ref>. Often, the discrimination between anger and sadness, which differ mainly in arousal, is more accurate than the discrimination of states which differ mainly in valence (recognition: <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b58">[59]</ref>, generation: <ref type="bibr" target="#b179">[180]</ref>, <ref type="bibr" target="#b181">[182]</ref>). An exception is <ref type="bibr" target="#b50">[51]</ref>, where joy and anger are most easily distinguishable. Only a few studies investigate fear <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b99">[100]</ref>, <ref type="bibr" target="#b131">[132]</ref>. Sawada et al. <ref type="bibr" target="#b99">[100]</ref> analyze the complete set of Ekman's basic emotions and find that surprise, disgust, and fear are difficult to express by simple arm movements. The observation that some affective states are less easily conveyed by body movement or movements of individual body parts is also supported by perceptual user studies in psychology, e.g., <ref type="bibr" target="#b88">[89]</ref>, <ref type="bibr" target="#b90">[91]</ref>, <ref type="bibr" target="#b91">[92]</ref>. These findings highlight the importance of both appropriate movement and body part selection to facilitate communication. For some affective states, other contextual cues or modalities (e.g., facial expression) might also be needed.</p><p>A smaller number of studies utilize a dimensional approach for representation (automatic recognition: <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b128">[129]</ref>, <ref type="bibr" target="#b131">[132]</ref>, <ref type="bibr" target="#b137">[138]</ref>, <ref type="bibr" target="#b141">[142]</ref>, <ref type="bibr" target="#b143">[144]</ref>, <ref type="bibr" target="#b149">[150]</ref>, <ref type="bibr" target="#b150">[151]</ref>, generation: <ref type="bibr" target="#b161">[162]</ref>, <ref type="bibr" target="#b162">[163]</ref>, <ref type="bibr" target="#b178">[179]</ref>). They typically all use the dimensions of valence and arousal. Each dimension is either categorized <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b143">[144]</ref> or a regression model is applied to model continuous values for valence and arousal <ref type="bibr" target="#b128">[129]</ref>, <ref type="bibr" target="#b137">[138]</ref>, <ref type="bibr" target="#b149">[150]</ref>. Better discrimination of arousal compared to valence is reported for automatic recognition in <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b137">[138]</ref>. The few studies on generative models using a dimensional approach report similar results: differences in valence are harder to communicate by modifying the functional movement walking than differences in arousal <ref type="bibr" target="#b161">[162]</ref>, and the combination of high valence and low arousal may not be clearly represented in affect-expressive movements <ref type="bibr" target="#b162">[163]</ref>, <ref type="bibr" target="#b178">[179]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">What Movement Features and Computational</head><p>Models Are Used?</p><p>The set of possible features which can be calculated from movement data is large; calculated feature sets differ largely among the studies. The feature sets can be derived heuristically, by dimensionality reduction techniques, or from findings of psychological studies. Most studies include speed as a feature. The contribution of speed to the perception of affective states is confirmed by perceptual studies in psychology, e.g., <ref type="bibr" target="#b87">[88]</ref>, <ref type="bibr" target="#b90">[91]</ref>, <ref type="bibr" target="#b91">[92]</ref>. Furthermore, a feature or multiple features describing the spatial extent of a movement is included in the majority of the studies. The computational models can be divided into direct models which directly relate movement features to affective states, and two-stage models which first model the relation between descriptors and affective states, and second model the relation between movement features and descriptors, e.g., fluidity. Several studies utilize a movement notation system for designing a two-stage model (Laban: e.g., <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b137">[138]</ref>, <ref type="bibr" target="#b157">[158]</ref>, <ref type="bibr" target="#b181">[182]</ref>, BAP: <ref type="bibr" target="#b36">[37]</ref>). Considering the generation of affect-expressive movements, the use of a movement notation system facilitates generalization across different movements and embodiments. The recently proposed BAP system provides a more detailed categorization of body movements than the Laban system, and its applicability to affect-expressive movements has been shown in <ref type="bibr" target="#b36">[37]</ref>.</p><p>Experimental evaluation of a computational model for generation of affect-expressive movements through user studies is essential. The design of such user studies varies widely across the works on generative models. Recommendations on designing and conducting human studies in HRI are summarized in <ref type="bibr" target="#b184">[185]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Directions for Future Work</head><p>To enhance generalization across individual movements and move towards integration into interactive scenarios, the following future directions are suggested:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.1">A Notation System for Affect-Expressive Movements</head><p>Movement features and descriptors vary largely across studies on automatic recognition and generation. A psychologically accepted and validated notation system that relates affective states to bodily expression would provide a common basis facilitating interdisciplinary research. Developing computational models on such a common framework enables comparison between different studies, provides a foundation for the choice of movement features and descriptors, and facilitates the fusion with other modalities to convey affective states. From the computational perspective, it is desired that such a framework describes which movement features represent specific affective states, what influence cultural and inter-individual differences have on bodily expression, and how the embodiment influences expressiveness. The first approaches towards this direction have been undertaken, utilizing the Laban or BAP movement notation system. However, there are some limitations to the application of the current notation systems for computational affectexpressive movement analysis. Except for BAP, the existing notation systems do not provide a validated set of behavioral action units with defined boundaries and phase relationships such as those of FACS. The adoption of a notation system such as BAP for systematic coding of nuances in bodily expression of emotion could drive forward the research in computational analysis of affect-expressive movements. To the best of our knowledge, there is only one study on validating the reliability of BAP, and that study is conducted by BAP's authors <ref type="bibr" target="#b24">[25]</ref>. More experiments are needed to verify BAP intercoder consistency in coding bodily movements either as a whole or movements of individual body parts. Furthermore, to enable the application of the movement notation systems in computational movement analysis, measurable physical correlates (e.g., velocity) of the notation descriptors (e.g., Laban Effort components <ref type="bibr" target="#b74">[75]</ref>) need to be identified and used for quantifying the descriptors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.2">Representation of Affective States: Choice of States and Dimensions</head><p>Automatic recognition and generation have used either a dimensional or categorical representation of affective states.</p><p>Choosing an appropriate representation also includes the consideration of how detailed the representation should be and which affective states should be covered <ref type="bibr" target="#b0">[1]</ref>. To date, most studies include the categories of anger, happiness, and sadness, or the dimensions of pleasure and arousal. Findings from psychology indicate that body movements are not limited to conveying only basic emotions <ref type="bibr" target="#b96">[97]</ref> and future studies may consider including affective states beyond basic emotions. This is also motivated by individual studies in HCI/HRI investigating additional affective states including mood and feelings (recognition: frustration, elation, panic, amusement, relief, despair, interest, irritation, pride, boredom, confusion, delight, flow, frustration, innocence, and laughter types <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b129">[130]</ref>, <ref type="bibr" target="#b130">[131]</ref>, <ref type="bibr" target="#b131">[132]</ref>, <ref type="bibr" target="#b138">[139]</ref>, <ref type="bibr" target="#b185">[186]</ref>; generation: pride, disgust, surprise, fear, relaxation, and nervousness <ref type="bibr" target="#b93">[94]</ref>, <ref type="bibr" target="#b157">[158]</ref>, <ref type="bibr" target="#b162">[163]</ref>, <ref type="bibr" target="#b179">[180]</ref>, <ref type="bibr" target="#b181">[182]</ref>). It is further of interest which affective phenomena are reliably conveyed by body movements. Affect-expressive movements may also provide the utility to detect affective states which are less clearly conveyed through other modalities or which can only be reliably recognized by observing more than a single modality, e.g., pride, which human observers can only decode reliably using both facial and bodily cues <ref type="bibr" target="#b26">[27]</ref>. Taking a multi-modal approach opens the opportunity to consider a broader range of affective states which have for HCI/HRI. Considering a dimensional representation, it may be beneficial to include additional dimensions beyond pleasure and arousal, and future work may address determining the appropriate choice of dimensions that are most relevant for HCI/HRI applications. Individual studies on automatic recognition and generation indicate that also other dimensions may be expressed and perceived from body movements (automatic recognition: dominance/power, potency, avoidance, expectation, intensity and interest <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b139">[140]</ref>, <ref type="bibr" target="#b143">[144]</ref>, <ref type="bibr" target="#b149">[150]</ref>; generation: dominance <ref type="bibr" target="#b161">[162]</ref>). Adding the dimensions potency and expectation (unpredictability) is motivated by the studies <ref type="bibr" target="#b186">[187]</ref>, <ref type="bibr" target="#b187">[188]</ref>, which emphasize that the two dimensions pleasure and arousal are not sufficient to describe a large variety of emotions. Furthermore, Cowie et al. recommends that engagement is an essential dimension for HCI/HRI, but it is rarely studied in psychology <ref type="bibr">[24, p. 17]</ref> or used, e.g., as evaluation parameter in <ref type="bibr" target="#b151">[152]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.3">Generalization for Different Embodiments</head><p>A computational model for the generation of affect-expressive movements has not been proposed which generalizes across different kinematics and appearances. Developing such a model remains a future research area. Its evaluation can be performed for either physical robots or virtual agents. A user study on affect-expressive movements comparing the perception of affective walking styles for a physical robot and its virtual animation and similar user studies on facial expressions show that the expressiveness of a robot and of its animated display differ only slightly <ref type="bibr" target="#b123">[124]</ref>, <ref type="bibr" target="#b124">[125]</ref>, <ref type="bibr" target="#b161">[162]</ref>. This enables an evaluation of computational models for a large variety of kinematics and appearances in virtual environments, where animations can be more easily modified than the hardware design of a real robot. When the kinematics/dynamics and the appearance of the animation and a physical robot are the same, results on the performance of the computational model in terms of expressiveness of the animated robot can be transferred to the physical robot and design guidelines for developing the hardware of the real, affect-expressive robot can be provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.4">The Benefit of Integrating Context Knowledge</head><p>Context knowledge summarizes information about the environment, interaction partners (e.g., information about gender, personality traits, and culture), current activities, and interactions. The current approaches for automatic recognition and generation of affect-expressive movements largely do not take context knowledge into account. A future direction is the development of adaptive systems to select appropriate movements through interaction with the user. A robot may learn to choose from a set of affectexpressive movements these motions which most likely match the expectations of the current interaction partner. Furthermore, analyzing the context when a movement is performed may lead to more robust recognition of affective states. When the context is considered, actions can indicate affective states, such as freezing during a conversation. Taking the context into account enhances automatic recognition and generation of affect-expressive movements by interpreting whether taken actions express an affective state, reasoning how probable the expressed affective state is in the current scenario, minimizing the risk that movements are misinterpreted as affective expressions even though they are only functional, and adapting the behavior of a virtual agent or robot to the expectations of a user.</p><p>Multi-modal recognition provides a methodology to detect affective states conveyed through movements in the context of other modalities, e.g., <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b128">[129]</ref>, <ref type="bibr" target="#b130">[131]</ref>, <ref type="bibr" target="#b140">[141]</ref>, <ref type="bibr" target="#b152">[153]</ref>. Combining several modalities, such as conveying affective states through facial expressions, speech, and bodily expressions provides more reliable estimates for automatic recognition and enhances believability and congruency for generation. This also provides a methodology to include affective expressions which are difficult to express by a single modality, e.g., pride <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.5">Integration into Interactive Scenarios</head><p>The long term goal of research on affective expressions is to integrate the automatic recognition and generation of affectexpressive movements into HCI/HRI scenarios. Key challenges include dealing with expressions that are natural and variable in duration and intensity, and of integrating these two modules into higher-level cognitive and interaction models.</p><p>Acted expressions may differ from natural expressions occurring during HCI/HRI interaction <ref type="bibr" target="#b1">[2]</ref>. The transition from acted or elicited expressions to natural expressions can be guided by results of psychological studies on affective expressions in daily life and during human-human interaction, e.g., only a small number of expressions are perceived as intense, most are intermediate, and some are not emotional in a natural scenario <ref type="bibr" target="#b23">[24]</ref>. This study indicates that machine learning algorithms need to consider different intensities of affective expressions. This has been done by qualifying categorical labels with a level of intensity (e.g., strong happy, weak happy) <ref type="bibr" target="#b187">[188]</ref>, and, for the dimensional representation, moving towards regression techniques <ref type="bibr" target="#b128">[129]</ref> or by adding intensity as an additional dimension <ref type="bibr" target="#b187">[188]</ref>. Furthermore, affect-expressive movements can vary in duration, and onsets and offsets of a movement can be indistinct. Considering automatic recognition, this influences the length of the segmentation windows: the expression of very brief emotions can be missed with large windows; but short windows may result in analyzing incomplete movements. Here, analyzing individual movements can be avoided by generalization across different movements, e.g., by general motion descriptors, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b130">[131]</ref>, <ref type="bibr" target="#b131">[132]</ref>. The use of a movement notations system can provide guidance for choosing such motion descriptors, e.g., the Effort and Shape components of Laban.</p><p>When moving towards HCI/HRI scenarios, it is often required to integrate automatic recognition of affective states and/or generation of affective expressions into higher-level cognitive models. Cognitive models can include, e.g., perception of context cues, motivational processes, emotion generation processes, and action selection processes. When integrating generative models into a cognitive system, the interface protocol should specify parameters for intensity, duration, and repetition of an affective expression <ref type="bibr" target="#b175">[176]</ref>, <ref type="bibr" target="#b176">[177]</ref>. Furthermore, generative models can be shaped by personality traits, e.g., angry and happy characters have preferences for different expressions <ref type="bibr" target="#b151">[152]</ref>.</p><p>Furthermore, the interaction between several individuals or between a virtual agent/robot and individuals can be modeled. Here, psychological theories on human-human nonverbal interaction can provide valuable guidance <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b188">[189]</ref>, <ref type="bibr" target="#b189">[190]</ref>, <ref type="bibr" target="#b190">[191]</ref>. This is closely related to social signal processing in HCI/HRI <ref type="bibr" target="#b191">[192]</ref>, <ref type="bibr" target="#b192">[193]</ref>. Most of the reviewed approaches detect the affective state of one individual. Extensions to the interaction and relation of several subjects or analysis of group behavior should be considered. An example considering affect-expressive movements is the study by Varni et al., who propose a real-time algorithm to analyze the synchronization of affective behavior, empathy, and the emergence of functional roles in a social group during music performances and music listening <ref type="bibr" target="#b193">[194]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUDING REMARKS</head><p>To date, essential achievements have been made to enable automatic recognition and generation of affect-expressive movements in HCI and HRI for a limited set of specific movements, primarily in laboratory settings. For both recognition and generation, communicative, functional, artistic, and abstract movements have been studied. However, only a small subset of all possible movements, i.e., gestures, walking, waving, and head movements, have been explored in detail. In these studies, affective states are represented either by categorical labels or a dimensional approach. A few studies demonstrate that affective states beyond basic emotions and beyond changes in pleasure and arousal can be communicated through affect-expressive movements. When modeling the relationship of affective states and movement features, movement notation systems (Laban, Delsarte, or BAP) can provide guidance for choosing motion descriptors. The use of a common movement notation system for affect-expressive movements would facilitate the transfer of knowledge from psychological studies to their applications in HCI/HRI and the generalization across movement types and embodiments. Finally, the development of agents capable of recognizing and generating affect-expressive movements during natural, interactive scenarios, integrating context knowledge and adapting to individual users remains a key challenge for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The PAD model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>7 .</head><label>7</label><figDesc>Person-dependent recognition refers to the case where both training and test data come from the same individual, and person-independent recognition refers to the case where the training and test data come from different individuals.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>1949-3045 ß 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, VOL. 4, NO. 4, OCTOBER-DECEMBER 2013</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors acknowledge the support of the Toronto Rehabilitation Institute, which receives funding under the Provincial Rehabilitation Research Program from the Ministry of Health and Long-Term Care in Ontario. The views expressed do not necessarily reflect those of the Ministry. This work was also supported by the Natural Sciences and Engineering Research Council of Canada. The authors wish to thank Prof. Gentiane Venture and Dr. Ritta Baddoura for their additional suggestions on the emotion literature. The authors also wish to thank the anonymous reviewers for the thorough and insightful review comments. Their suggestions significantly improved the quality and readability of the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Affect Detection: An Interdisciplinary Review of Models, Methods, and Their Applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Calvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Mello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Compuitng</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="37" />
			<date type="published" when="2010-06">Jan.-June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic, Dimensional and Continuous Emotion Recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Synthetic Emotions</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="99" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Challenges in Real-Life Emotion Annotation and Machine Learning Based Detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vidrascu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="407" to="422" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emotion Representation, Analysis and Synthesis in Continuous Space: A Survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. FG, Workshop Emotion Synthesis, Representation, and Analysis in Continuous Space</title>
		<meeting>IEEE Int&apos;l Conf. FG, Workshop Emotion Synthesis, Representation, and Analysis in Continuous Space</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">From the Lab to the Real World: Affect Recognition Using Multiple Cues Modalities</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<editor>J. Or</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>InTech Education and Publishing</publisher>
			<biblScope unit="page" from="185" to="218" />
		</imprint>
	</monogr>
	<note>Affective Computing: Focus on Emotion Expression, Synthesis, and Recognition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic Analysis of Facial Expressions: The State of the Art</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J M</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1424" to="1445" />
			<date type="published" when="2000-12">Dec. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="58" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Survey of Non-Facial/Non-Verbal Affective Expressions for Appearance-Constrained Robots</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bethel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man, and Cybernetics, Part C: Applications and Rev</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="92" />
			<date type="published" when="2008-01">Jan. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Repertoire of Nonverbal Behavior: Categories, Origins, Usage, and Coding</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semiotica</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="98" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Head and Body Cues in the Judgement of Emotion: A Reformulation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Percept Motor Skill</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="711" to="724" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Planalp</surname></persName>
		</author>
		<title level="m">Communicating Emotion: Social, Moral, and Cultural Processes</title>
		<imprint>
			<publisher>Cambridge Univ. Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bodily Expressions of Emotion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wallbott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European J. Social Psychology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="879" to="896" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attributing Emotion to Static Body Postures: Recognition Accuracy, Confusions, and Viewpoint Dependence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Coulson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="139" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Floyd</surname></persName>
		</author>
		<title level="m">Nonverbal Communication. Allyn and Bacon</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hwang</surname></persName>
		</author>
		<title level="m">Nonverbal Communication: Science and Applications</title>
		<imprint>
			<publisher>Sage Publications</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perception of the Smile and Other Emotions of the Body and Face at Different Distances</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Walk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Walters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Psychonomic Soc</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">510</biblScope>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detecting Deception from the Body or Face</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="288" to="298" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Body Cues, Not Facial Expressions, Discriminate between Intense Positive and Negative Emotions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Aviezer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Trope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Todorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">338</biblScope>
			<biblScope unit="issue">6111</biblScope>
			<biblScope unit="page" from="1225" to="1229" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Affective Body Expression Perception and Recognition: A Survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kleinsmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bianchi-Berthouze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="38" />
			<date type="published" when="2013-03">Jan.-Mar. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Affect and Emotions in Intelligent Agents: Why and How?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ortony</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
	<note>Affective Information Processing</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Emotion and Emotional Competence: Conceptual and Theoretical Issues for Modelling Agents</title>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Blueprint for Affective Computing: A Sourcebook</title>
		<imprint>
			<publisher>Oxford Univ. Press</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">What Are Emotions? and How Can They Be Measured?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Science Information</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="695" to="729" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Affective Reactions to Consumption Situations: A Pilot Investigation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Derbaix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Economic Psychology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="325" to="355" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Emotion: Concepts and Definitions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sussman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Zeev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion-Oriented Systems</title>
		<imprint>
			<biblScope unit="page" from="9" to="30" />
			<date type="published" when="2011">2011</date>
			<publisher>Spinger</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Body Action and Posture Coding System (BAP): Development and Reliability</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mortillaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Measures of Emotion: A Review</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Mauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition Emotion</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="209" to="237" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Show Your Pride: Evidence for a Discrete Emotion Expression</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Tracy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="194" to="197" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Perceiving Affect from Arm Movement</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pollick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Paterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruderlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<date type="published" when="2001-12">Dec. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Computational Models of Emotion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Petta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Blueprint for Affective Computing: A Sourcebook</title>
		<imprint>
			<publisher>Oxford Univ. Press</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="21" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Embodiment of Sadness and Depression-Gait Patterns Associated with Dysphoric Mood</title>
		<author>
			<persName><forename type="first">J</forename><surname>Michalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vollmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heidenreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schulte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychosomatic Medicine</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="580" to="587" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The Communication of Emotion</title>
		<author>
			<persName><forename type="first">R</forename><surname>Buck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Guilford Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The Principles of Mathematics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Russel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1903">1903</date>
			<publisher>Allen &amp; Unwin</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Varieties of Cues to Emotion in Naturally Occurring Situations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Planalp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition Emotion</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Why Bodies? Twelve Reasons for Including Bodily Expressions in Affective Neuroscience</title>
		<author>
			<persName><forename type="first">B</forename><surname>De Gelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Gelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Trans. Royal Soc. London. Series B</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="issue">1535</biblScope>
			<biblScope unit="page" from="3475" to="3484" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Standing Up for the Body. Recent Progress in Uncovering the Networks Involved in the Perception of Bodies and Bodily Expressions</title>
		<author>
			<persName><forename type="first">B</forename><surname>De Gelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Den Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Meeren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Kret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tamietto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroscience and Biobehavioral Rev</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="513" to="527" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Argyle</surname></persName>
			<affiliation>
				<orgName type="collaboration">Bodily Communication</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<pubPlace>Methuen</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Emotion Expression in Body Action and Posture</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mortillaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1085" to="1101" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic Temporal Segment Detection and Affect Recognition from Face and Body Display</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="64" to="84" />
			<date type="published" when="2009-02">Feb. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Darwin, Perception, and Facial Expression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals New York Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">1000</biblScope>
			<biblScope unit="page" from="105" to="221" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Four Models of Basic Emotions: A Review of Ekman and Cordaro, Izard, Levenson, and Panksepp and Watt</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Tracy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Randles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion Rev</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="397" to="405" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A New Pan-Cultural Facial Expression of Emotion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Motivation and Emotion</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="159" to="168" />
			<date type="published" when="1986-06">June 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Are There Basic Emotions?</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Rev</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="550" to="553" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Circumplex Models of Personality and Emotions</title>
	</analytic>
	<monogr>
		<title level="m">American Psychological Assoc</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Plutchik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Conte</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Pleasure-Arousal-Dominance: A General Framework for Describing and Measuring Individual Differences in Temperament</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mehrabian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Psychology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="261" to="292" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The Structure of Current Affect: Controversies and Emerging Consensus</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Directions in Psychological Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="14" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Emotional Category Data on Images from the International Affective Picture System</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mikels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fredrickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Larkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lindberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Magold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reuter-Lorenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="626" to="630" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The Varieties of Consumption Experience: Comparing Two Typologies of Emotion in Consumer Behavior</title>
		<author>
			<persName><forename type="first">W</forename><surname>Havlena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Holbrook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Consumer Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="394" to="404" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Evidence for a Three-Factor Theory of Emotions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mehrabian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Research in Personality</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="273" to="294" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Solving the Emotion Paradox: Categorization and the Experience of Emotion</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Barrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Personality and Social Psychology Rev</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="46" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Oxford Dictionaries</title>
		<imprint>
			<date type="published" when="2010-04">Apr. 2010</date>
			<publisher>Oxford Univ. Press</publisher>
		</imprint>
	</monogr>
	<note>Gesture</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Gesture-Based Affective Computing on Motion Capture Data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kapur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Virji-Babul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzanetakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Driessen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. First Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</title>
		<meeting>First Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Beyond Facial Expressions: Learning Human Emotion from Body Gestures</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conf</title>
		<meeting>British Machine Vision Conf</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Towards Recognizing Emotion with Affective Dimensions through Body Gestures</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R D</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Osano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Automatic Face and Gesture Recognition (FG)</title>
		<meeting>Int&apos;l Conf. Automatic Face and Gesture Recognition (FG)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Recognition of Affect Based on Gait Patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1050" to="1061" />
			<date type="published" when="2010-08">Aug. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Recognition of Emotions in Gait Patterns by Means of Artificial Neural Nets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sch€ Ollhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lubienetzki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>F€ Olling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kokenge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Davids</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="79" to="92" />
			<date type="published" when="2008-06">June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Detecting Affect from Non-Stylised Body Motions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bernhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Second Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</title>
		<meeting>Second Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="59" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Expressive Interfaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Camurri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mazzarino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Volpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition, Technology and Work</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="15" to="22" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Emotion Recognition from Dance Image Sequences Using Contour Approximation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-I</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U.-M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Syntactic, and Statistical Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="547" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Recognising Human Emotions from Body Movement and Gesture Dynamics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villabla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Camurri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</title>
		<meeting>Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="71" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Discriminative Functional Analysis of Human Movements</title>
		<author>
			<persName><forename type="first">A</forename><surname>Samadani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kuli C</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="1829" to="1839" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">The Facial Action Coding System</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
			<publisher>Consulting Psychologists Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Guest</surname></persName>
		</author>
		<title level="m">Dance Notation: The Process of Recording Movement on Paper. Dance Horizons</title>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">The Bodily Action Research of Ray L. Birdwhistell and Paul Ekman: Implications for Oral Interpretation Theory</title>
		<author>
			<persName><forename type="first">K</forename><surname>White</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
		</imprint>
		<respStmt>
			<orgName>Univ. of Arizona</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Notation Systems for Coding Nonverbal Behavior: A Review</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Rozensky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Honor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Psychopathology and Behavioral Assessment</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="132" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The Specific Affect Coding System (SPAFF)</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Coan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Gottman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Emotion Elicitation and Assessment</title>
		<imprint>
			<publisher>Oxford Univ. Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="267" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Kinesics and Context: Essays on Body Motion Communication</title>
		<author>
			<persName><forename type="first">R</forename><surname>Birdwhistell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970">1970</date>
			<publisher>Univ. of Pennsylvania Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">CAO: A Fully Automatic Emoticon Analysis System Based on Theory of Kinesics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ptaszynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dybala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rzepka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Araki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="59" />
			<date type="published" when="2010-06">Jan. June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Laban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Effort</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><surname>Evans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1947">1947</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The Mastery of Movement</title>
		<author>
			<persName><forename type="first">R</forename><surname>Laban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ullmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plays</title>
		<imprint>
			<date type="published" when="1971">1971</date>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Effort-Shape Analysis of Movement: The Unity of Expression and Function</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bartenieff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965">1965</date>
		</imprint>
		<respStmt>
			<orgName>Albert Einstein College of Medicine, Yeshiva Univ.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Robot-Human Interface Using Laban Movement Analysis Inside a Bayesian Framework</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
		<respStmt>
			<orgName>Univ. of Coimbra</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">The EMOTE Model for Effort and Shape</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Badler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computer Graphics and Interactive Techniques</title>
		<meeting>Conf. Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Quantitative Analysis of Impression of Robot Bodily Expression Based on Laban Movement Theory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Robotics Soc. Japan</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="104" to="111" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Efficient Motion Retrieval in Large Databases</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I</forename><surname>Badler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Symp. Interactive 3D Graphics and Games (I3D)</title>
		<meeting>Symp. Interactive 3D Graphics and Games (I3D)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Laban Effort and Shape Analysis of Affective Hand and Arm Movements</title>
		<author>
			<persName><forename type="first">A</forename><surname>Samadani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gorbet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kuli C</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</title>
		<meeting>Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="343" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Communicating Emotions and Mental States to Robots in a Real Time Parallel Framework Using Laban Movement Analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lourens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Berkel</surname></persName>
		</author>
		<author>
			<persName><surname>Barakova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotic and Autonomous Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="1256" to="1265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Shawn</surname></persName>
		</author>
		<title level="m">Every Little Movement: A Book About Franc ¸ois Delsarte. Dance Horizons</title>
		<imprint>
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">DelsArtMap: Applying Delsarte&apos;s Aesthetic System to Virtual Agents</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pasquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Nasr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Int&apos;l Conf. Intelligent Virtual Agents</title>
		<meeting>10th Int&apos;l Conf. Intelligent Virtual Agents</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="139" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">An Exploration of Delsarte&apos;s Structural Acting System</title>
		<author>
			<persName><forename type="first">S</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carnicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gratch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Okhmatovskaia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rizzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Sixth Int&apos;l Conf. Intelligent Virtual Agents (IVA)</title>
		<meeting>Sixth Int&apos;l Conf. Intelligent Virtual Agents (IVA)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="80" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">AutoBAP: Automatic Coding of Body Action and Posture Units from Wearable Sensors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Velloso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gellersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</title>
		<meeting>Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="135" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Proxemics, kinesics, and Gaze</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Harrigan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The New Handbook of Methods in Nonverbal Behavior Research</title>
		<imprint>
			<publisher>Oxford Univ. Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="137" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Perception of Movement and Dancer Characteristics from Point Light Displays of Dance</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brownlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Egbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Radcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Record</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="411" to="421" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Recognizing the Sex of a Walker from a Dynamic Point-Light Display</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kozlowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cutting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="575" to="580" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Recognizing Friends by Their Walk: Gait Perception without Familiarity Cues</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cutting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kozlowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Psychonomic Soc</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="353" to="356" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Perception of Emotion from Dynamic Point-Light Displays Represented in Dance</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dittrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Troscianko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="727" to="738" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Seeing Music Performance: Visual Influences on Perception and Experience</title>
		<author>
			<persName><forename type="first">W</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Russo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semiotica</title>
		<imprint>
			<biblScope unit="issue">156</biblScope>
			<biblScope unit="page" from="203" to="227" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Children&apos;s Decoding of Emotion in Expressive Body Movement: The Development of Cue Attunement</title>
		<author>
			<persName><forename type="first">R</forename><surname>Boone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental Psychology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1007" to="1016" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Recognizing Emotion from Dance Movement: Comparison of Spectator Recognition and Automated Techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Camurri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lagerl€</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Volpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Human-Computer Studies</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="213" to="225" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Emotion Perception from Dynamic and Static Body Expressions in Point-Light and Full-Light Displays</title>
		<author>
			<persName><forename type="first">A</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dittrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gemmell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="717" to="746" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">The Identification of Emotions from Gait Information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Montepare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Nonverbal Behavioral</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="33" to="42" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Motion Capture and Emotion: Affect Detection in Whole Body Movement</title>
		<author>
			<persName><forename type="first">E</forename><surname>Crane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</title>
		<meeting>Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="95" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Critical Features for the Perception of Emotion from Gait</title>
		<author>
			<persName><forename type="first">C</forename><surname>Roether</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Omlor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Giese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Kinetic Architectures and Geotextile Installations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Beesley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Riverside Architectural Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Remarks on Designing of Emotional Movement for Simple Communication Robot</title>
		<author>
			<persName><forename type="first">K</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hosokawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hashimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Industrial Technology (ICIT)</title>
		<meeting>IEEE Int&apos;l Conf. Industrial Technology (ICIT)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="585" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Rigid Head Motion in Expressive Speech Animation: Analysis and Synthesis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1075" to="1086" />
			<date type="published" when="2007-03">Mar. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">J. Fast, Body Language. Pocket Books</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">A Study of the Judgment of Manual Expression as Presented in Still and Motion Pictures</title>
		<author>
			<persName><forename type="first">L</forename><surname>Carmichael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wessell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Social Psychology</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="142" />
			<date type="published" when="1937">1937</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Affective Prosody in American Sign Language</title>
		<author>
			<persName><forename type="first">J</forename><surname>Reilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Mcintire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seago</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sign Language Studies</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="113" to="128" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Perception of Emotions in the Hand Movement Quality of Finnish Sign Language</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hietanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lepp€</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Lehtonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="64" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Expression of Emotions in Dance: Relation between Arm Movement Characteristics and Emotion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sawada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perceptual and Motor Skills</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="697" to="708" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">A Study of Human Performance in Recognizing Expressive Hand Movements</title>
		<author>
			<persName><forename type="first">A</forename><surname>Samadani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dehart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kuli C</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kubica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gorbet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Int&apos;l Symp</title>
		<imprint>
			<biblScope unit="page" from="93" to="100" />
			<date type="published" when="2011">2011</date>
			<publisher>RO-MAN</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Perception and Generation of Affective Hand Movements</title>
		<author>
			<persName><forename type="first">A</forename><surname>Samadani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kubica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gorbet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kuli C</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Social Robotics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="51" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Gender Effects in Decoding Nonverbal Cues</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bull</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="845" to="857" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Gender Differences in Judgments of Multiple Emotions from Facial Expressions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="206" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Accuracy of the Judgment of Facial Expression of Emotions as a Function of Sex and Level of Education</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kirouac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="7" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Sex Differences in the Perception of Affective Facial Expressions: Do Men Really Lack Emotional Sensitivity?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Montagne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kessels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frigerio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">De</forename><surname>Haan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Perrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="136" to="141" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Communication of Individual Emotions by Spontaneous Facial Expressions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Manstead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="737" to="743" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Sex Differences in the Encoding and Decoding of Negative Facial Emotions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rotter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="148" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Perceptual Skill in Decoding Facial Affect</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Palchoudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perceptual and Motor Skills</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="96" to="98" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Gender, Emotional Expression, and Parent-Child Boundaries</title>
		<author>
			<persName><forename type="first">L</forename><surname>Brody</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Lawrence Erlbaum Assoc</publisher>
			<biblScope unit="page" from="139" to="170" />
		</imprint>
	</monogr>
	<note>Emotion: Interdisciplinary Perspectives</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Women&apos;s Greater Ability to Perceive Happy Facial Emotion Automatically: Gender Differences in Affective Priming</title>
		<author>
			<persName><forename type="first">U</forename><surname>Donges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suslow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>p. e41745</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Gender Affects Body Language Reading</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kr€ Uger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Enck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kr€ Ageloh-Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pavlova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">He Throws Like a Girl (but Only When He&apos;s Sad): Emotion Affects Sex-Decoding of Biological Motion Displays</title>
		<author>
			<persName><forename type="first">K</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pollick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="265" to="280" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Facilitating the Perception of Anger and Fear in Male and Female Walkers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Halovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kroos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AISB Symp. Mental States, Emotions and Their Embodiment</title>
		<meeting>AISB Symp. Mental States, Emotions and Their Embodiment</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="3" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<author>
			<persName><forename type="first">Aldebaran-Robotics</forename><surname>Company</surname></persName>
		</author>
		<title level="m">URL: www.aldebaran-robotics. com</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Emotional Body Language Displayed by Artificial Agents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cañamero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Interactive Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2012-03">Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Perception of Affect Elicited by Robot Motion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saerbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bartneck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM/IEEE Int&apos;l Conf. Human-Robot Interaction</title>
		<meeting>ACM/IEEE Int&apos;l Conf. Human-Robot Interaction</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="53" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">An Experimental Study of Apparent Behavior</title>
		<author>
			<persName><forename type="first">F</forename><surname>Heider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Psychology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="243" to="259" />
			<date type="published" when="1944">1944</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Perception of Human Motion with Different Geometric Models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tumblin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Am. J. Psychology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="307" to="316" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Evaluating the Emotional Content of Human Motions on Real and Virtual Characters</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Org</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mchugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>O'sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Symp. Applied Perception in Graphics and Visualization (APGV)</title>
		<meeting>Symp. Applied Perception in Graphics and Visualization (APGV)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<title level="m" type="main">How Motion Reveals Intention: Categorizing Social Interactions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Oxford Univ. Press</publisher>
			<biblScope unit="page" from="257" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Emotional Interaction through Physical Movement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. HCI: Intelligent Multimodal Interaction Environments</title>
		<meeting>Int&apos;l Conf. HCI: Intelligent Multimodal Interaction Environments</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="401" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Gender Differences in the Perception of Affective Movements</title>
		<author>
			<persName><forename type="first">A</forename><surname>Samadani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gorbet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kuli C</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Workshop Human Behaviour Understanding</title>
		<meeting>Int&apos;l Workshop Human Behaviour Understanding</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">In Your Face, Robot! The Influence of a Character&apos;s Embodiment on How Users Perceive Its Emotional Expressions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bartneck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reichenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Breemen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Design and Emotion</title>
		<meeting>Conf. Design and Emotion</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="32" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Comparing a Computer Agent with a Humanoid Robot</title>
		<author>
			<persName><forename type="first">A</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kiesler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fussell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Torrey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM/IEEE Int&apos;l Conf. Human-Robot Interaction (HRI)</title>
		<meeting>ACM/IEEE Int&apos;l Conf. Human-Robot Interaction (HRI)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">The Expression of Emotions in Man and Animals</title>
		<author>
			<persName><forename type="first">C</forename><surname>Darwin</surname></persName>
		</author>
		<imprint>
			<publisher>Chicago Univ. Press</publisher>
			<biblScope unit="page">1872</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">What Is an Emotion?</title>
		<author>
			<persName><forename type="first">W</forename><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mind</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="188" to="205" />
			<date type="published" when="1884">1884</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Affective Computing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Continuous Prediction of Spontaneous Affect from Multiple Cues and Modalities in Valence-Arousal Space</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="92" to="105" />
			<date type="published" when="2011-06">Apr.-June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Deception Detection through Automatic, Unobtrusive Analysis of Nonverbal Behavior</title>
		<author>
			<persName><forename type="first">T</forename><surname>Meservy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Burgoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Nunamaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Twitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tsechpenakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="36" to="43" />
			<date type="published" when="2005-09">Sept. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Multimodal Emotion Recognition in Speech-Based Interaction Using Facial Expression, Body Gesture and Acoustic Analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kessous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Caridakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Multimodal User Interfaces</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="48" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Toward a Minimal Representation of Affective Gestures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Glowinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Camurri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Volpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mortillaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="106" to="118" />
			<date type="published" when="2011-06">Apr.-June 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Analyzing Posture and Affect in Task-Oriented Tutoring</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Grafsgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Florida Artificial Intelligence Research Soc. Conf</title>
		<meeting>25th Florida Artificial Intelligence Research Soc. Conf</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="438" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Towards Real-Time Affect Detection Based on Sample Entropy Analysis of Expressive Gesture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Glowinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fourth Int&apos;l Conf. Affective Computing and Intelligent Interaction</title>
		<meeting>Fourth Int&apos;l Conf. Affective Computing and Intelligent Interaction</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="527" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">A Two-Fold Pca-Approach for Inter-Individual Recognition of Emotions in Natural Walking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Int&apos;l Conf. MLDM</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">A Comparison of PCA, KPCA and LDA for Feature Extraction to Recognize Affect in Gait Patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Seiberl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>K€ Uhnlenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwirtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</title>
		<meeting>Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="195" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Extraction of Spatio-Temporal Primitives of Emotional Body Expressions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Omlor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Giese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">10-12</biblScope>
			<biblScope unit="page" from="1938" to="1942" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Toward E-Motion-Based Music Retrieval a Study of Affective Gesture Recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Amelynck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grachten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Noorden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="250" to="259" />
			<date type="published" when="2012-06">Apr. June 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Automatic Detection of Learner&apos;s Affect from Gross Body Language</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graesser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="150" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">Automated Posture Analysis for Detecting Learner&apos;s Interest Level</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition Workshop (CVPRW)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition Workshop (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="49" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Modeling Naturalistic Affective States via Facial, Vocal, and Bodily Expressions Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Karpouzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Caridakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kessous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raouzaiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Malatesta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kollias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Artificial Intelligence for Human Computing</title>
		<meeting>Int&apos;l Conf. Artificial Intelligence for Human Computing</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="91" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Automatic Recognition of Non-Acted Affective Postures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kleinsmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bianchi-Berthouze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man, and Cybernetics Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1027" to="1038" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">A Motion Capture Library for the Study of Identity, Gender and Emotion Perception from Biological Motion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Paterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pollick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="134" to="141" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Recognizing Affective Dimensions from Body Posture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kleinsmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bianchi-Berthouze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Second Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</title>
		<meeting>Second Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="48" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Using Speech Data to Recognize Emotion in Human Gait</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Okuno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Third Int&apos;l Conf. Human Behavior Understanding</title>
		<meeting>Third Int&apos;l Conf. Human Behavior Understanding</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="52" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Emotion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Keltner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Encyclopedia of Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="162" to="166" />
			<date type="published" when="2000">2000</date>
			<publisher>Oxford Univ. Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Gender Differences in Nonverbal Communication of Emotion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Horgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gender and Emotion</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="97" to="117" />
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge Univ. Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Head Pose Estimation in Computer Vision: A Survey</title>
		<author>
			<persName><forename type="first">E</forename><surname>Murphy-Chutorian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="607" to="626" />
			<date type="published" when="2009-04">Apr. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Gesture Recognition: A Survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Acharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man, and Cybernetics, Part C: Applications and Rev</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="311" to="324" />
			<date type="published" when="2007-05">May 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Dimensional Emotion Prediction from Spontaneous Head Gestures for Interaction with Sensitive Artificial Listeners</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</title>
		<meeting>Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="371" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Automatic Natural Expression Recognition Using Head Movement and Skin Color Features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Monkaresi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Calvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Working Conf. Advanced Visual Interfaces</title>
		<meeting>Int&apos;l Working Conf. Advanced Visual Interfaces</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="657" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Building Autonomous Sensitive Artificial Listeners</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schroder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bevacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pammi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pelachaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De Sevin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wollmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="183" />
			<date type="published" when="2012-06">Apr. June 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">A Generic Framework for the Inference of User States in Human Computer Interaction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Glodek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Layher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tschechne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schwenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Palm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Multimodal User Interfaces</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="117" to="141" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Analysis of Expressive Gesture: The Eyesweb Expressive Gesture Processing Library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Camurri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mazzarino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Volpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Fifth Int&apos;l Gesture Workshop Gesture-Based Comm. in Human-Computer Interaction</title>
		<imprint>
			<biblScope unit="page" from="460" to="467" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Expression of Affects in Embodied Conversational Agents</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hyniewska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Niewiadomski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pelachaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Blueprint for Affective Computing, chapter</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="213" to="221" />
			<date type="published" when="2010">2010</date>
			<publisher>Oxford Univ. Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<title level="m">Designing Sociable Robots</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<monogr>
		<title level="m" type="main">Evaluating Emotion Expressing Robots in Affective Space</title>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sosnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>I-Tech Education and Publishing</publisher>
			<biblScope unit="page" from="235" to="245" />
		</imprint>
	</monogr>
	<note>Human-Robot Interaction. chapter 12</note>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Motion Rendering System for Emotion Expression of Human Form Robots Based on Laban Movement Analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Masuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Symp</title>
		<meeting>IEEE Int&apos;l Symp</meeting>
		<imprint>
			<publisher>RO-MAN</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="344" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Dynamic Facial Expression Analysis and Synthesis with MPEG-4 Facial Animation Parameters</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1383" to="1396" />
			<date type="published" when="2008-10">Oct. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Whole Body Emotion Expression for KOBIAN Humanoid Robot-Preliminary Experiments with Different Emotional Patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zecca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mizoguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kawabata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Itoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Takanishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Symp</title>
		<meeting>IEEE Int&apos;l Symp</meeting>
		<imprint>
			<publisher>RO-MAN</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="381" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Towards an Affect Space for Robots to Display Emotional Body Language</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Canamero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Symp</title>
		<meeting>IEEE Int&apos;l Symp</meeting>
		<imprint>
			<publisher>RO-MAN</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="491" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Towards Mapping Emotive Gait Patterns from Human to Robot</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schwimmbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Symp</title>
		<meeting>IEEE Int&apos;l Symp</meeting>
		<imprint>
			<publisher>RO-MAN</publisher>
			<date type="published" when="2010-09">Sept. 2010</date>
			<biblScope unit="page" from="258" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Interpretation of Emotional Body Language Displayed by Robots</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hiolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mazel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cañamero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Workshop Affective Interaction in Natural Environments (AFFINE)</title>
		<meeting>Int&apos;l Workshop Affective Interaction in Natural Environments (AFFINE)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Priniciples of Traditional Animation Applied to 3D Computer Animation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lasseter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<monogr>
		<title level="m" type="main">Disney Animation: The Illusion of Life</title>
		<author>
			<persName><forename type="first">F</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Johnston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Abbeville Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Kerlow</surname></persName>
		</author>
		<title level="m">The Art of 3D: Computer Animation and Effects</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">The Role of Emotion in Believable Agents</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. ACM</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="122" to="125" />
			<date type="published" when="1994-07">July 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Biometic Animated Creatures</title>
		<author>
			<persName><forename type="first">B</forename><surname>Blumberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biologically Inspired Intelligent Robots</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="47" to="71" />
			<date type="published" when="2003">2003</date>
			<publisher>SPIE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Evaluating the Communication of Emotion via Expressive Gesture Copying Behaviour in an Embodied Humanoid Agent</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</title>
		<meeting>Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="215" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">Analysis of Emotional Gestures for the Generation of Expressive Copying Behaviour in an Embodied Agent</title>
		<author>
			<persName><forename type="first">G</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Seventh Int&apos;l Gesture Workshop Gesture-Based Human-Computer Interaction and Simulation</title>
		<imprint>
			<biblScope unit="page" from="193" to="198" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Emotion from Motion</title>
		<author>
			<persName><forename type="first">K</forename><surname>Amaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruderlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Calvert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Graphics Interface (GI &apos;96)</title>
		<meeting>Conf. Graphics Interface (GI &apos;96)</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="222" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Fourier Principles for Emotion-Based Human Figure Animation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Unuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Anjyo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Takeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIG-GRAPH</title>
		<meeting>ACM SIG-GRAPH</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="91" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Verbs and Adverbs: Multidimensional Motion Interpolation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bodenheimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="32" to="40" />
			<date type="published" when="1998-10">Sept./Oct. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Style Machines</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIG-GRAPH</title>
		<meeting>ACM SIG-GRAPH</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Learning Physics-Based Motion Style with Nonlinear Inverse Optimization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Popovi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1071" to="1081" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Towards a Common Framework for Multimodal Generation: The Behavior Markup Language</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krenn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marsella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pelachaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pirker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Thorisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vilhjalmsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</title>
		<meeting>Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="21" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Representing Emotions and Related States in Technological Systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schr€ Oder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pirker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lamolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zovato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emotion-Oriented Systems</title>
		<imprint>
			<biblScope unit="page" from="369" to="387" />
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Expressive Copying Behavior for Social Agents: A Perceptual Analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man, and Cybernetics, Part A: Systems and Humans</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="776" to="783" />
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Motion Modification Method to Control affective Nuances for Robots</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nakagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shinozawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Akimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hagita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/RSJ Int&apos;l Conf. Intelligent Robots and Systems (IROS)</title>
		<meeting>IEEE/RSJ Int&apos;l Conf. Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="5003" to="5008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Expressing and Interpreting Emotional Movements in Social Games with Robots</title>
		<author>
			<persName><forename type="first">E</forename><surname>Barakova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lourens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Personal and Ubiquitous Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="457" to="467" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Emotion Detection from Body Motion of Human Form Robot Based on Laban Movement Analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Masuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Itoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Int&apos;l Conf. Principles of Practice in Multi-Agent Systems (PRIMA)</title>
		<meeting>12th Int&apos;l Conf. Principles of Practice in Multi-Agent Systems (PRIMA)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="322" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Discrimination of Emotion from Movement and Addition of Emotion in Movement to Improve Human-Coexistence Robot&apos;s Personal Affinity</title>
		<author>
			<persName><forename type="first">T</forename><surname>Matsumaru</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Symp</title>
		<meeting>IEEE Int&apos;l Symp</meeting>
		<imprint>
			<publisher>RO-MAN</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="387" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">Lateral Asymmetry of Bodily Emotion Expression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Roether</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Omlor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Giese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="329" to="330" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Children&apos;s Understanding of Emotion in Dance</title>
		<author>
			<persName><forename type="first">I</forename><surname>Lagerl€</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Djerf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European J. Developmental Psychology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="409" to="431" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Review of Human Studies Methods in HRI and Recommendations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Bethel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Social Robotics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="347" to="359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Laughter Type Recognition from Whole Body Motion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcloughlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Curran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bianchi-Berthouze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</title>
		<meeting>Int&apos;l Conf. Affective Computing and Intelligent Interaction (ACII)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="349" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">The World of Emotion Is Not Two-Dimensional</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Fontaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R P</forename><surname>Ellsw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Science</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1050" to="1057" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">The Semaine Corpus of Emotionally Coloured Character Interactions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Multimedia and Expo (ICME)</title>
		<meeting>IEEE Int&apos;l Conf. Multimedia and Expo (ICME)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1079" to="1084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">A Discrepancy-Arousal Explanation of Mutual Influence in Expressive Behavior for Adult-Adult and Infant-Adult Dyadic Interaction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cappella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Greene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comm. Monographs</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="89" to="114" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Floyd</surname></persName>
		</author>
		<title level="m">Communicating Affection: Interpersonal Behavior and Social Context</title>
		<imprint>
			<publisher>Cambridge Univ. Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<monogr>
		<title level="m" type="main">Nonverbal Communication in Close Relationships</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Floyd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Lawrance Erlbaum Associates</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Social Signal Processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="108" to="111" />
			<date type="published" when="2007-07">July 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Bridging the Gap between Social Animal and Unsocial Machine: A Survey of Social Signal Processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pelachaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>D'errico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schr€ Oder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="87" />
			<date type="published" when="2012-03">Jan.-Mar. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">A System for Real-Time Multimodal Analysis of Nonverbal Affective Social Interaction in User-Centric Media</title>
		<author>
			<persName><forename type="first">G</forename><surname>Varni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Volpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Camurri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="576" to="590" />
			<date type="published" when="2010-10">Oct. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<monogr>
		<title level="m" type="main">Michelle Karg is a postdoctoral fellow in the Electrical &amp; Computer Engineering Department at the University of Waterloo. Her research interests include machine learning for human movement analysis applied to affective computing, rehabilitation, and clinical movement analysis</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<monogr>
		<title level="m" type="main">Samadani is working toward the PhD degree in the University of Waterloo&apos;s Electrical and Computer Engineering Department. His research interests include affective computing, human movement recognition and generation, and human-machine interaction. Rob Gorbet is an associate professor at the University of Waterloo&apos;s Centre for Knowledge Integration. His research interests include the design and creation of interactive artistic installations</title>
		<author>
			<persName><forename type="first">Ali-Akbar</forename></persName>
			<affiliation>
				<orgName type="collaboration">interdisciplinary collaboration</orgName>
			</affiliation>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">His research interests include robotics, robot vision, attention, visual servoing, social robotics</title>
	</analytic>
	<monogr>
		<title level="s">Coburg University of Applied Sciences and Arts</title>
		<imprint/>
	</monogr>
	<note>Kolja K€ uhnlenz is professor at the Dept. of EE and CS. and emotions</note>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Jesse Hoey is an associate professor in the Cheriton School of Computer Science at the University of Waterloo. He is also an adjunct scientist at the Toronto Rehabilitation Institute, where he is a coleader of the AI and Robotics Research Team</title>
	</analytic>
	<monogr>
		<title level="m">His research is on artificial intelligence and health informatics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<monogr>
		<title level="m" type="main">For more information on this or any other computing topic, please visit our Digital Library at</title>
		<ptr target="www.computer.org/publications/dlib" />
		<imprint/>
		<respStmt>
			<orgName>Dana Kuli c is an assistant professor with the Department of Electrical and Computer Engineering, University of Waterloo, Canada</orgName>
		</respStmt>
	</monogr>
	<note>Her research interests include robot learning, humanoid robots, and human motion analysis</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
