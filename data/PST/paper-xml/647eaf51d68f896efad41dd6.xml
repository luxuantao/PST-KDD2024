<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Conversational Recommendation Systems via Counterfactual Data Simulation</title>
				<funder ref="#_bqkUebR">
					<orgName type="full">Outstanding Innovative Talents Cultivation Funded Programs 2022 of Renmin University of China</orgName>
				</funder>
				<funder ref="#_gtfUwNx">
					<orgName type="full">Beijing Natural Science Foundation</orgName>
				</funder>
				<funder ref="#_SYjgdT6">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-06-05">5 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaolei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xinyu</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Pan</surname></persName>
							<email>panfan3@huawei.com</email>
						</author>
						<author>
							<persName><forename type="first">Zhao</forename><surname>Cao</surname></persName>
							<email>caozhao1@huawei.com</email>
						</author>
						<author>
							<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
							<email>jrwen@ruc.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Huawei Poisson Lab</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">KDD &apos;23</orgName>
								<address>
									<addrLine>August 6-10</addrLine>
									<postCode>2023</postCode>
									<settlement>Long Beach</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Conversational Recommendation Systems via Counterfactual Data Simulation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-05">5 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3580305.3599387</idno>
					<idno type="arXiv">arXiv:2306.02842v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>? Information systems ? Recommender systems Conversational Recommender System</term>
					<term>Counterfactual Data Augmentation</term>
					<term>Prompt Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conversational recommender systems (CRSs) aim to provide recommendation services via natural language conversations. Although a number of approaches have been proposed for developing capable CRSs, they typically rely on sufficient training data for training. Since it is difficult to annotate recommendation-oriented dialogue datasets, existing CRS approaches often suffer from the issue of insufficient training due to the scarcity of training data.</p><p>To address this issue, in this paper, we propose a CounterFactual data simulation approach for CRS, named CFCRS, to alleviate the issue of data scarcity in CRSs. Our approach is developed based on the framework of counterfactual data augmentation, which gradually incorporates the rewriting to the user preference from a real dialogue without interfering with the entire conversation flow. To develop our approach, we characterize user preference and organize the conversation flow by the entities involved in the dialogue, and design a multi-stage recommendation dialogue simulator based on a conversation flow language model. Under the guidance of the learned user preference and dialogue schema, the flow language model can produce reasonable, coherent conversation flows, which can be further realized into complete dialogues. Based on the simulator, we perform the intervention at the representations of the interacted entities of target users, and design an adversarial training method with a curriculum schedule that can gradually optimize the data augmentation strategy. Extensive experiments show that our ? Beijing Key Laboratory of Big Data Management and Analysis Methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The recent success of conversational intelligence <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref> has empowered a more convenient way for information seeking by conversational recommender systems (CRSs) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref>, which aims to provide high-quality recommendation service through multi-turn natural language conversations. Typically, a CRS recommends the suitable items that satisfy the user need via a recommender module, and generates the proper response based on the conversation context and predicted items via a conversation module. These two modules are systematically integrated to fulfill the information-seeking task.</p><p>To develop capable CRSs, various approaches have been proposed in the literature <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b49">50]</ref> based on deep neural networks. In particular, the powerful Transformer network <ref type="bibr" target="#b1">[2]</ref> and pre-trained language models (PLM) <ref type="bibr" target="#b35">[36]</ref> have largely raised the performance bar on conversational recommendation. These approaches rely on high-quality recommendation-oriented conversation data for model training. While it is difficult to manually create large-scale CRS datasets, which require well-trained annotators to generate coherent, diverse conversation flow in an information-seeking scenario <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b53">54]</ref>. Therefore, existing CRS datasets <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22]</ref> are often limited in data size, lacking sufficient coverage of diverse information needs and user preferences. To alleviate this issue, existing studies incorporate external data (e.g., knowledge graphs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b51">52]</ref>) and model resources (e.g., DialoGPT <ref type="bibr" target="#b47">[48]</ref>) to reduce the demand for training data in developing a capable CRS.</p><p>Despite the performance improvement, the fundamental issue of insufficient training in existing CRSs has not been well addressed due to the scarcity of training datasets. As a general solution to data scarcity, data augmentation techniques <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">43]</ref> have been widely applied in a variety of tasks, which either use heuristic strategies <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b41">42]</ref> or learnable models <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b44">45]</ref> for enlarging the data size. However, it is challenging to augment high-quality recommendation-oriented dialogues (short as recommendation dialogues) with automatic approaches, since it needs to mimic the interactive information-seeking process via a reasonable, coherent conversation flow. To be reasonable, the conversation scenario should be designed with meaningful user needs and suitable item recommendations, which conform to the factual information in the given domain. To be coherent, the augmented user preference should be consistent throughout the whole conversation, which should be well clarified and maintained as the conversation progresses. Considering these difficulties, existing work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref> that uses specific rewriting strategies cannot generate high-quality CRS datasets.</p><p>To enhance the reasonableness and coherence of the conversation flow, we take a holistic perspective to develop the augmentation approach for recommendation dialogues by gradually incorporating the rewriting or adaptation into a real dialogue. Specially, each rewriting is expected to be carefully controlled without interfering with the entire conversation flow. Indeed, such intuition can be well fit into the framework of counterfactual data augmentation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref>, which incorporates counterfactual learning for augmenting the limited data. In this setting, the essence of our approach is to answer the key question: "What the dialogues would be if we intervene on the observed user preference?", where user preference is considered to be the most important factor to determine a conversation flow. To instantiate it, we consider characterizing user preference and organizing the conversation flow by the entities involved in the dialogue (e.g., movie actors and genres). Further, our rewriting strategy is implemented by a learnable edit function, which can produce informative edits to the entity representations for improving the recommendation module. In this way, the original user preference is gradually revised and finally reaches the level that a high-quality yet different conversation is augmented.</p><p>To this end, in this paper, we present the proposed CounterFactual data simulation approach for CRS, named CFCRS, for alleviating the issue of data scarcity in CRSs. Our core idea is to leverage counterfactual learning to augment user preference and then employ the augmented user preference to simulate conversation data. Specifically, we design a recommendation dialogue simulator that can generate reasonable, coherent conversations for two target users. To guarantee the quality of the simulated conversations, we design a flow language model to generate the conversation flow, which is guided by the learned user preference and dialogue sketch. Based on the dialogue simulator, we perform the intervention at the representations of the interacted entities of target users, and design an adversarial training method with a curriculum schedule that can gradually optimize the edit function towards an improved recommendation capacity of CRSs.</p><p>To the best of our knowledge, it is the first time that counterfactual data simulation has been utilized to improve CRS models. Our proposed framework is agnostic to model implementations, hence is general to various CRS methods. To evaluate the effectiveness of our approach, we evaluate its performance with several representative CRS models on two public CRS datasets. Experimental results show that our approach can consistently boost the performance of these models, and outperform other data augmentation methods, especially when the training data is limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we summarize the related work as follows.</p><p>Conversational Recommender System. Conversational recommender systems (CRSs) aim to provide recommendation services through conversational interactions. One line of work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b52">53]</ref> relies on pre-defined interactive actions (e.g., asking preferences about item attributes or making recommendations) and hand-crafted templates to converse with users. They mainly focus on capturing user preferences and giving accurate recommendations within as few turns as possible. Another line of work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b51">52]</ref> focuses on interacting with users through more free-form natural language conversations. They aim to capture the preferences from the conversation context and then generate the recommended items with persuasive responses. The above CRS methods are mostly developed by deep neural networks, which require sufficient high-quality data for training. However, it is expensive to annotate high-quality CRS examples and existing datasets are generally limited in scale. To address it, external resources like knowledge graphs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b51">52]</ref> and reviews <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b54">55]</ref> have been introduced to enrich the datasets. However, the fundamental problem of insufficient training examples has not been well solved. In this work, we aim to solve the data scarcity problem via counterfactual data simulation.</p><p>Counterfactual Data Augmentation. Counterfactual data augmentation <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b55">56]</ref> focuses on generating unrecorded counterfactual examples from the real ones. Recently, it has been leveraged to alleviate the data scarcity problem and can improve the performance and robustness of deep neural networks. For example, in recommender systems, CASR <ref type="bibr" target="#b40">[41]</ref> proposes to generate counterfactual user behavior sequences based on the real ones to supply the data for training sequential recommendation models. While for open-domain dialogue generation, CAPT <ref type="bibr" target="#b25">[26]</ref> uses counterfactual inference to automatically augment high-quality responses with different semantics to solve the one-to-many problem. In this work, we apply counterfactual data augmentation to the CRS task, aiming to obtain sufficient high-quality data. We also propose a curriculum learning strategy to gradually optimize the data augmentation strategy for CRSs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>In this section, we present the proposed CounterFactual data simulation approach for CRS, named CFCRS, for alleviating the issue of data scarcity in CRSs, which is depicted in Figure <ref type="figure">1</ref>. The overview of our approach CFCRS. We first adopt curriculum counterfactual learning to augment the user preference at the representation level, and then use the flow language model guided by user and schema prompts to generate conversation flows, which are then realized into dialogues. The edit function and CRS model are optimized with adversarial training to improve both the quality of the augmented data and the recommendation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of Our Approach</head><p>Task Formulation. Conversational recommender systems (CRSs) aim to provide accurate item recommendation services through multi-turn natural language conversations. At each turn, the system either makes recommendations or chats with the user for preference elicitation. Such a process ends when the user accepts the recommended items or leaves. Formally, at the ( ? + 1)-th turn, given the dialogue history ? ? = {? ? } ? ?=1 consisting of ?-turn utterances and the item set I, the system should (1) select a set of candidate items I ? from the entire item set I to recommend, and (2) generate the response utterance ? ?+1 to the user. Besides, knowledge graph <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b54">55]</ref> as an important auxiliary resource is usually available, denoted by G. Typically, a CRS consists of the recommender module (parameterized by ? ? ) and the conversation module (parameterized by ? ? ), which are responsible for the recommendation and response generation tasks, respectively. General Model Learning. Formally, let D = {?? ? , ? ? , ? ? ?} denote the set of training samples, where ? ? is the dialogue history, ? ? is the ground-truth response, and ? ? is the recommended item for the ?-th training sample. The optimization objectives for the two modules can be denoted as follows:</p><formula xml:id="formula_0">? ? ? (D) = - ?? ?? ? ,? ? ? ?? log ?(? ? |? ? ; ? ? ),<label>(1)</label></formula><formula xml:id="formula_1">? ? ? (D) = - ?? ?? ? ,? ? ? ?? log ?(? ? |? ? ; ? ? ),<label>(2)</label></formula><p>where ?(?) and ?(?) are the recommender and conversation modules, respectively. In the literature, existing CRSs mainly focus on designing various models or architectures to implement the two modules. For the recommendation module, it can be implemented with collaborative filtering <ref type="bibr" target="#b17">[18]</ref>, GNN <ref type="bibr" target="#b51">[52]</ref>, or Transformer <ref type="bibr" target="#b56">[57]</ref> models. For the conversation module, it can be implemented with the vanilla Transformer <ref type="bibr" target="#b1">[2]</ref> or PLM <ref type="bibr" target="#b38">[39]</ref>. These approaches rely on high-quality CRS datasets to train the underlying models, which are often limited in size. To address this limitation, we propose to simulate high-quality data for conversation recommendation, which can be generally applied to various CRSs.</p><p>Counterfactual Learning for Dialogue Simulation. Our approach is inspired by the recent progress on counterfactual data augmentation <ref type="bibr" target="#b24">[25]</ref>, which incorporates counterfactual learning for augmenting the limited data. In our setting, the essence of our approach is to answer the core question: "What the dialogues would be if we intervene on the observed user preference?". As the basis of our approach, we design a recommendation-oriented dialogue simulator (short as recommendation dialogue simulator) that can generate reasonable, coherent conversations tailored for two target users (Section 3.2). Our recommendation dialogue simulator adopts a multi-stage generation process guided by the learned user preference and dialogue sketch: flow schema ? conversation flow ? dialogue realization. Based on the simulator, we construct the data augmentation via counterfactual learning (Section 3.3), which performs the intervention at the representations of the interacted entities of a target user. Further, we design an adversarial training method with a curriculum schedule that can gradually optimize the edit function towards an improved recommendation capacity of CRSs. In what follows, we introduce the two parts in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Recommendation Dialogue Simulator</head><p>The goal of the recommendation dialogue simulator is to generate recommendation-oriented conversation data, so as to improve the performance of existing CRSs. Typically, it is difficult to create fluent, coherent conversation data, since it needs to simulate the free interaction for information seeking between two real users via chit-chat. As our solution, we develop a multi-stage generation process that first generates the conversation flow according to the predicted flow schema and then realizes the dialogue based on the generated flow. In our approach, we first introduce the basic concepts of conversation flow and flow schema.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Conversation</head><p>Flow and Flow Schema. The conversation flow explicitly traces the key elements (i.e., entities) of the informationseeking process. For example, given a two-turn conversation:</p><p>[Seeker]: I love all kinds of comedy movies.</p><p>[Recommender]: Have you seen 21 Jump Street?</p><p>[Seeker]: Yes, I love this film because Jonah Hill is in it.</p><p>[Recommender]: Try another comedy movie with him, Superbad. we can derive a conversation flow: comedy ? 21 Jump Street ? Jonah Hill ? comedy ? Superbad. Based on such a flow, we can further generalize it into a flow schema: genre ? item ? actor ? genre ? item.</p><p>Formally, a conversation flow ? ?,? between two users ? and ? is characterized as a sequence of mentioned entities in a conversation arranged in the occurrence order, denoted by</p><formula xml:id="formula_2">? ?,? = ?? 1 , ? ? ? , ? ? , ? ? ? , ? ? ?,</formula><p>and the corresponding flow schema (with an equal length to the flow) is characterized as a sequence of type tokens, denoted by</p><formula xml:id="formula_3">? ?,? = ?? 1 , ? ? ? , ? ? , ? ? ? , ? ? ?,</formula><p>where ? ? is a mentioned entity from a knowledge graph (KG) E and ? ? = Type(? ? ) indicating the type of ? ? . As we can see, conversation flow and flow schema are useful to generate concrete conversation content by capturing the entity preferences of users and establishing the dialogue sketch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Preference</head><p>Prompt Guided Flow Language Model. In our approach, we design a flow language model (FLM) that is parameterized by ? ? based on the preference prompts for generating the conversation flow. Specifically, to generate a conversation flow, we first sample two target users ? and ? as the seeker and recommender, respectively, and then employ the two users to predict a flow schema ??,? . Then, the representations of target users (i.e., user prompt) and the predicted schema (i.e., schema prompt) are taken as the prompts to the FLM for generating the conversation flow as follows:</p><formula xml:id="formula_4">? ?,? ? FLM ? ? , ? ? user prompt , {? ? } ? ?=1 schema prompt ; ? ? .<label>(3)</label></formula><p>Next, we discuss how to derive the two parts of prompts.</p><p>User Prompt Learning. Since the simulated dialogue occurs between the two target users, it is important to consider their preferences for generating the conversation flow. To capture the user preference, in recommender systems, it is common to assign each user a unique user ID, and learn the ID embedding based on the interacted items or entities as the user preference representation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>. However, in our simulation setting, we would like to generate more diverse user representations that are not limited to the real users in the CRS datasets. For this purpose, we do not explicitly maintain a user ID but learn ID-agnostic user representations. Specifically, following the previous work on CRSs <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b51">52]</ref>, we assume that a KG is available and extend this KG by attaching user nodes to their interacted entity nodes to compose a new heterogeneous knowledge graph (HKG), denoted as G. To capture relational semantics between entities, we utilize R-GCN <ref type="bibr" target="#b31">[32]</ref> to learn entity representations on the HKG. Formally, let ? denote a node placeholder for the HKG, associated with an embedding vector ? ? ? R ? ? derived from R-GCN, where ? ? denotes the embedding size. We utilize the self-attention mechanism to aggregate entity embeddings as the preference representation of the user ?:</p><formula xml:id="formula_5">? ? = E ? ? ? , (4) ? = softmax(? ? ? tanh(W ? E ? )),</formula><p>where E ? is the matrix consisting of the embeddings of all the interacted entities of user ?, ? is the attention weight vector reflecting the importance of each entity, and W ? and ? are trainable parameters. A major advantage of this representation method is that it can be easily adapted to new users by modeling the entity preference based on the associated interaction records. Schema Prompt Learning. In order to produce fluent, coherent conversations, we employ frequent flow schema to structure the conversation. Although the conversation flows can be very diverse, the frequent flow schemas for a CRS corpus are usually limited. Thus, we consider employing real CRS datasets to construct flow schemas with frequent pattern mining algorithms <ref type="bibr" target="#b8">[9]</ref>, and obtain a set of frequent flow schemas, denoted as S. Then, the schema prediction task is cast as a classification problem over the schema set S based on user preference. Formally, we compute the prediction probability for a flow schema as follows:</p><formula xml:id="formula_6">Pr(? ?,? |?, ?) = softmax MLP([? ? , ? ? ]) ,<label>(5)</label></formula><p>where ? and ? are the two target users involved in the dialogue, and ? ?,? is the flow schema. In practice, first, we select the most probable flow schema according to Eq. ( <ref type="formula" target="#formula_6">5</ref>). Then, we obtain the corresponding type embeddings by decomposing the predicted schema into type tokens {? ? } ? ?=1 , where each type token embedding ? ? is obtained by looking up the type embedding table.</p><p>Flow Language Model Pre-Training. To model the conversation flow, we construct an FLM based on Transformer, which utilizes the encoder-decoder architecture. Following Eq. ( <ref type="formula" target="#formula_4">3</ref>), the encoder takes the learned user prompt (i.e., ? ? and ? ? ) and schema prompt (i.e., {? ? } ? ?=1 ) as input, and the decoder generates the conversation flow in an autoregressive manner based on the prompt. Formally, let ? ? be the embedding of a token ? ? (an entity) in the conversation flow ? ?,? , and the probability of the flow to be generated is formulated as:</p><formula xml:id="formula_7">Pr(? ?,? ) = ? ?? ?=1 Pr(? ? |? 0 , . . . , ? ? -1 )<label>(6)</label></formula><formula xml:id="formula_8">= ? ?? ?=1 softmax W[? ? ; ? ?,? ] + ?</formula><p>where ? ?,? is the encoding of prompt, W and ? are trainable parameters. To pre-train the FLM, we require large amounts of conversation flows from various user pairs. Since the original CRS dataset is usually limited in conversation size, we consider generating pseudo conversation flows for pre-training. The basic procedure consists of three major steps: (i) we first randomly sample one flow schema from the frequent flow schema set S; (ii) then, we sample an entity sequence from the constructed HKG G as the conversation flow according to the schema; (iii) finally, we randomly divide the entities in the sequence into two groups, which correspond to the entity preference of two users involving the conversation. When a schema cannot lead to a reachable entity path, we continue to sample another schema.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Dialogue Realization.</head><p>With the pre-trained FLM and entity preference of users, we can generate the corresponding conversation flows at a large scale. Next, we realize the generated conversation flows into recommendation dialogues.</p><p>Here, we adopt a simple template-based approach for dialogue realization. Specifically, we first collect the templates from observed dialogues by delexicalization, i.e., substituting the mentioned entities with placeholders, e.g., "&lt;genre&gt;" for "comedy". Then, the entities in conversation flows can be sequentially filled into these templates as new recommendation dialogues. For instance, an utterance "I am in a mood for something scary" would be converted to the template "I am in a mood for something &lt;genre&gt;". Since the focus of this work is to enhance the recommendation ability of CRSs instead of the general chit-chat ability, we do not adopt pre-trained dialogue models (e.g., DialogGPT <ref type="bibr" target="#b47">[48]</ref>) to realize these utterances. Our proposed method is simple yet effective to ensure fluency in language and faithfulness in conversation flow.</p><p>After building the recommendation dialogue simulator, we can employ it to generate simulated data with user preference representations as input (as will be used in Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Curriculum Counterfactual Learning</head><p>Although the above recommendation dialogue simulator can effectively enlarge the dataset by simulation, it is still limited to the actual users in existing datasets. In this part, we introduce a curriculum counterfactual learning approach that can learn to generate diverse data by augmenting the user preference representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Counterfactual User Preference Augmentation.</head><p>Recall that in Section 3.2.2, we utilize a self-attentive mechanism to learn the user preference representation based on the interacted entities with Eq. ( <ref type="formula">4</ref>). Formally, the set of interacted entities of user ? is denoted as E ? = {? 1 , . . . , ? ? , . . . , ? ? and their embeddings are also aggregated as a matrix</p><formula xml:id="formula_9">E ? = [? 1 , . . . , ? ? , . . . , ? ? ].</formula><p>In existing work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28]</ref>, discrete <ref type="bibr" target="#b20">[21]</ref> and continuous <ref type="bibr" target="#b27">[28]</ref> itemlevel edits have been explored for user data augmentation. To combine the merits of both approaches, we consider an entity-level edit to augment new user preferences. Specifically, we revise one entity embedding at each time with a specially designed edit function ? (?), in which the entity selection is discrete and the embedding revision is continuous. Such a way can generate diverse user preferences while gradually incorporating controllable revisions. To instantiate the edit function, a number of model choices can be considered, e.g., neural networks. However, we empirically find that it is difficult to optimize such an edit function, due to the lack of supervision signals in real datasets. Thus, we consider a simple yet effective edit function that directly adds a small disturbance vector ? ? :</p><formula xml:id="formula_10">? (? ? ) = ? ? + ? ? ,<label>(7)</label></formula><p>We denote all the disturbance vectors as ? ? .</p><p>According to <ref type="bibr" target="#b7">[8]</ref>, such a simplified edit function is easier to learn and interpret, since samples near the decision boundary are usually discriminative in revealing the underlying data patterns. For each user ?, we can perform the edit ? times, so as to produce ? different augmentations, each editing one specific entity embedding in E ? to generate the augmented user preference ?? (originally ? ? ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Adversarial</head><p>Learning with Curriculum Schedule. As our edit function can be learned in a differentiable manner, we propose to use adversarial learning to enhance the informativeness of the augmented user preference representations. Intuitively, a more informative training instance tends to cause a larger loss in the recommendation accuracy, as such a user preference has not been well captured by the current model. Taking an adversarial learning perspective, the counterfactual edit function (parameterized by ? ? ) aims to maximize the loss of the recommender module (parameterized by ? ? ), while the recommender module aims to minimize its loss on the simulated data. In addition, we perform adversarial learning with a curriculum schedule to stabilize the optimization.</p><p>Adversarial Training. Let ? ? ? (? | ?? , ?? ) denote the recommendation dialogue simulator, which returns the probability of generating a dialogue ? given the edited embeddings of two users ?? and ?? by ? ? . The learning objective can be formulated as follows:</p><formula xml:id="formula_11">? ? * ? ,? * ? = min ? ? max ? ? E ??? ? ? (? | ?? , ?? ) [? ? ? (?) -?? ? ? ? ? 2 2 ],<label>(8)</label></formula><p>where ? ? = {? ? , ? ? } is the edit vectors for users ? and ? (Eq. ( <ref type="formula" target="#formula_10">7</ref>)), ? ? ? (?) is the loss of the recommendation module for the generated data ?, and ? is the regularization weight for ? ? . Note that Eq. ( <ref type="formula" target="#formula_11">8</ref>) presents the optimization objective for a pair of users, which can be easily extended to all user pairs. To optimize the above objective (with two groups of parameters ? ? and ? ? ), we can alternatively optimize each group of parameters by keeping the other group fixed. It is relatively straightforward to train the parameters of the recommender module (i.e., ? ? ) by a standard recommendation loss (e.g., cross-entropy loss <ref type="bibr" target="#b29">[30]</ref>). However, it is infeasible to directly optimize the edit vectors in an end-to-end way, since it involves the generation of discrete conversation data. To tackle this issue, we adopt the classic REINFORCE algorithm <ref type="bibr" target="#b43">[44]</ref> to update the parameters as follows:</p><formula xml:id="formula_12">? ? = ? ? +? ? ?? ? =1 ? ? ? (? ? )? ? ? log ? ? ? (? ? | ?? , ?? ) -2? ?? ? ,<label>(9)</label></formula><p>where ? is the learning rate and ? conversations are sampled.</p><p>Curriculum Arrangement. In order to keep the training stable, we consider a curriculum learning approach that gradually increases the augmentation level: small variations are encouraged at the beginning of training, while larger variations can be gradually applied to enhance the model capacity. As shown in Eq. ( <ref type="formula" target="#formula_11">8</ref>), we incorporate a controlling weight ? on ? ? . To simulate counterfactual data in an easy-to-difficult process, we dynamically tune the augmentation level in each iteration. Specifically, we apply an annealing mechanism to regularize ? ? with a shrinking ?:</p><formula xml:id="formula_13">? = ? ? ? (? ) , (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>where ? is the initial weight, ? is the decay ratio, and ? is the current course. In this way, as the course gets more difficult, i.e., the augmentation level increases, the CRS model can continually learn from diverse and informative training samples to improve its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Parameter Learning</head><p>The parameters of our framework consist of four groups, namely the recommendation dialogue simulator ? ? , the counterfactual edit function ? ? , and the recommender module ? ? and conversation modules ? ? of the target CRS model. Algorithm 1 presents the training algorithm of our framework. First of all, we pre-train the parameters of the recommendation dialogue simulator ? ? with the union of real and pseudo conversation flow data using the cross-entropy loss. After pre-training, parameters ? ? are fixed. Then, we perform curriculum counterfactual learning to augment new data for CRS learning. In each iteration, the parameters of the counterfactual edit function ? ? Algorithm 1: The training algorithm of our framework.</p><p>Input: The conversational recommendation dataset D, HKG G Output: Parameters of the recommender module ? ? and conversation module ? ? in CRSs. <ref type="foot" target="#foot_0">1</ref> Pre-train the parameters of the recommendation dialogue simulator ? ? with the union of real and pseudo data sampled from G.</p><p>2 Pre-train the parameters of the recommender module ? ? using the real dataset D.</p><formula xml:id="formula_15">3 for ? = 1 ? ? do 4</formula><p>Set the regularization parameter ? according to curriculum arrangement using Eq. ( <ref type="formula" target="#formula_13">10</ref>).</p><p>5</p><p>Optimize the parameters of edit function ? ? by maximizing the loss of the recommender module using Eq. ( <ref type="formula" target="#formula_12">9</ref>) and derive new user preference ?.</p><p>6</p><p>Use new user preference ? to simulate data ? with the recommendation dialogue simulator by Eq. ( <ref type="formula" target="#formula_4">3</ref>) and Eq. ( <ref type="formula">4</ref>).</p><p>7</p><p>Optimize the parameters of the recommender module ? ? by minimizing its loss on simulated data ?. 8 end 9 Optimize the conversation module ? ? with the augmented data. 10 return ? ? and ? ? . and the recommender module of the target CRS model ? ? are optimized via adversarial learning using Eq. ( <ref type="formula" target="#formula_11">8</ref>). Specifically, we first learn the counterfactual edit function to maximize the loss of the recommender module, and then optimize the recommender module to minimize its loss on the simulated data. After the curriculum learning schedule, we optimize the parameters of the conversation module ? ? with the union of simulated and real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head><p>In this section, we first set up the experiments, then report the results and give a detailed analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. To verify the effectiveness of our approach, we conduct experiments on two widely used English CRS datasets, i.e., Re-Dial <ref type="bibr" target="#b17">[18]</ref> and INSPIRED <ref type="bibr" target="#b9">[10]</ref>. The ReDial dataset is an English CRS dataset about movie recommendations, and is constructed through crowdsourcing workers on Amazon Mechanical Turk (AMT). Similar to ReDial, the INSPIRED dataset is also an English CRS dataset about movie recommendations, but with a much smaller size. The statistics of both datasets are summarized in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Baselines. Here we consider two major tasks for CRS evaluation, namely recommendation and conversation. For comparison, we select several representative methods (including both CRS models and adapted PLMs) tailored to each task.</p><p>? BERT <ref type="bibr" target="#b3">[4]</ref>: It is a bidirectional PLM pre-trained via the masked language modeling task on a large-scale general corpus. We utilize the representation of the [???] token for recommendation.</p><p>? GPT-2 <ref type="bibr" target="#b28">[29]</ref>: It is an autoregressive PLM pre-trained via the language modeling task on large-scale general corpora. We concatenate the utterances in the conversation history as inputs, and take the generated text for response while using the representation of the last token for recommendation.</p><p>? DialoGPT <ref type="bibr" target="#b47">[48]</ref>: It continues to pre-train GPT-2 on large-scale dialogue corpora. We use it in the same way as GPT-2.</p><p>? ReDial <ref type="bibr" target="#b17">[18]</ref>: It is proposed along with the ReDial dataset, which includes a conversation module based on HRED <ref type="bibr" target="#b34">[35]</ref> and a recommendation module based on a denoising auto-encoder <ref type="bibr" target="#b10">[11]</ref>.</p><p>? KBRD <ref type="bibr" target="#b1">[2]</ref>: It introduces DBpedia to enhance the semantics of entities mentioned in the dialogues.</p><p>? BARCOR <ref type="bibr" target="#b36">[37]</ref>: It proposes a unified framework based on BART, which tackles two tasks with a single model.</p><p>? UniCRS <ref type="bibr" target="#b38">[39]</ref>: It designs knowledge-enhanced prompts based on DialoGPT to fulfill both tasks in a unified approach.</p><p>Among these baselines, BERT, GPT-2, and DialoGPT are PLMs, where BERT and GPT-2 is pre-trained on general corpora while DialoGPT is pre-trained on dialogue corpora. We fine-tune these PLMs to encode the dialogue and generate items to recommend and utterances to respond to. ReDial, KBRD, BARCOR, and Uni-CRS are CRS methods, where ReDial and KBRD use mentioned entities for recommendation, BARCOR utilizes dialogue texts for recommendation, and UniCRS makes use of both entities and texts for recommendation. To verify the generality of our framework, we apply it to KBRD, BARCOR, and UniCRS. To demonstrate the effectiveness of our framework, we compare it with several representative data augmentation methods.</p><p>? EDA <ref type="bibr" target="#b41">[42]</ref>: It augments new examples by randomly performing edit operations, i.e., replacement, insertion, swap, and deletion.</p><p>? Mixup <ref type="bibr" target="#b45">[46]</ref>: It augments new examples in the continuous latent space by linear interpolations of input representations and labels of two random examples.</p><p>Evaluation Metrics. Following existing work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18]</ref>, we adopt different metrics to evaluate the recommendation and conversation tasks separately. For the recommendation task, following <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b53">54]</ref>, we use Recall@?, MRR@?, and NDCG@? (?=10,50). For the conversation task, following <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b54">55]</ref>, we adopt Distinct-? (?=2,3,4) to evaluate the diversity of the generated responses. Besides, following KGSF <ref type="bibr" target="#b51">[52]</ref>, we invite three annotators to score the generated responses from two aspects, namely Fluency and Informativeness. The range of scores is 0 to 2. For all the above metrics, we calculate and report the average scores on all test examples. Implementation Details. We implement all the baseline models based on the open-source toolkit CRSLab <ref type="bibr" target="#b50">[51]</ref> 1 , which contains comprehensive CRS models and benchmark datasets. For the recommendation dialogue simulator, we adopt a Transformer with 12-layer encoders and decoders as the FLM, and its hidden size and embedding size are 768. To be consistent with the FLM, the hidden size of the user prompt and schema prompt is also 768. In curriculum counterfactual learning, the maximum training iterations are Table <ref type="table">2</ref>: Results on the recommendation task. The best methods in each group are marked in bold. Numbers marked with * indicate that the improvement is statistically significant compared with the baseline (t-test with p-value &lt; 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>ReDial INSPIRED Models Recall@10 Recall@50 MRR@10 MRR@50 NDCG@10 NDCG@50 Recall@10 Recall@50 MRR@10 MRR@50 NDCG@10 NDCG@50 set to 20, and we adopt the early stopping strategy. The initial value of the regularization weight and the decay ratio is tuned in the range of [10 -1 , 10 -2 , 10 -3 ] and [0.9, 0.8, 0.7] for different models. We use AdamW <ref type="bibr" target="#b22">[23]</ref> with the default parameter setting to optimize the parameters in our framework. The learning rate is mostly set to 1? -4 and tuned in the range of [5? -5 , 1? -4 , 5? -4 , 1? -3 ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation on Recommendation Task</head><p>In this part, we conduct experiments to evaluate the effectiveness of our model on the recommendation task.</p><p>Automatic Evaluation. Table <ref type="table">2</ref> shows the performance of different methods on the recommendation task. First, we can see that KBRD, BARCOR, and UniCRS mostly outperform the other baselines in all metrics. The three methods all incorporate external KGs to enrich the information of mentioned entities in the conversation context, which can effectively alleviate the data scarcity problem and better capture user intents and preferences. Among the three methods, UniCRS performs the best in all metrics. UniCRS utilizes knowledge-enhanced prompts to guide the PLM, and incorporates a pre-training task to improve the quality of prompts. Such a way can effectively endow the PLM with entity knowledge for better performance.</p><p>Second, for the two data augmentation baselines, we observe that most of the time they both improve the performance of the three CRS methods. It indicates the effectiveness of data augmentation strategies in the CRS task, since the training data is not sufficient. However, we can see that the improvement is not stable, and even causes performance degradation for UniCRS on the ReDial dataset. A possible reason is that the two methods only rely on heuristic rules to modify the original examples for augmenting new ones, which makes it hard to guarantee the quality of the augmented data and may even produce abnormal conversations.</p><p>Finally, we can see that our model can improve the performance of the three CRS methods by a large margin. It indicates the effectiveness and generality of our framework. Furthermore, our approach mostly outperforms the two data augmentation baselines significantly. In our approach, we use a counterfactual data simulation approach, which includes a pre-trained FLM to guarantee the coherence of the conversation flow and adversarial training to enhance the informativeness of simulated data. Besides, we utilize the curriculum learning strategy to gradually optimize CRS models using examples with different augmentation levels, which further improves the stability of the training process.</p><p>Ablation Study. Our approach incorporates several important components to improve the quality of the augmented data. To verify the effectiveness of each component, we conduct the ablation study on BARCOR using the ReDial and INSPIRED datasets. We report the results of Recall@10 and Recall@50. We consider removing the curriculum schedule, the adversarial training objective, the FLM, the frequent flow schemas, and the template-based dialogue realization, respectively. The results are shown in Figure <ref type="figure" target="#fig_1">3</ref>. We can see that removing any component would lead to performance degradation. It indicates that all the components in our model are useful to improve the performance of the recommendation task. Among them, performance decreases the most after removing the template-based dialogue realization. It indicates that the template-based dialogue realization is important in our approach, since it can ensure fluency in language and faithfulness in conversation flow without introducing noise to the simulated data, which is beneficial for the improvement of the recommendation ability of CRSs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation on Conversation Task</head><p>In this part, we conduct experiments to verify the effectiveness of our model on the conversation task.</p><p>Automatic Evaluation. We show the evaluation results of automatic metrics about different methods in Table <ref type="table" target="#tab_3">4</ref>. As we can see, the methods using PLMs (i.e., GPT-2, DialoGPT, BARCOR, and UniCRS) mostly achieve better performance than other methods. Since PLMs have been pre-trained with generative tasks on large-scale corpora, they can quickly adapt to the CRS task and generate diverse responses after fine-tuning. Among these methods, UniCRS mostly achieves the best performance. Since UniCRS is based on DialoGPT, a PLM that has been pre-trained on large-scale dialogue corpora, it is more capable of generating responses. It also performs semantic fusion and prompt pre-training to inject task-specific knowledge into DialoGPT, helping generate more informative responses. Besides, we can see that the two data augmentation methods also improve the performance of the three CRS models. Despite the fact that the improvement is not stable, it can demonstrate that CRS models are hungry for training data.</p><p>Finally, our model also consistently boosts the performance of the three CRS models, and significantly outperforms the two data augmentation methods. It further indicates the effectiveness and Human Evaluation. To provide a more qualified evaluation of the conversation task, we conduct the human evaluation following previous work <ref type="bibr" target="#b51">[52]</ref>. We select KBRD, BARCOR, and UniCRS as the backbone, and implement our approach on them. We invite three annotators to evaluate the fluency and informativeness of the generated responses from examples from these models, and present the results on the ReDial dataset in Table <ref type="table" target="#tab_4">5</ref>. The average Cohen's kappa between any two annotators is 0.89, which indicates good agreement. First, we can also see a similar tendency to the automatic metrics: UniCRS &gt; BARCOR &gt; KBRD. It indicates the effectiveness of UniCRS which incorporates DialoGPT and knowledge-enhanced prompts. Besides, the two data augmentation methods can consistently improve the quality of the generated response, but their performance order is not stable. A possible reason is that they rely on heuristic rules for augmentation without considering the target model, which may produce useless examples for specific CRS models. Although the augmented examples may contain noise, they are still able to alleviate the data-hungry problem of CRS models. Finally, our approach can consistently outperform these baseline models. It further demonstrates the effectiveness of our framework, which can augment more high-quality examples to improve the training of CRS models, helping them generate fluent and informative responses.  some extent. To validate this, we simulate a data scarcity scenario by sampling different proportions of the training data, i.e., 20%, 40%, 60%, 80%, and 100%. We implement our approach on KBRD and report the results of the recommendation and conversation tasks on the ReDial dataset.</p><p>Figure <ref type="figure">2</ref> shows the evaluation results in different data scarcity settings. As we can see, with just 20% training data, our approach can still achieve comparable performance with KBRD that is trained using 100% data. It indicates that our approach can augment highquality conversations, greatly alleviating the data scarcity problem. Besides, with less available training data, the performance of our approach is relatively stable. It also shows the potential of our approach to dealing with the cold-start scenario in real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Hyper-parameters Analysis</head><p>In our framework, there are two major hyper-parameters to tune: the ratios of augmented examples for each instance and the weights of the L2-norm loss ? during the adversarial training. Here, we investigate the effect of each hyper-parameter on our approach. We conduct the analysis experiments on the recommendation and conversation tasks on the ReDial and INSPIRED datasets. We implement our approach on KBRD and report the results for the two hyper-parameters in Figure <ref type="figure" target="#fig_1">3</ref> and Figure <ref type="figure">4</ref>, respectively.</p><p>First, we can see that for the ReDial dataset, the performance is stable when tuning the two hyper-parameters. It indicates that our approach is not too sensitive to the two hyper-parameters on this dataset. Whereas, too large or too small weights of the L2-norm loss ? would cause performance degradation, since too large ? might </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we proposed a counterfactual data simulation approach, named CFCRS, for alleviating the issue of data scarcity in CRSs. We developed our approach under the framework of counterfactual data augmentation, and employed counterfactual learning to enhance the quality of the augmented recommendation dialogue data. Specially, in our approach, we characterized the conversation flow and user preference via the entities mentioned in the conversation. Our approach gradually augmented the user preference from a real dialogue without interfering with the entire conversation flow. Such an augmentation strategy was well learned by an adversarial training method with a curriculum schedule. As a key component, we designed a multi-stage recommendation dialogue simulator based on a conversation flow language model, which can generate reasonable, coherent conversation flows for dialogue realization. Extensive experiments have shown that our approach can consistently boost the performance of several competitive CRSs, and outperform other data augmentation methods. Currently, our approach adopts a multi-stage stimulation method to generate recommendation dialogue data. For future work, we will investigate more unified and simplified approaches for highquality data augmentation, such as the utilization of large language models <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b48">49]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure1: The overview of our approach CFCRS. We first adopt curriculum counterfactual learning to augment the user preference at the representation level, and then use the flow language model guided by user and schema prompts to generate conversation flows, which are then realized into dialogues. The edit function and CRS model are optimized with adversarial training to improve both the quality of the augmented data and the recommendation performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance comparison w.r.t. different ratios of augmented examples on ReDial and Inspired dataset. We implement our approach on KBRD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets after preprocessing.</figDesc><table><row><cell>Dataset</cell><cell cols="3">#Dialogues #Utterances #Items</cell></row><row><cell>INSPIRED</cell><cell>1,001</cell><cell>35,811</cell><cell>1,783</cell></row><row><cell>ReDial</cell><cell>10,006</cell><cell>182,150</cell><cell>51,699</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation analysis on the recommendation task. "FLM" denotes the flow language model and "Template" denotes template-based dialogue realization. "-" denotes removing the corresponding component.</figDesc><table><row><cell>Datasets</cell><cell>ReDial</cell><cell></cell><cell cols="2">INSPIRED</cell></row><row><cell>Metrics</cell><cell cols="4">Recall@10 Recall@50 Recall@10 Recall@50</cell></row><row><cell>BARCOR</cell><cell>0.169</cell><cell>0.374</cell><cell>0.185</cell><cell>0.339</cell></row><row><cell>+CFCRS</cell><cell>0.198</cell><cell>0.406</cell><cell>0.246</cell><cell>0.421</cell></row><row><cell>-Curriculum</cell><cell>0.187</cell><cell>0.399</cell><cell>0.190</cell><cell>0.405</cell></row><row><cell>-Adversarial</cell><cell>0.184</cell><cell>0.389</cell><cell>0.225</cell><cell>0.395</cell></row><row><cell>-FLM</cell><cell>0.181</cell><cell>0.385</cell><cell>0.211</cell><cell>0.390</cell></row><row><cell>-Frequent Schema</cell><cell>0.186</cell><cell>0.394</cell><cell>0.246</cell><cell>0.407</cell></row><row><cell>-Template</cell><cell>0.183</cell><cell>0.382</cell><cell>0.174</cell><cell>0.385</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Automatic evaluation results on the conversation task. We abbreviate Distinct-2,3,4 as Dist-2,3,4. The best methods in each group are marked in bold. Numbers marked with * indicate that the improvement is statistically significant compared with the baseline (t-test with p-value &lt; 0.05).</figDesc><table><row><cell>Datasets</cell><cell></cell><cell>ReDial</cell><cell></cell><cell></cell><cell>INSPIRED</cell><cell></cell></row><row><cell>Models</cell><cell cols="6">Dist-2 Dist-3 Dist-4 Dist-2 Dist-3 Dist-4</cell></row><row><cell>ReDial</cell><cell>0.023</cell><cell>0.236</cell><cell>0.228</cell><cell>0.153</cell><cell>0.255</cell><cell>0.397</cell></row><row><cell>GPT-2</cell><cell>0.354</cell><cell>0.486</cell><cell>0.441</cell><cell>2.347</cell><cell>3.691</cell><cell>4.568</cell></row><row><cell>DialoGPT</cell><cell>0.476</cell><cell>0.559</cell><cell>0.486</cell><cell>2.408</cell><cell>3.720</cell><cell>4.560</cell></row><row><cell>KBRD</cell><cell>0.198</cell><cell>0.339</cell><cell>0.473</cell><cell>0.223</cell><cell>0.415</cell><cell>0.616</cell></row><row><cell>KBRD-EDA</cell><cell>0.323</cell><cell>0.476</cell><cell>0.565</cell><cell>0.466</cell><cell>0.856</cell><cell>1.174</cell></row><row><cell>KBRD-mixup</cell><cell>0.172</cell><cell>0.292</cell><cell>0.449</cell><cell>0.362</cell><cell>0.680</cell><cell>0.987</cell></row><row><cell>KBRD-CFCRS</cell><cell cols="6">0.477* 0.603* 0.728* 0.573* 1.148* 1.645*</cell></row><row><cell>BARCOR</cell><cell>0.404</cell><cell>0.540</cell><cell>0.654</cell><cell>2.923</cell><cell>4.172</cell><cell>4.992</cell></row><row><cell>BARCOR-EDA</cell><cell>0.522</cell><cell>0.698</cell><cell>0.717</cell><cell>3.597</cell><cell>5.108</cell><cell>5.959</cell></row><row><cell>BARCOR-mixup</cell><cell>0.568</cell><cell>0.704</cell><cell>0.740</cell><cell>3.856</cell><cell>5.582</cell><cell>6.576</cell></row><row><cell cols="7">BARCOR-CFCRS 0.701* 0.971* 0.969* 4.081* 5.953* 6.979*</cell></row><row><cell>UniCRS</cell><cell>0.351</cell><cell>0.631</cell><cell>0.897</cell><cell>2.809</cell><cell>4.530</cell><cell>5.555</cell></row><row><cell>UniCRS-EDA</cell><cell>0.440</cell><cell>0.801</cell><cell>1.141</cell><cell>2.882</cell><cell>4.859</cell><cell>6.166</cell></row><row><cell>UniCRS-mixup</cell><cell>0.412</cell><cell>0.701</cell><cell>0.918</cell><cell>2.517</cell><cell>4.173</cell><cell>5.496</cell></row><row><cell>UniCRS-CFCRS</cell><cell cols="6">0.632* 1.195* 1.524* 4.225* 6.824* 8.155*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Human evaluation results about the conversation task on the ReDial dataset. framework among different CRS methods. Besides, we can see that with the help of our approach, the model KBRD can even outperform PLM-based methods on the ReDial dataset. It shows that our proposed data augmentation approach suits well with KBRD, and can inspire its potential to generate high-quality responses.</figDesc><table><row><cell>Models</cell><cell cols="2">Fluency Informativeness</cell></row><row><cell>KBRD</cell><cell>0.91</cell><cell>0.86</cell></row><row><cell>KBRD-EDA</cell><cell>1.12</cell><cell>1.04</cell></row><row><cell>KBRD-mixup</cell><cell>1.05</cell><cell>0.93</cell></row><row><cell>KBRD-CFCRS</cell><cell>1.27</cell><cell>1.09</cell></row><row><cell>BARCOR</cell><cell>1.23</cell><cell>1.14</cell></row><row><cell>BARCOR-EDA</cell><cell>1.29</cell><cell>1.22</cell></row><row><cell>BARCOR-mixup</cell><cell>1.36</cell><cell>1.30</cell></row><row><cell>BARCOR-CFCRS</cell><cell>1.47</cell><cell>1.38</cell></row><row><cell>UniCRS</cell><cell>1.41</cell><cell>1.33</cell></row><row><cell>UniCRS-EDA</cell><cell>1.57</cell><cell>1.49</cell></row><row><cell>UniCRS-mixup</cell><cell>1.48</cell><cell>1.41</cell></row><row><cell>UniCRS-CFCRS</cell><cell>1.69</cell><cell>1.60</cell></row><row><cell>generality of our</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Redial Inspired Figure 4: Performance comparison w.r.t. different weights of the L2-norm loss ? on ReDial and Inspired dataset. We implement our approach on KBRD. punish the noise vectors too much, while too small one may bring too much noise. Second, for the INSPIRED dataset, we can see the performance is not stable. A possible reason is that INSPIRED owns very few conversations in the training set, which may hurt the robustness of CRS models. Besides, on the INSPIRED dataset, the best points of the two hyper-parameters in the recommendation and conversation tasks are different. The reason may be that the two tasks focus on different goals, which may lead to conflict in their best hyper-parameter settings.</figDesc><table><row><cell>0.49</cell><cell cols="2">Redial Recall@50</cell><cell>Inspired</cell><cell>1.54</cell><cell>Dist-4</cell><cell></cell><cell></cell></row><row><cell>0.47</cell><cell></cell><cell></cell><cell></cell><cell>1.49</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.45</cell><cell></cell><cell></cell><cell></cell><cell>1.44</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.43</cell><cell></cell><cell></cell><cell></cell><cell>1.39</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.0001 0.001 0.41</cell><cell>0.01</cell><cell>0.1</cell><cell>0</cell><cell>0.0001 0.001 1.34</cell><cell>0.01</cell><cell>0.1</cell><cell>0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/RUCAIBox/CRSLab</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We are thankful to <rs type="person">Jinhao Jiang</rs> for his supportive work. This work was partially supported by <rs type="funder">National Natural Science Foundation of China</rs> under Grant No. <rs type="grantNumber">62222215</rs>, <rs type="funder">Beijing Natural Science Foundation</rs> under Grant No. <rs type="grantNumber">4222027</rs>, <rs type="programName">Beijing Outstanding Young Scientist Program</rs> under Grant No. <rs type="grantNumber">BJJWZYJH012019100020098</rs>, the <rs type="funder">Outstanding Innovative Talents Cultivation Funded Programs 2022 of Renmin University of China</rs>. <rs type="person">Xin Zhao</rs> is the corresponding author.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_SYjgdT6">
					<idno type="grant-number">62222215</idno>
				</org>
				<org type="funding" xml:id="_gtfUwNx">
					<idno type="grant-number">4222027</idno>
					<orgName type="program" subtype="full">Beijing Outstanding Young Scientist Program</orgName>
				</org>
				<org type="funding" xml:id="_bqkUebR">
					<idno type="grant-number">BJJWZYJH012019100020098</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey on dialogue systems: Recent advances and new frontiers</title>
		<author>
			<persName><forename type="first">Hongshen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Sigkdd Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="25" to="35" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards Knowledge-Based Recommender Dialog System</title>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1803" to="1813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards conversational recommender systems</title>
		<author>
			<persName><forename type="first">Konstantina</forename><surname>Christakopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="815" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Varun</forename><surname>Steven Y Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarath</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soroush</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teruko</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Mitamura</surname></persName>
		</author>
		<author>
			<persName><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03075</idno>
		<title level="m">A survey of data augmentation approaches for NLP</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Xiangnan He, Maarten de Rijke, and Tat-Seng Chua. 2021. Advances and challenges in conversational recommender systems: A survey</title>
		<author>
			<persName><forename type="first">Chongming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="100" to="126" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural approaches to conversational AI</title>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1371" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Counterfactual visual explanations</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2376" to="2384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Frequent pattern mining: current status and future directions</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data mining and knowledge discovery</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="55" to="86" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">INSPIRED: Toward Sociable Recommendation Dialog Systems</title>
		<author>
			<persName><forename type="first">Dongyeop</forename><surname>Shirley Anugrah Hayati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingxiaoyang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8142" to="8152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th international conference on world wide web</title>
		<meeting>the 26th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Session-based Recommendations with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Bal?zs</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linas</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domonkos</forename><surname>Tikk</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.06939" />
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<editor>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02">2016. 2016. May 2-4, 2016</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sequence-to-Sequence Data Augmentation for Dialogue Language Understanding</title>
		<author>
			<persName><forename type="first">Yutai</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1234" to="1245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey on conversational recommender systems</title>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahtsham</forename><surname>Manzoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanling</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-attentive sequential recommendation</title>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on data mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Submodular optimization-based diverse paraphrasing and its effectiveness in data augmentation</title>
		<author>
			<persName><forename type="first">Ashutosh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Bhattamishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manik</forename><surname>Bhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3609" to="3619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Estimation-action-reflection: Towards deep interaction between conversational and recommender systems</title>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards deep conversational recommendations</title>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannes</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>Laurent Charlin, and Chris Pal</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Xiang Ao, Fuzhen Zhuang, and Qing He. 2022. User-centric conversational recommendation with multi-aspect user modeling</title>
		<author>
			<persName><forename type="first">Shuokai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongchun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<biblScope unit="page" from="223" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Counterfactual data augmentation for neural machine translation</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Augmenting sequential recommendation with pseudo-prior items via reversely pre-training transformer</title>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 44th international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1608" to="1612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards Conversational Recommendation over Multi-Type Dialogs</title>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Yu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1036" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">RevCore: Review-Augmented Conversational Recommendation</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuguang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youzheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL Findings</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1161" to="1173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bias Challenges in Counterfactual Data Augmentation</title>
		<author>
			<persName><forename type="first">Yangze</forename><surname>Chandra Mouli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI 2022 Workshop on Causal Representation Learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.16838</idno>
		<title level="m">Counterfactual Data Augmentation via Perspective Transition for Open-Domain Dialogues</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Counterfactual data augmentation using locally factored dynamics</title>
		<author>
			<persName><forename type="first">Silviu</forename><surname>Pitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3976" to="3990" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Memory augmented multiinstance contrastive predictive coding for sequential recommendation</title>
		<author>
			<persName><forename type="first">Ruihong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Maarten de Rijke, and Zhumin Chen. 2022. Variational Reasoning about User Preferences for Conversational Recommendation</title>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huasheng</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantically Equivalent Adversarial Rules for Debugging NLP models</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>T?lio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1079</idno>
		<ptr target="https://doi.org/10.18653/v1/P18-1079" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<title level="s">Long Papers</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-15">2018. July 15-20, 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="856" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European semantic web conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Chenzhan</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.04798</idno>
		<title level="m">Multi-grained Hypergraph Interest Modeling for Conversational Recommendation</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName><forename type="first">Connor</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taghi</surname></persName>
		</author>
		<author>
			<persName><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of big data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning</title>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">RecInDial: A Unified Framework for Conversational Recommendation with Pretrained Language Models</title>
		<author>
			<persName><forename type="first">Lingzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kam-Fai</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="489" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><surname>Ting-Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Nung</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.14257</idno>
		<title level="m">BARCOR: Towards A Unified Framework for Conversational Recommendation Systems</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models</title>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.13112</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.09363</idno>
		<title level="m">Towards Unified Conversational Recommender Systems via Knowledge-Enhanced Prompt Learning</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust Machine Comprehension Models via Adversarial Training</title>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-2091</idno>
		<ptr target="https://doi.org/10.18653/v1/n18-2091" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<title level="s">Short Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01">2018. June 1-6, 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="575" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Counterfactual data-augmented sequential recommendation</title>
		<author>
			<persName><forename type="first">Zhenlei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingsen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongteng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval</title>
		<meeting>the 44th international ACM SIGIR conference on research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks</title>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1670</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1670" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="6381" to="6387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Qingsong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaomin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingkun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12478</idno>
		<title level="m">Time series data augmentation for deep learning: A survey</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Williams</forename><surname>Ronald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992. 1992</date>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
	<note>Reinforcement learning</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised Data Augmentation for Consistency Training</title>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/44" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020</title>
		<imprint>
			<date type="published" when="2020-12-06">2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
	<note>feb0096faa8326192570788b38c1d1-Abstract.html</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhumin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.11868</idno>
		<title level="m">Variational Reasoning over Incomplete Knowledge Graphs for Conversational Recommendation</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">DIALOGPT: Large-Scale Generative Pre-training for Conversational Response Generation</title>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="270" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wayne Xin Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yupeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zican</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.18223</idno>
		<title level="m">A survey of large language models</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">CR-GIS: Improving Conversational Recommendation via Goal-aware Interest Sequence Modeling</title>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexian</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="400" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">CRSLab: An Open-Source Toolkit for Building Conversational Recommender System</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenzhan</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL: System Demonstrations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="185" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improving conversational recommender systems via knowledge graph based semantic fusion</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuqing</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingsong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1006" to="1014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Leveraging historical interaction data for improving conversational recommender system</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2349" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Towards Topic-Guided Conversational Recommender System</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4128" to="4139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Peng Jiang, and He Hu. 2022. C 2 -CRS: Coarse-to-Fine Contrastive Learning for Conversational Recommender System</title>
		<author>
			<persName><forename type="first">Yuanhang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM 2022</title>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="1488" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>Zmigrod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabrina</forename><forename type="middle">J</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1651" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Improving Conversational Recommender Systems via Transformerbased Sequential Modelling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2319" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
