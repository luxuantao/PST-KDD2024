<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Off-Line Recognition of Totally Unconstrained Handwritten Numerals Using Multilayer Cluster Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
							<email>swlee@huinan.korea.ac.kr.</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<addrLine>1,5-ka, Anam-dong, Seongbulc-ku</addrLine>
									<postCode>136-701</postCode>
									<settlement>Seoul, Ko-rea</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<addrLine>1,5-ka, Anam-dong, Seongbulc-ku</addrLine>
									<postCode>136-701</postCode>
									<settlement>Seoul, Ko-rea</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Off-Line Recognition of Totally Unconstrained Handwritten Numerals Using Multilayer Cluster Neural Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C7AE68F135AF79DAA3BC0C51CE69FB2D</idno>
					<note type="submission">received June 13,1994; revised Aug. 10,1995. Recommended for acceptance by M . Mohiuddin.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Totally unconstrained handwritten numeral recognition</term>
					<term>multilayer cluster neural network</term>
					<term>genetic algorithm</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a new scheme for off-line recognition of totally unconstrained handwritten numerals using a simple multilayer cluster neural network trained with the back propagation algorithm and show that the use of genetic algorithms avoids the problem of finding local minima in training the multilayer cluster neural network with gradient descent technique, and improves the recognition rates. In the proposed scheme, Kirsch masks are adopted for extracting feature vectors and a three-layer cluster neural network with five independent subnetworks is developed for classifying similar numerals efficiently. In order to verify the performance of the proposed multilayer cluster neural network, experiments with handwritten numeral database of Concordia University of Canada, that of Electro-Technical Laboratory of Japan, and that of Electronics and Telecommunications Research Institute of Korea were performed. For the case of determining the initial weights using a genetic algorithm, 97.1 O%, 99.12%, and 99.40% correct recognition rates were obtained, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>HANDWRITTEN character recognition systems have been proposed and implemented in a number of different ways. However, for the recognition of totally unconstrained handwritten numerals, it is probably safe to say that no simple scheme is likely to achieve high recognition and reliability rates. Thus recent efforts have been directed toward more sophisticated systems 111, <ref type="bibr">[21.</ref> In order to maximize the performance of unconstrained handwritten numeral recognition, the following two approaches could be considered; one is to design a feature extractor which does not miss important features while minimizing the number of meaningless pixels, and the other is to design a classifier which has good generalization power and minimum substitution error.</p><p>Our work has been motivated by these two approaches. The proposed scheme consists of two stages: a feature extraction stage for extracting four-directional local feature vectors with Kirsch masks and one global feature vector linearly compressed from a normalized input image, and a classification stage for recognizing numerals with a simple multilayer cluster neural network trained with the back propagation algorithm. In the case of using a neural network trained with the back propagation algorithm [3] as a classifier, initial weights of the neural network may cause the problem of finding local minima in training with the gradient descent technique because gradient descent techniques proceed by locally searching the immediate neighborhood of a current solution. However, the genetic algorithm has excellent global sampling abilities because it ensures broad coverage over the entire search domain. This suggests that using the genetic algorithm to provide good "seeds" from which the back propagation algorithm then continues to search will be effective <ref type="bibr">[4]</ref>. Therefore, we combined a genetic algorithm with the back propagation algorithm to avoid the problem of finding local minima in training with the gradient descent technique.</p><p>In order to verify the performance of the proposed multilayer cluster neural network, experiments with handwritten numeral database of Concordia University of Canada, that of Electro-Technical Laboratory(ETL1 of Japan, and that of Electronics and Telecommunications Research Institute(ETR1) of Korea were performed. For the case of determining the initial weights using a genetic algorithm, 97.10%, 99.12%, and 99.40% correct recognition rates were obtained, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">REVIEW OF EARLIER WORK</head><p>In this section rve review some methods for the recognition of totally unconstrained handwritten numerals and a multiple-expert system which incorporates different unconstrained handwritten numeral recognition methods. Then, we also review some representative recognition systems which have been implemented to classify unconstrained handwritten numerals with neural networks.</p><p>In the method of <ref type="bibr">Mai and Suen [6]</ref>, statistical data on the frequency of occurrence of features during training are stored in a database. The database is used to deduce the identification of an unknown sample.</p><p>To achieve even higher reliability by taking advantage of the inherent complementarity of the different unconstrained handwritten numeral recognition methods, <ref type="bibr">Suen et al.</ref> [SI incorporated these methods into a multiple-expert system.</p><p>Very good results were recently reported using neural networks. In the method of <ref type="bibr">Krzyzak et al. 191</ref>, features are first extracted from the contours of numerals: 15 complex Fourier descriptors from the outer contours and simple topological features from the inner contours. These features are then presented as input to a three-layer modified back propagation network. The modification aims at avoiding the problem of finding local minima in the gradient descent technique used to adjust the weights and results in a convergence rate which is twice as fast. Le Cun et al.</p><p>[lo] achieved excellent results with a back propagation network using size-normalized images as direct input. Their solution consists of a network architecture which is highly constrained and specifically designed for the task. There are four internal layers, two layers made of independent groups of feature extractors and two layers which perform averaging/sub-sampling. The last internal layer is fully connected to the ten-element output, but all other connections are local and use shared weights. In total, there are 4,635 units and 98,442 connections but only 2,578 independent parameters.</p><p>Recently, <ref type="bibr">Knerr et al. [ll]</ref> reported that a neural network classifier with single-layer training could be applied efficiently to the recognition of handwritten numerals. In this method, the STEPNET procedure decomposes the problem into simpler subproblems which can be solved by linear separators.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows the performance of some of the most reliable handwritten numeral recognition systems found in the literature.</p><p>In the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FEATURE EXTRACTION</head><p>In the case of handwritten numerals, it seems natural to use some a priori knowledge about the recognition task in order to transform the low level information of the pixel images into a data representation of higher level. Two possibilities arise: either a priori knowledge is used to design preprocessing operations which are carried out explicitly before classification, or it is used to constrain the general architecture of the classifier, which results in a classifier specialized in the recognition task at hand. We have chosen the first possibility. Numerals, whether handwritten or typed, are essentially line drawings, i.e., one-dimensional structures in a twodimensional space. Therefore, local detection of line segments seems to be an adequate feature extraction method. For each location in the image, information about the presence of a line segment of a given direction is stored in a feature map. However, besides mere a priori knowledge, there are other factors which may influence the performance of a recognition system, such as the need for fast computation or hardware limitations. The first-order differential edge detectors are adequate for local detection of a line segment and fast computation. The Frei-Chen edge detector, Kirsch edge detector, Prewitt edge detector, Sobel edge detector, and so on are the representative edge detectors of first differential edge detectors. However, among these edge detectors, the Kirsch edge detector has been known to detect fourdirectional edges more accurately than other edge detectors because the Kirsch edge detector considers all eight neighbors.</p><p>Kirsch defined a nonlinear edge enhancement algorithm as follows <ref type="bibr">[221:</ref> where</p><formula xml:id="formula_0">S k = Ak + Ak+l + Ak+21</formula><p>( 3 )</p><p>and</p><formula xml:id="formula_1">Tk = Ak+3 + Ak+4 + Ak+5 + Ak+6 + Ak+7 (4)</formula><p>In (2), G(i, j ) is the gradient of pixel (i, j), the subscripts of A are evaluated modulo 8, and Ai, (k = 0, 1, ..., 7) is eight neighbors of pixel (i, j ) defined as shown in Fig. <ref type="figure">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(i, j&gt; A3</head><p>F i g 1. Definition of eight neighbors of pixel (i, 1).</p><p>In this paper, we calculate directional feature vectors for horizointal(H), vertical(V), right-diagonal(R), and left-diagonal(L) directions as follows:</p><formula xml:id="formula_2">G(i, j)" = max( I 5S2 -3T2 1, I 5.5, -3T6 I ), G(i, j ) x = max( i 5S1 -3T, I, I 5S5 -3T5 I ), G(i, j ) L = max( I 5S3 -3T3 I, I 5S7 -3T7 I 1.</formula><p>(5) As a final feature extracting step, each 16 x 16 feature map is coinpressed to 4 x 4 feature map by accumulating pixels of each 4 x 4 subregion. Fig. <ref type="figure" target="#fig_0">2</ref> shows the Kirsch masks used for calculating directional feature vectors. In the case of using only these local features in totally unconstrained handwritten numeral recognition, global characteristics of input images may not be considered. Therefore, in this paper, we compressed the 16 x 16 normalized input image into 4 x 4 image and used this compressed image as a global feature. As a result, final feature vectors consist of 5 x 4 x 4 feature vectors; 4 x 4 x 4 local feature vectors and 1 ~4 x 4 global feature vector. Fig. <ref type="figure" target="#fig_1">3</ref> shows the overview of the proposed feature extraction step.</p><formula xml:id="formula_3">5 1 5 1 5 I -3 0 -3 3 4 -3 -3 -3 -3 -3 -3 -3 0 -3 M -3 -3 5 -3 -3 5 p J 5 -3 -3 5 -3 -3 ~[ (c) (4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MULTILAYER CLUSTER NEURAL NETWORK CLASSIFIER</head><p>A popular objective function in neural networks is the mean squared error(MSE) between the actual output of the network and a desired output. Minimizing the objective function is performed by a gradient descent procedure which requires the computation of the gradient of the objective function with respect to connection weights. Back propagation [3] is just an efficient way to compute this gradient. However, a naive back propagation network for identifying handwritten numerals is a large, fully-connected network, where all the urds in a layer are connected to all the units in the following layeds). This approach has severe deficiencies.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Network Architecture</head><p>The cluster neural network is that the units in a layer are clustered and each cluster is fully connected to a corresponding cluster in following layer, independently. The network architecture is presented in Fig. <ref type="figure" target="#fig_2">4</ref>. The input layer consists of five 4 x 4 maps because the network inputs are 4 ~4 x 4 local feature maps for fourdirections and 1 x 4 x 4 global feature map. Each feature map is fully connected to a corresponding cluster in the hidden layer, independently. However, the output layer is fully connected to all hidden units. The output is composed of 10 units: one per class.</p><p>When a pattern belonging to class i is presented, the desired output is +1 for the ith output, and zero for the other output units. Therefore, the three-layer cluster neural network has 170 units and 2,080 independent parameters. Besides the simplicity, one intuitive advantage of our threelayer cluster network is that while one subnetwork may be confused by a given input, others may not because each subnetwork has started from a different initial state and learned with different feature maps. One subnetwork, for example, may not be able to correctly classify specific sets of numerals because its weight pattern conflicts with those necessary to classify other numerals. This is to be expected since the totally unconstrained handwritten numerals are clearly not orthogonal. In other words, the sequence and types of strokes needed to form, for example, the digit '8' is very close to those required to form the digit '9.' Thus, it is more likely for a network to erroneously classify a ' 9' as an '8' than as a '5.'</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning</head><p>Each subnetwork between the input layer and the hidden layer starts with random initial weights and learns with different feature maps, respectively. All of the connections in the network are adaptive, and are trained with back propagation algorithm. The transfer function at each unit is the familiar sigmoid function as follows:</p><formula xml:id="formula_4">1 O = -( z w X + b i a s ) l + e</formula><p>where, 0 is the output of nonlinear transfer function, w is weight, and X is input to the unit. w and bias are randomly selected in start state and modified during training. The modification of weights is performed as follows: <ref type="bibr">(7)</ref> where, M is the training index, a is momentum, and E is MSE. After learning, each subnetwork will generally have different internal representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Determination of Optimized Initial Weights</head><p>Using Genetic Algorithm In order to determine the optimized initial weights of the multilayer cluster neural network, we combined a genetic algorithm with a back propagation algorithm. The genetic algorithm is an effective sampling technique because it ensures broad coverage over the entire search domain via stochastic search <ref type="bibr">[4]</ref>. The genetic algorithm works by collecting information from the early and virtually uniform sampling of early generations, and then using this information to guide subsequent sampling towards particularly promising regions. The selection of a new element of the domain to evaluate therefore exploits global information from across the domain. Therefore, combining the genetic algorithm's global sampling with back propagation's local searching seems a extremely natural.</p><p>Genetic algorithms are iterative procedures which maintain a "population" of candidate solutions to the objective function f(x):</p><formula xml:id="formula_5">P(f) = &lt;x*(t), *,(t), ..., XN(f)&gt; A w(n) = -€aE/aza + ~A W ( M -1)<label>(8)</label></formula><p>where, each x, in population P is a string of length L. In this paper, each xI represents a vector of initial weights of the multilayer cluster neural network. During each iteration step, called a "generation," the current population is evaluated, and, on the basis of that evaluation, a new population of candidate solutions is formed. A general sketch of the procedure appears in Fig. <ref type="figure" target="#fig_3">5</ref>. The initial population P(0) is usually chosen at random.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS AND ANALYSIS</head><p>In this paper, we used three different databases; handwritten numeral database of Concordia University of Canada, that of ETL of Japan, and that of ETRI of Korea.</p><p>Average 0% 75 7</p><p>100 200 300 400 500 600 700 800 900 1000 Epoch</p><p>Fig. <ref type="figure">6</ref> Recognition rates per epochs using a fully connected multilayer neural network and a cluster neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.9%</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>97.1%</head><p>Four-thousand numerals were used for training and 2,000 numerals were used for testing from the numeral database of Concordia University of Canada, 6,000 numerals were used for training and 4,000 numerals were used for testing from the ETL-I databases, and 4,000 numerals were used for training and 2,000 numerals were used for testing from the numeral database of Electronics and Telecommunications Research Institute, respectively.</p><p>The genetic algorithm has been used to create a population of initial weight vectors for the multilayer cluster neural network and then the back propagation algorithm has been used to optimize each of these. The MSE of each result was used for evaluating the initial weight vectors. Basic parameters of the genetic algorithm are presented in Table <ref type="table">2</ref>.  Fig. <ref type="figure">7</ref> shows the examples from the handwritten numeral database which were misclassified. Table <ref type="table">4</ref> shows the confusion matrix for the method BP+GA using the handwritten numeral database of Concordia University. Fig. <ref type="figure" target="#fig_4">8</ref> shows atypical data 1101. The proposed multilayer cluster neural network classifies these correctly, even though they are quite unlike anything in the training set.</p><p>The proposed scheme has been also implemented on wavefront array processor architecture. For the detailed description of the parallel hardware implementation, refer to 1231. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUDING REMARKS</head><p>In this paper, we proposed a new scheme for off-line recognition of totally unconstrained handwritten numerals using a simple multilayer cluster neural network trained with the back propagation algorithm.</p><p>The proposed multilayer cluster neural network has five independent subnetworks between the input layer and the hidden layer. Each subnetwork starts with the optimized initial weights determined by using genetic algorithm combined with the back propagation algorithm and learns with different feature maps usins the back urouagation algorithm. resuectivelv. After learning. each subnetwork has different internal representations. Consequently, the proposed multilayer cluster neural network could classify similar numerals efficiently.</p><p>In this paper, we used a simple multilayer cluster neural network which has 10 output units: one per class. However, considering multiple models for the class which has wide variations, it is expected that the performance of proposed scheme will be improved.</p><p>Further investigation should be made, however, to design a locally constrained cluster network architecture which has good generalization and involves multiple models and to develop a technique in which segmentation and recognition are integrated for the recognition of unconstrained handwritten, connected numerals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CKNOWLEDGMENT</head><p>The author wishes to thank the anonymous reviewers for their helpful comments in improving the earlier draft of this paper. This work was supported by the Directed Basic Research Fund of Korea Science and Engineering Foundation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Kirsch masks, (a) horizontal, (b) vertical, (c) right-diagonal, (d) left-diagonal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Overview of feature extraction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Proposed simple cluster neural network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Genetic algorithm combined with a back propagation algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 7. Examples of misclassified numerals</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 COMPILATION OF SOME OF THE BEST RESULTS FOUND IN LITERATURE Methods Recognized Substituted Rejected Reliability Training Testing pp1 ~ [5]"</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>93 10</cell><cell>295</cell><cell>395</cell><cell>9698</cell><cell>4000</cell><cell>2000</cell><cell>166</cell></row><row><cell>[6]'</cell><cell>92 95</cell><cell>215</cell><cell>490</cell><cell>9774</cell><cell>4000</cell><cell>2000</cell><cell>166</cell></row><row><cell>[7]"</cell><cell>9390</cell><cell>160</cell><cell>450</cell><cell>9832</cell><cell>4000</cell><cell>2000</cell><cell>166</cell></row><row><cell>[SI"</cell><cell>93 05</cell><cell>0 0 0</cell><cell>695</cell><cell>10000</cell><cell>4000</cell><cell>2000</cell><cell>166</cell></row><row><cell>[9]"</cell><cell>8640</cell><cell>100</cell><cell>1260</cell><cell>9885</cell><cell>4000</cell><cell>2000</cell><cell>166</cell></row><row><cell>[9]*</cell><cell>94 85</cell><cell>5 15</cell><cell>000</cell><cell>9485</cell><cell>4000</cell><cell>2000</cell><cell>166</cell></row><row><cell>[IO]</cell><cell>9000</cell><cell>100</cell><cell>900</cell><cell>9890</cell><cell>7291</cell><cell>2007</cell><cell>300</cell></row><row><cell>[IO]</cell><cell>9200</cell><cell>2 0 0</cell><cell>6 0 0</cell><cell>97 80</cell><cell>7291</cell><cell>2007</cell><cell>300</cell></row><row><cell>[Ill</cell><cell>9030</cell><cell>100</cell><cell>8 70</cell><cell>98 90</cell><cell>7200</cell><cell>1800</cell><cell></cell></row><row><cell>[I21</cell><cell>87 85</cell><cell>490</cell><cell>725</cell><cell>9472</cell><cell>5000</cell><cell>3540</cell><cell>166</cell></row><row><cell>[I31</cell><cell>9087</cell><cell>292</cell><cell>621</cell><cell>9689</cell><cell>15000</cell><cell>10000</cell><cell></cell></row><row><cell>[I41</cell><cell>9554</cell><cell>199</cell><cell>247</cell><cell>9796</cell><cell></cell><cell>2711</cell><cell>300</cell></row><row><cell>[I41</cell><cell>97 10</cell><cell>0 96</cell><cell>I 9 4</cell><cell>99 02</cell><cell></cell><cell cols="2">1762 300</cell></row><row><cell>[151</cell><cell>9950</cell><cell>050</cell><cell>000</cell><cell>9950</cell><cell>5000</cell><cell>5000</cell><cell></cell></row><row><cell>[161</cell><cell>9635</cell><cell>100</cell><cell>265</cell><cell>9897</cell><cell></cell><cell>6000</cell><cell>166</cell></row><row><cell>[I61</cell><cell>9820</cell><cell>077</cell><cell>1 0 3</cell><cell>9923</cell><cell></cell><cell>2219</cell><cell>300</cell></row><row><cell>[17]</cell><cell>9330</cell><cell>250</cell><cell>420</cell><cell>9739</cell><cell>1820</cell><cell>7100</cell><cell>300</cell></row><row><cell>[18]</cell><cell>9797</cell><cell>203</cell><cell>0 0</cell><cell>9797</cell><cell>8783</cell><cell>7394</cell><cell></cell></row><row><cell>[I91</cell><cell>8795</cell><cell>104</cell><cell>1101</cell><cell>9883</cell><cell></cell><cell>2103</cell><cell></cell></row><row><cell>[20]</cell><cell>9260</cell><cell>460</cell><cell>2 80</cell><cell>95 30</cell><cell>19377</cell><cell>19377</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 ,</head><label>1</label><figDesc>'*' denotes the case in which the handwritten numeral database of Concordia University of Canada has been used. The fifth column of the table is the reliability of the algorithm, which is computed as shown in the following equation:</figDesc><table><row><cell>Reliability</cell><cell>Correct Recognition Rate Correct Recognition Rate + Substitution Error Rate</cell></row></table><note><p><p><p>=</p>(1) For further information on earlier work, refer to</p>[11 and [211.    </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>If the number of weights is kept reasonably small, the network cannot even learn the training set accurately. On the other hand, if it is made big enough to learn the training set, the excessive number of parameters causes overfitting[lo]. In this paper, we propose a simple three-layer cluster neural network with five independent subnetworks</figDesc><table><row><cell cols="2">Directional ieeature</cell><cell></cell><cell></cell></row><row><cell></cell><cell>extmction with Kirsch masks</cell><cell>+ Horizontal</cell><cell>Vertical</cell><cell>+</cell></row><row><cell>16x16 Normalvation</cell><cell></cell><cell></cell><cell></cell><cell>4x4r4 local feaiure vectors</cell></row><row><cell></cell><cell></cell><cell>Right-diagonal</cell><cell>Left-dragonal</cell></row><row><cell>Input image</cell><cell>Normalized</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1magc</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Compression</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 RESULTS</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>ON THE TESTING SE1</cell></row><row><cell cols="2">1 1 BP+GA 1 1 97.10% I 99.12% I 99.40% 1 1</cell></row><row><cell>~~</cell><cell>~</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Shape Analysis Model with Applications to a Character Recognition System</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nadal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Legault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Rocha</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Pavlidis</surname></persName>
		</editor>
		<meeting>IEEE<address><addrLine>Cambridge, Mass</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">July 1992. Apr. 1994. 1986</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="393" to="404" />
		</imprint>
	</monogr>
	<note>Parallel Distributed Processing</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evolving Networks: Using the Genetic Algorithm with Connectionist Learning</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Belew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mclnerney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Life</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Laugton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Farmer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Rasmussen</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="511" to="547" />
			<date type="published" when="1991">1991</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Structural Classification and Relaxation Matching of Totally Unconstrained Handwritten ZIP-Code Numbers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="31" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Generalized Knowledge-Based System for the Recognition of Unconstrained Handwritten Numerals</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="835" to="848" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Contour Tracing and Parametric Approximations for Digitized Patterns</title>
		<author>
			<persName><forename type="first">R</forename><surname>Legault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Shape Recognition</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Krzyzak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Kasvand</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</editor>
		<imprint>
			<publisher>World Scientific Publishing</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="225" to="240" />
		</imprint>
	</monogr>
	<note>Singapore</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognition of Handwritten Numerals Based on the Concept of Multiple Experts</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nadal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Legault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of First Intl. Workshop on Frontiers in Handwriting Recognition</title>
		<meeting>of First Intl. Workshop on Frontiers in Handwriting Recognition<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-04">Apr. 1990</date>
			<biblScope unit="page" from="131" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unconstrained Handwritten Character Classification Using Modified Backpropagation Model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krzyzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of First Int. Workshop Frontiers in Handwriting Recognition</title>
		<meeting>of First Int. Workshop Frontiers in Handwriting Recognition<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-04">Apr. 1990</date>
			<biblScope unit="page" from="155" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Constrained Neural Network for Unconstrained Handwritten Digit Recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of First Int. Workshop on Frontiers in Handwriting Recognition</title>
		<meeting>of First Int. Workshop on Frontiers in Handwriting Recognition<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-04">Apr. 1990</date>
			<biblScope unit="page" from="145" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Handwritten Digit Recognition by Neural Networks with Single-Layer Training</title>
		<author>
			<persName><forename type="first">S</forename><surname>Knerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Personnaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dreyfus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l. 1. Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="1987">Nov. 1992. 1987</date>
		</imprint>
	</monogr>
	<note>IEEE Trans. Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding Handwritten Text in A Structured Environment: Determining ZIP Codes from Addresses</title>
		<author>
			<persName><forename type="first">M</forename><surname>Beun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari ; Duerr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Haettich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tropf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Combination of Statistical and Syntactical Pattern Recognition Applied to Classification of Unconstrained Handwritten Numerals</title>
		<imprint>
			<date type="published" when="1973">1991. 1973. 1980</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="189" to="199" />
		</imprint>
	</monogr>
	<note>Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pipelined Systems for Recognition of Handwritten Digit in USPS ZIP Codes</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Gadrr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hepp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Forester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Peurach</surname></persName>
		</author>
		<author>
			<persName><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. U.S. Postal Service Advanced Technology Conf</title>
		<meeting>U.S. Postal Service Advanced Technology Conf</meeting>
		<imprint>
			<date type="published" when="1990-11">Nov. 1990</date>
			<biblScope unit="page" from="539" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Stroke-Based Approach to Handwritten Numeral Recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Kuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PYOC. US. Postal Service Advanced Technology Conf</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="1033" to="1041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Practical Implementation of a Radial Basis Function Network for Handwritten Digit Recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lemarie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Gillies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Second Int. Conf. on Document Analysis and Recognition</title>
		<meeting>Second Int. Conf. on Document Analysis and Recognition<address><addrLine>Tsukuba, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1191">1191. 1989</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="243" />
		</imprint>
	</monogr>
	<note>Machine Vision and Applications</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A New Set of Constraint-Free Character Recognition Grammars</title>
		<author>
			<persName><forename type="first">L</forename><surname>Stringa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1210" to="1217" />
			<date type="published" when="1990-12">Dec. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Handprinted Digit Recognition: A Comparison of Algorithms</title>
		<author>
			<persName><forename type="first">D.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Third Intl. Workshop Frontiers in Handwriting Recognition</title>
		<meeting>Third Intl. Workshop Frontiers in Handwriting Recognition<address><addrLine>Buffalo, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-05">May 1993</date>
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Pratt</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<meeting><address><addrLine>New York Wiley</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parallel Hardware Implementation of Handwritten Character Recognition System on Wavefront Array Processor Architecture</title>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Third Intl. Conf. Document Analysis and Recognition</title>
		<meeting>Third Intl. Conf. Document Analysis and Recognition<address><addrLine>Montreal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-10">Aug. 1995. Oct. 1993</date>
			<biblScope unit="page" from="412" to="415" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
