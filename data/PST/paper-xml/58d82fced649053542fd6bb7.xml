<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A generalized power iteration method for solving quadratic problem on the Stiefel manifold</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-05-05">May 5, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Feiping</forename><surname>Nie</surname></persName>
							<email>feipingnie@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Center for OPTical IMagery Analysis and Learning (OPTIMAL)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
							<email>ruizhang8633@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Center for OPTical IMagery Analysis and Learning (OPTIMAL)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Center for OPTical IMagery Analysis and Learning (OPTIMAL)</orgName>
								<orgName type="department" key="dep2">Xi&apos;an Institute of Optics and Precision Mechanics</orgName>
								<orgName type="laboratory">State Key Laboratory of Transient Optics and Photonics</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>710119</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A generalized power iteration method for solving quadratic problem on the Stiefel manifold</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-05-05">May 5, 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">47B17C2075DDE3C8443AEB69874FA585</idno>
					<idno type="DOI">10.1007/s11432-016-9021-9</idno>
					<note type="submission">Received October 18, 2016; accepted January 10, 2017;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>quadratic problem</term>
					<term>Stiefel manifold</term>
					<term>power iteration</term>
					<term>procrustes problem</term>
					<term>orthogonal least square regression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we first propose a novel generalized power iteration (GPI) method to solve the quadratic problem on the Stiefel manifold (QPSM) as min W T W =I Tr(W T AW -2W T B) along with the theoretical analysis. Accordingly, its special case known as the orthogonal least square regression (OLSR) is under further investigation. Based on the aforementioned studies, we then majorly focus on solving the unbalanced orthogonal procrustes problem (UOPP). As a result, not only a general convergent algorithm is derived theoretically but the efficiency of the proposed approach is verified empirically as well.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The orthogonal procrustes problem (OPP) is the least square problem on the Stiefel manifold. The OPP originates from the factor analysis in psychometrics during 1950s and 1960s <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. The major purpose is to determine an orthogonal matrix that rotates the factor matrix to best fit some hypothesis matrix. The balanced case of the OPP was surveyed in multiple introductory textbooks such as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Recently, due to the wide applications of the orthogonal regression in computer science, see <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, solving the unbalanced OPP (UOPP) is under increasing concern. Multiple approaches are proposed to solve UOPP such as the expansion balanced algorithm (EB), the right hand side and the left hand side relaxation (RSR), (LSR), the successive projection (SP) and the Lagrangian relaxation (LR). In <ref type="bibr" target="#b6">[7]</ref>, the EB method employs the expanded balanced OPP as its objective function. In <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> respectively, the RSR and the LSR approaches update the solution row by row or column by column iteratively based on solving the least square regression with a quadratic equality constraint (LSQE). In <ref type="bibr" target="#b9">[10]</ref>, the SP method updates the solution column by column by virtue of the projection method combined with correction techniques (PMCT) discussed by <ref type="bibr" target="#b10">[11]</ref>, which is efficient to solve LSQE. In <ref type="bibr" target="#b11">[12]</ref>, the LR method solves UOPP by selecting different Lagrangian multipliers.</p><p>All the approaches mentioned above could converge to the solution of UOPP successfully, whereas they deal with more complex procedures, which represent high orders of complexity. Furthermore, all these methods initialize the parameters deliberately to optimize their proposed algorithms. Last but not least, all these approaches are unable to deal with a more general problem known as the quadratic problem on the Stiefel manifold (QPSM).</p><p>To address the referred deficiencies, we derive a novel generalized power iteration method (GPI) for QPSM in order to efficiently solve the orthogonal least square regression (OLSR) and UOPP with a random initial guess and concise computational steps. In sum, the proposed GPI method can deal with a more general problem known as QPSM than other approaches. Furthermore, the experimental results show that the proposed GPI method not only takes much less CPU time for the convergence but becomes more efficient dealing with the data matrix of large dimension as well.</p><p>Notations. For any matrix M , Frobenius norm is defined as M 2 F = Tr(M T M ), where Tr(•) is the trace operator. For any positive integer n, I n denotes a n × n identity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Power iteration method revisited</head><p>The power iteration method is an iterative algorithm to seek the dominant eigenvalue and the related eigenvector of any given symmetric matrix A ∈ R m×m , where the dominant eigenvalue is defined as the greatest eigenvalue in magnitude. The power iteration can be performed as the following steps:</p><p>(1) Initialization. Random initialize a vector w ∈ R m×1 , which has a nonzero component in the direction of the dominant eigenvector.</p><p>(2) Update m ← Aw.</p><p>(3) Calculate q = m m 2 . (4) Update w ← q.</p><p>(5) Iteratively perform the steps (2)-(4) until convergence. The power iteration could be further extended to the orthogonal iteration (also called subspace iteration or simultaneous iteration) method to find the first k (k m) dominant eigenvalues and their associated eigenvectors for the given matrix A. The orthogonal iteration method could be described as the following iterative algorithm:</p><p>(1) Initialization. Random initialize W ∈ R m×k .</p><p>(2) Update M ← AW .</p><p>(3) Calculate QR = M via the compact QR factorization of M , where Q ∈ R m×k and R ∈ R k×k . (4) Update W ← Q.</p><p>(5) Iteratively perform the steps (2)-(4) until convergence. Apparently, the orthogonal iteration method above indicates a normalization process, which is similar as the normalization in the power iteration method. When the matrix A is positive semi-definite (psd), the orthogonal iteration method is equivalent to solving the following optimization problem: max</p><formula xml:id="formula_0">W T W =I k Tr(W T AW ).<label>(1)</label></formula><p>Therefore, the orthogonal iteration method is equivalent to the following steps under the psd matrix A:</p><p>(1) Initialization. Random initialize W ∈ R m×k .</p><p>(2) Update M ← AW .</p><p>(3) Calculate U SV T = M via the compact SVD method of M , where U ∈ R m×k , S ∈ R k×k and V ∈ R k×k .</p><p>(4) Update W ← U V T .</p><p>(5) Iteratively perform the steps (2)-(4) until convergence.</p><p>From the observation, the solution of the above algorithm as W K differs from the solution of the orthogonal iteration method as W by the form, where KK T = I k . However, the difference between the solutions of these two algorithms does not affect the objective value of the problem (1) due to the following derivation:</p><formula xml:id="formula_1">Tr((W K) T AW K) = Tr(W T AW KK T ) = Tr(W T AW ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Quadratic problem on the Stiefel manifold</head><p>The Stiefel manifold ν m,k is a set of the matrices W ∈ R m×k , which have orthonormal columns as</p><formula xml:id="formula_2">ν m,k = {W ∈ R m×k : W T W = I k }.</formula><p>In this section, a novel approach is derived to unravel the following QPSM <ref type="bibr" target="#b12">[13]</ref> as min</p><formula xml:id="formula_3">W T W =I k Tr(W T AW -2W T B),<label>(2)</label></formula><p>where W ∈ R m×k , B ∈ R m×k and the symmetric matrix A ∈ R m×m . In order to solve the problem (2), QPSM in (2) can be further relaxed into max</p><formula xml:id="formula_4">W T W =I k Tr(W T ÃW ) + 2Tr(W T B),<label>(3)</label></formula><p>where Ã = αI m -A ∈ R m×m . The relaxation parameter α is an arbitrary constant such that Ã is a positive definite (pd) matrix. To be more specific, the relaxation parameter α could be easily set as the dominant eigenvalue of A, which could be fast obtained by the power method discussed in the previous section. Instead of the method of the Lagrangian multipliers to deal with an optimization problem with orthogonal constraints, one may use a geometric optimization algorithm tailored to the Stiefel manifold, such as, for example, the one surveyed in <ref type="bibr" target="#b13">[14]</ref>. Accordingly, the Lagrangian function for the problem (3) can be written as</p><formula xml:id="formula_5">L 1 (W, Λ) = Tr(W T ÃW ) + 2Tr(W T B) -Tr(Λ(W T W -I k )).<label>(4)</label></formula><p>From Eq. ( <ref type="formula" target="#formula_5">4</ref>), we could obtain the KKT condition for the problem (3) as</p><formula xml:id="formula_6">∂L 1 ∂W = 2 ÃW + 2B -2W Λ = 0,<label>(5)</label></formula><p>which is difficult to solve directly. Thus, motivated by <ref type="bibr" target="#b14">[15]</ref> and the power iteration method mentioned in Section 2, we could propose the following iterative algorithm:</p><p>(</p><formula xml:id="formula_7">1) Initialization. Random initialize W ∈ R m×k such that W T W = I k . (2) Update M ∈ R m×k ← 2 ÃW + 2B.</formula><p>(3) Calculate W * by solving the following problem: max</p><formula xml:id="formula_8">W T W =I k Tr(W T M ). (<label>6</label></formula><formula xml:id="formula_9">) (4) Update W ← W * .</formula><p>(5) Iteratively perform the steps (2)-(4) until convergence. Besides, a closed form solution of the problem (6) can be achieved by the following derivation.</p><p>Suppose the full SVD of</p><formula xml:id="formula_10">M is M = UΣV T with U ∈ R m×m , Σ ∈ R m×k and V ∈ R k×k , then we have Tr(W T M ) = Tr(W T UΣV T ) = Tr(ΣV T W T U) = Tr(ΣZ) = k i=1 σ ii z ii ,</formula><p>where Z = V T W T U ∈ R k×m with z ii and σ ii being the (i, i)-th elements of the matrix Z and Σ, respectively.</p><p>Note that ZZ T = I k , thus |z ii | 1. On the other hand, σ ii 0 since σ ii is a singular value of the matrix M . Therefore, we have</p><formula xml:id="formula_11">Tr(W T M ) = k i=1 z ii σ ii k i=1 σ ii .</formula><p>Apparently, the equality holds when z ii = 1, (1 i k). That is to say, Tr(W T M ) reaches the maximum when the matrix Z = [I k , 0] ∈ R k×m . Recall that Z = V T W T U, thus the optimal solution to the problem (6) can be represented as</p><formula xml:id="formula_12">W = UZ T V T = U[I k ; 0]V T .<label>(7)</label></formula><p>Since Eq. ( <ref type="formula" target="#formula_12">7</ref>) is based upon the full SVD of the matrix M , Eq. ( <ref type="formula" target="#formula_12">7</ref>) can be rewritten as W = U V T via the compact SVD of the matrix M , where</p><formula xml:id="formula_13">M = U SV T with U ∈ R m×k , S ∈ R k×k and V ∈ R k×k .</formula><p>Based on the above analysis, the GPI can be summarized in Algorithm 1.</p><p>We will prove that the proposed Algorithm 1 converges monotonically to the local minimum of QPSM (2).</p><p>Algorithm 1 Generalized power iteration method (GPI)</p><formula xml:id="formula_14">Input: The symmetric matrix A ∈ R m×m and the matrix B ∈ R m×k . Initialize a random W ∈ R m×k satisfying W T W = I k and α via power method such that Ã = αIm -A ∈ R m×m is a positive definite matrix. (1) Update M ← 2 ÃW + 2B. (2) Calculate U SV T = M via the compact SVD method of M where U ∈ R m×k , S ∈ R k×k and V ∈ R k×k . (3) Update W ← U V T .</formula><p>Iteratively perform the steps (1)-( <ref type="formula" target="#formula_4">3</ref>) until the algorithm converges.</p><p>Step (3) of Algorithm 1 is an instance of a class of methods, called manifold retractions, to update a matrix on the Stiefel manifold, that were discussed in details in <ref type="bibr" target="#b15">[16]</ref>. Proof. Since the matrix Ã is positive definite (pd), we could rewrite Ã = L T L via Cholesky factorization. Therefore, we have the following proof for Lemma 1 as</p><formula xml:id="formula_15">L W -LW 2 F 0 ⇒ Tr( W T Ã W ) -2Tr( W T ÃW ) + Tr(W T ÃW ) 0.</formula><p>Theorem 1. The Algorithm 1 decreases the value of the objective function in (2) monotonically in each iteration until it converges.</p><p>Proof. Suppose the updated W is W in Algorithm 1, then we have</p><formula xml:id="formula_16">Tr( W T M ) Tr(W T M ), (<label>8</label></formula><formula xml:id="formula_17">)</formula><p>since W is the optimal solution of the problem <ref type="bibr" target="#b5">(6)</ref>. Based on the fact that M = 2 ÃW + 2B, Eq. ( <ref type="formula" target="#formula_16">8</ref>) can be further illustrated as</p><formula xml:id="formula_18">2Tr( W T ÃW ) + 2Tr( W T B) 2Tr(W T ÃW ) + 2Tr(W T B).<label>(9)</label></formula><p>Based on Lemma 1 and Eq. ( <ref type="formula" target="#formula_18">9</ref>), we could infer that</p><formula xml:id="formula_19">Tr( W T Ã W ) + 2Tr( W T B) Tr(W T ÃW ) + 2Tr(W T B) ⇒ Tr( W T A W ) -2Tr( W T B) Tr(W T AW ) -2Tr(W T B),</formula><p>which indicates that Algorithm 1 decreases the objective value of QPSM in (2) in each iteration until the algorithm converges.</p><p>Theorem 2. Algorithm 1 converges to a local minimum of the QPSM problem (2).</p><p>Proof. Since Algorithm 1 performs based on solving the problem (6) in each iteration, the Lagrangian function for the solution of Algorithm 1 can be represented as</p><formula xml:id="formula_20">L 2 (W, Λ) = Tr(W T M ) -Tr(Λ(W T W -I k )).<label>(10)</label></formula><p>Therefore, the solution of Algorithm 1 satisfies the following KKT condition:</p><formula xml:id="formula_21">∂L 2 ∂W = M -2W Λ = 0. (<label>11</label></formula><formula xml:id="formula_22">)</formula><p>Generally speaking, the matrix M will be updated by W in each iteration under Algorithm 1. Since Algorithm 1 converges to the optimal solution W , i.e., W = W due to Theorem 1, Eq. ( <ref type="formula" target="#formula_21">11</ref>) can be further formulated by substituting M = 2 ÃW + 2B as</p><formula xml:id="formula_23">∂L 2 ∂W = 2 ÃW + 2B -2W Λ = 0. (<label>12</label></formula><formula xml:id="formula_24">)</formula><p>By comparing ( <ref type="formula" target="#formula_6">5</ref>) and ( <ref type="formula" target="#formula_23">12</ref>), we could draw the conclusion that the solution of Algorithm 1 and the problem (3) satisfy the same KKT condition. Therefore, Algorithm 1 converges to a local minimum of QPSM (2) since the problems ( <ref type="formula" target="#formula_3">2</ref>) and ( <ref type="formula" target="#formula_4">3</ref>) are equivalent.</p><p>Besides, the problem ( <ref type="formula" target="#formula_8">6</ref>) has an unique solution under full column-rank matrix M due to the uniqueness of the SVD method. On the other hand, the experimental results in Section 5 represent that the proposed GPI method uniformly converges to the same objective value with a large amount of random initial guesses. Based on the unique solution of the problem (6) and the associated experimental results, it is rational to conjecture that the proposed GPI method converges to the global minimum of QPSM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Two special cases of quadratic problem on the Stiefel manifold</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Orthogonal least square regression</head><p>The orthogonal least square regression (OLSR) can be written as min</p><formula xml:id="formula_25">W T W =I k ,b X T W + 1b T -Y 2 F ,<label>(13)</label></formula><p>where the data matrix X ∈ R m×n and the hypothesis matrix Y ∈ R n×k with 1 = (1, 1, . . . , 1) T ∈ R n×1 . Moreover, W ∈ R m×k is the regression matrix and b ∈ R k×1 is the bias vector. Obviously, b is free from any constraint. By virtue of the extreme value condition with regard to b, we can derive as</p><formula xml:id="formula_26">∂ X T W + 1b T -Y 2 F ∂b = 0 ⇒ W T X1 + b1 T 1 -Y T 1 = 0 ⇒ b = 1 n (Y T 1 -W T X1).</formula><p>By substituting the above result as b = 1 n (Y T 1 -W T X1), Eq. ( <ref type="formula" target="#formula_25">13</ref>) can be simplified to min</p><formula xml:id="formula_27">W T W =I k H(X T W -Y ) 2 F ,<label>(14)</label></formula><p>where H = I n -1 n 11 T . Accordingly, the problem ( <ref type="formula" target="#formula_27">14</ref>) can be further reformulated into min</p><formula xml:id="formula_28">W T W =I k Tr(W T AW -2W T B),<label>(15)</label></formula><p>in which</p><formula xml:id="formula_29">A = XHX T , B = XHY.</formula><p>Apparently, Eq. ( <ref type="formula" target="#formula_28">15</ref>) is in the exact same form as QPSM in <ref type="bibr" target="#b1">(2)</ref>. Therefore, OLSR in ( <ref type="formula" target="#formula_25">13</ref>) can be solved via Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unbalanced orthogonal procrustes problem</head><p>Definition 1. With Q ∈ R m×k , E ∈ R n×m and G ∈ R n×k , we name the optimization problem min</p><formula xml:id="formula_30">Q T Q=I k EQ -G 2 F (<label>16</label></formula><formula xml:id="formula_31">)</formula><p>(1) balanced OPP if and only if m = k;</p><p>(2) unbalanced orthogonal procrustes problem (UOPP) if and only if m &gt; k. Especially when Q serves as a column vector (k = 1), the problem ( <ref type="formula" target="#formula_30">16</ref>) degenerates to min</p><formula xml:id="formula_32">q T q=1 Eq -g 2 2 ,<label>(17)</label></formula><p>which is known as the least square problem with a LSQE. In <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, LSQE in ( <ref type="formula" target="#formula_32">17</ref>) can be solved with the closed form solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Balanced orthogonal procrustes problem revisited</head><p>To solve the balanced OPP (m = k), we could expand Eq. ( <ref type="formula" target="#formula_30">16</ref>) into min</p><formula xml:id="formula_33">Q T Q=I k EQ -G 2 F ⇒ min Q T Q=I k E 2 F + G 2 F -2Tr(Q T E T G) ⇒ max Q T Q=I k Tr(Q T E T G),</formula><p>which is same as the problem <ref type="bibr" target="#b5">(6)</ref> with treating E T G = M . Thus, the balanced OPP has the analytical solution of the closed form (7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Unbalanced orthogonal procrustes problem</head><p>When m &gt; k, UOPP ( <ref type="formula" target="#formula_30">16</ref>) can be expanded into min</p><formula xml:id="formula_34">Q T Q=I k EQ -G 2 F ⇒ min Q T Q=I k Tr(Q T E T EQ -2Q T E T G).<label>(18)</label></formula><p>Denote E T E = A and E T G = B, then Eq. ( <ref type="formula" target="#formula_34">18</ref>) is in the exact same form as QPSM <ref type="bibr" target="#b1">(2)</ref>. Based on Algorithm 1, Algorithm 2 can be proposed to converge to a local minimum of UOPP monotonically due to the theoretical supports proved in Section 3.</p><p>Algorithm 2 GPI for solving UOPP in ( <ref type="formula" target="#formula_30">16</ref>)</p><formula xml:id="formula_35">Input: The matrix E ∈ R n×m and the matrix G ∈ R n×k where m &gt; k. Initialize Q ∈ R m×k and γ such that Q T Q = I k and the matrix γIm -E T E is positive definite, respectively. While not converge do (1) Update matrix M ← 2(γIm -E T E)Q + 2E T G. (2) Calculate U ∈ R m×k and V ∈ R k×k via the compact SVD of M as M = U SV T . (3) Update Q ← U V T . End while Return Q.</formula><p>Generally speaking, QPSM cannot be reformulated into UOPP while UOPP could always be rewritten into QPSM. Therefore, the GPI method is more general than other approaches, which can only cope with UOPP. Based on the experimental results involved in the next section, the proposed GPI method takes much less time to converge to the solution of UOPP. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental results</head><p>In this section, we analyze and report the numerical results of the GPI method represented by both Algorithms 1 and 2. We randomly choose the test data matrix with normally distributed singular values.</p><p>Besides, the computer we use is MacBook Air, whose CPU is 1.4 GHz Intel Core i5, RAM is 4 GB 1600 MHz DDR3 and operating system is OS X Yosemite 10.10.5.</p><p>Case 1 (parameter dependence): Firstly, we try to investigate the GPI method in Algorithm 2 via varying the relaxation parameter γ. Suppose l e is the largest eigenvalue of E T E, then we can let γ = δl e such that γI m -E T E is a positive definite matrix, where δ is an arbitrary constant.</p><p>From Figure <ref type="figure" target="#fig_0">1</ref>, we can further notice that although the convergence rate for Algorithm 2 is inversely proportional to the value of γ, the relaxation parameter γ does not affect the uniform convergence of the GPI method.</p><p>Case 2 (CPU time comparison for solving UOPP): Secondly, we further investigate the proposed GPI method in Algorithm 2 by comparing it with five existing approaches mentioned in Section 1 as EB <ref type="bibr" target="#b6">[7]</ref>, RSR <ref type="bibr" target="#b7">[8]</ref>, LSR <ref type="bibr" target="#b8">[9]</ref>, SP <ref type="bibr" target="#b9">[10]</ref> and LR <ref type="bibr" target="#b11">[12]</ref>.</p><p>Based on solving LSQE problem, RSR <ref type="bibr" target="#b7">[8]</ref> and LSR <ref type="bibr" target="#b8">[9]</ref> respectively update the solution row by row and column by column iteratively. EB <ref type="bibr" target="#b6">[7]</ref> utilizes the expanded balanced OPP as the objective function. SP <ref type="bibr" target="#b9">[10]</ref> employs the projection method combined with correction techniques (PMCT) <ref type="bibr" target="#b10">[11]</ref>. LR <ref type="bibr" target="#b11">[12]</ref> solves UOPP by fixing different Lagrangian multipliers. The proposed GPI method includes two terms as E T E outside the loop and ÃW within the loop, whose orders of complexity are m 2 n and m 2 k, respectively. Besides, these two terms have the highest orders of complexity for the proposed GPI method. Besides, the order of the complexity for each method is shown in Table <ref type="table" target="#tab_0">1</ref>.</p><p>The comparative results are based on fixing E as the square matrix at first hand (Table <ref type="table" target="#tab_1">2</ref>) and then extend E to a more general case (Table <ref type="table" target="#tab_2">3</ref>) afterwards. (Mark '-' in Tables <ref type="table" target="#tab_1">2</ref> and<ref type="table" target="#tab_2">3</ref> represents that it takes too much time to record in the tables.)</p><p>(1) From Figure <ref type="figure" target="#fig_2">2</ref>, we notice that the existing methods as EB <ref type="bibr" target="#b6">[7]</ref>, RSR <ref type="bibr" target="#b7">[8]</ref>, LSR <ref type="bibr" target="#b8">[9]</ref>, SP <ref type="bibr" target="#b9">[10]</ref> LR <ref type="bibr" target="#b11">[12]</ref> and the proposed GPI method converge to the same objective value under the same input data. Besides, our proposed GPI method converges faster than other approaches during iteration.</p><p>(2) From Table <ref type="table" target="#tab_0">1</ref>, the proposed GPI method has the lowest order of complexity due to its succinct computational process to obtain the optimal solution. During the experiments, we observe that the iteration number t for the LR method is usually very large for the convergence. Thus, the time consumption for LR method is much larger than that for the proposed GPI method though orders of complexity for these two approaches seem close. Besides, the GPI method becomes more efficient when n (the number of data) is large.</p><p>(3) From Table <ref type="table" target="#tab_1">2</ref>, the proposed Algorithm 2 (GPI) serves as the most efficient method under the square matrix case.</p><p>(4) From Table <ref type="table" target="#tab_2">3</ref>, we can observe that LSR <ref type="bibr" target="#b8">[9]</ref>, SP <ref type="bibr" target="#b9">[10]</ref> and RSR <ref type="bibr" target="#b7">[8]</ref> are unable to compete with LR <ref type="bibr" target="#b11">[12]</ref>, EB <ref type="bibr" target="#b6">[7]</ref> and GPI due to the complex updating procedures including the expanded OPP and solving LSQE. Especially when the dimension increases, the superiority of our proposed GPI method would be more obvious.</p><p>Case 3 (CPU time comparison for solving LSQE): Finally, the projection method combined with correction techniques (PMCT) <ref type="bibr" target="#b10">[11]</ref> is compared to the GPI method in Algorithm 2 targeting at solving the least square regression with a LSQE in <ref type="bibr" target="#b16">(17)</ref>. Actually, solving LSQE ( <ref type="formula" target="#formula_32">17</ref>) is no different from solving Comparisons of the convergence rate are performed for 6 approaches including EB <ref type="bibr" target="#b6">[7]</ref>, RSR <ref type="bibr" target="#b7">[8]</ref>, LSR <ref type="bibr" target="#b8">[9]</ref>, SP <ref type="bibr" target="#b9">[10]</ref> LR <ref type="bibr" target="#b11">[12]</ref> and our GPI method under 3 different data matrices.  UOPP <ref type="bibr" target="#b15">(16)</ref> under k = 1.</p><p>(1) From Figure <ref type="figure" target="#fig_4">3</ref>, we can notice that PMCT <ref type="bibr" target="#b10">[11]</ref> and Algorithm 2 (GPI) converge to the same objective value though in terms of the different patterns.</p><p>(2) From Figure <ref type="figure">4</ref>, Algorithm 2 (GPI) takes much less time for convergence than PMCT <ref type="bibr" target="#b10">[11]</ref> does.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Lemma 1 .</head><label>1</label><figDesc>If the symmetric matrix Ã ∈ R m×m is positive definite (pd), then Tr( W T Ã W ) -2Tr( W T ÃW ) + Tr(W T ÃW ) 0, where W ∈ R m×k and W ∈ R m×k are arbitrary matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 (</head><label>1</label><figDesc>Figure 1 (Color online) Comparisons of 6 different values of δ are performed under the GPI method with 3 different data matrices. (a) (50, 100, 30); (b) (80, 170, 80); (c) (40, 120, 60).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 (</head><label>2</label><figDesc>Figure2(Color online) Comparisons of the convergence rate are performed for 6 approaches including EB<ref type="bibr" target="#b6">[7]</ref>, RSR<ref type="bibr" target="#b7">[8]</ref>, LSR<ref type="bibr" target="#b8">[9]</ref>, SP<ref type="bibr" target="#b9">[10]</ref> LR<ref type="bibr" target="#b11">[12]</ref> and our GPI method under 3 different data matrices. (a) (100, 10, 100); (b) (100, 15, 100); (c)(200,<ref type="bibr" target="#b14">15,</ref> 200).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure2(Color online) Comparisons of the convergence rate are performed for 6 approaches including EB<ref type="bibr" target="#b6">[7]</ref>, RSR<ref type="bibr" target="#b7">[8]</ref>, LSR<ref type="bibr" target="#b8">[9]</ref>, SP<ref type="bibr" target="#b9">[10]</ref> LR<ref type="bibr" target="#b11">[12]</ref> and our GPI method under 3 different data matrices. (a) (100, 10, 100); (b) (100, 15, 100); (c)(200,<ref type="bibr" target="#b14">15,</ref> 200).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 (</head><label>3</label><figDesc>Figure 3 (Color online) Comparisons of PMCT [11] and GPI are performed over 2 different data matrices. (a) (900, 1000); (b) (2000, 1700).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Orders of complexity for 6 algorithms a)</figDesc><table><row><cell></cell><cell>RSR [8]</cell><cell>LSR [9]</cell><cell>SP [10]</cell></row><row><cell>Order of the complexity</cell><cell>O(mnk + m 3 kt)</cell><cell>O(mnk + m 3 kt)</cell><cell>O(mnk + (m 2 n + m 3 )t)</cell></row><row><cell></cell><cell>LR [12]</cell><cell>EB [7]</cell><cell>GPI (ours)</cell></row><row><cell>Order of the complexity</cell><cell>O(m 2 n + nk 2 + m 2 kt)</cell><cell>O(m 3 + (m 2 n + m 3 )t)</cell><cell>O(m 2 n + m 2 kt)</cell></row><row><cell cols="3">a) t stands for the iteration number and (n, m, k) stands for the dimension.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Comparisons of CPU time (s) under the square matrix E for case 2 b)</figDesc><table><row><cell>(n = m = 200)</cell><cell>RSR [8]</cell><cell>LSR [9]</cell><cell>SP [10]</cell><cell>LR [12]</cell><cell>EB [7]</cell><cell>GPI (ours)</cell></row><row><cell>k = 10</cell><cell>64.940</cell><cell>23.426</cell><cell>2.386</cell><cell>0.541</cell><cell>0.337</cell><cell>0.228</cell></row><row><cell>k = 15</cell><cell>136.020</cell><cell>21.635</cell><cell>3.221</cell><cell>1.134</cell><cell>0.347</cell><cell>0.226</cell></row><row><cell>k = 20</cell><cell>229.851</cell><cell>20.560</cell><cell>5.054</cell><cell>1.806</cell><cell>0.445s</cell><cell>0.273</cell></row><row><cell>(n = m = 1000)</cell><cell>RSR [8]</cell><cell>LSR [9]</cell><cell>SP [10]</cell><cell>LR [12]</cell><cell>EB [7]</cell><cell>GPI (ours)</cell></row><row><cell>k = 10</cell><cell>-</cell><cell>842.849</cell><cell>132.232</cell><cell>3.869</cell><cell>11.440</cell><cell>1.290</cell></row><row><cell>k = 15</cell><cell>-</cell><cell>851.231</cell><cell>196.761</cell><cell>5.180</cell><cell>12.534</cell><cell>1.434</cell></row><row><cell>k = 20</cell><cell>-</cell><cell>860.746</cell><cell>260.132</cell><cell>7.700</cell><cell>12.625</cell><cell>1.575</cell></row><row><cell cols="3">b) Iteration stops when EQ i-1 -G 2 F -EQ i -G 2 F</cell><cell cols="2">τ where τ = 10 -3 .</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Comparisons of CPU time (s) under the general dimension for case 2 b)</figDesc><table><row><cell>Dimension (n, m, k)</cell><cell>RSR [8]</cell><cell>LSR [9]</cell><cell>SP [10]</cell><cell>LR [12]</cell><cell>EB [7]</cell><cell>GPI (ours)</cell></row><row><cell>(5000, 500, 15)</cell><cell>713.156</cell><cell>528.034</cell><cell>450.028</cell><cell>20.709</cell><cell>16.554</cell><cell>3.581</cell></row><row><cell>(10000, 1000, 30)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56.772</cell><cell>191.970</cell><cell>9.384</cell></row><row><cell>(3000, 3000, 90)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>186.125</cell><cell>395.401</cell><cell>17.320</cell></row><row><cell>(30000, 1500, 30)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>306.132</cell><cell>1056.311</cell><cell>19.440</cell></row><row><cell>(5000, 4000, 100)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>405.937</cell><cell>1187.512</cell><cell>30.128</cell></row><row><cell>(100000, 3000, 50)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>215.173</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Concluding remarks</head><p>In this paper, we analyze the QPSM by deriving a novel GPI method. Based on the proposed GPI method, two special and significant cases of QPSM known as the orthogonal least square regression and the unbalanced orthogonal procrustes problem are under further investigation. With the theoretical supports, the GPI method decreases the objective value of the QPSM problem monotonically to a local minimum until convergence. Eventually, the effectiveness and the superiority of the proposed GPI method are verified empirically. In sum, the proposed GPI method not only takes less CPU time to converge to the optimal solution with a random initial guess but becomes much more efficient especially for the data matrix of large dimension as well.</p><p>Conflict of interest The authors declare that they have no conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The orthogonal approximation of an oblique simple structure in factor analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="429" to="440" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The procrustes program: producing direct rotation to test a hypothesized factor structure</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hurley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cattell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav Sci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="258" to="262" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F Matrix</forename><surname>Van Loan</surname></persName>
		</author>
		<author>
			<persName><surname>Computations</surname></persName>
		</author>
		<author>
			<persName><surname>Baltimore</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>The Johns Hopkins University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Algorithms for the weighted orthogonal procrustes problem and other least squares problems</title>
		<author>
			<persName><forename type="first">V</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Umeå University</publisher>
			<pubPlace>Umeå</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Dissertation for Ph.D. Degree</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Online algorithm based on support vectors for orthogonal regression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Borges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn Lett</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1394" to="1404" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The orthogonally constrained regression revisted</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Trendafilov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Comput Graph Stat</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="746" to="771" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A problem with congruence</title>
		<author>
			<persName><forename type="first">B</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Psychometric Society</title>
		<meeting>the Annual Meeting of the Psychometric Society<address><addrLine>Monterey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A parallel algorithm for the unbalanced orthogonal procrustes problem</title>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parall Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="913" to="923" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The procrustes problem for orthogonal stiefel matrices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bojanczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lutoborski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J Sci Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1291" to="1304" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Successive projection method for solving the unbalanced procrustes problem</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci China Ser A-Math</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="971" to="986" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A projection method for least square problems with quadratic equality constraint</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J Matr Anal Appl</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="188" to="212" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Partial lagrangian relaxation for the unbalanced orthogonal procrustes problem</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math Meth Oper Res</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="225" to="237" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalized power method for sparse principal component analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Journée</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="517" to="553" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Formulation and integration of learning differential equations on the Stiefel manifold</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fiori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1697" to="1701" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimal mean robust principal component analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning<address><addrLine>Beijing</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2755" to="2763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Empirical arithmetic averaging over the compact stiefel manifold</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fiori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Signal Process</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="883" to="894" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A constrained eigenvalue problem</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Matt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Signal Processing and Parallel Algorithms</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="677" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transductive learning via spectral graph partitioning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning</title>
		<meeting>the 20th International Conference on Machine Learning<address><addrLine>Washington</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
