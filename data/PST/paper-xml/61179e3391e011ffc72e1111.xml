<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ASAP: Fast Mobile Application Switch via Adaptive Prepaging</title>
				<funder ref="#_297baYd">
					<orgName type="full">Samsung Research Funding &amp; Incubation Center of Samsung Electronics</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Seung</roleName><forename type="first">Sam</forename><surname>Son</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Seoul National University ? Sungkyunkwan University ? ? Google</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yul</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yunho</forename><surname>Jin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Seoul National University ? Sungkyunkwan University ? ? Google</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonghyun</forename><surname>Bae</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Seoul National University ? Sungkyunkwan University ? ? Google</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinkyu</forename><surname>Jeong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sungkyunkwan</forename><surname>University</surname></persName>
						</author>
						<author>
							<persName><forename type="first">;</forename><surname>Tae</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Ham</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Seoul National University ? Sungkyunkwan University ? ? Google</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jae</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Seoul National University ? Sungkyunkwan University ? ? Google</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Seung</forename><forename type="middle">Yul</forename><surname>Lee</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Seoul National University ? Sungkyunkwan University ? ? Google</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">?</forename><surname>Tae</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Seoul National University ? Sungkyunkwan University ? ? Google</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongil</forename><surname>Yoon</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Seoul National University ? Sungkyunkwan University ? ? Google</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Hongil Yoon, Google</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ASAP: Fast Mobile Application Switch via Adaptive Prepaging</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With mobile applications' ever-increasing demands for memory capacity, along with a steady increase in the number of applications running concurrently, memory capacity is becoming a scarce resource on mobile devices. When the memory pressure is high, current mobile OSes often kill application processes that have not been used recently to reclaim memory space. This leads to a long delay when a user relaunches the killed application, which degrades the user experience. Even if this mechanism is disabled to utilize a compression-based in-memory swap mechanism, relaunching the application still incurs a substantial latency penalty as it requires the decompression of compressed anonymous pages and a stream of I/O accesses to retrieve file-backed pages into memory. This paper identifies conventional demand paging as the primary source of this inefficiency and proposes ASAP, a mechanism for fast application switch via adaptive prepaging on mobile devices. ASAP performs prepaging by combining i) high-precision switch footprint estimators for both file-backed and anonymous pages, and ii) efficient implementation of the prepaging mechanism to minimize resource waste for CPU cycles and disk bandwidth during an application switch. Our evaluation using eight real-world applications on Google Pixel 4 and Pixel 3a demonstrates that ASAP can reduce the switch time by 22.2% and 28.3% on average, respectively (with a maximum of 33.3% and 35.7%, respectively), over the vanilla Android 10.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the broad capabilities and flexibility of mobile computing, mobile applications continue to tout rich features to meet users' diverse demands. This entails a continuous increase in both codes and data footprint <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b26">31]</ref>. This trend has resulted in a constant demand for larger memory capacity on mobile devices to address memory pressure issues. However, the cost of the device and the power/area budget often limit its size.</p><p>Modern mobile OSes support virtual memory with compression-based swap <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b8">13]</ref>. The virtual memory pro-vides an illusion of physical memory space larger than the actual memory capacity, enabling multiple applications to run concurrently even under high memory pressure. However, the benefits come with additional overhead, degrading performance. Slow I/O accesses increase the latency of fetching non-resident file-backed pages from storage. To fetch anonymous pages in the compressed swap space, they first need to be decompressed by CPUs at a page fault. Allocating free pages also consumes system resources. Fetching pages on-demand via demand paging may not efficiently utilize available resources such as CPU cycles and I/O bandwidth.</p><p>Our empirical analysis shows that the application switch time can increase by a factor of 4? (in the order of hundreds of milliseconds) when the system is experiencing memory pressure, possibly when running many background applications. This slowdown is mainly attributed to the long blocking time introduced by demand paging for both file-backed and anonymous pages during the application switch rather than freeing allocated pages.</p><p>A recent study shows that today's smartphone users often run more than 10 applications <ref type="bibr" target="#b20">[25]</ref>, and thus it is likely that the system is often operating under memory pressure unless the phone has a large main memory capacity. It is also known that users switch between applications more than 100 times per day <ref type="bibr" target="#b6">[9]</ref>. We speculate that such frequent, long-latency events can potentially affect smartphone user experience negatively. In this paper, we aim to reduce the latency of the application switch by minimizing the demand-paging related slowdown. To achieve this goal, we propose ASAP, a mechanism for fast application switch via adaptive prepaging. ASAP builds on the following key observations:</p><p>? Hardware resources for fetching non-resident pages are underutilized during the application switch when the system is under memory pressure. For eight popular Android applications, CPU utilization is 34.2% during the switch. Also, only 19.4% of the maximum disk bandwidth is used on average.</p><p>? File-backed pages and anonymous pages have differ-ent characteristics in their switch footprint, a set of accessed pages during the application switch. In particular, the switch footprint for file-backed pages is much more invariant-about 75% of all accessed file-backed pages are invariant across switches, while only 44% of anonymous pages are invariant. This motivates us to use different prediction strategies for prepaging them.</p><p>We capitalize on these empirical observations to develop an effective prepaging approach. The first observation suggests that it is promising to utilize available resources to prepage pages likely to be accessed at the beginning of an application switch. The prepaging is helpful to maximize the effective CPU and disk bandwidth utilization, which can translate to performance gains (i.e., reduced switch time). The second observation suggests that the target pages to fetch need to be adapted at runtime to capture the applications' dynamically changing page access patterns. This improves the prediction accuracy for the switch footprint, hence making ASAP more effective.</p><p>At an application switch, ASAP wakes up multiple prepaging threads to start fetching both file-backed pages and anonymous pages. These threads run in parallel with application threads to overlap prepaging with application computation. To accurately predict switch footprint pages, ASAP employs an adaptive prediction mechanism. Specifically, a single predictor maintains two tables: a candidate table and a target table. The predictor promotes or demotes pages between the two tables based on the runtime information of their access patterns. The prepaging threads issue fetch requests only for the pages in the target table, while pages having a smaller chance of being accessed are maintained in the candidate table.</p><p>We have implemented ASAP in Android OS and evaluated it using a set of eight popular mobile applications on Google Pixel 4 and Pixel 3a. The evaluation results show that ASAP considerably reduces the application switch time under memory pressure. ASAP reduces the switching time by 22.2% and 28.3% on average (33.3% and 35.7% at maximum) on Pixel 4 and Pixel 3a, respectively, over the vanilla Android 10. This improvement is attributed to an average of 39.8% and 25.2% increase in CPU and disk bandwidth utilization, respectively, as well as 79.3% and 68.4% prediction accuracy for file-backed and anonymous pages, respectively.</p><p>In summary, this paper makes the following contributions:</p><p>? We empirically analyze the performance bottleneck of the application switch to identify opportunities for prepaging as a solution to the problem.</p><p>? We propose ASAP, an adaptive prepaging technique to reduce the switch time, which is a key user interaction on the mobile device. ASAP is application-agnostic without requiring any change to application codes.</p><p>? We integrate ASAP into Android OS and evaluate its performance by using eight popular mobile applications on high-end and mid-end devices (Google Pixel 4 and Pixel 3a). The results demonstrate the effectiveness of ASAP for reducing the application switch time by 22.2% and 28.3% on average, respectively, over the vanilla Android 10.</p><p>2 Background and Motivation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Android Application Memory Management</head><p>Application Lifecycle and Memory Management. In Android OS, an application (specifically the application activity) is either in the foreground (i.e., having focus) or in the background (e.g., not visible). In other words, the application that a user is actively using is considered to be in the foreground, while the applications that have been launched but are not currently being used are considered to be in the background.</p><p>When the system has sufficient DRAM, all application data are kept in memory. However, a user often utilizes many different applications over time, and eventually, application data exceed the DRAM capacity. In such a case, the Android low memory killer daemon (lmkd) identifies the least essential application (e.g., one in the background) and kills it so that the memory space that it occupied is freed <ref type="bibr" target="#b16">[21,</ref><ref type="bibr" target="#b21">26]</ref>. Note that this does not necessarily result in the complete loss of the application state since Android applications often store a minimal set of its states when the application is moved to the background. With this mechanism, Android OS only stores a small set of essential application data in memory. For this reason, when a user starts an application that was moved to the background a long time ago and hence killed by lmkd, the application data is not resident in memory. Instead, the application needs to recreate all of its activities from scratch utilizing the saved state information. On the other hand, when a user starts an application that was moved to the background very recently, it is much more likely that this application's data still resides in memory, and the application will be ready-to-use in a much shorter period. The time Android OS requires for the former case is called launch time and the latter is called switch time (sometimes also called hot launch time).</p><p>Compression-based Swap. An alternative approach to secure the free memory space is the compression-based swap, which compresses the least essential memory pages and stores them in a separate memory region. Later, when the application accesses the compressed pages, they are decompressed back to memory. Compared to the traditional disk-based swap mechanism, the compression-based in-memory swap is faster since it can avoid long-latency disk accesses. This approach's drawback is that i) compressed memory pages still consume memory capacity, and ii) compression/decompression spends CPU cycles. This mechanism is enabled by default in many commercial mobile OSes such as Android OS and Apple iOS <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b8">13]</ref>. However, in practice, Android OS by default does not actively utilize this mechanism since lmkd is often triggered first to reclaim memory space before a swap happens <ref type="bibr" target="#b16">[21,</ref><ref type="bibr" target="#b20">25,</ref><ref type="bibr" target="#b21">26,</ref><ref type="bibr" target="#b29">35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Launch Time and Switch Time</head><p>When a user relaunches an application after a while since its last usage, the latency to reload may differ depending on the system's memory pressure. For example, if the system's memory pressure is low (e.g., the system has not used much memory since the application's last launch), the application's data will still reside in memory. Thus the application could reload quite quickly (i.e., ideal switch time). On the other hand, if the system's memory pressure is high (e.g., the user utilized many different apps during the time window), the application will be killed by lmkd. The relaunch is highly likely to require recreating the application's activities, incurring a much longer delay (i.e., launch time). Finally, if lmkd is disabled, the compression-based swap mechanism will come into play. The application's anonymous pages will be stored in memory in a compressed form, and the file-backed pages will be discarded. In this case, relaunching an application requires decompressing some of the application's anonymous pages and reloading file-backed pages from the disk.</p><p>Figure <ref type="figure">1</ref> presents the application launch/switch time of eight real-world applications (AB: Angry Bird, CC: Candy Crush Saga, NY: New York Times, YT: YouTube, FB: Facebook, TW: Twitter, CH: Google Chrome, QR: Quora) on Google Pixel 4. The figure shows that the switch time is lower than the launch time in all applications. This indicates that reconstructing activities of an application from scratch requires more time than retrieving the relevant anonymous pages and file-backed pages from memory and disks, respectively. This implies that an aggressive setting of Android lmkd increases the time to relaunch the application, which confirms the findings of the previous literature <ref type="bibr" target="#b13">[18,</ref><ref type="bibr" target="#b15">20,</ref><ref type="bibr" target="#b16">21]</ref>. To avoid this unnecessary delay in relaunching the application, it is better to lower the lmkd threshold (or even disable it) so that the system can utilize compression-based swap more actively.</p><p>This figure also shows a significant gap between the ideal switch time and the switch times under memory pressure. The gap between the ideal switch time and the switch time (filebacked pages in disk) quantifies the overhead of retrieving file-backed pages from the disk. The gap between the switch time (file-backed pages in disk) and the switch time (most pages not in memory) indicates the overhead of decompressing anonymous pages from the compressed memory pool. In fact, this overhead increases the application switch time by a factor of 4? relative to the ideal switch time on average. Unfortunately, the real-world switch time is often closer to the switch time (most pages not in memory) than the ideal switch time when we consider recent trends: i) an increase in an application's memory capacity requirements and ii) an increase in the number of apps that a user runs concurrently.</p><p>To the best of our knowledge, there are no concrete studies on the threshold of user perception of the application switch. However, previous studies <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b22">27]</ref> on related contexts imply that the delay of hundreds of milliseconds in the application switch may degrade the user experience. According to Card <ref type="bibr" target="#b4">[7]</ref>, users feel that a system is reacting instantaneously only when the response time is shorter than 100 ms. Olenski <ref type="bibr" target="#b22">[27]</ref> reports that a 100 ms delay in web page loading can degrade the user experience, resulting in a 1% drop in a company's revenue. Thus, we are convinced that maintaining a lower switch time over a wide variety of usage scenarios is critical for user experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Opportunities for Prepaging</head><p>Limitations of Demand-Paging. Figure <ref type="figure">1</ref> shows that the switch time under memory pressure is substantially worse than the ideal switch time. We find that such a huge overhead resulting from i) decompressing anonymous pages and ii) retrieving file-backed pages from the disk is attributable to the inefficiencies of demand paging. remains relatively low (i.e., less than 50%) for all applications except AB. Similarly, disk bandwidth utilization is also much lower than the sustainable peak bandwidth most of the time. As shown in Figure <ref type="figure" target="#fig_1">3</ref> for Google Pixel 3a, its disk bandwidth utilization is higher than that of the high-end device (Pixel 4) because the disk bandwidth is relatively low. However, the empirical results show that the resources are not still fully utilized in both cases.</p><p>Ideally, the CPU should have been fully utilized to decompress compressed memory pages, and disk bandwidth should have been saturated to retrieve file-backed pages from the disk. However, since the default demand paging mechanism initiates the decompression of memory pages and I/O accesses only at a page fault, the system wastes available resources and spends more application switch time than necessary.</p><p>Opportunities and Challenges of Prepaging. The key idea behind ASAP is that we can significantly improve the switch time by letting prepaging threads aggressively decompress memory pages and perform I/O accesses before the application codes demand them. By doing so, ASAP can fully exploit the available system resources (i.e., CPU cycles and disk bandwidth), making the switch time under memory pressure much closer to the ideal switch time. There are two main challenges in this approach. First, the system should effectively identify the switch footprint, a set of pages to be accessed during the switch. Second, the prepaging threads should be efficiently implemented to fully exploit available resources while minimizing their interference with application threads. The following sections describe how ASAP addresses these two challenges. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ASAP Design Overview</head><p>The empirical observations in the previous section suggest that it is promising to design an adaptive prepaging. We first set two key requirements to design a practical prepaging mechanism:</p><p>1. The proposed design should be able to accurately predict a set of pages that are likely to be accessed during an application's switch time (i.e., switch footprint).</p><p>2. The proposed design should be able to maximize the efficiency of prepaging by achieving high system resource utilization (i.e., CPU cycles and disk bandwidth) without interfering with the execution of application threads.</p><p>ASAP satisfies these requirements with Switch Footprint Estimator (SFE) and Prepaging Manager. We have integrated them into the application switching process in Linux kernel. Thus, ASAP is application-agnostic without requiring any changes to application codes.</p><p>Figure <ref type="figure">4</ref> illustrates the overall structure of ASAP with key components shaded in gray. SFE consists of two estimators: one for anonymous pages and the other for file-backed pages. Based on the analysis on the switch footprint, SFE for filebacked pages utilizes offline profiling results as well as a lightweight runtime module to estimate the mostly invariant switch footprint of file-backed pages. On the other hand, SFE for anonymous pages is designed to track a dynamic switch footprint of anonymous pages by gradually promoting pages that are likely to be fetched again during the next switch.</p><p>These estimators generate a set of target pages for prepaging, which are retrieved at the beginning of an Prepaging Manager is responsible for prepaging threads that are used to fetch target pages from a prepaging target table. It monitors a timing signal that notifies the start and the end of the application switch event from the Android framework. Prepaging Manager promptly wakes up inactive prepaging threads for the switched application when it receives a start signal for application switch, and then it initiates prepaging. Multiple prepaging threads are created according to the number of available CPU cores and run in parallel with application threads to fully utilize the available system resources such as CPU cycles and disk bandwidth. Once they finish issuing fetch requests for all the pages from the prepaging target table, the prepaging manager makes them sleep again until the next switch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Switch Footprint Estimator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Switch Footprint Analysis</head><p>To effectively estimate the targets for prepaging, it is important to understand the characteristics of the switch footprint: a set of pages that are accessed during the switch time. For this purpose, we perform an experiment that exhaustively records all pages accessed across 10 switches for each application (experimental details are available in Section 6.1). For this we cleared the access bit of all present PTE in the address space just before the switch and then checked them right after the switch is completed.</p><p>File-backed Pages. Figure <ref type="figure">5</ref> shows the switch footprint composition for file-backed pages. The stacked bar shows how many times pages are accessed over the 10 application switches. The switch footprint is largely invariant in this case.</p><p>On average, about 75% of pages are accessed 9 or 10 times and only 10% are accessed fewer than five times. This highly invariant access pattern of the file-backed pages is due to the fact that a large part of codes and shared library files keep being loaded for the execution of an application. Anonymous Pages. Figure <ref type="figure">6</ref> shows the switch footprint composition for anonymous pages. The access pattern is not as invariant as file-backed pages. About 44% of the anonymous pages are accessed 9 or 10 times across 10 switches. The portion of the invariant (i.e., always accessed) pages is much smaller as the set of accessed anonymous pages easily changes when the application context changes.</p><p>Implications. As the characteristics of the switch footprint for anonymous pages and file-backed pages differ, so should their switch footprint estimators. Estimation for file-backed pages can exploit the fact that file-backed pages are highly invariant to minimize the runtime overhead. On the other hand, estimation for anonymous pages needs to rely more on the runtime information so that it can correctly track dynamically changing switch footprints across switch events. Still, the runtime overhead of tracking the switch footprint for anonymous pages is relatively low as the number of anonymous pages in the switch footprint is much smaller than that of the file-backed pages, as shown in Figure <ref type="figure" target="#fig_3">7</ref>. The rest of this section discusses the SFE design for both file-backed pages (Section 4.2) and anonymous pages (Section 4.3). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Estimator for File-Backed Pages</head><p>As shown in Figure <ref type="figure">5</ref>, a major portion of file-backed pages accessed during the application switch are invariant across switches. To exploit this characteristic, SFE for file-backed pages first performs offline profiling to identify the set of potential candidates for prepaging, and then later utilizes minimal runtime information to maintain a concise set of prepaging targets, as shown in Figure <ref type="figure">8</ref>.</p><p>Offline Profiling. The estimator performs offline profiling to obtain a set of prepaging candidates. For this purpose, we measure the file-backed pages that are accessed during ten switch events for each app, as in Figure <ref type="figure">5</ref>. Then, pages accessed more than eight times (out of ten trials) are considered to be frequently accessed. The resulting set of pages is stored as a file (Offline Candidate Table ). Specifically, as shown in Figure <ref type="figure">8</ref>, the profiled result is stored as a map, where a filename is a key and a list of pairs (offset, len) is a Each pair represents [offset, offset + len) pages within a file that are accessed during an application switch (we call it an extent in the rest of this paper). Later, the profiled result is reloaded at the launch time of this application. Fault Logging. Fault logging happens at every switch event. Specifically, SFE logs the inode and page indices of all faulted extents received from the kernel until the end of the switch time. This is stored in a fault buffer, which is later utilized by the estimator after the end of the switch time to update its prepaging targets.</p><p>Prepaging Target Management -Insertion. Once the switch finishes, a background thread performs prepaging target management, exploiting the information from the offline profiling and the fault logging. Prepaging Target </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Estimator for Anonymous Pages</head><p>As shown in Figure <ref type="figure">6</ref>, the set of anonymous pages accessed during the application switch changes much more frequently than files. Moreover, anonymous pages are allocated whenever the application is launched, and thus offline profiling is not helpful for identifying prepaging candidates. To effectively track the switch footprint for anonymous pages, we focus on runtime analysis, unlike the case of file-backed pages (Section 4.2  table that it belongs to. But, whenever a page is accessed, the timeout counter of an identifier is reset to the default timeout counter value (e.g., 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Prepaging Manager</head><p>Whenever an application switch event occurs, ASAP's Prepaging Manager wakes up threads. They prefetch pages in the Prepaging Target Table, which eventually constructs corresponding PTEs. We apply different prepaging policies to anonymous pages and file-backed pages as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Prepaging Anonymous Pages</head><p>Prepaging of anonymous pages requires decompressing swapped out pages in the compressed in-memory swap space. Hence, the task is compute-intensive, and the task should be carefully scheduled not to incur CPU contention between application threads and prepaging threads. Although the CPU utilization is low, as we reported in Section 2.3, the application threads can demand more CPU resources due to reduced page fault events by prepaging operations.</p><p>To this end, the prepaging manager maintains a set of threads for anonymous page prepaging. We pinned a thread on each core, and we assigned the lowest priority (i.e., SCHED _ IDLE <ref type="bibr" target="#b12">[17]</ref>) to them. This allows us to opportunistically utilize the surplus CPU resources for the prepaging of anonymous pages while not incurring any CPU contention with the application threads.</p><p>The distribution of prepaging work is done in a work sharing manner. Each thread retrieves a batch (16 pages) from the Prepaging Target Table, and then conducts the prepaging operations for pages in the batch. Specifically, for each virtual page in the batch, each thread checks whether the virtual page is present in the process's address space. If nonpresent, it issues a swap-in operation for the virtual page to the swap subsystem (i.e., the swap cache). The swap-in operation eventually becomes the decompression operation in the in-memory compressed swap device. After the target page is decompressed, the thread finally makes the corresponding PTE point to the swapped-in page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Prepaging File-backed Pages</head><p>The prepaging manager maintains another set of threads for prepaging file-backed pages. However, file-backed pages impose a higher miss penalty than anonymous pages due to long disk I/O time. Therefore, we take a different prepaging policy for file-backed pages as follows.</p><p>First, a file is a unit of prepaging work distribution. Each thread is assigned a file from the Prepaging Target Table, and then prefetches pages of the file. For the prepaging operation, each thread issues asynchronous page cache read operations for the corresponding extents in the Prepaging Target Table . 
Note that the Prepaging Target Table estimates pages are not only accessed through page faults but are also accessed via read/write system calls. Hence, not all prefetched pages need to be mapped in the process's virtual address space. Prepaging the pages in the Prepaging Target Table places fetched pages into the page cache, and thus page faults can still occur for those prefetched pages. However, their fault handling cost is light when the corresponding pages are in the page cache (i.e., minor faults in Linux). When a page fault occurs for a file-backed page, the page fault handler retrieves pages surrounding the missing page from the page cache and maps them all together in the virtual memory to reduce page fault frequency; hence performing prepaging.</p><p>Second, we dedicate at least one thread to continuously perform prepaging of large file-backed pages. Figure <ref type="figure" target="#fig_5">10</ref> shows the cumulative distribution of accessed pages of files in the switch footprint of the applications. As shown in the figure, about 90% of the total number of accessed pages is part of the top 15% of large files. Thus, we can expect spatial locality of such accesses to large files. This indicates that if those files are assigned to the prefetching threads with the SCHED _ IDLE priority, pages of the files are not likely to be prefetched on time due to the lowest CPU scheduling priority. To avoid this problem, we designate one thread with SCHED _ NORMAL priority running on the big core to be in charge of prefetching pages of large files. Considering the big-LITTLE heterogeneity of the CPU cores in mobile systems, the thread is assigned to run on a big core to maximize the prefetching performance. We have empirically found that this configuration is effective in reducing the miss ratio as well as the CPU contention with application threads.</p><p>Lastly, during the prefetching of file-backed pages, file metadata should be carefully handled. Unless file metadata (e.g., logical block addresses of file pages) is in memory, the metadata retrieval incurs additional delay, which in turn degrades the effectiveness of prefetching <ref type="bibr" target="#b18">[23]</ref>. This problem exacerbates because of our extent-based prefetching. The metadata read I/O can stay behind a large prefetching I/O request, thereby blocking the prefetching threads as well as the application threads. To avoid this problem, our scheme attempts to read metadata blocks before prefetching pages of a corresponding file. The metadata block reads are done by accessing file pages with a large stride in file offset, 512 pages in our case, because a direct block contains LBAs of 512 data blocks in F2FS <ref type="bibr" target="#b17">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>In this section, we evaluate the effectiveness of ASAP. Section 6.1 describes the evaluation methodology and workloads. Then, we evaluate the latency benefits of our proposal in Section 6.2. Section 6.3 analyzes the accuracy of the switch footprint estimator. We evaluate the efficacy of the prepaging manager by considering improvement of the effective disk bandwidth and CPU utilization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Methodology</head><p>Switching Latency Measurement. To measure the application switching latency, we used the am command in the Android debug bridge (adb) <ref type="bibr" target="#b0">[2]</ref>. This command starts a selected application and reports two types of switching latency. One is latency from a user's touch to the first rendering, and the other one is latency from a user's touch to the full rendering <ref type="bibr">[5]</ref>. The latter is reported only when the application developer implements the debug callback. The information is reported only by the YT application among eight benchmark applications. Thus, we use the time to the initial rendering as a metric. For the YT application, we observe that the additional latency overhead of the full rendering is less than 5% of the switch latency (10-20 ms). Users could also start to interact with applications in the middle of the rendering <ref type="bibr" target="#b11">[16]</ref>. The actual latency overhead is expected to be insignificant when the performance benefits of ASAP are considered. This justifies our usage of the time to the initial rendering as the metric for evaluation. System Configuration. For our evaluation, we use Google Pixel 4 and Pixel 3a, which represent high-end and mid-end devices, respectively. Table <ref type="table" target="#tab_4">1</ref> describes their specifications. We implement ASAP in Android 10. When measuring the application switch overhead under memory pressure, we consider two aspects for our experimental methodology.    Browse questions and answers Table <ref type="table">3</ref>: Chosen 3 application test sequences. Sequence 1 YT-CH-CC-AB-NY-QR-FB-TW Sequence 2 QR-NY-CH-CC-YT-TW-FB-AB Sequence 3 AB-FB-QR-TW-CC-CH-YT-NY First, we favor the compression based swap approach over the lmkd, which often acts first to secure free memory and prevents the system from being under memory pressure. Note that Android currently enables both features by default. We disable the lmkd for our evaluation to solely analyze the performance impact on application switch under memory pressure.</p><p>Second, users show different application usage patterns such as a spectrum of day-to-day use applications and the use of multitasking features. These lead to different memory usage patterns even among smartphone users who have the same devices.</p><p>In this work, thus, we focus on evaluating the memory pressure impact of the application switch for a fixed set of a wide spectrum of top rated applications (refer to Table <ref type="table" target="#tab_5">2</ref>). We enable memory ballooning by considering the entire footprint of the target applications instead of enabling numerous applications to cause memory pressure for the target devices. The effective memory size of both Pixel 4 and Pixel 3a is 4GB. Throughout our evaluation, we refer to the switch time measured on this configuration as the baseline switch time.</p><p>Workloads and Automation of Tests. In order to reduce the run-to-run variation in the experimental results, we carefully devise an automation program that closely mimics a set of pre-determined user interactions with adb. For example, the Facebook (FB) usage pattern contains scrolling down the main news feed, searching for user profiles, and watching their timelines. Another example would be YouTube (YT), where our program searches and watches different video clips.</p><p>The details of the usage patterns are listed in Table <ref type="table" target="#tab_5">2</ref>.</p><p>After execution of a certain application, e.g., Candy Crush (CC), we switch to the next application, e.g., TW, by following a pre-determined sequence of applications. As there are 8! available application sequences for eight applications, we chose three random sequences to evaluate ASAP (Table <ref type="table">3</ref>). The start and end time of the application switching operation is informed by the Android activity manager <ref type="bibr">[1,</ref><ref type="bibr">12]</ref>. We iterate the selected sequence 10 times and measure the application switch time. With this user interaction automation program, we repetitively conduct the same evaluation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Application Switch Latency</head><p>Figure <ref type="figure" target="#fig_7">11</ref> presents the speedup of ASAP based on Pixel 4 and Pixel 3a, respectively, for 8 applications. We also evaluate the speedup by selectively enabling prepaging for either anonymous pages or file-backed pages. Compared to the baseline, ASAP shows an average of 22.2% and 28.3% performance improvement, and a maximum of 33.3% (YT) and 35.7% (TW) on Pixel 4 and Pixel 3a, respectively. We observe 6.8% and 14.6% performance improvement on each device on average when ASAP performs prepaging only for anonymous pages (Anon-only). Among the eight applications, YT and TW show the most noticeable latency reduction on Pixel 4 and Pixel 3a, respectively. With prepaging for file-backed page only (File-only), the latency is reduced by 18.3% and 14.4% on average. Here, YT and CH show a substantial latency reduction on each device.</p><p>When both SFEs are enabled (ASAP), we observe additional performance benefits for most cases as expected. However, in NY and QR on Pixel 4, integrating both SFEs does not further reduce their switch latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Estimator Efficiency</head><p>Figure <ref type="figure" target="#fig_0">12</ref> presents the efficiency of the proposed switch footprint estimators for both Anonymous SFE and File-backed SFE. Since we observe similar performance trends on both devices, we will only present the results on Pixel 4 in the rest of this section. Precision is defined as a fraction of correctly prepaged pages among entire prepaged pages. Recall is defined as a fraction of correctly prepaged pages among all faulting pages during the switch time when the baseline system is considered. Anonymous SFE shows an average of 68.4% precision and 60.4% recall. File-backed SFE shows an average of 79.3% precision and 52.2% recall. The Switch Footprint Estimator for file-backed pages shows better precision relative to that of the Switch Footprint Estimator for anonymous pages. The difference comes from the fact that the switch footprint of file-backed pages is more static, as described in Figure <ref type="figure">5</ref>.</p><p>The gap between precision and recall comes from the coverage of the prepaging target tables. Note that both precision and recall have the same numerator value while the denominator of recall can cover more pages that have not been fetched by the proposed prepaging scheme. We see a larger gap tween the precision and the recall of file-backed page prepaging relative to the gap of anonymous page prepaging. This could result from the limited coverage of the candidate pages that is based on the static profiling. The static profiling approach may not capture the entire set of pages that are likely to cause faults at runtime. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Resource Utilization</head><p>To show the efficacy of ASAP on prepaging, we evaluated the changes in CPU and memory bandwidth utilization on Pixel 4. The bandwidth utilization is computed as the ratio of achieved file read throughput to the maximum sequential throughput measured in fio <ref type="bibr">[10]</ref>. As depicted in Figure <ref type="figure" target="#fig_1">13</ref>, ASAP eagerly allocates threads for decompression, which increases the CPU utilization to 1.18? on average over the total switch time, compared to the baseline switch. We also notice the maximum of 1.35? utilization increase. The CPU has been under utilized at the beginning of the application switch. In most cases, the anonymous prepaging threads have a large window of opportunity to fully exploit the available CPU resources. Therefore, when ASAP is enabled, the CPU utilization improves at the early stages of switching. Because the throughput of zram actually scales depending on the number of CPU cores, the anonymous prepaging threads can prepage anonymous pages at great speed. For most applications, prepaging threads finish at around the first 30% of normalized switching time. After that, the CPU utilization follows the CPU utilization pattern of the baseline. On the same page as the CPU, ASAP also improves the I/O bandwidth by 25.2% on average, as shown in Figure <ref type="figure">14</ref>. In most cases, we observe a noticeable increase in the I/O bandwidth at the early stages of switching and the maximum achieved bandwidth is also higher than that of the baseline. ASAP does not induce significant improvement over the baseline in the case of AB. This is because AB is a highly parallel application with high I/O utilization. Therefore, the I/O bandwidth improvement from our asynchronous I/O threads is limited. The empirical analysis substantiates that ASAP efficaciously exploits the resources at the beginning of the switch to considerably reduce  the application switch latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Efficiency of Core Scheduling</head><p>To quantify the effect of core scheduling of the file prepaging threads (Section 5.2), we compare our policy on Pixel 4 with four other static policies: big 1-core, big 4-core, LITTLE 1-core, and LITTLE 4-core. For example, in LITTLE 4-core, four file prepaging threads are scheduled on the four LITTLE cores, and the threads are assigned the SCHED _ NORMAL priority. And we enabled only file prepaging to reduce performance deviation. Figure <ref type="figure" target="#fig_10">15</ref> shows the delta of application switch latency (the latency of the four naive policies minus the latency of our scheme). Each naive policy shows 1.06?, 1.05?, 1.02?, 1.04? times slower than ours on average. Hence, our performance advantage comes from the fact that our policy is versatile to different situations. For example, the big 4-core policy showed 14% and 13% better performance than our policy on YT and QR, however its performance falls dramatically on AB since AB utilizes both CPU and disk bandwidth intensively, so file prepaging threads contended a lot with AB's application threads. On the other hand, the LITTLE 4-core policy is better than ours in QR and AB, but it is vulnerable to applications requiring heavy file I/O because of the slow prepaging speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Overhead</head><p>Anonymous SFE maintains an Online Candidate Access bit logging (clearing access bits at the beginning and inspecting them at the end of the switch time) extends the switch time by up to 14ms. Also, prepaging target management operations which opportunistically runs between the switch events takes 40ms CPU time in the worst case.</p><p>Finally, mis-prediction events result in extra fetch overhead, which could increase the energy consumption. On average, ASAP fetches an extra 10MB for anonymous pages and file-backed pages, respectively. Also the peak throughput of decompression and the disk bandwidth are 2GB/s and 600MB/s on Pixel 4, respectively. Therefore, each extra fetch takes tens of milliseconds. When the peak power of UFS 2.1 [33] and TDP of Snapdragon 855 <ref type="bibr" target="#b27">[32]</ref> are considered, these extra fetches require negligible overhead. Actually, we expect ASAP to save the energy consumption of the entire device including other components (e.g., display) because ASAP reduces the total switch latency. Thus, this marginal energy overhead can be easily offset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Efficient Memory Management in Mobile Systems. Modern mobile systems reclaim free pages by killing the least essential applications (e.g., low memory killer in Android <ref type="bibr" target="#b21">[26]</ref>). The traditional low memory killer selects a victim process by considering the priority and the number of pages of application only. SmartLMK <ref type="bibr" target="#b14">[19]</ref> proposes to kill an application to minimize the expected user-perceived application performance by carefully considering application usage statistics and application launch times. However, killing an application process is the most aggressive policy in memory reclamation <ref type="bibr" target="#b3">[6]</ref>, and whenever a killed application is launched again, it takes a large amount of computation and I/O operations, which can increase the user-perceived launch latency and the energy consumption of mobile devices <ref type="bibr" target="#b16">[21,</ref><ref type="bibr" target="#b19">24]</ref> To end this senseless killing, Marvin <ref type="bibr" target="#b16">[21]</ref> swaps out predicted unlikely-to-be-used pages to disks using ahead-oftime swap by modifying Android runtime (ART). Similarly, SmartSwap <ref type="bibr" target="#b29">[35]</ref> includes process-level early page swap based on the prediction result but by addressing kernel codes. A2S <ref type="bibr" target="#b13">[18]</ref> combines the low memory killer and the compressed swap together by carefully selecting the victim pages for swapout and the victim process to kill. Acclaim <ref type="bibr" target="#b20">[25]</ref> prioritizes pages of foreground processes over those of background processes during swapping. <ref type="bibr">Kwon et al. [20]</ref> propose to swap-out GPU buffers of background processes to relieve memory pressure on mobile devices. Chae et al. <ref type="bibr" target="#b5">[8]</ref> propose to extend the swap space of mobile systems to the cloud. Accelerating Application Launch. Numerous studies have been conducted to shorten the application launch time, and most have tried to prefetch data effectively <ref type="bibr">[12,</ref><ref type="bibr" target="#b10">15,</ref><ref type="bibr" target="#b23">28,</ref><ref type="bibr" target="#b25">30,</ref><ref type="bibr" target="#b28">34]</ref>. FAST <ref type="bibr" target="#b10">[15]</ref> profiles I/O sequences during application launches and uses the profiled sequences for data prefetching.</p><p>FALCON <ref type="bibr" target="#b28">[34]</ref> adopts machine learning to predict the users' application usage pattern. It predicts the next application a user is going to use and preloads the contents of the predicted applications. Nagarajan et al. <ref type="bibr" target="#b23">[28]</ref> uses collaborative filtering to predict the impending applications while PREPP <ref type="bibr" target="#b25">[30]</ref> uses prediction by the partial matching technique. <ref type="bibr">IORap [12]</ref> in Android 11 profiles the required I/O during several cold-runs of an application and predicts which I/O will be required and does it in advance. These works only focus on predicting applications or I/O patterns during application launch events. However, our work predicts I/O patterns or memory access footprint during application switch events. Efficiently Utilizing Disk I/O Bandwidth. The disk I/O performance is important to the user-perceived application performance. Accordingly, the efficient use of disk I/O is important. SmartIO <ref type="bibr" target="#b24">[29]</ref> discovers that read I/O operations are penalized by write I/O operations and proposes to prioritize read I/O operations over write ones. Joo et al. <ref type="bibr" target="#b9">[14]</ref> finds that swap I/O patterns for page faults are not efficient due to their small and random I/O request patterns. To overcome these inefficiencies, they insert pads to build large sequential I/O requests, which is more efficient in flash-based disks. FastTrack <ref type="bibr" target="#b7">[11]</ref> prioritizes I/O requests from foreground applications over those from background ones throughout the entire I/O stack. These approaches are complementary to our work in terms of improving the I/O efficiency during disk access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>The goal of this paper is to improve user experience on mobile devices, focusing on the application switch, which is one of the most important user interactions. We proposed (ASAP), an adaptive prepaging scheme that accurately retrieves pages ahead of time that are expected to be accessed during application switch by fully exploiting the available system resources. Our experimental results based on real-world Android OS applications show that ASAP can reduce the application switch latency under memory pressure by 22.2% and 28.3% on representative high-end and mid-end smartphones, respectively. While ASAP was evaluated in the context of the application switch, we believe that it can easily be extended to reducing application launch time as well.</p><p>We open sourced Linux kernel we modified for ASAP. The code is available at https://github.com/SNU-ARC/atc21 -asap-kernel.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: CPU and disk bandwidth utilization of a high-end device (Pixel 4) during the switch time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: CPU and disk bandwidth utilization of a mid-end device (Pixel 3a) during the switch time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Switch locality analysis for file-backed pages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Switch footprint of anonymous and file-backed pages across different applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Switch footprint estimator for anonymous pages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Cumulative number of accessed pages CDF of files across the various applications during switch from one application to another one. Files are sorted by size. 100% indicates the largest file.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Normalized speedup of application switching latency on (a) Pixel 4 and (b) Pixel 3a. Numbers in parentheses indicate absolute switching latency of the baseline system in ms. Error bar shows standard deviation over different sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Figure 12: Switch footprint estimator performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :Figure 14 :</head><label>1314</label><figDesc>Figure 13: CPU utilization. X-axis is a timeline normalized to baseline's switch time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Switching latency changes depending on different core scheduling policy compared to ASAP's core scheduling policies. Positive latency change means that the static policy is worse than ASAP's policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,-9.00,-10.01,630.00,255.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="1,-9.00,543.00,630.00,259.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Tablestoresinformation for extents that are to be fetched by the prepaging threads. Ideally, we should insert only those extents that are likely to be fetched in the near future. To identify such an extent, the estimator first inspects an extent in the fault buffer and checks if the extent is also found in the Offline Candidate Table.If so, the estimator inserts the corresponding entries into the Prepaging Target Table.Prepaging Target Management -Extent Merging. The Prepaging Target Table may have multiple extents on the same file. In such a case, if two extents are close to each other (e.g., the end of one extent is less than 16 pages apart from the start of the other extent), we merge those two extents and create a larger extent that covers both. This is to avoid issuing multiple fragmented I/O requests and instead issue a single, sequential large I/O request, which is often handled much more efficiently. Prepaging Target Management -Eviction. Eviction from a Prepaging Target Table happens when the fetched page turns out to be not utilized during a switch time. Specifically, the estimator checks the mapcount of each fetched page after the switch, and removes the page from the Prepaging Target Table if the mapcount is zero. When a page is part of an extent, the extent is divided into two smaller extents.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>). Policies of the estimator are depicted in Figure 9. Fault Logging. During the application switch time, like the estimator for file-backed pages, the Switch Footprint estimator for anonymous pages logs all anonymous page faults. Fault information is logged at a fault buffer for later usage. Access Logging. To track access information during switch time, this estimator clears the access bit of every PTE represented by each page identifier in both the Prepaging Target Table and the Online Candidate Table before every application switch time. Then, right before the end of the switch time, the access bits of all pages in both tables are again checked to identify a set of pages that are accessed during switch time. Prepaging Target Management -Check &amp; Insertion. After the application switch time, this estimator first checks if each page in the fault buffer is not already present in the Online Candidate Table nor the Prepaging Target Table. If there are pages that are not already present in the tables, they are inserted into the Online Candidate Table. Prepaging Target Management -Promotion. Also, the estimator checks if each page in Online Candidate Table has been accessed during the switch time by inspecting the access log. If a page has been accessed during the last switch time, the page in the Online Candidate Table is then promoted to the Prepaging Target Table. Prepaging Target Management -Eviction. Every page in both the Online Candidate Table and the Prepaging Target Ta-</figDesc><table><row><cell></cell><cell>Offline Profiling</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Runtime</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Offline Candidate Table</cell><cell>Fault Buffer</cell><cell></cell><cell cols="2">Offline Candidate Table</cell><cell></cell><cell cols="2">Prepaging Target Table</cell></row><row><cell></cell><cell>file name</cell><cell>extent</cell><cell>(inum, (offset, len))</cell><cell></cell><cell>inum</cell><cell>extent</cell><cell></cell><cell>inum</cell><cell>extent</cell></row><row><cell></cell><cell>/foo.vdex</cell><cell>(1,4),?</cell><cell>(32, (2, 7))</cell><cell></cell><cell>32</cell><cell>(1,4),?</cell><cell></cell><cell>32</cell><cell>(2,7), ?</cell></row><row><cell>Insertion T% (e.g 80%) of the time More than</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>entries Inspection Matching</cell><cell>No match Eviction ?</cell><cell>?</cell><cell>inum &amp; extents Insertion Matching</cell><cell>?</cell><cell>Entries Eviction ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>found</cell><cell></cell><cell></cell><cell></cell><cell>without</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mapping</cell></row><row><cell></cell><cell></cell><cell cols="5">Figure 8: Switch footprint estimator for file-backed pages.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Fault Buffer</cell><cell cols="3">Online Candidate Table</cell><cell></cell><cell cols="3">Prepaging Target Table</cell></row><row><cell></cell><cell cols="2">(VA, ASID)</cell><cell>VA</cell><cell>ASID</cell><cell>timout</cell><cell></cell><cell>VA</cell><cell>ASID</cell><cell>timout</cell></row><row><cell></cell><cell cols="2">(0xa2df44e2, 32)</cell><cell>0xd12a58c9</cell><cell>32</cell><cell>3</cell><cell></cell><cell>0x1004ed8a</cell><cell>45</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0x93df12ab</cell><cell>76</cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Insertion Faulted pages</cell><cell>?</cell><cell>Insertion If not in</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>Promotion Accessed</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell cols="2">either tables</cell><cell cols="2">Entries Eviction</cell><cell>time during switch</cell><cell></cell><cell cols="2">Entries Eviction</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">triggering</cell><cell></cell><cell></cell><cell cols="2">triggering</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>timeout</cell><cell></cell><cell></cell><cell></cell><cell>timeout</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0x93df12ab</cell><cell>76</cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>ble has its own timeout counter, which is the number of switch events a page can experience before getting evicted from a table. The timeout counter (e.g., 5) of a page is decremented after every switch time. If a specific page is not accessed until the timeout counter reaches zero, it is evicted from the</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">: Device Specifications</cell></row><row><cell cols="2">Device</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Google Pixel 4</cell><cell>Google Pixel 3a</cell></row><row><cell></cell><cell>CPU</cell><cell></cell><cell cols="6">Octa-core Qualcomm Snapdragon 855</cell><cell>Octa-core Qualcomm Snapdragon 670</cell></row><row><cell cols="2">DRAM</cell><cell></cell><cell></cell><cell cols="5">6GB LPDDR4x (eff. 4GB)</cell><cell>4GB LPDDR4x</cell></row><row><cell cols="2">Storage</cell><cell></cell><cell></cell><cell></cell><cell cols="4">64GB UFS 2.1</cell><cell>64GB eMMC 5.1</cell></row><row><cell></cell><cell>OS</cell><cell></cell><cell cols="7">Android 10.0.0 (r41) with Linux kernel 4.14 Android 10.0.0 (r41) with Linux kernel 4.9</cell></row><row><cell></cell><cell>zram</cell><cell></cell><cell></cell><cell></cell><cell cols="3">2GB (default)</cell><cell></cell><cell>2GB (default)</cell></row><row><cell>40</cell><cell></cell><cell></cell><cell cols="2">Anon-only</cell><cell cols="2">File-only</cell><cell>ASAP</cell><cell></cell></row><row><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Baseline</cell><cell>AB</cell><cell>CC</cell><cell>NY</cell><cell>YT</cell><cell>FB</cell><cell>TW</cell><cell>CH</cell><cell>QR Geom</cell></row><row><cell>latency (ms)</cell><cell cols="8">(894) (558) (291) (463) (373) (334) (485) (353) (441)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Applications and automated interactions to change contexts.</figDesc><table><row><cell>Application</cell><cell>Automated Usage Patterns</cell></row><row><cell>Angry Bird (AB)</cell><cell>Play a stage</cell></row><row><cell>Candy Crush (CC)</cell><cell>Play a stage</cell></row><row><cell cols="2">New York Times (NY) Browse and read articles</cell></row><row><cell>Youtube (YT)</cell><cell>Watch videos</cell></row><row><cell>Facebook (FB)</cell><cell>Browse and read posts</cell></row><row><cell>Twitter (TW)</cell><cell>Browse and read posts</cell></row><row><cell>Chrome (CH)</cell><cell>Browse keywords</cell></row><row><cell>Quora (QR)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table, Prepaging Target Table, and anonymous fault buffer. Their peak size for 8 applications is 1MB, 2.5MB, and 0.5MB, respectively. The size of the Offline Candidate Table, Prepaging Target Table and file fault buffer used by File-backed SFE is 1.5MB, 0.2MB, and 0.5MB, respectively, at their peak respec-tively. On average ASAP uses about 800KB per application.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="126" xml:id="foot_0"><p>2021 USENIX Annual Technical Conference USENIX Association</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Lin Zhong</rs> for shepherding this paper. This work was supported by a research grant from <rs type="funder">Samsung Research Funding &amp; Incubation Center of Samsung Electronics</rs> under Project Number <rs type="grantNumber">SRFC-IT1702-52</rs>. <rs type="person">Hongil Yoon</rs> and <rs type="person">Jae W. Lee</rs> are the corresponding authors.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_297baYd">
					<idno type="grant-number">SRFC-IT1702-52</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://developer.android.com/studio/command-line/adb" />
		<title level="m">Android Debug Bridge</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://developer.android.com/topic/performance/memory-management" />
		<title level="m">Memory allocation among processes</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="https://sensortower.com/blog/ios-game-size-growth-2020" />
		<title level="m">The Average Size of the U.S. App Store&apos;s Top Games Has Grown 76% in Five Years</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Understanding the Linux Kernel: from I/O ports to process management</title>
		<author>
			<persName><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Bovet</surname></persName>
		</author>
		<author>
			<persName><surname>Cesati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The information visualizer, an information workspace</title>
		<author>
			<persName><forename type="first">K</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">G</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jock</forename><forename type="middle">D</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><surname>Mackinlay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;91</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems, CHI &apos;91<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="181" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CloudSwap: A cloud-assisted swap mechanism for mobile devices</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 16th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="462" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Measuring smartphone usage and task switching with log tracking and self-reports</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaheen</forename><surname>Kanthawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Kononova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prabu</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mobile Media &amp; Communication</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">FastTrack: Foreground app-aware I/O management for improving user experience of android smartphones</title>
		<author>
			<persName><forename type="first">Sangwook</forename><surname>Shane Hahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inhyuk</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donguk</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 USENIX Annual Technical Conference (USENIX ATC 18)</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<ptr target="https://developer.apple.com/videos/play/wwdc2018/416/" />
		<title level="m">iOS Memory Deep Dive</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Enlarging I/O size for faster loading of mobile applications</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Embedded Systems Letters</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="50" to="53" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast: Quick application launch on solid-state drives</title>
		<author>
			<persName><forename type="first">Yongsoo</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhee</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangsoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX Conference on File and Stroage Technologies, FAST&apos;11</title>
		<meeting>the 9th USENIX Conference on File and Stroage Technologies, FAST&apos;11<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving user perceived page load times using gaze</title>
		<author>
			<persName><forename type="first">Conor</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihoon</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aruna</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><forename type="middle">R</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17)</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2017-03">March 2017</date>
			<biblScope unit="page" from="545" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><surname>Scheduler</surname></persName>
		</author>
		<ptr target="https://www.kernel.org/doc/html/latest/scheduler/sched-design-CFS.html#scheduling-policies" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Application-aware swapping for mobile systems</title>
		<author>
			<persName><forename type="first">Sang-Hoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinkyu</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Soo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Embed. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5s</biblScope>
			<date type="published" when="2017-09">September 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SmartLMK: A memory reclamation scheme for improving user-perceived app launch time</title>
		<author>
			<persName><forename type="first">Sang-Hoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinkyu</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Soo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungryoul</forename><surname>Maeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Embed. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Managing GPU buffers for caching more apps in mobile systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on Embedded Software (EMSOFT)</title>
		<meeting>the 2015 International Conference on Embedded Software (EMSOFT)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End the senseless killing: Improving memory management for mobile operating systems</title>
		<author>
			<persName><forename type="first">Niel</forename><surname>Lebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 USENIX Annual Technical Conference (USENIX ATC 20)</title>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="873" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">F2FS: A new file system for flash storage</title>
		<author>
			<persName><forename type="first">Changman</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongho</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jooyoung</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangyeun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Conference on File and Storage Technologies (FAST 15)</title>
		<meeting><address><addrLine>Santa Clara, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2015-02">February 2015</date>
			<biblScope unit="page" from="273" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Asynchronous I/O stack: A low-latency kernel I/O stack for ultra-low latency SSDs</title>
		<author>
			<persName><forename type="first">Gyusun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seokha</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonsuk</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><forename type="middle">W</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinkyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 USENIX Annual Technical Conference (USENIX ATC 19)</title>
		<meeting><address><addrLine>Renton, WA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2019-07">July 2019</date>
			<biblScope unit="page" from="603" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Context-aware application scheduling in mobile systems: What will users do and not do next?</title>
		<author>
			<persName><forename type="first">Joohyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Euijin</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ness</forename><forename type="middle">B</forename><surname>Shroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing, UbiComp &apos;16</title>
		<meeting>the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing, UbiComp &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1235" to="1246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Acclaim: Adaptive memory reclaim to improve user experience in android systems</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachata</forename><surname>Ausavarungnirun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tei-Wei</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 USENIX Annual Technical Conference (USENIX ATC 20)</title>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="897" to="910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Low</forename><surname>Memory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Killer</forename><surname>Daemon</surname></persName>
		</author>
		<ptr target="https://source.android.com/devices/tech/perf/lmkd" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<ptr target="https://www.forbes.com/sites/steveolenski/2016/11/10/why-brands-are-fighting-over-milliseconds" />
		<title level="m">Why Brands Are Fighting Over Milliseconds</title>
		<imprint/>
	</monogr>
	<note>sh=4f52e2f14ad3</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Which app will you use next? collaborative filtering with interactional context</title>
		<author>
			<persName><forename type="first">Nagarajan</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyuk</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM Conference on Recommender Systems, RecSys &apos;13</title>
		<meeting>the 7th ACM Conference on Recommender Systems, RecSys &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="201" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reducing smartphone application delay through read/write isolation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijiang</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual International Conference on Mobile Systems, Applications, and Services, MobiSys &apos;15</title>
		<meeting>the 13th Annual International Conference on Mobile Systems, Applications, and Services, MobiSys &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="287" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Practical prediction and prefetch for faster access to applications on mobile phones</title>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Parate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>B?hmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing, UbiComp &apos;13</title>
		<meeting>the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing, UbiComp &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="275" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two billion devices and counting</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Knies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6" to="21" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<ptr target="https://www.notebookcheck.net/Qualcomm-Snapdragon-855-SoC-Benchmarks-and-Specs.375436.0.html" />
	</analytic>
	<monogr>
		<title level="j">Qualcomm Snapdragon</title>
		<imprint>
			<biblScope unit="volume">855</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast app launching for mobile devices using predictive user context</title>
		<author>
			<persName><forename type="first">Tingxin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Kansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Mobile Systems, Applications, and Services, MobiSys &apos;12</title>
		<meeting>the 10th International Conference on Mobile Systems, Applications, and Services, MobiSys &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="113" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Smartswap: High-performance and user experience friendly swapping in mobile systems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinting</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m">ACM/EDAC/IEEE Design Automation Conference (DAC)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
