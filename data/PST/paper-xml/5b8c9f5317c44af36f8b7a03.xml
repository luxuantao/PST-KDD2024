<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sentic LSTM: a Hybrid Network for Targeted Aspect-Based Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yukun</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>50 Nanyang Ave</addrLine>
									<postCode>639798</postCode>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haiyun</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>50 Nanyang Ave</addrLine>
									<postCode>639798</postCode>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tahir</forename><surname>Khan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Alef Education Consultancy</orgName>
								<address>
									<settlement>Abu Dhabi</settlement>
									<country key="AE">United Arab Emirates</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
							<email>cambria@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<addrLine>50 Nanyang Ave</addrLine>
									<postCode>639798</postCode>
									<country>Singapore, Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">•</forename><forename type="middle">Amir</forename><surname>Hussain</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Natural Sciences</orgName>
								<orgName type="institution">University of Stirling</orgName>
								<address>
									<postCode>FK9 4LA</postCode>
									<settlement>Stirling</settlement>
									<country key="GB">Scotland, UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cogn</forename><surname>Comput</surname></persName>
						</author>
						<title level="a" type="main">Sentic LSTM: a Hybrid Network for Targeted Aspect-Based Sentiment Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">23080CAEAAD5EC61E62FA7DDB6093543</idno>
					<idno type="DOI">10.1007/s12559-018-9549-x</idno>
					<note type="submission">Received: 21 October 2017 / Accepted: 20 February 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sentiment analysis has emerged as one of the most popular natural language processing (NLP) tasks in recent years. A classic setting of the task mainly involves classifying the overall sentiment polarity of the inputs. However, it is based on the assumption that the sentiment expressed in a sentence is unified and consistent, which does not hold in the reality. As a finegrained alternative of the task, analyzing the sentiment towards a specific target and aspect has drawn much attention from the community for its more practical assumption that sentiment is dependent on a particular set of aspects and entities. Recently, deep neural models have achieved great successes on sentiment analysis. As a functional simulation of the behavior of human brains and one of the most successful deep neural models for sequential data, long short-term memory (LSTM) networks are excellent in learning implicit knowledge from data. However, it is impossible for LSTM to acquire explicit knowledge such as commonsense facts from the training data for accomplishing their specific tasks. On the other hand, emerging knowledge bases have brought a variety of knowledge resources to our attention, and it has been acknowledged that incorporating the background knowledge is an important add-on for many NLP tasks. In this paper, we propose a knowledge-rich solution to targeted aspect-based sentiment analysis with a specific focus on leveraging commonsense knowledge in the deep neural sequential model. To explicitly model the inference of the dependent sentiment, we augment the LSTM with a stacked attention mechanism consisting of attention models for the target level and sentence level, respectively. In order to explicitly integrate the explicit knowledge with implicit knowledge, we propose an extension of LSTM, termed Sentic LSTM. The extended LSTM cell includes a separate output gate that interpolates the token-level memory and the concept-level input.</p><p>In addition, we propose an extension of Sentic LSTM by creating a hybrid of the LSTM and a recurrent additive network that simulates sentic patterns. In this paper, we are mainly concerned with a joint task combining the target-dependent aspect detection and targeted aspect-based polarity classification. The performance of proposed methods on this joint task is evaluated on two benchmark datasets. The experiment shows that the combination of proposed attention architecture and knowledge-embedded LSTM could outperform state-of-the-art methods in two targeted aspect sentiment tasks. We present a knowledge-rich solution for the task of targeted aspect-based sentiment analysis. Our model can effectively incorporate the commonsense knowledge into the deep neural network and be trained in an end-to-end manner. We show that the two-step attentive neural architecture as well as the proposed Sentic LSTM and H-Sentic-LSTM can achieve an improved performance on resolving the aspect categories and sentiment polarity for a targeted entity in its context over state-of-the-art systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Sentiment analysis <ref type="bibr" target="#b0">[1]</ref> has turned out to be a critical step for natural language processing (NLP) research, especially for social media data on online communities, blogs, Wikis, microblogging platforms, and other online collaborative media. As a branch of affective computing research <ref type="bibr" target="#b1">[2]</ref>, sentiment analysis aims to categorize text (but sometimes also audio and video <ref type="bibr" target="#b2">[3]</ref>) into either positive or negative (or neutral <ref type="bibr" target="#b3">[4]</ref> in some cases). Early works on sentiment analysis focus on resolving the overall sentiment of a text unit <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, assuming that a sentence expresses a unified sentiment or polarity. In contrast, aspect-based sentiment analysis (ABSA) <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref> extends the typical setting of sentiment analysis with a more realistic assumption that polarity is associated with specific aspects (or product features). Taking the sentence "The design of the space is good but the service is horrible" for an example, the sentiment expressed towards the two aspects, "space" and "service," is the completely opposite. On the other hand, targeted (or target-dependent) sentiment classification (TSA) <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> resolves the sentiment polarity of a given target expression in its context, assuming that sentiment is dependent on targeted entities. For instance, in the sentence "I just log on my <ref type="bibr">[facebook]</ref>. <ref type="bibr">[Transformer]</ref> is boring," the sentiment expressed towards <ref type="bibr">[Transformer]</ref> is negative, while no clear sentiment for <ref type="bibr">[facebook]</ref>. Most recently, targeted aspect-based sentiment analysis (TABSA) <ref type="bibr" target="#b14">[15]</ref> combines the challenges of ABSA and TSA. Namely, the task requires detection of the aspect category as well as the resolution of the polarity for specific aspects of a given targeted entity. Figure <ref type="figure" target="#fig_0">1</ref> illustrates another example of TABSA in SentiHood dataset<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b14">[15]</ref>.</p><p>As a popular and effective solution to many NLP task, deep learning methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref> have achieved great successes when applied to ABSA or TSA. Especially, neural sequential model, such as LSTM <ref type="bibr" target="#b18">[19]</ref>, attracts more and more attention for their capacity of representing sequential information. Although it is not scientifically validated that the LSTM is cognitive plausible, the functionality, especially the forget mechanism, of LSTM is considered simulating the functionality of human brains. Due to use of gated functions, LSTM can effectively learn implicit knowledge from sequences by avoiding the troublesome of gradient vanishing. In contrast, explicit knowledge such as commonsense knowledge is hard to be learned from the data where commonsense facts are not explicitly annotated <ref type="bibr" target="#b19">[20]</ref>.</p><p>We, therefore, target three problems remaining unsolved by current state-of-the-art methods. Firstly, given a target might be composed of multiple instances (mentions of the same target) or multiple words, existing research assumes all instances are of equal importance and simply computes an average vector over instances. This oversimplification conflicts with the fact that part of the target expression is more tightly tied with sentiment than the rest. Secondly, hierarchical attention exploited by existent methods only implicitly model the process of inferring the sentiment evidence as black-box.</p><p>Last but not least, existing research falls short in effectively incorporating external sentiment knowledge. The emergence of a variety of knowledge <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> has facilitated many applications of text processing <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref> and image processing <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> with accessing external information that are not available in the limited training data. Especially, the task of TABSA might benefit from using the affective commonsense knowledge <ref type="bibr" target="#b30">[31]</ref> for that the affective properties strongly correlate with aspects and sentiment polarity. To address these problems, we propose a two-step attention architecture with an extended LSTM cell that can better leverage the external knowledge. We identify our contribution as three folds:</p><p>1. We propose a two-step attention model which explicitly attends to first the words of target expression, and then to the whole sentence. The two-step attention model simulates the process of first locating and memorizing the targets and then searching for related sentiment cues over the whole sentence. 2. We extend the classic LSTM cell with components accounting for integration with external knowledge. The extended LSTM is capable of utilizing the explicit knowledge to control the information flows from time to time as well as generating the output fed into the classifier. 3. To the best of our knowledge, we are the first to incorporate affective commonsense knowledge into a deep neural network for modeling sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aspect-Based Sentiment Analysis</head><p>ABSA is the task to classify sentiment polarity with respect to a set of aspects. The biggest challenge faced by ABSA is how to effectively represent the aspect-specific sentiment information of the whole sentence. Early works on ABSA have mainly relied on feature engineering to characterize the sentences <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. Motivated by the success of deep learning methods in representation learning, many recent works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34]</ref> utilize deep neural networks, such as LSTM, to generate sentence embeddings (dense vector representation of sentences) which are then fed to a classifier as a low-dimensional feature vector. Moreover, the representation can be enhanced by using the attention mechanism <ref type="bibr" target="#b16">[17]</ref>, taking as input the word sequence and aspects. For each word of the sentence, the attention vector quantifies its sentiment salience as well as the relevance to the given aspect. The resulting sentiment representation benefits from attention mechanism for it overcomes the shortcoming of recurrent neural network (RNN) that suffers from information loss when only one single output (e.g., the output at the end of the sequence) is used by the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Targeted Sentiment Analysis</head><p>In comparison with the ABSA, targeted sentiment analysis aims to analyze the sentiment with regard to targeted entities in the sentence. It is thus critical for targeted sentiment analysis methods, e.g., the target-dependent LSTM model (TD-LSTM) and target connection LSTM model <ref type="bibr" target="#b11">[12]</ref>, to model the interaction between sentiment targets and the whole sentence. In order to obtain the target-dependent sentence representation, the TD-LSTM directly uses the hidden outputs of bidirectional-LSTM sentence encoders panning the target mentions, while TC-LSTM extends TD-LSTM by concatenating each input word vector with a target vector. Similar to ABSA, attention models are also applicable to targeted sentiment analysis. Rather than using a single level of attention, deep memory network <ref type="bibr" target="#b17">[18]</ref> and recurrent attention model <ref type="bibr" target="#b34">[35]</ref> have achieved superior performance by learning a deep attention over the singlelevel attention, for that multiple passes (or hops) over the input sequence could refine the attended words again and again to find the most important words. These existent approaches have either ignored the problem of multiple target instances (or words) or simply used an averaging vector over target expression <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Our method differs with existing methods by weighting each target word with an attention weight so that the given target tends to be represented by its most informative part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Targeted Aspect-Based Sentiment Analysis</head><p>Two baseline systems <ref type="bibr" target="#b14">[15]</ref> are proposed together with the SentiHood dataset: a feature-based logistic regression model and a LSTM-based model. The feature-based logistic regression model uses feature templates including n-grams tokens and POS tags extracted from the context of instances. The LSTM baseline can be seen as an adaptation of the TD-LSTM <ref type="bibr" target="#b11">[12]</ref> that simply uses the hidden outputs at the position of target instances assuming that all target instances are equally important.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorporating External Knowledge</head><p>Existent study on incorporating external knowledge into deep neural networks is also closely related to this work. External knowledge base has been typically used as a source of features <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>. Most recently, neural sequential models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b37">38]</ref> leverage the lower-dimensional continuous representation of knowledge concepts as additional inputs. However, these approaches have treated the computation of neural sequential models as a black-box without tight integration of the knowledge and the computational structure. Our Sentic LSTM is inspired by <ref type="bibr" target="#b24">[25]</ref> which adds a knowledge recall gate to the cell state of LSTM. However, our method differs from <ref type="bibr" target="#b24">[25]</ref> in the way of using external knowledge to generate the hidden outputs and controlling the information flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>In this section, we describe the proposed method in detail. We start with briefly introducing the task definition of TABSA followed by an overview of the whole neural architecture. Afterwards, we describe the two-step attention model. Lastly, we describe the proposed knowledgeembedded extension of the LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Definition</head><p>A sentence s is composed of a sequence of words. Similar to <ref type="bibr" target="#b13">[14]</ref>, we consider all mentions of the same target as a single target. A target t composed of m words (can be either consecutive or not) in sentence s, denoted as</p><formula xml:id="formula_0">T = {t 1 , t 2 , • • • , t i , • • • , t m }</formula><p>with t i referring to the position of ith word in the target expression, the task of TABSA can be divided into two subtasks. Firstly, it resolves the aspect categories of t belonging to a predefined set (Table <ref type="table" target="#tab_0">1</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>In this section, we provide an overview of our proposed method. Our proposed neural architecture is comprised of two components: the sequence encoder and a hierarchical attention components. Figure <ref type="figure" target="#fig_2">2</ref> illustrates our neural architecture. Given a sentence</p><formula xml:id="formula_1">s = {w 1 , w 2 , • • • , w L }, a look-up operation is first performed to convert input words into word embeddings {v 1 , v 2 , • • • , v L },</formula><p>where L is the length of the sentence. The sequence encoder transforms the word embeddings into a sequence of hidden outputs, on top of which the attention model is built. The targetlevel attention takes as input the hidden outputs of target expression (highlighted in red) and encodes the salience of each word via a self-attention. The target-level attention model then outputs a weighted sum of these hidden outputs as a vector representation of the given target. At the second step, we feed the target embedding together with aspect embeddings as queries to a sentence-level attention model to transform the whole sentence into a vector. A softmax layer is used for mapping the sentence vector to an output label (e.g., none, neural, negative, and positive for a fourclass setting; or none, negative, and positive for a three-class setting) that jointly represents its sentiment polarity and membership of an aspect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long Short-Term Memory Network</head><p>The sentence is encoded using an extension of RNN <ref type="bibr" target="#b38">[39]</ref>, termed LSTM <ref type="bibr" target="#b18">[19]</ref>, which was firstly introduced by <ref type="bibr" target="#b18">[19]</ref> to solve the vanishing and exploding gradient problem faced by the vanilla RNN. A typical LSTM cell contains three gates: forget gate, input gate, and output gate. These gates determine the information to flow in and flow out at the current time step. The cell is defined as below:  where each element of H is a concatenation of the corresponding hidden outputs of both forward and backward LSTM cells.</p><formula xml:id="formula_2">f i = σ (W f [x i , h i-1 ] + b f ) I i = σ (W I [x i , h i-1 ] + b I ) C i = tanh(W C [x i , h i-1 ] + b C ) C i = f i * C i-1 + I i * C i o i = σ (W o [x i , h i-1 ] + b o ) h i = o i * tanh(C i ),<label>(1)</label></formula><formula xml:id="formula_3">H = [h 1 , h 2 ...h L ] = -→ h 1 -→ h 2 • • • -→ h L ← - h 1 ← - h 2 • • • ← - h L ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target-Level Attention</head><p>Based on the attention mechanism, we calculate an attention vector for a target expression. A target might consist of consecutive or non-consecutive sequence of words, denoted as</p><formula xml:id="formula_4">T = {t 1 , t 2 , • • • , t m }</formula><p>, where t i is the location of an individual word in a target expression. The hidden outputs corresponding to T is denoted as</p><formula xml:id="formula_5">H = {h t 1 , h t 2 , • • • , h t m }.</formula><p>We compute the vector representation of a target tas</p><formula xml:id="formula_6">v t = H α = j α j h t j (2)</formula><p>, where the target attention vector</p><formula xml:id="formula_7">α = {α 1 , α 2 , • • • , α m }</formula><p>is distributed over target word sequence T . The attention vector α is a self-attention vector that takes nothing but the hidden output itself as input. The attention vector α of target expression is computed by feeding the hidden output into a bi-layer perceptron, as shown in Eq. 3.</p><p>α = sof tmax(W (2)  a tanh(W (1)  a H )),</p><p>where</p><formula xml:id="formula_9">W (1) a ∈ R d m ×d h and W (2)</formula><p>a ∈ R 1×d m are parameters of the attention component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence-Level Attention Model</head><p>Following the target-level attention, our model learns a target-and-aspect-specific sentence attention over all the words of a sentence. Given a sentence s of length L, the hidden outputs are denoted as</p><formula xml:id="formula_10">H = [h 1 , h 2 , • • • , h L ].</formula><p>An attention model computes a linear combination of the hidden vectors into a single vector, i.e.,</p><formula xml:id="formula_11">v a s,t = Hβ = i β i h i ,<label>(4)</label></formula><p>where the vector</p><formula xml:id="formula_12">β = [β 1 , β 2 , • • • , β L ]</formula><p>is called sentencelevel attention vector. Each element β i encodes the salience of the word w i in the sentence s with regard to the aspect a and target T . Existing research on target or aspect-based sentiment analysis mostly uses targets or aspect terms as queries. At first, each h i is transformed to a d m dimensional vector by a multi-layer neural network with a tanh activating function, followed by a dense softmax layer to generate a probability distribution over the words in sentence s, i.e.,</p><formula xml:id="formula_13">β a = sof tmax(v T a tanh(W m (H v t ))), (<label>5</label></formula><formula xml:id="formula_14">)</formula><p>where v a is the aspect embedding of aspect a, H v t is the operation concatenating v t to each h i , W <ref type="bibr" target="#b0">(1)</ref> m ∈ R d m ×d h is the matrix mapping row vectors of H to a d m dimensional space, and W <ref type="bibr" target="#b1">(2)</ref> m ∈ R 1×d m maps each new row vector to a unnormalized attention weight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentic LSTM</head><p>In this paper, we consider using the affective commonsense knowledge as our knowledge source to be embedded into the sequence model. Affective commonsense knowledge such as AffectNet <ref type="bibr" target="#b39">[40]</ref> contains concepts associated with a rich set of affective properties (as shown in Table <ref type="table" target="#tab_1">2</ref>). These affective properties provide not only concept-level features but also semantic links to the aspects and their sentiment polarity. For example, the concept "rotten fish" has property "KindOf-food" which can be directly related to aspects such as "restaurant" or "food quality," and properties such as "Arises-joy" could contribute positively to the classification of sentiment polarity.</p><p>However, AffectNet is of high dimensions which hinders it from being used in deep neural models. AffectiveSpace <ref type="bibr" target="#b30">[31]</ref> has been built to map the concepts of AffectNet to continuous low-dimensional embeddings (as shown in Fig. <ref type="figure" target="#fig_3">3</ref>) without losing the semantic and affective relatedness in the original space. Based on this new space of concepts, we embedded the concept-level information into deep neural sequential models to better classify the aspects and sentiment of sentences.</p><p>In order to leverage the commonsense knowledge with efficacy, we propose an extension of LSTM, termed Sentic LSTM. It is reasonable to assume that the knowledge concepts contain information complementary to the textual word sequence, especially when the knowledge base in use is designed to include abstract concepts. Our Sentic LSTM aims to entitle the concepts with two important roles: <ref type="bibr" target="#b0">(1)</ref> assisting with the filtering of information flowing from one time step to the next and (2) providing complementary information to the memory cell. At each time step i, we assume that a set of knowledge concept candidates can be triggered and mapped to a d c dimensional space. We denote the set of K concepts as {μ i,1 , μ i,2 , • • • , μ i,K }. First, we combine the candidate embeddings into a single vector as in Eq. 6.</p><formula xml:id="formula_15">μ i = 1 K j μ i,j<label>(6)</label></formula><p>In this paper, as we find that there exists only up to 4 extracted concepts for each time step, we simply use the  </p><formula xml:id="formula_16">f i = σ (W f [x i , h i-1 , μ i ] + b f ) I i = σ (W I [x i , h i-1 , μ i ] + b I ) C i = tanh(W C [x i , h i-1 ] + b C ) C i = f i * C i-1 + I i * C i o i = σ (W o [x i , h i-1 , μ i ] + b o ) o c i = σ (W co [x i , h i-1 , μ i ] + b co ) h i = o i * tanh(C i ) + o c i * tanh(W c μ i ) (7)</formula><p>Our extension of LSTM is illustrated in Eq. 7. At first, we assume that the affective concepts are meaningful cues to control the information of token-level information. For example, a multi-word concept "rotten fish" might indicate the word "rotten" is a sentiment-related modifier to its next word "fish" and hence less information should be filtered out at next time step. We thus add knowledge concepts to the forget, input, and output gate of standard LSTM to help filtering the information. The presence of affective concepts in the input gate is expected to prevent the memory cell from affected by input tokens conflicted with the pre-existed knowledge. Similarly, the output gate uses the knowledge to filter out irrelevant information stored in the memory. Another important feature of our extension of the LSTM is based on the assumption that the information from the concept-level output is complementary to the token level. Therefore, we extended the regular LSTM with an additional knowledge output gate o c i to output concept-level knowledge complementary to the token-level memory. Since AffectiveSpace is learned independently, we leverage a transformation matrix W c ∈ R d h ×d μ to map AffectiveSpace to the same space as the memory outputs. In other words, o c i models the relative contribution of the token level and concept level. Moreover, we notice that o c i * tanh(W c μ i ) actually resembles the functionality of the sentinel vector used by <ref type="bibr" target="#b27">[28]</ref> that allows the model to choose whether to use the affective knowledge or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hybrid Sentic LSTM</head><p>Inspired by the work <ref type="bibr" target="#b40">[41]</ref>, we propose a simplified version of Sentic LSTM that is a hybrid of LSTM and recurrent additive network. This variant of Sentic LSTM could involve the concept-level input to the recurrent connection and maintain a reduced number of parameters as compared with Sentic LSTM. Another intuition is that the additive operation on each concept embedding over time steps simulates the inference process done by sentic patterns <ref type="bibr" target="#b41">[42]</ref> that were used the previous system. A mathematical description of the hybrid Sentic LSTM is given in Eq. 8.</p><formula xml:id="formula_17">M i = W c μ i f i = σ (W f [x i , h i-1 , μ i ] + b f ) I i = σ (W I [x i , h i-1 , μ i ] + b I ) C i = tanh(W C [x i , h i-1 ] + b C ) o c i = σ (W co [x i , h i-1 , μ i ] + b co ) C i = f i * C i-1 + I i * C i + o c i * M i h i = C i (8)</formula><p>Similar to recurrent additive network, we could rewrite the hidden output at time step i as Eq. 9.</p><formula xml:id="formula_18">C μ i = f i * (C i-1 -C μ i ) + I i * C i + f i * C μ i + o c i * M i I i * C i + f i * C μ i + o c i * M = i j =0 w i j * M j (9)</formula><p>w i j is a product of the input and forget gates. We could see that the hidden output at time step i is a hybrid of a simplified LSTM, whose gates are coupled with both word-level and concept-level inputs, and a recurrent additive component accumulating the information from the conceptlevel input over previous time steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction and Parameter Learning</head><p>The objective to train our classier is defined as minimizing the sum of the cross-entropy losses of prediction on each target-aspect pair, i.e.,</p><formula xml:id="formula_19">L s = 1 |D| s∈D t∈s a∈A log p a c,t</formula><p>, where A is the set of predefined aspects and p a c,t is the probability of the golden-truth polarity class c given target t with respect to a sentiment category a, which is defined by a softmax function,</p><formula xml:id="formula_20">p a c,t = sof tmax(W p v a s,t + b a s )</formula><p>, where W p and b a s are the parameters to map the vector representation of target t to the polarity label of aspect a.</p><p>To avoid overfitting, we add a dropout layer with dropout probability of 0.5 after the embedding layer. We stop the training process of our model after 10 epochs and select the model that achieves the best performance on the development set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset and Resources</head><p>We evaluate our method on two datasets: SentiHood dataset <ref type="bibr" target="#b14">[15]</ref> and a subset of SemEval 2015 <ref type="bibr" target="#b42">[43]</ref>. The SentiHood dataset was built by querying Yahoo! Answers with location names of London city. To show the generalizability of our methods, we build a subset of the datasets used by SemEval-2015. We remove sentences containing no targets as well as NULL targets. To be comparable with SentiHood, we combine targets with the same surface form within the same sentence as mentions of the same target. In total, we have 1197 targets left in the training set and 542 targets left in the testing set. On average, each target has 1.06 aspects.</p><p>To inject the commonsense knowledge, we use a syntaxbased concept parser 2 to extract a set of concept candidates at each time step, and use pre-trained 100-dimensional AffectiveSpace embedding 3 as the concept embeddings. In the case that no concepts are extracted, a zero vector is used as the concept input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Setting</head><p>We evaluate our method on two subtasks of the targetspecific aspect-based sentiment analysis: (1) aspect categorization and (2) aspect-based sentiment classification. Following Saeidi et al. <ref type="bibr" target="#b14">[15]</ref>, we treat the outputs of aspectbased classification as hierarchical classes. For aspect categorization, we output the label with the highest probability for each aspect. The labels are, for example in the threeclass setting, "Positive," "Negative," and "None," where 2 http://github.com/senticnet/concept-parser 3 http://sentic.net/downloads "None" means the aspect should not be bonded to the given target. For aspect-based sentiment classification, we only look at probability of "Positive" and "Negative," while ignoring the scores of "None" <ref type="foot" target="#foot_1">4</ref> . For evaluating the aspectbased sentiment classification, we calculate the accuracy averaged over aspects. We evaluate the aspect categorization as a multi-label classification problem, and the results, therefore, are averaged over targets instead of aspects. We evaluate our methods and baseline systems using both loose and strict metrics. We report scores of three widely used evaluation metrics of multi-label classifier: Macro-F1, Micro-F1, and strict Accuracy (Strict Acc.).</p><p>Given the dataset D, the ground truth aspect categories of the target t ∈ D is denoted as Y t , while the predicted aspect categories denoted as Y t . The three metrics can be computed as</p><formula xml:id="formula_21">-Strict accuracy (Strict Acc.): 1 D t∈D σ (Y t = Y t ), where σ (•) is an indicator function. -Macro-F1 = 2 Ma-P×Ma-R</formula><p>Ma-P+Ma-R , which is based on Macro-Precision (Ma-P) and Micro-Recall (Ma-R) with <ref type="bibr">Ma</ref></p><formula xml:id="formula_22">-P = 1 |D| t∈D |Y t ∩ Y t | Y t , and Ma-R= 1 |D| t∈D |Y t ∩ Y t | Y t . -Micro-F1 = 2 Mi-P×Mi-R</formula><p>Mi-P+Mi-R , which is based on Micro-Precision (Mi-P) and Micro-Recall (Mi-R), where Mi-</p><formula xml:id="formula_23">P= t∈D |Y t ∩ Y t | t∈D Y t , and Mi-R= t∈D |Y t ∩ Y t | t∈D Y t .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance Comparison</head><p>We compare our proposed method with the methods that have been proposed for TABSA as well as those proposed for ABSA or TSA but applicable to TABSA. Furthermore, we also compare the performances of several variants of our proposed method in order to highlight our technical contribution. We run the model for multiple times and report the results that perform the best in development set. For the SemEval-2015 dataset, we report the results of the final epoch.</p><p>-TD-LSTM: This method is the adaptation of TD-LSTM <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>. It adopts Bi-LSTM to encode the sequential structure of a sentence and represents a given target using a vector averaged on the hidden outputs of target instances. -Bi-LSTM + TA: Our method learns an instance attention on top of the outputs of LSTM to model the contribution of each instance. -Bi-LSTM + TA + SA: In addition to target instance attention, we add to the model a sentence-level attention.</p><p>-Bi-LSTM + TA + DMN SA : The sentence-level attention is replaced by a dynamic memory network with multiple hops <ref type="bibr" target="#b17">[18]</ref>. We run the memory network with different numbers of hops and report the result with four hops which produces the best performance on development set of SentiHood. We exclude the case of zero hops which correspond to Bi-LSTM + TA + SA. -LSTM + TA + SA + KB Feat: Concepts are fed into the input layer as additional features. -LSTM + TA + SA + KBA: This is an integration of the method proposed by <ref type="bibr" target="#b27">[28]</ref>, which learns an attention over the concept embeddings. The concept embedding is combined with the hidden output before being fed into the classifier. -Recall-LSTM + TA + SA: LSTM is extended with a recall knowledge gate as in <ref type="bibr" target="#b24">[25]</ref>. -Sentic LSTM + TA + SA: The encoder is replaced with the proposed knowledge-embedded LSTM. -H-Sentic-LSTM + TA + SA: The hybrid Sentic LSTM that has a simplified recurrent additive component accounting for the concept-level input.</p><p>The word embedding of input layer is initialized by pretrained skip-gram model <ref type="bibr" target="#b43">[44]</ref> with 150 hidden units on a combination of Yelp<ref type="foot" target="#foot_2">5</ref> and Amazon review dataset <ref type="bibr" target="#b44">[45]</ref>, and we use 50 hidden units for the bidirectional LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of Attention Model</head><p>Tables <ref type="table" target="#tab_4">4</ref> and<ref type="table" target="#tab_5">5</ref> show the performance on the SentiHood dataset and SemEval-15 dataset, respectively. In comparison with the non-attention baseline (TD-LSTM), we can find that our best attention-based model significantly improves the aspect detection by more than 35% and sentiment classification by approximately 10% on the SentiHood dataset. It also achieves similar improvement of sentiment classification on the SemEval dataset. However, it is notable that, on the SemEval-2015 dataset, the improvement of aspect detection is relatively smaller. We conjecture the reason is that the SentiHood dataset has masked the target as a special word "LOCATION," which proved less informative than the full name of aspect targets that are used by SemEval-2015. Hence, using only the hidden outputs regarding the target does not suffice to represent the sentiment of the whole sentence in the SentiHood dataset. Although not significant, the target-level attention performs better than the target averaging model (i.e., TD-LSTM) for that the target attention is capable of identifying the part of target expressions with higher sentiment salience. On the other hand, it is notable that the two-step attention achieves significant improvements on both aspect categorization and sentiment classification, indicating that the targetand aspect-dependent sentence attention could retrieve information relevant to both tasks.</p><p>To our surprise, using multiple hops in the sentence-level attention fails to produce any improvement. The performance even falls down significantly on the SemEval-2015 dataset with a much smaller number of training instances but larger aspect set than SentiHood. We conjecture the reason is that using multi-hops increases the number of parameter to learn, making it less applicable to small and sparse dataset such as SemEval 2015.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization of Attention</head><p>We visualize the attention vectors of sentence-level attention in Fig. <ref type="figure" target="#fig_4">4</ref> with regard to "Transition-location" and "Price" aspects. The two attention vectors have encoded quite different concerns in the word sequence. In the first example, the "Transition-location" attention attends to the word "long" which is expressing a negative sentiment towards the target. In comparison, the "Price" attention attends the more to the word "cheap" that is related to the aspect. That is to say, the two attention vectors are capable of distinguishing information related to different aspects. As visualized in Fig. <ref type="figure" target="#fig_5">5</ref>, when there are multiple target instances (or words), the target-level attention is capable of selecting the first target instance which is tied with the clearer sentiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Result of Knowledge-Embedded LSTM</head><p>It can be seen from Tables <ref type="table" target="#tab_4">4</ref> and<ref type="table" target="#tab_5">5</ref> that injecting the knowledge into the model improves the performance in general. Since the affective space used in our experiment contains the information of affective properties that are semantically related to the aspects and sentiment polarity,  it is reasonable to find that it improves performance on both tasks. The results also show that proposed Sentic LSTM outperforms baseline knowledge-rich methods, even though not very significantly. Comparing Sentic LSTM with KB feats that uses extracted concepts as features, we can find that Sentic LSTM improves more on the aspect categorization part, indicating the advantage of using a knowledge output gate to choose between commonsense knowledge and inner memory. The superior performance of Sentic LSTM over Recall-LSTM and KBA indicates that the triggered knowledge concepts can also help to filter the information that conflicts with the commonsense. As compared to the standard Sentic LSTM, the hybrid of Sentic LSTM and recurrent additive neural network (i.e., H-Sentic-LSTM) achieves comparable performance with smaller number of parameters. We notice that using the recurrent additive operation on concept-level inputs improves the performance of aspect categorization on the SemEval dataset, indicating that the additive operation helps accumulating the evidence from a broader context, and the generality of the model could possibly benefit more from the reduced number of parameters given that the size of training data is much smaller than the SentiHood data.</p><p>On the other hand, the performance of H-Sentic-LSTM on resolving the polarity of aspect-target pairs is slightly under-performing Sentic LSTM. Since the additive operation is only able to introduce non-negative contribution from the previous step, we conjecture that the decrease of performance that might be a result of the additive operation does not suffice to simulate complicated sentiment patterns such as negation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AffectiveSpace Versus SentiWordNet</head><p>At last, we compare systems using different sentiment knowledge bases. SentiWordNet is a lexicon-based sentiment knowledge base consisting of word sense synsets annotated with sentiment polarity. However, it is notable that SentiWordNet contains neither commonsense concepts nor affective properties, which are the key features of AffectiveSpace. Consequently, we have to use randomly initialized embeddings for representing the SentiWordNet synsets. Word synsets are mapped to the same 100 dimension embeddings as AffectiveSpace does. Each word in the sentence is mapped to its word sense with the help of a Word Sense Disambiguation tool. We deliberately remove the neutral synsets (i.e., those having zero values for both positive and negative) to emphasize on the sentiment-bearing words. Table <ref type="table" target="#tab_6">6</ref> shows the comparison of the two knowledge base. We report the results using our overall best-performed system (Sentic LSTM + TA + SA). It shows that using the AffectiveSpace achieves superior performance than using SentiWordNet. We conjecture the reason is that the word sense synsets is not as informative as affective properties. Moreover, probably because the link between word senses and aspects are not straightforward, we find the gap in sentiment classification is smaller than aspect categorization.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we proposed a neural architecture together with two extensions of the standard LSTM for the task of targeted aspect-based sentiment analysis. We explicitly modeled the process of inferring sentiment aspects and polarity as a two-step attention model that encodes the target and full sentence in sequence. The target-level attention attends to the sentiment-salient part of a target expression and generates a more accurate representation of the target, while the aspect-target-dependent sentence-level attention searches for the target-and aspect-dependent evidence over the full sentence. Moreover, we validated the efficacy of the proposed extensions of the LSTM cell as well as the benefit of using affective commonsense knowledge for the task of targeted aspect-based sentiment analysis. In the future, we plan to take into account the relations between concepts when performing the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compliance with Ethical Standards</head><p>Conflict of Interest The authors declare that they have no conflict of interest.</p><p>Informed Consent Informed consent was not required as no human or animals were involved.</p><p>Human and Animal Rights This article does not contain any studies with human or animal subjects performed by any of the authors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Example of targeted aspect-based sentiment analysis in SentiHood</figDesc><graphic coords="2,178.23,60.89,365.83,117.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where f i , I i , and o i are the forget gate, input gate, and output gate, respectively. W f , W I , W o , b f , b I , and b o are the weight matrix and bias scalar for each gate. C i is the cell state and h i is the hidden output. A single LSTM typically encodes the sequence from only one direction. However, two LSTMs can also be stacked to be used as a bidirectional encoder, referred to as bidirectional LSTM. For a sentence s = {w 1 , w 2 , • • • , w L }, bidirectional LSTM produces a sequence of hidden outputs,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 Overview of the two-step attentive neural architecture</figDesc><graphic coords="4,84.63,544.13,425.32,155.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 Visualization of AffectiveSpace</figDesc><graphic coords="6,84.63,60.35,425.32,287.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 Example of sentence-level attention</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 Example of target-level attention</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>List of aspects defined in SentiHood dataset For example, the sentence "I live in [West London] for years. I like it and it is safe to live in much of [west London]. Except [Brent] maybe. " contains two targets, [W estLondon] and [Brent]. Our objective is to detect the aspects and classify the sentiment polarity. The desired output for [W estLondon]</figDesc><table><row><cell>General</cell><cell>Shopping</cell></row><row><cell>Price</cell><cell>Multicultural</cell></row><row><cell>Transit-location</cell><cell>Green-nature</cell></row><row><cell>Safety</cell><cell>Dinning</cell></row><row><cell>Live</cell><cell>Quiet</cell></row><row><cell>Nightlife</cell><cell>Touristy</cell></row><row><cell cols="2">Secondly, it classifies the sentiment polarity with regard to</cell></row><row><cell>each aspect category associated with t.</cell><cell></cell></row></table><note><p>is ['general':positive;'safety':positive ], while output for [Brent] should be ['general':negative;'safety':negative].</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Example of AffectNet</figDesc><table><row><cell>AffectNet</cell><cell>IsA-pet</cell><cell>KindOf-food</cell><cell>Arises-joy</cell><cell>...</cell></row><row><cell>Dog</cell><cell>0.981</cell><cell>0</cell><cell>0.789</cell><cell>...</cell></row><row><cell>Cupcake</cell><cell>0</cell><cell>0.922</cell><cell>0.910</cell><cell>...</cell></row><row><cell>Rotten fish</cell><cell>0</cell><cell>0.459</cell><cell>0</cell><cell>...</cell></row><row><cell>Police man</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>...</cell></row><row><cell>Win lottery</cell><cell>0</cell><cell>0</cell><cell>0.991</cell><cell>...</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>SentiHood dataset</figDesc><table><row><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table 3 shows statistics of SentiHood datasets. The whole dataset is split into train, test, and development set by the author. Overall, the entire dataset contains 5215 sentences, with 3862 sentences containing a single target and 1353 sentences containing multiple targets. It also shows that there are approximately two thirds of targets annotated with aspect-based sentiment polarity (train set, 2476 out of 2977; test set, 1241 out of 1898; development set, 619 out of 955). On average, each sentiment-bearing target has been annotated with 1.37 aspects.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Performance of systems on SentiHood dataset</figDesc><table><row><cell>Aspect categorization</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>Performance of</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>systems on SemEval-2015</cell><cell cols="2">Aspect categorization</cell><cell></cell><cell>Sentiment</cell></row><row><cell>dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Strict Acc.</cell><cell>Macro-F1</cell><cell>Micro-F1</cell><cell>Sentiment Acc.</cell></row><row><cell>TD-LSTM</cell><cell>65.49</cell><cell>70.56</cell><cell>69.00</cell><cell>68.57</cell></row><row><cell>LSTM+TA</cell><cell>66.42</cell><cell>71.71</cell><cell>70.06</cell><cell>69.24</cell></row><row><cell>LSTM+TA+SA</cell><cell>63.46</cell><cell>70.73</cell><cell>66.18</cell><cell>74.28</cell></row><row><cell>LSTM+TA+DMN SA</cell><cell>48.33</cell><cell>52.73</cell><cell>51.39</cell><cell>69.07</cell></row><row><cell>LSTM+TA+SA+KB Feat</cell><cell>65.68</cell><cell>74.46</cell><cell>70.71</cell><cell>76.13</cell></row><row><cell>LSTM+TA+SA+KBA</cell><cell>67.34</cell><cell>74.36</cell><cell>71.78</cell><cell>73.10</cell></row><row><cell>Recall-LSTM + TA + SA</cell><cell>66.05</cell><cell>72.90</cell><cell>69.66</cell><cell>74.11</cell></row><row><cell>Sentic LSTM + TA + SA</cell><cell>67.34</cell><cell>76.44</cell><cell>73.82</cell><cell>76.47</cell></row><row><cell>H-Sentic-LSTM + TA + SA</cell><cell>69.19</cell><cell>77.55</cell><cell>75.00</cell><cell>74.11</cell></row></table><note><p>Italics represent significance at 1% level over the the knowledge-free baselines (first 4 baselines)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>Comparison of systems AffectiveSpace and SentiWordNet (SH stands for SentiHood and SE stands for SemEval-15)</figDesc><table><row><cell>Aspect categorization</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The SentiHood dataset has masked all targeted entities with "Location" + INDEX.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>On SemEval 2015, we use "Negative," "Positive," "Neutral," and "None."</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>http://yelp.com.sg/dataset/challenge</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A practical guide to sentiment analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Feraco</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A review of affective computing: from unimodal analysis to multimodal fusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf Fusion</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="98" to="125" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Context-dependent sentiment analysis in usergenerated videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bayesian network based extreme learning machine for subjectivity detection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ragusa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gastaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zunino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Frankl. Inst</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1780" to="1797" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Yahoo! for amazon: sentiment extraction from small talk on the web</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Manag Sci</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1375" to="1388" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mining product reputations on the web</title>
		<author>
			<persName><forename type="first">S</forename><surname>Morinaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yamanishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tateishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the eighth ACM SIGKDD international conference on knowledge discovery and data mining<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semeval-2014 task 4: aspect based sentiment analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th international workshop on semantic evaluation</title>
		<meeting>the 8th international workshop on semantic evaluation<address><addrLine>Dublin</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics and Dublin City University</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="27" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semeval-2016 task 5: aspect based sentiment analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Smadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Ayyoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Clercq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Hoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Loukachevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kotelnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jiménez-Zafra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Eryigit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international workshop on semantic evaluation (SemEval-2016)</title>
		<meeting>the 10th international workshop on semantic evaluation (SemEval-2016)<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Aspect extraction for opinion mining with a deep convolutional neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl-Based Syst</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="42" to="49" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Aspnet: aspect extraction by bootstrapping generalization and propagation using an aspect network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="241" to="253" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sentic LDA: improving on LDA with semantic similarity for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bisio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCNN</title>
		<imprint>
			<biblScope unit="page" from="4465" to="4473" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Effective LSTMs for targetdependent sentiment classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th international conference on computational linguistics: technical papers</title>
		<meeting>COLING 2016, the 26th international conference on computational linguistics: technical papers<address><addrLine>Osaka</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3298" to="3307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive recursive neural network for target-dependent twitter sentiment classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd annual meeting of the association for computational linguistics</title>
		<meeting>the 52nd annual meeting of the association for computational linguistics<address><addrLine>Baltimore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="49" to="54" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multitarget-specific sentiment recognition on twitter</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zubiaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tdparse</surname></persName>
		</author>
		<author>
			<persName><surname>Procter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th conference of the European chapter</title>
		<meeting>the 15th conference of the European chapter<address><addrLine>Valencia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="483" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sentihood: targeted aspect based sentiment analysis dataset for urban neighbourhoods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saeidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th international conference on computational linguistics: technical papers. Osaka: The COLING 2016 Organizing Committee; 2016</title>
		<meeting>COLING 2016, the 26th international conference on computational linguistics: technical papers. Osaka: The COLING 2016 Organizing Committee; 2016</meeting>
		<imprint>
			<biblScope unit="page" from="1546" to="1556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">phrase recursive neural network for aspect-based sentiment analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shirai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 conference on empirical methods in natural processing</title>
		<meeting>the 2015 conference on empirical methods in natural processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2509" to="2514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention-based LSTM for aspect-level sentiment classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 conference on empirical methods in natural language processing</title>
		<meeting>the 2016 conference on empirical methods in natural language processing<address><addrLine>Austin</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="606" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aspect level sentiment classification with deep memory network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 conference on empirical methods in natural language processing</title>
		<meeting>the 2016 conference on empirical methods in natural language processing<address><addrLine>Austin</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Common sense computing: from the society of mind to digital intuition and beyond</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Havasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eckl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometric ID management and multimodal communication</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Fierrez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Ortega</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Esposito</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Drygajlo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Faundez-Zanuy</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5707</biblScope>
			<biblScope unit="page" from="252" to="259" />
		</imprint>
	</monogr>
	<note>of lecture notes in computer science</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sentiwordnet 3.0: an enhanced lexical resource for sentiment analysis and opinion mining</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baccianella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LREC</title>
		<imprint>
			<biblScope unit="page" from="2200" to="2204" />
			<date type="published" when="2010">2010</date>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<pubPlace>Valletta</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SenticNet 4: a semantic resource for sentiment analysis based on conceptual primitives</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th international conference on computational linguistics: technical papers. Osaka: The COLING</title>
		<meeting>COLING 2016, the 26th international conference on computational linguistics: technical papers. Osaka: The COLING</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="2666" to="2677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth conference on computational natural language learning</title>
		<meeting>the thirteenth conference on computational natural language learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature-enriched word embeddings for named entity recognition in open-domain conversations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bigot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="6055" to="6059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Incorporating loosestructured knowledge into LSTM with recall gate for conversation modeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05110</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning word representations for sentiment analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="843" to="851" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised commonsense knowledge enrichment for domainspecific sentiment analysis</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rokach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shabtai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="467" to="477" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Leveraging knowledge bases in LSTMs for improving machine reading</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th annual meeting of the association for computational linguistics</title>
		<meeting>the 55th annual meeting of the association for computational linguistics<address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1436" to="1446" />
		</imprint>
	</monogr>
	<note>long papers</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sentic album: content-, concept-, and context-based online personal photo management system</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="477" to="496" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Common sense knowledge for handwritten Chinese text recognition</title>
		<author>
			<persName><forename type="first">Q-F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="234" to="242" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">AffectiveSpace 2: enabling affective intuition for concept-level sentiment analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bisio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="page" from="508" to="514" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aspect-based polarity classification for SemEval task 4</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bogdanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dcu</surname></persName>
		</author>
		<author>
			<persName><surname>Tounsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th international workshop on semantic evaluation</title>
		<meeting>the 8th international workshop on semantic evaluation<address><addrLine>Dublin</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics and Dublin City University</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="223" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">NRC-Canada-2014: detecting aspects and sentiment in customer reviews</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th international workshop on semantic evaluation</title>
		<meeting>the 8th international workshop on semantic evaluation<address><addrLine>Dublin</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics and Dublin City University</publisher>
			<date type="published" when="2014">SemEval 2014. 2014</date>
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Aspect specific sentiment analysis using hierarchical deep learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and representation learning</title>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recurrent attention network on memory for aspect sentiment analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 conference on empirical methods in natural language processing</title>
		<meeting>the 2017 conference on empirical methods in natural language processing<address><addrLine>Copenhagen</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="463" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Conference resolution with world knowledge</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: human language technologies</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A knowledge-intensive model for prepositional phrase attachment</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="365" to="375" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pärnamaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00318</idno>
		<title level="m">A neural knowledge language model</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Signal Process</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for affective common-sense reasoning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bisio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07393.2017</idno>
		<imprint/>
	</monogr>
	<note>Recurrent additive networks</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sentic patterns Dependency-based rules for concept-level sentiment analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Winterstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl-Based Syst</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="45" to="63" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">SemEval-2015 task 12: aspect based sentiment analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pontiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Galanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Manandhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Androutsopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th international workshop on semantic evaluation</title>
		<meeting>the 9th international workshop on semantic evaluation<address><addrLine>Denver</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="486" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ups and downs: modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on world wide web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 25th international conference on world wide web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
