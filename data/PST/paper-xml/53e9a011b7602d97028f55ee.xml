<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Real-Time Head Nod and Shake Detector</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ashish</forename><surname>Kapoor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Media Lab</orgName>
								<orgName type="institution">Affective Computing</orgName>
								<address>
									<addrLine>20 Ames Street</addrLine>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Rosalind</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
							<email>picard@media.mit.edu</email>
							<affiliation key="aff1">
								<orgName type="department">MIT Media Lab</orgName>
								<orgName type="institution">Affective Computing</orgName>
								<address>
									<addrLine>20 Ames Street</addrLine>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Real-Time Head Nod and Shake Detector</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CB5692C15F503162193F1D26BA5FD1C7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Head Nod</term>
					<term>Head Shake</term>
					<term>Pupil Tracking</term>
					<term>HMM</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Head nods and head shakes are non-verbal gestures used often to communicate intent, emotion and to perform conversational functions. We describe a vision-based system that detects head nods and head shakes in real time and can act as a useful and basic interface to a machine. We use an infrared sensitive camera equipped with infrared LEDs to track pupils. The directions of head movements, determined using the position of pupils, are used as observations by a discrete Hidden Markov Model (HMM) based pattern analyzer to detect when a head nod/shake occurs. The system is trained and tested on natural data from ten users gathered in the presence of varied lighting and varied facial expressions. The system as described achieves a real time recognition accuracy of 78.46% on the test dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>A very large percentage of our communication is nonverbal, which includes all expressive signs, signals and cues that are used to send and receive messages apart from manual sign language and speech. Nonverbal gestures perform a number of different functions <ref type="bibr" target="#b1">[1]</ref>. Head nods and shakes can be used as a gesture to fulfill a semantic function (e.g., nod head instead of saying yes), to communicate emotions (e.g., nodding enthusiastically with approval) and as conversational feedback (e.g., to keep the conversation moving). Table <ref type="table" target="#tab_0">1</ref> shows some of the semantic functions and emotions associated with head nods and shakes. A system that could detect head nods and head shakes would be an important component in an interface that is expected to interact naturally with people.</p><p>Head nod, which is a vertical up-and-down movement of the head rhythmically raised and lowered, is an affirmative cue, widely used throughout the world to show understanding, approval, and agreement <ref type="bibr" target="#b3">[3]</ref> [4] <ref type="bibr">[8]</ref>. Head shake is rotation of the head horizontally from side-to-side and is nearly a universal sign of  <ref type="bibr">[8]</ref>. Although, head nod goes mostly with positive intent/emotions and head shake with negative intent/emotions there might be certain exceptions. For example, head nod might occur with a feeling of rage too. Head nods are also used as a conversational feedback, so a person, even if he does not agree, may nod his head while he is listening.</p><p>In this paper we describe a new vision-based system that detects head nods and head shakes in real time. Real time detection of head nods and shakes is difficult, as the head movements during a nod or shake are small, fast and jerky, causing many video-based face trackers to fail. We use an infrared sensitive camera equipped with infrared LEDs to track pupils robustly. A Hidden Markov Model (HMM) <ref type="bibr" target="#b10">[10]</ref> based classifier is used to detect the occurrence of head nods and head shakes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>A lot of work has been done on facial pose estimation as well as on face tracking. 3D facial pose estimation based on facial feature tracking has been suggested <ref type="bibr" target="#b6">[6]</ref>. Regions of skin and hair have been used to estimate 3D head pose as well <ref type="bibr" target="#b2">[2]</ref>. Rowley et al. <ref type="bibr">[11] [12]</ref> have described a neural network based face detector. As the head movements during a nod or shake are small, fast and jerky all these approaches are either unable to track the head in real time during those movements or the resolution they provide is insufficient to detect head nods and shakes.</p><p>There is much prior work on detecting and tracking the eyes. Tian et al <ref type="bibr" target="#b13">[13]</ref> use a dual state model to recover eye parameters using feature point tracking of the inner eye corners. This system requires that the eye templates be initialized manually in the first frame of the sequence, which prevents it from being automatic.</p><p>Many eye feature extraction methods are based on deformable templates <ref type="bibr" target="#b14">[14]</ref>, which are difficult to use for real-time eye tracking and have to be initialized properly to achieve a good performance. Morimoto et al <ref type="bibr" target="#b9">[9]</ref> have described a system to detect and track pupils using the red-eye effect. Haro et al <ref type="bibr" target="#b5">[5]</ref> have extended this system to highlight the pupils, which in turn are detected and tracked using Kalman filter and probabilistic PCA. Our infrared camera equipped with infrared LEDs, which is used to highlight</p><p>The difference image and track pupils, is an in-house built version of the IBM Blue Eyes camera (http://www.almaden.ibm.com/cs/blueeyes). Our approach to detect and track the pupils is motivated by the methods described in Morimoto et al <ref type="bibr" target="#b9">[9]</ref> and Haro et al <ref type="bibr" target="#b5">[5]</ref>.</p><p>Kawato and Ohya <ref type="bibr" target="#b7">[7]</ref> have described a system to detect head nods and head shakes in real time by directly detecting and tracking the "between-eyes" region. The "between-eyes" region is detected and tracked using a "circle frequency filter", which is a discrete Fourier transform of points lying on a circle, together with skin color information and templates. Head nods and head shakes are detected based on pre-defined rules applied to the positions of "between-eyes" in consecutive frames. We describe a simple system that tracks pupils robustly using an algorithm that requires very low processing. However, rather than using a rule-based approach to detect head nods and shakes, we adopt a statistical pattern matching approach trained and tested on natural data. In the next section we describe the infrared camera, the pupil tracking algorithm and the real-time head nod/shake detection based on HMMs. Followed by that we present an experimental evaluation and discuss the recognition results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE HEAD NOD &amp; SHAKE DETECTOR</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the overall architecture of the system. An infrared sensitive camera synchronized with infrared LEDs is used as a sensor and produces an image with highlighted pupils. The image obtained from the sensor is processed by the feature extraction module, which detects/tracks the pupil positions and infers the direction in which the head moved. The directions of the head movements in consecutive frames are used as a sequence of observations to train and test the HMMs in the pattern analyzer.</p><p>The whole system is very efficient and runs in real time at 30fps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sensor</head><p>The pupil tracking system is shown in Figure <ref type="figure" target="#fig_1">2</ref>. The whole unit is placed under the monitor pointing towards the users face. The system has an infrared sensitive camera coupled with two concentric rings of infrared LEDs. One set of LEDs is on the optical axis and produces the red-eye effect. The other set of LEDs, which are off axis, keeps the scene at about the same illumination. The two sets of LEDs are synchronized with the camera and are switched on and off to generate two interlaced images for a single frame. The image where the on-axis LEDs are on has white pupils whereas the image where the off-axis LEDs are on has black pupils. These two images are subtracted to get a difference image, which is used to track the pupils. Figure <ref type="figure">3</ref> shows a sample image, the de-interlaced images and the difference image obtained using the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Extraction</head><p>The direction of head movement is determined using the positions of the pupils in two consecutive frames. The pupils are detected and tracked using the difference image, which is noisy due to the interlacing and motion artifacts. Also, objects like glasses and earrings can show up as bright spots in the difference image due to their specularity. To remove this noise we first threshold the difference image using an adaptive thresholding algorithm <ref type="bibr" target="#b5">[5]</ref>.</p><p>First, the algorithm computes the histogram and then thresholds the image keeping only 0.1 % of the brightest pixels. All the nonzero pixels in the resulting image are set to 255 (maxval). The thresholded image is used to detect and to track the pupil. The pupil tracker is either in a detection mode or a tracking mode. Whenever there is information about the pupils in the previous frame the tracker is in tracking mode and whenever the previous frame has no information about the pupils the tracker switches to the detection mode. The feature extraction module is shown in Figure <ref type="figure">4</ref>.</p><p>During the tracking mode the tracker maintains a state vector, comprised of the spatial information about the pupils. Specifically, the average distance between the pupils during the current tracking phase and their x, y co-ordinates in the previous frames is maintained. To obtain the new positions of pupils a search for the largest connected component is limited to a bounding box centered on previous pupils. The new connected components are accepted as valid pupils when they satisfy a number of spatial constraints. If the area is greater and the displacement of their centers from previous pupil position lies below a certain threshold, the connected components are considered valid. Also if a connected component is found for both the eyes then the distance between these pupils is also compared with the average distance maintained in the state space to rule out false detections. Once the connected components are identified as valid pupil regions, the state vector is updated. Haro et al <ref type="bibr" target="#b5">[5]</ref> have used the Kalman filter, which incorporates position and velocity information, to track the candidate regions. Since the motion in the case of a head nod or shake is jerky, we refrain from using a Kalman filter that does not incorporate acceleration and other higher derivatives of position to track the pupils.</p><p>The tracker switches to the detection mode whenever there is no information about the pupils. In this mode the tracker simply selects the two largest connected components that have an area greater then a certain threshold. Again, to validate the regions, we apply some spatial constraints. This approach allows us to track the pupils efficiently. Head movements during head nods and head shakes do produce motion artifacts but due to the nature of our algorithm to spatially constrain the search space, it tracks the pupils well. In extreme cases when head movements are too fast, the pupils are lost as motion artifact overpowers the red-eye effect and the pupils are absent from the difference image altogether. For the purpose of detecting head nods and head shakes we found this tracking algorithm to be fairly reliable.</p><p>As mentioned earlier we use the direction of head movements as observations for the pattern analyzer. The feature extraction module tracks the pupil positions and based upon that it generates the observations. There are five observation symbols, which correspond to the head moving up, down, left, right, or none. Current pupil positions are compared with pupil positions in the previous frame. If the movement in the x direction is greater than the movement in the y direction then the observation symbol is labeled as left or right head movement depending upon which direction the head moved. Similarly if the movement in the y direction is greater then the movement in the x direction then the label is either up or down, depending upon the direction of the head movement. When the movements in both the x and y directions are below a certain threshold, then the symbol corresponding to none is generated.  side-to-side. We use a discrete HMM <ref type="bibr" target="#b10">[10]</ref> based pattern analyzer to detect when a head nod or a head shake occurs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Head Nod &amp; Shake Detection</head><p>Our pattern analyzer consists of two HMMs, one corresponding to head nods and one corresponding to head shakes. Both HMMs have three states and the observation set has five symbols corresponding to the head moving up, down, left, right and none. During the training phase sample head nods and head shakes were processed by the feature extraction module to obtain the sequence of observations, which were used to train the HMMs using the Baum Welch algorithm <ref type="bibr" target="#b10">[10]</ref>. In the testing phase, the forwardbackward procedure <ref type="bibr" target="#b10">[10]</ref> is used to compute the log likelihood for a sequence of N consecutive observations based on the two HMMs. We compare and threshold the log likelihood to label the sequence as a head nod or a head shake. The performance of the system depends upon N, which is the number of observations that constitute a sequence to be tested. If N is small, then slow head nods and shakes might not be detected. When N is large, then the detected head nods and head shakes might linger for some time after they end. Our system uses N=10, which we found sufficient to detect slow as well as subtle head nods/shakes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL EVALUATION</head><p>To collect a natural database for head nods and head shakes a Microsoft TM agent was programmed to ask a number of factual questions (see table <ref type="table" target="#tab_2">2</ref>), to which the subjects were asked to answer with a head nod or a head shake. We used this strategy to avoid collecting data with exaggerated head nods and head shakes, which people often made when asked to just nod/shake their head in front of a camera. Did you hear about it? 10. {Agent explains the event} Pretty bad isn't it? Ten subjects, among whom five were male, five female and two of them wore glasses, were recorded using the infrared camera while they interacted with the agent. We expected to have a total of 100 nods and shakes, but there were instances where the subjects responded to a question with nodding/shaking their head twice. Also, some subjects used head nods as conversational feedback to the agent. A total of 110 samples were collected with 62 head nods and 48 head shakes. Lighting conditions varied due to changes in sunlight coming through a window at different times of day and due to the collection of data from subjects in two different rooms. To further complicate the data, a number of different facial expressions and movements like smiles, and frowns were made by the subjects in addition to the nods and shakes. (Sometimes the agent elicited humor or other responses.) A random 40% of the head nods and 40 % of the head shakes were selected for training (see Table <ref type="table" target="#tab_3">3</ref>). The testing was done on the collected video sequences played using a VCR. This allowed us to test for actual real-time recognition accuracy on the whole set at 30fps. The feature extraction module processed the videos as explained earlier and observations from sequences of ten consecutive frames were used by the pattern analyzer to detect head nods and head shakes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS AND DISCUSSION</head><p>The recognition results are shown in Table <ref type="table" target="#tab_4">4</ref> and<ref type="table" target="#tab_5">5</ref>. The system was implemented on a Pentium-III 933 MHz Linux machine and a real-time recognition rate of 78.46% was achieved at 30 fps for head nods and head shakes in the test dataset. There were no confusions among head nods and head shakes, as the head movements in a head nod are very different from those in a head shake. Most of the head nods and head shakes that went undetected were the samples taken from the subjects that wore glasses. The specular nature of the glasses made it difficult for the pupil tracker to work well. Interestingly on one of the subjects with glasses, the pupil tracker tracked a bright specular point on the glass frame and hence was able to detect most of the head nods and head shakes. One of the head shakes that went undetected was because the subject closed his eyes while making the gesture. There were some false positives too. Some head nods were detected when the subject started laughing with the head going up and down rhythmically. Sample demonstration movies can be viewed at http://www.media.mit.edu/~ash/PUI01. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>We have described a system for real time detection of head nods and head shakes using pupil tracking. An infrared camera equipped with infrared LEDs is used to track the pupils. The directions of head movements in consecutive frames, which are inferred from the pupil tracking, are used as observations to train discrete HMMs in the pattern analyzer. Upon seeing a sequence of these observations, the pattern analyzer is able to detect head nods and head shakes in real time. The system was trained and tested on a natural database of head nods and head shakes collected using a Microsoft agent that prompted the subjects to nod/shake their heads by asking a series of questions. Recognition accuracy of 78.46% was achieved for head nods and head shakes on the videos in the test dataset streamed at 30 fps.</p><p>Future work includes a system for expression recognition, face and facial feature extraction, pose estimation and a system that integrates all these and acts as an interface that recognizes affect.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The system architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Camera to track pupils, placed under the monitor</figDesc><graphic coords="2,180.57,613.59,107.52,80.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .Figure 4 .</head><label>34</label><figDesc>Figure 3. Pupil tracking using the infrared camera</figDesc><graphic coords="2,402.81,629.07,70.44,55.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Typical sequences of head movements in a head nod and a head shakeFigure5shows typical patterns associated with the head movements in a nod and a shake. A head nod is a vertical up-anddown movement of the head rhythmically raised and lowered, whereas a head shake is rotation of the head horizontally from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 . Affective meanings of head nods/shakes</head><label>1</label><figDesc></figDesc><table><row><cell>Head Nods</cell><cell>Head Shakes</cell></row><row><cell>Approval</cell><cell>Disapproval</cell></row><row><cell>Understanding</cell><cell>Disbelief</cell></row><row><cell>Agreement</cell><cell>Negation</cell></row><row><cell cols="2">disapproval, disbelief, and negation [3] [4]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 . Ten questions asked by the agent</head><label>2</label><figDesc></figDesc><table><row><cell>1. Are the instructions clear?</cell></row><row><cell>2. Are you male?</cell></row><row><cell>3. Are you female?</cell></row><row><cell>4. Are you a student at Media Lab?</cell></row><row><cell>5. Are you a student at Boston University?</cell></row><row><cell>6. Were you born in Boston?</cell></row><row><cell>7. Do you like Boston?</cell></row><row><cell>8. Do you like weather here in Boston?</cell></row><row><cell>9. A terrible thing just happened in Nepal recently.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 . Details of training and testing data Train Test</head><label>3</label><figDesc></figDesc><table><row><cell>Head Nods</cell><cell>25</cell><cell>37</cell></row><row><cell>Head Shakes</cell><cell>20</cell><cell>28</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 . Recognition results for the training set Recognized Head Nods Head Shake Misses</head><label>4</label><figDesc></figDesc><table><row><cell>Head Nods</cell><cell>23</cell><cell>0</cell><cell>2</cell></row><row><cell>Head Shakes</cell><cell>0</cell><cell>19</cell><cell>1</cell></row><row><cell></cell><cell cols="3">Recognition Rate for Head Nods : 92.0 %</cell></row><row><cell></cell><cell cols="3">Recognition Rate for Head Shakes : 95.0 %</cell></row><row><cell></cell><cell cols="2">Combined Recognition Rate</cell><cell>: 93.34 %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 . Recognition results for the testing set</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Recognized</cell><cell></cell></row><row><cell></cell><cell>Head Nods</cell><cell>Head Shake</cell><cell>Misses</cell></row><row><cell>Head Nods</cell><cell>30</cell><cell>0</cell><cell>7</cell></row><row><cell>Head Shakes</cell><cell>0</cell><cell>21</cell><cell>7</cell></row><row><cell></cell><cell cols="3">Recognition Rate for Head Nods : 81.08 %</cell></row><row><cell></cell><cell cols="3">Recognition Rate for Head Shakes : 75.0 %</cell></row><row><cell></cell><cell cols="2">Combined Recognition Rate</cell><cell>: 78.46 %</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>We thank Dave Koons, Ted Selker, Surj Patel and Yue Hann Chin for their help to build the infrared camera sensor. This research was supported by NSF ROLE grant 0087768.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nudge Nudge Wink Wink: Elements of Face-to-Face Conversation for Embodied Conversational Agents</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cassell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Embodied Conversational Agents</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">D Head Pose Estimation without Feature Tracking</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fukumoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yachida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>IEEE International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Charles</forename><surname>Darwin</surname></persName>
		</author>
		<title level="m">The Expression of the Emotions in Man and Animals</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1872">1872. 1998</date>
		</imprint>
	</monogr>
	<note>third edition</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Givens</surname></persName>
		</author>
		<ptr target="http://members.aol.com/nonverbal2/diction1.htm#TheNONVERBALDICTIONARY" />
		<title level="m">Dictionary of gestures, signs &amp; body language cues</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting and Tracking Eyes by Using their Physiological Properties, Dynamics and Appearance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Haro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Flickner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2000-06">June 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3-D Facial Pose and Gaze Point Estimation using a Robust Real-Time Tracking Paradigm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heinzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zelinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>IEEE International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-time Detection of Nodding and Head-shaking by Directly Detecting and Tracking the &quot;Between-Eyes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kawato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ohya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>IEEE International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bodytalk: The Meaning of Human Gestures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Morris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Crown Publishers</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Pupil Detection and Tracking Using Multiple Light Sources</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Flickner</surname></persName>
		</author>
		<ptr target="http://domino.watson.ibm.com/library/cyberdig.nsf/Home" />
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>IBM Almaden Research Center</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A tutorial on Hidden Markov Models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1989-02">February 1989</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural Network-Based Face Detection</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1998-01">January 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rotation Invariant Neural Network-Based Face Detection</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Rowley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual-state Parametric Eye Tracking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>IEEE International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature Extraction from Faces using Deformable Templates</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haallinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
