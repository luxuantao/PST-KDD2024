<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unifying Model Explainability and Robustness for Joint Text Classification and Rationale Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-12-20">20 Dec 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dongfang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology (Shenzhen)</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Baotian</forename><surname>Hu</surname></persName>
							<email>hubaotian@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology (Shenzhen)</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingcai</forename><surname>Chen</surname></persName>
							<email>qingcai.chen@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology (Shenzhen)</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tujie</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology (Shenzhen)</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingcong</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology (Shenzhen)</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yunan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology (Shenzhen)</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unifying Model Explainability and Robustness for Joint Text Classification and Rationale Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-12-20">20 Dec 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2112.10424v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent works have shown explainability and robustness are two crucial ingredients of trustworthy and reliable text classification. However, previous works usually address one of two aspects: i) how to extract accurate rationales for explainability while being beneficial to prediction; ii) how to make the predictive model robust to different types of adversarial attacks. Intuitively, a model that produces helpful explanations should be more robust against adversarial attacks, because we cannot trust the model that outputs explanations but changes its prediction under small perturbations. To this end, we propose a joint classification and rationale extraction model named AT-BMC. It includes two key mechanisms: mixed Adversarial Training (AT) is designed to use various perturbations in discrete and embedding space to improve the model's robustness, and Boundary Match Constraint (BMC) helps to locate rationales more precisely with the guidance of boundary information. Performances on benchmark datasets demonstrate that the proposed AT-BMC outperforms baselines on both classification and rationale extraction by a large margin. Robustness analysis shows that the proposed AT-BMC decreases the attack success rate effectively by up to 69%. The empirical results indicate that there are connections between robust models and better explanations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural models have demonstrated their superior ability on text classification task, especially when based on pre-trained language models (PLMs) <ref type="bibr">(Devlin et al. 2019;</ref><ref type="bibr" target="#b13">Liu et al. 2019</ref>). However, they are more like black boxes compared to traditional machine learning methods such as logistic regression and decision tree. It is notoriously difficult to understand why neural models produced particular predictions (Samek, <ref type="bibr" target="#b27">Wiegand, and Müller 2017;</ref><ref type="bibr" target="#b27">Rudin 2019)</ref>. One practical approach is to extract prediction's rationales from input <ref type="bibr" target="#b12">(Lipton 2016;</ref><ref type="bibr">Camburu et al. 2018;</ref><ref type="bibr" target="#b29">Thorne et al. 2019)</ref>. The rationales can be defined as text snippets or subsets of the input text. The assumption is that a correct prediction can be made from the rationale alone <ref type="bibr" target="#b10">(Lei, Barzilay, and Jaakkola 2016;</ref><ref type="bibr" target="#b2">Bastings, Aziz, and Titov 2019;</ref><ref type="bibr" target="#b5">DeYoung et al. 2020)</ref>. In other words, rationales should be sufficient to support the model's prediction. Our work also falls into this scope, which aims to "The Saint Takes Over" stars George Sanders as Simon Templar, aka "The Saint" in this 1940 entry into the series. It also stars Wendy Barrie, Jonathan Hale and Paul Guilfoyle ...It is very enjoyable.</p><p>"The Saint Takes Over" stars Halo Sanders as Kathrina Templar, aka "The Saint" in this 988 entry into the series. It also stars Wendy Barrie, Jonathan Hale and Paul Cobb ... It's very enjoyable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Positive (100%)</head><p>Negative (80%)</p><p>Original Text:</p><p>Adversarial Example:</p><p>Golden Label:</p><p>Table <ref type="table">1</ref>: We test the pre-trained BERT-base movie review classifier using one adversarial example generated by CHECK-LIST <ref type="bibr" target="#b26">(Ribeiro et al. 2020)</ref>. The general meaning of the text remains unchanged. However, the predictions of the model change from positive to negative, while the associated rationales which enable users to verify the predictions quickly do not change. Human-labeled rationales are shown in the italic font.</p><p>achieve better prediction performance and model explainability by extracting prediction closely-related rationales.</p><p>Previous works proposed to use the pipeline approach where task prediction is performed in two steps: the explanation phase and the subsequent prediction phase <ref type="bibr" target="#b10">(Lei, Barzilay, and Jaakkola 2016)</ref>. The challenge is to attain superior task performance conditioned on the extractive rationale. Since most works that employ this framework tend to rely solely on task labels, they sample rationales from input in the explanation phase. For example, these models simulate the intractable sampling step by proposing optimization procedures based on reinforcement learning approaches <ref type="bibr" target="#b10">(Lei, Barzilay, and Jaakkola 2016;</ref><ref type="bibr" target="#b32">Yoon, Jordon, and van der Schaar 2019)</ref> and reparameterization techniques <ref type="bibr" target="#b2">(Bastings, Aziz, and Titov 2019;</ref><ref type="bibr" target="#b8">Latcinnik and Berant 2020)</ref>, which may be sensitive to hyperparameters and requires complicated training process <ref type="bibr" target="#b5">(Jain et al. 2020)</ref>. Instead, we optimize the joint likelihood of class labels and extractive rationales for the input examples. Although it is a relatively straightforward way to optimize the explanation phase models, there are at least two challenges for this task. Firstly, previous works are vulnerable to different types of adversarial attack. For example, a classifier suffers from defending against the labeling-preserving adversaries as shown in Table <ref type="table">1</ref>. If adding small perturbations to the input modifies the model's prediction, we cannot trust the explanations output from the model. We further analyze existing methods suffer from text attacks by using robustness test, which performs model-agnostic attacks on the trained classification model ( §4.3). Secondly, the explicit boundary information is ignored, leading to inaccurate extraction. For example, "interesting" and "inspiring" are boundaries of the rationale for the text "this film is interesting and inspiring.", while "is" and "." are general tokens whose representation is different from emotion words. Besides, models that use rationales to train explanation phase models do not consider the supervision signal from task <ref type="bibr" target="#b5">(DeYoung et al. 2020)</ref>.</p><p>To address these challenges, we propose a joint classification and rationale extraction framework AT-BMC where task prediction and rationale extraction are learned jointly with mixed Adversarial Training (AT) and Boundary Match Constraint (BMC). Firstly, we apply perturbation in both the discrete text space and the embedding space to improve both the generalization and robustness of the model. On the one hand, we generate adversarial examples at word-level while preserving the rationale unchanged. The perturbations also maintain prediction invariance. On the other hand, our adversarial training in the embedding space refines the standard adversarial training <ref type="bibr" target="#b17">(Madry et al. 2018</ref>) in computation efficiency and training smoothness. Secondly, we consider matching constraints by modeling both the boundary positions, which allows the model to further focus on the boundary-relevant regions. The main idea of boundary constraint is to make the sequence labeling model to consider the boundary information when locating entities. By matching a predicted start index of a rational span with its corresponding end index, the global sequence labeling information is fused with local region-aware information. In addition, we condition the extraction models on the classification label through label embedding. We conduct extensive experiments on two benchmark datasets (i.e., Movie Reviews and MultiRC). The experimental results demonstrate that AT-BMC outperforms the competitive baselines on classification and rationale extraction by a large margin. Robustness analysis further shows that AT-BMC can effectively improve the robustness of the models where the attack success rate decreases from 96% to 27% under strong adversarial attacks. The code is available at https://github.com/crazyofapple/AT-BMC.</p><p>The contributions of this paper are summarized as follows:</p><p>• Different from previous methods such as pipelines, we propose AT-BMC for joint classification and rationale extraction, which allows two parts to improve each other. We also show that our approach can be applied in the context of only limited annotated examples.</p><p>• To the best of our knowledge, this is the first work that considers explainability and robustness both in one text classification model. As a step towards understanding the connection between explainability and robustness, we provide evidence that robust models lead to better rationales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Rationale Extraction The task aims to extract snippets that can support prediction in input sequences. These text snippets allows humans to verify the correctness of predictions quickly <ref type="bibr" target="#b33">(Zaidan and Eisner 2008;</ref><ref type="bibr" target="#b34">Zhang, Marshall, and Wallace 2016;</ref><ref type="bibr" target="#b27">Ross, Hughes, and Doshi-Velez 2017;</ref><ref type="bibr" target="#b5">DeYoung et al. 2020)</ref>. For example, <ref type="bibr" target="#b21">Paranjape et al. (2020a)</ref> leverage the information bottleneck principle to extract rationales of desired conciseness. <ref type="bibr" target="#b28">Sha, Camburu, and Lukasiewicz (2020)</ref> propose a selector-predictor method to squeeze information from the predictor to guide the selector in extracting the rationales. Unlike post-hoc methods <ref type="bibr" target="#b25">(Ribeiro, Singh, and Guestrin 2016;</ref><ref type="bibr" target="#b15">Lundberg and Lee 2017)</ref>  properties. On the other hand, our goal is to focus on understanding the connection between the two in text classification tasks, and we hope it sheds light on the future development of such methods in natural language processing tasks.</p><p>3 Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Text Classification with Rationale Extraction</head><p>The aim of this paper is to design a model that can yield accurate predictions and provide closely-related extractive rationales (i.e., supporting evidence) as potential reasons for predictions. Taking the sentiment classification as an example, for the text "titanic is so close to being the perfect movie...", the predictive label of it is positive, and one of rationales for this prediction is "so close to being the perfect movie".</p><p>Therefore, text classification with rationale extraction can be formalized as follows. Given a sequence of words as input, namely</p><formula xml:id="formula_0">x = [x 1 , • • • , x L ],</formula><p>where L is the length of the sequence and x i denotes the i-th word. The goal is to infer the task label ŷ and to assign each word x i with a boolean label denoted as êi ∈ {0, 1}, where êi = 1 indicates word i is a part of the rationale. We denote the rationale of the sequence as ê ∈ {0, 1} L . The corresponding golden label is denoted as y and human-labeled rationale is denoted as e, both of them is used for training. Here, rationales are sequences of words and hence a potential rationale is a sub-sequence of the input sequence. Note that one text sample may contain multiple non-overlapping sub-sequences as rationale spans.</p><p>ADV -Augment </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Framework</head><p>We propose to construct a model that consists of an extraction network g and a prediction network f , given the training data consisting of n points D = {(x 1 , y 1 )...(x n , y n )}. The prediction network f first maps the text sequence x i to the task output ŷi . At the same time, the input x i is also fed into rationale extraction network g to output supporting evidence e i .</p><p>The basic scheme follows an multi-task learning (MTL) framework <ref type="bibr" target="#b3">(Caruana 1997)</ref> with two tasks -( <ref type="formula">1</ref>) rationale extraction and (2) the actual prediction task. We adopt the shared encoder architecture of MTL where both the tasks share the same encoder enc(•) but different decoders. Formally, the conditional likelihood of the output labels and evidence, given the input, can be written as:</p><formula xml:id="formula_1">L = n i=1 log p(y i , e i |x i ).</formula><p>(1)</p><p>Note that given rationale data, our objective is to learn enc(•), f (•), and g(•) that predict both the task labels y i and rationale labels e i given x i . We assume that only m points have evidence annotations e. Therefore, we can factorize the likelihood as follows:</p><formula xml:id="formula_2">L = n i=1 log p(y i |x i ) + m j=1 log p(e j |ŷ j , x j ). (2)</formula><p>Consequently, the predicted label and the text sequence x i are fed into the extraction network g to generate the evidence labels êi . The overall architecture of our approach is presented in Figure <ref type="figure" target="#fig_0">1</ref>  <ref type="formula">2018</ref>), we also condition the extraction model on the predicted label output from the classification model. We implement this by using an embedding lookup layer, and add the label embedding to each token representation of encoder. Moreover, applying label-specific embedding can help to validate different behaviour of the rationales via changing the ŷi .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mixed Adversarial Training</head><p>Since the searching space of adversarial attacks is large and marked rationales is limited, we perform discrete adversarial attack based data augmentation on the samples with rationale. By introducing the word-level perturbed versions of existing samples, we can recursively reuse augmentation to significantly expand the training data set. For simplicity, the validation here only considers adding one new textual edit for each sample. In addition, considering the label-preserving of rationales, perturbations only include those parts of the sentence that are outside of the rationales.</p><p>Specifically, we change text by using the linguistic transformations following the CHECKLIST behavioral testing <ref type="bibr" target="#b26">(Ribeiro et al. 2020</ref>). We use 4 invariance test by using TextAttack <ref type="bibr" target="#b19">(Morris et al. 2020)</ref>, including name replacement, position replacement, number change, and contraction/expansion of named entities. The invariance test is to apply perturbations that retain the label to the input and to expect the model predictions to remain unchanged. The more detail descriptions of each transformation are: (a) name replacement refers to the conversion of the input by replacing the name of the recognized name entity; (b) location conversion refers to the conversion of the recognized location entity of a sentence to another location given the location dictionary; (c) number change refers to the recognition of the number in the sentence to return the sentence with the changed number; and (d) the last transformation refers to the expansion or contraction of the identified name entity combination. All replacement words (from pre-defined named entity dictionary) have the same part-of-speech as the original word tagged by flair <ref type="bibr" target="#b0">(Akbik et al. 2019)</ref>. Here, the percentage of words to replace per augmented example is set to 0.2.</p><p>Apart from applying perturbation to the input text directly, we also leverage adversarial training operated on the embedding space as an effective regularization to improve the shared encoder enc(•) generalization and reduce robust error. It aims to minimize the following objective:</p><formula xml:id="formula_3">min θ E (x,y)∼D {L adv + L classif y ; θ} ,<label>(3)</label></formula><formula xml:id="formula_4">L adv = α • L pat + β • L kl ,<label>(4</label></formula><p>) where L classif y = L(f θ (x), y) is the cross-entropy loss on original data, L pat is the perturbations-based adversarial training loss (PAT), and L kl is a smoothing adversarial regularization term. α, β is a hyperparameter. We define the PAT loss as:</p><formula xml:id="formula_5">L pat (θ) = max ||δ||≤ L(f θ (x + δ), y) , (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where L is the cross-entropy loss on adversarial embeddings and δ is the perturbation. We constrain δ by using Frobenius norm bounded by . The outer minimization in Eqn.</p><p>(3) can be dealt with SGD for optimization. And the inner maximization in Eqn. ( <ref type="formula" target="#formula_5">5</ref>) can be solved reliably by projected gradient descent (PGD) <ref type="bibr" target="#b17">(Madry et al. 2018)</ref>. It is a standard method for large-scale constrained optimization, which takes the following step with step-size η at each iteration:</p><formula xml:id="formula_7">δ t+1 = Π ||δ||≤ (δ t + ηg(δ t )/||g(δ t )|| F ) ,<label>(6)</label></formula><p>where g(δ t ) = ∇ δ L(f θ (x + δ), y) is the gradient of the loss w.r.t. δ, and Π ||δ||≤ performs a projection onto the -ball. To further regularize the trade-off between standard objective and adversarial objective, we consider label smoothness in the embedding neighborhood by using term L kl (θ), which is defined as:</p><formula xml:id="formula_8">L kl (θ) = max ||δ||≤ L kl (f θ (x + δ), f θ (x)) ,<label>(7)</label></formula><p>where L kl (p, q) = [KL(p||q) + KL(q||p)]/2, p, q denote the two probability distributions, and KL(•) denotes the Kullback-Leibler divergence with temperature equals 1.0.</p><p>In contrast to Eqn. (5) which promotes adversarial attacks that retain labels, Eqn. ( <ref type="formula" target="#formula_8">7</ref>) further asserts that the confidence level of the prediction should also be similar on the probability vector, which is characterized by the simplex form ∆ where its dimension equals the number of classes.</p><p>Compared with standard training, K-step PGD requires K forward-backward passes through the network, which is computationally expensive. Besides, only the last step of perturbation is used for model parameter update after K-step. We follow free adversarial training framework in FreeLB <ref type="bibr" target="#b35">(Zhu et al. 2019)</ref> to perform multiple PGD iterations to construct the adversarial embedding and iterate the cumulative parameter gradient ∇ θ L in each iteration. Afterward, the model parameters θ are updated one at a time with the accumulated gradients effectively, by virtually creating one batch that is K times larger than sampled mini-batch. For convenience, we provide the details of adversarial training on embedding space in Algorithm 1. for each batch B sampled from D do 4:</p><p>δ ∼ N 0, σ 2 I 5:</p><p>g θ 0 ← 0 6:</p><p>for t = 1 . . . K do 7:</p><p>Accumulate gradient of θ given δt−1: 8:</p><formula xml:id="formula_9">g θ t ← g θ t−1 + 1 K EB[∇ θ (L adv + L classif y )] 9:</formula><p>Update the perturbation δ via gradient ascend: 10:</p><formula xml:id="formula_10">g δ adv ← ∇ δ L adv 11: δt ← Π δ F ≤ (δt−1 + η • g δ adv / g δ adv F ) 12:</formula><p>end for 13:</p><p>θ ← θ − τ g θ K 14: end for 15: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Boundary Match Constraint</head><p>For rationale extraction, the start/end boundaries can be captured by CRF decoder. As the CRF learns the conditional probability of the label sequence given the observation sequence features, it can be seen as maximum log-likelihood objective function conditioned on the observation X. However, CRF has the limitation of occasionally generating illegal sequences of tags, as it only encourages legal transitions and penalizes illegal ones softly <ref type="bibr" target="#b31">(Wei et al. 2021)</ref>. Hence, we propose to use a boundary constraint to encourage it to be more accurate when positioning the boundary.</p><p>The basic idea of boundary constraint is to match a predicted start index of a rational span with its corresponding end index. Given the sequence hidden representations H output from enc(•), we first predict the probability of each token as the starting indices, as follows:</p><formula xml:id="formula_11">P s = Softmax(HW s ) ∈ R L×2 ,<label>(8)</label></formula><p>where W s ∈ R d×2 is the weights to learn and d is the hidden size. Each row of P s presents the probability distribution of each index being the start position of an word. Similarly, we can calculate the end index prediction logits by using another matrix W e to obtain probability matrix P e :</p><formula xml:id="formula_12">P e = Softmax(HW e ) ∈ R L×2 .<label>(9)</label></formula><p>After that, by applying argmax to each row of P s and P e , we obtain the predicted indexes that might be the starting or ending positions, i.e., Ês and Êe :</p><formula xml:id="formula_13">Ês , Êe = {i, j | argmax(P (i) s ) = 1, argmax(P (j) e ) = 1} ,<label>(10)</label></formula><formula xml:id="formula_14">where i = 1, • • • , L, j = 1, • • • , L.</formula><p>Given any start index i ∈ Ês and end index j ∈ Êe , a binary classification model is trained to predict the probability that they should be matched:</p><formula xml:id="formula_15">o i,j = sigmoid(W • [H i , H j ]) ,<label>(11)</label></formula><p>where W ∈ R 1×2d is the weights to learn. Hence, the span match loss is</p><formula xml:id="formula_16">L match = − i,j c i,j log o i,j ,<label>(12)</label></formula><p>where c i,j denotes the golden labels for whether this rowcolumn position should be matched with the start index and end index of rationale spans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training</head><p>We define the final weighted loss as follow,</p><formula xml:id="formula_17">L = L classif y + Lextract + λ1L adv + λ2L match ,<label>(13)</label></formula><p>where λ 1 , λ 2 are hyper-parameters of L adv in Eqn. ( <ref type="formula" target="#formula_4">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>Evaluation Metrics For classification, we use classification accuracy between the predicted class label and the actual label. For rationale extraction, we report the token F1 to evaluate the quality of extraction. The micro-averaged F1 score computes at the token level between the predicted evidence spans tokens and the gold rationale tokens in terms of sets of predicted positions.</p><p>Experimental Setup We use the BERT-base model released by Google to encode text <ref type="bibr">(Devlin et al. 2019)</ref>. We also compare the performance of AT-BMC which uses the pre-trained RoBERTa large model <ref type="bibr" target="#b13">(Liu et al. 2019)</ref>. Our model is orthogonal to the choice of the pre-trained language model. We use AdamW optimizer <ref type="bibr" target="#b14">(Loshchilov and Hutter 2019)</ref>  Comparison of Baselines We compare our AT-BMC method with following competitive methods for classification and rationale extraction in both datasets: 1) The pipeline approaches <ref type="bibr" target="#b9">(Lehman et al. 2019;</ref><ref type="bibr" target="#b10">Lei, Barzilay,</ref> and Jaakkola 2016) use independent parts for both extraction and prediction. These two pipeline modules are trained with classification labels and rationales labels, respectively. 2) The information bottleneck method <ref type="bibr" target="#b22">(Paranjape et al. 2020b</ref>) extracts sentence-level rationales by measurement of maximal (and minimal) mutual information with the label (and input).</p><p>3) The FRESH approach (Jain et al. 2020) extract k tokens by using attention scores for downstream classification. 4) The weakly-and semi-supervised methods <ref type="bibr" target="#b23">(Pruthi et al. 2020</ref>) present a classify-then-extract framework condition the rationale extraction on the classification. The pipeline approach uses RNNs, whereas the base model is BERT-base for IB, FRESH, and <ref type="bibr" target="#b23">(Pruthi et al. 2020)</ref>, as same as ours for a fair comparison. The performances of baselines are from reference papers. As shown in Table <ref type="table" target="#tab_2">2</ref>, our model improves over the previous models on both datasets. In the task of rationale extraction, AT-BMC (BERT-base) and AT-BMC (RoBERTa-large) improves 4.3 and 13.3 F1 points over the previous models on the Movie Reviews dataset. Moreover, on the MultiRC dataset, our method also improves the F1 up to 3.3 and 10.8 points. On the other hand, AT-BMC (BERTbase) improves 0.8 and 1.3 in terms of accuracy respectively, which might come mainly from two main aspects: one is multi-task learning and the other is adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Robustness Evaluation</head><p>The robustness test is to perform model-agnostic attacks on the trained classification model. It verifies the robustness of our method to attack samples; hence we can better classify and extract evidence by which humans verify predictions. It assumes that the model input is text sequences and returns outputs that the objective function can process. In this process, the attack algorithm will find the disturbance of the Table <ref type="table">3</ref>: Comparison of baselines and AT-BMC using three adversarial attack methods. The baseline is the vanilla BERT fine-tuned on the IMDB dataset <ref type="bibr" target="#b16">(Maas et al. 2011)</ref>. We compare performance in accuracy under attack, attack success rate (SR), average percentage of perturbed words in the sentence (PW), average attack queries of all examples (AQ) and total attack time. All numbers are reported on 100 test instances. ↑ (↓) represents that the higher (lower) the better.</p><p>input sequence that satisfies the attack target and follows a certain language restriction. In this way, the attack to models is framed as a combinatorial search problem. In order to find a series of perturbations that produce a successful adversarial example, the attacker must search through all possible conversions. We refer the reader to <ref type="bibr" target="#b19">(Morris et al. 2020</ref>) for more details.  Instead of measuring robustness by interpretability robustness where rationales should be invariant to small perturbations in the input, we consider three different attack methods (i.e., TextFooler <ref type="bibr" target="#b6">(Jin et al. 2020)</ref>, TextBugger <ref type="bibr" target="#b11">(Li et al. 2019)</ref>, and PWWS <ref type="bibr" target="#b24">(Ren et al. 2019</ref>)) to classification robustness. In the test, we focus on the success rate of the attack. TextFooler and TextBugger use a mixture of measures (e.g., word embedding distance, part-of-speech tags match), and a word replacement mechanism is designed to attack the existing model; PWWS greedily uses word importance ranking to replace parts of sentences, where word saliency and synonym swap scores are used to calculate word importance. The results of classification models on the Movie Reviews test set are presented in Table <ref type="table">3</ref>. Overall, AT-BMC achieves the best performance on all metrics consistently across different attack methods. Notably, AT-BMC substantially outperforms the baseline by 69.75% success rate under PWWS attack. We attribute this to AT-BMC's generalizability obtained by adversarial training. Interestingly, from the results in the second column, using a joint framework also seems to enhance generalizability and robustness in this domain. We also report the percentage of words being replaced for attacking as average word modification rates. Our method needs more modified attack queries with higher word modification rates under all attacks. It indicates that the model is harder to attack and hence requires more words to be replaced. In Figure <ref type="figure" target="#fig_5">2</ref>   <ref type="figure" target="#fig_6">3</ref>, we compare the performance of models with varying proportions of human-labeled rationales in the training set. We find that the model achieves extraction accuracy above 40 on the test set when only 5% of examples with labeling signals. As the ratio of these marker examples increases, the performance of the model improves. Since the manual labeling of these annotations is time-consuming and labor-intensive, this might imply that our approach can stably generate reasonable interpretations without many manual annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis and Discussion</head><p>Ablation Study To study the effect of each part, we conduct ablation experiments. The results are also shown in Table <ref type="table" target="#tab_2">2</ref>. From the experimental results, adversarial examples, boundary match loss, and virtual adversarial training all contribute to the performance improvement of the model, each helping to improve the model performance by about two percentage points in rationale extraction. We can see that the match loss improves token F1 relatively the most on the Movie Reviews dataset, probably because after adding this term, the extraction model may focus more on local boundary information. For the original task accuracy, adversarial training can boost performance, and both adversarial data augmentation and virtual adversarial training in the embedding space can bring improvements. It illustrates that these two methods are complementary. The results also show that considering on the predicted label improves the extraction F1 by 0.8 and 1.1 points on both datasets.</p><p>Effectiveness Evaluation Human-labeled rationales contain some implicit information that represents partial inductive bias in predictions. The interpretable AI community would like to use them as a guide for evaluating model interpretations and possibly for teaching models to make robust and reasonable decisions. Compared with the answer prediction task, existing models have relatively lower prediction results on the Token F1 metric in the rationale extraction task. Hence, we randomly sampled 50 correct examples and ask two annotators to judge the relevance between extracted rationales and labels. The inter-rater agreement coefficient between two annotators is 0.85. The relevance results of two datasets are 45 and 47, which means high scores between predicted labels and extracted rationales. We also compare the model without the boundary match constraint. The relevance results are 39 and 42. The comparison shows that by matching starting and ending tokens, the model can be more relevant to labels and aligned with human interpretation.</p><p>Error Analysis We conduct an error analysis on extraction rationales generated by our model on 50 randomly chosen examples from the development set of Movie Reviews. The major error types are summarized as follows: 1) the dominant type (48%) is that the model outputs contain bags of small fragments, which overlap with human evidence. For example, some fragments are emotional expressions, such as "perfect", "some of the best". 2) the second type (24%) is caused by incomplete or inadequate annotations. For example, the model outputs "it's a very impressive film" and "wonderfully presented story", while humans only annotate the former. It demonstrated that marked rationales do not necessarily have high comprehensiveness by including all relevant information, which aligns with the findings of Carton, Rathore, and Tan (2020). 3) the last type (28%) is caused by several factors, such as the text is too long and the evidence is outside the truncated text; or the prediction of the model itself is wrong. It shows that only learning from the supervision signal may be affected by annotation artifacts and variance between human-annotated rationales. And how the machine can help us correct superficial clues instead of learning could be another interesting topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we focus on how to jointly classify and provide extracted rationales, so that humans can use it to verify the correctness of the prediction. We propose a method AT-BMC for jointly modeling text classification and rationale extraction using mixed adversarial training and boundary match constraint. The results on two public data sets show that our method improves the performance of the model on the task, especially with increasing extraction token F1 for rationales. Besides, AT-BMC can remarkably decrease the attack success rate compared to the baseline under different attack methods. The results indicate that robust models lead to better extracted rationale in text classification tasks. In the future, we will explore how to apply our model to more domains (e.g., medical and legal domains).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overall architecture of the proposed AT-BMC method for joint classification and rationale extraction with mixed adversarial training and boundary match constraint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>. Since we can optimize the classification objective for all n instances and the extraction objective for m instances, it allows us to train the model when only part of samples have human-labeled rationales. Concretely, we first encode the input into hidden representation by using pre-trained language models (Devlin et al. 2019; Liu et al. 2019) as the shared encoder. Then we use a linear classifier to model p θ (y|x) with the cross entropy loss L classif y and a linear-chain CRF (Lafferty, McCallum, and Pereira 2001) to model p φ (e|x, y; θ) with negative loglikehood L extract . The outputs are predicted label by the linear classifier and rationale spans generated by the CRF decoder. Inspired by Wang et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 :</head><label>1</label><figDesc>Embedding-level Adversarial Training Algorithm    Require: Training samples D, perturbation bound , learning rate τ , ascent steps K, ascent step size η, the total number of epochs T , the variance of the random initialization σ 2 . 1: Initialize θ 2: for epoch = 1 . . . T do 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>with a batch size of 4 for model training. The initial learning rate, the maximum sequence length, the dropout rate, the gradient accumulation steps, the training epoch and the hidden size d are set to 2 × 10 −5 , 512, 0.1, 8, 30, 768 respectively. We clip the gradient norm within 1.0. The learning parameters are selected based on the best performance on the development set. Early stopping is also applied based on model performance on the development set. Our models are trained with NVIDIA Tesla V100s (Ubuntu 18.04 LTS &amp; PyTorch). We set the perturbation size = 1 × 10 −5 , the step size η = 1 × 10 −3 , ascent iteration step K = 2 and the variance of normal distribution σ = 1 × 10 −5 . The weight parameters λ 1 , λ 2 are set to 1.0, 0.1 respectively. The augmented adversarial examples of both datasets are 1198 and 24007.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(BERT-base) Pruthi et al. (2020) AT-BMC w/o adv (BERT-base) AT-BMC (BERT-base) 50%</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Extraction F1 curves on the development set of Movie Reviews. Our model with BERT-base is trained on 100%, 50% of the fraction of rationales in the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison in task accuracy shown in red and extraction performance shown in blue as the number of rationales retained in the training set varies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>MultiRC (Khashabi et al. 2018). The corpus comprises of multiple-choice questions and answers from various sources along with supporting evidence. This dataset concatenates each answer candidate to one question and assigns a binary label to it (i.e., whether this answer can answer the question or not). Each QA pair is associated with a related passage that is annotated with sentence-level rationales. The training set, development set, and test set consist of 24029, 3214, and 4848 available examples. The rationale in this dataset contains sufficient context to allow the human to discern whether the given answer to the question is True or False.</figDesc><table><row><cell>),</cell></row><row><cell>L match in Eqn. (12), respectively. Note that at the training</cell></row><row><cell>time, all ground-truth labels are fed into the models to learn</cell></row><row><cell>all components of rationale extraction network and prediction</cell></row><row><cell>network simultaneously. The marked rationales for the label</cell></row><row><cell>in training are the collection of input text sequences annotated</cell></row><row><cell>by humans. The same pre-trained encoder is used by shared</cell></row><row><cell>parameters. During inference, the model recognizes rationale</cell></row><row><cell>spans of samples with golden human-labeled rationale for</cell></row><row><cell>extraction evaluations.</cell></row><row><cell>4 Experiments</cell></row><row><cell>4.1 Dataset</cell></row></table><note>Movie Reviews<ref type="bibr" target="#b23">(Pruthi et al. 2020)</ref>. This dataset includes 50k movie reviews from IMDB dataset<ref type="bibr" target="#b16">(Maas et al. 2011</ref>) and 1.8k movie reviews with human-labeled rationales collected by<ref type="bibr" target="#b34">Zaidan, Eisner, and Piatko (2007)</ref>. The labels marked by the annotator are binary sentiment indicators (i.e., Positive and Negative). The training set, development set, and test set consist of 26198, 12800, and 12800 available examples, respectively, while the types of movies in each subset are different. Note that the samples with rationale are divided into 1200, 300, and 300 respectively. Due to the long text of the review comment, it is necessary to verify the correctness of the prediction by extracting evidence. Moreover, it is also applicable to adversarial attack scenarios.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison on two text classification tasks with rationale extraction. We report test set results of AT-BMC that using different encoders (i.e., BERT-base and RoBERTa-large). Results of our AT-BMC method are averaged across 3 different seeds.</figDesc><table><row><cell>Methods</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>we compare the evaluation curves of the different methods on the development set over training time. We take the performance on the Movie Reviews development set at each evaluation checkpoint. The magnitude of change in our method is much smaller relative to other methods that did not perform adversarial training and boundary match constraint, and it converges gradually as training time changes. It illustrates that our method reinforces the robustness of the model on the development set during training, thereby making it more stable for training and less variance, which alleviates hyperparameter sensitivity and high variance in existing methods.Our approach can also be applied in the context of which there are only limited annotated examples. As shown inFigure  </figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the valuable feedback of Wenpeng Yin, Yuxiang Wu, Shuoran Jiang and the insightful comments and suggestions of the anonymous reviewers. This work is jointly supported by grants: Natural Science Foundation of China (Grant No. 61872113  and Grant No. 62006061), Stable Support Program for Higher Education Institutions of Shenzhen (No. GXWD20201230155427003-20200824155011001), Strategic Emerging Industry Development Special Funds of Shenzhen (No. XMHT20190108009 and No. JCYJ20200109113403826), Fundamental Research Fund of Shenzhen (No. JCYJ20190806112210067), and Tencent Group.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP</title>
		<author>
			<persName><forename type="first">A</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schweter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
				<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Demonstrations</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adversarial Robustness on In-and Out-Distribution Improves Explainability</title>
		<author>
			<persName><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Augustin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Meinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
				<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2020. August 23-28, 2020</date>
		</imprint>
	</monogr>
	<note>Proceedings, Part XXVI</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interpretable Neural Predictions with Differentiable Binary Variables</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bastings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rathore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; Montréal, Canada. Carton, S</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2019. 2018. 2018. 2018. December 3-8, 2018. 2020</date>
		</imprint>
	</monogr>
	<note>Proc. of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Posterior Differential Regularization with f-divergence for Improving Model Robustness</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
				<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="1997">1997. 2021</date>
		</imprint>
	</monogr>
	<note>Multitask learning. Machine learning</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event</title>
				<editor>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2021. August 14-18, 2021. 2019</date>
		</imprint>
	</monogr>
	<note>Proc. of NAACL-HLT</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SMART: Robust and Efficient Fine-Tuning for Pretrained Natural Language Models through Principled Regularized Optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deyoung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Rajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lunz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schönlieb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">;</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pinter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
				<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2020. 2019. 9-15 June 2019. 2015. 2020. 2017. 2020</date>
		</imprint>
	</monogr>
	<note>Proc. of ACL</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-02-07">2020. February 7-12, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences</title>
		<author>
			<persName><forename type="first">D</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Latcinnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05569</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning (ICML 2001)</title>
				<meeting>the Eighteenth International Conference on Machine Learning (ICML 2001)<address><addrLine>Williams College, Williamstown, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-06-28">2001. June 28 -July 1, 2001. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Explaining Question Answering Models through Text Generation</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inferring Which Medical Treatments Work from Reports of Clinical Trials</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deyoung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rationalizing Neural Predictions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">TextBugger: Generating Adversarial Text Against Real-world Applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-02-24">2021. 2019. February 24-27, 2019</date>
		</imprint>
	</monogr>
	<note>26th Annual Network and Distributed System Security Symposium, NDSS 2019</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Mythos of Model Interpretability</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08994</idno>
	</analytic>
	<monogr>
		<title level="m">Adversarial Training for Large Neural Language Models</title>
				<imprint>
			<date type="published" when="2016">2016. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>WHI</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
				<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A Unified Approach to Interpreting Model Predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning Word Vectors for Sentiment Analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards Deep Learning Models Resistant to Adversarial Attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
				<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP</title>
		<author>
			<persName><forename type="first">J</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lifland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grigsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Connecting Interpretability and Robustness in Decision Trees through Separation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moshkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
				<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24">2021. 18-24 July 2021</date>
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An Information Bottleneck Approach for Controlling Conciseness in Rationale Extraction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thickstun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An Information Bottleneck Approach for Controlling Conciseness in Rationale Extraction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Paranjape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thickstun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly-and Semi-supervised Evidence Extraction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pruthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Che</forename></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Why Should I Trust You?&quot;: Explaining the Predictions of Any Classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-13">2016. August 13-17, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond Accuracy: Behavioral Testing of NLP Models with CheckList</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
				<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.08296</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017</title>
				<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-19">2017. August 19-25, 2017. Rudin, C. 2019. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning from the Best: Rationalizing Prediction by Adversarial Information Calibration</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generating Token-Level Explanations for Natural Language Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust Machine Comprehension Models via Adversarial Training</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
				<editor>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</editor>
		<meeting>of NAACL-HLT<address><addrLine>Wang, Y</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>Proc. of ACL</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Masked Conditional Random Fields for Sequence Labeling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
				<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">INVASE: Instance-wise Variable Selection using Neural Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
				<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling Annotators: A Generative Approach to Learning from Annotator Rationales</title>
		<author>
			<persName><forename type="first">O</forename><surname>Zaidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Using &quot;Annotator Rationales&quot; to Improve Machine Learning for Text Categorization</title>
		<author>
			<persName><forename type="first">O</forename><surname>Zaidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Piatko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference</title>
				<imprint>
			<date type="published" when="2007">2007. 2020. 2016</date>
		</imprint>
	</monogr>
	<note>Proc. of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11764</idno>
		<title level="m">Freelb: Enhanced adversarial training for language understanding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
