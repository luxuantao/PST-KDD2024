<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiway Spectral Graph Partitioning: Cut Functions, Cheeger Inequalities, and a Simple Algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-02-08">February 8, 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lars</forename><surname>Eld?n</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Link?ping University</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multiway Spectral Graph Partitioning: Cut Functions, Cheeger Inequalities, and a Simple Algorithm</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-02-08">February 8, 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2302.03615v1[math.NA]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Undirected graph</term>
					<term>multiway spectral partitioning</term>
					<term>adjacency matrix</term>
					<term>eigenvalue</term>
					<term>cut function</term>
					<term>Cheeger inequality</term>
					<term>indicator form</term>
					<term>algorithm. MSC classification: 65F30</term>
					<term>05C50</term>
					<term>68R10</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The problem of multiway partitioning of an undirected graph is considered. A spectral method is used, where the k &gt; 2 largest eigenvalues of the normalized adjacency matrix (equivalently, the k smallest eigenvalues of the normalized graph Laplacian) are computed. It is shown that the information necessary for partitioning is contained in the subspace spanned by the k eigenvectors. The partitioning is encoded in a matrix ? in indicator form, which is computed by approximating the eigenvector matrix by a product of ? and an orthogonal matrix. A measure of the distance of a graph to being k-partitionable is defined, as well as two cut (cost) functions, for which Cheeger inequalities are proved; thus the relation between the eigenvalue and partitioning problems is established. Numerical examples are given that demonstrate that the partitioning algorithm is efficient and robust.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Let G be an undirected graph on n nodes. A key problem in many applications (see e.g. the surveys <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9]</ref>) is to partition the graph into k subgraphs, which are internally well connected, and where each subgraph is only loosely connected to the others. Spectral 2-partitioning (partitioning into two subgraphs, i.e. k = 2) is based on early results by Fiedler <ref type="bibr" target="#b7">[8]</ref>. This is a standard method for clustering that has the advantage over many other clustering methods in that it has a well developed mathematical theory, see <ref type="bibr" target="#b1">[2]</ref>. The subject of the present paper is multiway spectral partitioning, or spectral k-partitioning, which is the partitioning into k &gt; 2 subgraphs at the same time.</p><p>Traditionally, spectral partitioning is motivated by first defining a cost function (we will call it a cut function) for partitioning the connected graph G into subgraphs, see <ref type="bibr" target="#b22">[23,</ref><ref type="bibr">Section 5]</ref>. Minimization of the cost function leads to an optimization problem over all possible partitionings of the graph, which is an NP-hard problem. Spectral partitioning is a way of solving relaxed versions of such a problem, where the optimization is done over vectors in R n , and the problem becomes that of finding the k smallest eigenvalues of the Laplacian of the graph and the corresponding matrix of eigenvectors X ? R n?k . The partitioning information is then computed from the eigenvectors, using a clustering method for n points in R k (in the literature usually the Kmeans algorithm).</p><p>The eigenvalue problem can be formulated, min</p><formula xml:id="formula_0">Y T Y =I I -Y T AY ,</formula><p>(A is the adjacency matrix and ? denotes the Frobenius norm), which is solved by the eigenvector matrix X. We define the minimum in (1) as a measure of the distance of the graph to being k-partitionable.</p><p>The method presented in this paper is based on two observations. Firstly, the information needed for spectral k-partitioning is contained, not in the eigenvectors as such, but rather the subspace spanned by the eigenvectors. That subspace can be represented by XQ for any orthogonal matrix Q. Secondly, any k-partitioning can be encoded by a matrix ? ? R n?k , where ? ij = 0 if node i belongs to partition j, and ? ij = 0 otherwise. Thus, given the matrix X of eigenvectors, we will solve approximately the problem, min</p><formula xml:id="formula_1">Q, ? XQ -? ,<label>(1)</label></formula><p>with suitable constraints on Q and ?. The partitioning is then obtained directly from ?. An algorithm for solving <ref type="bibr" target="#b0">(1)</ref> was given in <ref type="bibr" target="#b6">[7]</ref>. It is the basic ingredient in the novel fast and reliable multiway partitioning algorithm. Two cut functions are defined, based on the representation of the partitioning as the matrix ?. From these definitions Cheeger inequalities can be proved, and thereby the relation between the eigenvalue problem and the partitioning problem is established.</p><p>The paper is organized as follows. After a short description of notation and relevant concepts in Section 2, we briefly review the classical 2-partitioning problem in Section 3. The k-partitioning problem is introduced in Section 4. Our formulation of the partitioning problem leads naturally to a cut function, for which a Cheeger inequality follows directly, Section 4.1. An alternating method for solving (1) is described in Section 4.2, and the computation of a starting approximation <ref type="bibr" target="#b2">[3]</ref> in Section 4.3. Another cut function that gives a tighter Cheeger inequality is presented in Section 4. <ref type="bibr" target="#b3">4</ref>. Numerical examples with data from applications are given in Section 5; there variants of the new method are compared to each other and to Kmeans clustering. In Section 6 some final conclusions are given.</p><p>Surveys of graph partitioning can be found in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b8">9]</ref>. Early results on multiway spectral partitioning and cut functions are given in <ref type="bibr" target="#b12">[13]</ref>. The multiway partitioning algorithm described in this paper uses the one in <ref type="bibr" target="#b2">[3]</ref> as a starting approximation. It is also related to the algorithms in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, see Section 4. Cut functions and Cheeger inequalities for k-partitioning are discussed in <ref type="bibr" target="#b11">[12]</ref>. In this paper we assume that the number of clusters k is given a priori. The problem of choosing k from the data is discussed in <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Notation and Preliminaries</head><p>The identity matrix is denoted I; its dimension will be clear from the context. The group of orthogonal matrices Q ? R k?k is denoted O(k). A matrix X ? R n?k is said to be in indicator form <ref type="foot" target="#foot_0">1</ref> , if, for some permutation matrix P , we can write it as</p><formula xml:id="formula_2">X = P ? ? ? ? ? x 1 0 ? ? ? 0 0 x 2 ? ? ? 0 . . . . . . . . . 0 0 ? ? ? x k ? ? ? ? ?</formula><p>, where all components of the vectors x i are nonzero. The set of matrices X ? R n?k , which are in indicator form and satisfy</p><formula xml:id="formula_3">X T X = I, is denoted O I (n, k). If X ? O I (n, k) is nonnegative, then we say X ? O + I (n, k).</formula><p>A k-partitioning of the integers 1, 2, . . . , n is a grouping into k disjoint and non-empty subsets, such that the union of the subsets is equal to the whole set. Such a grouping can be encoded as a matrix in O I (n, k).</p><p>We will assume that all eigenvectors of symmetric matrices are normalized to Euclidean length 1. We define unnormalized constant vectors,</p><formula xml:id="formula_4">g i = ? ? ? ? ? 1 1 . . . 1 ? ? ? ? ? ? R ni , g = ? ? ? ? ? 1 1 . . . 1 ? ? ? ? ? ? R n .<label>(2)</label></formula><p>Throughout we will use the Frobenius matrix norm, A = ( i,j a 2 ij ) 1/2 , and corresponding inner product A, B = tr(A T B).</p><p>Let the symmetric, nonnegative matrix B ? R n?n be the unnormalized adjacency matrix of an undirected graph G over n nodes (vertices). This means that an element b ij is nonnegative, if there is an edge between nodes i and j, otherwise b ij = 0. We let b ij be the edge weight. Throughout we assume that the diagonal of the adjacency matrix is zero, i.e., no self-loops are allowed in the graph.</p><p>Let d = Bg be the degree vector, where d i is the degree, i.e. the number of edges, of node i. The volume of the graph is defined Vol(G) = n i=1 d i . Define the diagonal matrix D = diag(d); thus we have Bg = Dg. The normalized adjacency matrix is defined</p><formula xml:id="formula_5">A = D -1/2 BD -1/2 .</formula><p>Denote the eigenvalues of A by ? i , i = 1, 2, . . . , n, and assume the eigenvalues are ordered,</p><formula xml:id="formula_6">? 1 ? ? 2 ? ? ? ? ? ? n .</formula><p>The normalized Laplacian matrix for G is I -A. Denote its eigenvalues by ? i . The eigenvalue problems for the normalized adjacency and the normalized Laplacian matrices are equivalent. The eigenvectors are the same, and we have</p><formula xml:id="formula_7">? i = 1 -? i , i = 1, 2, . . . , n.</formula><p>In this paper we will use the normalized adjacency matrix<ref type="foot" target="#foot_1">2</ref> ; the equivalence to the Laplacian formulation gives automatically the connection to the cost (cut) functions for partitioning the graph.</p><p>A subgraph is a subset of the nodes and corresponding edges. The graph is called connected if there is no subgraph isolated from the rest of the graph. Connectedness is equivalent to irreducibility of the adjacency matrix. Any symmetric matrix A is called reducible, if there exists a permutation matrix P such that P AP T is block-diagonal,</p><formula xml:id="formula_8">P AP T = A 1 0 0 A 2 .</formula><p>If there is no such permutation, then the matrix is called irreducible. Thus an undirected graph is connected if and only if its adjacency matrix is irreducible. If the adjacency matrix is reducible, and can be permuted to block-diagonal form with k square blocks, we will call it k-reducible. We will use the corresponding term k-partitionable for a graph that has at least k components. Throughout we will identify the graph with its adjacency matrix, and choose the concepts and notation that are most convenient in the actual situation. We now recall some basic properties of the eigenvalue problem for the normalized adjacency matrix A and its relation to connectedness of the graph (for an elementary introduction, see e.g. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">Chapter 10]</ref>, and for a comprehensive treatment, see <ref type="bibr" target="#b13">[14,</ref><ref type="bibr">Chapter 8]</ref>, <ref type="bibr" target="#b22">[23]</ref>).</p><p>The largest eigenvalue of A is ? 1 = 1, with eigenvector x 1 = ?D 1/2 g (? is a normalization constant). Clearly x 1 is positive (it is called the Perron vector). If A is irreducible (G is connected), then ? 1 is a simple eigenvalue, and x 1 is the only positive eigenvector. The second eigenvalue is strictly less than 1, and, due to orthogonality, x T 1 x 2 = 0, and x 2 (the Fiedler vector <ref type="bibr" target="#b7">[8]</ref>) must have positive and negative components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Spectral 2-partitioning</head><p>The purpose of spectral 2-partitioning is to find a partitioning of the the nodes in the graph so that the cost for splitting the graph in two is as small as possible.</p><p>As cut function it is common to use conductance. Let R be a set of nodes and R C its complement. Then the conductance of the partitioning is defined <ref type="bibr" target="#b1">[2]</ref>,</p><formula xml:id="formula_9">h(R) = |E(R, R C )| min(Vol(R), Vol(R C ))</formula><p>,</p><p>where |E(R, R C )| is the sum of the weights of all edges that start in R and end in R C . The Cheeger constant of the graph is</p><formula xml:id="formula_10">h G = min h(R),</formula><p>where the minimization is done over all partitionings. Spectral 2-partitioning is based on the fact that the closeness of the graph to disconnectedness is related to the smallness of the quantity 1 -? 2 . This is expressed in the Cheeger inequality</p><formula xml:id="formula_11">[2], h 2 G 2 &lt; 1 -? 2 ? 2h G .<label>(3)</label></formula><p>We can interpret this as follows:</p><p>There is a partitioning of the graph with small conductance if and only if 1 -? 2 is small.</p><p>Furthermore, an algorithm for computing a partitioning that is close to optimal is based on properties of the eigenvectors: x 1 &gt; 0 and x T 1 x 2 = 0. The algorithm is sketched in Algorithm 1.</p><p>Algorithm 1 Spectral 2-partitioning: Given a normalized adjacency matrix A ? R n?k , compute a reordering and partitioning, such that the cost for splitting the graph is low.</p><p>1. Compute the two largest eigenvalues and corresponding eigenvectors x 1 and x 2 of A. 2. Reorder the elements of the Fiedler vector x 2 in ascending order. Apply the reordering to the adjacency matrix. 3. Compute the conductance for partitioning the graph in the neighborhood of the sign change of the reordered vector, and split where the conductance is smallest.</p><p>Assume for the moment that the graph is disconnected, having two components, each of which is well connected. The normalized adjacency matrix can be written</p><formula xml:id="formula_12">A = A 1 0 0 A 2 .</formula><p>A has a double eigenvalue 1, and the nonunique eigenvectors can be written in indicator form</p><formula xml:id="formula_13">X = x 1 0 0 x 2 , x i &gt; 0, i = 1, 2,</formula><p>where x 1 and x 2 are the Perron vectors of A 1 and A 2 , respectively. If we add an edge with small weight ? between the components, then, from the perturbation theory for the symmetric eigenvalue problem [10, Chapter 8.1], the corresponding adjacency matrix A ? has the eigenvalues 1 and ? 2 ? 1 -? &lt; 1, and the unique eigenvectors are given by</p><formula xml:id="formula_14">X ? ? 1 ? 2 x 1 -x 1 x 2 x 2 .</formula><p>The important observation is that while the eigenvectors have been rotated by approximately 45 ? from indicator form, the subspace spanned by the eigenvectors has changed by ? [10, Chapter 8.1]. So instead of using the Fiedler vector in Algorithm 1, we could try to determine a rotation Q that will make X ? Q close to indicator form, and obtain the partitioning information from that. This idea can be generalized to the case k &gt; 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multiway Spectral Partitioning</head><p>To find more than two subgraphs, the 2-partitioning algorithm can be applied recursively. However, this is reported to give poor solutions in some common cases <ref type="bibr" target="#b18">[19]</ref>. Therefore it is often preferable to try to find a partitioning into k &gt; 2 subgraphs at the same time. A natural generalization of 2-partitioning is to compute the k largest eigenvalues of A, and the corresponding matrix of eigenvectors X = (x 1 x 2 . . . x k ) ? R n?k . However, the useful sign pattern of the first two eigenvectors does does not carry over in a way that can be easily used to find the k-partitioning. A standard method is to apply the Kmeans algorithm to the rows of X (see e.g. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref>). However, Kmeans has the disadvantage that different applications of the algorithm to the same data often give different results, cf. Section 5.</p><p>We will now describe the ideas behind spectral partitioning for k &gt; 2, and we will emphasize the well-known invariance properties of the underlying eigenvalue problem. Consider the unnormalized adjacency matrix B of a connected graph on n nodes, and define the normalized adjacency matrix</p><formula xml:id="formula_15">A = D -1/2 BD -1/2 ; (<label>4</label></formula><formula xml:id="formula_16">)</formula><p>the normalization matrix D is the standard degree matrix, satisfying Dg = Bg. With this normalization, the largest eigenvalue of A is equal to 1, and, due to the connectedness of the graph,</p><formula xml:id="formula_17">? ? &lt; 1, ? = 2, 3, . . . , k.</formula><p>Throughout we will assume that ? k &gt; ? k+1 . The basis of spectral partitioning for k &gt; 2 is to compute the k largest eigenvalues of A, and the corresponding eigenvectors. The latter can be found by considering matrices Y ? R n?k and the minimization problem min</p><formula xml:id="formula_18">Y T Y =I I -Y T AY .<label>(5)</label></formula><p>It is well-known that the eigenvector matrix X solves this problem, and the minimum is equal to</p><formula xml:id="formula_19">k ?=2 (1 -? ? ) 2 1/2 =: L k ,<label>(6)</label></formula><p>(since ? 1 = 1 ). Clearly, the solution is not unique: any matrix Y = XQ, for arbitrary Q ? O(k) is a minimizer. If instead we define the minimization problem ( <ref type="formula" target="#formula_18">5</ref>) over subspaces in R n of dimension k (the Grassmann manifold <ref type="bibr" target="#b4">[5]</ref>), then, under the assumption ? k &gt; ? k+1 , the problem has a unique solution, which is the subspace spanned by X.</p><p>The following simple result is a direct consequence of the basic properties of the eigenvalue and partitioning problems. Proposition 4.1. Let A be the normalized adjacency matrix of an undirected graph, and assume that</p><formula xml:id="formula_20">? k &gt; ? k+1 . Then L k = 0 if and only if A is k-reducible (the graph is k-partitionable).</formula><p>From the perturbation theory of the symmetric eigenvalue problem [10, Chapter 8.1] the eigenvalues ? 1 , . . . , ? k are continuous functions of perturbations of the adjacency matrix (as long as ? k &gt; ? k+1 ). Therefore, in analogy to Fiedler's definition of (1 -? 2 ) as algebraic connectivity <ref type="bibr" target="#b7">[8]</ref> for 2-partitioning, we can use L k as a measure of the distance of a graph to being k-partitionable. See also Appendix A for an illustration of the properties of L k .</p><p>Clearly, L k does not depend on the ordering of the nodes; equivalently, it does not depend on symmetric permutations of B. We will see that the same is valid for the cut functions that we will define. Therefore, it is no restriction to discuss and illustrate the partitioning problem in terms of any particular ordering. We write</p><formula xml:id="formula_21">B = D B + E,<label>(7)</label></formula><p>where</p><formula xml:id="formula_22">D B = ? ? ? ? ? B 1 0 ? ? ? 0 0 B 2 0 . . . . . . . . . 0 0 ? ? ? B k ? ? ? ? ? , E = ? ? ? ? ? 0 E 12 ? ? ? E 1k E T 12 0 E 2k . . . . . . . . . E T 1k E T 2k ? ? ? 0 ? ? ? ? ? .<label>(8)</label></formula><p>We assume that for i</p><formula xml:id="formula_23">= 1, 2, . . . , k, B i ? R ni?ni , with k i=1 n i = n.</formula><p>Note that B is irreducible (and the corresponding graph is connected) if at least one matrix E ij or E T ij in each block row is nonzero. The spectral partitioning problem can be loosely formulated:</p><p>Find a permutation P for some original adjacency matrix B, giving B = P BP T , and a k-partitioning, such that, in some sense (which will soon be clear), D B is considerably larger than E.</p><p>The partitioning of the graph, and the equivalent blocking of the adjacency matrix, can be represented in terms of a matrix ? ? R n?k in indicator form, where ? ij = 0, if node i belongs to partition j, and ? ij = 0 otherwise. This indicator matrix will be computed from the eigenvector matrix X that solves <ref type="bibr" target="#b4">(5)</ref>. But first we will derive a cut (cost) function for the partitioning of the graph. We will come back to cut functions in Section 4.4; here it is enough to say that the cut function is a measure of how much the partitioning defined by ( <ref type="formula" target="#formula_21">7</ref>)-( <ref type="formula" target="#formula_22">8</ref>) deviates from block-diagonal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">A Cut Function</head><p>As indicator matrices are crucial in the partitioning problem, we will evaluate I -Y T AY for one particular indicator matrix Y , satisfying the constraint Y T Y = I. First write the matrix D using the same partitioning as in <ref type="bibr" target="#b7">(8)</ref>, <ref type="formula" target="#formula_4">2</ref>)), and define the matrices</p><formula xml:id="formula_24">D = ? ? ? ? ? D 1 0 ? ? ? 0 0 D 2 0 . . . . . . . . . 0 0 ? ? ? D k ? ? ? ? ? , put ? i = g T i D i g i , i = 1, 2, . . . , k, (recall (</formula><formula xml:id="formula_25">? = ? ? ? ? ? ? 1 I 0 ? ? ? 0 0 ? 2 I 0 . . . . . . . . . 0 0 ? ? ? ? k I ? ? ? ? ? ? R n?n , G = ? ? ? ? ? g 1 0 ? ? ? 0 0 g 2 0 . . . . . . . . . 0 0 ? ? ? g k ? ? ? ? ? ? R n?k . (9) Then put Y = D 1/2 ? -1/2 G.</formula><p>Clearly Y is positive and in indicator form. It is straightforward to verify that Y T Y = I. Using ( <ref type="formula" target="#formula_15">4</ref>) and ( <ref type="formula" target="#formula_21">7</ref>) we can write</p><formula xml:id="formula_26">I -Y T AY = I -G T ? -1/2 D B ? -1/2 G -G T ? -1/2 E? -1/2 G =: ?,</formula><p>where the symmetric matrix ? ? R k?k is given by</p><formula xml:id="formula_27">? ij = 1 -g T i B i g i /? i , i = j, -g T i E ij g j /(? i ? j ) 1/2 , i = j.<label>(10)</label></formula><p>We now define a cut function for a given k-partitioning (8),</p><formula xml:id="formula_28">? cut = ? .<label>(11)</label></formula><p>Note that ? ii is equal to 1 minus the sum of the weights of edges within partition i, divided by the total weights of edges within and out from partition i. So ? ii ? 0. Similarly, ? ij for i = j is minus the sum of the weights of the edges between partitions i and j, divided by the square roots of the total weights of the partitions. This description of ? cut shows that the function is invariant under renumberings of the nodes. Therefore it is no restriction to define the function in terms of the blocking of the adjacency matrix for a particular numbering. Further define</p><formula xml:id="formula_29">? cut G = min ? cut ,<label>(12)</label></formula><p>where the minimum is taken over all k-partitionings of the graph. Obviously it is NP-hard to compute ? cut G . It is easy to see that G is k-partitionable if and only if ? cut G = 0. Clearly we have the Cheeger inequality,</p><formula xml:id="formula_30">L k = k ?=2 (1 -? ? ) 2 1/2 ? ? cut G . (<label>13</label></formula><formula xml:id="formula_31">)</formula><p>We can interpret the inequality as follows:</p><p>If there exists a partitioning for which ? cut is small, then the k largest eigenvalues of the normalized adjacency matrix A are close to 1.</p><p>In the graph partitioning literature a function of the type of ? cut is often referred to as a cut, see e.g. <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b10">11]</ref>, or expansion constant <ref type="bibr" target="#b15">[16]</ref>. Our cut function ? cut is very natural to the problem, as the Cheeger inequality <ref type="bibr" target="#b12">(13)</ref> follows automatically from the definition. Another related cut function will be given in Section 4.4.</p><p>We will now present a partitioning algorithm. The key observation is that any spectral partitioning algorithm is equivalent to computing an indicator matrix from the eigenvector matrix X. At first sight, the eigenvector matrix X is far from being in indicator form (e.g. the first eigenvector is positive). But the Cheeger inequality <ref type="bibr" target="#b12">(13)</ref> does not depend on which matrix is used to represent the subspace that is the solution of <ref type="bibr" target="#b4">(5)</ref>. Instead of X we may consider XQ for some orthogonal matrix Q. Thus we may ask: Is there a Q such that XQ is close to indicator form?</p><p>We know that if A is k-reducible, then the eigenvector matrix can be written in indicator form. If ? k -? k+1 &gt; 0, and this difference is not small, then a small perturbation of A gives a small perturbation of the subspace spanned by the eigenvectors <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">Chapter 8.1.3]</ref>. Therefore, if A is close to k-reducible, then there will exist a Q such that XQ is close to indicator form. So we are lead to the minimization problem, min</p><formula xml:id="formula_32">Q,? XQ -? = min Q,? X -?Q T , Q ? O(k), (<label>14</label></formula><formula xml:id="formula_33">)</formula><p>where ? is required to be in indicator form. If we can solve <ref type="bibr" target="#b13">(14)</ref>, then the partitioning can be read directly from the non-zeros of ?. In the following section we describe Algorithm 3, for solving <ref type="bibr" target="#b13">(14)</ref>. Using that, we state a simple k-partitioning algorithm in Algorithm 2.</p><p>Algorithm 2 k-partitioning algorithm: Given the normalized adjacency matrix A of a graph, compute a k-partitioning.</p><p>1. Compute the eigenvector matrix X ? R n?k corresponding to the k largest eigenvalues of A.</p><p>2. Solve ( <ref type="formula" target="#formula_32">14</ref>) using Algorithm 3.</p><p>3. Find the partitioning from ?, and compute the cut function ? cut .</p><p>4.2 A Semi-Sparse Orthogonal Approximation of X.</p><p>Consider the optimization problem <ref type="bibr" target="#b13">(14)</ref>. We want to approximate the matrix of eigenvectors by a product of matrices, where the first factor is in indicator form, and the second is an orthogonal matrix. This can be called a semi-sparse orthogonal approximation. To our knowledge there is no explicit solution of this problem. In <ref type="bibr" target="#b6">[7]</ref> we give an alternating algorithm. Assume that Q is given, put Y = XQ and consider the problem</p><formula xml:id="formula_34">min ? Y -? ,<label>(15)</label></formula><p>where ? is to be in indicator form. This constraint is equivalent to the requirement that exactly one element of each row of ? is nonzero. Therefore (15) consists of n independent minimization problems, one for each row, and the solution is given by</p><formula xml:id="formula_35">? ij = y i?i , j = ? i , 0, otherwise. , i = 1, 2, . . . , n,<label>(16)</label></formula><p>where</p><formula xml:id="formula_36">? i = arg max ? |y i? |, i = 1, 2, . . . , n.<label>(17)</label></formula><p>Then assume that ? is given. The minimization problem min</p><formula xml:id="formula_37">Q X -?Q T , Q ? O(k),<label>(18)</label></formula><p>is an orthogonal Procrustes problem [10, Section 6.4.1]. Let ? T X = U ?V T be the Singular Value Decomposition (SVD). The solution of ( <ref type="formula" target="#formula_37">18</ref>) is</p><formula xml:id="formula_38">Q = V U T .</formula><p>Starting with an initial approximation of ? the algorithm alternates between solving the problems for Q and ?, and stops when the gradient of the objective function is small enough in norm. As is common with alternating algorithms, the convergence rate is linear. It is of course crucial to have a good initial approximation; we will come back to this in the following subsection.</p><p>As the optimization problem is non-convex, we cannot guarantee that the computed solution is globally optimal. To check if we are close to a local minimum, we compute gradients. Let</p><formula xml:id="formula_39">r(?, Q) = 1 2 X -?Q T 2 ,</formula><p>Algorithm 3 SSO algorithm: Given X ? R n?k and an initial approximation ? in indicator form, compute an SSO approximation X ? ?Q T . repeat 1. Compute Q as the solution of (18).</p><p>2. Put Y = XQ and compute ? from ( <ref type="formula" target="#formula_34">15</ref>)- <ref type="bibr" target="#b16">(17)</ref>. until convergence and denote the gradients</p><formula xml:id="formula_40">? ? and ? Q (the Q-gradient must take into account that Q ? O(k)).</formula><p>Proposition 4.2. Assume that the indicator structure of ? is fixed, and let ? and Q be the output of Algorithm 3. Then</p><formula xml:id="formula_41">? ? = 0, ? Q = 1 2 ? T XQ -(XQ) T ? .</formula><p>The proof is given in Appendix B.</p><p>To our knowledge the idea to use an alternating algorithm for computing the partitioning was first used in [26, <ref type="bibr">Algorithm 1]</ref> (see also <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">Section 3.4]</ref>). However, there it was assumed that the indicator matrix had the form ? = P G, with a permutation matrix P , and G as in <ref type="bibr" target="#b8">(9)</ref>.</p><p>The approximation can also be written</p><formula xml:id="formula_42">X ? ?SQ T ,<label>(19)</label></formula><p>where ? ? O I (n, k), S is diagonal, S = diag(s 1 , s 2 , . . . , s k ). We use the convention that the s i are positive and ordered by magnitude. The following lemma will later be needed.</p><p>Lemma 4.3. The quantities s i satisfy</p><formula xml:id="formula_43">1 ? s 1 ? s 2 ? ? ? ? ? s k .</formula><p>Proof. The matrix Y in (15) has columns of length 1. The columns of ? are obtained by putting some of the elements of Y equal to zero. Therefore the length of column i of ?, which is the the quantity s i , is less than or equal to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">A Starting Approximation</head><p>In <ref type="bibr" target="#b2">[3]</ref> a direct multiway spectral partitioning method, called CPQR, is described. It computes a QR decomposition with column pivoting [10, Section 5.4.2] and the polar decomposition [10, Section 9.4.3]. The method is summarized in Algorithm 4.</p><p>The QR decomposition is only used to find the k rows of X that "dominate", in the sense that they are close to being the best basis vectors in R k among all Algorithm 4 CPQR algorithm <ref type="bibr" target="#b2">[3]</ref>: Given the matrix of eigenvectors X ? R n?k , compute a semi-sparse approximation X ? ? 0 Q T 0 .</p><p>1. Compute the QR decomposition with column pivoting</p><formula xml:id="formula_44">X T ? 0 = U R,</formula><p>where ? is a permutation matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Partition</head><formula xml:id="formula_45">X T ? 0 = (Z 1 Z 2 ), Z 1 ? R k?k ,</formula><p>and compute the SVD, Z 1 = U Z ? Z V T Z , giving the polar decomposition</p><formula xml:id="formula_46">Z 1 = Q 0 H, Q 0 = U Z V T Z , H = V Z ? Z V T Z ,</formula><p>where Q 0 is orthogonal and H is symmetric.</p><p>3. Compute XQ 0 and determine a matrix ? 0 in indicator form as in ( <ref type="formula" target="#formula_36">17</ref>)-( <ref type="formula" target="#formula_35">16</ref>).</p><p>row vectors (cf. the discussion in [10, Section 5.4]). Therefore, these vectors should be good for determining an orthogonal matrix Q 0 such that XQ 0 is close to indicator form, and the corresponding ? 0 should be a good starting point for a partitioning algorithm. This is confirmed in our numerical experiments.</p><p>It is suggested in <ref type="bibr" target="#b2">[3]</ref> to use CPQR as a starting point for spectral partitioning by Kmeans clustering. We use Q 0 as starting approximation for SSO.</p><p>In <ref type="bibr" target="#b2">[3]</ref> it is proved that if X is close to an indicator matrix W (in the restricted sense of this term, i.e., the nonzero elements of each column are equal), then there is a permutation matrix ? such that XQ 0 is close to W ?. Therefore it is not surprising that it also gives a good approximation,</p><formula xml:id="formula_47">X ? ? 0 Q T 0 = ? 0 S 0 Q T 0 ,<label>(20)</label></formula><p>where ? 0 and ? 0 are indicator matrices in the sense of this paper, ? 0 ? O I (n, k), and S 0 ? R k?k is diagonal (as in <ref type="bibr" target="#b18">(19)</ref>).</p><p>If we apply the CPQR algorithm to a matrix of eigenvectors, which satisfies (20) with equality, then, naturally, it recovers the exact solution. This follows from the results in <ref type="bibr" target="#b2">[3]</ref>; it is also straightforward to give a a constructive proof. Therefore, for a general disconnected graph, CPQR can be used to compute the indicator matrix for partitioning the graph into its components<ref type="foot" target="#foot_2">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Another Cut Function</head><p>As we noted in the introduction, cut functions are often used in the literature, first as a motivation for the spectral partitioning approach, and then as a mea-sure to evaluate the quality of a computed k-partitioning. In this paper we emphasize the second use of the function.</p><p>Let ? denote a function, defined for any given k-partitioning of a graph G.</p><formula xml:id="formula_48">Let ? G = min ?,</formula><p>where the minimum is taken over all k-partitionings.</p><p>We will call ? a cut function if 1. ? ? 0, and it is invariant under renumberings of the nodes of the graph,</p><formula xml:id="formula_49">2. ? G = 0, if and only if G is k-partitionable.</formula><p>It is easy to see that ? cut defined in Section 4.1 is a cut function.</p><p>Due to the permutation invariance we can write the cut function in terms of a normalized adjacency matrix, in blocked form,</p><formula xml:id="formula_50">A = D -1/2 BD -1/2 = ? ? ? ? ? A 1 F 12 ? ? ? F 1k F T 12 A 2 ? ? ? F 2k . . . . . . . . . . . . F T 1k F T 2k ? ? ? A k ? ? ? ? ? ,<label>(21)</label></formula><p>where the normalization matrix is</p><formula xml:id="formula_51">D = ? ? ? ? ? D 1 0 ? ? ? 0 0 D 2 ? ? ? 0 . . . . . . . . . . . . 0 0 ? ? ? D k ? ? ? ? ? .</formula><p>For the CPQR and SSO methods we can define a cut function based on the approximation X ? ?SQ T , where ? ? O I (n, k). If A is not far from being k-reducible we can expect that only a small fraction of the nonzero elements of ? are negative (cf. our numerical experiments in Section 5). In any case, define ? + = |?|, where |?| denotes element-wise absolute value. Clearly,</p><formula xml:id="formula_52">? + ? O + I (n, k). Define ? cut = I -? T + A? + .<label>(22)</label></formula><p>Clearly, ? cut is invariant under symmetric permutations. The minimal ? cut is defined ? cut G = min</p><formula xml:id="formula_53">??O + I (n,k) I -? T A? . (<label>23</label></formula><formula xml:id="formula_54">)</formula><p>Proposition 4.4. ? cut is a cut function, and the Cheeger inequality,</p><formula xml:id="formula_55">L k = min Y T Y =I I -Y T AY ? ? cut G ,<label>(24)</label></formula><p>is satisfied.</p><p>Proof. We will show that ? cut G = 0 if and only if G is k-partitionable. Let</p><formula xml:id="formula_56">? + = ? ? ? ? ? ? 1 0 ? ? ? 0 0 ? 2 ? ? ? 0 . . . . . . . . . 0 0 ? ? ? ? k ? ? ? ? ? ? O + I (n, k),</formula><p>where the partitioning corresponds to that in A <ref type="bibr" target="#b20">(21)</ref>. Put</p><formula xml:id="formula_57">? ? = I -? T + A? + .</formula><p>The off-diagonal elements of ? ? are</p><formula xml:id="formula_58">-? T i F ij ? j ,</formula><p>which is a weighted sum of the elements in F ij , so it is zero if and only if</p><formula xml:id="formula_59">F ij = 0. The i'th diagonal element of ? ? is 1 -(0 ? ? ? ? T i ? ? ? 0)A ? ? ? ? ? ? ? ? 0 . . . ? i . . . 0 ? ? ? ? ? ? ? ? = 1 -? T i A i ? i ? 0,</formula><p>where the inequality follows from the fact that the eigenvalues of A are smaller than or equal to 1.</p><p>If A is k-reducible then all F ij are zero, and the minimum in ( <ref type="formula" target="#formula_53">23</ref>) is equal to zero and is attained when the ? i are the eigenvectors of the A i . If A is k-irreducible, then at least one F ij is nonzero, and ? cut G &gt; 0. The Cheeger inequality follows immediately from the definition of ? cut G . An approximate inequality from the left can also be obtained. It is based on the approximation X ? ?SQ T (if XQ cannot be approximated reasonably well by a matrix in indicator form, then spectral partitioning is not well motivated). Theorem 4.5. Assume that ? k &gt; ? k+1 , and let X be a solution of (5), and let Z = ?SQ T be an SSO approximation with ? ? 0. Further, let s min = min i (s i ) be the smallest diagonal element of S. Assume that L k &lt; 1. Then</p><formula xml:id="formula_60">s 4 min 1 + 4 ? T AX ? cut 2 ? L k + O( ? 2 ), (<label>25</label></formula><formula xml:id="formula_61">)</formula><p>where ? = X -Z.</p><p>Proof. Putting L(X) = I -X T AX, we have</p><formula xml:id="formula_62">L(X) = I -(Z + ?) T A(Z + ?) = I -Z T AZ -? T AX -X T A? + O( ? 2 ),</formula><p>and</p><formula xml:id="formula_63">L(X), L(X) = L(Z), L(Z) -L(Z), ? T AX -L(Z), X T A? -? T AX, L(X) -X T A?, L(X) + O( ? 2 ) = L(Z), L(Z) -L(X), ? T AX -L(X), X T A? -? T AX, L(X) -X T A?, L(X) + O( ? 2 ).</formula><p>The Cauchy-Schwarz inequality gives</p><formula xml:id="formula_64">| ? T AX, L(X) | ? ? T AX L(X) , so L(X), L(X) ? L(Z), L(Z) -4 ? T AX L(X) + O( ? 2 ).</formula><p>Rearranging this we get</p><formula xml:id="formula_65">L(Z) 2 ? L(X) 2 + 4 ? T AX L(X) + O( ? 2 ) (26) ? L(X) (1 + 4 ? T AX ) + O( ? 2 ), (<label>27</label></formula><formula xml:id="formula_66">)</formula><p>where we have used L k = L(X) &lt; 1. We now bound L(Z) from below:</p><formula xml:id="formula_67">L(Z) = I -QS? T A?SQ T = I -S? T A?S =: ? S ,</formula><p>where the elements of ? S are given by</p><formula xml:id="formula_68">? ij = 1 -s 2 i ? T i A i ? i , i = j, -s i s j ? T i F ij ? j , i = j.</formula><p>Since from Lemma 4.3 s i ? 1, we have</p><formula xml:id="formula_69">1 -s 2 i ? T i A i ? i ? 1 -? T i A i ? i ? s 2 min (1 -? T i A i ? i ). and, s i s j ? T i F ij ? j ? s 2 min ? T i F ij ? j , which gives L(Z) ? s 2 min ? cut .</formula><p>Combining this and ( <ref type="formula">26</ref>)-( <ref type="formula" target="#formula_65">27</ref>) we have the inequality <ref type="bibr" target="#b24">(25)</ref>.</p><p>Note that since I = (XQ) T XQ ? S? T ?S = S 2 , s min is not much smaller than 1. Furthermore, if ? is small, then ?AX is small, since X has orthonormal columns, and AX = X?, where ? is the matrix of eigenvalues ? i , satisfying ? i ? 1. Thus, from the two Cheeger inequalities ( <ref type="formula" target="#formula_55">24</ref>) and ( <ref type="formula" target="#formula_60">25</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>), we have the following statement:</head><p>There is a partitioning of the graph with small ? cut if and only if L k and ? are small.</p><p>To compute ? cut for a given partitioning we require an approximation X ? ?SQ T . Thus we can use ? cut as a posteriori measure of the quality of the computed partitioning from CPQR and the SSO method. If ? cut is close to L k then it is unlikely that there exists another partitioning substantially closer to optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Numerical Experiments</head><p>The numerical experiments were performed on a standard desktop computer using Matlab R2019b. The three algorithms described in this paper were tested on a few examples. We compared 1) CPQR, 2) SSO, where ? is initialized as the solution of min ? X -? , 3) QR+SSO: SSO initialized with the CPQR solution, and 4) Kmeans with the standard Matlab initialization (initial cluster centers are chosen randomly).</p><p>As a measure of the quality of a partitioning, we will use ? cut defined in ( <ref type="formula" target="#formula_27">10</ref>)- <ref type="bibr" target="#b10">(11)</ref>, and ? cut . For comparison we also give the common measure NCut (normalized cut) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15]</ref> for a partitioning of the unnormalized adjacency matrix B. Let R i denote the indices belonging to partition i, and R C i the indices not in partition i. NCut is given by</p><formula xml:id="formula_70">NCut = k i=1 |E(R i , R C i )| ? i ,</formula><p>where</p><formula xml:id="formula_71">|E(R i , R C i )</formula><p>| is defined in Section 3. In the cases when there was an a priori known correct partitioning, we used the Rand Index <ref type="bibr" target="#b17">[18]</ref> to evaluate the quality of the computed partitioning. The index is described in <ref type="bibr" target="#b25">[26]</ref>: Given two partitionings U and V , "let a be the number of pairs of objects that are in the same set in U and in the same set in V , and b the number of pairs of objects that are in different sets in U and in different sets in V . The Rand index is given by RI = (a + b)/ n 2 . If RI = 1, the two partitions are identical". We used a Matlab implementation from GitHub<ref type="foot" target="#foot_3">4</ref> .</p><p>For CPQR, SSO and QR+SSO we give the norm of the residual XQ -? / X . In the SSO iterations we stopped the iterations when no nonzero positions in ? changed from one iteration to the next, and ? Q ? 10 -<ref type="foot" target="#foot_4">5</ref> . The execution times for all methods were less than 1 second for all examples.</p><p>Synthetic Data We generated random sparse adjacency matrices of dimension 11,000, with 6 clusters. A parameter determined the closeness to reducibility. In Table <ref type="table">1</ref> we give the results for an example, where the matrix was relatively close to reducibility, and the gap between ? 6 and ? 7 was quite large.</p><p>It is seen that for this well-conditioned example, all methods performed very well. In the second case, shown in Table <ref type="table" target="#tab_0">2</ref>, we let the matrix deviate further from reducibility, which is seen from the eigenvalues and cut values. Still the partitionings were very close to correct.</p><p>Astrophysics Collaboration Network This data set 5 is a collaboration network for the arXiv Astrophysics category. It has several components; the largest components consists of 17903 nodes. The density of the adjacency matrix is 0.012%. The results are given in Table <ref type="table">3</ref> of partitions k is difficult because the eigenvalues decay very slowly. We chose k = 6 as in <ref type="bibr" target="#b2">[3]</ref>. Sometimes Kmeans gave results significantly better (in terms of NCut and ? cut ) than the other three methods, sometimes significantly worse. The behavior of CPQR, SSO, and SSO+CPQR was very consistent. In order to compare the partitioning performance of the four methods, we computed pairwise Rand indices, presented in Table <ref type="table" target="#tab_1">4</ref>. We see that, in spite of the fact that ? cut for Kmeans was much higher than for the other methods, the partitioning in this particular run was almost the same. Power Grid Graph The power grid data<ref type="foot" target="#foot_5">6</ref> are described in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b18">19]</ref>. The graph contains information about the power grid of the Western States in the USA. The edges are power lines between different types of stations. The adjacency matrix has dimension 4941. In <ref type="bibr" target="#b18">[19]</ref> it is partitioned with k = 4, but it can be partitioned much further at low cost, which is seen in the very slow decay of the eigenvalues. We used k = 14. The results are given in Table <ref type="table" target="#tab_2">5</ref>. With such a small gap between ? 14 and ? 15 we cannot expect the subspace to be very well determined. Still CPQR, SSO, and QR+SSO consistently gave practically the same results for different runs with the same parameters. The results for Kmeans, on the other hand, always differed between different runs. The partitioning given by Kmeans deviated significantly from that given by the other three methods, see not perform as consistently as the other methods and Tables <ref type="table" target="#tab_5">7</ref> and<ref type="table" target="#tab_6">8</ref> indicate that the partitioning has significantly lower quality than that given by the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CPQR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CPQR</head><p>Twitch Social Network We downloaded a Twitch Social network from the SNAP collection<ref type="foot" target="#foot_7">8</ref> . The data have been collected and described in <ref type="bibr" target="#b19">[20]</ref>. The Portuguese language network (PTBR) is a connected undirected graph on 1912 nodes. We attempted to partition it for different values of k, but the results for all methods were inconsistent, probably due to the fact that there are many very small subgraphs that are loosely connected to the rest of the graph. Therefore we replaced the unnormalized adjacency method B by 0.999B + 0.001gg T , i.e. we made the graph complete with edges of small weight (we also removed selfloops). This technique is called regularization and is described e.g. in <ref type="bibr" target="#b24">[25]</ref>. Now the results with CPQR, SSO, and QR+SSO stabilized and were the same in different runs, as expected. The behavior of Kmeans, on the other hand, remained very inconsistent. The results for k = 4 are given Tables <ref type="table">9</ref><ref type="table" target="#tab_7">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mesh Graph</head><p>The problem of partitioning a graph arises in the context of computing a good ordering for the parallel factorization of sparse, symmetric matrices <ref type="bibr" target="#b16">[17]</ref>. For load balancing it is important to partition the graph in subgraphs of approximately equal size. We constructed a small square grid of SSO did not converge to the same point as QR+SSO. CPQR and QR+SSO gave the same partitionings, see Figure <ref type="figure" target="#fig_0">1</ref>. The behavior of Kmeans was inconsistent; often it gave the same partitioning as CPQR and QR+SSO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion of Examples</head><p>We emphasize that most of the examples are chosen so that L k is small or at least not large. This means that there is a strong motivation for using the spectral approach.</p><p>The first conclusion that can be drawn from the experiments is that Kmeans is outperformed by the other methods, because of its inconsistent behavior and the fact that it never gives solutions of better quality. SSO never performs better than QR+SSO. CPQR gives a very good starting approximation for QR+SSO. For most examples the difference between results of CPQR and QR+SSO is quite small. Since the execution times of both methods are very low, it is worth the small effort in performing the SSO iterations, to get a smaller residual, a smaller value of ? Q , and, in some cases, a partitioning of higher quality.</p><p>The algorithm QR+SSO is fast: for our largest example the execution time was considerably shorter than the time for solving the eigenvalue problem (using the Matlab function eigs). For our examples the number of iterations for QR+SSO is surprisingly small, in spite of the fact that alternating iterations have linear convergence rate. Therefore, for this type of problems, it does not seem worth the effort to develop a more advanced method for solving min X -?Q T .</p><p>The computed indicator matrix ? was nonnegative for almost all test examples: for the yeast data 8 out of 1458 non-zeros were small and negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>As we noted in the introduction the standard approach in spectral graph partitioning is to start with a particular cut function and then show that a relaxation leads to the solution of an eigenvalue problem for the graph Laplacian. In this paper we do the converse: we start with the eigenvalue/eigenvector problem, and show that if we restrict the admissible solution to a set of vectors in indicator form, then we get cut functions. From a theoretical point of view, the cut functions establish the relation between the eigenvalue and graph partitioning problems. From a practical point of view, the cut functions can be used to measure the quality of the computed partitioning.</p><p>We give a simple, efficient and robust algorithm for computing indicator vectors from eigenvectors. Our experiments show that for problems that are not far being k-partitionable the new algorithm is to be preferred over Kmeans. It is of interest to investigate the applicability of the algorithm to a wider range Our measure L k for k-partitioning does not have the same monotonicity property: if an edge is added, then usually L k becomes larger, but it can also become smaller, depending on the structure. To illustrate this, we computed the smallest eigenvalues of the normalized Laplacian of three graphs, constructed so that they can naturally be partitioned into three subgraphs, see Figure <ref type="figure">2</ref>. In A 1 we added an edge centrally in the subgraph, far from the partitioning positions, whereas in A 2 we added an edge close to one of the partitioning positions. In Table <ref type="table" target="#tab_8">12</ref> we give the smallest eigenvalues, cut functions, and L 3 for the three graphs. In A 1 we made the second subgraph slightly "heavier", which makes it "cheaper" to 3-partition the graph (cf. the cut functions). In A 2 another edge will have to be broken in the partitioning into subgraphs, and therefore the distance of the third graph to being 3-partitionable is larger.</p><formula xml:id="formula_72">A</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Gradients</head><p>Proof of Proposition 4.2 Assume that the indicator structure of ? is fixed, and write</p><formula xml:id="formula_73">r(?, Q) = 1 2 X -?Q T 2 = 1 2 Y -?, Y -? = 1 2 Y -?, R ,</formula><p>where Y = XQ, and R = Y -?. Put ?(t) = ? + t??, where ?? has the same indicator structure as ?. Then dr dt t=0 = -??, R = tr(R T ??) = 0, since, from ( <ref type="formula" target="#formula_35">16</ref>)-( <ref type="formula" target="#formula_36">17</ref>), the corresponding columns of ?? and R have complementary indicator structure.</p><p>To compute the gradient with respect to Q, we must take into account that Q is orthogonal. We parameterize Q(t) along a geodesic curve in O(k), starting from Q(0) in the direction T , where T is a skew-symmetric matrix. It is wellknown, see e.g. <ref type="bibr" target="#b4">[5]</ref>, that the geodesic in O(k) and its t-derivative are (irrelevant elements are denoted by * ). Straightforward computation gives,</p><formula xml:id="formula_74">Q(t) = Q(0) exp(tT ), Q(t) = Q(0)T exp(tT ).</formula><formula xml:id="formula_75">S T k C k -S k C k = S T k-1 C k-1 + s k cT k * * 0 - S k-1 C k-1 * * s T k c k . Thus tr S T k C k -S k C k = tr S T k-1 C k-1 -S k-1 C k-1 + (c T k -c T k )s k . (<label>28</label></formula><formula xml:id="formula_76">)</formula><p>By induction</p><formula xml:id="formula_77">tr S T k C k -S k C k = k i=2 (c T i -c T i )s i ,</formula><p>which shows that we can identify the operator ? Q with the matrix</p><formula xml:id="formula_78">C = (c T 2 -c T 2 ) (c T 3 -c T 3 ) ? ? ? (c T k -c T k ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It is easy to see that</head><formula xml:id="formula_79">? Q = C = 1 2 C -C T .</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Mesh graph. Left to right: regions computed by CPQR, QR+SSO, and Kmeans.</figDesc><graphic url="image-5.png" coords="21,265.08,165.82,72.39,72.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 Figure 2 :</head><label>22</label><figDesc>Figure 2: Adjacency matrices of three graphs with unweighted edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2 C 2 -S 2 C 2 c 11 c 12 c 21 c 22 = (c 21 -</head><label>22222221</label><figDesc>It is no restriction to assume thatQ(0) = I. With r(t) = 1 2 Y -?Q(t) T , R , we have r (0) = dr dt t=0 = ?T, R = tr T T ? T R . Let T = S -S T ,where S is strictly lower triangular, put C = ? T R and consider r (0) = tr(S T C-SC).We now use induction over the dimension k, and denote S k ? R k?k and C k ? R k?k . For k = 2 we have tr(S T c 12 )s 21 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Synthetic data II. First 7 eigenvalues: 1, 0.549, 0.494, 0.459, 0.437, 0.382, 0.313. L k = 1.20.</figDesc><table><row><cell>. The problem to choose the number</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 :</head><label>4</label><figDesc>Astrophysics graph. Pairwise Rand indices.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SSO</cell><cell cols="2">QR+SSO Kmeans</cell></row><row><cell></cell><cell cols="2">Residual</cell><cell>0.044</cell><cell>0.044</cell><cell>0.044</cell><cell></cell></row><row><cell></cell><cell>? Q</cell><cell></cell><cell cols="3">0.0086 0.3 ? 10 -10 0.3 ? 10 -9</cell><cell></cell></row><row><cell></cell><cell cols="2">Iterations</cell><cell></cell><cell>2/1</cell><cell>1/1</cell><cell></cell></row><row><cell></cell><cell>NCut</cell><cell></cell><cell>0.522</cell><cell>0.522</cell><cell>0.522</cell><cell>1.39</cell></row><row><cell></cell><cell>? cut</cell><cell></cell><cell>0.268</cell><cell>0.268</cell><cell>0.268</cell><cell>1.22</cell></row><row><cell></cell><cell>? cut</cell><cell></cell><cell>0.032</cell><cell>0.032</cell><cell>0.032</cell><cell></cell></row><row><cell>Table</cell><cell>3:</cell><cell cols="2">Astrophysics</cell><cell>graph.</cell><cell>First</cell><cell>7</cell><cell>eigenvalues:</cell></row><row><cell cols="6">1, 0.994, 0.990, 0.984, 0.983, 0.983, 0.980. L k = 0.031.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 :</head><label>5</label><figDesc>Power grid graph. First 15 eigenvalues decay very slowly: 1, 0.9997, . . . , 0.9959, 0.9954. L k = 0.0092.</figDesc><table><row><cell></cell><cell></cell><cell>SSO</cell><cell cols="2">QR+SSO Kmeans</cell></row><row><cell>Residual</cell><cell>0.301</cell><cell>0.290</cell><cell>0.290</cell><cell></cell></row><row><cell>? Q</cell><cell>0.253</cell><cell cols="2">0.2 ? 10 -5 0.5 ? 10 -5</cell><cell></cell></row><row><cell>Iterations</cell><cell></cell><cell>11/2</cell><cell>5/1</cell><cell></cell></row><row><cell>NCut</cell><cell>0.254</cell><cell>0.243</cell><cell>0.246</cell><cell>2.71</cell></row><row><cell>? cut</cell><cell>0.085</cell><cell>0.082</cell><cell>0.083</cell><cell>1.47</cell></row><row><cell>? cut</cell><cell>0.0296</cell><cell>0.0284</cell><cell>0.0287</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 ,</head><label>6</label><figDesc>indicating that it is of lower quality.</figDesc><table><row><cell></cell><cell cols="3">SSO QR+SSO Kmeans</cell></row><row><cell>CPQR</cell><cell>0.992</cell><cell>0.992</cell><cell>0.903</cell></row><row><cell>SSO</cell><cell></cell><cell>0.999</cell><cell>0.902</cell></row><row><cell>QR+SSO</cell><cell></cell><cell></cell><cell>0.901</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Power grid graph: Pairwise Rand indices.Yeast Protein Network We tested a data set describing protein interactions contained in yeast7 . The largest connected component had 1458 nodes. Again the eigenvalue decay was very slow. We chose k = 3 because there was a slightly larger gap between ? 3 and ? 4 . The results are given in Table7.</figDesc><table><row><cell></cell><cell>CPQR</cell><cell>SSO</cell><cell cols="2">QR+SSO Kmeans</cell></row><row><cell>Residual</cell><cell>0.202</cell><cell>0.188</cell><cell>0.188</cell><cell></cell></row><row><cell>? Q</cell><cell>0.117</cell><cell cols="2">0.9 ? 10 -6 0.07 ? 10 -5</cell><cell></cell></row><row><cell>Iterations</cell><cell></cell><cell>3/1</cell><cell>2/2</cell><cell></cell></row><row><cell>NCut</cell><cell>0.145</cell><cell>0.143</cell><cell>0.143</cell><cell>1.291</cell></row><row><cell>? cut</cell><cell>0.106</cell><cell>0.105</cell><cell>0.105</cell><cell>1.178</cell></row><row><cell>? cut</cell><cell>0.0201</cell><cell>0.0197</cell><cell>0.0197</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Yeast data. First 4 eigenvalues: 1, 0.9917, 0.9848, 0.9831. L k = 0.0174.Again the results of CPQR, SSO, and QR+SSO were stable for different runs, while those for Kmeans differed. We also ran the data with k = 12. There Kmeans failed. The Rand indices are given in Table8. Again Kmeans did</figDesc><table><row><cell></cell><cell cols="3">SSO QR+SSO Kmeans</cell></row><row><cell>CPQR</cell><cell>0.999</cell><cell>0.999</cell><cell>0.631</cell></row><row><cell>SSO</cell><cell></cell><cell>1</cell><cell>0.632</cell></row><row><cell>QR+SSO</cell><cell></cell><cell></cell><cell>0.632</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Yeast data: Pairwise rand indices.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 10 :</head><label>10</label><figDesc>Twitch data: Pairwise rand indices.</figDesc><table><row><cell></cell><cell></cell><cell>CPQR</cell><cell>SSO</cell><cell cols="2">QR+SSO Kmeans</cell></row><row><cell></cell><cell>Residual</cell><cell>0.268</cell><cell>0.254</cell><cell cols="2">0.254</cell></row><row><cell></cell><cell>? Q</cell><cell>0.148</cell><cell cols="3">0.1 ? 10 -5 0.4 ? 10 -5</cell></row><row><cell></cell><cell>Iterations</cell><cell></cell><cell>6/2</cell><cell>3/2</cell></row><row><cell></cell><cell>NCut</cell><cell>1.58</cell><cell>1.58</cell><cell>1.58</cell><cell>1.55</cell></row><row><cell></cell><cell>? cut</cell><cell>0.936</cell><cell>0.935</cell><cell cols="2">0.935</cell><cell>0.915</cell></row><row><cell></cell><cell>? cut</cell><cell>0.719</cell><cell>0.718</cell><cell cols="2">0.718</cell></row><row><cell cols="6">Table 9: Twitch data. First 5 eigenvalues: 1, 0.750, 0.593, 0.539, 0.472. L k =</cell></row><row><cell>0.664.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">SSO QR+SSO Kmeans</cell></row><row><cell></cell><cell cols="2">CPQR</cell><cell>0.931</cell><cell>0.931</cell><cell>0.757</cell></row><row><cell></cell><cell>SSO</cell><cell></cell><cell></cell><cell>1</cell><cell>0.707</cell></row><row><cell></cell><cell cols="2">QR+SSO</cell><cell></cell><cell></cell><cell>0.707</cell></row><row><cell></cell><cell></cell><cell>CPQR</cell><cell>SSO</cell><cell cols="2">QR+SSO Kmeans</cell></row><row><cell></cell><cell>Residual</cell><cell>0.364</cell><cell>0.752</cell><cell cols="2">0.361</cell></row><row><cell></cell><cell>? Q</cell><cell>0.106</cell><cell cols="3">0.02 ? 10 -5 0.3 ? 10 -5</cell></row><row><cell></cell><cell>Iterations</cell><cell></cell><cell>4/2</cell><cell cols="2">1/3</cell></row><row><cell></cell><cell>NCut</cell><cell>0.270</cell><cell>0.585</cell><cell cols="2">0.270</cell><cell>0.350</cell></row><row><cell></cell><cell>? cut</cell><cell>0.131</cell><cell>0.296</cell><cell cols="2">0.131</cell><cell>0.180</cell></row><row><cell></cell><cell>? cut</cell><cell>0.0539</cell><cell>0.262</cell><cell cols="2">0.0539</cell></row><row><cell>Table 11:</cell><cell cols="2">Mesh graph.</cell><cell cols="3">First 7 eigenvalues:</cell><cell>1, 0.998, 0.997, 0.995,</cell></row><row><cell cols="4">0.992, 0.989, 0.988. L k = 0.0155.</cell><cell></cell></row></table><note><p><p><p><p><p>size 32 ? 32, and partitioned the graph into 6 subgraphs. In order to make the first 7 eigenvalues distinct, we let unnormalized adjacency matrix be</p>B = B 0 ? I + 0.7(I ? B 0 ),</p>where B 0 is tridiagonal with 0 on the diagonal and 1 on the sub-and superdiagonals. The results are presented in Table</p>11</p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 12 :</head><label>12</label><figDesc>Three graphs. Four smallest eigenvalues 1 -? i of the normalized Laplacian, cut functions, and L 3 .</figDesc><table><row><cell></cell><cell></cell><cell>A 1</cell><cell>A 2</cell></row><row><cell cols="2">1 -? 4 0.802</cell><cell>0.899</cell><cell>0.798</cell></row><row><cell cols="2">1 -? 3 0.111</cell><cell>0.106</cell><cell>0.152</cell></row><row><cell cols="4">1 -? 2 0.0335 0.0367 0.0414</cell></row><row><cell>1 -? 1</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>? cut</cell><cell>0.129</cell><cell>0.123</cell><cell>0.192</cell></row><row><cell>? cut</cell><cell>0.133</cell><cell>0.126</cell><cell>0.189</cell></row><row><cell>L 3</cell><cell>0.116</cell><cell>0.112</cell><cell>0.157</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Our definition is less restricted than that in e.g.<ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b2">3]</ref>, where the nonzero elements of the columns are assumed to be equal.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>For large and sparse adjacency matrices it usually easier to compute the largest eigenvalues of the adjacency matrix than the smallest eigenvalues of the Laplacian.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The problem can alternatively be solved by an algorithm of Tarjan<ref type="bibr" target="#b21">[22]</ref>. A Matlab implementation by D. Gleich is available at https://se.mathworks.com/matlabcentral/ fileexchange/24134-gaimc-graph-algorithms-in-matlab-code.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Chris McComb (2022). Adjusted Rand Index (https://github.com/cmccomb/rand_ index), GitHub. Retrieved October 26, 2022.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Downloaded from http://snap.stanford.edu/data/ca-AstroPh.html in October 2022.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>Downloaded from http://konect.cc/networks/opsahl-powergrid/ in October 2022.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Downloaded from http://konect.cc/networks/moreno_propro/ in October 2022.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>http://snap.stanford.edu/data/twitch-social-networks.html, downloaded in December 2022.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>A Closeness of a Graph to being k-Partitionable Fiedler <ref type="bibr" target="#b7">[8]</ref> defined the second smallest eigenvalue of the unnormalized Laplacian to be the algebraic connectivity of the graph. This measure has the property that if an edge is added, then the connectivity cannot become smaller.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Measuring the stability of spectral clustering</title>
		<author>
			<persName><forename type="first">E</forename><surname>Andreotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Edelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Guglielmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lubich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">610</biblScope>
			<biblScope unit="page" from="673" to="697" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
		<title level="m">Spectral Graph Theory. CBMS Regional Conference Series in Mathematics Number 92</title>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simple, direct and efficient multi-way spectral clustering. Information and Inference: A</title>
		<author>
			<persName><forename type="first">A</forename><surname>Damle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Minden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the IMA</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="181" to="203" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Co-Clustering Documents and Words Using Bipartite Spectral Graph Partitioning</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th ACM-SIGKDD Conference</title>
		<meeting>7th ACM-SIGKDD Conference</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="269" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Geometry of Algorithms with Orthogonality Constraints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="303" to="353" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Eld?n</surname></persName>
		</author>
		<title level="m">Matrix Methods in Data Mining and Pattern Recognition, Second Edition. SIAM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">L</forename><surname>Eld?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Trendafilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pca</forename><surname>Semisparse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Algebraic Connectivity of Graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fiedler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Czech. Mat. J</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="298" to="305" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Community detection in networks: A user guide</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Reports</title>
		<imprint>
			<biblScope unit="volume">659</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<pubPlace>Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multiway spectral partitioning and higher-order Cheeger inequalities</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Gharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Trevisan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014-12">dec 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Many sparse cuts via higher eigenvalues</title>
		<author>
			<persName><forename type="first">A</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tetali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Vempala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC &apos;12: Proceedings of the Forty-Fourth Annual ACM Symposium on Theory of Computing</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multiway cuts and spectral clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<idno>442</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Washington, Seattle</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Statistics, Univ</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Washington</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Matrix Analysis and Applied Linear Algebra. SIAM, Philadelphia</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiway pspectral graph cuts on Grassmann manifolds</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pasadakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Alappat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Schenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wellein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="791" to="829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Partitioning well-clustered graphs: Spectral clustering works</title>
		<author>
			<persName><forename type="first">R</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zanetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR: Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Partitioning Sparse Matrices with Eigenvectors of Graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pothen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-P</forename><surname>Liou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="430" to="452" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Objective criteria for the evaluation of clustering methods</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Rand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="846" to="850" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">First-principles multiway spectral partitioning of graphs</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Riolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E J</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-Scale attributed node embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Depth-First Search and Linear Graph Algorithms</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="146" to="160" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Collective dynamics of &apos;small-world&apos; networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="page" from="440" to="442" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Understanding regularized spectral clustering via graph conductance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multiway Spectral Clustering: A Margin-Based Perspective</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="383" to="403" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
