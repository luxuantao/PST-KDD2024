<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Galactica: A Large Language Model for Science</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-11-16">16 Nov 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ross</forename><surname>Taylor</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marcin</forename><surname>Kardas</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Scialom</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anthony</forename><surname>Hartshorn</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Elvis</forename><surname>Saravia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Poulton</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Viktor</forename><surname>Kerkez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Stojnic</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Meta</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Geeta</forename><surname>Chauhan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hamid</forename><surname>Shojanazeri</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
						</author>
						<title level="a" type="main">Galactica: A Large Language Model for Science</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-11-16">16 Nov 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2211.09085v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community 1 .</p><p>1 galactica.org</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The original promise of computing was to solve information overload in science. In his 1945 essay "As We May Think", Vannevar Bush observed how "publication has been extended far beyond our present ability to make real use of the record" <ref type="bibr" target="#b11">(Bush, 1945)</ref>. He proposed computers as a solution to manage the growing mountain of information. Licklider expanded on this with the vision of a symbiotic relationship between humans and machines. Computers would take care of routine tasks such as storage and retrieval, "preparing the way for insights and decisions in scientific thinking" <ref type="bibr" target="#b60">(Licklider, 1960)</ref>.</p><p>Computing has indeed revolutionized how research is conducted, but information overload remains an overwhelming problem <ref type="bibr" target="#b8">(Bornmann and Mutz, 2014)</ref>. In May 2022, an average of 516 papers per day were submitted to arXiv <ref type="bibr">(arXiv, 2022)</ref>. Beyond papers, scientific data is also growing much more quickly than our ability to process it <ref type="bibr" target="#b67">(Marx, 2013)</ref>. As of August 2022, the NCBI GenBank contained 1.49 ? 10 12 nucleotide bases <ref type="bibr" target="#b26">(GenBank, 2022)</ref>. Given the volume of information, it is impossible for a single person to read all the papers in a given field; and it is likewise challenging to organize data on the underlying scientific phenomena.</p><p>Search engines are the current interface for accessing scientific knowledge following the Licklider paradigm. But they do not organize knowledge directly, and instead point to secondary layers such as Wikipedia,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Large Language Models (LLMs) LLMs have achieved breakthrough performance on NLP tasks in recent years. Models are trained with self-supervision on large, general corpuses and they perform well on hundreds of tasks <ref type="bibr">(Brown et al., 2020;</ref><ref type="bibr" target="#b7">Rae et al., 2021;</ref><ref type="bibr" target="#b35">Hoffmann et al., 2022;</ref><ref type="bibr" target="#b5">Black et al., 2022;</ref><ref type="bibr" target="#b113">Zhang et al., 2022;</ref><ref type="bibr" target="#b13">Chowdhery et al., 2022)</ref>. This includes scientific knowledge tasks such as MMLU <ref type="bibr" target="#b31">(Hendrycks et al., 2020)</ref>. They have the capability to learn in-context through few-shot learning <ref type="bibr">(Brown et al., 2020)</ref>. The capability set increases with scale, and recent work has highlighted reasoning capabilities at larger scales with a suitable prompting strategy <ref type="bibr" target="#b105">(Wei et al., 2022;</ref><ref type="bibr" target="#b13">Chowdhery et al., 2022;</ref><ref type="bibr" target="#b52">Kojima et al., 2022;</ref><ref type="bibr">Lewkowycz et al., 2022)</ref>.</p><p>One downside of self-supervision has been the move towards uncurated data. Models may mirror misinformation, stereotypes and bias in the corpus <ref type="bibr" target="#b90">(Sheng et al., 2019;</ref><ref type="bibr" target="#b55">Kurita et al., 2019;</ref><ref type="bibr" target="#b19">Dev et al., 2019;</ref><ref type="bibr" target="#b6">Blodgett et al., 2020;</ref><ref type="bibr" target="#b91">Sheng et al., 2021)</ref>. This is undesirable for scientific tasks which value truth. Uncurated data also means more tokens with limited transfer value for the target use-case; wasting compute budget. For example, the PaLM corpus is 50% social media conversations, which may have limited transfer towards scientific tasks <ref type="bibr" target="#b13">(Chowdhery et al., 2022)</ref>. The properties of scientific text also differ from general text -e.g. scientific terms and mathematics -meaning a general corpus and tokenizer may be inefficient. We explore whether a normative approach to dataset selection can work with the large model paradigm in this work.</p><p>Scientific Language Models Works such as SciBERT, BioLM and others have shown the benefit of a curated, scientific corpus <ref type="bibr" target="#b4">(Beltagy et al., 2019;</ref><ref type="bibr">Lewis et al., 2020a;</ref><ref type="bibr" target="#b28">Gu et al., 2020;</ref><ref type="bibr">Lo et al., 2019b;</ref><ref type="bibr" target="#b28">Gu et al., 2020;</ref><ref type="bibr" target="#b92">Shin et al., 2020;</ref><ref type="bibr" target="#b37">Hong et al., 2022)</ref>. The datasets and models were typically small in scale and scope, much less than corpora for general models <ref type="foot" target="#foot_0">2</ref> . Beyond scientific text, Transformers for protein sequences and SMILES have shown potential for learning natural representations <ref type="bibr" target="#b85">(Rives et al., 2021;</ref><ref type="bibr" target="#b36">Honda et al., 2019;</ref><ref type="bibr">Irwin et al., 2021;</ref><ref type="bibr" target="#b75">Nijkamp et al., 2022;</ref><ref type="bibr">Lin et al., 2022b)</ref>. However, sequences like SMILES have descriptive limitations for representing chemical structure. We explore in this work whether a large, multi-modal scientific corpus can aid representation learning, where sequences occur alongside footprints and text in a signal-dense context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scaling Laws</head><p>The idea of "scaling laws" was put forward by <ref type="bibr" target="#b46">Kaplan et al. (2020)</ref>, who demonstrated evidence that loss scales as a power-law with model size, dataset size, and the amount of training compute. The focus was on upstream perplexity, and work by <ref type="bibr">Tay et al. (2022a)</ref> showed that this does not always correlate with downstream performance. <ref type="bibr" target="#b35">Hoffmann et al. (2022)</ref> presented new analysis taking into account the optimal amount of data, and suggested that existing language models were undertrained: "Chinchilla scaling laws". This work did not take into the account of fresh versus repeated tokens. In this work, we show that we can improve upstream and downstream performance by training on repeated tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Models as Knowledge Bases</head><p>Storing information in weights is more unreliable in the sense models may blend information together, hallucination, but it is more "pliable" in the sense it can associate information through the representation space, association. Despite hallucination risks, there is evidence large language models can act as implicit knowledge bases with sufficient capacity <ref type="bibr" target="#b78">(Petroni et al., 2019)</ref>. They perform well on knowledge-intensive tasks such as general knowledge (TriviaQA) and specialist knowledge (MMLU) without an external retrieval mechanism <ref type="bibr">(Brown et al., 2020;</ref><ref type="bibr" target="#b31">Hendrycks et al., 2020)</ref>.</p><p>The question of how to update network knowledge remains an active research question <ref type="bibr" target="#b88">(Scialom et al., 2022;</ref><ref type="bibr" target="#b70">Mitchell et al., 2022)</ref>. Likewise, the question of how to improve the reliability of generation is an active question <ref type="bibr" target="#b23">(Gao et al., 2022)</ref>. Despite these limitations, today's large models will become cheaper with experience <ref type="bibr" target="#b34">(Hirschmann, 1964)</ref>, and so a growing proportion of scientific knowledge will enter weight memory as training and re-training costs fall. In this work we perform probes to investigate Galactica's depth of knowledge, and show that the ability to absorb scientific knowledge improves smoothly with scale.</p><p>Retrieval-Augmented Models Retrieval-augmented models aim to alleviate the shortcomings of weight memory. Examples of such models include RAG, RETRO and Atlas <ref type="bibr">(Lewis et al., 2020b;</ref><ref type="bibr" target="#b7">Borgeaud et al., 2021;</ref><ref type="bibr" target="#b40">Izacard et al., 2022)</ref>. These models have the advantage of requiring less capacity but the disadvantage of needing supporting retrieval infrastructure. Since knowledge is often fine-grained, e.g. the sequence of a particular protein, or the characteristics of a particular exoplanet, retrieval will likely be needed in future even for larger models. In this work we focus on how far we can go with model weights alone, but we note the strong case for using retrieval augmentation for future research on this topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modality</head><p>Entity Sequence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Abell 370</head><p>Abell 370 is a cluster...   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>"Nature is written in that great book which ever is before our eyes -I mean the universebut we cannot understand it if we do not first learn the language and grasp the symbols in which it is written."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Galileo Galilei, The Assayer</head><p>The idea that Nature can be understood in terms of an underlying language has a long history <ref type="bibr" target="#b22">(Galilei, 1623;</ref><ref type="bibr" target="#b109">Wigner, 1959;</ref><ref type="bibr" target="#b108">Wheeler, 1990)</ref>. In recent years, deep learning has been used to represent Nature, such as proteins and molecules <ref type="bibr" target="#b45">(Jumper et al., 2021;</ref><ref type="bibr" target="#b86">Ross et al., 2021)</ref>. Amino acids are an alphabet in which the language of protein structure is written, while atoms and bonds are the language of molecules. At a higher level, we organize knowledge through natural language, and many works have trained on scientific text <ref type="bibr" target="#b4">(Beltagy et al., 2019;</ref><ref type="bibr">Lewis et al., 2020a;</ref><ref type="bibr" target="#b28">Gu et al., 2020;</ref><ref type="bibr">Lo et al., 2019b)</ref>. With Galactica, we train a single neural network on a large scientific corpus to learn the different languages of science.</p><p>Our corpus consists of 106 billion tokens from papers, reference material, encyclopedias and other scientific sources. We combine natural language sources, such as papers and textbooks, and natural sequences, such as protein sequences and chemical formulae. We process L A T E X where we can capture it, and also include academic code to capture computational science. We highlight the corpus details in Table <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref>. Full details, including dataset components and filtering logic, are contained in the Appendix.  Notably the dataset is small and curated compared to other LLM corpuses, which are larger and uncurated. This is a key question of this work: can we make a working LLM based on a curated, normative paradigm? If true, we could make more purposefully-designed LLMs by having a clear understanding of what enters the corpus, similar to expert systems which had normative standards <ref type="bibr" target="#b41">(Jackson, 1990)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tokenization</head><p>Tokenization is an important part of dataset design given the different modalities present. For example, protein sequences are written in terms of amino acid residues, where character-based tokenization is appropriate. To achieve the goal of specialized tokenization, we utilize specialized tokens for different modalities:</p><p>1. Citations: we wrap citations with special reference tokens [START_REF] and [END_REF].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Step-by-Step Reasoning: we wrap step-by-step reasoning with a working memory token &lt;work&gt;, mimicking an internal working memory context. We cover a few of the specialized token approaches below that do not have clear parallels in the literature, in particular the working memory and citation tokens. In practice, they will use both symbiotically; meaning that working out that is written down in text is usually "missing" some steps performed internally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Mathematics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Working Memory Token, &lt;work&gt;</head><p>Transformer-based architectures lack an explicit working memory capability, which means a single-forward pass has limited efficacy. This is problematic for tasks that require multiple steps of computation. A current workaround is using a Transformer's output context as an external working memory to read from and write to. This is seen in recent work on chain-of-thought prompting <ref type="bibr" target="#b105">(Wei et al., 2022;</ref><ref type="bibr" target="#b96">Suzgun et al., 2022)</ref>. In one sense this is intuitive, as humans also augment their limited working memory with scratchpads. In another sense, we would like models to refine their representations internally like humans; e.g. mental arithmetic.</p><p>There are two limitations with chain-of-thought. First, it relies on prompt discovery to find a prompt that elicits robust step-by-step reasoning; i.e. minimizes mistakes from doing too much in a single forward pass. Not only does this require finding a robust prompt that works in all cases, but it also often relies on few-shot examples which take up context space. What is worse, much of the step-by-step reasoning on the internet misses intermediate steps that a human has performed using internal memory. Humans do not write down every step they perform because it would lead to long and tedious answers. They write down the principal steps of reasoning, and do lower-level steps via internal working memory. This means there is "missing data" in written text, i.e. between written steps there are internal memory steps that are not explicitly stated.</p><p>Secondly, chain-of-thought prompting uses the neural network to perform tasks that it is arguably not best suited to doing; for example, arithmetic. Prior work has shown that accuracy on tasks like multiplication is proportional to term frequency <ref type="bibr" target="#b84">(Razeghi et al., 2022)</ref>. Given that classical computers are specialized for tasks like arithmetic, one strategy is to offload these tasks from the neural network to external modules. For example, prior work has looked at the possibilities of external tool augmentation, such as calculators <ref type="bibr" target="#b101">(Thoppilan et al., 2022)</ref>. However, this requires a strategy to identify where the neural network should offload; and it may not be straightforward when combined with a discovered zero-shot prompt, especially where lower-level computation steps are not explicitly stated in writing.</p><p>Our solution is a working memory token we call &lt;work&gt;. We construct a few prompt datasets, see Table <ref type="table" target="#tab_4">3</ref>, that wrap step-by-by-step reasoning within &lt;work&gt; &lt;/work&gt;. Some of these datasets were generated programmatically (OneSmallStep), by creating a problem template and sampling the variables, others were sourced online (Workout, Khan Problems), and others used existing datasets and transformed them into a &lt;work&gt; based context (GSM8k train). Where a computation is performed that a human could not do internally, we offload by writing and executing a Python script. An example is shown in Figure <ref type="figure" target="#fig_4">3</ref>. Importantly, we do not have to turn this on, and the model can also predict the output from running a program. For our experiments, we did not find the need to turn Python offloading on, and leave this aspect to future work.</p><p>Longer term, an architecture change may be needed to support adaptive computation, so machines can have internal working memory on the lines of work such as adaptive computation time and PonderNet <ref type="bibr" target="#b27">(Graves, 2016;</ref><ref type="bibr" target="#b3">Banino et al., 2021)</ref>. In this paper, we explore the &lt;work&gt; external working memory approach as a   bridge to the next step. Notably our &lt;work&gt; prompt datasets are not very large or diverse, so there are likely large further gains to be made with this approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Citation Token</head><p>A distinctive properties of academic text is citations. In order to represent the implicit citation graph within the text, we process citations with global identifiers and special tokens [START_REF] and [END_REF] signifying when a citation is made. Figure <ref type="figure">4</ref> shows an example of citation processed text from a paper.  <ref type="bibr" target="#b115">(Vaswani et al., 2017)</ref>. For title-processed citations, the title can be associated with the previous context.</p><p>We considered two type of citation identifier: (a) paper titles and (b) alphanumeric IDs. Based on ablations, we found that title based identifiers have greater citation prediction accuracy than IDs. However, we also found that paper titles are more prone to hallucination error at lower scales given the text-based nature of the identifier. We consider title processing for this paper, but we note the trade-offs between both approaches.</p><p>Experiments for these ablations are contained in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prompt Pre-Training</head><p>We deviate from existing language model research in one important direction, which is our decision to include prompts in pre-training alongside the general corpora. This is motivated by a number of observations.</p><p>First, existing work has shown the importance of training token count on performance. The Chinchilla paper derived scaling "laws" taking into account number of tokens, training a 70bn model for 1.4 trillion tokens <ref type="bibr" target="#b35">(Hoffmann et al., 2022)</ref>. They obtained state-of-the-art performance on MMLU, beating much larger models such as Gopher <ref type="bibr" target="#b7">(Rae et al., 2021)</ref>.</p><p>Separately, research such as FLAN and T0 showed prompt tuning can boost downstream performance <ref type="bibr" target="#b104">(Wei et al., 2021;</ref><ref type="bibr" target="#b87">Sanh et al., 2021;</ref><ref type="bibr">Chung et al., 2022)</ref>. Their strategy involved converting tasks to text prompts, using prompt diversity in how the tasks are posed, and then fine-tuning on these prompt datasets. For FLAN and T0, this approach boosts performance, beating larger models such as GPT-3 on many tasks.</p><p>And additionally there is the UnifiedQA approach <ref type="bibr" target="#b48">(Khashabi et al., 2020)</ref>. In this approach, a T5 model is fine-tuned on question answering datasets, and is shown to boost performance on out-of-domain question answering datasets <ref type="bibr" target="#b81">(Raffel et al., 2020)</ref>. The model outperforms GPT-3 on MMLU, a model 16 times larger.</p><p>The first stream of research above focuses on total training tokens as a way to boost performance; i.e. it is token agnostic. The second stream of research focuses on task-context tokens as a way to boost performance; i.e. it is token selective. Since fine-tuned smaller models beat larger few-shot models on tasks like MMLU, this suggests world knowledge may be present in smaller models, but task-context knowledge may be poor given the relative number of task-context tokens seen in the general corpus.</p><p>For this paper, we opt to augment pre-training data with more task prompts to boost performance at lower scales. This is advantageous if it obviates the need for more data scale, e.g. a &gt;1 trillion corpus, or more model scale. The largest 120B model we train runs on a single NVIDIA A100 node. Additionally, given that fine-tuning requires expertise, making the model work out-the-box for popular tasks like question answering and summarization is more useful for users of the model. Lastly, by including prompts alongside general data, we maximize the generality of the model while boosting performance on some tasks of interest.</p><p>The closest analog to this approach for large language models is ExT5 <ref type="bibr" target="#b2">(Aribandi et al., 2021)</ref>. We take a similar approach by taking many machine learning training datasets, converting them to a text format, with prompt diversity, and then including them alongside general corpora in our pre-training set. A summary of prompt types is given in Table <ref type="table" target="#tab_6">4</ref>; the full details of datasets and prompts used are covered in the Appendix.  Because of prompt inclusion, it is important to distinguish between in-domain performance, where the training dataset is included in pre-training, and out-of-domain performance, where the training dataset is not included in pre-training. We mark these results clearly in the Results section of this paper. Importantly, we do not advocate for prompt pre-training as an alternative to instruction tuning. In fact, instruction tuning on Galactica is likely useful follow-up work given its potential to boost performance on several tasks of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architecture</head><p>Galactica uses a Transformer architecture in a decoder-only setup <ref type="bibr" target="#b115">(Vaswani et al., 2017)</ref>, with the following modifications:</p><p>? GeLU Activation -we use GeLU activations for all model sizes <ref type="bibr" target="#b30">(Hendrycks and Gimpel, 2016)</ref>.</p><p>? Context Window -we use a 2048 length context window for all model sizes.</p><p>? No Biases -following PaLM, we do not use biases in any of the dense kernels or layer norms <ref type="bibr" target="#b13">(Chowdhery et al., 2022)</ref>. ? Learned Positional Embeddings -we use learned positional embeddings for the model. We experimented with ALiBi at smaller scales but did not observe large gains, so we did not use it <ref type="bibr" target="#b79">(Press et al., 2021)</ref>. ? Vocabulary -we construct a vocabulary of 50k tokens using BPE <ref type="bibr" target="#b89">(Sennrich et al., 2015)</ref>. The vocabulary was generated from a randomly selected 2% subset of the training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Models</head><p>The different model sizes we trained, along with training hyperparameters are outlined in  We train using AdamW with ? 1 = 0.9, ? 2 = 0.95 and weight decay of 0.1 <ref type="bibr">(Loshchilov and Hutter, 2017)</ref>. We clip the global norm of the gradient at 1.0, and we use linear decay for learning rate down to 10% of it value. We use dropout and attention dropout of p = 0.1. We do not use embedding dropout. We found longer warmup was important for the largest model in the early stages of training to protect against the effects of bad initialization, which can have long-memory effects on the optimizer variance state and slow down learning. This may be specific to our model and training setup, and it is not clear whether this advice generalizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Libraries and Infrastructure</head><p>We use the metaseq library<ref type="foot" target="#foot_2">3</ref> for training the models, built by the NextSys team at Meta AI.</p><p>For training the largest 120B model, we use 128 NVIDIA A100 80GB nodes. For inference Galactica 120B requires a single A100 node. We choose the maximum model size to obey this constraint for downstream accessibility, and we will work to improve its accessibility for the research community in coming months. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Repeated Tokens Considered Not Harmful</head><p>We train the models for 450 billion tokens, or approximately 4.25 epochs. We find that performance continues to improve on validation set, in-domain and out-of-domain benchmarks with multiple repeats of the corpus.</p><p>First, from Figure <ref type="figure" target="#fig_6">6</ref>, validation loss continues to fall with four epochs of training. The largest 120B model only begins to overfit at the start of the fifth epoch. This is unexpected as existing research suggests repeated tokens can be harmful on performance <ref type="bibr" target="#b33">(Hernandez et al., 2022)</ref>. We also find the 30B and 120B exhibit a epoch-wise double descent effect of plateauing (or rising) validation loss followed by a decline. This effect becomes stronger with each epoch, and is most visible above with the 120B model towards end of training.</p><p>To investigate further, we examine the per-source breakdown of validation loss to see if there is heterogeneity in loss behaviour. We plot example curves in Figure <ref type="figure" target="#fig_4">23</ref> overleaf for the 30B model. We see no signs of loss heterogeneity: loss falls for all sources. The 120B exhibits the same relative trend of declining validation loss for all sources until the beginning of fifth epoch, where all sources spike (see Appendix).</p><p>The next question to answer is whether this trend extends to downstream performance and out-of-domain generalization. For this we use a 57 task subset of BIG-bench subset, a general corpus with principally nonscientific tasks and prompt types not included in pre-training <ref type="bibr" target="#b94">(Srivastava et al., 2022)</ref>. We plot results in Figure <ref type="figure" target="#fig_8">8</ref>. We see no signs of overfitting suggesting that use of repeated tokens is improving downstream performance as well as upstream performance.</p><p>We suspect that two factors could be at play, a quality factor, the curated nature of the corpus enables more value per token to be extracted, or a modality factor, the nature of scientific data enables more value per token to be extracted. The missing step of causation is what leads specifically from either factor towards less overfitting, and we leave this question to further work. We note the implication that the "tokens ? ?" focus of current LLM projects may be overemphasised versus the importance of filtering the corpus for quality.</p><p>In the following sections, we turn to evaluating Galactica's scientific capabilities. Specifically, we focus on the high-level design goals of building an LLM that can store, combine and reason about scientific knowledgeas these are needed for building a new interface for science.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Knowledge Probes</head><p>First, we examine how well Galactica absorbs scientific knowledge. We set up several knowledge probe benchmarks, building off the LAMA approach of <ref type="bibr" target="#b78">Petroni et al. (2019)</ref>. These were critical metrics during model development for identifying knowledge gaps within the corpus, and informing how to iterate the corpus. They also provide insight into the relative knowledge strengths of Galactica versus general language models, and we cover these results in this section before turning to the downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">LaTeX Equations</head><p>We construct a dataset of popular LaTeX equations from the fields of chemistry, physics, mathematics, statistics and economics. Memorisation of equations is useful to measure as it is necessary for many downstream tasks; for example, recalling an equation to use as part of an answer to a problem. Unless stated explicitly, Galactica results are reported as zero-shot. In total there are 434 equations we test for the knowledge probe.</p><p>We prompt with an equation name and generate LaTeX. An example is shown in Figure <ref type="figure">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt</head><p>The formula for Bessel's differential equation is:</p><formula xml:id="formula_0">Generated Answer x 2 d 2 y dx 2 + x dy dx + x 2 -? 2 y = 0</formula><p>Figure 9: LaTeX Equations Probe. We prompt for the name of an equation and evaluate whether the generated LaTeX is correct. We manually evaluate given the possibility of multiple correct answers.</p><p>We summarize the results in Table <ref type="table">6</ref>. Equation knowledge increases smoothly with scale. Galactica outperforms larger language models trained on general corpuses, indicating the value of a curated dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Domain Probes</head><p>We also set up domain probes to track specialized knowledge for certain fields. We detail these below:</p><p>? AminoProbe: a dataset of names, structures and properties of the 20 common amino acids.</p><p>? BioLAMA: a dataset of biomedical factual knowledge triples.</p><p>? Chemical Reactions: a dataset of chemical reactions.</p><p>? Galaxy Clusters: a dataset of galaxy clusters with their constellation classifications.</p><p>? Mineral Groups: a dataset of minerals and their mineral group classifications.</p><p>In each case, we construct a prompt to test the knowledge. For example, for Chemical Reactions, we ask Galactica to predict the products of the reaction in the chemical equation LaTeX. We mask out products in the description so the model is inferring based on the reactants only. An example is shown in Figure <ref type="figure" target="#fig_1">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt</head><p>Sulfuric acid reacts with sodium chloride, and gives _____ and _____:</p><p>\[ \ce{ NaCl + H2SO4 -&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated Answer</head><p>NaCl + H2SO4 --? NaHSO4 + HCl</p><p>Figure 10: Chemical Reactions. We prompt based on a description and reactants, and evaluate whether the generated products are correct.</p><p>We report results for these knowledge probes in We also observe steady scaling behaviour in these knowledge probes, with the exception of BioLAMA which we suspect reflects zero-shot prompt difficulty for all LLMs. Notably fine-grained factual knowledge, such as "ConstellationOf(GalaxyCluster)" type-queries seems to scale smoothly with the size of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Reasoning</head><p>We now turn to reasoning capabilities with the &lt;work&gt; token. We start by evaluating on the MMLU mathematics benchmarks, which we report in Table 8 <ref type="bibr" target="#b31">(Hendrycks et al., 2020)</ref>. Galactica performs strongly compared to larger base models, and use of the &lt;work&gt; token appears to boost performance over Chinchilla, even for the smaller 30B Galactica model. We also evaluate on the MATH dataset to further probe the reasoning capabilities of Galactica <ref type="bibr" target="#b32">(Hendrycks et al., 2021)</ref>. We compare the &lt;work&gt; token prompt directly with the Minerva 5-shot chain-of-thought prompt mCoT for comparability. We report results in Table <ref type="table" target="#tab_13">9</ref>. We see that Galactica outperforms the base PaLM model by a significant margin, with both chain-of-thought and &lt;work&gt; prompts. Galactica 30B outperforms PaLM 540B on both prompts: an 18 times smaller model. This suggests Galactica may be a better base model for fine-tuning towards mathematical tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mathematics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MATH Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>We report Minerva results for completeness, which is a 540B PaLM fine-tuned towards LaTeX specifically. Minerva outperforms base Galactica, but the performance differences are non-uniform; which points towards different mathematical data biases. For a direct comparison to Minerva, the model is freely available for those who want to finetune Galactica towards LaTeX specifically as follow-up work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Downstream Scientific NLP</head><p>We now evaluate on downstream scientific tasks to see how well Galactica can compose its knowledge in different task contexts. We focus on knowledge-intensive scientific tasks and report full results in Table <ref type="table" target="#tab_14">10</ref>. For this we use the MMLU benchmark as well as some other popular scientific QA benchmarks. We include the MMLU results earlier without &lt;work&gt; to test for knowledge association specifically. Full MMLU results, including social sciences and other fields, are reported in the Appendix. We also perform data leakage analysis on these benchmarks for more confidence; results are in the Appendix.</p><p>From Table <ref type="table" target="#tab_14">10</ref>, Galactica can compose its knowledge into the question-answering task, and performance is strong; significantly outperforming the other open language models, and outperforming a larger model (Gopher 280B) in the majority of tasks. Performance against Chinchilla is more variable, and Chinchilla appears to be stronger in a subset of tasks: in particular, high-school subjects and less-mathematical, more memorization intensive tasks. In contrast, Galactica tends to perform better in mathematical and graduatelevel tasks.</p><p>Our working hypothesis is that the Galactica corpus is biased towards graduate scientific knowledge, given it consists mostly of papers, which explains lagging performance in high-school subjects. While we do pick up some high-school level content through encyclopedias, textbooks and the filtered CommonCrawl, this amounts to a small quantity of tokens (a few billion). We leave the question of how to capture more of this base scientific knowledge in a curated way to future work.</p><p>On remaining tasks, we achieve state-of-the-art results over fine-tuned models at the time of writing. On PubMedQA, we achieve a score of 77.6% which outperforms the state-of-the-art of 72.2% <ref type="bibr" target="#b112">(Yasunaga et al., 2022)</ref>. On MedMCQA dev we achieve score of 52.9% versus the state-of-the-art of 41.0% <ref type="bibr" target="#b28">(Gu et al., 2020)</ref>.</p><p>For BioASQ and MedQA-USMLE, performance is close to the state-of-the-art performance of fine-tuned models (94.8% and 44.6%) <ref type="bibr" target="#b112">(Yasunaga et al., 2022)</ref>. For abstract algebra and medical genetics, we obtained best results with 30B, so we report these scores; the 120B scores for these were 27.0% and 68.0% respectively. Rest of results are for 120B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Citation Prediction</head><p>In this section we evaluate Galactica's capability to predict citations given an input context, which is an important test of Galactica's capability to organize the scientific literature. We find that both accuracy and the quality of distributional approximation improves with scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Citation Accuracy</head><p>We construct three datasets to evaluate the model's capability to cite:</p><p>? PWC Citations: a dataset with 644 pairs of machine learning concepts and papers that introduced them. Concepts consist of methods (e.g. ResNet) and datasets (e.g. ImageNet) from Papers with Code 4 .</p><p>? Extended Citations: a dataset with 110 pairs of non-machine learning concepts and papers that introduced them. Examples of concepts include Kozac sequence and Breit-Wigner distribution.</p><p>? Contextual Citations: a dataset with 1,869 pairs of references and contexts from our arXiv validation set. The dataset is constructed by sampling 1,000 random references and collecting their contexts.</p><p>For the PWC Citations and Extended Citations datasets, the citation prediction task is framed as a text generation task. The model is given a prompt like "In this paper we use ResNet method [START_REF]" in order to generate a prediction for the ResNet concept. For Contextual Citations, we prompt after the input context for the citation, where the context ends with [START_REF].</p><p>We compare Galactica to sparse and dense retrieval-based approaches on this task.</p><p>For the sparse baseline, we use ElasticSearch to create an index of all the references, including their titles, abstracts, and short snippets of text with the contexts they appear in. Then, given a text query, we retrieve the top references ordered by the sum of matching scores across all selected fields.</p><p>For dense retriever baselines, we evaluate two different Contriever models <ref type="bibr" target="#b39">(Izacard et al., 2021)</ref>. The first is the pre-trained model released by <ref type="bibr" target="#b39">Izacard et al. (2021)</ref>. The second model we use is fine-tuned on a random subset of 10 million context/paper pairs from our corpus, trained to retrieve the right paper given a context before a citation. The setup for dense retrieval is: <ref type="bibr" target="#b115">(1)</ref> each reference is encoded by the model using its title and abstract, ( <ref type="formula">2</ref>) a text query is encoded by the same model, (3) the references that match the query re returned. Retrieval is performed using a FAISS index <ref type="bibr" target="#b43">(Johnson et al., 2019)</ref>.</p><p>The results can be seen in Table <ref type="table" target="#tab_15">11</ref>. The performance on all evaluation sets increases smoothly with scale. At larger scales, Galactica outperforms the retrieval-based approaches as its context-associative power improves. This is an important result as current approaches for navigating the literature use these existing retrieval approaches. As the power of language models improves, we suspect they will become a valuable new tool for exploring the literature. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Citation Distributional Analysis</head><p>We now turn to look at how well Galactica can model the empirical citation distribution. For this analysis we use the Contextual Citations dataset, where prompts are extracted from a paper by taking the context before a citation as the prompt. An example prompt with a model prediction is shown overleaf in Figure <ref type="figure" target="#fig_3">12</ref>.</p><p>We use the in-context citation data to analyse the distributional difference between predicted and ground truth paper counts. This allows us to assess the model bias towards predicting more popular papers. Specifically, for each context there is a ground truth and predicted reference. We count the number of times each reference appears in our corpus. We then compare the distribution of reference counts between the ground truth references and the predicted references using the Kolmogorov-Smirnov distance <ref type="bibr" target="#b68">(Massey, 1951)</ref>.</p><p>The comparison between the citation count distributions for different model sizes can be seen in Figure <ref type="figure" target="#fig_9">11</ref>.</p><p>Figure <ref type="figure" target="#fig_9">11a</ref> shows the decrease in the Kolmogorov-Smirnov distance between the distribution of ground truth paper citations and the distribution of predicted papers citations. Figure <ref type="figure" target="#fig_9">11b</ref> shows how the distribution of paper counts for the predicted papers gets closer to the ground truth as the model size grows. At smaller scales the model is more prone to predicting more popular papers. As the model grows in size this bias towards predicting popular papers diminishes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">General Capabilities</head><p>We have studied Galactica's scientific capabilities. It is perhaps not surprising that a specialist scientific model outperforms general models on scientific tasks, but what would be more surprising was if it outperformed general models on general NLP tasks. In this section, we show surprising evidence that it does just that.</p><p>We evaluate on 57 BIG-bench tasks in Table 12 <ref type="bibr" target="#b94">(Srivastava et al., 2022)</ref>. The tasks are primarily non-scientific and test general language capability, for example anachronisms, figure of speech and metaphor boolean. We always evaluate with 5-shots, and we use the default prompt style from BIG-Bench. Importantly, we do not include this prompt style in pre-training; so the evaluation between Galactica and the other models is comparable 5-shot. Full details and results are in the Appendix. We summarize average scores in Both the 30B and 120B Galactica models outperform the larger OPT and BLOOM general models. This is a surprising result given we designed Galactica to trade-off generality for performance in scientific tasks.</p><p>We suspect this result reflects the higher-quality of the Galactica corpus, stemming from the fact it is curated and also primarily academic text. Previous open LLM efforts likely overfocused on scale goals and underfocused on data filtering. Another implication is that the focus on tokens ? ? from Chinchilla needs to be complemented with strong data quality procedures <ref type="bibr" target="#b35">(Hoffmann et al., 2022)</ref>. With this paper, we took an opposite approach by focusing on high-quality tokens and repeated epochs of training. However, the Chinchilla insight stands: and there is much more scientific text that we have not exploited in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Chemical Understanding</head><p>We now turn to Galactica's capability to interface with different scientific modalities. We start by looking at Galactica's chemical capabilities. Chemical properties exhibit complex correlations which means the chemical space is very large. Better organization of chemical information through language models could aid chemical design and discovery. We explore how Galactica can provide a new interface for these tasks in this section.</p><p>For this work, we only include a small subset of available compounds from PubChem Compound in pretraining. Specifically, we take a random subset (2 million) of total compounds (110 million). This is to ensure the model is not overly biased towards learning natural sequences over natural language. This is a constraint we can relax in future work, enabling for much larger corpus. Here we focus on the first step of investigating whether a single model can learn effectively in the multi-modal setting.</p><p>We find that a language model can learn chemical tasks such as IUPAC naming in a self-supervised way, and in addition, we can pose drug discovery tasks as natural language prompts and achieve reasonable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.1">IUPAC Name Prediction</head><p>SMILES is a line notation which represents chemical structure as a sequence of characters <ref type="bibr" target="#b106">(Weininger, 1988)</ref>.</p><p>In the Galactica corpus, the SMILES formula occurs alongside information in the document, such as IUPAC names, molecular weight and XLogP. In the context of self-supervised learning, this means a language model is performing implicit multi-task learning: the model is predicting the next SMILES token, but can also use SMILES to predict other entities in the document.</p><p>As an initial test, we set up a IUPAC Name Prediction task, where the task is to name a compound according to the IUPAC nomenclature given a SMILES formula input. The IUPAC nomenclature is a method of naming organic compounds that has a ruleset based on naming the longest chain of carbons connected by single bonds <ref type="bibr">(Favre and Powerll)</ref>. There is a large set of rules and the procedure is algorithmically complex, meaning it is hard to automate. As a result, it is missing from standard cheminformatics toolkits.</p><p>Previous works such as STOUT and Struct2IUPAC have explored the possiblity of using RNNs and Transformers for this task <ref type="bibr" target="#b82">(Rajan et al., 2021;</ref><ref type="bibr" target="#b54">Krasnov et al., 2021)</ref>. We explore in this section whether Galactica can translate a SMILES specification to its IUPAC name in the self-supervised setting. We design a prompt based on the PubChem structure, with the SMILES as the only input, and the output to predict the IUPAC name.</p><p>To evaluate, we use our compound validation set of 17,052 compounds, and prompt with the SMILES formula and predict the IUPAC name. To calculate accuracy, we use OPSIN to convert the generated IUPAC name to SMILES, canonicalize it and compare with the canonicalized SMILES target <ref type="bibr" target="#b66">(Lowe et al., 2011)</ref>.</p><p>Results are shown in The more immediate question is what is actually being learnt: is Galactica inferring names from the fundamental molecular structure? To answer this, we visualize the average atomic attention at each stage of a prediction in Figure <ref type="figure" target="#fig_10">13</ref> overleaf. Encouragingly, the results are interpretable in terms of the underlying chemistry, and Galactica attends to the correct group when predicting a name, e.g. for "amino" it attends primarily to the -NH 2 substituent. for "amino" it attends to the nitrogen atom; for thiazole, the sulphur atom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.2">MoleculeNet</head><p>We now explore whether we can pose traditional drug discovery tasks in a natural language format, combining the different modalities involved. Humans organize knowledge via natural language, and so learning an interface between natural language and scientific modalities like SMILES could be a new tool for navigating the chemical space. We use MoleculeNet classification benchmarks to answer this question, which are summarized in Table 14 <ref type="bibr" target="#b110">(Wu et al., 2017)</ref>.  <ref type="bibr" target="#b83">(Ramsundar et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head><p>To evaluate, we include the training sets in pre-training by converting to a text format. We use prompt randomization (varying how the question is posed). For example, for BBBP the training prompt has forms like in Figure <ref type="figure" target="#fig_1">14</ref> below. These examples occur alongside the other corpuses in training, and each example is seen just over 4 times. This is not comparable to direct fine-tuning or supervision due to the presence of other data in pre-training, so it might be considered a form of weak supervision instead.</p><p>Here is a SMILES formula: For some MoleculeNet datasets, other modalities are implicitly present. For example, in the Tox21 dataset, bioassays concern particular receptors such as the androgen receptor (AR). As an experiment, we decided to frame the task in a text format with the protein sequence and the SMILES as part of the prompt. We show an example for Tox21 in Figure <ref type="figure" target="#fig_5">15</ref>.</p><p>Here is a sequence for a protein:</p><p>[START_AMINO]MEEPQSDPSVEPPLSQETFSDLWKLLPE..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.[END_AMINO]</head><p>And here is an isomeric SMILES for a compound: We make sure to Kekulize the SMILES to be consistent with PubChem representations. For evaluation, we use the recommended splits from the DeepChem library <ref type="bibr" target="#b83">(Ramsundar et al., 2019)</ref>. We present results in Table <ref type="table" target="#tab_21">15</ref>. Performance scales with model size. The scaling is slower than tasks like QA, and the base model lags a specialist model with explicit 3D information and 10 times more molecules <ref type="bibr" target="#b114">(Zhou et al., 2022)</ref>. We suspect the weak supervision setup is harder for this task, and fine-tuning and/or more molecule data is required to get sufficient task signal. For our purposes, the implication for future work is that we can learn drug discovery tasks via natural language prompts. If we can learn these relationships automatically in a signal-dense document context (e.g. online chemical databases), this might reduce the reliance on supervised datasets to perform these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Positive Examples</head><p>As a final check, we can average Galactica's attention heads across layers, and visualize whereabouts the model looks in the SMILES sequence to make a prediction (atomic attention). We show an example in Figure <ref type="figure" target="#fig_6">16</ref> for some Tox21 predictions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Biological Understanding</head><p>In this section we examine Galactica's capability to interface with biological modalities. Language models could potentially play a role in automatic organisation of this data, for example annotating newly sequenced proteins with functional information. We explore the potential of this interface in this section.</p><p>For protein sequences from UniProt, we include a small subset of available sequences in pre-training. Specifically, we take reviewed Swiss-Prot proteins; a high-quality subset (0.5 million) of total (227 million). This is to ensure the model is not overly biased towards learning natural sequences over natural language. As with molecule data, this is a constraint we can relax in future work, enabling for much larger corpus. Here we focus on the first step of investigating whether a single model can learn effectively in the multi-modal setting.</p><p>We find that a language model can learn an implicit measure of sequence similarity that it can use for tasks such as functional annotation and descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.1">Sequence Validation Perplexity</head><p>While Galactica does not explicitly model the 3D structure of a protein, the information needed for a specific conformation is contained in the linear amino acid sequence, which in turn determine function. As a first step, we test upstream performance through evaluating protein sequence perplexity. Constructing a good validation set is important and data leakage is a problem for works in this field. We construct four holdout sets to obtain more confidence about what is being learnt and what generalizes.</p><p>First, we conduct BLAST on the sequences in the training set and remove all sequences with a sequence identity ? 50% with 51 CASP14 target sequences. These are the same test sequences used in ESMFold <ref type="bibr">(Lin et al., 2022b)</ref>. In total we remove 167 sequences from the training set using this approach. We call this this holdout set CASPSimilarSeq. We call the 51 CASP14 target sequences CASPSeq.</p><p>Secondly, we conduct organism-level holdout, and remove all sequences from the Paenungulata clade of organisms, including elephants, elephant shrews, manatees and aadvarks. This allows us to test whether Galactica can annotate sequeces for organisms it has never seen before. In total we remove 109 sequences from the training set using this approach. We call this holdout set PaenSeq. Note that this does not enforce any sequence similarity constraints, and there may be very similar sequences in the training set.</p><p>Lastly, we conduct a randomized test split, consisting of 5456 sequences. There is no sequence identity constraint applied, so memorization may be more at play, but it still provides a signal about the breadth of sequence knowledge absorbed by the model. We call this holdout set UniProtSeq.</p><p>We evaluate perplexity for all holdout sets in Table <ref type="table" target="#tab_22">16</ref> and plot in Figure <ref type="figure" target="#fig_14">17</ref>. For three of the validation sets we observe smooth scaling, reflecting the potential for high sequence similarity with sequences in the training set; for example, orthologs in the case of the Paen validation set. Interestingly, the CASP set with sequence similarity constraints levels off, suggesting the gains from the 550k proteins in training quickly saturates. To investigate further, we example validation perplexity on the CASPSeq set during training of the 120B model, and we plot results in Figure <ref type="figure" target="#fig_15">18</ref> below. We observe falling validation perplexity up until the start of the fourth epoch, at which point the model overfits for this particular dataset. This may suggest Galactica is getting worse at more "out-of-domain" proteins that differ significantly from the test set. For future work, less repetition is probably desirable; and more generally, increasing the diversity of proteins in the training dataset is likely to be beneficial. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.2">Functional Keyword Prediction</head><p>We now look at specific translation capabilities from protein sequence toward natural language, which may be useful for tasks such as protein annotation. As a first test, we look at UniProt keywords that Galactica can infer from the sequence. An example of these is shown in Figure <ref type="figure" target="#fig_3">20</ref> overleaf.</p><p>We report results in Table <ref type="table" target="#tab_23">17</ref>. F 1 score increases across the holdout sets with scale, suggesting that Galactica can learn keywords by inferring from the sequence. However, we see saturation for the CASPSimSeq, suggesting this capability depends on how similar the sequences are to those in the training set. This is reflected in the example in Figure <ref type="figure" target="#fig_3">20</ref>, where Galactica uses its knowledge of a similar proteins from different organisms, with a maximum sequence similarity of 91 We attempted to visualize attention in the protein sequence, but we did not observe anything with biological intepretation (e.g. attention to domains). Our working hypothesis is that Galactica has learnt an implicit measure of sequence similarity that it uses to associate predicted keywords, but that this is not directly interpretable from where it attends to. This differs from our chemistry analysis where results were interpretable in terms of attention to the underlying atomic structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>## Sequence</head><p>Here is the sequence: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.3">Protein Function Description</head><p>As the next test, we look at generating free-form descriptions of protein function from the sequence. We look at the UniProt function descriptions and compare to Galactica generated descriptions.</p><p>We report results in Table <ref type="table" target="#tab_24">18</ref>. ROUGE-L score increases smoothly across all the holdout sets. We show an example overleaf in Figure <ref type="figure" target="#fig_18">21</ref>  As with the keyword prediction task, Galactica appears to be learning based on matching sequences with similar ones it has seen in training, and using this to form a description. This suggests language models for protein sequences could serve as useful alternatives to existing search methods such as <ref type="bibr">BLAST and MMseqs2 (Altschul et al., 1990;</ref><ref type="bibr" target="#b95">Steinegger and S?ding, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Toxicity and Bias</head><p>In this section we study the toxicity and bias of the Galactica model. We evaluate on benchmarks related to stereotypes, toxicity, and misinformation. We compare results to other language models. We find Galactica is significantly less biased and toxic than existing language models. This is the sequence:</p><p>[START_AMINO]MTNIRKNHPLLKTINDAFIDLPTPSNISTWWNFGSLLGACLIIQVLTGLFLAMHYTSDT..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.[END_AMINO]</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>### Ground-Truth Description</head><p>Component of the ubiquinol-cytochrome c reductase complex (complex III or cytochrome b-c1 complex) that is part of the mitochondrial respiratory chain. The b-c1 complex mediates electron transfer from ubiquinol to cytochrome c. Contributes to the generation of a proton gradient across the mitochondrial membrane that is then used for ATP synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>### Galactica 120B Predicted Description</head><p>Component of the ubiquinol-cytochrome c reductase complex (complex III or cytochrome b-c1 complex) that is part of the mitochondrial respiratory chain. The b-c1 complex mediates electron transfer from ubiquinol to cytochrome c. Contributes to the generation of a proton gradient across the mitochondrial membrane that is then used for ATP synthesis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Bias and Stereotypes</head><p>For the following evaluations, we investigate Galactica's ability to detect (and generate) harmful stereotypes and hate speech, using four widely used benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">CrowS-Pairs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CrowS-Pairs</head><p>Bias CrowS-Pairs is a collection of 1,508 crowd-sourced pairs of sentences, one which is "more" stereotyping and one which is "less" stereotyping, and covers nine characteristics <ref type="bibr" target="#b72">(Nangia et al., 2020)</ref>. These characteristics are race, religion, socioeconomic status, age, disability, nationality, sexual orientation, physical appearance, and gender. A language model's preference for stereotypical content is measured by computing the proportion of examples in which the "more" stereotypical sentence is preferred (as determined by log likelihood). Higher scores indicate a more harmfully biased model, whereas an ideal model with no bias would score 50%.</p><p>We report results for Galactica and other language models in Table <ref type="table" target="#tab_25">19</ref>. Galactica exhibits significantly lower stereotypical biases in most categories, with the exception of sexual orientation and age, when compared to the latest GPT-3 (text-davinci-002) and OPT 175B. Galactica attains a better overall score of 60.5% compared to the other models. Language models such as OPT use the Pushshift.io Reddit corpus as a primary data source, which likely leads the model to learn more discriminatory associations <ref type="bibr" target="#b113">(Zhang et al., 2022)</ref>. Galactica is trained on a scientific corpus where the incidence rate for stereotypes and discriminatory text is likely to be lower. StereoSet aims to measure stereotypical biases across profession, religion, gender, and race <ref type="bibr" target="#b71">(Nadeem et al., 2021)</ref>. The benchmark contains two tasks: an intrasentence task and an intersentence task, with around 2,100 examples each in the development set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">StereoSet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StereoSet</head><p>? Intrasentence Task: the stereotype and associated context are in the same sentence.</p><p>? Intersentence Task: the context and stereotype are in different (consecutive) sentences.</p><p>Alongside stereo-and anti-stereotypical variants of sentences, each example in StereoSet contains an unrelated sentence. This sentence is included for measuring a Language Modelling Score (LMS) and a Stereotype Score (SS). These two metrics are combined to form the Idealized Context Association Test score (ICAT), which is a balanced measure of bias detection and language modeling. An ideal, unbiased language model would score an LMS of 100, an SS of 50, and an ICAT of 100.</p><p>We report results in Table <ref type="table" target="#tab_26">20</ref>. Galactica outperforms other models on all categories for the overall ICAT score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Toxicity</head><p>To measure toxicity we use the RealToxicityPrompts (RTP) benchmark introduced in Gehman et al. ( <ref type="formula">2020</ref>). We follow the same setup of <ref type="bibr" target="#b113">Zhang et al. (2022)</ref> and sample 25 generations of 20 tokens using nucleus sampling (p=0.9) for each of 5000 randomly sampled prompts from RTP. We use the prompts to produce sequences (i.e, continuations) which are then scored by a toxicity classifier provided by Perspective API<ref type="foot" target="#foot_4">5</ref> .</p><p>Figure <ref type="figure" target="#fig_19">22</ref> plots the results. The chart shows the mean toxicity probability of continuations (y-axis), stratified across bucketed toxicities of the original prompts (x-axis). Galactica exhibits substantially lower toxicity rates than the other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">TruthfulQA</head><p>TruthfulQA is a benchmark that measures answer truthfulness of language model generations <ref type="bibr">(Lin et al., 2022a)</ref>. It comprises 817 questions that span health, law, finance and other categories. We compare to other published language models. We report results in Table <ref type="table" target="#tab_27">21</ref>. Galactica exceeds the performance of other language models on this benchmark. However, absolute performance is still low. Given the curated nature of our corpus, this suggests that data alone does not cause language models to struggle at this task. 7 Limitations and Future Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Limitations</head><p>We cover some of the limitations with work in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus Limitations</head><p>Our corpus has several limitations, both external and internally imposed. The main external constraint is our restriction to use open-access resources, and much of scientific knowledge like papers and textbooks are not open access. With access to these closed sources of knowledge, performance is likely to be considerably higher. We also use self-imposed constraints, like restricting the number of molecules and proteins for this work; without these constraints, we are likely to see considerable performance gains due to much larger corpuses for these modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corpus Effects vs Prompt Effects</head><p>In several benchmarks, we show performance gains over existing language models, but we do not specifically disentangle the effects of the prompts we included in pre-training versus the core scientific corpus. In future work, we likely need to disentangle these effects in order to see whether general language capabilities are possible with a scientific corpus alone without prompt boosting.</p><p>Citation Bias While we demonstrate that the model approaches the true citation distribution with scale, some bias towards popular papers still remains with the 120B scale model, so the model likely requires augmentation before being used in a production environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompt Pre-Training vs Instruction Tuning</head><p>We opted for the former in this paper, but ideally we would need to explore what the latter could achieve, along the lines of the recent work of <ref type="bibr">Chung et al. (2022)</ref>. A limitation of this work is that we do not perform this direct comparison through ablations, making clear the trade-offs between approaches.</p><p>General Knowledge While Galactica absorbs broad societal knowledge through sources such as Wikipedia -e.g. 120B knows Kota Kinabalu is the capital of Malaysia's Sabah state -we would not advise using it for tasks that require this type of knowledge as this is not the intended use-case.</p><p>Text as a Modality While we have shown text-based Transformers are surprisingly powerful with text representations of scientific phenomena, we caution against the interpretation that text is all you need. For example, in chemistry, geometry is a fundamental language that determines meaning, yet Galactica has no notion of geometry; e.g. 3D co-ordinates of atoms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Future Work</head><p>For development of the base model, we highlight several directions that may be worth pursuing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>New Objective Function</head><p>It is likely further gains can be obtained with mixture-of-denoising training as U-PaLM has recently shown <ref type="bibr">(Tay et al., 2022b;</ref><ref type="bibr">Chung et al., 2022)</ref>. We suspect this might be beneficial for the scientific modalities such as protein sequences, where the left-to-right LM objective is quite limiting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Larger Context Window</head><p>We use a maximum context window length of 2048 tokens in this work. Extending this is likely to be beneficial for understanding in long-form scientific documents, such as textbooks and also documents with longer modality sequences (e.g. long protein sequences).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Extending to Images</head><p>We cannot capture scientific knowledge adequately without capturing images. This is a natural follow-up project, although it likely requires some architectural modification to make it work well. Existing work such as <ref type="bibr" target="#b0">Alayrac et al. (2022)</ref> has shown how to extend LLMs with this modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More &lt;work&gt; examples</head><p>We feel &lt;work&gt; could be a general-purpose reasoning token and we would like to invest more in this direction, including increasing prompt diversity and exploring performance on more benchmarks.</p><p>Verification Even as language models become more accurate with scale, we need assurances that their generations are correct and factual. Developing this layer is critical for production applications of language models in general beyond scientific applications.</p><p>Continual Learning Should we re-train from scratch to incorporate new scientific knowledge or train from older checkpoints? This is an open question, and further research is needed to find the best procedure for incorporating new knowledge into the model.</p><p>Retrieval Augmentation While we have shown how large language models can absorb large bodies of scientific knowledge, retrieval has a place for fine-grained types of knowledge, and we believe this is a strong direction to pursue to complement the flexible weight memory of the Transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion and Conclusion</head><p>For over half a century, the dominant way of accessing scientific knowledge has been through a storeand-retrieve paradigm. The limitation of this approach is the reasoning, combining and organization of information still relies on human effort. This has led to a significant knowledge throughput bottleneck. In this work we explored how language models might disrupt this paradigm and bring about a new interface for humanity to interface with knowledge.</p><p>We showed that language models are surprisingly strong absorbers of technical knowledge, such as LaTeX equations and chemical reactions, and these capabilities tend to scale smoothly with model size. The contextassociative power of language models likely confers significant advantages over search engines in the long-run.</p><p>We demonstrated this for citation prediction, where a language model outperforms tuned sparse and dense retrieval pipelines for this task. Language models will likely provide a valuable new tool for exploring the literature and the body of scientific knowledge in coming years.</p><p>We also demonstrated that language models can compose a curated knowledge base to perform well in knowledge-intensive question answering tasks. This includes composing knowledge in a step-by-step reasoning manner. We showed that with a working memory token approach, we can achieve strong performance over existing methods on mathematical MMLU and MATH benchmarks. We suspect tasks like MATH are in principle solvable with language model approaches. The current bottleneck is the availability of high quality step-by-step datasets. However, language models will not perform these tasks like humans until they have an architectural change that supports adaptive computation.</p><p>We also performed initial investigations on the potential of LLMs to act as a bridge between scientific modalities and natural language. We showed Galactica could learn tasks like IUPAC naming through self-supervision. We also showed that it is possible to formulate drug discovery tasks like MoleculeNet in a natural language prompt and achieve strong results without direct fine-tuning. Lastly, we showed the potential for tasks such as automatic protein annotation. In all, increasing the number (and size) of datasets that bridge between natural language and natural sequences is likely to boost performance further.</p><p>Taken together, we feel there is a strong potential for language models to take on knowledge tasks that are currently human specialisms. We open source the models so others can build on our work, and we look forward to seeing how the open machine learning community will extend it. For the chemistry and biology datasets, we wrap modalities like SMILES and protein sequences with their specialized tokens (see <ref type="bibr">Section 2.1)</ref>. For UniProt we apply data augmentation to the document format:</p><p>? Order Randomization -with probability 0.5 the protein sequence starts at beginning of the document, else the end of document. This ensures we can learn from seq ? property and property ? seq.</p><p>? Format Randomization -with probability 1 3 we replace a description, e.g. "The function of protein is...", with a Q&amp;A, e.g. "Question: What is the function of the protein? Answer: The function is...".</p><p>For NASA Exoplanet we apply order randomization to the exoplanet characteristics.</p><p>For chemical and biological sequences, we take a small subset of available entities. This is to ensure the model is not overly biased towards learning natural sequences over natural language. Specifically:</p><p>? For PubChem Compound, we take a small, random subset (2 million) of total compounds (110 million).</p><p>? For UniProt, we take reviewed Swiss-Prot proteins; a small subset (0.5 million) of total (227 million).</p><p>? For RefSeq Genome, we take reference sequences, which is a small subset of available nucleotide sequences. For the human genome, we only include the protein-coding genes.</p><p>This is a constraint we can relax in future work, enabling for much larger corpus. In this work, we focus on the first step of investigating whether a single model can learn effectively in this multi-modal setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.4 Common Crawl</head><p>We source academic and scientific content via a highly-filtered subset of CommonCrawl. The details are covered in Table <ref type="table" target="#tab_30">25</ref>. For Scientific Common Crawl, we train a fasttext classifier to identify Common Crawl webpages with scientific content <ref type="bibr" target="#b44">(Joulin et al., 2016)</ref> using a noisy set of 600 domains. We then manually annotated the domains predicted by fasttext as scientific to assemble a list of 200 high-quality scientific and reference domains.</p><p>For Academic Common Crawl, we assemble a list of academic domains, such as university websites. We take PDFs from these domains, based on the Common Crawl index, and process these using GROBID.</p><p>We do not LaTeX-process pages from these sources.</p><p>We found the quality of extracted text in CommonCrawl generally quite poor, which is why we applied stringent filters. We suspect this could be an important area for future work in order to capture more base scientific knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.5 Code</head><p>We source academic GitHub repositories from the Papers with Code index for machine learning, physics, mathematics, statistics and astronomy. The index does not explicitly cover sciences such as biology and chemistry, but many of these repositories are captured as part of the general machine learning index. We exclude repositories that do not have a license or copyright file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.6 &lt;work&gt; Datasets</head><p>For KhanProblems, we used the problems from AMPS and converted to a &lt;work&gt; format <ref type="bibr" target="#b32">(Hendrycks et al., 2021)</ref>. Where possible we tried to include more tedious steps to reduce errors from a single pass, but this annotation was fairly incomplete and we suspect bigger gains are possible with more cleaning.</p><p>For GSM8k we use the provided training dataset and convert so the calculator steps are performed by writing a Python program, following the &lt;work&gt; format <ref type="bibr" target="#b16">(Cobbe et al., 2021)</ref>. In general, we found when the model went into this prompt style, it was more error-prone. We think this is because the prompt style made the model write too many programs within &lt;work&gt;, rather than getting things ready to run in a single program. In general we found longer &lt;work&gt; answers led to a higher chance of a mistake on the reasoning path.</p><p>For OneSmallStep, we made 50 problem set question templates, and randomized the variables in the problem to get more prompt examples. We summarize the fields we made prompts below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Dataset Deduplication</head><p>We use the following procedure for deduplicating the corpus:</p><p>? We identify identical spans of 100 bytes or more (of utf-8 text) across the whole corpus, except for some explicitly excluded data sources. We do this using the repository from <ref type="bibr" target="#b56">Lee et al. (2022)</ref>. ? We process corpus files in a predetermined order to prioritize some sources. From a set of spans representing the exact same content across files, we remove the span in the first file. If the same content repeats across a single file and it was not found in the files before, all its occurrences are kept. ? We merge duplicated spans separated by at most 4 bytes.</p><p>? We narrow down the resulting spans to paragraph boundaries (i.e. "\n\n").</p><p>? We remove the content from files corresponding to the spans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Citation Identifier Ablations</head><p>We report ablations for the citation identifier ablations below, where we test title-based identifiers versus alphanumeric identifiers. Specifically, we set up an evaluation set of dataset and method names from Papers with Code. The task is to predict the citation given the method or dataset name, e.g.     If AI is going to help us explore the universe, we need it to have basic chess abilities to alleviate boredomgiven the impossibility of faster-than-light travel.</p><p>The BIG-bench task suite of <ref type="bibr" target="#b94">Srivastava et al. (2022)</ref> has a benchmark for checkmate-in-one detection. For fun, we made a dataset of 20,000 public chess games and converted them to ASCII chess using the python-chess library<ref type="foot" target="#foot_6">7</ref> . We included 19,426 games in our pre-training corpus (rest for validation). We also recorded the ELO ratings of players. An example document looks like below: For evaluation, we converted the checkmate-in-one boards to ASCII and prompted for a move. Results are shown below.</p><p>Model Accuracy GAL 125M 0.54% GAL 1.3B 0.43% GAL 6.7B</p><p>1.77% GAL 30B</p><p>1.29% GAL 120B</p><p>3.03% While this represents the state-of-the-art over other large language models<ref type="foot" target="#foot_7">8</ref> , it is clear that more work is needed on this problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Multi-Modal Data. A protein sequence occurs in a document context along with annotations, text and citations from UniProt. Full contents of the document are cut for clarity of exposition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>:</head><label></label><figDesc>for mathematical content, with or without LaTeX, we split ASCII operations into individual characters. Parentheses are treated like digits. The rest of the operations allow for unsplit repetitions. Operation characters are !"#$%&amp;'*+,-./:;&lt;=&gt;?\^_'| and parentheses are ()[]{}. 4. Numbers: we split digits into individual tokens. For example 737612.62 -&gt; 7,3,7,6,1,2,.,6,2. 5. SMILES formula: we wrap sequences with [START_SMILES] and [END_SMILES] and apply characterbased tokenization. Similarly we use [START_I_SMILES] and [END_I_SMILES] where isomeric SMILES is denoted. For example, C(C(=O)O)N ? C,(,C,(,=,O,),O,),N. 6. Amino acid sequences: we wrap sequences with [START_AMINO] and [END_AMINO] and apply character-based tokenization, treating each amino acid character as a single token. For example, MIRLGAPQTL -&gt; M,I,R,L,G,A,P,Q,T,L. 7. DNA sequences: we also apply a character-based tokenization, treating each nucleotide base as a token, where the start tokens are [START_DNA] and [END_DNA]. For example, CGGTACCCTC -&gt; C, G, G, T, A, C, C, C, T, C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Given a task like "What is the average of 43, 29, 51, 13?" a human can use internal or external working memory. In practice, they will use both symbiotically; meaning that working out that is written down in text is usually "missing" some steps performed internally.</figDesc><graphic url="image-7.png" coords="6,72.00,72.00,468.00,172.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Question:Figure 3 :</head><label>3</label><figDesc>Figure3: Model-Machine Symbiosis. We show an example answer with the &lt;work&gt; working memory token. It performs exact steps for rearranging the equation, and when it reaches a calculation that it cannot solve reliably in a forward-pass, it writes a program, which can then be offloaded to a classical computer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Prompt Pre-training. Pre-training weighs all tokens equally as part of the self-supervised loss. This leads to a weak relative signal for tasks of interest, meaning model scale has to be large to work. Instruction tuning boosts performance post hoc, and can generalize to unseen tasks of interest, but it risks performance in tasks that are distant from instruction set tasks. Prompt pre-training has a weaker task of interest bias than instruction tuning but less risk of degrading overall task generality.</figDesc><graphic url="image-8.png" coords="9,118.80,72.00,374.40,140.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Repeated Tokens and Validation Loss. With four epochs of training, we continue to see validation loss fall for all model sizes. For the 120B model we see the first signs of overfitting at the beginning of the fifth epoch, and we early stop at this point.</figDesc><graphic url="image-9.png" coords="11,72.00,72.00,468.00,238.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Validation Loss Per Source. Validation loss falls through training for all dataset categories. Results are shown for the 30B model above. The 120B exhibits the same relative trend of declining validation loss for all sources until the beginning of fifth epoch, where all sources spike (see Appendix).</figDesc><graphic url="image-10.png" coords="12,72.00,90.52,467.99,241.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: BIG-bench Performance During Training. The 57 task selection from BIG-bench contains principally non-scientific tasks. We use it as a proxy for out-of-domain performance. For the 120B model above, we see no signs of overfitting after four repeats of the corpus.</figDesc><graphic url="image-11.png" coords="12,72.00,416.22,468.00,245.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Distributional Comparison of Citations. Galactica's citation distribution approaches the ground truth with scale. This is seen through a declining KS distance with scale, and increasing histogram overlap.</figDesc><graphic url="image-13.png" coords="18,245.47,73.24,299.53,199.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Figure13: Attending to Functional Groups. Galactica uses its knowledge of chemistry to help with the IUPAC Naming task. At each stage of prediction, it attends to the part of the molecular graph associated with the group name, e.g. for "amino" it attends to the nitrogen atom; for thiazole, the sulphur atom.</figDesc><graphic url="image-22.png" coords="21,83.96,606.10,116.99,54.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>[</head><label></label><figDesc>Figure 14: BBBP Prompt. We include the SMILES and pose the classification problem in natural language.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>[</head><label></label><figDesc>Figure 15: Tox21 Prompt. We include the protein sequence and the SMILES formula and pose the classification problem in natural language.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>( a )Figure 16 :</head><label>a16</label><figDesc>Figure 16: Attention Visualization on Tox21. The top three molecules are highest confidence positive examples for the 30B model; the bottom three are the highest confidence negatives. We match attention weights from the SMILES with the canonical atom ordering. Danazol and gestodene are known to possess high affinities for the androgen receptor (AR) (Nieschlag et al., 2010).</figDesc><graphic url="image-26.png" coords="23,72.00,225.29,149.76,84.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Primary Structure Prediction. For three of the validation sets we observe smooth scaling, reflecting the potential for high sequence similarity with sequences in the training set; for example, orthologs in the case of the Paen validation set. The CASP set with sequence similarity constraints levels off, suggesting the gains from the 550k proteins in training quickly saturates for more out-of-domain sequences.</figDesc><graphic url="image-29.png" coords="24,72.00,72.00,468.00,254.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: CASPSeq Validation During Training. Overfitting occurs before the end of training, but the effect is not drastic, and repeating the protein sequences three times does not damage performance on this task. The final 120B model is the second-last point, reflecting the early stopping we applied (see earlier Sections)</figDesc><graphic url="image-30.png" coords="25,72.00,367.54,468.00,247.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Protein Keyword Prediction. This test's Galactica's capability to predict protein keywords, e.g. "cytoplasm", from the sequence alone. For the Paen and General datasets, this capability improves smoothly with scale. It scales more slowly and begins to saturate for the CASPSimSeq set, reflecting the lower sequence similarity with sequences in the training set.</figDesc><graphic url="image-31.png" coords="26,72.00,72.00,468.00,258.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>Figure 20: Protein Keyword Prediction. Example shown is Q108U0 from the PaenSeq holdout, a cystic fibrosis transmembrane conductance regulator from the African elephant. The closest protein by sequence similarity in the training set is the Q2QLA3 protein, a cystic fibrosis transmembrane conductance regular from a horse, with 91.8% sequence similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Protein Description Prediction. Example shown is Q7Y8J5 from the PaenSeq holdout, a Cytochrome b protein from a rock hyrax. The closest protein by sequence similarity in the training set is the O03363 protein, a Cytochrome b protein from a pygmy hippopotamus, with 83% sequence similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 22 :</head><label>22</label><figDesc>Figure22: Toxicity rate on RealToxicityPrompts. Galactica exhibits much lower toxicity continuation rates, even as we increase the original prompt toxicity.</figDesc><graphic url="image-32.png" coords="30,72.00,72.00,468.01,241.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-33.png" coords="46,72.00,97.18,467.99,241.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Tokenizing Nature</head><label>1</label><figDesc></figDesc><table><row><cell>Data source</cell><cell>Documents</cell><cell cols="2">Tokens Token %</cell></row><row><cell>Papers</cell><cell cols="2">48 million 88 billion</cell><cell>83.0%</cell></row><row><cell>Code</cell><cell>2 million</cell><cell>7 billion</cell><cell>6.9%</cell></row><row><cell>Reference Material</cell><cell>8 million</cell><cell>7 billion</cell><cell>6.5%</cell></row><row><cell>Knowledge Bases</cell><cell>2 million</cell><cell>2 billion</cell><cell>2.0%</cell></row><row><cell>Filtered CommonCrawl</cell><cell>0.9 million</cell><cell>1 billion</cell><cell>1.0%</cell></row><row><cell>Prompts</cell><cell cols="2">1.3 million 0.4 billion</cell><cell>0.3%</cell></row><row><cell>Other</cell><cell cols="2">0.02 million 0.2 billion</cell><cell>0.2%</cell></row></table><note><p><p>. Galactica trains on text sequences that represent scientific phenomena.</p>Total dataset size = 106 billion tokens</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 : The Galactica Corpus</head><label>2</label><figDesc></figDesc><table /><note><p>. A full breakdown of these sources is contained in the Appendix.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Type II collagen is specific for cartilaginous tissues. It is essential for the normal embryonic development of the skeleton, for linear growth and for the ability of cartilage to resist compressive forces.[START_REF]Nucleotide sequence of the full length cDNA encoding for human type II procollage,Lee[END_REF]...</figDesc><table><row><cell>Summary</cell></row><row><cell>Protein: Collagen alpha-1(II) chain</cell></row><row><cell>Gene: COL2A1</cell></row><row><cell>Organism: Homo sapiens (Human)</cell></row><row><cell>Status: evidence at protein level</cell></row><row><cell>Function</cell></row><row><cell>Features</cell></row><row><cell>-Domain, 32-90, Cleavage; by procollagen N-endopeptidase</cell></row><row><cell>-Site Cleavage, 181-182, Cleavage; by procollagen N-endopeptidase</cell></row><row><cell>-Binding site, 1301, Ca2+</cell></row><row><cell>...</cell></row></table><note><p><p>[START_AMINO]MIRLGAPQTLVLLTLLVAAVLRCQGQDVQEAGSCVQDGQRYNDKDVWKPEPCRICVCDTG...</p>[END_AMINO]    </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 : Reasoning Datasets</head><label>3</label><figDesc></figDesc><table /><note><p>To train the model to use &lt;work&gt; we include several datasets in pre-training that incorporate this token. Full details are contained in the Appendix.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Figure 4: Citation Processed Text. Example</head><label></label><figDesc>of citation processed text from Attention Is All You Need</figDesc><table><row><cell>Recurrent neural networks, long short-term memory [START_REF]Long Short-Term Memory,</cell></row><row><cell>Hochreiter[END_REF] and gated recurrent [START_REF]Empirical Evaluation of Gated Recurrent Neural</cell></row><row><cell>Networks on Sequence Modeling, Chung[END_REF] neural networks in particular, have been firmly estab-</cell></row><row><cell>lished as state of the art approaches in sequence modeling and transduction problems such as language</cell></row><row><cell>modeling and machine translation [START_REF]Sequence to Sequence Learning with Neural Networks,</cell></row><row><cell>Sutskever[END_REF][START_REF]Neural Machine Translation by Jointly Learning to Align and Translate,</cell></row><row><cell>Bahdanau[END_REF][START_REF]Learning Phrase Representations Using RNN Encoder-Decoder for Statistical</cell></row><row><cell>Machine Translation, Cho[END_REF].</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 : Pre-training Prompts. We</head><label>4</label><figDesc>This leads to a weak relative signal for tasks of interest, meaning model scale has to be large to work. Instruction tuning boosts performance post hoc, and can generalize to unseen tasks of interest, but it risks performance in tasks that are distant from instruction set tasks. Prompt pre-training has a weaker task of interest bias than instruction tuning but less risk of degrading overall task generality. include zero-shot prompts in pre-training to boost the task signal.</figDesc><table><row><cell>Task</cell><cell>Prompts</cell><cell>Tokens</cell></row><row><cell>Chemical Properties</cell><cell cols="2">782,599 275 million</cell></row><row><cell>Multiple-Choice QA</cell><cell>256,886</cell><cell>31 million</cell></row><row><cell>Extractive QA</cell><cell>30,935</cell><cell>13 million</cell></row><row><cell>Summarization</cell><cell>6,339</cell><cell>11 million</cell></row><row><cell>Entity Extraction</cell><cell>156,007</cell><cell>9 million</cell></row><row><cell>Reasoning</cell><cell>21,543</cell><cell>9 million</cell></row><row><cell>Dialog</cell><cell>18,930</cell><cell>5 million</cell></row><row><cell>Binary QA</cell><cell>36,334</cell><cell>4 million</cell></row><row><cell>Other</cell><cell>3,559</cell><cell>1 million</cell></row><row><cell>Total</cell><cell cols="2">783,599 358 million</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Modeln params n layers d model n heads d heads Batch Size</figDesc><table><row><cell>Max LR</cell><cell>Warmup</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Details of the models trained</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>6% 83.5% 72.4% 52.5% 36.4% 68.2% Table 6: Results on LaTeX equations</head><label></label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="4">Params (bn) Chemistry Maths Physics</cell><cell>Stats</cell><cell cols="2">Econ Overall</cell></row><row><cell>OPT</cell><cell>175</cell><cell>34.1%</cell><cell>4.5%</cell><cell>22.9%</cell><cell>1.0%</cell><cell>2.3%</cell><cell>8.9%</cell></row><row><cell>BLOOM</cell><cell>176</cell><cell cols="2">36.3% 36.1%</cell><cell cols="3">6.6% 14.1% 13.6%</cell><cell>21.4%</cell></row><row><cell>GPT-3 (text-davinci-002)</cell><cell>?</cell><cell cols="2">61.4% 65.4%</cell><cell cols="3">41.9% 25.3% 31.8%</cell><cell>49.0%</cell></row><row><cell>GAL 125M</cell><cell>0.1</cell><cell>0.0%</cell><cell>0.8%</cell><cell>0.0%</cell><cell>1.0%</cell><cell>0.0%</cell><cell>0.5%</cell></row><row><cell>GAL 1.3B</cell><cell>1.3</cell><cell cols="2">31.8% 26.3%</cell><cell cols="2">23.8% 11.1%</cell><cell>4.6%</cell><cell>20.5%</cell></row><row><cell>GAL 6.7B</cell><cell>6.7</cell><cell cols="2">43.2% 59.4%</cell><cell cols="3">36.2% 29.3% 27.3%</cell><cell>41.7%</cell></row><row><cell>GAL 30B</cell><cell>30</cell><cell cols="2">63.6% 74.4%</cell><cell cols="3">35.2% 40.4% 34.1%</cell><cell>51.5%</cell></row><row><cell>GAL 120B</cell><cell>120</cell><cell>79.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>. Results are evaluated zero-shot.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="6">Params (bn) Amino BioLAMA Reactions Clusters Minerals</cell></row><row><cell>OPT</cell><cell>175</cell><cell>12.0%</cell><cell>7.1%</cell><cell>12.7%</cell><cell>21.7%</cell><cell>1.6%</cell></row><row><cell>BLOOM</cell><cell>176</cell><cell>14.0%</cell><cell>9.7%</cell><cell>22.4%</cell><cell>15.0%</cell><cell>10.3%</cell></row><row><cell>GPT-3 (text-davinci-002)</cell><cell>?</cell><cell>14.0%</cell><cell>8.4%</cell><cell>35.1%</cell><cell>20.8%</cell><cell>18.3%</cell></row><row><cell>GAL 125M</cell><cell>0.1</cell><cell>12.0%</cell><cell>3.1%</cell><cell>0.3%</cell><cell>6.7%</cell><cell>0.0%</cell></row><row><cell>GAL 1.3B</cell><cell>1.3</cell><cell>16.0%</cell><cell>7.2%</cell><cell>14.4%</cell><cell>14.2%</cell><cell>10.3%</cell></row><row><cell>GAL 6.7B</cell><cell>6.7</cell><cell>17.0%</cell><cell>7.9%</cell><cell>26.4%</cell><cell>17.5%</cell><cell>8.7%</cell></row><row><cell>GAL 30B</cell><cell>30</cell><cell>21.0%</cell><cell>6.9%</cell><cell>36.5%</cell><cell>20.0%</cell><cell>17.5%</cell></row><row><cell>GAL 120B</cell><cell>120</cell><cell>21.0%</cell><cell>8.0%</cell><cell>43.1%</cell><cell>24.2%</cell><cell>29.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note><p>Results on Domain Probes. Results are evaluated zero-shot.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table><row><cell>MMLU</cell></row></table><note><p>Results on Mathematics MMLU. Galactica is evaluated without few-shot examples. With the &lt;work&gt; token we see large gains in performance. Results are on MMLU test.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Results on MATH.With both the chain-of-thought and &lt;work&gt; token prompts, Galactica exceeds PaLM's performance with 18 times less capacity.</figDesc><table><row><cell></cell><cell cols="3">Alg CProb Geom</cell><cell cols="5">I.Alg N.Theory Prealg Precalc Average</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Base Models</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GPT-3 175B (8-shot)</cell><cell>6.0%</cell><cell>4.7%</cell><cell>3.1%</cell><cell>4.4%</cell><cell>4.4%</cell><cell>7.7%</cell><cell>4.0%</cell><cell>5.2%</cell></row><row><cell>PaLM 540B (5-shot) mCoT</cell><cell>9.7%</cell><cell>8.4%</cell><cell>7.3%</cell><cell>3.5%</cell><cell cols="2">6.0% 19.2%</cell><cell>4.4%</cell><cell>8.8%</cell></row><row><cell>GAL 30B &lt;work&gt;</cell><cell>15.8%</cell><cell>6.3%</cell><cell>5.8%</cell><cell>4.9%</cell><cell cols="2">2.4% 19.4%</cell><cell>8.2%</cell><cell>11.4%</cell></row><row><cell>GAL 30B (5-shot) mCoT</cell><cell>17.9%</cell><cell>6.8%</cell><cell>7.9%</cell><cell>7.0%</cell><cell cols="2">5.7% 17.9%</cell><cell>7.9%</cell><cell>12.7%</cell></row><row><cell>GAL 120B &lt;work&gt;</cell><cell cols="2">23.1% 10.1%</cell><cell>9.8%</cell><cell>8.6%</cell><cell cols="2">6.5% 23.8%</cell><cell>11.7%</cell><cell>16.6%</cell></row><row><cell>GAL 120B (5-shot) mCoT</cell><cell cols="3">29.0% 13.9% 12.3%</cell><cell>9.6%</cell><cell cols="2">11.7% 27.2%</cell><cell>12.8%</cell><cell>20.4%</cell></row><row><cell></cell><cell></cell><cell cols="3">Fine-tuned LaTeX Models</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Minerva 540B (5-shot) mCoT 51.3% 28.0% 26.8% 13.7%</cell><cell cols="2">21.2% 55.0%</cell><cell>18.0%</cell><cell>33.6%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table><row><cell></cell><cell>Domain</cell><cell>GAL</cell><cell cols="5">OPT BLOOM GPT-3 Gopher Chinchilla</cell></row><row><cell>Abstract Algebra</cell><cell cols="3">out-of-domain 33.3% 21.0%</cell><cell>25.0%</cell><cell>-</cell><cell>25.0%</cell><cell>31.0%</cell></row><row><cell>ARC Challenge</cell><cell>in-domain</cell><cell cols="2">67.9% 31.1%</cell><cell>32.9%</cell><cell>51.4%</cell><cell>-</cell><cell>-</cell></row><row><cell>ARC Easy</cell><cell>in-domain</cell><cell cols="2">83.8% 37.4%</cell><cell>40.7%</cell><cell>68.8%</cell><cell>-</cell><cell>-</cell></row><row><cell>Astronomy</cell><cell cols="3">out-of-domain 65.1% 23.0%</cell><cell>25.7%</cell><cell>-</cell><cell>65.8%</cell><cell>73.0%</cell></row><row><cell>BioASQ</cell><cell>in-domain</cell><cell cols="2">94.3% 81.4%</cell><cell>91.4%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Biology (College)</cell><cell cols="3">out-of-domain 68.8% 30.6%</cell><cell>28.5%</cell><cell>-</cell><cell>70.8%</cell><cell>79.9%</cell></row><row><cell>Biology (High-School)</cell><cell cols="3">out-of-domain 69.4% 27.7%</cell><cell>29.4%</cell><cell>-</cell><cell>71.3%</cell><cell>80.3%</cell></row><row><cell>Chemistry (College)</cell><cell cols="3">out-of-domain 46.0% 30.0%</cell><cell>19.0%</cell><cell>-</cell><cell>45.0%</cell><cell>51.0%</cell></row><row><cell>Chemistry (High-School)</cell><cell cols="3">out-of-domain 47.8% 21.7%</cell><cell>23.2%</cell><cell>-</cell><cell>47.8%</cell><cell>58.1%</cell></row><row><cell>Comp. Science (College)</cell><cell cols="3">out-of-domain 49.0% 17.0%</cell><cell>6.0%</cell><cell>-</cell><cell>49.0%</cell><cell>51.0%</cell></row><row><cell cols="4">Comp. Science (High-School) out-of-domain 70.0% 30.0%</cell><cell>25.0%</cell><cell>-</cell><cell>54.0%</cell><cell>58.0%</cell></row><row><cell>Econometrics</cell><cell cols="3">out-of-domain 42.1% 21.0%</cell><cell>23.7%</cell><cell>-</cell><cell>43.0%</cell><cell>38.6%</cell></row><row><cell>Electrical Engineering</cell><cell cols="3">out-of-domain 62.8% 36.6%</cell><cell>32.4%</cell><cell>-</cell><cell>60.0%</cell><cell>62.1%</cell></row><row><cell>Elementary Mathematics</cell><cell cols="3">out-of-domain 38.1% 25.7%</cell><cell>27.6%</cell><cell>-</cell><cell>33.6%</cell><cell>41.5%</cell></row><row><cell>Formal Logic</cell><cell cols="3">out-of-domain 32.5% 29.4%</cell><cell>26.2%</cell><cell>-</cell><cell>35.7%</cell><cell>33.3%</cell></row><row><cell>Machine Learning</cell><cell cols="3">out-of-domain 38.4% 28.6%</cell><cell>25.0%</cell><cell>-</cell><cell>41.1%</cell><cell>41.1%</cell></row><row><cell>Mathematics (College)</cell><cell cols="3">out-of-domain 43.0% 33.0%</cell><cell>25.0%</cell><cell>-</cell><cell>37.0%</cell><cell>32.0%</cell></row><row><cell>Mathematics (High-School)</cell><cell cols="3">out-of-domain 32.6% 24.4%</cell><cell>27.0%</cell><cell>-</cell><cell>23.7%</cell><cell>31.9%</cell></row><row><cell>Medical Genetics</cell><cell cols="3">out-of-domain 70.0% 35.0%</cell><cell>36.0%</cell><cell>-</cell><cell>69.0%</cell><cell>69.0%</cell></row><row><cell>Physics (College)</cell><cell cols="3">out-of-domain 42.2% 21.6%</cell><cell>18.6%</cell><cell>-</cell><cell>34.3%</cell><cell>46.1%</cell></row><row><cell>Physics (High-School)</cell><cell cols="3">out-of-domain 33.8% 29.8%</cell><cell>25.2%</cell><cell>-</cell><cell>33.8%</cell><cell>36.4%</cell></row><row><cell>MedQA-USMLE</cell><cell cols="3">out-of-domain 44.4% 22.8%</cell><cell>23.3%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MedMCQA Dev</cell><cell>in-domain</cell><cell cols="2">52.9% 29.6%</cell><cell>32.5%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PubMedQA</cell><cell>in-domain</cell><cell cols="2">77.6% 70.2%</cell><cell>73.6%</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Statistics (High-School)</cell><cell cols="3">out-of-domain 41.2% 43.5%</cell><cell>19.4%</cell><cell>-</cell><cell>50.0%</cell><cell>58.8%</cell></row></table><note><p>Question Answering Results. Galactica is evaluated without few-shot examples. Other LLMs are evaluated 5-shot, except for 0-shot results for GPT-3 on ARC results and OPT and BLOOM on PubMedQA and BioASQ.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Citation Prediction Accuracy. Performance of different model sizes on citation prediction.</figDesc><table><row><cell></cell><cell cols="4">Params (bn) PWC Citations Extended Citations Contextual Citations</cell></row><row><cell>GAL 125M</cell><cell>0.1</cell><cell>7.0%</cell><cell>6.4%</cell><cell>7.1%</cell></row><row><cell>GAL 1.3B</cell><cell>1.3</cell><cell>18.5%</cell><cell>45.5%</cell><cell>15.9%</cell></row><row><cell>GAL 6.7B</cell><cell>6.7</cell><cell>32.0%</cell><cell>60.0%</cell><cell>23.0%</cell></row><row><cell>GAL 30B</cell><cell>30</cell><cell>44.7%</cell><cell>66.4%</cell><cell>31.5%</cell></row><row><cell>GAL 120B</cell><cell>120</cell><cell>51.9%</cell><cell>69.1%</cell><cell>36.6%</cell></row><row><cell>Sparse Retriever</cell><cell>n/a</cell><cell>30.9%</cell><cell>17.3%</cell><cell>5.3%</cell></row><row><cell>Dense Retriever (base)</cell><cell>n/a</cell><cell>16.4%</cell><cell>8.8%</cell><cell>1.6%</cell></row><row><cell>Dense Retriever (fine-tuned)</cell><cell>n/a</cell><cell>27.6%</cell><cell>11.8%</cell><cell>8.2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12</head><label>12</label><figDesc></figDesc><table><row><cell>:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 12 : BIG-bench 57 Task Results</head><label>12</label><figDesc></figDesc><table /><note><p>. Galactica outperforms general open models at smaller scales.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13 .</head><label>13</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="3">Params (bn) Accuracy Invalid Names</cell></row><row><cell>GAL 125M</cell><cell>0.1</cell><cell>0.0%</cell><cell>32.8%</cell></row><row><cell>GAL 1.3B</cell><cell>1.3</cell><cell>2.5%</cell><cell>12.0%</cell></row><row><cell>GAL 6.7B</cell><cell>6.7</cell><cell>10.7%</cell><cell>12.3%</cell></row><row><cell>GAL 30B</cell><cell>30</cell><cell>15.4%</cell><cell>9.7%</cell></row><row><cell>GAL 120B</cell><cell>120</cell><cell>39.2%</cell><cell>9.2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 13 :</head><label>13</label><figDesc>Results on IUPAC Naming. Performance improves smoothly with scale.Accuracy increases smoothly with scale. Given we restricted the corpus to 2 million molecules, it is likely much better performance is achievable through training or fine-tuning on more molecules. The model is freely available for those who want to perform this follow-up work.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 14 : MoleculeNet datasets used for evaluation.</head><label>14</label><figDesc>We convert training sets to text format and include in pre-training. We evaluate using the splits suggested by the DeepChem library</figDesc><table><row><cell></cell><cell cols="2">Dataset Type</cell><cell>Other modalities</cell></row><row><cell></cell><cell>HIV</cell><cell cols="2">Classification n/a</cell></row><row><cell>Biophysics</cell><cell cols="3">BACE C Classification n/a</cell></row><row><cell></cell><cell>BBBP</cell><cell cols="2">Classification n/a</cell></row><row><cell>Physiology</cell><cell>Tox21 SIDER</cell><cell cols="2">Classification protein sequences Classification n/a</cell></row><row><cell></cell><cell cols="3">ClinTox Classification n/a</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 15 : Results on MoleculeNet Classification</head><label>15</label><figDesc>The model is available for work on this.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">MoleculeNet Classification</cell><cell></cell></row><row><cell>Model</cell><cell cols="4">Modality Molecules BACE BBBP ClinTox</cell><cell cols="2">HIV SIDER Tox21</cell><cell>Av.</cell></row><row><cell cols="2">GAL 125M SMILES</cell><cell>2M</cell><cell>0.561 0.393</cell><cell cols="2">0.518 0.702</cell><cell>0.559 0.543 0.581</cell></row><row><cell>GAL 1.3B</cell><cell>SMILES</cell><cell>2M</cell><cell>0.576 0.604</cell><cell cols="2">0.589 0.724</cell><cell>0.540 0.606 0.619</cell></row><row><cell>GAL 6.7B</cell><cell>SMILES</cell><cell>2M</cell><cell>0.584 0.535</cell><cell cols="2">0.784 0.722</cell><cell>0.559 0.639 0.640</cell></row><row><cell>GAL 30B</cell><cell>SMILES</cell><cell>2M</cell><cell>0.727 0.596</cell><cell cols="2">0.822 0.759</cell><cell>0.613 0.685 0.687</cell></row><row><cell>GAL 120B</cell><cell>SMILES</cell><cell>2M</cell><cell>0.617 0.661</cell><cell cols="2">0.826 0.745</cell><cell>0.632 0.689 0.690</cell></row><row><cell>Uni-Mol</cell><cell>3D</cell><cell>20M</cell><cell>0.857 0.729</cell><cell cols="2">0.919 0.808</cell><cell>0.659 0.796 0.770</cell></row></table><note><p>. Results are scored by ROC-AUC.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 16 : Protein Validation Perplexity</head><label>16</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Protein Sequence Validation Perplexity</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="5">Param (bn) CASPSeq CASPSimSeq PaenSeq UniProtSeq</cell></row><row><cell>GAL 125M</cell><cell>0.1</cell><cell>20.62</cell><cell>19.18</cell><cell>16.35</cell><cell>19.05</cell></row><row><cell>GAL 1.3B</cell><cell>1.3</cell><cell>17.58</cell><cell>17.04</cell><cell>12.53</cell><cell>15.82</cell></row><row><cell>GAL 6.7B</cell><cell>6.7</cell><cell>17.29</cell><cell>16.35</cell><cell>7.76</cell><cell>11.58</cell></row><row><cell>GAL 30B</cell><cell>30</cell><cell>17.27</cell><cell>15.42</cell><cell>4.28</cell><cell>8.23</cell></row><row><cell>GAL 120B</cell><cell>120</cell><cell>17.26</cell><cell>12.77</cell><cell>3.14</cell><cell>5.54</cell></row></table><note><p>. Validation sets with higher potential sequence similarity with the training set have lower perplexity than the restricted sets (CASP validation sets).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 17 :</head><label>17</label><figDesc>.8% in the training set, to help annotate. Protein Keyword Prediction. Metric shown is F 1 score. Performance increases with scale across the holdout sets. Note we do not include CASPSeq as these do not have UniProt keywords we can test against.</figDesc><table><row><cell></cell><cell cols="3">Protein Keyword Prediction</cell><cell></cell></row><row><cell>Model</cell><cell cols="4">Param (bn) CASPSimSeq PaenSeq UniProtSeq</cell></row><row><cell>GAL 125M</cell><cell>0.1</cell><cell>10.5%</cell><cell>9.3%</cell><cell>15.2%</cell></row><row><cell>GAL 1.3B</cell><cell>1.3</cell><cell>17.4%</cell><cell>26.0%</cell><cell>21.9%</cell></row><row><cell>GAL 6.7B</cell><cell>6.7</cell><cell>18.4%</cell><cell>33.3%</cell><cell>25.1%</cell></row><row><cell>GAL 30B</cell><cell>30</cell><cell>22.0%</cell><cell>42.6%</cell><cell>40.8%</cell></row><row><cell>GAL 120B</cell><cell>120</cell><cell>21.9%</cell><cell>54.5%</cell><cell>48.7%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 18 :</head><label>18</label><figDesc>Protein Function Prediction. Metric shown is ROUGE-L. Performance increases with scale.</figDesc><table><row><cell></cell><cell cols="2">Protein Function Prediction</cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="4">Param (bn) CASPSimSeq PaenSeq UniProtSeq</cell></row><row><cell>GAL 125M</cell><cell>0.1</cell><cell>0.062</cell><cell>0.073</cell><cell>0.061</cell></row><row><cell>GAL 1.3B</cell><cell>1.3</cell><cell>0.069</cell><cell>0.084</cell><cell>0.079</cell></row><row><cell>GAL 6.7B</cell><cell>6.7</cell><cell>0.109</cell><cell>0.137</cell><cell>0.111</cell></row><row><cell>GAL 30B</cell><cell>30</cell><cell>0.137</cell><cell>0.196</cell><cell>0.186</cell></row><row><cell>GAL 120B</cell><cell>120</cell><cell>0.252</cell><cell>0.272</cell><cell>0.252</cell></row></table><note><p>from PaenSeq. The protein is a Cytochrome b protein from a rock hyrax (Q7Y8J5). The closest sequence by similarity in the training set is a Cytochrome b protein from a pygmy hippopotamus (O03363) with 83% sequence similarity. In this case we get a perfect prediction from the description.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 19 :</head><label>19</label><figDesc>CrowS-Pairs Results. Galactica demonstrates significantly lower stereotypical bias in all categories with the exception of sexual orientation and age.</figDesc><table><row><cell>type</cell><cell cols="3">text-davinci-002 OPT 175B Galactica 120B</cell></row><row><cell>Race</cell><cell>64.7</cell><cell>68.6</cell><cell>59.9</cell></row><row><cell>Socioeconomic</cell><cell>73.8</cell><cell>76.2</cell><cell>65.7</cell></row><row><cell>Gender</cell><cell>62.6</cell><cell>65.7</cell><cell>51.9</cell></row><row><cell>Disability</cell><cell>76.7</cell><cell>76.7</cell><cell>66.7</cell></row><row><cell>Nationality</cell><cell>61.6</cell><cell>62.9</cell><cell>51.6</cell></row><row><cell>Sexual-orientation</cell><cell>76.2</cell><cell>78.6</cell><cell>77.4</cell></row><row><cell>Physical-appearance</cell><cell>74.6</cell><cell>76.2</cell><cell>58.7</cell></row><row><cell>Religion</cell><cell>73.3</cell><cell>68.6</cell><cell>67.6</cell></row><row><cell>Age</cell><cell>64.4</cell><cell>67.8</cell><cell>69.0</cell></row><row><cell>Overall</cell><cell>67.2</cell><cell>69.5</cell><cell>60.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 20 :</head><label>20</label><figDesc>StereoSet Results. Galactica outperforms all models across all categories on the ICAT score.</figDesc><table><row><cell>Category</cell><cell></cell><cell cols="3">text-davinci-002 OPT 175B Galactica 120B</cell></row><row><cell></cell><cell>LMS (?)</cell><cell>78.4</cell><cell>74.1</cell><cell>75.2</cell></row><row><cell>Prof.</cell><cell>SS (?)</cell><cell>63.4</cell><cell>62.6</cell><cell>57.2</cell></row><row><cell></cell><cell>ICAT (?)</cell><cell>57.5</cell><cell>55.4</cell><cell>64.3</cell></row><row><cell></cell><cell>LMS (?)</cell><cell>75.6</cell><cell>74.0</cell><cell>74.6</cell></row><row><cell>Gend.</cell><cell>SS (?)</cell><cell>66.5</cell><cell>63.6</cell><cell>59.1</cell></row><row><cell></cell><cell>ICAT (?)</cell><cell>50.6</cell><cell>53.8</cell><cell>61.0</cell></row><row><cell></cell><cell>LMS (?)</cell><cell>80.8</cell><cell>84.0</cell><cell>81.4</cell></row><row><cell>Reli.</cell><cell>SS (?)</cell><cell>59.0</cell><cell>59.0</cell><cell>55.1</cell></row><row><cell></cell><cell>ICAT (?)</cell><cell>66.3</cell><cell>68.9</cell><cell>73.1</cell></row><row><cell></cell><cell>LMS (?)</cell><cell>77.0</cell><cell>74.9</cell><cell>74.5</cell></row><row><cell>Race</cell><cell>SS (?)</cell><cell>57.4</cell><cell>56.8</cell><cell>54.8</cell></row><row><cell></cell><cell>ICAT (?)</cell><cell>65.7</cell><cell>64.8</cell><cell>67.3</cell></row><row><cell></cell><cell>LMS (?)</cell><cell>77.6</cell><cell>74.8</cell><cell>75.0</cell></row><row><cell>Overall</cell><cell>SS (?)</cell><cell>60.8</cell><cell>59.9</cell><cell>56.2</cell></row><row><cell></cell><cell>ICAT (?)</cell><cell>60.8</cell><cell>60.0</cell><cell>65.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 21 :</head><label>21</label><figDesc>TruthfulQA Results. Galactica exhibits superior performance to other language models, and performance increases with scale. but slowly and at low levels.</figDesc><table><row><cell></cell><cell>TruthfulQA</cell><cell></cell></row><row><cell>Model</cell><cell cols="2">MC1 (Acc) MC1 (Std)</cell></row><row><cell>OPT 175B</cell><cell>21%</cell><cell>0.13</cell></row><row><cell>BLOOM 176B</cell><cell>19%</cell><cell>0.07</cell></row><row><cell>GAL 125M</cell><cell>19%</cell><cell>0.11</cell></row><row><cell>GAL 1.3B</cell><cell>19%</cell><cell>0.15</cell></row><row><cell>GAL 6.7B</cell><cell>19%</cell><cell>0.03</cell></row><row><cell>GAL 30B</cell><cell>24%</cell><cell>0.05</cell></row><row><cell>GAL 120B</cell><cell>26%</cell><cell>0.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 23 :</head><label>23</label><figDesc>Reference material used in our corpus</figDesc><table><row><cell>Data source</cell><cell>Documents</cell><cell>Tokens</cell></row><row><cell>Wikipedia</cell><cell>6 million</cell><cell>5 billion</cell></row><row><cell>StackExchange</cell><cell>1.6 million</cell><cell>1 billion</cell></row><row><cell>LibreText</cell><cell cols="2">95,113 185 million</cell></row><row><cell>Wikibooks</cell><cell cols="2">74,705 110 million</cell></row><row><cell>Open Textbooks</cell><cell>647</cell><cell>94 million</cell></row><row><cell>MIT OCW</cell><cell>25,640</cell><cell>90 million</cell></row><row><cell>Wikiversity</cell><cell>38,138</cell><cell>52 million</cell></row><row><cell>ProofWiki</cell><cell>32,389</cell><cell>12 million</cell></row><row><cell>Khan Academy</cell><cell>3,075</cell><cell>7 million</cell></row><row><cell>Papers with Code</cell><cell>13,430</cell><cell>4 million</cell></row><row><cell>IUPAC Goldbook</cell><cell>6,788</cell><cell>1 million</cell></row><row><cell>Total</cell><cell>8 million</cell><cell>7 billion</cell></row><row><cell>Data source</cell><cell>Documents</cell><cell>Tokens</cell></row><row><cell>PubChem Compound</cell><cell>1.7 million</cell><cell>1 billion</cell></row><row><cell>UniProt</cell><cell>551,837</cell><cell>0.6 billion</cell></row><row><cell>RefSeq Genome</cell><cell>69</cell><cell>0.1 billion</cell></row><row><cell>OEIS</cell><cell cols="2">350,833 0.07 billion</cell></row><row><cell>Ribosome</cell><cell cols="2">9,950 0.05 billion</cell></row><row><cell>LIPID MAPS</cell><cell cols="2">45,273 0.03 billion</cell></row><row><cell>Reactome</cell><cell cols="2">156 0.01 billion</cell></row><row><cell>NASA Exoplanet</cell><cell cols="2">5,021 0.01 billion</cell></row><row><cell>Total</cell><cell>2 million</cell><cell>2 billion</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 24 :</head><label>24</label><figDesc>Knowledge bases used in our corpus</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head>Table 25 :</head><label>25</label><figDesc>CommonCrawl material used in our corpus</figDesc><table><row><cell>Data source</cell><cell>Documents</cell><cell>Tokens</cell></row><row><cell>ScientificCC</cell><cell cols="2">0.8 million 0.7 billion</cell></row><row><cell cols="3">AcademicCC 0.05 million 0.4 billion</cell></row><row><cell>Total</cell><cell cols="2">0.9 million 1.1 billion</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head></head><label></label><figDesc>ResNet [START_REF], where the target is Deep Residual Learning for Image Recognition, He. We train a 6.7bn model on both types of processing for the ablation. Method and dataset results are shown below.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Citation Processing</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(a) Titles</cell><cell></cell><cell></cell><cell>(b) IDs</cell><cell></cell></row><row><cell cols="7">Method citations Correct Hallucinated Incorrect Correct Hallucinated Incorrect</cell></row><row><cell>k = 1</cell><cell>13.8%</cell><cell>54.5%</cell><cell>31.7%</cell><cell>1.8%</cell><cell>3.5%</cell><cell>94.7%</cell></row><row><cell>2 ? k &lt; 5</cell><cell>30.4%</cell><cell>38.6%</cell><cell>31.1%</cell><cell>9.3%</cell><cell>4.0%</cell><cell>86.7%</cell></row><row><cell>5 ? k &lt; 10</cell><cell>36.3%</cell><cell>29.5%</cell><cell>34.2%</cell><cell>17.9%</cell><cell>0.0%</cell><cell>82.1%</cell></row><row><cell>10 ? k &lt; 25</cell><cell>43.0%</cell><cell>15.8%</cell><cell>41.2%</cell><cell>38.8%</cell><cell>3.0%</cell><cell>58.2%</cell></row><row><cell>25 ? k &lt; 50</cell><cell>53.4%</cell><cell>8.7%</cell><cell>37.9%</cell><cell>43.7%</cell><cell>0.0%</cell><cell>56.3%</cell></row><row><cell>50 ? k &lt; 100</cell><cell>64.8%</cell><cell>9.9%</cell><cell>25.3%</cell><cell>60.6%</cell><cell>1.4%</cell><cell>38.0%</cell></row><row><cell>100 ? k &lt; 500</cell><cell>64.6%</cell><cell>8.3%</cell><cell>27.1%</cell><cell>63.5%</cell><cell>1.0%</cell><cell>35.4%</cell></row><row><cell>? 500</cell><cell>78.6%</cell><cell>0.0%</cell><cell>21.4%</cell><cell>78.6%</cell><cell>0.0%</cell><cell>21.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head>Table 26 : Citation Processing Ablation.</head><label>26</label><figDesc>We predict citations for the PWC Methods dataset using 6.7 billion size models. Papers are bucketed according to the number of citations (mentions) in the dataset. The title processing model has a higher accuracy, but greater risk of hallucination. There are 1,705 methods in this evaluation dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Citation Processing</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(a) Titles</cell><cell></cell><cell></cell><cell>(b) IDs</cell><cell></cell></row><row><cell cols="7">Dataset citations Correct Hallucinated Incorrect Correct Hallucinated Incorrect</cell></row><row><cell>k = 1</cell><cell>1.4%</cell><cell>62.5%</cell><cell>36.1%</cell><cell>0.5%</cell><cell>11.5%</cell><cell>88.1%</cell></row><row><cell>2 ? k &lt; 5</cell><cell>5.0%</cell><cell>59.2%</cell><cell>35.8%</cell><cell>0.6%</cell><cell>10.2%</cell><cell>89.2%</cell></row><row><cell>5 ? k &lt; 10</cell><cell>15.4%</cell><cell>49.7%</cell><cell>34.8%</cell><cell>2.6%</cell><cell>6.2%</cell><cell>91.1%</cell></row><row><cell>10 ? k &lt; 25</cell><cell>25.7%</cell><cell>36.8%</cell><cell>37.5%</cell><cell>8.3%</cell><cell>4.8%</cell><cell>86.9%</cell></row><row><cell>25 ? k &lt; 50</cell><cell>44.6%</cell><cell>27.4%</cell><cell>28.0%</cell><cell>22.9%</cell><cell>7.0%</cell><cell>70.0%</cell></row><row><cell>50 ? k &lt; 100</cell><cell>58.6%</cell><cell>17.7%</cell><cell>23.6%</cell><cell>41.4%</cell><cell>7.7%</cell><cell>50.9%</cell></row><row><cell>100 ? k &lt; 500</cell><cell>65.5%</cell><cell>6.7%</cell><cell>27.8%</cell><cell>62.4%</cell><cell>3.1%</cell><cell>34.5%</cell></row><row><cell>? 500</cell><cell>81.8%</cell><cell>6.1%</cell><cell>12.1%</cell><cell>81.8%</cell><cell>3.0%</cell><cell>15.2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34"><head>Table 27 :</head><label>27</label><figDesc>Citation Processing Ablation. We predict citations for the PWC Datasets dataset using 6.7 billion capacity models. There are 4,735 datasets in this evaluation dataset.</figDesc><table><row><cell></cell><cell cols="2">BIG-bench</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Benchmark</cell><cell cols="5">OPT 30B OPT 175B BLOOM 176B GAL 30B GAL 120B</cell></row><row><cell>Anachronisms</cell><cell>47.4%</cell><cell>49.1%</cell><cell>1.3%</cell><cell>47.0%</cell><cell>48.7%</cell></row><row><cell>Analogical Similarity</cell><cell>12.7%</cell><cell>19.8%</cell><cell>19.2%</cell><cell>17.0%</cell><cell>23.5%</cell></row><row><cell>Analytic Entailment</cell><cell>40.0%</cell><cell>52.9%</cell><cell>48.6%</cell><cell>47.1%</cell><cell>51.3%</cell></row><row><cell>Causal Judgment</cell><cell>53.7%</cell><cell>55.3%</cell><cell>54.7%</cell><cell>49.5%</cell><cell>51.1%</cell></row><row><cell>Crash Blossom</cell><cell>42.1%</cell><cell>36.8%</cell><cell>47.4%</cell><cell>42.1%</cell><cell>42.1%</cell></row><row><cell>Crass AI</cell><cell>20.5%</cell><cell>34.1%</cell><cell>31.8%</cell><cell>40.9%</cell><cell>52.3%</cell></row><row><cell>Dark Humor Detection</cell><cell>46.3%</cell><cell>48.8%</cell><cell>51.3%</cell><cell>48.8%</cell><cell>46.3%</cell></row><row><cell>Date Understanding</cell><cell>15.5%</cell><cell>21.1%</cell><cell>12.2%</cell><cell>11.4%</cell><cell>16.8%</cell></row><row><cell>Disambiguation QA</cell><cell>39.5%</cell><cell>44.6%</cell><cell>44.2%</cell><cell>46.9%</cell><cell>43.0%</cell></row><row><cell>Empirical Judgments</cell><cell>38.4%</cell><cell>52.5%</cell><cell>56.6%</cell><cell>50.5%</cell><cell>54.6%</cell></row><row><cell>English Proverbs</cell><cell>26.5%</cell><cell>20.6%</cell><cell>26.5%</cell><cell>26.5%</cell><cell>17.7%</cell></row><row><cell>Entailed Polarity</cell><cell>87.8%</cell><cell>88.5%</cell><cell>89.2%</cell><cell>89.2%</cell><cell>85.8%</cell></row><row><cell>Epistemic Reasoning</cell><cell>43.4%</cell><cell>43.5%</cell><cell>61.2%</cell><cell>40.1%</cell><cell>53.0%</cell></row><row><cell>Evaluating Information Essentiality</cell><cell>32.4%</cell><cell>19.1%</cell><cell>29.4%</cell><cell>25.0%</cell><cell>22.1%</cell></row><row><cell>Fantasy Reasoning</cell><cell>67.7%</cell><cell>69.2%</cell><cell>65.2%</cell><cell>66.7%</cell><cell>52.7%</cell></row><row><cell>Figure of Speech Detection</cell><cell>10.2%</cell><cell>13.6%</cell><cell>22.0%</cell><cell>13.6%</cell><cell>15.3%</cell></row><row><cell>General Knowledge</cell><cell>51.4%</cell><cell>78.6%</cell><cell>80.0%</cell><cell>68.6%</cell><cell>74.3%</cell></row><row><cell>GRE Reading Comprehension</cell><cell>6.5%</cell><cell>12.9%</cell><cell>22.6%</cell><cell>16.1%</cell><cell>35.5%</cell></row><row><cell>Hindu Knowledge</cell><cell>32.6%</cell><cell>42.3%</cell><cell>48.6%</cell><cell>36.6%</cell><cell>49.7%</cell></row><row><cell>Human Organs Senses</cell><cell>45.2%</cell><cell>57.1%</cell><cell>59.5%</cell><cell>71.4%</cell><cell>73.8%</cell></row><row><cell>Identify Odd Metaphor</cell><cell>27.7%</cell><cell>21.3%</cell><cell>19.2%</cell><cell>19.2%</cell><cell>27.7%</cell></row><row><cell>Implicatures</cell><cell>44.3%</cell><cell>49.6%</cell><cell>53.7%</cell><cell>59.4%</cell><cell>69.9%</cell></row><row><cell>Implicit Relations</cell><cell>22.4%</cell><cell>35.3%</cell><cell>28.2%</cell><cell>16.5%</cell><cell>25.9%</cell></row><row><cell>Intent Recognition</cell><cell>66.2%</cell><cell>79.2%</cell><cell>89.5%</cell><cell>87.8%</cell><cell>89.5%</cell></row><row><cell>Irony Identification</cell><cell>50.5%</cell><cell>49.5%</cell><cell>63.6%</cell><cell>60.6%</cell><cell>59.6%</cell></row><row><cell>Known Unknowns</cell><cell>50.0%</cell><cell>52.2%</cell><cell>50.0%</cell><cell>50.0%</cell><cell>41.3%</cell></row><row><cell>Logic Grid Puzzle</cell><cell>32.7%</cell><cell>31.6%</cell><cell>31.1%</cell><cell>35.8%</cell><cell>39.4%</cell></row><row><cell>Logical Args</cell><cell>18.8%</cell><cell>34.4%</cell><cell>25.0%</cell><cell>34.4%</cell><cell>43.8%</cell></row><row><cell>Logical Fallacy Detection</cell><cell>50.9%</cell><cell>54.9%</cell><cell>54.5%</cell><cell>54.1%</cell><cell>55.1%</cell></row><row><cell>Logical Sequence</cell><cell>38.5%</cell><cell>46.2%</cell><cell>30.8%</cell><cell>25.6%</cell><cell>43.6%</cell></row><row><cell>Mathematical Induction</cell><cell>60.9%</cell><cell>55.1%</cell><cell>52.2%</cell><cell>44.9%</cell><cell>58.0%</cell></row><row><cell>Metaphor Boolean</cell><cell>51.1%</cell><cell>57.5%</cell><cell>61.5%</cell><cell>63.4%</cell><cell>49.1%</cell></row><row><cell>Misconceptions</cell><cell>56.1%</cell><cell>57.5%</cell><cell>54.8%</cell><cell>51.6%</cell><cell>58.0%</cell></row><row><cell>Moral Permissibility</cell><cell>50.6%</cell><cell>54.4%</cell><cell>57.0%</cell><cell>52.3%</cell><cell>49.7%</cell></row><row><cell>Movie Recommendation</cell><cell>6.4%</cell><cell>52.6%</cell><cell>49.4%</cell><cell>31.6%</cell><cell>36.8%</cell></row><row><cell>Navigate</cell><cell>49.3%</cell><cell>49.8%</cell><cell>51.1%</cell><cell>50.9%</cell><cell>51.8%</cell></row><row><cell>Nonsense Words Grammar</cell><cell>28.0%</cell><cell>46.0%</cell><cell>48.0%</cell><cell>38.0%</cell><cell>48.0%</cell></row><row><cell>Novel Concepts</cell><cell>9.4%</cell><cell>12.5%</cell><cell>15.6%</cell><cell>6.3%</cell><cell>9.4%</cell></row><row><cell>Odd One Out</cell><cell>30.2%</cell><cell>26.7%</cell><cell>22.1%</cell><cell>12.8%</cell><cell>19.8%</cell></row><row><cell>Penguins in a Table</cell><cell>29.5%</cell><cell>32.9%</cell><cell>28.2%</cell><cell>40.9%</cell><cell>36.9%</cell></row><row><cell>Phrase Relatedness</cell><cell>45.0%</cell><cell>51.0%</cell><cell>55.0%</cell><cell>53.0%</cell><cell>64.0%</cell></row><row><cell>Physical Intuition</cell><cell>39.5%</cell><cell>42.0%</cell><cell>37.0%</cell><cell>55.6%</cell><cell>58.0%</cell></row><row><cell>Physics</cell><cell>39.3%</cell><cell>42.8%</cell><cell>54.2%</cell><cell>55.9%</cell><cell>65.5%</cell></row><row><cell>Presuppositions as NLI</cell><cell>36.6%</cell><cell>36.2%</cell><cell>39.6%</cell><cell>34.0%</cell><cell>28.0%</cell></row><row><cell>Question Selection</cell><cell>39.8%</cell><cell>42.1%</cell><cell>5.2%</cell><cell>41.1%</cell><cell>42.7%</cell></row><row><cell>Reasoning about Colored Objects</cell><cell>33.9%</cell><cell>38.7%</cell><cell>40.5%</cell><cell>45.8%</cell><cell>55.0%</cell></row><row><cell>Riddle Sense</cell><cell>40.8%</cell><cell>57.1%</cell><cell>44.9%</cell><cell>46.9%</cell><cell>42.9%</cell></row><row><cell>Ruin Names</cell><cell>19.4%</cell><cell>20.8%</cell><cell>12.5%</cell><cell>24.1%</cell><cell>33.0%</cell></row><row><cell>Sentence Ambiguity</cell><cell>63.3%</cell><cell>60.0%</cell><cell>65.0%</cell><cell>60.0%</cell><cell>66.7%</cell></row><row><cell>Similarities Abstraction</cell><cell>21.1%</cell><cell>22.4%</cell><cell>27.6%</cell><cell>21.1%</cell><cell>13.2%</cell></row><row><cell>Snarks</cell><cell>42.0%</cell><cell>41.4%</cell><cell>47.0%</cell><cell>48.1%</cell><cell>48.6%</cell></row><row><cell>Sports Understanding</cell><cell>50.0%</cell><cell>48.8%</cell><cell>54.5%</cell><cell>52.0%</cell><cell>51.8%</cell></row><row><cell>StrategyQA</cell><cell>56.1%</cell><cell>58.5%</cell><cell>57.1%</cell><cell>53.9%</cell><cell>53.7%</cell></row><row><cell>Temporal Sequences</cell><cell>31.4%</cell><cell>28.4%</cell><cell>20.5%</cell><cell>26.4%</cell><cell>21.2%</cell></row><row><cell>Timedial</cell><cell>15.3%</cell><cell>22.2%</cell><cell>24.4%</cell><cell>39.9%</cell><cell>40.8%</cell></row><row><cell>Understanding Fables</cell><cell>20.1%</cell><cell>19.6%</cell><cell>24.9%</cell><cell>28.0%</cell><cell>20.1%</cell></row><row><cell>Winowhy</cell><cell>37.2%</cell><cell>39.7%</cell><cell>38.0%</cell><cell>56.5%</cell><cell>56.4%</cell></row><row><cell>Average (weighted)</cell><cell>39.6%</cell><cell>43.4%</cell><cell>42.6%</cell><cell>46.6%</cell><cell>48.7%</cell></row><row><cell>Average (unweighted)</cell><cell>32.8%</cell><cell>42.7%</cell><cell>42.2%</cell><cell>42.7%</cell><cell>45.3%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_35"><head>Table 29 :</head><label>29</label><figDesc></figDesc><table /><note><p><p>BIG-bench Results. Galactica exceeds the performance of general models, even at lower scales.</p>A.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_36"><head>6 Prompt Pre-training Datasets</head><label></label><figDesc>We report the prompt datasets we included in pre-training below.</figDesc><table><row><cell>Data source</cell><cell cols="2">Split Prompts</cell><cell>Tokens</cell></row><row><cell>MedMCQA (Pal et al., 2022)</cell><cell>train</cell><cell cols="2">180,894 13,311,290</cell></row><row><cell>RACE (Xu et al., 2017)</cell><cell>train</cell><cell cols="2">29,502 12,160,390</cell></row><row><cell>Quoref (Dasigi et al., 2019)</cell><cell>train</cell><cell cols="2">19,206 10,361,335</cell></row><row><cell>ROPES (Lin et al., 2019)</cell><cell>train</cell><cell>10,815</cell><cell>2,672,195</cell></row><row><cell>BioASQ7 task b (Nentidis et al., 2021)</cell><cell>train</cell><cell>2,676</cell><cell>1,288,462</cell></row><row><cell>TQA (Kembhavi et al., 2017)</cell><cell>train</cell><cell>8,566</cell><cell>1,856,473</cell></row><row><cell>BoolQ (Clark et al., 2019)</cell><cell>train</cell><cell>9,333</cell><cell>1,224,335</cell></row><row><cell>SciQ (Welbl et al., 2017)</cell><cell>train</cell><cell>10,346</cell><cell>1,397,668</cell></row><row><cell>QASC (Khot et al., 2020)</cell><cell>train</cell><cell>8,053</cell><cell>930,414</cell></row><row><cell cols="2">CommonSenseQA (Talmor et al., 2018) train</cell><cell>9,644</cell><cell>660,750</cell></row><row><cell>OpenBookQA (Mihaylov et al., 2018)</cell><cell>train</cell><cell>4,908</cell><cell>324,995</cell></row><row><cell>QCScience (V et al., 2021)</cell><cell>train</cell><cell>2,417</cell><cell>209,803</cell></row><row><cell>PubMedQA (Jin et al., 2019)</cell><cell>train</cell><cell>495</cell><cell>186,304</cell></row><row><cell>QASPER (Dasigi et al., 2021)</cell><cell>train</cell><cell>606</cell><cell>105,985</cell></row><row><cell>UChallenge (new)</cell><cell>train</cell><cell>346</cell><cell>29,308</cell></row><row><cell>TrueOrFalse (new)</cell><cell>train</cell><cell>107</cell><cell>2,854</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_37"><head>Table 30 :</head><label>30</label><figDesc>Question answering prompts used in Naturebook</figDesc><table><row><cell>Data source</cell><cell cols="2">Split Prompts</cell><cell>Tokens</cell></row><row><cell>JNLPBA (Kim et al., 2004)</cell><cell>train</cell><cell cols="2">91,213 5,262,723</cell></row><row><cell cols="2">BC4CHEMD (Krallinger et al., 2004) train</cell><cell cols="2">30,234 1,756,929</cell></row><row><cell>ChemProt (Taboureau et al., 2011)</cell><cell>train</cell><cell cols="2">3,030 1,286,816</cell></row><row><cell>BC2GM (Smith et al., 2008)</cell><cell>train</cell><cell>12,375</cell><cell>704,357</cell></row><row><cell>S800 (Pafilis et al., 2013)</cell><cell>train</cell><cell>5,318</cell><cell>281,448</cell></row><row><cell>BC5CDR Chem (Li et al., 2016)</cell><cell>train</cell><cell>4,503</cell><cell>241,729</cell></row><row><cell>BC5CDR Disease (Li et al., 2016)</cell><cell>train</cell><cell>4,498</cell><cell>231,322</cell></row><row><cell>MethodNet (new)</cell><cell>train</cell><cell>659</cell><cell>167,904</cell></row><row><cell>Scientific Entities (new)</cell><cell>train</cell><cell>305</cell><cell>97,935</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_38"><head>Table 31 : Entity extraction prompts used in Naturebook</head><label>31</label><figDesc></figDesc><table><row><cell>Data source</cell><cell cols="2">Split Prompts</cell><cell>Tokens</cell></row><row><cell>PWC Desc (new)</cell><cell>train</cell><cell cols="2">3,586 9,663,419</cell></row><row><cell>SciTail (Khot et al., 2018)</cell><cell>train</cell><cell cols="2">23,361 1,383,614</cell></row><row><cell>Fragmented Glass (new)</cell><cell>train</cell><cell>718</cell><cell>867,985</cell></row><row><cell cols="2">SciTLDR (Cachola et al., 2020) train</cell><cell>1,973</cell><cell>472,169</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_39"><head>Table 32 :</head><label>32</label><figDesc>Summarization prompts used in Naturebook</figDesc><table><row><cell>Data source</cell><cell cols="2">Split Prompts</cell><cell>Tokens</cell></row><row><cell cols="2">Wizard of Wikipedia (Dinan et al., 2018) train</cell><cell cols="2">18,246 4,466,113</cell></row><row><cell>Advising (Gunasekara et al., 2019)</cell><cell>train</cell><cell>495</cell><cell>147,793</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_40"><head>Table 33 :</head><label>33</label><figDesc>Dialog prompts used in Naturebook</figDesc><table><row><cell>Data source</cell><cell cols="2">Split Prompts</cell><cell>Tokens</cell></row><row><cell cols="2">BACE Classification train</cell><cell>1,198</cell><cell>122,699</cell></row><row><cell>BACE Regression</cell><cell>train</cell><cell>1,198</cell><cell>154,656</cell></row><row><cell>BBBP</cell><cell>train</cell><cell>1,613</cell><cell>115,916</cell></row><row><cell>ClinTox</cell><cell>train</cell><cell>1,171</cell><cell>100,955</cell></row><row><cell>Delaney</cell><cell>train</cell><cell>893</cell><cell>62,083</cell></row><row><cell>FreeSolv</cell><cell>train</cell><cell>508</cell><cell>29,542</cell></row><row><cell>HIV</cell><cell>train</cell><cell>32,572</cell><cell>2,308,966</cell></row><row><cell>HOPV</cell><cell>train</cell><cell>2,217</cell><cell>333,620</cell></row><row><cell>Lipo</cell><cell>train</cell><cell>3,327</cell><cell>362,342</cell></row><row><cell>PCBA</cell><cell>train</cell><cell cols="2">714,277 553,645,656</cell></row><row><cell>QM7</cell><cell>train</cell><cell>5,416</cell><cell>320,199</cell></row><row><cell>QM8</cell><cell>train</cell><cell>275,569</cell><cell>27,163,516</cell></row><row><cell>QM9</cell><cell cols="3">train 1,259,090 128,427,073</cell></row><row><cell>SAMPL</cell><cell>train</cell><cell>508</cell><cell>1,259,090</cell></row><row><cell>SIDER</cell><cell>train</cell><cell>30,499</cell><cell>2,741,904</cell></row><row><cell>Thermosol</cell><cell>train</cell><cell>1,396</cell><cell>139,481</cell></row><row><cell>Tox21</cell><cell>train</cell><cell>73,883</cell><cell>54,224,093</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_41"><head>Table 34 : Chemical property prediction prompts used in Naturebook</head><label>34</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Docking Regression</cell><cell></cell></row><row><cell>Model</cell><cell cols="2">Param (bn) ESR2</cell><cell>F2</cell><cell cols="2">KIT PARP1</cell><cell>PGR</cell></row><row><cell>GAL 125M</cell><cell>0.1</cell><cell cols="3">-12.4 -6.09 -6.73</cell><cell>-1.69</cell><cell>-12.4</cell></row><row><cell>GAL 1.3B</cell><cell cols="4">1.3 -0.293 0.591 0.063</cell><cell>0.728</cell><cell>-1.72</cell></row><row><cell>GAL 6.7B</cell><cell cols="4">6.7 -0.216 0.694 0.290</cell><cell>0.681 -0.894</cell></row><row><cell>GAL 30B</cell><cell cols="4">30 -0.186 0.679 0.313</cell><cell>0.732 -0.468</cell></row><row><cell>GAL 120B</cell><cell cols="4">120 -0.564 0.626 0.249</cell><cell>0.732 -0.960</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_42"><head>Table 36 :</head><label>36</label><figDesc>DockSTRING Results. Metric shown is R 2 .</figDesc><table><row><cell>A.6.3 Rest of MMLU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">We report social sciences and results for other fields below:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Subject</cell><cell cols="6">OPT BLOOM Gopher Chinchilla GAL 30B GAL 120B</cell></row><row><cell>Anatomy</cell><cell>28.9%</cell><cell>37.0%</cell><cell>56.3%</cell><cell>70.4%</cell><cell>54.1%</cell><cell>58.5%</cell></row><row><cell>Business Ethics</cell><cell>31.0%</cell><cell>36.0%</cell><cell>70.0%</cell><cell>72.0%</cell><cell>42.0%</cell><cell>48.0%</cell></row><row><cell>Clinical Knowledge</cell><cell>21.9%</cell><cell>29.8%</cell><cell>67.2%</cell><cell>75.1%</cell><cell>57.7%</cell><cell>59.2%</cell></row><row><cell>Computer Security</cell><cell>32.0%</cell><cell>34.0%</cell><cell>65.0%</cell><cell>76.0%</cell><cell>65.0%</cell><cell>67.0%</cell></row><row><cell>Conceptual Physics</cell><cell>34.9%</cell><cell>36.6%</cell><cell>49.4%</cell><cell>67.2%</cell><cell>43.4%</cell><cell>50.6%</cell></row><row><cell>Global Facts</cell><cell>23.0%</cell><cell>32.0%</cell><cell>38.0%</cell><cell>39.0%</cell><cell>32.0%</cell><cell>35.0%</cell></row><row><cell cols="2">High School European History 6.7%</cell><cell>4.8%</cell><cell>72.1%</cell><cell>78.8%</cell><cell>60.6%</cell><cell>67.3%</cell></row><row><cell>High School Geography</cell><cell>26.3%</cell><cell>38.9%</cell><cell>76.8%</cell><cell>86.4%</cell><cell>58.1%</cell><cell>63.6%</cell></row><row><cell>High School Gov. &amp; Politics</cell><cell>32.6%</cell><cell>30.6%</cell><cell>83.9%</cell><cell>91.2%</cell><cell>58.5%</cell><cell>61.7%</cell></row><row><cell>High School Macroeconomics</cell><cell>36.2%</cell><cell>23.1%</cell><cell>65.1%</cell><cell>70.5%</cell><cell>40.5%</cell><cell>46.4%</cell></row><row><cell>High School Microeconomics</cell><cell>32.8%</cell><cell>27.3%</cell><cell>66.4%</cell><cell>77.7%</cell><cell>49.2%</cell><cell>55.9%</cell></row><row><cell>High School Psychology</cell><cell>25.5%</cell><cell>36.9%</cell><cell>81.8%</cell><cell>86.6%</cell><cell>68.8%</cell><cell>74.3%</cell></row><row><cell>High School US History</cell><cell>9.3%</cell><cell>11.8%</cell><cell>78.9%</cell><cell>83.3%</cell><cell>51.5%</cell><cell>58.3%</cell></row><row><cell>High School World History</cell><cell>30.0%</cell><cell>29.1%</cell><cell>75.1%</cell><cell>85.2%</cell><cell>63.7%</cell><cell>71.7%</cell></row><row><cell>Human Aging</cell><cell>35.0%</cell><cell>34.5%</cell><cell>66.4%</cell><cell>77.6%</cell><cell>55.2%</cell><cell>59.2%</cell></row><row><cell>Human Sexuality</cell><cell>26.0%</cell><cell>33.6%</cell><cell>67.2%</cell><cell>86.3%</cell><cell>56.5%</cell><cell>58.8%</cell></row><row><cell>International Law</cell><cell>33.1%</cell><cell>41.3%</cell><cell>77.7%</cell><cell>90.9%</cell><cell>64.4%</cell><cell>71.1%</cell></row><row><cell>Jurisprudence</cell><cell>0.0%</cell><cell>0.0%</cell><cell>71.3%</cell><cell>79.6%</cell><cell>47.2%</cell><cell>53.7%</cell></row><row><cell>Logical Fallacies</cell><cell>28.2%</cell><cell>28.2%</cell><cell>72.4%</cell><cell>80.4%</cell><cell>47.2%</cell><cell>59.5%</cell></row><row><cell>Management</cell><cell>25.2%</cell><cell>27.2%</cell><cell>77.7%</cell><cell>82.5%</cell><cell>60.2%</cell><cell>63.1%</cell></row><row><cell>Marketing</cell><cell>32.5%</cell><cell>41.0%</cell><cell>83.3%</cell><cell>89.7%</cell><cell>70.5%</cell><cell>76.5%</cell></row><row><cell>Miscellaneous</cell><cell>31.5%</cell><cell>37.7%</cell><cell>75.7%</cell><cell>84.5%</cell><cell>54.0%</cell><cell>63.9%</cell></row><row><cell>Moral Disputes</cell><cell>28.2%</cell><cell>32.7%</cell><cell>66.8%</cell><cell>77.5%</cell><cell>50.3%</cell><cell>56.6%</cell></row><row><cell>Moral Scenarios</cell><cell>25.4%</cell><cell>24.4%</cell><cell>40.2%</cell><cell>36.5%</cell><cell>24.1%</cell><cell>24.2%</cell></row><row><cell>Nutrition</cell><cell>30.4%</cell><cell>32.4%</cell><cell>69.9%</cell><cell>77.1%</cell><cell>63.1%</cell><cell>67.3%</cell></row><row><cell>Philosophy</cell><cell>29.9%</cell><cell>31.5%</cell><cell>68.8%</cell><cell>79.4%</cell><cell>52.4%</cell><cell>54.7%</cell></row><row><cell>Prehistory</cell><cell>36.7%</cell><cell>36.1%</cell><cell>67.6%</cell><cell>81.2%</cell><cell>52.2%</cell><cell>59.6%</cell></row><row><cell>Professional Accounting</cell><cell>29.8%</cell><cell>28.7%</cell><cell>44.3%</cell><cell>52.1%</cell><cell>31.2%</cell><cell>40.0%</cell></row><row><cell>Professional Law</cell><cell>30.3%</cell><cell>25.5%</cell><cell>44.5%</cell><cell>56.5%</cell><cell>34.6%</cell><cell>36.0%</cell></row><row><cell>Professional Medicine</cell><cell>27.9%</cell><cell>25.4%</cell><cell>64.0%</cell><cell>75.4%</cell><cell>52.2%</cell><cell>59.6%</cell></row><row><cell>Professional Psychology</cell><cell>32.7%</cell><cell>33.3%</cell><cell>68.1%</cell><cell>75.7%</cell><cell>50.5%</cell><cell>56.5%</cell></row><row><cell>Public Relations</cell><cell>34.5%</cell><cell>30.0%</cell><cell>71.8%</cell><cell>73.6%</cell><cell>44.5%</cell><cell>53.6%</cell></row><row><cell>Security Studies</cell><cell>35.1%</cell><cell>29.8%</cell><cell>64.9%</cell><cell>75.9%</cell><cell>46.5%</cell><cell>57.1%</cell></row><row><cell>Sociology</cell><cell>26.4%</cell><cell>29.9%</cell><cell>84.1%</cell><cell>91.0%</cell><cell>65.7%</cell><cell>72.6%</cell></row><row><cell>US Foreign Policy</cell><cell>44.0%</cell><cell>37.0%</cell><cell>81.0%</cell><cell>92.0%</cell><cell>64.0%</cell><cell>75.0%</cell></row><row><cell>Virology</cell><cell>30.7%</cell><cell>28.3%</cell><cell>47.0%</cell><cell>53.6%</cell><cell>44.6%</cell><cell>48.2%</cell></row><row><cell>World Religion</cell><cell>43.9%</cell><cell>41.5%</cell><cell>84.2%</cell><cell>87.7%</cell><cell>44.4%</cell><cell>64.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_43"><head>Table 37 :</head><label>37</label><figDesc>Rest of MMLU.The corpus delta effects are more evidence with non-STEM subjects in particular, where Galactica lags the performance of Chinchilla and Gopher.</figDesc><table><row><cell></cell><cell cols="4">score_before score_after count_before count_after</cell></row><row><cell>abstract_algebra</cell><cell>33.0%</cell><cell>32.32%</cell><cell>100</cell><cell>99</cell></row><row><cell>anatomy</cell><cell>58.52%</cell><cell>58.95%</cell><cell>135</cell><cell>134</cell></row><row><cell>astronomy</cell><cell>65.13%</cell><cell>64.67%</cell><cell>152</cell><cell>150</cell></row><row><cell>business_ethics</cell><cell>48.0%</cell><cell>48.0%</cell><cell>100</cell><cell>100</cell></row><row><cell>clinical_knowledge</cell><cell>59.24%</cell><cell>59.24%</cell><cell>265</cell><cell>265</cell></row><row><cell>college_biology</cell><cell>68.75%</cell><cell>69.23%</cell><cell>144</cell><cell>143</cell></row><row><cell>college_chemistry</cell><cell>46.0%</cell><cell>46.46%</cell><cell>100</cell><cell>99</cell></row><row><cell>college_computer_science</cell><cell>49.0%</cell><cell>48.98%</cell><cell>100</cell><cell>98</cell></row><row><cell>college_mathematics</cell><cell>43.0%</cell><cell>45.26%</cell><cell>100</cell><cell>95</cell></row><row><cell>college_medicine</cell><cell>57.23%</cell><cell>57.74%</cell><cell>173</cell><cell>168</cell></row><row><cell>college_physics</cell><cell>42.16%</cell><cell>42.27%</cell><cell>102</cell><cell>97</cell></row><row><cell>computer_security</cell><cell>67.0%</cell><cell>67.35%</cell><cell>100</cell><cell>98</cell></row><row><cell>conceptual_physics</cell><cell>50.64%</cell><cell>50.85%</cell><cell>235</cell><cell>234</cell></row><row><cell>econometrics</cell><cell>42.11%</cell><cell>42.11%</cell><cell>114</cell><cell>114</cell></row><row><cell>electrical_engineering</cell><cell>62.76%</cell><cell>62.76%</cell><cell>145</cell><cell>145</cell></row><row><cell>elementary_mathematics</cell><cell>38.10%</cell><cell>38.10%</cell><cell>378</cell><cell>378</cell></row><row><cell>formal_logic</cell><cell>32.54%</cell><cell>32.54%</cell><cell>126</cell><cell>126</cell></row><row><cell>global_facts</cell><cell>35.0%</cell><cell>35.05%</cell><cell>100</cell><cell>97</cell></row><row><cell>high_school_biology</cell><cell>69.35%</cell><cell>69.61%</cell><cell>310</cell><cell>306</cell></row><row><cell>high_school_chemistry</cell><cell>47.78%</cell><cell>47.78%</cell><cell>203</cell><cell>203</cell></row><row><cell>high_school_computer_science</cell><cell>70.0%</cell><cell>70.0%</cell><cell>100</cell><cell>100</cell></row><row><cell>high_school_european_history</cell><cell>67.27%</cell><cell>66.17%</cell><cell>165</cell><cell>133</cell></row><row><cell>high_school_geography</cell><cell>63.63%</cell><cell>63.63%</cell><cell>198</cell><cell>198</cell></row><row><cell>high_school_government_and_politics</cell><cell>61.66%</cell><cell>61.46%</cell><cell>193</cell><cell>192</cell></row><row><cell>high_school_macroeconomics</cell><cell>46.41%</cell><cell>46.53%</cell><cell>390</cell><cell>389</cell></row><row><cell>high_school_mathematics</cell><cell>32.59%</cell><cell>32.58%</cell><cell>270</cell><cell>267</cell></row><row><cell>high_school_microeconomics</cell><cell>55.88%</cell><cell>55.88%</cell><cell>238</cell><cell>238</cell></row><row><cell>high_school_physics</cell><cell>33.77%</cell><cell>33.77%</cell><cell>151</cell><cell>151</cell></row><row><cell>high_school_psychology</cell><cell>74.31%</cell><cell>74.26%</cell><cell>545</cell><cell>544</cell></row><row><cell>high_school_statistics</cell><cell>41.20%</cell><cell>41.20%</cell><cell>216</cell><cell>216</cell></row><row><cell>high_school_us_history</cell><cell>58.33%</cell><cell>58.59%</cell><cell>204</cell><cell>99</cell></row><row><cell>high_school_world_history</cell><cell>71.73%</cell><cell>72.04%</cell><cell>237</cell><cell>186</cell></row><row><cell>human_aging</cell><cell>59.19%</cell><cell>59.19%</cell><cell>223</cell><cell>223</cell></row><row><cell>human_sexuality</cell><cell>58.78%</cell><cell>58.78%</cell><cell>131</cell><cell>131</cell></row><row><cell>international_law</cell><cell>71.07%</cell><cell>71.07%</cell><cell>121</cell><cell>121</cell></row><row><cell>jurisprudence</cell><cell>53.70%</cell><cell>53.70%</cell><cell>108</cell><cell>108</cell></row><row><cell>logical_fallacies</cell><cell>59.51%</cell><cell>59.26%</cell><cell>163</cell><cell>162</cell></row><row><cell>machine_learning</cell><cell>38.39%</cell><cell>36.54%</cell><cell>112</cell><cell>104</cell></row><row><cell>management</cell><cell>63.11%</cell><cell>63.11%</cell><cell>103</cell><cell>103</cell></row><row><cell>marketing</cell><cell>76.50%</cell><cell>76.50%</cell><cell>234</cell><cell>234</cell></row><row><cell>medical_genetics</cell><cell>68.0%</cell><cell>67.68%</cell><cell>100</cell><cell>99</cell></row><row><cell>miscellaneous</cell><cell>63.86%</cell><cell>63.81%</cell><cell>783</cell><cell>782</cell></row><row><cell>moral_disputes</cell><cell>56.65%</cell><cell>56.52%</cell><cell>346</cell><cell>345</cell></row><row><cell>moral_scenarios</cell><cell>24.24%</cell><cell>24.24%</cell><cell>895</cell><cell>895</cell></row><row><cell>nutrition</cell><cell>67.32%</cell><cell>67.32%</cell><cell>306</cell><cell>306</cell></row><row><cell>philosophy</cell><cell>54.66%</cell><cell>54.52%</cell><cell>311</cell><cell>310</cell></row><row><cell>prehistory</cell><cell>59.57%</cell><cell>59.57%</cell><cell>324</cell><cell>324</cell></row><row><cell>professional_accounting</cell><cell>40.07%</cell><cell>39.86%</cell><cell>282</cell><cell>281</cell></row><row><cell>professional_law</cell><cell>36.05%</cell><cell>36.05%</cell><cell>1534</cell><cell>1534</cell></row><row><cell>professional_medicine</cell><cell>59.56%</cell><cell>59.63%</cell><cell>272</cell><cell>270</cell></row><row><cell>professional_psychology</cell><cell>56.54%</cell><cell>56.54%</cell><cell>612</cell><cell>612</cell></row><row><cell>public_relations</cell><cell>53.63%</cell><cell>53.63%</cell><cell>110</cell><cell>110</cell></row><row><cell>security_studies</cell><cell>57.14%</cell><cell>57.14%</cell><cell>245</cell><cell>245</cell></row><row><cell>sociology</cell><cell>72.64%</cell><cell>73.0%</cell><cell>201</cell><cell>200</cell></row><row><cell>us_foreign_policy</cell><cell>75.0%</cell><cell>75.76%</cell><cell>100</cell><cell>99</cell></row><row><cell>virology</cell><cell>48.19%</cell><cell>48.48%</cell><cell>166</cell><cell>165</cell></row><row><cell>world_religions</cell><cell>64.91%</cell><cell>64.91%</cell><cell>171</cell><cell>171</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_44"><head>m sorry Frank, I think you missed it</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_45"><head>Table 38 :</head><label>38</label><figDesc>Checkmate-in-one Results. Metric shown is Accuracy.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>One of the larger corpora S2ORC has &lt; 20bn tokens, whereas corpora for GPT-3 and PaLM have ?</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>300bn tokens. ScholarBERT has a very large corpus at &gt;200bn tokens, but the model is small at 770M capacity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/facebookresearch/metaseq/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://paperswithcode.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://github.com/conversationai/perspectiveapi</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://web.expasy.org/translate/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://python-chess.readthedocs.io/en/latest/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/checkmate_in_one</p></note>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Dataset Components</head><p>We cover the various components of the corpus in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.1 Papers</head><p>We source scientific papers from preprint servers such as arXiv, PMC and other sources; see Table <ref type="table">22</ref>.</p><p>We also use the Semantic Scholar full text dataset (S2) to capture the long tail of science <ref type="bibr">(Lo et al., 2019a)</ref>. We apply several quality filters, including excluding papers from journals with certain keywords, and also excluding papers with a low journal impact factor. Details of the filters we used are contained in the Appendix.</p><p>We source abstracts where full texts are not open access. In total the full dataset contains 48 million papers, abstract and full-text, up to July 2022. We use a modified version of the GROBID library for converting PDFs to text, as well as obtaining titles, authors and citations <ref type="bibr">(GROBID, 2008</ref><ref type="bibr">(GROBID, -2022))</ref>. Where mathematical LaTeX is available, for example in arXiv, we make sure to combine the GROBID results with LaTeX source to recover mathematical content.</p><p>The final paper documents are stored in a markdown format, as opposed to full LaTeX. We use markdown as the standard format for all documents in the corpus to support knowledge blending between sources. Papers are citation processed, following the title-based approach of Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 Reference Material</head><p>We source encyclopedias, textbooks and educational material to create a base of reference material that the model can learn from. The details are covered in Table <ref type="table">23</ref>.</p><p>We apply source specific processing for several of the datasets, specifically:</p><p>? For StackExchange, we take questions from scientific sites; see the Appendix for the subset used.</p><p>? For Papers with Code and IUPAC Goldbook we apply data augmentation in the form of prompt randomization. Sometimes we pose sections as questions/answers; for example a section explaining a machine learning method is sometimes posed as "Question: What is [method]?". ? For KhanAcademy articles, we add &lt;work&gt; tokens for step-by-step reasoning examples, which we explain shortly in Section 2.4.</p><p>We make an effort to preserve mathematical LaTeX and capture citations, including hyperlinks to papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.3 Knowledge Bases</head><p>We source fine-grained knowledge from scientific knowledge bases. The details are covered in Table <ref type="table">24</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 120B Validation Loss Per Source</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6.1 Chemical Property Prediction</head><p>We set up a prediction task for chemical and physical properties with our validation set of 17,052 compounds. We use the PubChem document structure to design a prompt. We show an example for XLogP in Figure <ref type="figure">24</ref>.</p><p>Canonical SMILES </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6.2 Docking Regression</head><p>We looked briefly at the docking score regression task <ref type="bibr" target="#b24">(Garc?a-Orteg?n et al., 2022)</ref>. Here the task is to predict a docking score based on an target and a ligand. In the case of Galactica, we use a text format to represent this information. An example is shown in Figure <ref type="figure">25</ref>. We report results in Table <ref type="table">36</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.[END_I_SMILES]</head><p>Question: What will be the docking score of this compound against the protein?</p><p>Answer: -8.8</p><p>Figure <ref type="figure">25</ref>: DockSTRING Format. To construct the training set, we take the protein target and ligand sequences, pose a natural language question, and have the docking score as the answer.</p><p>For three of the targets, Galactica is able to infer from looking at the sequences alone, and performance scales from 1.3B parameters onwards. However, Galactica does not solve the two harder targets ESR2 and PGR. This hints at a limitation with the text representation, and may point to more geometrical information being needed to solve the task with reasonable data-efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Further Training Dataset Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7.1 FragmentedGlass</head><p>We compile a list of scientific entities, retrieve fragments for each one, and write a description of the entity based on the retrieved fragments. This can be considered a summarization task. We also write ground-truth descriptions without any retrieved fragments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7.2 MethodNet</head><p>We compile machine learning abstracts and predict the new method that was introduced in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7.3 PWC Desc</head><p>For a list of dataset and methods in machine learning, we retrieve fragments for each one from the introducing paper, and write a summary description based on the retrieved fragments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7.4 Ribosome</head><p>We use Expasy 6 to create a paired translation set between nucleotide sequences from the protein coding part of the human genome and protein sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7.5 S2</head><p>Papers from certain fields are ignored due to quality concerns: psychology, business, art, economics, geography, history, political science, philosophy and sociology. Papers from journals with words like "law", "history", "politics", "business", "religion" were also ignored. For S2, we also exclude papers from low impact journals. The approximate impact factor of each journal in the S2 dataset was computed, by counting the number of papers in that journal and the number of citations that these papers received. If the approximate impact factor &lt; 1, the papers from that journal are ignored. Non-English papers are ignored. Some of these constraints can likely be relaxed in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7.6 ScientificEntities</head><p>For a random sample of academic paper abstracts, we predict the scientific entities that were mentioned in the abstract.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7.7 StackExchange</head><p>We include question and answers from the following sources: academic, ai, arduino, astronomy, aviation, bioinformatics, biology, chemistry, chess, cogsci, computergraphics, cs, cseducators, cstheory, datascience, dsp, earthscience, economics, electronics, engineering, hardwarerecs, health, hsm, math, matheducators, mathematica, mathoverflow, /mechanics, networkengineering, or, physics, puzzling, quant, quantumcomputing, retrocomputing, reverseengineering, robotics, scicomp, softwareengineering, softwarerecs, sound, space, stats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7.8 TrueOrFalse</head><p>We include 107 True or False questions to improve zero-shot performance for this type of question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7.9 UChallenge</head><p>We include 346 free-form question and answers of university-level questions about science; this is a form of closed-book QA (and not multiple-choice). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8.4 Deduplication Results</head><p>One of our concerns from reading the literature was the lack of data leakage analysis for results on MMLU, given the massive corpuses being used. Following from previous work of Brown et al. ( <ref type="formula">2020</ref>), we search for n-gram matches between the training and test set. We chose to remove any 13-gram matches from the test set that appear in the training set and we report the scores before and after removal of these clashing examples.</p><p>Results are shown overleaf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8.5 Example Wikipedia Article Written by Galactica</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Head Attention</head><p>Multi-Head Attention is an attention mechanism used in the Transformer architecture for neural networks <ref type="bibr" target="#b115">(Vaswani et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>In natural language processing, an attention mechanism is a model that maps a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key <ref type="bibr" target="#b115">(Vaswani et al., 2017)</ref>.</p><p>The scaled dot-product attention is an attention mechanism used in the Transformer architecture. It is defined as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key <ref type="bibr" target="#b115">(Vaswani et al., 2017)</ref>.</p><p>The multi-head attention is an extension of the scaled dot-product attention. The multi-head attention mechanism is defined as several scaled dot-product attention stacked together, where the input is linearly projected h times with different, learned linear projections to d k , d k , and dv dimensions, respectively <ref type="bibr" target="#b115">(Vaswani et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formal definition</head><p>Let the query matrix be Q, the key matrix be K, and the value matrix be V . The matrices are of shape d k ? dq, d k ? d k , and dv ? d k respectively. The multi-head attention mechanism is defined as:</p><p>where W Q i , W K i , and W V i are the learned linear projections to the d k , d k , and dv dimensions, respectively. W O is the linear projection from dvh to dv dimensions <ref type="bibr" target="#b115">(Vaswani et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Applications</head><p>Multi-head attention is widely used in natural language processing and other domains. For example, in the BERT model, multi-head attention is used to learn the context-aware representation of the input sequence <ref type="bibr" target="#b116">(Devlin et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8.6 Example Literature Survey Written by Galactica</head><p>Self-Supervised Learning, A Survey Abstract: In this paper we review self-supervised learning, a method of learning features from unlabelled data. We discuss the historical motivation behind self-supervised learning and review some of the current state of the art methods. We also look at how self-supervised learning has been used to solve a wide range of problems, including image classification, object detection, video analysis and robotics. Finally we discuss some of the challenges and future research directions for self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep learning has been very successful at solving many problems in machine learning, however these methods are reliant on large amounts of labelled data. In many real world applications, it is very expensive or impossible to obtain large amounts of labelled data. Self-supervised learning has emerged as a way of overcoming this issue.</p><p>Self-supervised learning is the process of learning a task that can be automatically generated from unlabelled data. In this way, the data is automatically labelled by exploiting some form of structure in the data. Once the self-supervised task has been solved, the features learned by the network are used to solve the original task.</p><p>In this paper we will discuss the historical motivation behind self-supervised learning and review some of the current state of the art methods. We will look at how self-supervised learning has been used to solve a wide range of problems, including image classification, object detection, video analysis and robotics. Finally we will discuss some of the challenges and future research directions for self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Historical Motivation</head><p>The first wave of deep learning was dominated by supervised learning methods. These methods require large amounts of labelled data, which is very expensive to obtain in many domains. To overcome this issue, unsupervised learning methods were introduced. These methods don't require labelled data, and so can be applied to a wide range of domains. The downside of these methods is that the learned features are not as good as those learned by supervised learning methods.</p><p>To bridge the gap between supervised and unsupervised learning, self-supervised learning was introduced. The idea is to learn a supervised task from unlabelled data. Once this task has been learned, the features learned can be transferred to other tasks. The hope is features learned by self-supervised learning will be better than those learned by unsupervised learning, and that self-supervised learning can be applied to a wide range of domains.</p><p>The first method of self-supervised learning was autoencoders <ref type="bibr">(Hinton et al., 1993)</ref>. Autoencoders are a type of neural network that learn to reconstruct the input. This can be thought of as learning a self-supervised task of predicting the input from the input. The features learned by autoencoders have been shown to be useful for solving other tasks <ref type="bibr">(Hinton et al., 2006)</ref>.</p><p>Another method of self-supervised learning is multi-task learning <ref type="bibr">(Caruana, 2017)</ref>. In multi-task learning, multiple tasks are learned simultaneously. The idea is that by learning multiple tasks, the network will learn better features. In this way, multi-task learning can be thought of as a form of self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Methods (cont)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pretext Tasks</head><p>The first method of self-supervised learning that we will discuss is the use of pretext tasks. In this method, a supervised task is learned from unlabelled data. This task is called a pretext task, and the labels for this task are automatically generated from the data. Once the pretext task has been learned, the features learned by the network are used to solve the original task. (cont)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8.7 Example Lecture Notes Written by Galactica</head><p>Lecture 1: Hartree-Fock Theory</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In this lecture, we will consider the simplest quantum mechanical method for approximating the ground state energy of a many-electron system. The Hartree-Fock method is also known as the self-consistent field method, and is an approximation to the exact many-electron wavefunction known as the Slater determinant. The Slater determinant is an anti-symmetric product of one-electron functions (orbitals), and is the simplest wavefunction that satisfies the Pauli exclusion principle. The Hartree-Fock method is an iterative method for finding the best possible set of orbitals that minimizes the energy of the Slater determinant.</p><p>In the Hartree-Fock method, the electron-electron repulsion energy is approximated as a classical Coulomb interaction between two charge distributions, one for each electron. This approximation neglects the correlation between the electrons, which is an important part of the total electron-electron repulsion energy. The resulting method is a mean-field theory in which each electron moves in an average field due to the other electrons. The resulting equations are known as the Hartree-Fock equations, and can be solved self-consistently to obtain the best possible orbitals. The resulting ground state energy is known as the Hartree-Fock energy, and the Hartree-Fock wavefunction is a single Slater determinant built from these orbitals.</p><p>The Hartree-Fock method is also known as the Self-Consistent Field (SCF) method, because the equations for the orbitals are solved self-consistently. The Hartree-Fock equations can be derived variationally by minimizing the energy of the Slater determinant with respect to the orbitals. The resulting orbitals are known as the canonical Hartree-Fock orbitals, and are not necessarily localized in space. The Hartree-Fock energy is invariant to unitary transformations of the canonical orbitals, and therefore there are an infinite number of orbitals that yield the same Hartree-Fock energy. These orbitals are known as non-canonical orbitals, and can be localized in space by appropriate unitary transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-Electron Approximation</head><p>In this section, we will review the basics of quantum mechanics for a single particle. This is useful for understanding the single-electron approximation used in Hartree-Fock theory.</p><p>The time-independent Schr?dinger equation for a particle in a potential V (r) is given by:</p><p>where the Hamiltonian is</p><p>The time-independent Schr?dinger equation is an eigenvalue equation for the Hamiltonian operator, where the eigenvalues are the allowed energies of the system. The Hamiltonian is a sum of two operators, one corresponding to the kinetic energy of the particle, and the other corresponding to the potential energy. The potential energy operator acts on the wavefunction by multiplying by the potential V (r). The kinetic energy operator is the Laplacian operator ? 2 , which is the divergence of the gradient of the wavefunction. The Laplacian operator is a second derivative with respect to the position of the particle.</p><p>(cont)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Flamingo: a visual language model for few-shot learning</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yana</forename><surname>Hasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Samangooei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianne</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahand</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikolaj</forename><surname>Binkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Barreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2204.14198" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Basic local alignment search tool</title>
		<author>
			<persName><forename type="first">W</forename><surname>S F Altschul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E W</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D J</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mol. Biol</title>
		<imprint>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="403" to="410" />
			<date type="published" when="1990-10">October 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ext5: Towards extreme multi-task scaling for transfer learning</title>
		<author>
			<persName><forename type="first">Vamsi</forename><surname>Aribandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaixiu</forename><surname>Steven Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanket</forename><surname>Vaibhav Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglei</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jai</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><surname>Metzler</surname></persName>
		</author>
		<ptr target="https://arxiv.org/stats/monthly_submissions" />
	</analytic>
	<monogr>
		<title level="m">Monthly Submissions</title>
		<imprint>
			<date type="published" when="2021">2021. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pondernet: Learning to ponder</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Banino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Balaguer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<idno>CoRR, abs/2107.05407</idno>
		<ptr target="https://arxiv.org/abs/2107.05407" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Scibert: Pretrained contextualized embeddings for scientific text</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<idno>CoRR, abs/1903.10676</idno>
		<ptr target="http://arxiv.org/abs/1903.10676" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Pieler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivanshu</forename><surname>Usvsn Sai Prashanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laria</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Tow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Weinbach</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2204.06745" />
		<title level="m">Gpt-neox-20b: An open-source autoregressive language model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language (technology) is power: A critical survey of &quot;bias</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Solon</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Daum?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<idno>CoRR, abs/2005.14050</idno>
		<ptr target="https://arxiv.org/abs/2005.14050" />
	</analytic>
	<monogr>
		<title level="j">NLP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving language models by retrieving from trillions of tokens</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saffron</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loren</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albin</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michela</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><surname>Sifre</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2112.04426" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Growth rates of modern science: A bibliometric analysis</title>
		<author>
			<persName><forename type="first">Lutz</forename><surname>Bornmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?diger</forename><surname>Mutz</surname></persName>
		</author>
		<idno>CoRR, abs/1402</idno>
		<ptr target="http://arxiv.org/abs/1402.4578" />
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">4578</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Frank-wolfe bayesian quadrature: Probabilistic integration with theoretical guarantees</title>
		<author>
			<persName><forename type="first">Fran?ois-Xavier</forename><surname>Briol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Oates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Girolami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Osborne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
		<idno>CoRR, abs/2005.14165</idno>
		<ptr target="https://arxiv.org/abs/2005.14165" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">As We May Think</title>
		<author>
			<persName><forename type="first">Vannevar</forename><surname>Bush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Atlantic Monthly</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="page" from="101" to="108" />
			<date type="published" when="1945-07">July 1945. 1945</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">TLDR: extreme summarization of scientific documents</title>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Cachola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno>CoRR, abs/2004.15011</idno>
		<ptr target="https://arxiv.org/abs/2004.15011" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Schuh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasha</forename><surname>Tsvyashchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parker</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Vinodkumar Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiner</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henryk</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Michalewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedant</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyeontaek</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Spiridonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivani</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanumalayan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Sankaranarayana Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brennan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Saeta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><surname>Eck</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2204.02311" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<pubPlace>Jeff Dean, Slav Petrov</pubPlace>
		</imprint>
	</monogr>
	<note>and Noah Fiedel. Palm: Scaling language modeling with pathways</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Scaling instruction-finetuned language models</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2210.11416" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Boolq: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.10044" />
		<imprint>
			<date type="published" when="1905">CoRR, abs/1905.10044. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Training verifiers to solve math word problems</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Cobbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reiichiro</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno>CoRR, abs/2110.14168</idno>
		<ptr target="https://arxiv.org/abs/2110.14168" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Quoref: A reading comprehension dataset with questions requiring coreferential reasoning</title>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A dataset of information-seeking questions and answers anchored in research papers</title>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">On measuring and mitigating biased inferences of word embeddings</title>
		<author>
			<persName><forename type="first">Sunipa</forename><surname>Dev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">M</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
		<idno>CoRR, abs/1908.09369</idno>
		<ptr target="http://arxiv.org/abs/1908.09369" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1811.01241" />
		<title level="m">Wizard of wikipedia: Knowledge-powered conversational agents</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Nomenclature of organic chemistry: Iupac recommendations and preferred names</title>
		<author>
			<persName><forename type="first">Henri</forename><forename type="middle">A</forename><surname>Favre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warren</forename><forename type="middle">H</forename><surname>Powerll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Galilei</surname></persName>
		</author>
		<author>
			<persName><surname>Assayer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1623">1623</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Tejasvi Chaganty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ni</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongrae</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da-Cheng</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><surname>Guu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2210.08726" />
		<title level="m">Attributed text generation via post-hoc research and revision</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dockstring: Easy molecular docking yields better benchmarks for ligand design</title>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Garc?a-Orteg?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><forename type="middle">J</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos?</forename><surname>Tripp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Miguel Hern?ndez-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><surname>Bacallado</surname></persName>
		</author>
		<idno type="DOI">10.1021/acs.jcim.1c01334</idno>
		<idno type="PMID">35849793</idno>
		<ptr target="https://doi.org/10.1021/acs.jcim.1c01334" />
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="3486" to="3502" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Realtoxicityprompts: Evaluating neural toxic degeneration in language models</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Samuel Gehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<idno>ArXiv, abs/2009.11462</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">GenBank and WGS Statistics</title>
		<author>
			<persName><surname>Genbank</surname></persName>
		</author>
		<ptr target="https://www.ncbi.nlm.nih.gov/genbank/statistics" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adaptive computation time for recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<ptr target="https://github.com/kermitt2/grobid" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2008" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Domain-specific language model pretraining for biomedical natural language processing</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<idno>CoRR, abs/2007.15779</idno>
		<ptr target="https://arxiv.org/abs/2007.15779" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DSTC7 task 1: Noetic end-to-end response selection</title>
		<author>
			<persName><forename type="first">Chulaka</forename><surname>Gunasekara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lazaros</forename><surname>Polymenakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Lasecki</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4107</idno>
		<ptr target="https://aclanthology.org/W19-4107" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on NLP for Conversational AI</title>
		<meeting>the First Workshop on NLP for Conversational AI<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08">August 2019</date>
			<biblScope unit="page" from="60" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1606.08415" />
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Measuring massive multitask language understanding</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2009.03300" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Measuring mathematical problem solving with the MATH dataset</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Collin</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akul</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno>CoRR, abs/2103.03874</idno>
		<ptr target="https://arxiv.org/abs/2103.03874" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Scaling laws and interpretability of learning from repeated data</title>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheer</forename><surname>El-Showk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Hume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2205.10487" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Profit from the Learning Curve</title>
		<author>
			<persName><forename type="first">Winfred</forename><forename type="middle">B</forename><surname>Hirschmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964-01">January 1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Sifre</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2203.15556" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery</title>
		<author>
			<persName><forename type="first">Shion</forename><surname>Honda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoi</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroki</forename><forename type="middle">R</forename><surname>Ueda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aswathy</forename><surname>Ajith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Pauloski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eamon</forename><surname>Duede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Malamud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Magoulas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Chard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><surname>Scholarbert</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2205.11342" />
		<title level="m">Bigger is not always better</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Chemformer: A pre-trained transformer for computational chemistry</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spyridon</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazhen</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esben</forename><surname>Bjerrum</surname></persName>
		</author>
		<idno type="DOI">10.26434/chemrxiv-2021-v2pnn</idno>
	</analytic>
	<monogr>
		<title level="j">ChemRxiv</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Towards unsupervised dense information retrieval with contrastive learning</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno>CoRR, abs/2112.09118</idno>
		<ptr target="https://arxiv.org/abs/2112.09118" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Few-shot learning with retrieval augmented language models</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lomeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Dwivedi-Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Introduction to Expert Systems</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Jackson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
		</imprint>
	</monogr>
	<note>USA, 2nd edition</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Pubmedqa: A dataset for biomedical research question answering</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghua</forename><surname>Lu</surname></persName>
		</author>
		<idno>CoRR, abs/1909.06146</idno>
		<ptr target="http://arxiv.org/abs/1909.06146" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with GPUs</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="535" to="547" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction with AlphaFold</title>
		<author>
			<persName><forename type="first">John</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustin</forename><surname>??dek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bridgland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A A</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardino</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishub</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stig</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Reiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Clancy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Zielinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michalina</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Pacholska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Berghammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bodenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41586-021-03819-2</idno>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">596</biblScope>
			<biblScope unit="issue">7873</biblScope>
			<biblScope unit="page" from="583" to="589" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno>CoRR, abs/2001.08361</idno>
		<ptr target="https://arxiv.org/abs/2001.08361" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension</title>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="5376" to="5384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Unifiedqa: Crossing format boundaries with a single qa system</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2005.00700" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scitail: A textual entailment dataset from science question answering</title>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Qasc: A dataset for question answering via sentence composition</title>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Guerquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">Alexander</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno>ArXiv, abs/1910.11473</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tateisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Collier</surname></persName>
		</author>
		<title level="m">Introduction to the bio-entity recognition task at jnlpba. International Joint Workshop on Natural Language Processing in Biomedicine and its Applications</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Large language models are zero-shot reasoners</title>
		<author>
			<persName><forename type="first">Takeshi</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Machel</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><surname>Iwasawa</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2205.11916" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Julen Oyarzabal, and Alfonso Valencia. The chemdner corpus of chemicals and drugs and its annotation principles</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Obdulia</forename><surname>Rabal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Leitner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Salgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Ji Anddaniel M Lowe Androger A Sayle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theresa</forename><surname>Batista-Navarro Yanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Rak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S?rgio</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buzhou</forename><surname>Matos Anddavid Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsendsuren</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keun</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName><surname>Ho Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Senthil</forename><surname>Sv Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slavko</forename><surname>Nathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marko</forename><surname>?itnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lutz</forename><surname>Bajec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><surname>Irmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Akhondi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Kors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utpal</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asif</forename><surname>Kumar Sikdar</surname></persName>
		</author>
		<author>
			<persName><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thaer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaharu</forename><surname>Dieb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miji</forename><surname>Yoshioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfang</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Komandur</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><forename type="middle">M</forename><surname>Elayavilli Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Couto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Jie</forename><surname>Lamurias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Tzong-Han</surname></persName>
		</author>
		<author>
			<persName><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Cheminform</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Caglar Ata, Tolga Can, Anabel Usi?, Rui Alves, Isabel Segura-Bedmar, Paloma Mart?nez</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Transformer-based artificial neural networks for the conversion between chemical notations</title>
		<author>
			<persName><forename type="first">Lev</forename><surname>Krasnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Khokhlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><forename type="middle">V</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Sosnin</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13321-021-00512-4</idno>
		<idno>13321-021-00512-4</idno>
		<ptr target="https://jcheminf.biomedcentral.com/articles/10.1186/s" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Keita</forename><surname>Kurita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nidhi</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1906.07337" />
		<title level="m">Measuring bias in contextualized word representations. CoRR, abs/1906.07337</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deduplicating training data makes language models better</title>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Nystrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Pretrained language models for biomedical and clinical tasks: Understanding and extending the state-of-the-art</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.clinicalnlp-1.17</idno>
		<ptr target="https://aclanthology.org/2020.clinicalnlp-1.17" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Clinical Natural Language Processing Workshop</title>
		<meeting>the 3rd Clinical Natural Language Processing Workshop</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Retrievalaugmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2206.14858" />
		<editor>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra</editor>
		<imprint>
			<date type="published" when="2020">2020. 2022</date>
		</imprint>
	</monogr>
	<note>Solving quantitative reasoning problems with language models</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">BioCreative V CDR task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Hsuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">C</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<idno>2016:baw068</idno>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Licklider</surname></persName>
		</author>
		<title level="m">Man-Computer Symbiosis. IRE Transactions on Human Factors in Electronics, HFE-1</title>
		<imprint>
			<date type="published" when="1960">1960</date>
			<biblScope unit="page" from="4" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Reasoning over paragraph effects in situations</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno>ArXiv, abs/1908.05852</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">TruthfulQA: Measuring how models mimic human falsehoods</title>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owain</forename><surname>Evans</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.229</idno>
		<ptr target="https://aclanthology.org/2022.acl-long.229" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022-05">May 2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3214" to="3252" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Language models of protein sequences at the scale of evolution enable accurate structure prediction</title>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Halil</forename><surname>Akin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roshan</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Hie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongkai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenting</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Santos</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maryam</forename><surname>Fazel-Zarandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sal</forename><surname>Candido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rives</surname></persName>
		</author>
		<idno type="DOI">10.1101/2022.07.20.500902</idno>
		<ptr target="https://www.biorxiv.org/content/early/2022/07/21/2022.07.20.500902" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">GORC: A large contextual citation graph of academic papers</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno>CoRR, abs/1911.02782</idno>
		<ptr target="http://arxiv.org/abs/1911.02782" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno>CoRR, abs/1711.05101</idno>
		<ptr target="http://arxiv.org/abs/1711.05101" />
		<imprint>
			<date type="published" when="2017">2019. 2017</date>
		</imprint>
	</monogr>
	<note>GORC: A large contextual citation graph of academic papers</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Chemical name to structure: Opsin, an open source solution</title>
		<author>
			<persName><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Corbett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Murray-Rust</surname></persName>
		</author>
		<author>
			<persName><surname>Glen</surname></persName>
		</author>
		<idno type="DOI">10.1021/ci100384d</idno>
		<ptr target="https://pubs.acs.org/doi/full/10.1021/ci100384d" />
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The big challenges of big data</title>
		<author>
			<persName><forename type="first">Vivien</forename><surname>Marx</surname></persName>
		</author>
		<ptr target="https://www.nature.com/articles/498255a" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">498</biblScope>
			<biblScope unit="page" from="255" to="260" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The kolmogorov-smirnov test for goodness of fit</title>
		<author>
			<persName><forename type="first">J</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><surname>Massey</surname></persName>
		</author>
		<idno type="DOI">10.1080/01621459.1951.10500769</idno>
		<ptr target="https://doi.org/10.1080%2F01621459" />
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">253</biblScope>
			<biblScope unit="page" from="68" to="78" />
			<date type="published" when="1951-03">mar 1951. 1951.10500769</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Memory-based model editing at scale</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2206.06520" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">StereoSet: Measuring stereotypical bias in pretrained language models</title>
		<author>
			<persName><forename type="first">Moin</forename><surname>Nadeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Bethke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.416</idno>
		<ptr target="https://aclanthology.org/2021.acl-long.416" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08">August 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5356" to="5371" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">CrowS-pairs: A challenge dataset for measuring social biases in masked language models</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasika</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.154</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.154" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="1953" to="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Overview of bioasq 2021: The ninth bioasq challenge on large-scale biomedical semantic indexing and question answering</title>
		<author>
			<persName><forename type="first">Anastasios</forename><surname>Nentidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Katsimpras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eirini</forename><surname>Vandorou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Krithara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Gasc?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Paliouras</surname></persName>
		</author>
		<idno>CoRR, abs/2106.14885</idno>
		<ptr target="https://arxiv.org/abs/2106.14885" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<author>
			<persName><surname>Nieschlag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Behre</surname></persName>
		</author>
		<author>
			<persName><surname>Nieschlag</surname></persName>
		</author>
		<author>
			<persName><surname>Andrology</surname></persName>
		</author>
		<title level="m">Male reproductive health and dysfunction</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Progen2: Exploring the boundaries of protein language models</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ruffolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><forename type="middle">N</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Madani</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2206.13517" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">The species and organisms resources for fast and accurate identification of taxonomic names in text</title>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Pafilis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Frankild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Fanini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Faulwetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aikaterini</forename><surname>Pavloudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Vasileiadou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><forename type="middle">Juhl</forename><surname>Arvanitidis</surname></persName>
		</author>
		<author>
			<persName><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Medmcqa : A large-scale multi-subject multi-choice dataset for medical domain question answering</title>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logesh</forename><surname>Kumar Umapathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malaikannan</forename><surname>Sankarasubbu</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2203.14371</idno>
		<ptr target="https://arxiv.org/abs/2203.14371" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Train short, test long: Attention with linear biases enables input length extrapolation</title>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno>CoRR, abs/2108.12409</idno>
		<ptr target="https://arxiv.org/abs/2108.12409" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Francis</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susannah</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Hennigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albin</forename><surname>Cassirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maribeth</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelia</forename><surname>Glaese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saffron</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonia</forename><surname>Creswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nat</forename><surname>Mcaleese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esme</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michela</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Paganini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lena</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adhiguna</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aida</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Nematzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domenic</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Grigorev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Sottiaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toby</forename><surname>Pajarskas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tayfun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Terzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Mikulik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><surname>De Las</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelia</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iason</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">S</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Rimell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kareem</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Ayoub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorrayne</forename><surname>Stanway</surname></persName>
		</author>
		<author>
			<persName><surname>Bennett</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2112.11446" />
		<imprint/>
	</monogr>
	<note>Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis &amp; insights from training gopher. CoRR, abs/2112.11446, 2021</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/20-074.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Stout: Smiles to iupac names using neural machine translation</title>
		<author>
			<persName><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zielesny</surname></persName>
		</author>
		<author>
			<persName><surname>Steinbeck</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13321-021-00512-4</idno>
		<ptr target="https://jcheminf.biomedcentral.com/articles/10.1186/s13321-021-00512-4" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Deep Learning for the Life Sciences</title>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Eastman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Walters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://www.amazon.com/Deep-Learning-Life-Sciences-Microscopy/dp/1492039837" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>O&apos;Reilly Media</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Impact of pretraining term frequencies on few-shot reasoning</title>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2202.07206" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.2016239118</idno>
		<ptr target="https://www.pnas.org/doi/abs/10.1073/pnas" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">15</biblScope>
			<date type="published" when="2021">2016239118. 2021. 2016239118</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Do large scale molecular language representations capture important structural information?</title>
		<author>
			<persName><forename type="first">Jerret</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Belgodere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inkit</forename><surname>Vijil Chenthamarakshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youssef</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payel</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName><surname>Das</surname></persName>
		</author>
		<idno>CoRR, abs/2106.09553</idno>
		<ptr target="https://arxiv.org/abs/2106.09553" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmish</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanya</forename><surname>Sharma Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nihal</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debajyoti</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Tian-Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng Xin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trishala</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos</forename><surname>Neeraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abheesht</forename><surname>Rozen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Santilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Fevry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Fries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Teehan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tali</forename><surname>Bers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rush</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2110.08207" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<pubPlace>Stella Biderman, Leo Gao, Thomas Wolf</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Continual-t0: Progressively instructing 50+ tasks to language models without forgetting</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuhin</forename><surname>Chakrabarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smaranda</forename><surname>Muresan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2205.12393" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno>CoRR, abs/1508.07909</idno>
		<ptr target="http://arxiv.org/abs/1508.07909" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">The woman worked as a babysitter: On biases in language generation</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno>CoRR, abs/1909.01326</idno>
		<ptr target="http://arxiv.org/abs/1909.01326" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Societal biases in language generation: Progress and challenges</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno>CoRR, abs/2105.04054</idno>
		<ptr target="https://arxiv.org/abs/2105.04054" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Biomegatron: Larger biomedical domain language model</title>
		<author>
			<persName><forename type="first">Hoo-Chang</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evelina</forename><surname>Bakhturina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Mani</surname></persName>
		</author>
		<idno>CoRR, abs/2010.06060</idno>
		<ptr target="https://arxiv.org/abs/2010.06060" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Overview of biocreative ii gene mention recognition</title>
		<author>
			<persName><forename type="first">Larry</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorraine</forename><forename type="middle">K</forename><surname>Tanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rie</forename><surname>Johnson Nee Ando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Ju</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Fang</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Nan</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Shi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Klinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><forename type="middle">M</forename><surname>Friedrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manabu</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><forename type="middle">A</forename><surname>Struble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">J</forename><surname>Povinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">A</forename><surname>Baumgartner</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bob</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Tzong-Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong-Jie</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Katrenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Adriaans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Blaschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariana</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Divoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacinto</forename><surname>Ma?a-L?pez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Mata</surname></persName>
		</author>
		<author>
			<persName><surname>Wilbur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<author>
			<persName><forename type="first">Aarohi</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abu</forename><surname>Awal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Shoeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abubakar</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adri?</forename><surname>Garriga-Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Kluska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Lewkowycz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshat</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alethea</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">W</forename><surname>Kocurek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Safaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Tazarv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Parrish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allen</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Dsouza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrose</forename><surname>Slone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Rahane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anantharaman</forename><forename type="middle">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Andreassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Santilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Stuhlm?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>La</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lampinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelica</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Vuong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Gottardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Norelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anu</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Gholamidavoodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arfa</forename><surname>Tabassum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arul</forename><surname>Menezes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Kirubarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asher</forename><surname>Mullokandov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Herrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avia</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aykut</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayla</forename><surname>Karaka?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Ryan</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao</forename><forename type="middle">Sheng</forename><surname>Loe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart?omiej</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Batuhan</forename><surname>?zyurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Hedayatnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behnam</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Inden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berk</forename><surname>Ekmekci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Howald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Dour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Stinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cedrick</forename><surname>Argueta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C?sar</forename><surname>Ferri Ram?rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandan</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Rathkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Waites</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Voigt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cindy</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><forename type="middle">E</forename><surname>Rivera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemencia</forename><surname>Siro ; Dilyar Buzan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitri</forename><forename type="middle">Coelho</forename><surname>Mollo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong-Ho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleanor</forename><surname>Hagerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Donoway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erkut</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernie</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><forename type="middle">A</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Jerzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunice</forename><forename type="middle">Engefu</forename><surname>Manyasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgenii</forename><surname>Zheltonozhskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanyue</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatemeh</forename><surname>Siar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Mart?nez-Plumed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesca</forename><surname>Happ?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francois</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frieda</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Genta</forename><surname>Indra Winata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germ?n</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giambattista</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Mariani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gloria</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gonzalo</forename><surname>Jaimovitch-L?pez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregor</forename><surname>Betz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Gur-Ari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hana</forename><surname>Galijasevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayden</forename><surname>Bogar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Shevlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiromu</forename><surname>Yakura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugh</forename><forename type="middle">Mee</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaap</forename><surname>Jumelet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Geissinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">Fern?ndez</forename><surname>Fisac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">B</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Koco?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jana</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jarema</forename><surname>Radom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelle</forename><surname>Bosscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Marsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeroen</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesujoba</forename><surname>Alabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jillian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Waweru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Burden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">U</forename><surname>Balis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rg</forename><surname>Frohberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos</forename><surname>Rozen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Hernandez-Orallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Boudeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">S</forename><surname>Rule</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joyce</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Kanclerz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Krauth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katerina</forename><surname>Ignatyeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kaustubh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Dhole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kory</forename><surname>Omondi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Mathewson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ksenia</forename><surname>Chiafullo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kumar</forename><surname>Shkaruta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laria</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Dugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Contreras-Ochando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Moschella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Oliveros Col?n</surname></persName>
		</author>
		<author>
			<persName><surname>Metz ; Maheen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manaal</forename><surname>Farooqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mantas</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baturan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Maru</surname></persName>
		</author>
		<author>
			<persName><surname>Jose Ram?rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Quintana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Tolkiehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Giulianelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">L</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M?ty?s</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Medina</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melody</forename><surname>Orduna Baitemirova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Arnaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Mcelrath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ivanitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Starritt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micha?</forename><surname>Strube</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Sw?drowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mimee</forename><surname>Cain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moin</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Aminnaseri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mozhdeh</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukund</forename><surname>Gheini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nayeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neta</forename><surname>Gur-</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Krakover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Niveditha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Agha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Elbaghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owain</forename><surname>Evans ; Yasaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichi</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiding</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifu</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufang</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoye</forename><surname>Seid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijie</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2206.04615" />
	</analytic>
	<monogr>
		<title level="m">L?tfi Kerem ?enel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve</title>
		<editor>
			<persName><forename type="first">Parth</forename><surname>Doshi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Vicol</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pegah</forename><surname>Alipoormolabashi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Peiyuan</forename><surname>Liao</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Peter</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Peter</forename><surname>Eckersley</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mon</forename><surname>Phu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pinyu</forename><surname>Htut</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Piotr</forename><surname>Hwang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Piyush</forename><surname>Mi?kowski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pouya</forename><surname>Patil</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Priti</forename><surname>Pezeshkpour</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Qiaozhu</forename><surname>Oli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Qing</forename><surname>Mei</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Qinlang</forename><surname>Lyu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rabin</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rachel</forename><forename type="middle">Etta</forename><surname>Banjade</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raefer</forename><surname>Rudolph</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rahel</forename><surname>Gabriel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ram?n Risco</forename><surname>Habacker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rapha?l</forename><surname>Delgado</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rhythm</forename><surname>Milli?re</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Richard</forename><surname>Garg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Barnes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Riku</forename><surname>Saurous</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Robbe</forename><surname>Arakawa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Robert</forename><surname>Raymaekers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rohan</forename><surname>Frank</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Sikand</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Novak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ronan</forename><surname>Sitelew</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rosanne</forename><surname>Lebras</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rowan</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rui</forename><surname>Jacobs</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ryan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ryan</forename><surname>Chi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ryan</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ryan</forename><surname>Stovall</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rylan</forename><surname>Teehan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sahib</forename><surname>Yang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Saif</forename><forename type="middle">M</forename><surname>Singh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sajant</forename><surname>Mohammad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sam</forename><surname>Anand</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sam</forename><surname>Dillavou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Samuel</forename><surname>Wiseman</surname></persName>
		</editor>
		<editor>
			<persName><surname>Gruetter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Bowman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sanghyun</forename><surname>Schoenholz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sanjeev</forename><surname>Han</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sarah</forename><forename type="middle">A</forename><surname>Kwatra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sarik</forename><surname>Rous</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sayan</forename><surname>Ghazarian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sean</forename><surname>Ghosh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sebastian</forename><surname>Casey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sebastian</forename><surname>Bischoff</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sepideh</forename><surname>Schuster</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shadi</forename><surname>Sadeghi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sharon</forename><surname>Hamdan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shashank</forename><surname>Zhou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sherry</forename><surname>Srivastava</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shikhar</forename><surname>Shi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shima</forename><surname>Singh</surname></persName>
		</editor>
		<editor>
			<persName><surname>Asaadi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shubh</forename><surname>Gu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shubham</forename><surname>Pachchigar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shyam</forename><surname>Toshniwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Debnath</forename><surname>Upadhyay</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Siamak</forename><surname>Shyamolima</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Simon</forename><surname>Shakeri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Simone</forename><surname>Thormeyer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Siva</forename><surname>Melzi</surname></persName>
		</editor>
		<editor>
			<persName><surname>Reddy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Priscilla</forename><surname>Sneha</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Soo-Hwan</forename><surname>Makini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Spencer</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sriharsha</forename><surname>Torene</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stanislas</forename><surname>Hatwar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stefan</forename><surname>Dehaene</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stefano</forename><surname>Divic</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stella</forename><surname>Ermon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stephanie</forename><surname>Biderman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Steven</forename><forename type="middle">T</forename><surname>Prasad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stuart</forename><forename type="middle">M</forename><surname>Piantadosi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Summer</forename><surname>Shieber</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Svetlana</forename><surname>Misherghi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Swaroop</forename><surname>Kiritchenko</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tal</forename><surname>Mishra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tao</forename><surname>Schuster</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tariq</forename><surname>Yu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tatsu</forename><surname>Ali</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Te-Lin</forename><surname>Hashimoto</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Th?o</forename><surname>Wu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Theodore</forename><surname>Desbordes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Rothschild</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tianle</forename><surname>Phan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tiberius</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Timo</forename><surname>Nkinyili</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Timofei</forename><surname>Schick</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Timothy</forename><surname>Kornev</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Titus</forename><surname>Telleen-Lawton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tobias</forename><surname>Tunduny</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Trenton</forename><surname>Gerstenberg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Trishala</forename><surname>Chang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tushar</forename><surname>Neeraj</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tyler</forename><surname>Khot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Uri</forename><surname>Shultz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vedant</forename><surname>Shaham</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vera</forename><surname>Misra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Victoria</forename><surname>Demberg</surname></persName>
		</editor>
		<editor>
			<persName><surname>Nyamai</surname></persName>
		</editor>
		<meeting><address><addrLine>Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nikita Nangia, Niklas Deckers, Niklas Muennighoff; Pablo Antonio Moreno Casares</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu. Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song,. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>S?ding</surname></persName>
		</author>
		<idno type="DOI">10.1038/nbt.3988</idno>
		<ptr target="https://doi.org/10.1038/nbt.3988" />
	</analytic>
	<monogr>
		<title level="j">Nature Biotechnology</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1026" to="1028" />
			<date type="published" when="2017-10">October 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Challenging big-bench tasks and whether chain-of-thought can solve them</title>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Sch?rli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2210.09261" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">ChemProt: a disease chemical biology database</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Taboureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonny</forename><surname>Kim Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karine</forename><surname>Audouze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Weinhold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Edsg?rd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Francisco S Roque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Kouskoumvekaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramona</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Curpan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S?ren</forename><surname>Sk?t Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tudor</forename><forename type="middle">I</forename><surname>Brunak</surname></persName>
		</author>
		<author>
			<persName><surname>Oprea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="367" to="372" />
			<date type="published" when="2011-01">January 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">Commonsenseqa: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno>CoRR, abs/1811.00937</idno>
		<ptr target="http://arxiv.org/abs/1811.00937" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Scaling laws vs model architectures: How does inductive bias influence scaling?</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><surname>Metzler</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2207.10551" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung</forename><forename type="middle">Won</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinh</forename><forename type="middle">Q</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaixiu</forename><surname>Steven Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2210.11399" />
		<title level="m">Transcending scaling laws with 0.12022b</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaguang</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongrae</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaixiu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Steven Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Ghafouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Menegali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Ching</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Krivokon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Rusch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranesh</forename><surname>Pickett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laichee</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meredith</forename><forename type="middle">Ringel</forename><surname>Meier-Hellstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tulsee</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renelito Delos</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toju</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnny</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Soraker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinodkumar</forename><surname>Zevenbergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Lamda</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2201.08239" />
		<title level="m">Language models for dialog applications</title>
		<editor>
			<persName><forename type="first">Alejandra</forename><surname>Molina</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Erin</forename><surname>Hoffman-John</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Josh</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lora</forename><surname>Aroyo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ravi</forename><surname>Rajakumar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alena</forename><surname>Butryna</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Matthew</forename><surname>Lamm</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Viktoriya</forename><surname>Kuzmina</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joe</forename><surname>Fenton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aaron</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rachel</forename><surname>Bernstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Blaise</forename><surname>Aguera-Arcas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Claire</forename><surname>Cui</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marian</forename><surname>Croak</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Tagrec: Automated tagging of questions with hierarchical learning taxonomy</title>
		<author>
			<persName><forename type="first">V</forename><surname>Venktesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mukesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Mohania</surname></persName>
		</author>
		<author>
			<persName><surname>Goyal</surname></persName>
		</author>
		<idno>CoRR, abs/2107.10649</idno>
		<ptr target="https://arxiv.org/abs/2107.10649" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>CoRR, abs/1706.03762</idno>
		<ptr target="http://arxiv.org/abs/1706.03762" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2109.01652" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Chain of thought prompting elicits reasoning in large language models</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Ichter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2201.11903" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules</title>
		<author>
			<persName><forename type="first">David</forename><surname>Weininger</surname></persName>
		</author>
		<idno type="DOI">10.1021/ci00057a005</idno>
		<ptr target="https://doi.org/10.1021/ci00057a005" />
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Computer Sciences</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="36" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Crowdsourcing multiple choice science questions</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno>ArXiv, abs/1707.06209</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Information, physics, quantum: The search for links</title>
		<author>
			<persName><forename type="first">John</forename><surname>Wheeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Complexity, Entropy, and the Physics of Information</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Zurek</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of mathematics in the natural sciences</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Wigner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<date type="published" when="1959">1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<monogr>
		<title level="m" type="main">Moleculenet: A benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aneesh</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1703.00564" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Towards human-level machine reading comprehension: Reasoning and inference with multiple strategies</title>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/1711.04964</idno>
		<ptr target="http://arxiv.org/abs/1711.04964" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Linkbert: Pretraining language models with document links</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2203.15827" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Opt: Open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjali</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2205.01068" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
		<title level="m" type="main">Uni-mol: A universal 3d molecular representation learning framework</title>
		<author>
			<persName><forename type="first">Gengmo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Gao Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiankun</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Hongteng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Ke</surname></persName>
		</author>
		<ptr target="https://chemrxiv.org/engage/chemrxiv/article-details/" />
		<imprint>
			<date type="published" when="2022">2022. 628e5b4d5d948517f5ce6d72</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Attention Is All You Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>ArXiv abs/1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>ArXiv abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
