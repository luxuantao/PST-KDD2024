<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Learning in Tracking Control Based on the Dual Critic Network Design</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Haibo He, Senior Member, IEEE</roleName><forename type="first">Zhen</forename><surname>Ni</surname></persName>
							<email>ni@ele.uri.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical, Computer and Biomedical Engineering</orgName>
								<orgName type="institution">University of Rhode Island</orgName>
								<address>
									<postCode>02881</postCode>
									<settlement>Kingston</settlement>
									<region>RI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Jinyu</forename><surname>Wen</surname></persName>
							<email>jinyu.wen@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical, Computer and Biomedical Engineering</orgName>
								<orgName type="institution">University of Rhode Island</orgName>
								<address>
									<postCode>02881</postCode>
									<settlement>Kingston</settlement>
									<region>RI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">College of Electrical, Electronic and Engineering</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Learning in Tracking Control Based on the Dual Critic Network Design</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0CCBDE73B126208A2A4D8869802E6D47</idno>
					<idno type="DOI">10.1109/TNNLS.2013.2247627</idno>
					<note type="submission">received November 29, 2011; revised November 28, 2012; accepted February 10, 2013.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Adaptive critic design (ACD)</term>
					<term>adaptive dynamic programming (ADP)</term>
					<term>internal goal</term>
					<term>lyapunov stability analysis</term>
					<term>online learning</term>
					<term>reinforcement learning</term>
					<term>tracking control</term>
					<term>virtual reality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a new adaptive dynamic programming approach by integrating a reference network that provides an internal goal representation to help the systems learning and optimization. Specifically, we build the reference network on top of the critic network to form a dual critic network design that contains the detailed internal goal representation to help approximate the value function. This internal goal signal, working as the reinforcement signal for the critic network in our design, is adaptively generated by the reference network and can also be adjusted automatically. In this way, we provide an alternative choice rather than crafting the reinforcement signal manually from prior knowledge. In this paper, we adopt the online action-dependent heuristic dynamic programming (ADHDP) design and provide the detailed design of the dual critic network structure. Detailed Lyapunov stability analysis for our proposed approach is presented to support the proposed structure from a theoretical point of view. Furthermore, we also develop a virtual reality platform to demonstrate the realtime simulation of our approach under different disturbance situations. The overall adaptive learning performance has been tested on two tracking control benchmarks with a tracking filter. For comparative studies, we also present the tracking performance with the typical ADHDP, and the simulation results justify the improved performance with our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>D EVELOPING brain-like intelligence has become a cutting-edge research topic in the computational intelligence field. While the latest research on adaptive dynamic programming (ADP)/adaptive critic design (ACD) <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b5">[6]</ref> has shown great success on machine intelligence and applications, there are still many grand challenges in reaching truly brainlike intelligence. One of the most important questions is how to design a general system that can learn and optimize adaptively and efficiently like the human brain to achieve the final goal over time <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. In this paper, we propose a novel structure based on heuristic dynamic programming (HDP) to tackle this problem.</p><p>In recent years, extensive efforts have been dedicated to developing machine intelligence, and ADP is one of the most critical approaches to (hopefully) bring such a level of intelligence closer to reality <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b9">[10]</ref>. Generally speaking, ADP can be categorized into three typical structures <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. 1) HDP, which was proposed to provide a control action sequence with the action network and use the critic network to approximate the value function. For instance, in online HDP version such as <ref type="bibr" target="#b12">[13]</ref>, the critic network was designed to critique the generated action by propagating a temporal difference (TD) between two consecutive estimates of the value function. 2) Dual heuristic dynamic programming (DHDP), the key point of which is to employ the critic network to approximate the derivatives of the value function with respect to the state vectors. 3) Globalized dual heuristic dynamic programming (GDHP), which takes advantage of both HDP and DHP by employing the critic network to approximate both the value function and its derivatives with respect to the state vectors.</p><p>Moreover, various versions have been developed based on these typical structures, such as the action-dependent (AD) version <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref> by taking the control action as one of the inputs for the critic network, and the model-dependent version <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> by taking the estimates of the model network as part of the inputs for the critic network.</p><p>Currently, many implementations of these ADP designs have been tested on both mathematical benchmarks and engineering applications, such as the infinite-time optimal tracking control with greedy HDP algorithm in <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>, the HDP controller for nonlinear discrete time tracking problem in <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b24">[25]</ref>, and the engine torque and exhaust air-fuel ratio tracking control based on AD HDP in <ref type="bibr" target="#b25">[26]</ref>. In <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b28">[29]</ref>, the looper system control in the iron industry was improved with the Levenberg-Marquardt (LM) algorithm instead of backpropagation as the weights updating rules in neural networks, and in <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b32">[33]</ref> the turbogenerator/power system control was improved with HDP and DHP controllers, and many others <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b38">[39]</ref>.</p><p>Although there are many successful applications with ADP approaches, many of them use either a binary reinforcement signal ("0" and "-1") or some specific discrete reinforcement signals ("0," "-0.4," and "-1") <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Some others crafted the reinforcement signal manually with past experience or prior knowledge. For instance, in <ref type="bibr" target="#b10">[11]</ref>, the square of the difference between the reference signal and the actual output of the system is set as the reinforcement signal. In <ref type="bibr" target="#b39">[40]</ref>, the reinforcement signal is defined with different weights for various components, with some of the error components even having nonlinear coefficients. Therefore, an intuitive question is: can the reinforcement signal be assigned without any (or minimum) prior knowledge or human intervention and be adjusted adaptively when the operation environment changes?</p><p>In this paper, we propose a novel ADP structure with dual critic networks that provide the internal goal representation adaptively according to the system's behaviors to tackle this question. Specifically, we introduce one reference network to be on top of the typical ADP design, through which we maintain the advantage of a model-free AD structure <ref type="bibr" target="#b12">[13]</ref>.</p><p>In this way, we build one reference network on top of the critic network to work as an integrated dual critic network structure, where the reference network provides the critic network with an internal reinforcement signal. Unlike the binary reinforcement signal from the external environment, this internal signal is continuous and bounded. It not only works as one of the input vectors of the critic network but also contributes to the parameter tuning for the critic network. Moreover, it can be adjusted adaptively and automatically by the reference network according the system states and the control action. As this internal signal is not crafted manually or with any prior knowledge, we believe that our proposed dual critic network design can provide useful suggestions about how to adaptively and automatically assign a proper reinforcement signal for the general system to achieve the final goal over time adaptively. We would like to note that, although there are many successful applications on tracking control with adaptive/fuzzy controllers <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b42">[43]</ref>, many such approaches require an accurate system model to be able to achieve the control or tracking performance. However, in real-world complex problems, it is not uncommon that such an accurate system model is very difficult, or even impossible sometimes, to obtain due to the complexity of the system. Therefore, we consider our proposed dual critic model-free ADP approach to be particularly useful for such complex systems.</p><p>Motivated by our previous work <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b43">[44]</ref>, we extend our idea to the tracking control problems with several major new contributions. First, we integrate a tracking filter in this paper to handle the tracking problem rather than the balancing problem. Second, we develop the Lyapunov stability analysis to provide the theoretical support of our method, which is important to understand the convergence of this method. Third, we develop a virtual reality (VR) platform to demonstrate the performance of our method, with active interaction of the external environment. Finally, we conduct many more experiments, including two types of noises/disturbances, for the system to demonstrate its performance. We would like to note that the terminology of "reference network" we use in this paper is similar to the "goal network" and "goal generator network" we discussed in several of our recent papers <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b44">[45]</ref>, and <ref type="bibr" target="#b24">[25]</ref>. The underlying principle of both the reference network and the goal network/goal generator network is the The rest of this paper is organized as follows. Section II provides the architecture description of the dual critic network and the tracking filter design. Section III presents the implementation of the dual critic network HDP design with a tracking filter, together with its associated learning algorithms. Detailed experiment setup and simulation results are given in Sections IV and V, respectively. For both cases, we provide comparative studies between our approach and the typical HDP approach under the same settings. Detailed Lyapunov stability analysis is presented in Section VI. Finally, Section VII concludes this work with some discussions on future research directions. The detailed pseudo-code for the implementation of our method is given in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. HDP WITH A DUAL CRITIC NETWORK FOR NONLINEAR TRACKING CONTROL</head><p>The schematic diagram of our proposed idea is presented in Fig. <ref type="figure" target="#fig_3">1</ref>. The action network is kept the same as in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b20">[21]</ref>. For the critic network, we integrate with one reference network, and therefore there are two networks in the critic network block as presented in Fig. <ref type="figure" target="#fig_16">2</ref>. The tracking filter is added here to show the performance on the tracking control problem. In the rest of this section, we will introduce the dual critic network block and the tracking filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dual Critic Network in ADP Design</head><p>The motivation of this dual critic network design is twofold: one is to provide an internal goal representation for the critic network; the other is to help approximate the cost-to-go in detail since the internal reinforcement signal works as one of the input vectors for the critic network.</p><p>From the system-level view in Fig. <ref type="figure" target="#fig_3">1</ref>, we can see that the parameters in the dual critic network block can not only be tuned by an external signal but also be adjusted by itself. Specifically, the reference network in the top of the block is tuned by the error function with the external reward signal, while the critic network at the bottom of the block is tuned by the error function with the internal reinforcement signal. As presented in Fig. <ref type="figure" target="#fig_16">2</ref>, the reference network observes a regular reward signal r (k) (usually a binary value) from external environment and provides the critic network with a detailed internal reinforcement signal s(k) (usually a continuous value) by justifying the system state vectors X (k) and control action u(k). In order to approximate the value function J (k) well, the critic network keeps the same inputs as the reference network in addition to the internal signal s(k). Moreover, s(k) also contributes to the error function of the critic network, as the dash line shown inside the block. Since the s(k) can be automatically adjusted according to the state vectors X (k) and the control action u(k), we regard it as an adaptive reinforcement signal. In summary, the key idea of our dual critic design is to use the reference network to automatically and adaptively generate the internal goal signal, rather than hand-crafted in the traditional ADP approaches, to guide the decision-making process for the optimal action at any time instance to accomplish the final goal. This reference network can also actively interact with the critic network and action network, either directly (for critic network) or indirectly (for action network), to support the action selection in a principled way. We will further present the detailed learning architecture and algorithm for such a dual structure design in Section III-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Tracking Filter</head><p>In order to demonstrate the improvement of our proposed dual-critic controller, we would like to test it on nonlinear tracking control problem with a tracking filter, as presented in Fig. <ref type="figure" target="#fig_3">1</ref>. The inner structure of the tracking filter is presented in Fig. <ref type="figure" target="#fig_1">3</ref>, which was originally from <ref type="bibr" target="#b45">[46]</ref> and later developed in <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b21">[22]</ref>. To be clear, we introduce the design of the tracking filter as follows.</p><p>The nonlinear system function is defined in a general form as</p><formula xml:id="formula_0">x(k + 1) = f (x(k)) + u(k) + d(k) (1)</formula><p>where f (x(k)) is the nonlinear system function,  The nonlinear system function f (x(k)) is assumed to be unknown in the simulation and can be approximated by the action network here. The approximation value is denoted as f (x(k)), which works as one of the inputs of the filter. Moreover, the inputs of the filter also include the current state vector X (k) and the desired trajectory value x d (k) and x d (k + 1), as presented in Fig. <ref type="figure" target="#fig_1">3</ref>. The error e(k) is defined as the difference between the current state value and the desired value as follows:</p><formula xml:id="formula_1">x(k) = [ x 1 (k) x 2 (k) x 3 (k) • • • x n (k) ],</formula><formula xml:id="formula_2">e(k) = x(k) -x d (k)<label>(2)</label></formula><p>and the filtered tracking error ē(k) is defined as</p><formula xml:id="formula_3">ē(k) = e(k) + λ 1 e n-1 (k) + • • • + λ n-1 e 1 (k) (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where e n-1 (k), . . . , e 1 (k) are the past error values, which means that e n-i (k) = e n (k -i ), i = 0, 1, . . . , n -1, n ∈ . For brevity, we define ∧ = [λ n-1 , λ n-2 , . . . , λ 1 ], where λ i ∈ . Therefore, (3) can be rewritten as</p><formula xml:id="formula_5">ē(k) = [∧ I ]e(k).<label>(4)</label></formula><p>Again, we can rewrite (3) for time instance k + 1 as</p><formula xml:id="formula_6">ē(k + 1) = e(k + 1) + λ 1 e n-1 (k + 1) + • • • +λ n-1 e 1 (k + 1).<label>(5)</label></formula><p>Substituting (1) into (5), we get</p><formula xml:id="formula_7">ē(k + 1) = f (x(k)) -x d (k + 1) + λ 1 e n-1 (k + 1) + • • • +λ n-1 e 1 (k + 1) + u(k) + d(k) = f (x(k)) -x d (k + 1) + λ 1 e n (k) + • • • +λ n-1 e 2 (k) + u(k) + d(k). (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>Similar to <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b21">[22]</ref>, we define that the control sequence</p><formula xml:id="formula_9">u(k) = x d (k + 1) -f (x(k)) + k v ē(k) -λ 1 e n (k) - • • • -λ n-1 e 2 (k). (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>Substituting ( <ref type="formula" target="#formula_9">7</ref>) into (6), we get</p><formula xml:id="formula_11">ē(k + 1) = K v ē(k) -f (x(k)) + d(k) (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>where f (x(k)) is the nonlinear system function approximation error given by</p><formula xml:id="formula_13">f (x(k)) = f (x(k)) -f (x(k)) (9)</formula><p>and K v is the gain value. Assuming that f (x(k)) is bounded, the system will be stable if 0 &lt; K vmax &lt; 1, where K vmax is the maximum eigenvalue of K v <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. IMPLEMENTATION OF HDP WITH DUAL CRITIC NETWORK FOR NONLINEAR TRACKING CONTROL</head><p>In this section, we provide detailed procedures on how to implement our proposed dual critic network HDP approach with a tracking filter. We also define the external reinforcement signal and the internal reinforcement signal here. A brief pseudocode (Algorithm 1) is provided in this section to clearly demonstrate the implementation of the dual critic network. Moreover, a detailed algorithm-level implementation (Algorithm 2) is presented in the Appendix to show the implementation of our proposed approach. Multilayer perceptron (MLP) neural network has been one of the most popular techniques to approximate the nonlinear function in the ADP community <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Therefore, we follow this trend and adopt MLP in our ADP design as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. External and Internal Reinforcement Signal</head><p>As mentioned previously, there are two types of reinforcement signal in our proposed approach. One is the external reinforcement signal, which comes from the environment, and the other is internal reinforcement signal, which comes from the reference network and works as an internal goal that guides the system's behavior specifically. We will discuss these two signals in detail in the following.</p><p>External reinforcement signal r (k) is defined according to the current filtered tracking error ē(k) as</p><formula xml:id="formula_14">r (k) = r 1 (k) r 2 (k) , . . . , r m (k) ∈ m (10) with r i (k) = 0, -1, i f ēi (k) ≤ c i f ēi (k) ≥ c , i = 1, 2, 3, . . . , m<label>(11)</label></formula><p>where • represents the Euclidean vector 2-norm and c is the constant threshold for the filtered tracking error. The binary value 0 represents a good tracking performance while -1 means a poor one.</p><p>The internal reinforcement signal s(k) is the output of the reference network bounded in [-1, 1] and can be adaptively adjusted as the system states change. At the feed-forward stage, the internal signal works as one of the inputs for the critic network. At the feed-backward stage, the parameters in the critic network are tuned by the error function with s(k). Therefore, the internal reinforcement signal s(k) closely connects the reference network and the critic network as a whole block. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning and Optimization of Dual Critic Network</head><p>Compared to the typical HDP design in <ref type="bibr" target="#b12">[13]</ref>, the learning and optimization of the critic network here is associated with the reference network as presented in Fig. <ref type="figure" target="#fig_2">4</ref>. At the forward stage, the reference network obtains the inputs of the state vector X (k) and the control action u(k) and provides an internal reinforcement signal s(k) for the critic network. Then the critic network updates the cost-to-go signal J (k). At the backward stage, the reference network will first be tuned by the error function <ref type="bibr" target="#b14">(15)</ref> with r (k) and the updated cost-togo signal J (k). After this is done, the reference network will provide the updated internal reinforcement signal s(k) for the critic network, which will then be adjusted with the error function <ref type="bibr" target="#b11">(12)</ref>. The learning process will repeat until the terminal conditions are satisfied. The pseudocode in Algorithm 1 shows exactly this learning procedure.</p><p>The error function of the critic network is defined as follows:</p><formula xml:id="formula_15">e c (k) = α J (k) -[J (k -1) -s(k)]; E c (k) = 1 2 e 2 c (k)<label>(12)</label></formula><p>where</p><formula xml:id="formula_16">J (k) = ω (1) c (k) • φ ω (2) c (k) • x c (k) (<label>13</label></formula><formula xml:id="formula_17">)</formula><p>and ω (1)  c (k) and ω (2)  c (k) refer to the weights of the input to the hidden layer and the hidden to the output layer in critic network, respectively. x c (k) is the input vector of the critic network and it contains the state vector X (k), the control action u(k), and the internal signal s(k). φ stands for the sigmoid function that refines the output into the range of</p><formula xml:id="formula_18">[-1, 1]. s(k) is defined as s(k) = φ ω (1) r (k) • φ ω (2) r (k) • x r (k) (<label>14</label></formula><formula xml:id="formula_19">)</formula><p>where ω (1)  r (k) and ω (2)  r (k) refer the weights of the input to the hidden layer and the hidden to the output layer in reference network, respectively. x r (k) is the input vector of the reference network and it contains the state vector X (k) and the control action u(k).</p><p>The error function of the reference network is defined as  Given that the state vector X (k) has n elements and the control action u(k) is a single control unit, the inputs for the reference network and the critic network will be (n + 1) and (n + 2), as presented in Fig. <ref type="figure" target="#fig_4">5</ref>(a) and (b), respectively. The chain backpropagation rule is employed for the neural networks to learn and adapt their weights.</p><formula xml:id="formula_20">e r (k) = α J (k) -[J (k -1) -r (k)]; E r (k) = 1 2 e 2 r (k). (<label>15</label></formula><p>1) Re f er ence Networ k: The reference network is introduced here to provide an internal goal representation for the critic network. The internal goal s(k) is defined as</p><formula xml:id="formula_21">s(k) = 1 -exp -l(k) 1 + exp -l(k) (16) l(k) = N rh i=1 w (2) r i (t)y i (k) (<label>17</label></formula><formula xml:id="formula_22">)</formula><formula xml:id="formula_23">y i (k) = 1 -ex p -z i (k) 1 + ex p -z i (k) , i = 1, . . . , N rh (<label>18</label></formula><formula xml:id="formula_24">)</formula><formula xml:id="formula_25">z i (k) = n+1 j =1</formula><p>w (1)  r i, j (t)x r j (k), i = 1, . . . , N rh <ref type="bibr" target="#b18">(19)</ref> where z i is the input of the i th hidden node and y i is the corresponding output of this hidden node after the sigmoid function, l is the input to the output node, N rh is the number of hidden neurons in the reference network, and x r j is the input vector of the reference network, which has (n + 1) input nodes as presented in Fig. <ref type="figure" target="#fig_4">5(a)</ref>. The procedure of backpropagation rule applied to the reference network is illustrated below. 1) w <ref type="bibr" target="#b1">(2)</ref> r : Reference network weights adjustment from the hidden layer to the output layer</p><formula xml:id="formula_26">w (2) r i (k) = η r (k) - ∂ E r (k) ∂w (2) r i (k) (<label>20</label></formula><formula xml:id="formula_27">)</formula><p>where η r (k) is the learning rate of the reference network at time instance k, and</p><formula xml:id="formula_28">∂ E r (k) ∂w (2) r i (k) = ∂ E r (k) ∂ J (k) ∂ J (k) ∂s(k) ∂s(k) ∂l(k) ∂l(k) ∂w (2) r i (k) = αe r (k) • 1 2 (1 -(s(k)) 2 ) • y i (k) • N rh i=1 [w (2) c i (k) 1 2 (1 -p 2 i (k))w (1) c i, n+2 (k)]. (21)</formula><p>2) w (1)  r : Reference network weight adjustment from the input layer to the hidden layer w (1)  r i, j (k) = η r (k) -</p><formula xml:id="formula_29">∂ E r (k) ∂w (1)</formula><formula xml:id="formula_30">r i, j (k) (22) ∂ E r (k) ∂w (1)</formula><formula xml:id="formula_31">r i, j (t) = ∂ E r (k) ∂ J (k) ∂ J (k) ∂s(k) ∂s(k) ∂l(k) ∂l(k) ∂y i (k) ∂y i (k) ∂z i (k) ∂z i (k) ∂w (1)</formula><formula xml:id="formula_32">r i, j (k) = αe r (k) • 1 2 (1 -y 2 i (k)) • x r j (k) • 1 2 (1 -(s(k)) 2 ) •w (2) r i (k) • N rh i=1 w (2) c i (k) 1 2 (1 -p 2 i (k))w (1) c i, n+2 (k) . (<label>23</label></formula><formula xml:id="formula_33">)</formula><p>Once the internal goal s(k) is updated in reference network, we can adapt the weight tuning in the critic network.</p><p>2) Critic Networ k: In the literature, the critic network is applied to approximate the cost function, and its inputs normally contain the state vector X (k) and the control unit u(k). Here we add one more input with the internal goal s(k) and hope that s(k) can provide the critic network with detailed goal representation that contributes to the system's decision making. The cost-to-go signal J (k) is defined as  <ref type="bibr" target="#b25">(26)</ref> where q i and p i are the input and output of the i th hidden node in the critic network, respectively, and x cj is input vector of the critic network with n + 2 nodes, as presented in Fig. <ref type="figure" target="#fig_4">5(b)</ref>.</p><formula xml:id="formula_34">J (k) = N ch i=1 w (2) c i (k) p i (k) (<label>24</label></formula><formula xml:id="formula_35">)</formula><formula xml:id="formula_36">p i (k) = 1 -ex p -q i (k) 1 + ex p -q i (k) , i = 1, . . . , N ch (<label>25</label></formula><formula xml:id="formula_37">) (a) (b)</formula><formula xml:id="formula_38">q i (k) = n+2 j =1 w (1) c i, j (k)x cj (k), i = 1, . . . , N ch</formula><p>The procedure of the backpropagation rule applied to the critic network is provided in the following. 1) w <ref type="bibr" target="#b1">(2)</ref> c : Critic network weight adjustment from the hidden layer to the output layer</p><formula xml:id="formula_39">w (2) c i (k) = η c (k) - ∂ E c (k) ∂w (2) c i (k)<label>(27)</label></formula><p>where η c (k) is the learning rate of the critic network at time instance k</p><formula xml:id="formula_40">∂ E c (k) ∂w (2) c i (k) = ∂ E c (k) ∂ J (k) ∂ J (k) ∂w (2) c i (k) = αe c (k) • p i (k). (<label>28</label></formula><formula xml:id="formula_41">)</formula><p>2) w <ref type="bibr" target="#b0">(1)</ref> c : Critic network weight adjustment from the input layer to the hidden layer w (1)  </p><formula xml:id="formula_42">c i, j (k) = η c (k) - ∂ E c (k) ∂w (1) c i, j (k) (29) ∂ E c (k) ∂w (1) c i, j (k) = ∂ E c (t) ∂ J (k) ∂ J (k) ∂ p i (k) ∂ p i (k) ∂q i (k) ∂q i (k) ∂w (1) c i, j (k) = αe c (k) • w (2) c i (k) • 1 2 (1 -p 2 i (k))x cj (k). (<label>30</label></formula><formula xml:id="formula_43">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Interaction of the Action Network and the Tracking Filter</head><p>The control action in this paper is generated by the tracking filter, as shown in Fig. <ref type="figure" target="#fig_5">6(b</ref>). The action network here is to approximate the nonlinear system function f (x(k)), and the approximation error f (x(k)) is added to the error function of the action network. Note that f (x(k)) cannot be obtained directly from (9) since f (x(k)) is assumed unknown in this paper. The model network is commonly used in the ADP/ACD designs for tracking control <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Here we apply the same technique to predict the state vector x(k + 1) and get ê(k + 1). Then, f (x(k)) can be obtained from <ref type="bibr" target="#b7">(8)</ref>.</p><p>The error function of the action network is defined as follows:</p><formula xml:id="formula_44">e a = J (k) + f (k); E a (k) = 1 2 e 2 a (k). (<label>31</label></formula><formula xml:id="formula_45">)</formula><p>The nonlinear system function f (x(k)) can be obtained as follows:</p><formula xml:id="formula_46">f (x(k)) = 1 -exp(-v(k)) 1 + exp(-v(k)) (32) v(k) = N ah i=1 w (2) a i (k)g i (k) (<label>33</label></formula><formula xml:id="formula_47">)</formula><formula xml:id="formula_48">g i (k) = 1 -exp(-h i (k)) 1 + exp(-h i (k)) , i = 1, . . . , N ah (<label>34</label></formula><formula xml:id="formula_49">)</formula><formula xml:id="formula_50">h i (k) = n j =1</formula><p>w (1)  a i, j (k)x a j (k), i = 1, . . . , N ah <ref type="bibr" target="#b34">(35)</ref> where h i and g i are the input and output of the i th hidden node in the action network, and v is the input for the output node. f is the output of the action network, and N ah is the total number of hidden nodes in the action network. And x a j is the input vector of the action network presented in Fig. <ref type="figure" target="#fig_5">6(a)</ref>. Note that the weights tuning of the action network should consider the tracking filter as well. 1) w</p><p>(2) a : Action network weights adjustment from the hidden layer to the output layer</p><formula xml:id="formula_51">w (2) a i (k) = η a (k) - ∂ E a (k) ∂w (2) a i (k)<label>(36)</label></formula><p>where η a (k) is the learning rate of the action network at time instance k. And ∂ E a (k) ∂w (2)  </p><formula xml:id="formula_52">a i (k) = ∂ E a (k) ∂ J (k) ∂ J (k) ∂u (k) ∂u (k) ∂v (k) ∂v (k)</formula><p>∂w (2)  a i (k)</p><formula xml:id="formula_53">(37) = e a (k) - 1 2 1 -f 2 (k) g i (k) • ⎧ ⎨ ⎩ N ch i=1 w (2) c i (k) 1 2 1 -p 2 i (k) w (1) c i,(n+1) (k) ⎫ ⎬ ⎭ . (<label>38</label></formula><formula xml:id="formula_54">)</formula><p>2) w</p><p>(1) a : Action network weights adjustment from the input layer to the hidden layer w (1)  </p><formula xml:id="formula_55">a i, j (k) = η a (k) - ∂ E a (k) ∂w (1) a i, j (k) (39) ∂ E a (k) ∂w (1)</formula><formula xml:id="formula_56">a i j (k) = ∂ E a (k) ∂ J (k) ∂ J (k) ∂u (k) ∂u (k) ∂v (k) ∂v (k) ∂ g i (k) ∂ g i (k) ∂h i (k) ∂h i (k) ∂w (1)</formula><formula xml:id="formula_57">a i j (k) (40) = N ah l=1</formula><p>w (2)  c l (k) w (1)  c l,(n+1) (k) We note that, similar to <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b26">[27]</ref>, the normalization of the weights will be employed during the learning and adaptation for all the networks used here. The weights are confined to the proper range by</p><formula xml:id="formula_58">1 2 1 -p 2 l (k) e a (k) × w (2) a i (k) x a j (k) 1 2 1 -f 2 (k) 1 2 1 -g 2 i (k) . (<label>41</label></formula><formula xml:id="formula_59">)</formula><formula xml:id="formula_60">w r (k + 1) = w r (k) + w r (k) a , a = max( a i j ) ∀a i j ∈ w r (k) + w r (k) (<label>42</label></formula><formula xml:id="formula_61">)</formula><formula xml:id="formula_62">w c (k + 1) = w c (k) + w c (k) b , b = max( b i j ) ∀b i j ∈ w c (k) + w c (k) (<label>43</label></formula><formula xml:id="formula_63">)</formula><formula xml:id="formula_64">w a (k + 1) = w a (k) + w a (k) c , c = max( c i j ) ∀c i j ∈ w a (k) + w a (k) . (<label>44</label></formula><formula xml:id="formula_65">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SIMULATION STUDY ONE</head><p>In this section, we present two numerical simulations based on the same system function. The motivation is to compare the tracking performance with our proposed approach and with the typical approach in <ref type="bibr" target="#b20">[21]</ref>, which is originally from <ref type="bibr" target="#b12">[13]</ref>. The system function is defined by the general nonlinear form as</p><formula xml:id="formula_66">x 1 (k + 1) = x 2 (k) x 2 (k + 1) = f (x(k)) + u(k) + d(k) (<label>45</label></formula><formula xml:id="formula_67">)</formula><p>where</p><formula xml:id="formula_68">f (x(k)) = - 4 11 • x 1 1 + x 2 2 + 2 5 x 2 (k) (<label>46</label></formula><formula xml:id="formula_69">)</formula><p>with f (x(k)) assumed to be unknown in the tracking process. Instead, the approximation value f (x(k)) can be obtained from the action network. And d(k) is the disturbance here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Example One</head><p>The objective of Example 1 is to track the sinusoidal signal with some harmonic signal with x 2 . The desired signal function is defined as x 2d = sin(ωkT ) cos(2ωkT + τ ), where ω = 0.2 rad/s and τ = π/2. We set the sample interval T = 50 ms and the total simulation time to be 150 s. In the literature, noise or disturbance is normally added in the simulation to see how robustness of the proposed approach will be, like in <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b46">[47]</ref>, and <ref type="bibr" target="#b47">[48]</ref>. In this paper, we adopt a similar technique as in <ref type="bibr" target="#b21">[22]</ref>. Disturbance d(k) = 1.5 is introduced at k = 1200, corresponding to t = 60 s. Otherwise, it is set to be 0. The input vector X is defined as (i.e., the network has four input nodes, four hidden nodes and one output node), 5 -4 -1, and 6 -4 -1, respectively. The parameters we used in the simulation are summarized as in Table <ref type="table" target="#tab_1">I</ref>. We note that the learning rates will drop once the tracking performance is "good" over time. Specifically, we will compare the mean square error (MSE) as expressed in <ref type="bibr" target="#b46">(47)</ref> with a certain threshold c</p><formula xml:id="formula_70">X in (k) = [e(k -1) e(k) x 2d (k -1) x 2d (k)], where e(k) = x 2 (k) -x 2d (k). She</formula><formula xml:id="formula_71">MSE = 1 N N i=1 (x -x d ) 2 (<label>47</label></formula><formula xml:id="formula_72">)</formula><p>where x is the state vector, x d is the desired tracking signal, and N is a preset integer. If the MSE is less than the threshold, then the learning rate will be divided by a certain number and the new threshold can be calculated by dividing another number correspondingly. This kind of evaluation will be repeated during the whole process of the tracking control. The detailed implementation is presented in Algorithm 2 (line 24 to 32).</p><p>For a comparative study, we have conducted the simulation with both approaches under the same parameters and environment settings. The weights in the neural networks used in both approaches are randomly selected from [-1, 1]. The starting point of the state vector is (0, 1.5), which is the same for both approaches. The typical tracking performance with our proposed approach and the typical HDP approach are IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS presented in Figs. <ref type="figure" target="#fig_6">7</ref> and<ref type="figure" target="#fig_8">8</ref>, respectively. From Fig. <ref type="figure" target="#fig_6">7</ref>, we can see that the tracking signal (solid line) can exactly follow the desired signal (dash line) within 1 s. In addition, the tracking signal can also quickly go back to the right track after the disturbance at 60 s. On the other hand, the tracking signal with the typical HDP approach can only follow the desired signal after 10 s. Moreover, the tracking signal needs more time (13 s) to go back to track the desired signal after the disturbance. From this example, we can see that our proposed approach shows not only faster learning process than the typical HDP approach but also better robustness against disturbance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Example Two</head><p>In order to show the adaptiveness of the proposed approach, we conducted another numerical example to track a signal that would change from a saw signal to a square signal and finally to a sinusoid signal. We used the same system function and environment settings as in Section IV-A, except that d(k) was set to be white Gaussian noise with a standard deviation of 0.005 for all k. The object is to track the desired signal with the state vector x 2 . The desired tracking signal is defined as</p><formula xml:id="formula_73">x 2d = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ A • 1 -t -T 0 2 , 0 &lt; t &lt; T 0 A • 1 -t -3T 0 2 , T 0 &lt; t &lt; 2T 0 -A, 2T 0 &lt; t &lt; 3T 0 or 4T 0 &lt; t &lt; 5T 0 A, 3T 0 &lt; t &lt; 4T 0 or 5T 0 &lt; t &lt; 6T 0 A • sin(ωt), 6T 0 &lt; t &lt; 7.5T 0 (<label>48</label></formula><formula xml:id="formula_74">)</formula><p>where t = kT , k is the step number, and T is the sample time (T = 50 ms). A = 0.95 is the amplitude of the signal. T 0 = 40 s is taken as the time internal for which each signal lasts (i.e., the signal will change after T 0 ). The difficulty of this task is that the controller needs to learn to track the desired signal, which will change over time, under the white Gaussian noise. Fig. <ref type="figure" target="#fig_9">9</ref> shows the typical tracking performance with our proposed approach, and one can clearly see the good transient tracking performance when the signal changes. But for the typical HDP approach, we  can see in Fig. <ref type="figure" target="#fig_10">10</ref> that the controller takes about 50 s to learn to follow the saw signal and also spends much time to learn when the desired signal changes to a rectangular signal. In addition, the controller also requires about 20 s to catch up after the desired signal changes to sinusoid signal. This indicates that our proposed approach shows better adaptiveness to this tracking problem than the typical HDP approach under noisy condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SIMULATION STUDY TWO</head><p>Instead of testing on two numerical cases, here we would like to evaluate our proposed approach on a continuous benchmark, i.e., the ball-and-beam tracking problem <ref type="bibr" target="#b48">[49]</ref>. There are many versions of this benchmark, and in this paper we adopt the model presented in Fig. <ref type="figure" target="#fig_11">11</ref>. The system contains a long beam that can be tilted by a servo or electric motor with a ball rolling back and forth on top of the beam. In this system, the driver is located at the center of the beam. The angle of the beam to the horizontal axis is measured by an incremental encoder, and the position of the ball can be obtained with cameras mounted on the top of system. Our proposed approach will learn to track the desired signal with the position of the ball.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation</head><p>From <ref type="bibr" target="#b48">[49]</ref>, we can obtain the equations of motion from the Lagrange equation as follows:</p><formula xml:id="formula_75">m + I b r 2 ẍ + (mr 2 + I b ) 1 r α -mx α2 = mg(sin α) (49) [m(x ) 2 + I b + I ω ] α + (2m ẋ x + bl 2 ) α + K l 2 α + (mr 2 + I b ) 1 r ẍ -mgx (cos α) = ul(cos α) (<label>50</label></formula><formula xml:id="formula_76">)</formula><p>where m = 0.0162 kg, is the mass of the ball; r = 0.02 m, is the roll radius of the ball; I b = 4.32 × 10 -5 kg • m 2 , is the inertia moment of the ball; b = 1 N s /m, is the friction coefficient of the drive mechanics; l = 0.48 m, is the radius of force application; l ω = 0.5 m, is the radius of the beam; K = 0.001N/m, the stiffness of the drive mechanics; g : 9.8 N/kg, is the gravity; I ω = 0.14025 kg • m 2 , is the moment of inertia of the beam; and u is the force of the drive mechanics.</p><p>In order to simplify the system model function, we define that x 1 = x represents the position of the ball, x 2 = ẋ represents the velocity the ball, x 3 = α is the angle of the beam with respect to the horizontal axis, and x 4 = α is the angular velocity of the beam. Therefore, the state vector can be defined as X = [x 1 x 2 x 3 x 4 ]. In this way, the system function ( <ref type="formula">49</ref>) and ( <ref type="formula" target="#formula_75">50</ref>) can be transformed into the following forms:  To make it clearer, we rewrite ( <ref type="formula" target="#formula_77">51</ref>) and ( <ref type="formula" target="#formula_79">52</ref>) with the specific value of all the parameters mentioned above into the approximate nonlinear state-space equations as follows:</p><formula xml:id="formula_77">m + I b r 2 ẋ2 + (mr 2 + I b ) 1 r ẋ4 = mx 1 x 2 4 + mg(sin x 3 ) (<label>51</label></formula><formula xml:id="formula_78">)</formula><formula xml:id="formula_79">(mr 2 + I b ) 1 r ẋ2 + [mx 2 1 + I b + I ω ] ẋ4 = (ul + mgx 1 ) × cos x 3 -(2mx 2 x 1 + bl 2 )x 4 -K l 2 x 3 . (<label>52</label></formula><formula xml:id="formula_80">)</formula><formula xml:id="formula_81">ẋ1 = x 2 (53) ẋ2 = 1.717 sin(x 3 ) (54) ẋ3 = x 4</formula><p>(55) ẋ4 = -0.241x 4 + 0.157x 1 cos(x 3 ) + 0.5 cos(x 3 ) • u. (56)</p><p>The objective is to track the sinusoid signal x 1d = 0.1 sin(ωt) with the position of ball (x 1 ), where ω = 0.1. This task requires the controller to not only keep the balance of the ball on the beam but also to track the desired signal using the position of the ball (x 1 ). That is to say, if x 1 is out of the bound ([-0.48, 0.48] m), or x 3 exceeds the angular velocity tolerance ([0.24, 0.24] rad/s), we will reset the ball to the initial starting point ([0 0 0 0]). Since the learning process of the neural network is continuous, we will assume that the weights can be carried on when the task is reset. Keen readers may also find that this is a continuous-time benchmark rather the discretetime case above. Here we apply the common technique that is extensively used in the literature <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b43">[44]</ref> of calling the continuous system model function with the ode45 function in MATLAB with the step size 0.02 s. The parameters used in this case are summarized in Table <ref type="table" target="#tab_2">II</ref>, and 5% uniform noise is also added to the sensor of x 1 to show the tracking performance under noisy conditions.</p><p>Figs. 12 and 13 present the tracking performance with our proposed approach and the typical HDP approach <ref type="bibr" target="#b20">[21]</ref>, respectively. Fig. <ref type="figure" target="#fig_0">12</ref> clearly shows that the ball is out of bounds at the very beginning, but can quickly track the desired signal within one period. The oscillation of the tracking signal (solid line) in the first period shows the learning process of the IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS  controller with our proposed approach. In Fig. <ref type="figure" target="#fig_13">13</ref>, we can see that the ball is out of bounds many times before it can track the desired signal. In other words, the controller spends the whole first period to learn to control the ball and it learns after about 80 s. The simulation results show that the controller with our proposed approach has better noise tolerance than with the typical HDP approach.</p><p>To provide a more accurate assessment of the tracking performance, here we summarize the quantitative measurements in terms of tracking error for all examples in study 1 (i.e., examples 1 and 2) and study 2 (i.e., the ball and beam benchmark). Here we adopt the evaluation function in <ref type="bibr" target="#b17">[18]</ref> defined as PER = N 0 e T (k)e(k), where e(k) is the tracking error in Fig. <ref type="figure" target="#fig_1">3</ref> and N refers to the number of steps in the simulation. The PER for Sections IV-A, IV-B, and V-A are summarized in Table <ref type="table" target="#tab_3">III</ref>.</p><p>From Table <ref type="table" target="#tab_3">III</ref>, one can see that our proposed dual critic HDP can achieve much lower PER (total tracking error) than with the typical HDP approach <ref type="bibr" target="#b20">[21]</ref>. The results also confirm that our proposed structure with the informative and adaptive reinforcement signal can outperform the typical HDP structure in terms of tracking accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. VR Demonstration of the Dual Critic Network Design</head><p>In this paper, we further apply our algorithm on VR environment to show real-time simulation of our approach during interaction with the external environment. VR can enable powerful human-computer interactions, and it is interesting to observe how the proposed algorithm works in real-time simulation without the requirement of setting up the real physical system. Here we would like to demonstrate the tracking performance of our proposed approach on the balland-beam benchmark <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>. And we also add disturbances  Typical tracking performance with our proposed approach in VR/simulation platform.</p><p>to see how robust our proposed approach can perform.</p><p>The VR platform is developed as in Fig. <ref type="figure" target="#fig_14">14</ref>, where one can see that the ball-and-beam system is at the center of the scene and the state vectors (x 1 and x 3 ) are displayed in the upperleft table of the figure. In order to be more realistic, we add a disturbance option (upper center) in the simulation. That is to say, the user can apply any disturbance between -8 to 8 N on the ball whenever needed, and the corresponding force will be applied to the system, as also displayed on the upper-right table.</p><p>Real-time simulation result on the tracking problem with our proposed approach is presented in Fig. <ref type="figure" target="#fig_15">15</ref>, where one can clearly see the online learning process. In other words, the first period of signal is like a distorted sine wave, while the signal in second period almost tracks the desired sine signal. At about 64 s, we added a 2.1 N disturbance on the ball and we can see that the controller spends about two periods to learn to get back to the right track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. LYAPUNOV STABILITY ANALYSIS</head><p>We now proceed to study the stability characteristics of our proposed method. Similar to <ref type="bibr" target="#b51">[52]</ref>, we employ φ a for the output of the hidden neurons in the action net-</p><formula xml:id="formula_82">work s.t. φ a (k) = [φ a,1 (k) φ a,2 (k) • • • φ a,N ah (k)] T and then φ a (k) 2 = φ T a (k)φ a (k).</formula><p>For the critic and reference networks, we adopt similar notations s.t. φ c and φ r are the output of the hidden neurons in the critic network and reference network, respectively. The estimated weights in the networks are denoted as ωa , ωc , and ωr , while the expected weights are denoted as ω a , ω c , and ω r . Therefore, the differences are defined as ωa = ωa -ω a , ωc = ωc -ω c , and ωr = ωr -ω r . We note that, although ωa , ωc , and ωr are unknown parameters, we assume they have upper bounds in this analysis because, when they exceed the preset upper bounds, we will normalize the weights according to ( <ref type="formula" target="#formula_60">42</ref>)- <ref type="bibr" target="#b43">(44)</ref>.</p><p>Define the Lyapunov function candidate as</p><formula xml:id="formula_83">V = V 1 + V 2 + V 3 + V 4 + V 5 (<label>57</label></formula><formula xml:id="formula_84">)</formula><p>where</p><formula xml:id="formula_85">V 1 = 1 γ 1 ēT (k) ē(k) (58) V 2 = 1 η c tr( ωT c (k) ωc (k)) (59) V 3 = 1 2 ζ c (k -1) 2 (60) V 4 = 1 γ 2 η a tr( ωT a (k) ωa (k)) (61) V 5 = 1 γ 3 η r tr( ωT r (k) ωr (k)).<label>(62)</label></formula><p>In (60</p><formula xml:id="formula_86">), ζ c (k -1) = ( ωc (k -1) -ω c ) T φ 1 (k -1) = ωT c (k -1) φ 1 (k -1) and γ i &gt; 0 for i = 1, 2, 3.</formula><p>The first difference of the Lyapunov function candidate can be written as</p><formula xml:id="formula_87">V = V 1 + V 2 + V 3 + V 4 + V 5 . (<label>63</label></formula><formula xml:id="formula_88">)</formula><p>For V 1 , we have</p><formula xml:id="formula_89">ē(k + 1) = K v ē(k) -f (x(k)) + d(k) (64) where f (x(k)) = f (x(k)) -f (x(k)) f (x(k)) = ω T a (k)φ a (k) + ε a (x(k)) f (x(k)) = ( ωa (k) -ω a (k)) T φ a (k) + ε a (x(k)) = ωT a (k)φ a (k) + ε a (x(k)). (<label>65</label></formula><formula xml:id="formula_90">) Therefore V 1 = 1 γ 1 ēT (k + 1) ē(k + 1) -ēT (k) ē(k) = 1 γ 1 K v ē(k) + ζ a (k) + ε a (k) + d(k) T •(K v ē(k) + ζ a (k) + ε a (k) + d(k)) -ēT (k) ē(k) ≤ 1 γ 1 3 K 2 v ē(k) 2 + ζ a (k) 2 + ε a (k) + d(k) 2 -ē(k) 2 ] ≤ 3 γ 1 K 2 vmax - 1 3 ē(k) 2 + ζ a (k) 2 + ε a (k) + d(k) 2 (66)</formula><p>where K vmax is the maximum eigenvalue of K v .</p><p>For V 2 , we have</p><formula xml:id="formula_91">ωc (k + 1) = ωc (k) -η c αφ c (k)(α ωT c (k)φ c (k) + s(k) -ωT c (k -1)φ c (k -1)) T (67)</formula><p>and the corresponding ωc (k + 1) can be expressed as</p><formula xml:id="formula_92">ωc (k + 1) = ωc (k) -η c αφ c (k) α ωT c (k)φ c (k) + s(k) -ωT c (k -1)φ c (k -1) T = I -η c α 2 φ c (k)φ T c (k) ωc (k) -η c αφ c (k) αω T c φ c (k) + s(k) -ωT c (k -1)φ c (k -1) T . (<label>68</label></formula><formula xml:id="formula_93">)</formula><p>Then</p><formula xml:id="formula_94">V 2 = 1 η c tr( ωT c (k + 1) ωc (k + 1) -ωT c (k) ωc (k)) = 1 η c tr ωT c (k)A T A ωc (k) -ωT c (k) ωc (k) +Bα 2 η 2 c φ T c (k)φ c (k)B T -ωT c (k)A T η c αφ c (k)B T -Bη c αφ T c (k)A ωc (k) (<label>69</label></formula><formula xml:id="formula_95">)</formula><p>where</p><formula xml:id="formula_96">A = I -η c α 2 φ c (k)φ T c (k) and B = αω T c φ c (k) + s(k) - ωT c (k -1)φ c (k -1). We note ωT c (k)A T A ωc (k) -ωT c (k) ωc (k) = ωT c (k)[(I -η c α 2 φ c φ T c ) T ×(I -η c α 2 φ c φ T c )] ωc (k) -ωT c (k) ωc (k) = -η c α 2 ζ c (k) 2 -η c α 2 ωT c (k)φ c φ T c ×(I -η c α 2 φ c φ T c ) ωc (k) (<label>70</label></formula><formula xml:id="formula_97">)</formula><p>where</p><formula xml:id="formula_98">ζ c (k) = ωT c (k)φ c (k). Therefore V 2 (k) = -α 2 ζ c (k) 2 -α 2 1 -η c α 2 φ c (k) 2 ζ c (k) 2 +η c α 2 φ c (k) 2 • αω T c φ c (k) + s(k) -ωT c (k -1)φ c (k -1) 2 -2tr[α I -η c α 2 φ c (k) 2 ζ c (k) • αω T c φ c (k) + s(k) -ωT c (k -1)φ c (k -1) T ].<label>(71)</label></formula><p>We would like to seek the upper bound of (71) by applying Cauchy-Schwarz inequality for the fourth term. Therefore, we have</p><formula xml:id="formula_99">V 2 (k) ≤ -α 2 ζ c (k) 2 -α 2 (1 -η c α 2 φ c (k) 2 ) • ζ c (k) 2 + αω T c φ c (k) + s(k) -ωT c (k -1)φ c (k -1) 2 -α 2 (1 -η c α 2 φ c (k) 2 ) • ζ c (k) + ω T c φ c (k) + α -1 s(k) -α -1 ωT c (k -1) ×φ c (k -1) 2 .</formula><p>(72)</p><p>From (72), we can further get</p><formula xml:id="formula_100">V 2 (k) ≤ -α 2 ζ c (k) 2 + 1 2 ζ c (k -1) 2 +2 αω T c φ c (k) + s(k) -ωT c (k -1)φ c (k -1) 2 -α 2 1 -η c α 2 φ c (k) 2 • ζ c (k) + ω T c φ c (k) + α -1 s(k) -α -1 ωT c (k -1) ×φ c (k -1) 2 . (<label>73</label></formula><formula xml:id="formula_101">)</formula><p>For V 3 (k), we have</p><formula xml:id="formula_102">V 3 (k) = 1 2 ( ζ c (k) 2 -ζ c (k -1) 2 ). (<label>74</label></formula><formula xml:id="formula_103">)</formula><p>For V 4 (k), we have</p><formula xml:id="formula_104">ωa (k + 1) = ωa (k) -η a φ a (k) ωT c (k)C a (k) • ( ωT c (k)φ c (k) + K v ē(k) -ē(k + 1)) T (75)</formula><p>where C a (k) is an N ch × 1 vector and its elements can be defined as</p><formula xml:id="formula_105">C a (k) = (1/2)(1 -φ c (k))ω c,n+1 (k).</formula><p>And then</p><formula xml:id="formula_106">ωa (k + 1) = ωa (k) -η a φ a (k) ωT c (k)C a (k) • ωT c (k)φ c (k) + K v ē(k) -ē(k + 1) T . (<label>76</label></formula><formula xml:id="formula_107">)</formula><p>With tracking error dynamics, we can rewrite (76) as</p><formula xml:id="formula_108">ωa (k + 1) = ωa (k) -η a φ a (k) ωT c (k)C a (k) • ωT c (k)φ c (k) + ζ a (k) -ε a (k) -d(k) T .<label>(77)</label></formula><p>Therefore, we have</p><formula xml:id="formula_109">V 4 (k) = 1 γ 2 η a tr ωT a (k + 1) ωa (k + 1) -ωT a (k) ωa (k) = 1 γ 2 tr -2 ωT c (k)C a (k)ζ a (k) • ωT c (k)φ T c (k) + ζ a (k) -ε a (k) -d(k) T +η a φ 2 a (k) • ωT c (k)C a (k) 2 • ωT c (k)φ c (k) + ζ a (k) -ε a (k) -d(k) 2 . (<label>78</label></formula><formula xml:id="formula_110">)</formula><p>In order to further simplify the formula, we can rewrite (78) as follows:</p><formula xml:id="formula_111">V 4 (k) = 1 γ 2 tr[-(1 -η a φ 2 a (k)) D 2 E 2 + D • E -ζ a (k) 2 -ζ a (k) 2 ]<label>(79)</label></formula><p>where</p><formula xml:id="formula_112">D = ωT c (k)C a (k) (80) E = ωT c (k)φ c (k) + ζ a (k) -ε a (k) -d(k). (<label>81</label></formula><formula xml:id="formula_113">)</formula><p>Applying the Cauchy-Schwarz inequality for (79), we have</p><formula xml:id="formula_114">V 4 (k) ≤ 1 γ 2 tr[F + 2 • D • E 2 + ζ a (k) 2 ]<label>(82)</label></formula><p>where</p><formula xml:id="formula_115">F = -(1 -η a φ a (k) 2 ) D 2 E 2 . (<label>83</label></formula><formula xml:id="formula_116">)</formula><p>For V 5 (k), we have</p><formula xml:id="formula_117">ωr (k + 1) = ωr (k) -η r φ r (k) ωT c (k)C r (k)(α ωT c (k)φ c (k) +r (k) -ωT c (k -1)φ c (k -1)) T (84)</formula><p>where C r (k) is an N ch × 1 vector and its elements can be defined as</p><formula xml:id="formula_118">C r (k) = (1/2)(1 -φ c (k))ω c,n+2 (k). Therefore V 5 (k) = 1 γ 3 η r tr ωT r (k + 1) ωr (k + 1) -ωT r (k) ωr (k) = 1 γ 3 tr -2 ωT r (k)C r (k)ζ r (k)(α ωT c (k)φ c (k) +r (k) -ωT c (k -1)φ c (k -1)) T +η r φ r (k) ωT r (k)C r (k) 2 α ωT c (k)φ c (k) +r (k) -ωT c (k -1)φ c (k -1) 2 . (<label>85</label></formula><formula xml:id="formula_119">)</formula><p>We can also simplify (85) by rewriting it as</p><formula xml:id="formula_120">V 5 (k) = 1 γ 3 tr[H + G • I -ζ r (k) 2 -ζ r (k) 2 ]<label>(86)</label></formula><p>where</p><formula xml:id="formula_121">G = (α ωT c (k)φ c (k) + r (k) -ωT c (k -1)φ c (k -1)) T (87) H = -(1 -η r φ r (k) 2 ) G 2 I 2 (88) I = ωT r (k)C r (k). (<label>89</label></formula><formula xml:id="formula_122">)</formula><p>Applying the Cauchy-Schwarz inequality for (86), we have</p><formula xml:id="formula_123">V 5 (k) ≤ 1 γ 3 tr H + 2 • G • I 2 + ζ r (k) 2 . (<label>90</label></formula><formula xml:id="formula_124">)</formula><p>Substituting (66), ( <ref type="formula" target="#formula_100">73</ref>), ( <ref type="formula" target="#formula_102">74</ref>), (82), and (90) into (57), we can get the first difference of the Lyapunov function candidate as</p><formula xml:id="formula_125">V (k) ≤ 3 γ 1 K 2 vmax - 1 3 ē(k) 2 + ζ a (k) 2 + ε a (k) +d(k) 2 -α 2 ζ c (k) 2 + 2 αω T c φ c (k) + s(k) -ωT c (k -1)φ c (k -1) 2 -α 2 1 -η c α 2 φ c (k) 2 • ζ c (k) + ω T c φ c (k) + α -1 s(k) -α -1 ωT c (k -1)φ c (k -1) 2 + 1 2 ζ c (k -1) 2 + 1 2 ( ζ c (k) 2 -ζ c (k -1) 2 ) + 1 γ 2 tr F + 2 • D • E 2 + ζ a (k) 2 + 1 γ 3 tr H + 2 • G • I 2 + ζ r (k) 2 . (<label>91</label></formula><formula xml:id="formula_126">)</formula><p>In order to express (91) in a clear way, we rewrite it as</p><formula xml:id="formula_127">V (k) ≤ 3 γ 1 K 2 vmax - 1 3 ē(k) 2 + ζ a (k) 2 + ε a (k) + d(k) 2 -α 2 - 1 2 ζ c (k) 2 -α 2 I -η c α 2 φ c (k) 2 • ζ c (k) + ω T c φ c (k) + α -1 s(k) -α -1 ωT c (k -1)φ c (k -1) 2 - 1 γ 2 (1 -η a φ a (k) 2 ) D 2 E 2 - 1 γ 3 (1 -η r φ r (k) 2 ) G 2 I 2 + Z 2 (92)</formula><p>where Z 2 is defined as</p><formula xml:id="formula_128">Z 2 = 1 γ 2 ζ a (k) 2 + 1 γ 3 ζ r (k) 2 +2 αω T c φ c (k) + s(k) -ωT c (k -1)φ c (k -1) 2 + 1 γ 2 2 • D • E 2 + 1 γ 3 2 • G • I 2 . (<label>93</label></formula><formula xml:id="formula_129">)</formula><p>And the upper bound for Z 2 is</p><formula xml:id="formula_130">Z 2 ≤ 2 γ 2 ω T am φ am 2 + 2 γ 3 ω T rm φ rm 2 +6(α 2 + 1) • ω T cm φ cm 2 + 6s 2 m + 6 γ 2 ω T cm C am 2 • ω T cm φ cm 2 + 2 ω T am φ am 2 + ε am + d m 2 + 6 γ 3 ω T rm C rm 2 • (α 2 + 1) • ω T cm φ cm 2 + r m 2 = 6 • α 2 + 1 + 1 γ 2 ω T cm C am 2 + 1 γ 3 (α 2 + 1) • ω T rm C rm 2 • ω T cm φ cm 2 + 2 γ 2 1 + 6 ω T cm C am 2 ω T am φ am 2 + 2 γ 3 ω T rm φ rm 2 + 6s 2 m + 6 γ 2 ω T cm C am 2 • ε am + d m 2 + 6 γ 3 ω T rm C rm 2 • r m 2 = Z 2 m (<label>94</label></formula><formula xml:id="formula_131">)</formula><p>where ω cm , ω am , ω rm , φ cm , φ am , φ rm , C am , C rm , s m , and r m are the upper bounds of ω c , ω a , ω r , φ c , φ a , φ r , C a , C r , s, and r , respectively. Equation (94) further implies that V (k) ≤ 0 if the following conditions hold:</p><formula xml:id="formula_132">0 &lt; K vmax &lt; √ 3 3 (95) √ 2 2 &lt; α &lt; 1 (96) η c α 2 φ c (k) 2 &lt; 1, η a φ a (k) 2 &lt; 1, η r φ r (k) 2 &lt; 1 (97) and ē(k) &gt; γ 1 1 -3K v max Z m (98) or ζ c (k) &gt; 1 (α 2 -1 2 ) Z m . (<label>99</label></formula><formula xml:id="formula_133">)</formula><p>According to a standard Lyapunov extension theorem <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, this demonstrates that the auxiliary error and the error in the weights estimates are uniformly ultimately bounded. And this further implies that the weights estimates are bounded correspondingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>This paper proposed a novel ADP structure that combines one reference network with the original critic network into a dual critic network design. The reference network provided the critic network with an internal goal representation that helped it to approximate the total cost-to-go signal in detail. Unlike the discrete reward signal from the external environment, this internal goal signal can be adjusted adaptively with regard to the system state and the control action. Therefore, past experience or prior knowledge is not a necessity to assign reinforcement signal value here. The dual critic network has a weight-turning path not only from the outside but also from the inside. Compared with the typical HDP design under the same simulation environment settings, our proposed dual critic network HDP can achieve better tracking performance on time-costing and the accumulated tracking error. In addition to various simulation studies and a VR platform development, we also presented detailed theoretical analysis in terms of Lyapunov stability analysis for our method.</p><p>As a new ADP approach, there are many interesting future research topics along this direction. For instance, in this paper we only used one reference network in our design. We are extending this dual design to see how it works with multiple reference networks on top of the critic network, similar to the hierarchical neural network structure. Some promising results have been achieved and reported in <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b44">[45]</ref>. Also, the weight-turning rules in our design are based on the classic chain backpropagation and we are interested to see how much improvement can be achieved if we implemented the LM algorithm or Kalman filter into our neural networks.   end while // online learning of the action network 61) end for //corresponding to step 8.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig. 1. Architecture design of the proposed HDP approach with a tracking filter.</figDesc><graphic coords="2,334.55,263.09,205.70,173.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Description of the tracking filter.</figDesc><graphic coords="3,334.55,53.81,205.70,173.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Learning schematic in dual critic network.</figDesc><graphic coords="4,354.95,53.45,164.54,138.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>) Algorithm 1</head><label>1</label><figDesc>Outline of Implementation of Dual Critic Network HDP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) Neural network structure of the reference network. (b) Neural network structure of the critic network.</figDesc><graphic coords="5,56.03,417.41,113.26,104.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. (a) Neural network structure of the action network. (b) Control action generator.</figDesc><graphic coords="6,56.03,53.81,113.28,104.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Typical tracking performance with our proposed approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>:</head><label></label><figDesc>where α Discount factor. c : Threshold on mean square error. η c : Initial learning rate of the critic network. η a : Initial learning rate of the action network. η r : Initial learning rate of the reference network. N c : Internal cycle of the critic network. N a : Internal cycle of the action network. N r : Internal cycle of the reference network. T c : Internal training error threshold for the critic network. T a : Internal training error threshold for the action network. T r : Internal training error threshold for the reference network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Typical tracking performance with HDP approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig.9. Typical tracking performance with our proposed approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Typical tracking performance with HDP approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Schematic diagram of the ball-and-beam system.</figDesc><graphic coords="9,66.47,53.93,216.02,183.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Typical tracking performance with our proposed approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Typical tracking performance with HDP approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Schematic of the VR platform.</figDesc><graphic coords="10,323.99,53.69,226.22,166.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15.Typical tracking performance with our proposed approach in VR/simulation platform.</figDesc><graphic coords="10,323.99,252.77,226.26,173.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Algorithm 2</head><label>2</label><figDesc>Algorithm-Level Implementation of Dual Critic Network of ADP for Tracking Control /* f (x) ⇐ ActNet (x, w a ), nonlinear function approximation with the action network; ActNet: the action network; x: state vector; w a : weights of ActNet; f (x): nonlinear system function approximation, the output of ActNet; u ⇐ Filter x d , x, f (x) , control action calculation; IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Filter: the tracking filter; x d : the desired reference signal; u: control action; s ⇐ RefNet (x, u, w r ), internal goal representation with the reference network; RefNet: the reference network; w r : weights of the RefNet; s: internal goal signal; J ⇐ CritNet (x, u, s, w c ), total cost-to-go signal approximated by the critic network; CritNet: the critic network; w c : weights of the CritNet; J:total cost-to-go signal, the output of the critic network; */ /*Note: the parameters N r , T r , η r , N c , T c , η c , N a , T a , and η a are all defined in Table I; */ 1) Initiate x(0) 2) Uniformly randomize w a(0), w r (0), w c (0) in [-1, 1] 3) f (x(0)) ⇐ ActNet (x(0), w a (0)) 4) u(0) ⇐ Filter x d , x(0), f (x(0)) 5) s(0) ⇐ RefNet (x(0), u(0), w r (0)) 6) J (0) ⇐ CritNet (x(0), u(0), s(0), w c (0)) 7) J prev = J (0) 8)for 1 to MaxStep do; 9) //weights are carried on through the whole learning process; 10) CurrentState ⇐ (x(k -1), u(k -1)); //obtain current state vectors from the external environment 11) w a(k) = w a (k -1); 12) w c (k) = w c (k -1); 13) w r (k) = w r (k -1); 14) f (x(k)) ⇐ ActNet (x(k), w a (k)); 15) u(k) ⇐ Filter x d , x(k), f (x(k)) ; 16) s(k) ⇐ RefNet (x(k), u(k), w r (k)); 17) J (k) ⇐ CritNet (x(k), u(k), s(k), w c (k)); 18)Obtain the tracking error ē(k) via (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>1 2 (</head><label>2</label><figDesc>α J (k) -(J (k -1)r (k)))2 ; 34) cyc = 0;<ref type="bibr" target="#b34">35)</ref> while (E r (k) &gt; T r &amp;cyc &gt; N r ) do // update the weights recursively;<ref type="bibr" target="#b35">36)</ref> w r (k) = w r (k) + w r (k) via (20) and (23); // update the s(k), J (k), E r (k), cyc correspondingly 37)s(k) ⇐ RefNet (x(k), u(k), w r (k)); 38) J (k) ⇐ CritNet (x(k), u(k), s(k), w c (k)); 39) E r (k) = 1 2 (α J (k) -(J (k -1)r (k))) 2 ; 40) cyc = cyc + 1; 41)end while // online learning of the reference network 42)E c (k) = 1 2 (α J (k) -(J (k -1)s(k))) 2 ; 43) cyc = 0; 44) while (E c (k) &gt; T c &amp;cyc &gt; N c ) do 45) w c (k) = w c (k) + w c (k) via (27) and (30); 46)J (k) ⇐ CritNet (x(k), u(k), s(k), w c (k)); 47) E c (k) = 1 2 (α J (k) -(J (k -1)s(k))) 2 ; 48) cyc = cyc + 1; 49)end while // online learning of the critic network 50)E a (k) = 1 2 (α J (k) + f (x(k)) -U c ) 2 ; 51) cyc = 0; 52) while (E a (k) &gt; T a &amp;cyc &gt; N a ) do 53) w a (k) = w a (k) + w a (k) via (36) and (41); // update the f(x(k)), u(k), s(k), J (k), E r (k), cyc correspondingly 54) f (x(k)) ⇐ ActNet (x(k), w a (k)); 55) u(k) ⇐ Filter x d , x(k), f (x(k)) ; 56) s(k) ⇐ RefNet (x(k), u(k), w r (k)); 57) J (k) ⇐ CritNet (x(k), u(k), s(k), w c (k)); 58) E a (k) = 1 2 (α J (k) + f (x(k) -U c )) 2 ; 59) cyc = cyc + 1; 60)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and x i (k) ∈ is the state value for the i th dimension at time instance k. u(k) is the control action, and d(k) is the disturbance bounded in [-d m , d m ], where d m is a constant value.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I SUMMARY</head><label>I</label><figDesc>OF THE PARAMETERS USED IN THE SIMULATION STUDY ONE</figDesc><table><row><cell>Para.</cell><cell>η c</cell><cell>η a</cell><cell>η r</cell><cell>K v</cell><cell>λ</cell><cell>c</cell><cell>*</cell></row><row><cell cols="4">Value 5e -3 8e -3 2.5e -3</cell><cell>0.1</cell><cell>0.2</cell><cell>1.5e -3</cell><cell>*</cell></row><row><cell>Para.</cell><cell>N c</cell><cell>N a</cell><cell>N r</cell><cell>T c</cell><cell>T a</cell><cell>T r</cell><cell>α</cell></row><row><cell>Value</cell><cell>40</cell><cell>150</cell><cell>50</cell><cell cols="4">1e -4 1e -5 1e -5 0.95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II SUMMARY</head><label>II</label><figDesc>OF THE PARAMETERS USED IN THE SIMULATION STUDY TWO</figDesc><table><row><cell>Para.</cell><cell>η c</cell><cell>η a</cell><cell>η r</cell><cell>K v</cell><cell>λ</cell><cell>c</cell><cell>*</cell></row><row><cell>Value</cell><cell>0.02</cell><cell>0.01</cell><cell>0.05</cell><cell>0.05</cell><cell>0.1</cell><cell>5e -5</cell><cell>*</cell></row><row><cell>Para.</cell><cell>N c</cell><cell>N a</cell><cell>N r</cell><cell>T c</cell><cell>T a</cell><cell>T r</cell><cell>α</cell></row><row><cell>Value</cell><cell>100</cell><cell>200</cell><cell>200</cell><cell>1e -4</cell><cell cols="2">1e -5 1e -5</cell><cell>0.95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III SUMMARY</head><label>III</label><figDesc>OF TOTAL TRACKING ERROR FOR THE SIMULATION CASES</figDesc><table><row><cell>Number</cell><cell cols="2">Example 1 Example 2</cell><cell>Ball and Beam</cell></row><row><cell>DualCritHDP</cell><cell>7.454</cell><cell>9.292</cell><cell>3.761</cell></row><row><cell>HDP</cell><cell>223.9</cell><cell>712.8</cell><cell>5 4 .49</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the National Science Foundation under grant CAREER ECCS 1053717 and grant CNS 1117314, the Army Research Office under grant W911NF-12-1-0378, the Defense Advanced Research Projects Agency under grant FA8650-11-1-7148, and the National Natural Science Foundation of China under grant 51228701.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>He is an active member of the IEEE Power &amp; Energy Society and Industry Application Society.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Reinforcement Learning and Approximate Dynamic Programming for Feedback Control</title>
		<editor>F. Lewis and D. Liu</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>IEEE Press</publisher>
			<pubPlace>Piscataway, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Handbook of Learning and Approximate Dynamic Programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<editor>W. B. Powell, and D. C. Wunsch</editor>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>IEEE Press</publisher>
			<pubPlace>Piscataway, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Dynamic Programming and Optimal Control</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Athena Scientific</publisher>
			<pubPlace>Belmont, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Approximate Dynamic Programming: Solving the Curses of Dimensionality</title>
		<author>
			<persName><forename type="first">W</forename><surname>Powell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive dynamic programming</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Lendaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Saeks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man, Cybern. C, Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="140" to="153" />
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Approximate dynamic programming strategies and their applicability for process control: A review and future directions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Control Autom. Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="263" to="278" />
			<date type="published" when="2004-09">Sep. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Self-Adaptive Systems for Machine Intelligence</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reinforcement learning and approximate dynamic programming (RLADP)-foundations, common misconceptions and challenges ahead</title>
		<author>
			<persName><forename type="first">P</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Reinforcement Learning and Approximate Dynamic Programming for Feedback Control</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Intelligence in the brain: A theory of how it works and how to build it</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="200" to="212" />
			<date type="published" when="2009-04">Apr. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ADP: The key direction for future research in intelligent control and understanding brain intelligence</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="898" to="900" />
			<date type="published" when="2008-08">Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive critic designs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wunsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="997" to="1007" />
			<date type="published" when="1997-09">Sep. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive dynamic programming: An introduction</title>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Intell. Mag</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="39" to="47" />
			<date type="published" when="2009-05">May 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Online learning control by association and reinforcement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="264" to="276" />
			<date type="published" when="2001-03">Mar. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Helicopter trimming and tracking control using direct neural dynamic programming</title>
		<author>
			<persName><forename type="first">R</forename><surname>Enns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="929" to="939" />
			<date type="published" when="2003-07">Jul. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reinforcement learning control based on multi-goal representation using hierarchical heuristic dynamic programming</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Prokhorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf</title>
		<meeting>Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Comparison of heuristic dynamic programming and dual heuristic programming adaptive critics for neurocontrol of a turbogenerator</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Venayagamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Wunsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="764" to="773" />
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptivecritic-based optimal neurocontrol for synchronous generators in a power system using MLP/RBF neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Venayagamoorthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Appl</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1529" to="1540" />
			<date type="published" when="2003-10">Sep.-Oct. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A novel infinte-time optimal tracking control scheme for a class of discrete-time nonlinear systems via the greedy HDP iteration algorithm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="937" to="942" />
			<date type="published" when="2008-08">Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Data-driven robust approximate optimal tracking control for unknown general nonlinear systems using adaptive dynamic programming method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2226" to="2236" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimal tracking control for a class of nonlinear discrete-time systems with time delays based on heuristic dynamic programming</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1851" to="1862" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Direct heuristic dynamic programming for nonlinear tracking conrol with filtered tracking error</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Tsakalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1617" to="1622" />
			<date type="published" when="2009-12">Dec. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reinforcement learning neural-networkbased controller for nonlinear discrete-time systems with input contraints</title>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jagannathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="425" to="436" />
			<date type="published" when="2007-04">Apr. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reinforcement-based neuro-output feedback control of nonlinear discrete-time systems with input constraints</title>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jagannathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="150" to="154" />
			<date type="published" when="2005-02">Feb. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural network-based state feedback control of nonlinear discrete-time system in non-strict feedback form</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jagannathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2073" to="2087" />
			<date type="published" when="2008-12">Dec. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real-time tracking control on adaptive critic design with uniformly ultimately bounded condition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symp. Adapt. Dynamic Program. Reinforcement Learn</title>
		<meeting>IEEE Symp. Adapt. Dynamic Program. Reinforcement Learn</meeting>
		<imprint>
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adaptive critic learning techniques for engine torque and air-fuel ratio control</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Javaherian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kovalenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="988" to="993" />
			<date type="published" when="2008-08">Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adaptive learning and control for MIMO system based on adaptive dynamic programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1133" to="1148" />
			<date type="published" when="2011-07">Jul. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An adaptive dynamic programming approach for closely-coupled mimo system control</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int. Symp</title>
		<meeting>8th Int. Symp</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="volume">6677</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An online actor-critic learning approach with levenberg-marquardt algorithm</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. Neural Netw</title>
		<meeting>Int. Joint Conf. Neural Netw</meeting>
		<imprint>
			<date type="published" when="2011-08">Aug. 2011</date>
			<biblScope unit="page" from="2333" to="2340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dual heuristic programming excitation neurocontrol for generators in a multimachine power system</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Venayagamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Wunsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ind. Appl</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="382" to="394" />
			<date type="published" when="2003-05">Apr.-May 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">DHP-based wide-area coordinating control of a power system with a large wind farm and multiple FACTS devices</title>
		<author>
			<persName><forename type="first">W</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Venayagamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Neural Netw</title>
		<meeting>IEEE Int. Conf. Neural Netw</meeting>
		<imprint>
			<date type="published" when="2007-08">Aug. 2007</date>
			<biblScope unit="page" from="2093" to="2098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Comparison of adaptive critics and classical approaches based wide area controllers for a power system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Venayagamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man, Cybern., B, Cybern</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1002" to="1007" />
			<date type="published" when="2008-08">Aug. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reactive power control of grid-connected wind farm based on adaptive dynamic programming</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A self-learning call admission control scheme for CDMA cellular networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1219" to="1228" />
			<date type="published" when="2005-09">Sep. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural-network-based nearoptimal control for a class of discrete-time affine nonlinear systems with control constraints</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1490" to="1503" />
			<date type="published" when="2009-09">Sep. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adaptive dynamic programming for finite-horizon optimal control of discrete-time nonlinear systems with ε-error bound</title>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="36" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neurocontroller alternatives for &apos;fuzzy&apos; ball-and-beam systems with nonuniform nonlinear friction</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Eaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Wunsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="423" to="435" />
			<date type="published" when="2000-03">Mar. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Simple and fast calculation of the second-order gradients for globalized dual heuristic dynamic programming in neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fairbank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Prokhorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1671" to="1676" />
			<date type="published" when="2012-10">Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Data-driven learning and control with multiple critic networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th World Congr</title>
		<meeting>10th World Congr</meeting>
		<imprint>
			<date type="published" when="2012-07">Jul. 2012</date>
			<biblScope unit="page" from="523" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Guidance in the use of adaptive critics for control</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Lendaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Neidhoefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Learning and Approximate Dynamic Programming</title>
		<meeting><address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="97" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adaptive tracking control using synthesized velocity from attitude measurements</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Queiroz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kapila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="947" to="953" />
			<date type="published" when="2001-06">Jun. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adaptive state feedback and tracking control of systems with actuator failures</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Autom. Control</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="95" />
			<date type="published" when="2001-01">Jan. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Nonlinear adaptive control using the fourier integral and its application to cstr systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Systems, Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="367" to="372" />
			<date type="published" when="2002-06">Jun. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A three-network architecture for on-line learning and optimization based on adaptive dynamic programming</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">in Reinforcement Learning and Approximate Dynamic Programming for Feedback Control</title>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>IEEE Press</publisher>
			<biblScope unit="page" from="78" to="95" />
			<pubPlace>Piscataway, NJ, USA</pubPlace>
		</imprint>
	</monogr>
	<note>Learning and optimization in hierarchical adaptive critic design</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Neural network control of robot manipulators and nonlinear systems</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jagannathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yesildirek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Network Robot Control</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Taylor &amp; Francis</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="173" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A simple solution to the bioreactor benchmark problem by application of Q-learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Feldkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Puskorius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. World Congr</title>
		<meeting>World Congr<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-07">Jul. 1995</date>
			<biblScope unit="page" from="326" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adaptive critic based neural networks for control (low order system applications)</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Biega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Amer. Control Conf</title>
		<meeting>Amer. Control Conf<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-06">Jun. 1995</date>
			<biblScope unit="page" from="335" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Stability and almost disturbance decoupling analysis of nonlinear system subject to feedback linearization and feedforward neural network controller</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1220" to="1230" />
			<date type="published" when="2008-07">Jul. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning and control in virtual reality for machine intelligence</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Int. Conf. Intell. Control Inf. Process</title>
		<meeting>3rd Int. Conf. Intell. Control Inf. ess</meeting>
		<imprint>
			<date type="published" when="2012-07">Jul. 2012</date>
			<biblScope unit="page" from="63" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName><surname>Online</surname></persName>
		</author>
		<ptr target="http://www.youtube.com/watch?v=OeZEDBz6ki0&amp;feature=youtu.be" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A boundedness result for the direct heuristic dynamic programming</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="229" to="235" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Applied Nonlinear Control</title>
		<author>
			<persName><forename type="first">J</forename><surname>Slotine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Khalil</surname></persName>
		</author>
		<title level="m">Nonlinear Systems</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>rd ed</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
