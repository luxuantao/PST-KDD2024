<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Consensus Group Stable Feature Selection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Steven</forename><surname>Loscalzo</surname></persName>
							<email>sloscal1@binghamton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Binghamton University</orgName>
								<address>
									<postCode>13902</postCode>
									<settlement>Binghamton</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Binghamton University</orgName>
								<address>
									<postCode>13902</postCode>
									<settlement>Binghamton</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chris</forename><surname>Ding</surname></persName>
							<email>chqding@uta.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Texas</orgName>
								<address>
									<postCode>76019</postCode>
									<settlement>Arlington Arlington</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Consensus Group Stable Feature Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E71765C9744E794DABE55CD96662D3AB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.2.8 [Database Management]: Database Applicationsdata mining; I.2.6 [Artificial Intelligence]: Learning Feature selection</term>
					<term>stability</term>
					<term>ensemble</term>
					<term>high-dimensional data</term>
					<term>small sample</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Stability is an important yet under-addressed issue in feature selection from high-dimensional and small sample data. In this paper, we show that stability of feature selection has a strong dependency on sample size. We propose a novel framework for stable feature selection which first identifies consensus feature groups from subsampling of training samples, and then performs feature selection by treating each consensus feature group as a single entity. Experiments on both synthetic and real-world data sets show that an algorithm developed under this framework is effective at alleviating the problem of small sample size and leads to more stable feature selection results and comparable or better generalization performance than state-of-the-art feature selection algorithms. Synthetic data sets and algorithm source code are available at http://www.cs.binghamton.edu/∼lyu/KDD09/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>High-dimensional small-sample data is common in biological applications like gene expression microarrays <ref type="bibr">[8]</ref> and proteomics mass spectrometry <ref type="bibr" target="#b20">[20]</ref>. Classification on such data is challenging due to the two distinct data characteristics: high dimensionality and small sample size. Many feature selection algorithms have been developed with a focus on improving classification accuracy while reducing dimensionality for such data <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b22">22]</ref>. The issues of feature relevance and redundancy have also been well studied <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b27">27]</ref>. A relatively neglected issue is the stability of feature selection -the insensitivity of the result of a feature selection algorithm to variations in the training set. This issue is important in many applications where feature selection is used as a knowledge discovery tool for identifying characteristic markers for the observed phenomena <ref type="bibr" target="#b19">[19]</ref>. For example, in microarray data analysis, a feature selection algorithm may select largely different subsets of features (genes) under variations to the training data, although most of these subsets are as good as each other in terms of classification performance <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b26">26]</ref>. Such instability dampens the confidence of domain experts in investigating any of the various subsets of selected features for biomarker identification.</p><p>In this paper, we demonstrate that stability of feature selection has a strong dependency on sample size. Moreover, we show that exploiting intrinsic feature groups in the underlying data distribution is effective at alleviating the effect of small sample size for high-dimensional data. Therefore, we propose a novel feature selection framework (as shown in Figure <ref type="figure" target="#fig_0">1</ref>) which approximates intrinsic feature groups by a set of consensus feature groups and performs feature selection and classification in the transformed feature space described by consensus feature groups.</p><p>Our framework is motivated by a key observation that intrinsic feature groups (or groups of correlated features) commonly exist in high-dimensional data, and such groups are resistant to the variations of training samples. For example, genes normally function in co-regulated groups, and such intrinsic groups are independent to the set of observed microarray samples. Moreover, the set of intrinsic feature groups can be approximated by a set of consensus feature groups obtained from subsampling of the training samples. Another observation is that treating each feature group as a single entity and performing learning at the group level allows the ensemble effect of each feature group to offset the random relevance variation of its group members. Intuitively, it is less likely for a group of irrelevant features to exhibit the same trend of correlation to the class (hence, showing artificial relevance) than for each group member to gain some correlation to the class under random subsampling, unless all features in the group are perfectly correlated. Therefore, discriminating relevant groups from irrelevant ones based on group relevance is less prone to overfitting than detection of relevant features on small samples. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, there are two new issues in consensus group based feature selection: <ref type="bibr" target="#b1">(1)</ref> identifying consensus feature groups from the given training data, and (2) representing each feature group by a single entity so that feature selection and classification can be performed on the transformed feature space. In our previous work <ref type="bibr" target="#b26">[26]</ref>, we developed an algorithm, DRAGS, which identifies dense feature groups in the sample space and uses a representative feature from each group in the subsequent feature selection and classification steps. The algorithm has shown some promising results w.r.t. the stability of the dense groups and the generalization ability of the selected features. However, there are two major limitations about DRAGS. First, DRAGS tries to identify dense feature groups in the sample space with dimensionality as high as dozens or a few hundreds (of samples) which makes density estimation difficult and unreliable. As a result, the feature groups found are not always stable under training data variations. Second, DRAGS faces the density vs. relevance dilemma -it limits the selection of relevant groups from dense groups for better stability of the selection results, however, it will miss some relevant features if those features are located in the relatively sparse regions. The new framework of consensus group based feature selection addresses these two issues.</p><p>The main contributions of this paper are: (i) conducting an in-depth study on the sample size dependency for the stability of feature selection; (ii) proposing a novel framework of consensus group based feature selection which alleviates the problem of small sample size; and (iii) developing a novel algorithm under this framework which overcomes the limitations of DRAGS. Experiments on both synthetic and real-world data sets show that the new algorithm leads to more stable feature selection results and comparable or better generalization performance than state-of-the-art feature selection algorithms DRAGS and SVM-RFE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SAMPLE SIZE DEPENDENCY</head><p>High-dimensional data with small samples permits too large a hypothesis space yet too few constraints (samples), which makes learning on such data very difficult and prone to model overfitting. In order to find a probably approximately correct (PAC) hypothesis, PAC learning theory <ref type="bibr" target="#b12">[12]</ref> gives a theoretic relationship between the number of samples needed in terms of the size of hypothesis space and the number of dimensions. For example, a binary data set with binary classes has a hypothesis space of size 2 2 n where n is the dimensionality. It would require O(2 n ) samples to learn a PAC hypothesis without any inductive bias <ref type="bibr" target="#b17">[17]</ref>.</p><p>Feature selection <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b13">13]</ref> is one effective approach to reducing dimensionality -finding a subset of features from the original features. The reduction of dimensionality results in an exponential shrinkage of the hypothesis space, and hence reduces the chance of model overfitting and improves the generalization of classification algorithms <ref type="bibr" target="#b18">[18]</ref>. However, feature selection itself is a challenging problem and receives increasing and intensified attention <ref type="bibr" target="#b16">[16]</ref>. The shortage of samples in high-dimensional data increases the difficulty in finding relevant features, and reduces the stability of feature selection results under variations of training samples.</p><p>We next illustrate based on synthetic data that successful detection of relevant features and the stability of feature selection results can have a strong dependency on the sample size. The merit of using synthetic data for illustration is two-fold. First, it allows us to examine the sample size dependency using training sets with a wide range of sample sizes and other properties being equal; second, it provides us prior knowledge about truly relevant features.</p><p>The data set used here consists of 1000 training samples randomly drawn from the same distribution P (X, Y ). The feature set X consists of 1000 features, including 100 mutually independent features, X1, X2, ..., X100, and a number of (10 ± 5) highly correlated features to each of these 100 features. Within each correlated group, the Pearson correlation of each feature pair is within (0.5,1), and the average pairwise correlation is below 0.75. The balanced binary class label Y is decided based on X1, X2, ..., X10 using a linear function of equal weight to these 10 truly relevant features.</p><p>We study SVM-RFE <ref type="bibr" target="#b9">[9]</ref>, an algorithm well known for its excellent generalization performance on high-dimensional small-sample data. The main process of SVM-RFE is to recursively eliminate features based on SVM, using the coefficients of the optimal decision boundary to measure the relevance of each feature. At each iteration, it trains a linear SVM classifier, and eliminates one or more features with the lowest weights. We apply SVM-RFE on the above training set with sample size 1000, and its three randomly drawn subsets of training samples with decreasing sample sizes 500, 200, and 100, in order to observe the sample size dependency of SVM-RFE w.r.t. successful detection of relevant features and the stability of the selected feature subsets.</p><p>To evaluate the stability of SVM-RFE for a given training set, we can simulate training data variation by a resampling procedure like bootstrapping or N-fold cross-validation. We choose 10-fold cross-validation. For each training set, SVM-RFE is repeatedly applied to 9 out of the 10 folds, while a different fold is hold out each time. The stability of SVM-RFE is calculated based on the average pair-wise subset similarity of the top 10 features (the optimal number of features) selected over the 10 folds. To evaluate the effectiveness of SVM-RFE in detecting relevant features, the average precision of the top 10 features w.r.t the 10 truly relevant features over the 10 folds is also calculated.</p><p>To illustrate the effectiveness of feature selection based on intrinsic feature groups, we exploit the prior knowledge  of existing feature groups in the synthetic data sets and replace each known feature group by its representative feature (the one closest to the group center). We then apply the SVM-RFE algorithm and the simple F -Statistic ranking to each transformed data set to selects the top 10 representative features, respectively. The group-based algorithms are evaluated in the same experimental setting as SVM-RFE.</p><p>Figure <ref type="figure" target="#fig_2">2</ref> shows the precision and stability of the selected top 10 features by SVM-RFE and the group-based SVM-RFE across different training sample sizes. The results of group-based F -Statistic ranking are almost the same as groupbased SVM-RFE, and hence are not shown in the figures. Clearly, SVM-RFE shows a strong dependency on the training sample size w.r.t. successful detection of the truly relevant features as well as the stability of the selected features in this example. When the sample size reduces from 1000 to 100, both precision and stability curves drop sharply. In contrast, the group-based algorithm shows much less dependency on the sample size, especially when sample size is over 200. Moreover, the group-based algorithm consistently outperforms SVM-RFE. Such observations suggest that intrinsic feature groups are stable under training sample variations even at small sample size, and discriminating relevant features from irrelevant ones at the group level is more effective than at the feature level on small samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GROUP BASED FEATURE SELECTION</head><p>The study in the previous section illustrates the effectiveness of feature selection based on intrinsic feature groups in the underlying data distribution in an ideal situation. In practice, it is a challenging problem to identify intrinsic feature groups from a small training set due to the shortage of samples to observe feature correlation. In this section, we first review a previously proposed framework of dense group based feature selection and the DGF and DRAGS algorithms. We then describe the details of the new framework of consensus group based feature selection (as outlined in Figure <ref type="figure" target="#fig_0">1</ref>) and a new algorithm under this framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dense Group Based Feature Selection</head><p>The dense group based framework is motivated by a key observation that in the sample space, the dense core regions (peak regions), measured by probabilistic density estimation, are stable with respect to sampling of the dimensions (samples). For example, a spherical Gaussian distribution in the 100-dimensional space will likely be a stable spherical Gaussian in any of the subspaces. The features near the core Algorithm 1 DGF (Dense Group Finder)</p><formula xml:id="formula_0">Input: data D = {xi} n i=1 , kernel bandwidth h Output: dense feature groups G 1 , G 2 , . . . , G L for i = 1 to n do Initialize j = 1, y i,j = x i repeat Compute y i,j+1</formula><p>according to (1) until convergence Set stationary point y i,c = y i,j+1 (make y i,c a peak p i ) Merge peak p i with its closest peak if their distance &lt; h end for For every unique peak p r , add</p><formula xml:id="formula_1">x i to G r if ||p r -x i || &lt; h</formula><p>of the spherical Gaussian, viewed as a core group are likely to be stable under sampling, although exactly which feature is closest to the peak could vary <ref type="bibr" target="#b26">[26]</ref>.</p><p>Given a training set D composed of n features and m samples, the data matrix is transposed such that original feature vectors become data points in the new feature space defined by the original samples. Algorithm 1, DGF (Dense Group Finder) was introduced in <ref type="bibr" target="#b26">[26]</ref> as a means to locate dense feature groups from data. DGF works by employing kernel density estimation <ref type="bibr" target="#b24">[24]</ref> and the iterative mean shift procedure <ref type="bibr" target="#b4">[4]</ref> on each of the features in the sample space. When the mean shift process converges, nearby features are gathered into feature groups and returned by the algorithm.</p><p>The main part of DGF is the iterative mean shift procedure for all features, which locates a density peak by starting the mean at a given feature x i and using other features in the local neighborhood (determined by a kernel bandwidth h) to shift the mean to a denser location. Specifically</p><formula xml:id="formula_2">y j+1 = P n i=1 xiK( y j -x i h ) P n i=1 K( y j -x i h ) j = 1, 2, ... (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>is used to determine the sequence of successive locations of the kernel K. The algorithm has a time complexity of O(λn 2 m), where λ is the number of iterations for each mean shift procedure to converge. Details of the algorithm and the choice of kernel function K and bandwidth are given in <ref type="bibr" target="#b26">[26]</ref>.</p><p>The groups found by DGF may or may not be relevant, and so these groups must be processed by a second algorithm DRAGS (Dense Relevant Attribute Group Selector) which relies on DGF to find dense feature groups and then evaluates and ranks the relevance of each feature group based on the average relevance (F -Statistic score) of features in each group. A representative feature (the one closest to the group center) from each selected top relevant group will be used for classification.</p><p>While the DGF and DRAGS algorithms have shown some promise w.r.t. the stability of the dense groups and the generalization performance of the selected features, the dense group based framework has two major limitations. In the first step, as mentioned in the Introduction, density estimation can be unreliable in high-dimensional spaces. Since the dimensionality of the feature space for estimating density peaks is as high as dozens or a few hundreds (i.e., the training sample size), the identified peaks are susceptible to variations of the dimensions (samples), and the stability of the identified dense groups suffer accordingly. Moreover, the overall sample size is still much smaller than the sample distribution, which can add to the instability of the groups </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Consensus Group Based Feature Selection</head><p>The consensus group based framework first approximates intrinsic feature groups by a set of consensus feature groups, and then performs feature selection in the transformed feature space described by consensus feature groups. We next describe each component in detail, and present a new algorithm CGS (Consensus Group Stable feature selection) which instantiates the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Identifying Consensus Groups</head><p>In practice, it is a challenging problem to identify intrinsic feature groups from a small training set due to the shortage of samples to observe feature correlation. Feature groups found on small samples can be suboptimal and instable under training sample variations. Our idea of approximating intrinsic feature groups by a set of consensus feature groups aggregated from multiple sets of feature groups originates from ensemble learning. It is well known that ensemble methods <ref type="bibr" target="#b2">[2]</ref> for classification which aggregate the predictions of multiple classifiers can achieve better generalization than a single classifier, if the ensemble of classifiers are correct and diverse. Similar to the idea of ensemble classification, ensemble clustering methods <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b23">23]</ref> have also be extensively studied, which aggregate clustering results from multiple clustering algorithms or from the same clustering algorithm under data manipulation. Although finding consensus feature groups can be considered as ensemble feature clustering, to the best of our knowledge, this is the first time that ensemble learning is applied to identify consensus feature groups for stable feature selection.</p><p>Similar to ensemble construction in classification and clustering, there are two essential steps in identifying consensus feature groups: Step (1), to create an ensemble of feature grouping results, and Step (2), to aggregate the ensemble into a single set of consensus feature groups. Algorithm 2, CGS , describes the key procedure for each of the two steps. In Step (1) CGS adopts the DGF algorithm introduced before as the base algorithm, and applies DGF on a number of (user-defined parameter t) bootstrapped training sets from a given training set D. The result of this step is an ensemble of feature groupings, {G 1  1 , . . . , G In Step <ref type="bibr" target="#b2">(2)</ref>, it is a non-trivial issue to aggregate a given ensemble of feature groupings {G 1  1 , . . . , G 1 L 1 , . . . , G t 1 , . . . , G t L t } into a final set of consensus groups {CG1, . . . , CGL}, where CG i is a consensus group. This issue resembles the wellstudied cluster ensemble problem -combining a given ensemble of clustering solutions into a final solution <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b23">23]</ref>. Previously, Strehl and Ghosh <ref type="bibr" target="#b23">[23]</ref> proposed two approaches, instance-based or cluster-based, to formulate the cluster ensemble problem. The instance-based approach models each instance as an entity and decides the similarity between each pair of instances based on how frequently they are clustered together among all clustering solutions. The cluster-based approach models each cluster in the ensemble as an entity, and decides the similarity between each pair of clusters based on the percentage of instances they share. Given the similarity matrix for all pairs of entities in either approach, a final clustering can be produced based on any hierarchical or graph clustering algorithm.</p><p>In this work, we chose the instance-based approach for the proposed CGS algorithm for two reasons. First, this approach is more efficient than the cluster-based approach w.r.t. both computation and space as the number of entities in the instance-based approach (i.e., the number of features) is often much smaller than the number of entities in the cluster-based approach (i.e., the total number of feature groups P t i=1 Li in the ensemble) under the experimental settings described in the following section. Second, our preliminary evaluation of both approaches shows that the consensus groups formed by the instance-based approach are consistently more stable than the cluster-based approach. Once Wi,j for every feature pairs is computed, the CGS algorithm applies agglomerative hierarchical clustering to group features into a final set of consensus feature groups. To reduce the effect of outliers, we use average linkage in deciding the similarity between two groups to be merged. The merging process continues as long as the two feature groups to be merged has a similarity value &gt; 0.5, indicating, on average, the feature pairs in the resulting group are also grouped together by a majority of the DGF runs. The use of majority voting provides a natural stopping criterion for deciding the final number of feature groups. The time complexity for Step (2) is O(n 2 t + n 2 logn).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Feature Selection based on Consensus Groups</head><p>Consensus groups found by CGS can still be comprised of irrelevant features, so, CGS continues to identify relevant groups from the consensus feature groups. CGS works by first forming a representative feature for each consensus feature group, and then evaluates the relevance of each feature group based on its representative feature. In our implementation, we use the feature closest to the group center to represent the group. Different feature selection algo-rithms and relevance measures can be adopted in the same framework to select relevant feature groups since each group has been represented by a single entity. In this work, since our investigative emphasis is on the effectiveness of consensus feature groups for stable feature selection, we use the simple method of individual feature evaluation based on F -Statistic to determine the group relevance in CGS, as we did for DRAGS in <ref type="bibr" target="#b26">[26]</ref>. However, there are two key differences between CGS and DRAGS. First, CGS relies on consensus feature groups, while DRAGS relies on dense feature groups. Second, CGS considers all consensus groups during the relevance selection step, while DRAGS limits the selection of relevant groups to the top dense groups. Therefore, CGS addresses two key limitations of DRAGS discussed previously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EMPIRICAL STUDY</head><p>In this section, we empirically study the framework of stable feature selection based on consensus feature groups. Section 4.1 introduces stability measures, Section 4.2 describes the data sets used and experimental procedures, and Section 4.3 presents results and discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Stability Measures</head><p>Evaluating the stability of feature selection algorithms requires some similarity measures for two sets of feature selection results. In our previous work <ref type="bibr" target="#b26">[26]</ref>, we introduced a general similarity measure for two sets of feature selection results</p><formula xml:id="formula_4">R 1 = {G i } |R 1 | i=1 and R 2 = {G j } |R 2 |</formula><p>j=1 , where R 1 and R2 can be either two sets of features or two sets of feature groups. R1 and R2 together are modeled as a weighted bipartite graph G = (V, E), with vertex partition V = R 1 ∪R 2 , and edge set</p><formula xml:id="formula_5">E = {(G i , G j )|G i ∈ R 1 , G j ∈ R 2 },</formula><p>and weight w (G i ,G j ) associated with each pair (G i , G j ). The overall similarity between R1 and R2 is defined as:</p><formula xml:id="formula_6">Sim(R 1 , R 2 ) = P (G i ,G j )∈M w (G i ,G j ) |M | , (<label>2</label></formula><formula xml:id="formula_7">)</formula><p>where M is a maximum matching in G (i.e., a subset of non-adjacent edges in E with largest sum of weights). Depending on how to decide w (G i ,G j ) , the measure has two forms: SimID and SimV , where the subscripts ID and V respectively indicate that each weight is decided based on feature indices or feature values. When G i and G j represent feature groups, for Sim ID , each weight w (G i ,G j ) is decided by the overlap between the two feature groups,</p><formula xml:id="formula_8">w (G i ,G j ) = 2|G i ∩ G j | |G1| + |G2| . (<label>3</label></formula><formula xml:id="formula_9">)</formula><p>For SimV , each weight is decided by the Pearson correlation coefficient between the centers of the two feature groups. In the special case when G i and G j only contain one feature, for Sim ID , w (G i ,G j ) = 1 for matching features and 0 otherwise; for Sim V , each weight can be simply decided by the correlation coefficient between the two individual features. Given the similarity measure, the stability of a feature selection algorithm is then measured as the average similarity of various feature selection results produced by the same algorithm under training data variations. In <ref type="bibr" target="#b26">[26]</ref>, we used the feature selection result from a full training set as a reference to compare various results under subsampling of the full training set. Although the procedure is more efficient than pair-wise comparison among various results, the evaluation result can be biased since the individual difference between two sets of results can be greater than their difference to a reference set. We use pair-wise similarity comparison in stability calculation in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>We perform our study on both synthetic data sets and real-world data sets. Besides the synthetic data set used in Section 2, we also create another data set with higher dimensionality and larger feature groups. A summary of these data sets is provided in Table <ref type="table" target="#tab_2">1</ref>. To assure comparable results, we follow the same procedure in generating both data sets as described in Section 2. For each data set, four different training sample sizes (100, 200, 500, and 1000) will be used to study the sample size dependency of the group-based algorithms as in Section 2. For real-world data, we use six frequently studied public microarray data sets characterized in Table <ref type="table" target="#tab_3">2</ref>.  To empirically evaluate the stability and accuracy of SVM-RFE, DRAGS, and CGS on a given data set, we apply the 10 fold cross-validation procedure. For each microrray data set described above, each feature selection algorithm is repeatedly applied to 9 out of the 10 folds, while a different fold is hold out each time. Different stability measures are calculated. In addition, a classifier is trained based on the selected features from the same training set and tested on the corresponding hold-out fold. The CV accuracies of linear SVM and KNN classification algorithms are calculated. The above process is repeated 10 times for different random partitions of the data set, and the results are averaged. For each of the two synthetic data sets, we follow the same 10×10-fold CV procedure above with two changes. First, an independent test set of 500 samples randomly generated from the same distribution as the training set is used in replacement of the hold-out fold. Second, in addition to stability and accuracy measures, we also measure the precision w.r.t. truly relevant features during each iteration of the 10×10 CV and obtained the average values. For each performance measure, two-sample paired t-tests between the best performing algorithm and the other two algorithms is used to decide the statistical significance of the difference between the two average values over the 10 random trials.</p><p>As to algorithm settings, for SVM-RFE, we eliminate 10 percent of the remaining features at each iteration. We use Weka's implementation <ref type="bibr" target="#b25">[25]</ref> of SVM (linear kernel, default C parameter) and KNN (K=1). For DRAGS, the selection of relevant groups is limited to the top 50 dense feature groups. The kernel bandwidth parameter h for the base DGF algorithm is set to be the average of the average distance to its K-nearest neighbors for all features. As discussed in <ref type="bibr" target="#b26">[26]</ref>, a reasonable K value should be sufficiently small in order to capture the heterogeneity of the data. In our experiments, for each synthetic data, the K value is set to be the average group size. For each microarray data set, the K value is chosen from 3 to 5 based on the stability of DGF through cross-validation. For CGS , the number of subsampling t is set to be 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Synthetic Data</head><p>Figure <ref type="figure" target="#fig_4">3</ref> compares SVM-RFE, DRAGS, and CGS algorithms by various performance measures on the two synthetic data sets D 1k and D 5k under increasing training sample size. For SVM-RFE, the same trends as seen in Figure <ref type="figure" target="#fig_2">2</ref>, Section 2 can be observed here on both data sets. SVM-RFE shows a strong dependency on the sample size w.r.t. successful detection of the truly relevant features (as shown in the precision figures in the left column) as well as the stability of the selected features (the Sim ID figures in the middle column). The SVM accuracies based on the selected features (right column) are consistent with the precision values.</p><p>CGS also shows the same trends as the group-based algorithms which perform feature selection based on representative features of the intrinsic groups in Section 2. Our initial experimental results (not included due to space limit) showed that the consensus groups identified by the ensemble version of the DGF algorithm approximate the intrinsic groups very well on these synthetic data sets even when the sample size is small. As a consequence, CGS which performs feature selection based on representative features of the consensus groups shows a much less dependency on the sample size than SVM-RFE. A close look at Figure <ref type="figure" target="#fig_4">3</ref> shows that the performance of CGS at sample size 200 is usually as good as SVM-RFE at over twice the sample size. Such observation indicates that consensus group based feature selection is an alternative way of improving the stability of feature selection instead of increasing the sample size. It is worthy to note that in many applications, increasing the number of training samples could be impractical or very costly. For example, in gene expression microarray data, each sample is from the tissue of a cancer patient, which is usually hard to obtain and costly to perform experiments on.</p><p>The inferior performance of DRAGS may appear surprising at the first look. Such performance is due to the the fact that for each data set, different feature groups have similar density, and the ratio of relevant to irrelevant groups is low (1/9 for D 1k and 1/24 for D 5k ). Therefore, the probability that a relevant group happens to be among the top 50 dense groups and considered by DRAGS is low. If DRAGS allowed all groups found by DGF to be considered for relevant group selection, its performance would be better on these data sets since the groups found by DGF are reasonably close to the true feature groups. However, for real-world data which could have heterogenous density among various groups, the dense group based framework faces the dilemma of the tradeoff between density v.s. relevance. Allowing DRAGS to select features from low density groups may increase the selection accuracy but the low density groups are sensitive to training data variations. The consensus group based framework proposed in this work avoids the above dilemma; it does not limit the selection from dense groups, and it improves the stability of the resulting feature groups based on the ensemble mechanism. CGS in general results in significantly higher accuracy than DRAGS and SVM-RFE on two data sets, Colon and SR-BCT, and comparable results in the other data sets. For all data sets, the stability trends w.r.t. Sim V measure (in Section 4.1) are consistent with those w.r.t SimID, and the accuracy trends from KNN are in general similar to SVM. Due to space limit, curves for Sim V and KNN accuracy are not reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Microarray Data</head><p>Although CGS is computationally more costly than DRAGS and SVM-RFE, the payoff of significantly improved stability makes CGS a valuable tool for biologists who seek to identify not only highly predictive features but also stable feature groups. Such feature groups provide valuable insights about how relevant features are correlated, and may suggest high-potential candidates for biomarker detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>There exist very limited studies on the stability of feature selection algorithms. An early work in this direction was done by <ref type="bibr" target="#b11">Kalousis et al. (2007)</ref> . Their work raised the issue of feature selection stability and compared the stability of a number of conventional feature selection algorithms under training data variation based on three stability measures on high-dimensional data. More recently, two approaches were proposed to explicitly achieve stable feature selection without sacrificing classification accuracy: the dense group based feature selection in our previous work <ref type="bibr" target="#b26">[26]</ref>, and ensemble feature selection <ref type="bibr" target="#b21">[21]</ref>. In the later, Saeys, et al. studied ensemble feature selection which aggregates the feature selection results from a conventional feature selection algorithm such as SVM-RFE repeatedly applied on different bootstrapped samples of the same training set. Their results show that the stability of ensemble SVM-RFE does not improve significantly from single run of SVM-RFE. Our ensemble approach in the proposed feature selection framework is different as it applies the idea of ensemble learning to identify consensus feature groups instead of consensus feature rankings or feature subsets. We also evaluated ensemble SVM-RFE and observed a similar trend as in <ref type="bibr" target="#b21">[21]</ref> in our initial study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this paper, we have studied the sample size dependency for stability of feature selection. We have proposed a novel consensus group based framework for stable feature selection. Experiments on both synthetic and real-world data sets show that the CGS algorithm is effective at alleviating the problem of small sample size, and the algorithm in general leads to more stable feature selection results and comparable or better generalization performance than two state-of-the-art feature selection algorithms, DRAGS and SVM-RFE. Future work is planned to investigate different ensemble methods for identifying consensus feature groups, for example, different ways of generating and aggregating ensemble based on DGF algorithm or other robust clustering algorithms.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A framework of consensus group based feature selection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Precision (a) and stability (b) of the selected features by SVM-RFE and group-based SVM-RFE on various training sample sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figures 4</head><label>4</label><figDesc>Figures 4 and 5 compare SVM-RFE, DRAGS, and CGS by various performance measures on the six microarray data sets used in this study. Figures in the left column compare the stability of CGS and DRAGS w.r.t. the similarity of the selected features groups. CGS shows significantly better stability than DRAGS for all six data sets except Leukemia and Lung. This verifies the effectiveness of the ensemble mechanism of CGS at stabilizing the feature groups produced by the DGF algorithm. Figures in the middle column compare the stability of CGS , DRAGS, and SVM-RFE, w.r.t. the similarity of the selected features. CGS is significantly better than DRAGS for all six data sets except Leukemia. Overall, the stability of CGS is the best among all three algorithm in comparison. Figures in the right column compare the SVM accuracy of the three algorithms.CGS in general results in significantly higher accuracy than DRAGS and SVM-RFE on two data sets, Colon and SR-BCT, and comparable results in the other data sets. For all data sets, the stability trends w.r.t. Sim V measure (in Section 4.1) are consistent with those w.r.t SimID, and the accuracy trends from KNN are in general similar to SVM. Due to space limit, curves for Sim V and KNN accuracy are not reported.Although CGS is computationally more costly than DRAGS and SVM-RFE, the payoff of significantly improved stability makes CGS a valuable tool for biologists who seek to identify not only highly predictive features but also stable feature groups. Such feature groups provide valuable insights about how relevant features are correlated, and may suggest high-potential candidates for biomarker detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of SVM-RFE, DRAGS, and CGS on synthetic data sets. Figures in the left, middle, and right columns respectively show the precision w.r.t. the truly relevant features, the stability w.r.t. Sim ID of the selected representative features, and the SVM classification accuracy, for the top 10 selected representative features under increasing sample size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of SVM-RFE, DRAGS, and CGS on Colon and Leukemia microarray data sets. Figures in the left, middle, and right columns respectively show the stability w.r.t. Sim ID of the selected feature groups, the stability w.r.t. Sim ID of the selected representative features, and the SVM classification accuracy, for various numbers of selected features (or groups).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of SVM-RFE, DRAGS, and CGS on Lung, Prostate, Lymphoma and SRBCT microarray data sets. Figures in the left, middle, and right columns respectively show the stability w.r.t. SimID of the selected feature groups, the stability w.r.t. Sim ID of the selected representative features, and the SVM classification accuracy, for various numbers of selected features (or groups).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 2 CGS (Consensus Group Stable Feature Selection Input: data D, # of subsampling t, relevance measure Φ Output: selected relevant consensus feature groups CG1, CG2, . . . , CG k // Identifying consensus groups for i = 1 to t do Construct bootstrapped training set D i from D Obtain dense feature groups by DGF (Di, h) end for for every pair of features X i and X j ∈ D Set W i,j =frequency of X i and X j grouped together/t end for Create consensus groups CG1, CG2, . . . , CGL by performing hierarchical clustering of all features based on Wi,j // Feature selection based on consensus groups for i = 1 to l do Obtain a representative feature Xi from CGi Measure relevance Φ(Xi) end for Rank CG 1 , CG 2 , . . . , CG L according to Φ(X i ) Select top k most relevant consensus groups found under training sample variations. In the second step, the framework limits the selection of relevant groups from dense groups, and will miss some relevant features if those features are located in the relatively sparse regions. The new framework proposed next addresses these two issues.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1 L 1 , . . . , G t 1 , . . . , G t L t }, where G i j represents the j-th feature group formed in the ith DGF run. This straightforward step has time complexity O(tλn 2 m) as the base DGF algorithm has time complexity O(λn 2 m).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Summary of Synthetic Data Sets.</figDesc><table><row><cell cols="2">Data Set Features</cell><cell>Feature Groups</cell><cell>Truly Rel. Feat.</cell></row><row><cell>D 1k</cell><cell>1000</cell><cell>100 (size 10 ± 5)</cell><cell>10</cell></row><row><cell>D 5k</cell><cell>5000</cell><cell>250 (size 20 ± 5)</cell><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Summary of Microarray Data Sets</figDesc><table><row><cell>Data Set</cell><cell cols="3">Genes Samples Classes</cell></row><row><cell>Colon</cell><cell>2000</cell><cell>62</cell><cell>2</cell></row><row><cell>Leukemia</cell><cell>7129</cell><cell>72</cell><cell>2</cell></row><row><cell>Prostate</cell><cell>6034</cell><cell>102</cell><cell>2</cell></row><row><cell>Lung</cell><cell>12533</cell><cell>181</cell><cell>2</cell></row><row><cell>Lymphoma</cell><cell>4026</cell><cell>62</cell><cell>3</cell></row><row><cell>SRBCT</cell><cell>2308</cell><cell>63</cell><cell>4</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>The authors would like to thank anonymous reviewers for their helpful comments. C. Ding is partially supported by NSF-CCF-0830780 and NSF-DMS-0844497.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Redundant feature elimination for multi-class problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Appice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ceci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rawles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Machine learning</title>
		<meeting>the 21st International Conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An empirical comparison of voting classification algorithms: Bagging, boosting, and variants</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="105" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Minimum reference set based feature selection for small sample classifications</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Joeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mean shift, mode seeking, and clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="790" to="799" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Minimum redundancy feature selection from microarray gene expression data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computational Systems Bioinformatics conference (CSB&apos;03)</title>
		<meeting>the Computational Systems Bioinformatics conference (CSB&apos;03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="523" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Random projection for high-dimensional data clustering: a cluster ensemble approach</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Z</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brodley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twentieth International Conference on Machine Learning</title>
		<meeting>the twentieth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="186" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An extensive empirical study of feature selection metrics for text classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Forman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1289" to="1305" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Molecular classification of cancer: class discovery and class prediction by gene expression monitoring</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Slonim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tamayo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="page" from="531" to="537" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gene selection for cancer classification using support vector machines</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barnhill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="389" to="422" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ensemble feature ranking</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cornuejols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marchiori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th European Conference on Principles and Practice of Knowledge Discovery in Databases</title>
		<meeting>the 8th European Conference on Principles and Practice of Knowledge Discovery in Databases</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="267" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stability of feature selection algorithms: a study on high-dimensional spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kalousis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Prados</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hilario</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="95" to="116" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An Introduction to Computational Learning Theory</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Vazirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wrappers for feature subset selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="273" to="324" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A comparative study of feature selection and multiclass classification methods for tissue classification based on gene expression</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ogihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2429" to="2437" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A comparative study on feature selection and classification methods using gene expression profiles and proteomic patterns</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Informatics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="51" to="60" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Toward integrating feature selection algorithms for classification and clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="491" to="502" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m">Machine Learning</title>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On feature selection: learning with exponentially many irrelevant features as training examples</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Conference on Machine Learning</title>
		<meeting>the Fifteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="404" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Phases of biomarker development for early detection of cancer</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Pepe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Natl Cancer Inst</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="1054" to="1060" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Use of proteomic patterns in serum to identify ovarian cancer</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Petricoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Ardekani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Hitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="page" from="572" to="577" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust feature selection using ensemble feature selection techniques</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Saeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Abeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">V</forename><surname>Peer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ECML Confernce</title>
		<meeting>the ECML Confernce</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="313" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Supervised feature selection via dependence estimation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="823" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cluster ensembles -a knowledge reuse framework for combining multiple partitions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="583" to="617" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Kernel Smoothing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Wand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Chapman and Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<title level="m">Data Mining -Pracitcal Machine Learning Tools and Techniques</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stable feature selection via dense feature groups</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Loscalzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Knowledge Discovery and Data Mining (KDD-08)</title>
		<meeting>the 14th ACM International Conference on Knowledge Discovery and Data Mining (KDD-08)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="803" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient feature selection via analysis of relevance and redundancy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1205" to="1224" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
