<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-12">12 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kabir</forename><surname>Ahuja</surname></persName>
							<email>t-kabirahuja@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shanu</forename><surname>Kumar</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft R&amp;D</orgName>
								<address>
									<settlement>Hyderabad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sandipan</forename><surname>Dandapat</surname></persName>
							<email>sadandap@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft R&amp;D</orgName>
								<address>
									<settlement>Hyderabad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Monojit</forename><surname>Choudhury</surname></persName>
							<email>monojitc@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi Task Learning For Zero Shot Performance Prediction of Multilingual Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-12">12 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2205.06130v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Massively Multilingual Transformer based Language Models have been observed to be surprisingly effective on zero-shot transfer across languages, though the performance varies from language to language depending on the pivot language(s) used for fine-tuning. In this work, we build upon some of the existing techniques for predicting the zero-shot performance on a task, by modeling it as a multitask learning problem. We jointly train predictive models for different tasks which helps us build more accurate predictors for tasks where we have test data in very few languages to measure the actual performance of the model. Our approach also lends us the ability to perform a much more robust feature selection, and identify a common set of features that influence zero-shot performance across a variety of tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multilingual models like mBERT <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref> and XLM-R <ref type="bibr" target="#b9">(Conneau et al., 2020)</ref> have been recently shown to be surprisingly effective for zero-shot transfer <ref type="bibr" target="#b28">(Pires et al., 2019)</ref>  <ref type="bibr" target="#b41">(Wu and Dredze, 2019)</ref>, where on fine-tuning for a task on one or a few languages, called pivots, they can perform well on languages unseen during training. The zero-shot performance however, is often not uniform across the languages and the multilingual models turn out to be much less effective for low resource languages <ref type="bibr" target="#b42">(Wu and Dredze, 2020;</ref><ref type="bibr" target="#b20">Lauscher et al., 2020)</ref> and the languages that are typologically distant from the pivots <ref type="bibr" target="#b20">(Lauscher et al., 2020)</ref>. What affects the zero-shot transfer across different languages is a subject of considerable interest and importance <ref type="bibr" target="#b19">(K et al., 2020;</ref><ref type="bibr" target="#b28">Pires et al., 2019;</ref><ref type="bibr" target="#b41">Wu and Dredze, 2019;</ref><ref type="bibr" target="#b20">Lauscher et al., 2020)</ref>, however there is little conclusive evidence and a few papers even show contradictory findings. <ref type="bibr" target="#b20">Lauscher et al. (2020)</ref> recently, showed that it is possible to predict the zero shot performance of mBERT and XLM-R on different languages by formulating it as a regression problem, with pretraining data size and typological similarities between the pivot and target languages as the input features, and the performance on downstream task as the prediction target. Along similar lines <ref type="bibr" target="#b37">Srinivasan et al. (2021)</ref> and <ref type="bibr" target="#b13">Dolicki and Spanakis (2021)</ref> explore zero-shot performance prediction with a larger set of features and different regression techniques.</p><p>However, the efficacy of these solutions are severely limited by the lack of training data, that is, the number of languages for which performance metrics are available for a given task. For instance, for most tasks in the popular XTREME-R <ref type="bibr" target="#b34">(Ruder et al., 2021)</ref> benchmark, there are data points for 7-11 languages. This not only makes zero-shot performance prediction a challenging problem, but also a very important one because for practical deployment of such multilingual models, one would ideally like to know its performance for all the languages the model is supposed to handle. As <ref type="bibr" target="#b37">Srinivasan et al. (2021)</ref> shows, accurate performance predictors can also help us build better and fairer multilingual models by suggesting data labeling strategies.</p><p>In this work, we propose multi-task learning <ref type="bibr" target="#b47">(Zhang and Yang, 2017)</ref> as an approach to mitigate training-data constraints and consequent over-fitting of the performance predictors to tasks and/or datasets. The contributions of our work are fourfold. First, we experiment with different multitask learning approaches, such as Group Lasso <ref type="bibr" target="#b46">(Yuan and Lin, 2006)</ref>, Collective Matrix Factorization <ref type="bibr" target="#b11">(Cortes, 2018)</ref>, <ref type="bibr">Multi-Task Deep Gaussian Process Regression (Bonilla et al., 2008)</ref> and Meta Agnostic Meta Learning <ref type="bibr" target="#b15">(Finn et al., 2017)</ref> for 11 tasks. We observe an overall 10% reduction in performance prediction errors compared to the best performing single-task models. The gains are even stronger when we just consider the tasks with very few data points (? 10), where we see a 20% drop in the mean absolute errors. Second, an interesting consequence of modelling this problem via multi-task learning is that we are able to predict performance on low resource languages much more accurately, where in some cases single-task approaches may perform even worse than the simple averaging baselines. Third, apart from the features used for zero-shot performance prediction in the previous work <ref type="bibr" target="#b20">(Lauscher et al., 2020;</ref><ref type="bibr" target="#b37">Srinivasan et al., 2021;</ref><ref type="bibr" target="#b13">Dolicki and Spanakis, 2021)</ref>, we also utilize metrics quantifying the quality of multilingual tokenizers as proposed in <ref type="bibr" target="#b35">(Rust et al., 2021)</ref> as features in our predictive models, which turn out to have strong predictive power for certain tasks. To the best of our knowledge, our work is the first to explore the impact of tokenizer quality specifically on zero-shot transfer. And fourth, our multi-task framework in general lends us with a much more robust selection of features affecting the zero-shot performance. This, in turn, lets us investigate the critical open question on what influences the zero-shot performances across languages more rigorously. As we shall see, our findings corroborate some of the previous conclusions, while others are extended or annulled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>Zero Shot Transfer. Multilingual models like mBERT <ref type="bibr" target="#b12">(Devlin et al., 2019)</ref> and XLM-R <ref type="bibr" target="#b9">(Conneau et al., 2020)</ref> have shown surprising effectiveness in zero-shot transfer, where fine-tuning the MMLM on a task in some source language often leads to impressive performance on the same task in other languages as well without explicitly training on them. <ref type="bibr" target="#b28">Pires et al. (2019)</ref> first observed this phenomenon for NER <ref type="bibr" target="#b38">(Tjong Kim Sang, 2002;</ref><ref type="bibr" target="#b39">Tjong Kim Sang and De Meulder, 2003;</ref><ref type="bibr" target="#b21">Levow, 2006)</ref> and POS tagging <ref type="bibr" target="#b26">(Nivre et al., 2018)</ref> tasks. Concurrently, <ref type="bibr" target="#b41">Wu and Dredze (2019)</ref> also showed this surprisingly cross lingual transfer ability of mBERT additionally on tasks like Document Classification <ref type="bibr" target="#b36">(Schwenk and Li, 2018)</ref>, Natural Language Inference <ref type="bibr" target="#b10">(Conneau et al., 2018)</ref> and Dependency Parsing <ref type="bibr" target="#b26">(Nivre et al., 2018)</ref>. Factors Affecting Zero Shot Transfer. <ref type="bibr" target="#b28">Pires et al. (2019)</ref> showed that vocabulary memorization played little role in zero-shot generalization as language pairs with little word piece overlap also exhibited impressive crosslingual performance. K et al. arrived at a similar conclusion by training BERT on an artificially generated language to zero out the word overlap with the target languages, and observed only minor drops in the performance compared to training the model on English. On the contrary <ref type="bibr" target="#b41">Wu and Dredze (2019)</ref>, observed strong correlations between the sub-word overlap and the zero-shot performance in four out of five tasks. <ref type="bibr" target="#b42">Wu and Dredze (2020)</ref> showed that mBERT performed much worse for zero-shot transfer to low resource languages (i.e., less pre-training data) than high resource ones on POS Tagging, NER and Dependency Parsing tasks. <ref type="bibr" target="#b20">Lauscher et al. (2020)</ref> also had a similar observation on tasks like XNLI and XQuAD <ref type="bibr" target="#b2">(Artetxe et al., 2020)</ref>, though they found that the zero-shot performance on NER, POS tagging and Dependency Parsing tasks might not strictly depend on the pre-training size and could be better explained by different linguistic relatedness features like syntactic and phonological similarities between the language pair. Similar dependence on the typological relatedness such as word order had also been observed by <ref type="bibr" target="#b28">Pires et al. (2019)</ref>.</p><p>Performance Prediction. Prior work has explored predicting the performance of machine learning models from unlabelled data by either measuring (dis)agreements between multiple classifiers <ref type="bibr" target="#b30">(Platanios et al., 2014</ref><ref type="bibr" target="#b29">(Platanios et al., , 2017) )</ref> or by utilizing underlying information about data distribution <ref type="bibr" target="#b14">(Domhan et al., 2015)</ref>. In the context of <ref type="bibr">NLP Birch et al. (2008)</ref> explored predicting the performance of a Machine Translation system by utilizing different explanatory variables for the language pairs. <ref type="bibr" target="#b23">Lin et al. (2019)</ref> proposed a learning to rank approach to choose transfer languages for cross lingual learning using several linguistic and dataset specific features.</p><p>Recently, there has been an interest in predicting the performance of NLP models without actually training or testing them, by formulating it as a regression problem. <ref type="bibr" target="#b43">Xia et al. (2020)</ref> showed that using experimental settings for an NLP experiment as inputs it is possible to accurately predict the performance on different languages and model architectures. <ref type="bibr" target="#b45">Ye et al. (2021)</ref> extended this work by proposing methods to do a fine-grained estimation of the performance as well as predicting well-callibrated confidence intervals. Specifically predicting the zero-shot performance of MMLMs was first explored in <ref type="bibr" target="#b20">Lauscher et al. (2020)</ref>, where they used a linear regression model to estimate the cross-lingual transfer performance based on pretraining data size and linguistic relatedness features. <ref type="bibr" target="#b37">Srinivasan et al. (2021)</ref> tackled this problem by utilizing XGBoost Regressor for the prediction along with a larger set of features. <ref type="bibr" target="#b13">Dolicki and Spanakis (2021)</ref> explored individual syntactic features for zero-shot performance prediction instead of working with aggregate similarity values, and showed about 2 to 4 times gain in performance. We extend all of these works by considering a multi-task learning approach, where performance prediction in a task utilizes not only the data available for that task, but also the patterns observed for other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Setup</head><p>We begin by defining the multi-task performance prediction problem and then describe the different linguistic and MMLM specific features used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-Task Performance Prediction Problem</head><p>Consider a pre-trained multilingual model M, trained using self supervision on a set of languages L. Let T be the set of downstream NLP tasks, P be the set of pivot (source) languages for which training data is available for the downstream tasks for fine-tuning and T be the set of target languages for which validation/test data is available. Note that P ? L and T ? L. We use the zero-shot setting similar to <ref type="bibr" target="#b20">Lauscher et al. (2020)</ref> which enforces P and T to be disjoint sets 1 , i.e., P ? T = ?.</p><p>We then define y M,t p,t ? R as the zero-shot performance on language t ? T on finetuning M on task t ? T in pivot language p ? P. Let x M p,t ? R n be the n-dimensional feature vector representing the corresponding train-test configuration. Since for our experiments we train and evaluate the performance prediction for a single model at a time, we will simplify the notations to y t p,t and x p,t . The predictor model can then be defined as the function f ?,? : R n ? T ? R, where ? ? R dg denotes the shared parameters across the tasks and the task specific parameters are given by ? ? R ds?|T| . The objective function for training such a predictor 1 Though beyond the scope of the current work, it is possible to extend this to a few-shot setting as discussed in <ref type="bibr" target="#b37">Srinivasan et al. (2021)</ref>. model can be defined as:</p><formula xml:id="formula_0">J(?, ?) = t?T p?P t?T f (x p,t , t; ?, ?) -y t p,t 2 2 + ? g ? 1 + ? s ? 1,1 + ? group ? 1,q<label>(1)</label></formula><p>The second and third terms regularize the global and task specific parameters independently, while the last term, l 1 /l q norm with q &gt; 1, ensures a block sparse selection of the task specific parameters. This term ensures a multi-task learning behavior even when there are no parameters shared across the tasks (i.e., ? = ?) through selection of common features across the tasks. Setting ? = ? and ? group = 0 leads to the single task setup of <ref type="bibr" target="#b20">Lauscher et al. (2020)</ref> and <ref type="bibr" target="#b37">Srinivasan et al. (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Features</head><p>We divide the set of features into two higher level categories, viz. the pairwise features defined for the pivot and target that measure the typological relatedness of the languages, and the individual features defined for the target language reflecting the state of its representation in M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Pairwise Features</head><p>Instead of directly using the different typological properties of the the two languages as features, we use the pairwise relatedness to avoid feature explosion. Subword Overlap : We define the subword overlap as the percentage of unique tokens that are common to the vocabularies of both the pivot and target languages. Let V p and V t be the subword vocabularies of p and t. The subword overlap is then defined as :</p><formula xml:id="formula_1">o sw (p, t) = |V p ? V t | |V p ? V t | (2)</formula><p>Similarity between Lang2Vec vectors: Following <ref type="bibr" target="#b23">Lin et al. (2019)</ref> and <ref type="bibr" target="#b20">Lauscher et al. (2020)</ref>, we compute the typological relatedness between p and t from the linguistic features provided by the URIEL project <ref type="bibr" target="#b24">(Littell et al., 2017)</ref>. We use syntactic (s syn (p, t)), phonological similarity (s pho (p, t)), genetic similarity (s gen (p, t)) and geographic distance (d geo (p, t)). For details, please see <ref type="bibr" target="#b24">Littell et al. (2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Individual Features</head><p>Pre-training Size: We use the log 10 of the size (in words) of the pre-training corpus in the target language, SIZE(t), as a feature. Rare Typological Traits: <ref type="bibr" target="#b37">Srinivasan et al. (2021)</ref> proposed this metric to capture the rarity of the typological features of a language in the representation of M. Every typological feature in WALS database is ranked based on the amount of pretraining data for the languages that contain the feature. For the language t, Mean Reciprocal Rank (MRR) of all of its features is then calculated and used as a feature -WMRR(t).</p><p>Tokenizer Features : In their recent work, <ref type="bibr" target="#b35">Rust et al. (2021)</ref> proposed two metrics, viz. tokenizer's fertility and proportion of continued words, to evaluate the quality of multilingual tokenizers on a given language. For target t, they define the tokenizer's fertility, FERT(t), as the average number of sub-words produced for every tokenized word in t's corpus. On the other hand, the proportion of continued words, PCW(t), measures how often the tokenizer chooses to continue a word across at least two tokens. They show that the multilingual models perform much worse on a task than their monolingual counterparts when the values of these metrics are higher for the multilingual tokenizer. We include FERT(t) and PCW(t) as features.</p><p>An important thing to note here is that the we do not use identity of a language as a feature while training the models, hence the performance prediction models are capable of generating predictions on new languages unseen during training. However, if the features of the new languages deviate significantly from the features seen during training, the predictions are expected to be less accurate as also observed in <ref type="bibr" target="#b43">Xia et al. (2020)</ref>; <ref type="bibr" target="#b37">Srinivasan et al. (2021)</ref> and is one of the main reasons for exploring a multi-task approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approaches</head><p>We extensively experiment with a wide-array of multi-task as well as single-task regression models to provide a fair comparison between different approaches to zero-shot performance prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines</head><p>Average Score Within a Task (AWT) : The performance for a pivot-target pair (p , t) on a task t is approximated by taking the average of the performance on all other target languages (pivot being fixed) in the same task t, i.e., f (x p,t , t) =</p><formula xml:id="formula_2">1 |T |-1 t ?T -{t} y t p,t .</formula><p>Average Score across the Tasks (AAT) : Here instead of averaging over all the target languages within a task, we approximate the performance on a given target language by averaging the scores for that language across the other tasks, i.e.,</p><formula xml:id="formula_3">f (x p,t , t) = 1 |T|-1 t ?T-{t} y t p,t .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Single Task Models</head><p>Lasso Regression: Lauscher et al. ( <ref type="formula">2020</ref>) train different linear regression models for each task.</p><p>Along similar lines, we experiment with linear regression, but also add an L1 regularization term, as we observed it usually leads to better predictors.</p><p>XGBoost Regressor: As shown in <ref type="bibr" target="#b37">Srinivasan et al. (2021)</ref>, XGBoost <ref type="bibr" target="#b7">(Chen and Guestrin, 2016)</ref> generally obtains impressive performance on this task, and hence we include it in our experiments as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multi Task Models</head><p>Group Lasso: l 1 /l q norm based blockregularization has been shown to be effective for multi-task learning in the setting of multi-linear regression <ref type="bibr" target="#b46">(Yuan and Lin, 2006;</ref><ref type="bibr" target="#b1">Argyriou et al., 2008)</ref>. For each task, consider separate linear regression models represented by the weight matrix ? ? R n?|T| . The l 1 /l q regularization term is given as:</p><formula xml:id="formula_4">? 1,q = n j=1 ( |T| t=1 |? jt | q ) 1/q</formula><p>, where ? jt denotes the weight for the feature j in the task t. For q &gt; 1, minimizing this term pushes the l q -norms corresponding to the weights of a given feature across the tasks to be sparse, which encourages multiple predictors to share similar sparsity patterns. In other words, a common set of features is selected for all the tasks. We use q = 2 for the group regularization term.</p><p>Since this can be restrictive in certain scenarios, some natural extensions to Group Lasso, such as Dirty Models <ref type="bibr" target="#b17">(Jalali et al., 2010)</ref> and Multi Level Lasso <ref type="bibr" target="#b25">(Lozano and Swirszcz, 2012)</ref>, have been proposed that separate out the task specific and global parameters. We experimented with these methods and observed equivalent or worse performance compared to Group Lasso. Collective Matrix Factorization (CMF) with Side Information: Low rank approximation for the task weights matrices forms one family of methods for multi-task learning <ref type="bibr" target="#b47">(Zhang and Yang, 2017;</ref><ref type="bibr" target="#b31">Pong et al., 2010;</ref><ref type="bibr" target="#b0">Ando et al., 2005)</ref>. As a direct analogue with collaborative filtering, here we can think of the tasks as users and pivot-target pairs as items. Consider the matrix Y ? R |T|?|P?T | , where each element of the matrix correspond to y t p,t . We can then decompose the matrix into task and language-pair specific factors as</p><formula xml:id="formula_5">Y ? TL T (3)</formula><p>where T ? R |T|?d latent and L ? R |P?T |?d latent are the task and language-pair factor matrices, and d latent is the number of factors. Additionally, in order to incorporate the feature information about the language pairs as discussed in section 3.2, we incorporate Collective Matrix Factorization approach <ref type="bibr" target="#b11">(Cortes, 2018)</ref>. It incorporates the attribute information about items and/or users in the factorization algorithm by decomposing the language-pair feature matrix X ? R |P?T |?n as LF T , such that L is shared across both decompositions. This helps to learn the latent representations for the pivot-language pairs from the task-wise performance as well as different linguistic and MMLM specific features<ref type="foot" target="#foot_0">2</ref> . In relation to Equation <ref type="formula" target="#formula_0">1</ref>, we can think of task factors T to correspond to the task specific parameters ?, languagepair factors L as the shared parameters ? and the predictor model as f (x p,t , t; ?, ?) = (TL T ) (p,t),t . Both L and T are regularized seperately, but there is no group regularization term (? group = 0).</p><p>Ye et al. ( <ref type="formula">2021</ref>) also uses a Tensor Factorization approach for performance prediction which is similar to our CMF method. However, they train separate models for each task and factorize over metric specific attributes instead for a fine-grained prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task Deep Gaussian Process Regression (MDGPR):</head><p>We use the multi-task variant of Gaussian Processes proposed in <ref type="bibr" target="#b5">Bonilla et al. (2008)</ref> and utilize deep neural networks to define the kernel functions as in Deep GPs <ref type="bibr" target="#b40">(Wilson et al., 2016)</ref>. For comparison, we also report the scores of the single-task variant of this method which we denote as DGPR. See Appendix (section A.1) for details.</p><p>Apart from these we also explore other multitask methods like Model Agnostic Meta Learning (MAML) <ref type="bibr" target="#b15">(Finn et al., 2017)</ref>, details of which we leave in the appendix (section A.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>In this section, we discuss our test conditions, datasets and training parameters for the different experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Test Conditions</head><p>We consider two different test conditions: Leave One Language Out (LOLO) and Leave Low Resource Languages Out (LLRO). Leave One Language Out: LOLO is a popular setup for multilingual performance prediction <ref type="bibr" target="#b20">(Lauscher et al., 2020;</ref><ref type="bibr" target="#b37">Srinivasan et al., 2021)</ref>, where for a given task, we choose a target language and move all of its instances from the prediction dataset to the test data. The models are then trained on the remaining languages and evaluated on the unseen test language. This is done for all the target languages available for a task, and the Mean Absolute Error (MAE) across languages is reported. In the multi-task setting we evaluate on one task at a time while considering the rest as helper tasks for which the entire data is used including the test language<ref type="foot" target="#foot_1">3</ref> . Leave Low Resource Languages Out: Through this evaluation strategy we try to emulate the real world use case where we only have test data available in high resource languages such as English, German and Chinese, and would like to estimate the performance on under-represented languages such as Swahili and Bengali. We use the language taxonomy provided by <ref type="bibr" target="#b18">Joshi et al. (2020)</ref> to categorize the languages into six classes (0 = low to 5 = high) based on the number of resources available. We then move languages belonging to class 3 or below to our test set and train the models on class 4 and 5 languages only. Similar to LOLO, here too we allow the helper tasks to retain all the languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Tasks and Datasets</head><p>We use the following 11 tasks provided in XTREME <ref type="bibr" target="#b16">(Hu et al., 2020)</ref> and XTREME-R <ref type="bibr" target="#b34">(Ruder et al., 2021)</ref> benchmarks: 1. Classification: XNLI <ref type="bibr" target="#b10">(Conneau et al., 2018)</ref> , PAWS-X <ref type="bibr" target="#b44">(Yang et al., 2019), and</ref><ref type="bibr">XCOPA (Ponti et al., 2020)</ref> 2. Structure Prediction: UDPOS <ref type="bibr" target="#b26">(Nivre et al., 2018)</ref>, and NER <ref type="bibr" target="#b27">(Pan et al., 2017)</ref> 3. Question Answering: XQUAD <ref type="bibr" target="#b2">(Artetxe et al., 2020)</ref>, MLQA <ref type="bibr" target="#b22">(Lewis et al., 2020)</ref>, and TyDiQA-GoldP <ref type="bibr" target="#b8">(Clark et al., 2020)</ref>  2019), Mewsli-X <ref type="bibr" target="#b6">(Botha et al., 2020;</ref><ref type="bibr" target="#b34">Ruder et al., 2021)</ref>, and LAReQA <ref type="bibr" target="#b33">(Roy et al., 2020)</ref> All of these datasets have training data present only in English i.e. P = {en}, and majority of the tasks have fewer than 10 target languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training Details</head><p>We train and evaluate our performance prediction models for mBERT (bert-base-multilingualcased) and XLM-R (xlm-roberta-large). For training XGBoost, we used 100 estimators with a maximum depth of 10. For Group Lasso, we used the implementation provided in the MuTaR software package<ref type="foot" target="#foot_2">4</ref> , and used a regularization strength of 0.01. We optimized CMF's objective function using Alternating Least Squares (ALS), used 5 latent factors with a regularization parameter equal to 0.1, and used the Collective Matrix Factorization python library<ref type="foot" target="#foot_3">5</ref> . In case of MDGPR, we used Radial Basis Function as the kernel and a two-layer MLP for learning latent features, with 50 and 10 units followed by ReLU activation. We set the learning rate and epochs as 0.01 and 200, and implemented it using GPyTorch<ref type="foot" target="#foot_4">6</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">LOLO Results</head><p>Table <ref type="table" target="#tab_0">1</ref> shows MAE (in %) for LOLO for different single-task and multi-task models on the tasks. For XLMR, we observe that multi-task models, primarily MDGPR, often outperform the best single-task models by significant margins, and for tasks like MewsliX we even see about 36% reduction in MAE. Overall, we see about 10% drop in LOLO errors on average for MDGPR compared to the best performing single-task model i.e. Lasso Regression. As expected, the benefit of multi-task learning is even more prominent when we consider the tasks for which only a few (? 10) data points are available. Here we see about 20% reduction in errors. For mBERT as well, we have similar observations, except that CMF performs slightly better than MDGPR.</p><p>Note that the Average across task baseline is quite competitive and performs better than singletask XGBoost and MAML in average, and better than all models for LAReQA.</p><p>Figure <ref type="figure">2</ref> plots the dependence of the number of helper tasks on the performance of the multi-task models. As expected, MAE decreases as helper tasks increase, especially for MDGPR and CMF. On a related note, the Pearson Correlation coefficient between MAE and number of tasks a target language is part of is found to be -0.39, though the trend in this case is not as clear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">LLRO Results</head><p>Predicting the performance on low resource languages, for which often standard training and test datasets are not available, can be an important use case where multi-task performance prediction can be helpful. Figure <ref type="figure">6</ref> in appendix shows the classwise <ref type="bibr" target="#b18">(Joshi et al., 2020)</ref> distribution of languages for the tasks that we consider in our experiments. As one would expect, for most tasks, test data is available for languages belonging to class-4 and class-5. Training performance prediction models without any task to transfer from can therefore, possibly lead to poor generalization on the low resource languages. On the other hand, for the same reason -lack of test data, building accurate predictors for low-resource languages is necessary.</p><p>MAE values for the LLRO evaluation setup are shown in figure <ref type="figure">1</ref> for XLMR. Results for mBERT follow similar trends and are reported in the Appendix (figure <ref type="figure">7</ref>). For both XLMR and mBERT we observe that the three main multi-task models -Group Lasso, CMF and MDGPR -outperform the single-task models and baselines. Interestingly, for XLMR, the single task models XGBoost and Lasso perform even worse than the Average within Tasks baseline. Overall we see around 18% and 11% drop in MAE for Group Lasso over the best performing single-task model, for XLMR and mBERT respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Feature Importance</head><p>An interesting consequence of zero-shot performance prediction is that the models can be directly used to infer the correlation (and possibly causation) between linguistic relatedness and pretraining conditions and zero-shot transferability. Multi-task learning, this context, help us make more robust inferences, as the models are less prone to overfitting to a particular task or dataset.</p><p>Figure <ref type="figure" target="#fig_0">3</ref> shows the SHAP values of the features for the Group Lasso model trained on XLMR's zero-shot performance data. As expected for Group Lasso, we see a block-sparsity behavior among the tasks. Features such as Rare Typological Traits (WMRR(t)), Tokenizer's Fertility (FERT(t)) and Genetic Similarity (s gen (p, t)) are ignored in all the tasks. In contrast, for the single-task lasso regression (Figure <ref type="figure" target="#fig_5">9</ref> in Appendix), we see different sets of features selected for different tasks, which for  the scale at which we operate, might not be indicative of the actual factors that affect the zero-shot performance in these tasks. Subword Overlap. Among the features that get selected for all tasks, we observe that Subword Overlap (o sw (p, t)) typically gets higher importance in retrieval (LAReQA and MewsliX) and sentence classification tasks (PAWS-X, XNLI). Since the retrieval tasks that we consider, as described in <ref type="bibr" target="#b34">Ruder et al. (2021)</ref>, measure the alignment between the cross lingual representations of semantically similar sentences, having a shared vocabulary between the languages can leak information from one to another <ref type="bibr" target="#b41">(Wu and Dredze, 2019)</ref> which might improve the retrieval performance. Interestingly, if we compare this with the feature importance scores for the single task lasso model (Figure <ref type="figure" target="#fig_5">9</ref> in Appendix), we do see MewsliX task getting higher importance for the subword overlap, but LAReQA gets virtually zero SHAP value for this feature, showcasing how single-task models can misinterpret two similar tasks as requiring very different features. Our observation reinforce the generally held notion that vocabulary overlap between the pivot and target is beneficial for zero-shot transfer <ref type="bibr" target="#b41">(Wu and Dredze, 2019)</ref>, especially for retrieval tasks, though some studies have argued otherwise <ref type="bibr" target="#b28">(Pires et al., 2019;</ref><ref type="bibr" target="#b19">K et al., 2020)</ref>. Tokenizer Features. For structure prediction (UDPOS and WikiAnn) and question answering (XQUAD and TyDiQA) tasks that require making predictions for each token in the input, we see that the tokenizer feature, PCW(t), receive a higher SHAP value. In contrast, for single-task lasso, here too we do not observe high importance of this feature across these related tasks. <ref type="bibr" target="#b35">Rust et al. (2021)</ref> note that languages such as Arabic where mBERT's multilingual tokenizer was found to be much worse than it's monolingual counterpart, there was a sharper drop in performance of mBERT compared to the monolingual model for QA, UDPOS and NER tasks than for sentiment classification. We believe that XLMR's surprisingly worse performance than mBERT for Chinese and Japanese on UDPOS might be correlated with it's significantly worse tokenizer for these languages based on the fertility (FERT) and Percentage Continued Words (PCW) feature values (see Appendix A.2 for exact values). The high SHAP values for PCW(t) further strengthen our belief<ref type="foot" target="#foot_5">7</ref> . Pre-training Size. Similar to the findings of <ref type="bibr" target="#b20">Lauscher et al. (2020)</ref>, we observe that pre-training corpus size has low SHAP value, and therefore, lower importance for lower level tasks such as UDPOS and NER, and higher SHAP values for higher level tasks like XNLI. Additionally, we extend their observations to tasks such as XCOPA, Tatoeba, MLQA and LAReQA where pre-training size seem to play a significant role in the performance prediction. Again, compared to single Lasso Regression model, we see a different selection pattern: Pre-training size receives a high SHAP value for UDPOS while for XNLI it is negligible. This neither fully conforms with our observations on the multi-task feature selections, nor with the previous work <ref type="bibr" target="#b20">(Lauscher et al., 2020)</ref>. Typological Relatedness Features. Out of all the typological relatedness features, we found Geographical Distance (d geo (p, t)) receiving highest SHAP values for all tasks, implying that geographical proximity between the pivot-target pair is an important factor in determining the zero-shot trans-ferability between them. <ref type="bibr" target="#b20">Lauscher et al. (2020)</ref> also observe positive correlations between geographical relatedness and zero-shot performance. The crosstask importance of geographic distance (unlike the other relatedness features) might be attributed to the 100% coverage across languages for the geographical vectors in the URIEL database. In contrast, Syntactic and Phonological vectors have missing values for a majority of the languages <ref type="bibr" target="#b24">(Littell et al., 2017)</ref>.</p><p>Like <ref type="bibr" target="#b20">Lauscher et al. (2020)</ref>, we also see some dependence on syntactic (s syn (p, t)) and phonological (s pho (p, t)) similarities for XLMR's zero shot performance on XNLI and XQUAD tasks respectively. However, in both cases we found that the tokenizer feature PCW(t) receives a much higher SHAP value. Interestingly, genetic similarity (s gen (p, t)) is not selected for any task, arguably due to the block sparsity in feature selection of Group Lasso. We do see some tasks receiving high SHAP values for s gen (p, t) in single-task lasso (Figure <ref type="figure" target="#fig_5">9</ref> in Appendix). However, the number of such tasks as well as the SHAP values are on the lower side, implying that genetic similarity might not provide any additional information for zero-shot transfer over and above the geographical, syntactic and phonological similarities.</p><p>Similar trends are observed in the case of mBERT as well (Figure <ref type="figure" target="#fig_6">10</ref> in appendix), with some minor differences. For instance, instead of PCW(t), FERT(t) receives higher SHAP value; s syn (p, t) also receives higher importance, especially for tasks like UDPOS and XNLI, which is consistent with the findings of <ref type="bibr" target="#b20">Lauscher et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>In this paper, we showed that the zero-shot performance prediction problem can be much more effectively and robustly solved by using multi-task learning approaches. We see significant reduction in errors compared to the baselines and single-task models, specifically for the tasks which have test sets available in a very few languages or when trying to predict the performance for low resource languages. Additionally, this approach allows us to robustly identify factors that influence zero-shot performance. Our findings in this context can be summarized as follows.</p><p>1. Subword overlap between the pivot and target has a strong positive influence on zero-shot trans-fer, especially for Retrieval tasks. 2. Quality of the target tokenizer, defined in terms of how often or how aggressively it splits the target tokens negatively influences zero-shot performance for word-level tasks such as POS tagging and Span extraction. 3. Pre-training size of the target positively influences zero-shot performance in many tasks, including XCOPA, Tatoeba, MLQA and LAReQA. 4. Geographical proximity between pivot and target is found to be uniformly important across all the tasks, unlike syntactic and phonological similarities, which are important for only some tasks.</p><p>This last finding is especially interesting. As described earlier, geographical proximity is a more clear, noise-free and complete feature compared to the other relatedness metrics. However, one could also argue that since neighboring languages tend to have high vocabulary and typological feature overlap due to contact processes and shared areal features, geographical distance is an extremely informative feature for zero-shot transfer. Two direct implications of these findings are: (1) for effective use of MMLMs, one should develop resources in at least one pivot language per geographic regions, and (2) one should work towards multilingual tokenizers that are effective for most languages.</p><p>There are a number of directions that can be explored in future related to our work. The prediction models can be extended to a multi-pivot and few-shot settings, as described in <ref type="bibr" target="#b37">Srinivasan et al. (2021)</ref>. Further probing experiments could be designed to understand the role of sub-word overlap on zero-shot transfer of Retrieval tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Additional Details of Approaches Used Gaussian Process Regression (GPR): We start by briefly reviewing Gaussian Processes (GP) in context of the zero-shot performance prediction problem. For a pivot-target language pair (p, t) and a task t, the GP prior and the likelihood function can be defined as: </p><formula xml:id="formula_6">f ? N (? t , K t ); y|f (x p,t ) ? N (</formula><p>The GP prior will be defined by replacing the task specific kernel K t in the equation 4 with the multi-task kernel K m . We use the optimization steps similar to DGP and the inference is done by using the standard GP formulae.</p><p>Relating MDGPR to equation 1, the global parameters ? are the parameters of the deep network g, and the task specific parameter ? is the positive semi-definite matrix K task . Model Agnostic Meta Learning (MAML): MAML <ref type="bibr" target="#b15">(Finn et al., 2017)</ref> is a popular meta learning algorithm that can be used to quickly adapt Deep Neural Networks on new tasks in a few-shot setting. In MAML, the set of initialization parameters for the neural network are explicitly learned such that the network can generalize well on a new task with a small number of gradient steps and training samples.</p><p>Relating to equation 1, the global parameters ? can be considered as the initial set of parameters for the neural network that are learned and shared across all the tasks. Task specific parameters ? are adapted from ? by taking K gradient steps using the task's performance data.</p><p>For evaluating a task t, we consider rest of the tasks in our dataset as helpers (t ? T-{t}) and use them to train the initial set of parameters ?. The initial parameters are then updated by fine-tuning the network on the training set for t using gradient descent. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Comparison between mBERT and XLMR Tokenizers</head><p>The FERT and PCW metrics as proposed by <ref type="bibr" target="#b35">Rust et al. (2021)</ref>, have been compared for mBERT and XLMR in figure <ref type="figure" target="#fig_1">4</ref>. As can be seen, for most languages the metric values are similar across the two tokenizers, however for languages like Chinese and Japanese, there is a dramatic increase in the values for XLMR. Interestingly, when we compare the zero-shot performance between mBERT and XLMR on structure prediction tasks like UD-POS and WikiANN, we see a surprisingly large drop (upto 20% absolute drop) in the performance for XLMR on these both Chinese and Japanese, whereas usually XLMR outperforms mBERT on these tasks (Refer to figure <ref type="figure">5</ref>). This observation along with the feature importance for the tokenizer features that we observed for Group Lasso (3) indicate that tokenizer quality might play some role in the zero-shot transfer capabilities of the multilingual models. Table <ref type="table">2</ref>: Mean Absolute Errors (Scaled by 100 for readability) for different models trained to predict the zero shot performance of mBERT. In the "Average" row we average the MAEs across all the tasks and in the "Average Low" Res Tasks", we consider the tasks with fewer than 10 target languages and take the average of the MAEs for those tasks.          </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Task-wise mean SHAP values of different features for the Group Lasso model trained on XLMR zero-shot performance data. Higher value implies stronger effect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Comparison of Tokenizer metrics as described by<ref type="bibr" target="#b35">Rust et al. (2021)</ref> on different languages for MBERT and XLMR. For most languages both model's have similar values of fertility and proportion of continued words, however for Chinese and Japanese the values for XLMR are much higher, which might indicate the subpar quality of XLMR's tokenizer in these languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Zero-shot performance comparison between mBERT and XLMR on (a) UDPOS and (b) WikiANN (NER) tasks, as given in Ruder et al. (2021)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Number of helper tasks vs. LOLO MAE for mBERT. Errors for different model types (Group Lasso, CMF and MDGPR) and tasks are scaled by diving them by the maximum value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Task-wise mean SHAP values of different features for the Single Task Lasso Regression model trained on XLMR zero-shot performance data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Task-wise mean SHAP values of different features for the Group Lasso model trained on mBERT zero-shot performance data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Task-wise mean SHAP values of different features for the Single Task Lasso Regression model trained on mBERT zero-shot performance data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>4. Retrieval: Tatoeba (Artetxe and Schwenk, Mean Absolute Error (scaled by 100 for readability) for LOLO for different approaches across tasks. We also report the average MAE across all tasks ("Average") and for tasks which has less than or equal to 10 languages ("Average (|T | ? 10)"). Task-wise results for mBERT can be found in the Appendix (table2)</figDesc><table><row><cell>MMLM</cell><cell>Task</cell><cell>|T |</cell><cell>Baselines</cell><cell></cell><cell cols="3">Single Task Models</cell><cell></cell><cell cols="2">Multi Task Models</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="9">Average within Task Average across Tasks Lasso XGBoost DGPR Group Lasso CMF MDGPR MAML</cell></row><row><cell></cell><cell>MLQA</cell><cell>7</cell><cell>2.92</cell><cell>2.26</cell><cell>4.33</cell><cell>2.91</cell><cell>3.26</cell><cell>2.21</cell><cell>2.66</cell><cell>2.96</cell><cell>4.89</cell></row><row><cell></cell><cell>PAWS</cell><cell>7</cell><cell>3.34</cell><cell>0.9</cell><cell>0.8</cell><cell>1.28</cell><cell>1.27</cell><cell>1.32</cell><cell>1.39</cell><cell>2.71</cell><cell>6.62</cell></row><row><cell></cell><cell>XCOPA</cell><cell>8</cell><cell>4.52</cell><cell>5.91</cell><cell>2.42</cell><cell>4.16</cell><cell>4.73</cell><cell>2.69</cell><cell>2.03</cell><cell>1.96</cell><cell>6.28</cell></row><row><cell></cell><cell>TyDiQA</cell><cell>9</cell><cell>4.29</cell><cell>5.48</cell><cell>5.89</cell><cell>5.63</cell><cell>6.56</cell><cell>5.04</cell><cell>5.88</cell><cell>4.61</cell><cell>4.96</cell></row><row><cell></cell><cell>XQUAD</cell><cell>10</cell><cell>4.90</cell><cell>4.22</cell><cell>4.54</cell><cell>6.56</cell><cell>4.13</cell><cell>4.16</cell><cell>3.86</cell><cell>3.15</cell><cell>6.85</cell></row><row><cell></cell><cell>LAReQA</cell><cell>10</cell><cell>2.10</cell><cell>1.51</cell><cell>1.53</cell><cell>1.56</cell><cell>1.78</cell><cell>1.52</cell><cell>1.87</cell><cell>2.69</cell><cell>8.22</cell></row><row><cell>XLMR</cell><cell>MewsliX XNLI</cell><cell>10 14</cell><cell>16.61 3.07</cell><cell>15.48 2.07</cell><cell>15.70 1.97</cell><cell>21.16 1.53</cell><cell>15.66 2.16</cell><cell>13.73 2.17</cell><cell>14.62 2.17</cell><cell>10.07 3.54</cell><cell>9.33 4.55</cell></row><row><cell></cell><cell>WikiANN</cell><cell>32</cell><cell>15.22</cell><cell>11.61</cell><cell>10.14</cell><cell>10.26</cell><cell>12.64</cell><cell>10.92</cell><cell>11.36</cell><cell>9.15</cell><cell>13.19</cell></row><row><cell></cell><cell>Tatoeba</cell><cell>35</cell><cell>8.69</cell><cell>8.68</cell><cell>5.82</cell><cell>7.14</cell><cell>6.80</cell><cell>5.83</cell><cell>6.08</cell><cell>8.09</cell><cell>9.72</cell></row><row><cell></cell><cell>UDPOS</cell><cell>48</cell><cell>10.15</cell><cell>7.65</cell><cell>7.52</cell><cell>5.12</cell><cell>6.02</cell><cell>7.72</cell><cell>7.89</cell><cell>5.88</cell><cell>10.71</cell></row><row><cell></cell><cell>Average</cell><cell>19</cell><cell>6.89</cell><cell>5.98</cell><cell>5.51</cell><cell>6.12</cell><cell>5.91</cell><cell>5.21</cell><cell>5.44</cell><cell>4.98</cell><cell>7.76</cell></row><row><cell></cell><cell>Average (|T | ? 10)</cell><cell>9</cell><cell>5.53</cell><cell>5.11</cell><cell>5.03</cell><cell>6.18</cell><cell>5.34</cell><cell>4.38</cell><cell>4.62</cell><cell>4.02</cell><cell>6.73</cell></row><row><cell>mBERT</cell><cell>Average Average (|T | ? 10)</cell><cell>19 9</cell><cell>8.69 6.96</cell><cell>6.57 5.64</cell><cell>5.55 4.99</cell><cell>6.86 6.54</cell><cell>6.10 5.73</cell><cell>5.45 4.44</cell><cell>5.08 4.18</cell><cell>5.12 4.53</cell><cell>8.14 7.51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Number of helper tasks vs. LOLO MAE. Errors for different model types (Group Lasso, CMF and MDGPR) and tasks are scaled by diving them by the maximum error value.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Normalized LOLO Error</cell><cell>0.4 0.5 0.6 0.7 0.8 0.9 1.0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4 Number of Helper Tasks 5 6 7 XLMR</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>Model Group Lasso CMF MDGPR</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">WMRR(t) 0 0 0 0 0 0 0 0 0 0 0 Mean SHAP values (XLMR) -Group Lasso PCW(t) FERT(t) d geo (p, t) s syn (p, t) s pho (p, t) s gen (p, t) o sw (p, t) 0.17 0.09 0 0.34 0.05 0.06 0 0.29 0.05 0.02 0 0.4 0.07 0.12 0 0.34 0.23 0.11 0 0.15 0.04 0.28 0 0.19 0.04 0.46 0 0.12 0.06 0.31 0 0.02 0.11 0.27 0 0.13 0.09 0.17 0 0.23 0.02 0.32 0 0.26 0.1 0.19 0 0.11 0.19 0.01 0 0.24 0.17 0.02 0 0.38 0.33 0.1 0 0.24 0.09 0.15 0 0.09 0.01 0.29 0 0.37 0.11 0.18 0 0.04 0.03 0.43 0 0.32 0.02 0.12 0 0.08 Figure 2: SIZE(t) LAReQA MewsliX MLQA TyDiQA PAWS-X WikiANN XNLI Tatoeba UDPOS XQUAD 0.63 0.05 0 0.14 0.04 0.06 0 0.07 XCOPA</cell></row><row><cell></cell><cell>14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>XLMR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Average LLRO Error</cell><cell>8 10 12</cell><cell>11.86</cell><cell>10.72</cell><cell>10.34</cell><cell>10.16</cell><cell>9.66</cell><cell>9.59</cell><cell>9.30</cell><cell>9.03</cell><cell>8.28</cell></row><row><cell></cell><cell>6</cell><cell>XGBoost</cell><cell>MAML</cell><cell>DGPR</cell><cell>Lasso</cell><cell>AWT</cell><cell>AAT</cell><cell>CMF</cell><cell>Group Lasso</cell><cell>MDGPR</cell></row><row><cell cols="11">Figure 1: Leave Low Resource Out (LLRO) results for</cell></row><row><cell cols="3">XLMR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>(x p,t , x p ,t ) is the kernel of the GP defined on the task t. ? 2 t denotes the noise variance. Wilson et al., 2016) to learn rich features from the observed data. Specifically, the kernel k t (x p,t , x p ,t ) now takes the transformed inputs as k t (x p,t , x p ,t ) = k t (g(x p,t ), g(x p ,t )) (5) where g(x) is a non-linear mapping given by a deep network. Please refer to Wilson et al. (2016) for a detail account on optimization of DGP. We use the multi-task variant of Gaussian Processes proposed in Bonilla et al. (2008) where inter-task similarities are learnt solely based on the task identities and the observed data for each task. Instead of learning task-specific kernels k t (g(x p,t ), g(x p ,t )), we will have a common kernel over the inputs as k(g(x p,t ), g(x p ,t )) and a positive semi-definite matrix K task for learning inter-task similarities. Specifically, we define the multi-task kernel K m as follows k m ([x p,t , t], [x p ,t , t ]) = k(g(x p,t ), g(x p ,t )) * k task (t, t ) (</figDesc><table><row><cell>y t p,t ; f (x p,t ), ? 2 t ) (4) where ? t is the mean and K t (p,t),(p ,t ) = k t Deep Gaussian Process Regression (DGPR): We use DGP (Multi-Task Deep Gaussian Process Regression (MDGPR):</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Note that we can use a similar approach for providing side information for the tasks as well.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Note that this is a reasonable relaxation to make as it is closer to the real world use case where we would have the evaluation data for some languages in the standard tasks and would like to utilize that to make predictions on the same languages for the new ftask.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://github.com/hichamjanati/mutar</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://github.com/david-cortes/ cmfrec</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://gpytorch.ai/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>Note that<ref type="bibr" target="#b35">Rust et al. (2021)</ref> shows the importance of tokenizer metrics for the case where the multilingual models are fine-tuned on the target language, whereas we analyze their importance for zero-shot transfer.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank the <rs type="institution">LITMUS team at Microsoft</rs> for their valuable inputs and feedback over the course of this project.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName><forename type="first">Rie</forename><surname>Kubota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ando</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convex multi-task feature learning</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="272" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the Cross-lingual Transferability of Monolingual Representations</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2020</title>
		<meeting>ACL 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<title level="m">Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond. Transactions of the ACL</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting success in machine translation</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="745" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-task gaussian process prediction</title>
		<author>
			<persName><forename type="first">Kian</forename><surname>Edwin V Bonilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Entity Linking in 100 Languages</title>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zifei</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.630</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7833" to="7845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In Transactions of the Association of Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">XNLI: Evaluating cross-lingual sentence representations</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2018</title>
		<meeting>EMNLP 2018</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2475" to="2485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cold-start recommendations in collective matrix factorization</title>
		<author>
			<persName><forename type="first">David</forename><surname>Cortes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.00366</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Analysing the impact of linguistic features on crosslingual transfer</title>
		<author>
			<persName><forename type="first">B?a?ej</forename><surname>Dolicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerasimos</forename><surname>Spanakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05975</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Domhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-fourth international joint conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation</title>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="4411" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A dirty model for multi-task learning</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujay</forename><surname>Sanghavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Chao Ruan</surname></persName>
		</author>
		<author>
			<persName><surname>Ravikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The state and fate of linguistic diversity and inclusion in the NLP world</title>
		<author>
			<persName><forename type="first">Pratik</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastin</forename><surname>Santy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Budhiraja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalika</forename><surname>Bali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monojit</forename><surname>Choudhury</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.560</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6282" to="6293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross-lingual ability of multilingual bert: An empirical study</title>
		<author>
			<persName><forename type="first">K</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Lauscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinit</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Glava?</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.363</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4483" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The third international Chinese language processing bakeoff: Word segmentation and named entity recognition</title>
		<author>
			<persName><forename type="first">Gina-Anne</forename><surname>Levow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
		<meeting>the Fifth SIGHAN Workshop on Chinese Language Processing<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="108" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MLQA: Evaluating Cross-lingual Extractive Question Answering</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2020</title>
		<meeting>ACL 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Choosing transfer languages for cross-lingual learning</title>
		<author>
			<persName><forename type="first">Yu-Hsiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chian-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengzhou</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Rijhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhisong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Littell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1301</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3125" to="3135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">URIEL and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Littell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Kairis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlisle</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
	<note>Short Papers. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multilevel lasso for sparse multi-task regression</title>
		<author>
			<persName><forename type="first">Aurelie</forename><forename type="middle">C</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grzegorz</forename><surname>Swirszcz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Coference on International Conference on Machine Learning</title>
		<meeting>the 29th International Coference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="595" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Abrams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?eljko</forename><surname>Agi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Ahrenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lene</forename><surname>Antonsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Jesus Aranzabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gashaw</forename><surname>Arutie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luma</forename><surname>Ateyah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Attia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Universal dependencies 2.2</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Crosslingual name tagging and linking for 282 languages</title>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017</title>
		<meeting>ACL 2017</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1946" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How multilingual is multilingual BERT?</title>
		<author>
			<persName><forename type="first">Telmo</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1493</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4996" to="5001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Estimating accuracy from unlabeled data: A probabilistic logic approach</title>
		<author>
			<persName><forename type="first">Emmanouil</forename><surname>Platanios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Avrim</forename><surname>Emmanouil Antonios Platanios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><surname>Mitchell</surname></persName>
		</author>
		<title level="m">Estimating accuracy from unlabeled data</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Trace norm regularization: Reformulations, algorithms, and multi-task learning</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Kei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3465" to="3489" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">XCOPA: A multilingual dataset for causal commonsense reasoning</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Glava?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianchu</forename><surname>Majewska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><surname>Korhonen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.185</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2362" to="2376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">LAReQA: Language-agnostic answer retrieval from a multilingual pool</title>
		<author>
			<persName><forename type="first">Uma</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.477</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5919" to="5930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">XTREME-R: Towards more challenging and nuanced multilingual evaluation</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.802</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10215" to="10245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">How good is your tokenizer? on the monolingual performance of multilingual language models</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Rust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3118" to="3135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A corpus for multilingual document classification in eight languages</title>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunayana</forename><surname>Sitaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanuja</forename><surname>Ganu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandipan</forename><surname>Dandapat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalika</forename><surname>Bali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monojit</forename><surname>Choudhury</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08875</idno>
		<title level="m">Predicting the performance of multilingual nlp models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-02: The 6th Conference on Natural Language Learning</title>
		<imprint>
			<date type="published" when="2002">2002. 2002. CoNLL-2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meulder</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep kernel learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gordon Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 19th International Conference on Artificial Intelligence and Statistics<address><addrLine>Cadiz, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT</title>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1077</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="833" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Are all languages created equal in multilingual BERT?</title>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.repl4nlp-1.16</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Representation Learning for NLP</title>
		<meeting>the 5th Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Predicting performance for natural language processing tasks</title>
		<author>
			<persName><forename type="first">Mengzhou</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonios</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.764</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8625" to="8646" />
		</imprint>
	</monogr>
	<note>Yiming Yang, and Graham Neubig</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">PAWS-X: A cross-lingual adversarial dataset for paraphrase identification</title>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2019</title>
		<meeting>EMNLP 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3685" to="3690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Towards more fine-grained and reliable NLP performance prediction</title>
		<author>
			<persName><forename type="first">Zihuiwen</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">Neubig. 2021</date>
			<biblScope unit="page" from="3703" to="3714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A survey on multitask learning</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2021.3070203</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
